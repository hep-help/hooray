[
 {
  "author":{
   "login":"nongiga"
  },
  "body":"Hi there,\r\n\r\nI am new to python and am really excited by AwkwardArray: it is exactly what I switched programming languages for!\r\n\r\nI have this question that might be ridiculously easy, but I couldn't find a good answer at the API or previous discussions:\r\n\r\nI have two AwkwardArrays (dumping partial records in JSON form):\r\n\r\n```\r\n{\r\n    \"CaseNum\": 3119905,\r\n    \"Isolates\": [\r\n      {\r\n        \"IsoNum\": 5567545\r\n        \"SeqsDir\": \"Sequencing_analysis/Seqplate1to13_17to25/\",\r\n        \"SiteString\": \"Urine\"\r\n      }\r\n    ],\r\n  }, \r\n```\r\n\r\n```\r\n{\r\n    \"CaseNum\": 1905256,\r\n    \"Seqplates\": [\r\n      \"1_E5\",\r\n      \"1_D5\"\r\n    ]\r\n  },\r\n  {\r\n    \"CaseNum\": 1907609,\r\n    \"Seqplates\": [\r\n      \"22_H10\",\r\n      \"23_H4\"\r\n    ]\r\n  },\r\n```\r\nThey are in different sizes. I want to merge the first with the second based on the RandomID. How do I do that most efficiently?\r\n\r\nThanks!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Hi! If I'm reading your question right, this is an important database/data analysis operation known as \"joining.\" Awkward Array doesn't have a built-in operation for joining because the scope of the project is to be \"NumPy with data structures,\" not \"Pandas with data structures.\" Obviously, the latter would also be useful, but one step at a time. (We're thinking about ways of doing this; see, for example, https://github.com/scikit-hep/awkward-1.0/issues/350#issuecomment-664574209 and search for discussions about Awkward Xarray.)\r\n\r\nBy \"NumPy\" vs \"Pandas,\" I'm referring to a level of abstraction. Pandas has this join operation (which they call \"merging\") as a [primary high-level feature](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html). With NumPy, you can do it, but it takes more steps. Likewise, you can do it with Awkward Array as a multi-step process, too. In the example below, I'll be using only vectorized operations (no Python for loops), though joining can't be done in _O(n)_ time (where _n_ is the length of the input arrays); I'm pretty sure the following is _O(n log n)_.\r\n\r\nLet me start with a highly simplified example because I'll be walking through the logic of it, and I want as few distractions as possible. It may be possible to wrap this up into a generic function, and maybe a function like that should become one of the built-in Awkward operations.\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import numpy as np\r\n>>> one = ak.Array([{\"x\": 3}, {\"x\": 1}, {\"x\": 5}, {\"x\": 1}, {\"x\": 2}])\r\n>>> two = ak.Array([{\"y\": 4}, {\"y\": 3}, {\"y\": 3}, {\"y\": 1}, {\"y\": 6}])\r\n```\r\n\r\n`one` and `two` are both arrays of records and they each have a field that will be used to relate one array to the other. If the records had more fields, as yours do, they would \"go along for the ride\" when joining `one` to `two`, so for simplicity, I've left them out entirely. I've added a lot of complications to the indexes so that this example is general enough:\r\n\r\n   * There are repeated indexes in `one`: in two cases, the `x` field is `1`. Only the _first_ of these will be matched to `{\"y\": 1}` in `two` (not the last, due to an implementation detail of `np.unique`).\r\n   * There are repeated indexes in `two`: in two cases, the `y` field is `3`. The matching `{\"x\": 3}` from `one` will be duplicated to combine with each of these.\r\n   * There are indexes in `one` that don't correspond to any in `two`, namely `{\"x\": 5}` and `{\"x\": 2}`. These will be dropped.\r\n   * There are indexes in `two` that don't correspond to any in `one`, namely `{\"y\": 4}` and `{\"y\": 6}`. These will be combined with a missing value (`None`) where a value from `one` is expected.\r\n\r\nNeedless to say, the join will not be symmetric. Any problems with `one`'s index result in skipping the record (it does not appear in the output, even as a placeholder) but any problems with `two`'s index result in duplicating a value from `one` or introducing a placeholder. That makes this either a [\"left outer join\" or a \"right outer join,\"](https://en.wikipedia.org/wiki/Relational_algebra#Outer_joins) depending on whether you consider `one` to be to the left of `two` or not. (If this gets wrapped up in a general-purpose function, care would need to be taken to get the order right to match the formal definition.) If you have an asymmetry between your two arrays, be sure to associate the right ones with `one` and `two`!\r\n\r\nWe're going to use NumPy functions because the corresponding Awkward functions don't exist. The `np.asarray(\u00b7)` and `ak.Array(\u00b7)` functions convert Awkward Arrays into NumPy arrays and back (they are identical to [ak.to_numpy](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_numpy.html) and [ak.from_numpy](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_numpy.html)) _without_ copying data (yay!) _if_ the Awkward Array is an n-dimensional, rectangular array. If it is not, then it raises an exception. One direction is more forgiving than the other because Awkward Array `types` are more general than NumPy `shape` + `dtypes`.\r\n\r\nThe relevant NumPy functions are [np.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html), which will create a sorted, no-duplicates copy of `one`'s index, and [np.searchsorted](https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html), which will find values in `two`'s index that match this dictionary.\r\n\r\n**Step 1:** create a dictionary of `one`'s index.\r\n\r\n```python\r\n>>> # Using the fact that we can zero-copy project out one's index:\r\n>>> one.x\r\n<Array [3, 1, 5, 1, 2] type='5 * int64'>\r\n>>> np.asarray(one.x)\r\narray([3, 1, 5, 1, 2])\r\n>>> dictionary, index = np.unique(np.asarray(one.x), return_index=True)\r\n>>> dictionary\r\narray([1, 2, 3, 5])\r\n>>> index\r\narray([1, 4, 0, 2])\r\n```\r\n\r\nThe `dictionary` is the sorted, no-duplicates copy of `one.x` and `index` is something we can use to reorder and de-duplicate actual records from `one`:\r\n\r\n```python\r\n>>> one[index]\r\n<Array [{x: 1}, {x: 2}, {x: 3}, {x: 5}] type='4 * {\"x\": int64}'>\r\n```\r\n\r\nbut we won't need that until the final step.\r\n\r\n**Step 2:** find the values in `two`'s index that are as close as possible to those in the dictionary.\r\n\r\n```python\r\n>>> closest = np.searchsorted(dictionary, np.asarray(two.y), side=\"left\")\r\n>>> closest\r\narray([3, 2, 2, 0, 4])\r\n```\r\n\r\nThis is a vectorized \"search sorted,\" which finds the closest match of all values in `two.y` to any value in `dictionary` in a single Python function call. The implicit loop over all elements in `two.y` is done in compiled code, so it doesn't matter how large your arrays are.\r\n\r\nThe return values of this function are the index positions in `dictionary` that are closest to each from `two.y`. It therefore has the same length as `two.y` (not the same length as `dictionary`).\r\n\r\n```python\r\n>>> len(closest), len(two.y), len(dictionary)\r\n(5, 5, 4)\r\n```\r\n\r\nUnfortunately, due to the way that `np.searchsorted` works, it maps values in `two.y` that are larger than any in `dictionary` to `len(dictionary)`. (Setting `side=\"right\"` doesn't fix this and only makes the next step more complicated.) Therefore, if we use this index on the dictionary:\r\n\r\n```python\r\n>>> dictionary[closest]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nIndexError: index 4 is out of bounds for axis 0 with size 4\r\n```\r\n\r\nthat one value gives us troubles. In the next step, we'll be applying a filter requiring each closest match to be an exact match (we want to _exactly_ match `one.x` to `two.y`), so values like this are a failure that we'll be wanting to filter out anyway. The thing that's unfortunate is that they're a special case failure.\r\n\r\n**Step 3:** filter out close matches that aren't exact matches.\r\n\r\nBecause of the issue described above, we'll have to filter out non-exact matches in two steps:\r\n\r\n   * values in `closest` that are beyond the length of `dictionary` (i.e. `4` in the example above, which is greater than all good matches)\r\n   * values in `closest` that don't exactly map to the original `two.y` (because they're in between ones that do, or less than all of them).\r\n\r\nYou can [filter an array with an array of booleans](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#filtering), but doing two filters is problematic because the first changes the length, such that the second array of booleans doesn't fit:\r\n\r\n```python\r\n>>> example = np.array([\"a\", \"b\", \"c\", \"d\", \"e\"])\r\n>>> drop_vowels1 = np.array([False, True, True, True, True])\r\n>>> drop_vowels2 = np.array([True, True, True, True, False])\r\n>>> example[drop_vowels1]\r\narray(['b', 'c', 'd', 'e'], dtype='<U1')\r\n>>> example[drop_vowels1][drop_vowels2]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nIndexError: boolean index did not match indexed array along dimension 0;\r\n            dimension is 4 but corresponding boolean dimension is 5\r\n\r\n>>> # What you'd have to do instead (yuck):\r\n>>> example[drop_vowels1][drop_vowels2[drop_vowels1]]\r\narray(['b', 'c', 'd'], dtype='<U1')\r\n```\r\n\r\nSince Awkward Array has missing values (more generally than NumPy's masked arrays), we've introduced [another kind of filtering operation, `array.mask[filter]`](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#ak-array-mask), that does not change the length of the array; it masks out values by replacing them with a placeholder `None`. Subsequent operations with indexes containing `None` pass the `None` values through as a placeholder. Thus, we can do the two steps of our filter without having to filter our filters, as we would in the NumPy example above.\r\n\r\n```python\r\n>>> is_within_range = ak.Array(closest).mask[closest < len(dictionary)]\r\n>>> is_within_range\r\n<Array [3, 2, 2, 0, None] type='5 * ?int64'>\r\n>>> is_good_match = ak.Array(dictionary)[is_within_range] == two.y\r\n>>> is_good_match\r\n<Array [False, True, True, True, None] type='5 * ?bool'>\r\n```\r\n\r\nNote that these need to be Awkward Arrays, not NumPy arrays, for this \"masked filter\" feature to work.\r\n\r\nNow we can apply this filter to the closest matches to get an indexer \"`reordering`\" that can be applied to the `dictionary` to get something that lines up with `two`. Since we're going with masked filters anyway, let's continue with that. (Otherwise, if you want the `None` values to become `False`, as a normal filtering array, use [ak.fill_none](https://awkward-array.readthedocs.io/en/latest/_auto/ak.fill_none.html) to convert them.)\r\n\r\n```python\r\n>>> reordering = ak.Array(closest).mask[is_good_match]\r\n>>> reordering\r\n<Array [None, 2, 2, 0, None] type='5 * ?int64'>\r\n```\r\n\r\nTo see how we can use this `reordering`, apply it to `dictionary`:\r\n\r\n```python\r\n>>> ak.Array(dictionary)[reordering]\r\n<Array [None, 3, 3, 1, None] type='5 * ?int64'>\r\n>>> two.y\r\n<Array [4, 3, 3, 1, 6] type='5 * int64'>\r\n```\r\n\r\nEverywhere that a value in the `dictionary` can be matched to a value in `two.y`, it is matched (duplicated if necessary), and everywhere else, it's `None`.\r\n\r\n**Step 4:** apply this `reordering` to `one`.\r\n\r\nWe'd like to apply `reordering` to `one`, but it indexes `dictionary` to match `two.y`. Remember how we asked `np.unique` for `return_index=True`? Now we'll be using that `index` to map `one` to `dictionary` and then use `reordering` to map `dictionary`'s order to `two`'s.\r\n\r\n```python\r\n>>> dictionary\r\narray([1, 2, 3, 5])\r\n>>> one[index]\r\n<Array [{x: 1}, {x: 2}, {x: 3}, {x: 5}] type='4 * {\"x\": int64}'>\r\n>>> one[index][reordering]\r\n<Array [None, {x: 3}, {x: 3}, {x: 1}, None] type='5 * ?{\"x\": int64}'>\r\n>>> two.y\r\n<Array [4, 3, 3, 1, 6] type='5 * int64'>\r\n```\r\n\r\n(This is [function composition](https://github.com/scikit-hep/awkward-1.0/blob/0.2.5/docs/theory/arrays-are-functions.pdf), in which the `index` and `reordering` are both `int \u2192 int` functions.)\r\n\r\nSo now that we can put full records from `one` in the same order as full records from `two`, we can use them in the same operations, including [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html), which creates records of records.\r\n\r\n```python\r\n>>> joined = ak.zip({\"one\": one[index][reordering], \"two\": two})\r\n>>> joined.type\r\n5 * {\"one\": ?{\"x\": int64}, \"two\": {\"y\": int64}}\r\n>>> joined.tolist()\r\n[{'one': None, 'two': {'y': 4}},\r\n {'one': {'x': 3}, 'two': {'y': 3}},\r\n {'one': {'x': 3}, 'two': {'y': 3}},\r\n {'one': {'x': 1}, 'two': {'y': 1}},\r\n {'one': None, 'two': {'y': 6}}]\r\n```\r\n\r\n(If you want to merge them into one level of record structure, which has potential issues if any field names are the same, then you'll have to build that manually with `ak.zip({\"x\": joined.one.x, \"y\": joined.two.y})` and so on for all fields you want to merge. There's no high-level function for that yet because we'd have to figure out an interface for deciding which side to get each field from and how to name them. SQL and Pandas have troubles with that, too.)\r\n\r\nAs promised, values in `one` with no corresponding value in `two` are dropped, values in `one` with duplicate indexes are dropped, values in `two` with no corresponding value in `one` are given a placeholder `None`, and values in `two` with duplicate indexes copy the whole `one` record. (Note: I'm using the word \"copy\" in a high-level sense: the actual array is built with a lazy indexer that doesn't literally copy values from `one` until you access them. Look at `joined.layout` to see this internal structure.) If `one` and `two` had any more fields than the ones we're using for indexing, those fields would go along for the ride: they'd be reordered and/or duplicated as needed.\r\n\r\nYour case may be simpler: you might not have any duplicates or mismatches, but the above procedure would still work. Your case might be more complex: you might need a full outer join, rather than a left/right outer join. I think you could get that with a few extra steps after this one, but not knowing whether you need that, I'm going to just stop here.\r\n\r\nGood luck!",
     "createdAt":"2021-01-03T19:26:36Z",
     "number":258751,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"nongiga"
        },
        "body":"WHOA! Thank you for the very thorough answer. It is late here but I'll try it out tomorrow and report.\r\n\r\nMy main challenge is that I am creating a complex, nested data structure using information from multiple sources. My broad plan was to create it in AwkwardArray (thus far the two pieces I put together were loaded in pandas, nested into JSON, and then converted into AwkwardArrays). Then I planned to save the awkwardarray in parquet, and then load only the necessary structures for each analysis and perform it using AwkwardArray (please let me know if it is a sane plan).\r\n\r\nMy guess is that even though it is not an operation that's inherent to AK it'd still be quicker than converting to pandas and back or struggling to maintain a nested structure that makes sense in pandas... right?\r\n\r\n(P.S. I might add more comments to this tomorrow as I go through it more thoroughly)\r\n",
        "createdAt":"2021-01-03T19:55:21Z",
        "number":258810
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"(I'm encouraged by the fact that these Discussions remain open after they're answered, so this can be used by many people in the future. I've been slow to write documentation, but this is kind of \"documentation on demand.\")\r\n\r\nI can't comment on the relative speed of doing this in Awkward Array as described above versus converting over to Pandas, doing the merge there, and converting back. Pandas might have precompiled operations that do it in fewer steps (i.e. fewer intermediate arrays). However, a conversion to Pandas would squash or not be permitted by nested data structures, if you have them, whereas the procedure I describe above works for any data structure, as long as the top level is a record structure.\r\n\r\nAll the steps in this procedure except the last are only working with one-dimensional arrays of integers, the indexers that would reorder/drop/duplicate records. The last step rearranges `one` to match `two`, which is independent of whatever structures might be lurking in `one` or `two`, other than their indexes (`x` and `y`). I don't think [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html) would give you any trouble, trying to zipper together fields within `one` or `two`, but if it does, use `depth_limit=1` to just combine them without \"zippering.\"\r\n\r\nThere might be a speed/functionality optimum if you\r\n\r\n1. project out just the indexes, `one.x` and `two.y`,\r\n2. wrap them each as Pandas series with the default sequential index that starts with `0`,\r\n3. merge them using Pandas,\r\n4. convert those (now rearranged) indexes into NumPy arrays,\r\n5. use those NumPy arrays as indexers on `one` and `two`.\r\n\r\nThat way, you're using Pandas for what it does best (join operations), but only on the indexing fields, then using its result to reorder/drop/duplicate arbitrary data in the Awkward Arrays. That would also make it easier to do inner joins and full outer joins, just by changing Pandas's `how` argument. If the index has any missing values, you'll need to preserve those as `None` in step 4, which might be tricky. In case it helps, note that [ak.from_numpy](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_numpy.html) will convert NumPy's masked arrays into Awkward Arrays with `None` values, in case you find it easier to preserve the missingness with masked arrays than some other format.",
        "createdAt":"2021-01-03T20:18:44Z",
        "number":258854
       },
       {
        "author":{
         "login":"nongiga"
        },
        "body":"Alright! I followed your instructions and only made minor changes for the last command that's particular to my case:\r\n\r\nIf I am trying to add a variable in two to one:\r\n```\r\ndictionary, index = np.unique(np.asarray(two.RandomID), return_index=True)\r\nclosest = np.searchsorted(dictionary, np.asarray(one.RandomID), side=\"left\")\r\nis_within_range = ak.Array(closest).mask[closest < len(dictionary)]\r\nis_good_match = ak.Array(dictionary)[is_within_range] == one.RandomID\r\nreordering = ak.Array(closest).mask[is_good_match]\r\none[\"Seqplates\"] = two.Seqplates[index][reordering]\r\nila.tolist()\r\n```\r\n\r\nHere I am only adding variable Seqplates to my existing list (outer-right I believe)\r\n\r\nIn my case I will be adding smaller datasets to a large dataset so this is valuable.",
        "createdAt":"2021-01-04T08:41:13Z",
        "number":259515
       },
       {
        "author":{
         "login":"nongiga"
        },
        "body":"I have a follow-up question (maybe it's more suitable for a different post):\r\n\r\nFollowing merging the information, I want to compare lists within my AwkwardArray:\r\n\r\nSo let's say I have these lists:\r\nOne:\r\n```\r\n{\r\n    \"CaseNum\": 1905256,\r\n    \"Isolates\": [\r\n      {\r\n        \"IsoNum\": 5567545\r\n        \"SeqsPlate\": \"1_E5\",\r\n      }\r\n    ],\r\n  }, \r\n```\r\nTwo:\r\n```\r\n{\r\n    \"CaseNum\": 1905256,\r\n    \"Seqplates\": [\r\n      \"1_E5\",\r\n      \"1_D5\"\r\n    ]\r\n  },\r\n  {\r\n    \"CaseNum\": 1907609,\r\n    \"Seqplates\": [\r\n      \"22_H10\",\r\n      \"23_H4\"\r\n    ]\r\n  },\r\n```\r\n\r\nI already merged them into a single array, joined:\r\n```\r\n{\r\n    \"CaseNum\": 1905256,\r\n    \"Isolates\": [\r\n      {\r\n        \"IsoNum\": 5567545\r\n        \"SeqsPlate\": \"1_E5\",\r\n      }\r\n     Seqplates: [\r\n      \"1_E5\",\r\n      \"1_D5\"\r\n    ]\r\n    ],\r\n  }, \r\n```\r\nNow I want to determine whether one.Isolates.Seqsplate is in one.Seqplate for each instance of Isolates. \r\n\r\nIn pseudocode:\r\n```\r\nfor record in joined:\r\n    for isolate in record.Isolates:\r\n        if Isolate.Seqplate in record.Seqplates:\r\n            Isolate.IsPresent=True\r\n        else:\r\n            Isolate.IsPresent=False\r\n````\r\nI want to save this information in a new field under Isolates, so that I get:\r\n\r\n ```\r\n{\r\n    \"CaseNum\": 1905256,\r\n    \"Isolates\": [\r\n      {\r\n        \"IsoNum\": 5567545,\r\n        \"SeqsPlate\": \"1_E5\",\r\n        \"IsPresent\": True\r\n      }\r\n     Seqplates: [\r\n      \"1_E5\",\r\n      \"1_D5\"\r\n    ]\r\n    ],\r\n  }, \r\n```\r\n\r\nIs there a more efficient way to do it in AK than to loop over the elements?",
        "createdAt":"2021-01-04T10:00:39Z",
        "number":259636
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"You can avoid a nested for loop in Python by competing the Cartesian product of the two arrays of lists with [ak.cartesian](https://awkward-array.readthedocs.io/en/latest/_auto/ak.cartesian.html).\r\n\r\nSpecifically, use `nested=True` so that the output is doubly nested but maintains the first-level lists' lengths, like\r\n\r\n```\r\n[[[(1, \"a\"), (1, \"b\")], [(2, \"a\"), (2, \"b\")], [(3, \"a\"), (3, \"b\")]], ...]\r\n````\r\n\r\nand then use `==`  to check for equality between the `slot0` and `slot1` of the tuple, and finally the [ak.any](https://awkward-array.readthedocs.io/en/latest/_auto/ak.any.html) reducer with `axis=-1` to replace the innermost lists with `True` if the have a match and `False` if they do not.\r\n\r\nOr you might want to pass the array into Numba and use [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html) to construct the boolean lists.",
        "createdAt":"2021-01-04T13:08:05Z",
        "number":259940
       },
       {
        "author":{
         "login":"nongiga"
        },
        "body":"I duly appreciate your response, since I'm beginning to realize my questions are more pythonian than particularly AwkwardArrayian  I'll try to post my solution here for future reference, if it's of any help.\r\n\r\nEdit: that was quicker than I thought, thank you for the instructions!\r\n```\r\ncart=ak.cartesian([joined.Isolates.Seqplates, joined.Seqplates], nested=True)\r\nis_present=ak.any(cart.slot0==cart.slot1, axis=-1)\r\nis_present=ak.fill_none(is_present, False)\r\njoined['Isolates']=ak.with_field(joined.Isolates, is_present, where=\"IsPresent\")\r\ndisplay(joined.tolist())\r\n```",
        "createdAt":"2021-01-04T13:53:19Z",
        "number":260030
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"> my questions are more pythonian than particularly AwkwardArrayian\r\n\r\nI wouldn't say so: you're asking which functions from the Awkward Array library would allow you to avoid Python for loops. A Python expert without knowledge of Awkward Array would have the same questions, so this is the right place to ask about that.\r\n\r\nIn some cases, like the above, I can't give explicit examples because I'm answering on my phone. I'm glad you could fill in the gaps and get a working solution!",
        "createdAt":"2021-01-04T14:21:00Z",
        "number":260102
       }
      ],
      "totalCount":7
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"> I am new to python and am really excited by AwkwardArray: it is exactly what I switched programming languages for!\r\n\r\nOut of curiosity, which programming language did you switch from? It looks like you're doing something with health care data?",
     "createdAt":"2021-01-03T19:28:07Z",
     "number":258755,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"nongiga"
        },
        "body":"Interestingly I was working with MATLAB. The matrix-based programming was very useful, but I got frustrated with the availability of bioinformatic tools. I didn't show it but in my project, I am merging both health records and sequencing data, and most of the sequencing analysis tools and written in python. Given that both this and 'big data' are usually in python I thought it'd be wise to switch in the long term.",
        "createdAt":"2021-01-03T19:42:48Z",
        "number":258784
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-01-03T13:48:51Z",
  "number":633,
  "title":"Merging arrays based on common fields (i.e. \"joining\" tables)",
  "url":"https://github.com/scikit-hep/awkward/discussions/633"
 },
 {
  "author":{
   "login":"nsmith-"
  },
  "body":"Consider\r\n```python\r\nimport awkward as ak\r\n\r\na = ak.Array(\r\n    [\r\n        [[1, 2], [3]],\r\n        [[4], [5, 6]],\r\n    ]\r\n)\r\nb = ak.Array(\r\n    [\r\n        [7],\r\n        [8, 9, 10],\r\n    ]\r\n)\r\nc = ak.Array(\r\n    [\r\n        [\r\n            [[(1, 7)], [(2, 7)]],\r\n            [[(3, 7)]],\r\n        ],\r\n        [\r\n            [[(4, 8), (4, 9), (4, 10)]],\r\n            [[(5, 8), (5, 9), (5, 10)], [(6, 8), (6, 9), (6, 10)]],\r\n        ],\r\n    ]\r\n)\r\n```\r\n\r\nI would expect some incantation of `ak.cartesian([a, b], ...)` to produce `c` but so far have not been successful. I think such a desire could be keyed by `nested=True, axis=-1`, i.e. differing depths can in principle be supported when nesting. But perhaps the full generality of `ak.cartesian` can only be specified with a tuple of axes?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I wouldn't want to complicate the interface to `ak.cartesian` any more than it already is. I don't think we're looking at a case of wanting to do a Cartesian product at two different `axis` values. (Which one would be used in the output? The first? The deepest?) I think we're looking at a case of wanting to make `b` enough like `a` that `ak.carteisan` applies without modification. Whatever that transformation is, it's likely to be useful for other things besides Cartesian products, so deeping it factored out of the `ak.cartesian` interface would be a good thing.\r\n\r\nTo start with, we have to figure out what we need to do to `b` to get it to fit with `a`. In this case, we need to slice it like the following:\r\n\r\n```python\r\n>>> b[[[0, 0], [1, 1]]]\r\n<Array [[[7], [7]], ... 8, 9, 10], [8, 9, 10]]] type='2 * 2 * var * int64'>\r\n```\r\n\r\nThat is, the `[7]` in index `0` should be replicated for each of `a`'s two lists in its first element and the `[8, 9, 10]` in index `1` should be replicated for each of `a`'s two lists in its second element.\r\n\r\nAnd then we can use it:\r\n\r\n```python\r\n>>> c = ak.cartesian((a, b[[[0, 0], [1, 1]]]), axis=2, nested=True)\r\n>>> c.type\r\n2 * var * var * var * (int64, int64)\r\n>>> c.tolist()\r\n[\r\n    [\r\n        [[(1, 7)], [(2, 7)]],\r\n        [[(3, 7)]]\r\n    ],\r\n    [\r\n        [[(4, 8), (4, 9), (4, 10)]],\r\n        [[(5, 8), (5, 9), (5, 10)], [(6, 8), (6, 9), (6, 10)]]\r\n    ]\r\n]\r\n```\r\n\r\nThen the question becomes, how do we get `[[0, 0], [1, 1]]` from `a`?\r\n\r\nMaybe you can think of what, in general, that function should be named or described as\u2014either for making `[[0, 0], [1, 1]]` or for making `b[[[0, 0], [1, 1]]]` directly (depending on what concept it's addressing). It's something like broadcasting, but more general than broadcasting (which is already being applied).\r\n\r\nFor getting this indexer in a not-too-ridiculous way, there's\r\n\r\n```python\r\n>>> ak.ones_like(ak.num(a, axis=2)) * np.arange(len(a))\r\n<Array [[0, 0], [1, 1]] type='2 * var * int64'>\r\n```\r\n\r\nThe `ak.num` function is being abused here: we're not interested in the number of elements in each list, but at `axis=2`, it produces the right number of items (and calculating `ak.num` is probably less expensive than, say, `ak.local_index` at `axis=1`). Then the `ak.ones_like` function makes all the values `1` while maintaining structure. Then multiplying by `np.arange(len(a))` gets the right item to duplicate in each slot.",
     "createdAt":"2021-01-04T19:17:28Z",
     "number":260911,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-01-04T18:36:13Z",
  "number":637,
  "title":"Nested cartesian product for deep arrays",
  "url":"https://github.com/scikit-hep/awkward/discussions/637"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"From @nongiga:\r\n\r\nI'll try to do this as easy to reproduce as I can but it's a little complicated.\r\n\r\nI have a data structure, that looks a little like this:\r\n```\r\n{\r\n    \"CaseNum\": 1905256,\r\n    \"Isolates\": [\r\n      {\r\n        \"IsoNum\": 5567545\r\n        \"SeqsPlate\": \"1_E5\",\r\n      }\r\n     Seqplates: [\r\n      \"1_E5\",\r\n      \"1_D5\"\r\n    ]\r\n    ],\r\n  }, \r\n```\r\nI am loading multiple csv files and saving their content in a subfield of the structure, making a final struct that looks more like this:\r\n```\r\n{\r\n    \"CaseNum\": 1905256,\r\n    \"Isolates\": [\r\n      {\r\n        \"IsoNum\": 5567545\r\n        \"SeqsPlate\": \"1_E5\",\r\n      }\r\n     Seqplates: [ \"1_E5\", \"1_D5\" ]\r\n    ],\r\n   \"Pangenome\":[\r\n       \"Gene\": [gene1, gene2, ...]\r\n      \"Annotation\": [annot1, annot2, ...]\r\n    ]\r\n  }, \r\n```\r\nI did it using the following code, a bit bukly but get the job done:\r\n\r\n```\r\ndatadir=os.getcwd()\r\n\r\n# get names of csv files\r\ndirnames=np.char.add(np.asarray(datadir+\"/pangenome/\", dtype=str), np.asarray(df.CaseNum,dtype=str))\r\nfilenames=np.char.add(dirnames,np.asarray('/gene_presence_absence.csv', dtype=str))\r\n\r\n# keep track over which of the files exist\r\nisfile=[os.path.isfile(fn) for fn in filenames]\r\n\r\n# function to read csv files into awkwardarray\r\ndef get_pangenome_data(filename):\r\n    roary_pa=csv.read_csv(filename)\r\n    roary_ak=ak.from_arrow(roary_pa)\r\n   # returns AK and the length of the array for later unflattening\r\n    return roary_ak, len(roary_ak)\r\n\r\n# loop through all existing csv files and get their data\r\npangenome_data=[ (get_pangenome_data(fn))  for fn in filenames[isfile] ]\r\n\r\n# get line count in each file for unflattening\r\nr_count=np.array([c for dat, c in pangenome_data])\r\ncount=np.zeros((len(ilp),),dtype=int) # all line counts are zero\r\ncount[isfile]=r_count # except for records that have a matching csv file\r\n\r\n# merge csv structs together into a single awkwardarray\r\npangenome_data=[dat for dat, c in pangenome_data]\r\npangenome_data=ak.concatenate(pangenome_data)\r\n\r\n# unflatten according to csv line counts to fit the main array/df\r\npangenome_data=ak.unflatten(pangneome_data,count)\r\n\r\n# assign pangenome data to array\r\ndf['Pangenome']=pangenome_data\r\n```\r\n\r\nProblem is, when I try to save the file in to_parquet\r\n`ak.to_parquet(df.Pangenome, datadir+\"/up_to_pangenome.parquet\", explode_records=True)`\r\nI get:\r\n`ArrowNotImplementedError: Unhandled type for Arrow to Parquet schema conversion: dense_union<0: string not null=0, 1: string not null=1>`\r\n\r\nNonetheless when I try to save the Isolates substructure within the awway:\r\n`ak.to_parquet(df.Isolates, datadir+\"/up_to_pangenome.parquet\", explode_records=True)`\r\n\r\nI get no error.\r\n\r\nWhen I print the type for ak.Pangenome I get:\r\n```\r\nak.type(df.Pangenome):\r\n20 * var * union[{\"Gene\": option[string],  \"Annotation\": option[string]}]\r\n```\r\n\r\nBut for the isolates I get:\r\n\r\n```\r\nak.type(df.Isolates)\r\n20 * var * {\"IsoNum\": float64, \"RowNum\": int64, \"SampleDate\": string, \"SeqSubDirName\": string, \"LowQualityThrs\": int64, \"Freezer_pos\": string, \"Used\": int64, \"SiteString\": string, \"Seqname\": string, \"IsSameStrain\": bool}\r\n```\r\n\r\nIs there a way to convert the union type to a different type within the list to circumvent this?\r\n\r\nThanks!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I manually moved this here because the error is a `ArrowNotImplementedError`, not something I can fix. (You can try the [Arrow JIRA](https://issues.apache.org/jira/projects/ARROW/issues/ARROW-10904?filter=allopenissues).) The discussion would be about ways of working around this (current) Arrow \u2192 Parquet limitation.\r\n\r\nI'll leave it for others to talk about strategies of turning unions into non-unions, as they'll be prompted by use-cases. Your use-case, however, shouldn't be happening: it's a union of exactly one type:\r\n\r\n```\r\n20 * var * union[{\"Gene\": option[string],  \"Annotation\": option[string]}]\r\n```\r\n\r\nshouldn't have been created; it should be\r\n\r\n```\r\n20 * var * {\"Gene\": option[string],  \"Annotation\": option[string]}\r\n```\r\n\r\ninstead. If you can find the single step that introduces that `union`, please report it as a bug for me to fix.\r\n\r\nHere's how unions are supposed to work. The following _must_ be a union type because it mixes integers and strings:\r\n\r\n```python\r\n>>> must_be_union = ak.Array([1, 2, 3, \"four\", \"five\"])\r\n>>> must_be_union\r\n<Array [1, 2, 3, 'four', 'five'] type='5 * union[int64, string]'>\r\n```\r\n\r\nThe following is still a union type because it _could_ contain multiple types, but this particular value does not.\r\n\r\n```python\r\n>>> union_type_but_not_values = must_be_union[:3]\r\n>>> union_type_but_not_values\r\n<Array [1, 2, 3] type='3 * union[int64, string]'>\r\n```\r\n\r\nWe may need new functions/tooling to deal with such cases. Similarly, #487 is an open issue to provide tooling for eliminating unwanted option-types.\r\n\r\nThe strange case you encountered is something I have to build manually because high-level functions are not supposed to produce it:\r\n\r\n```python\r\n>>> a = union_type_but_not_values.layout\r\n>>> bad_union = ak.Array(ak.layout.UnionArray8_64(a.tags, a.index, [a.contents[0]]))\r\n>>> bad_union\r\n<Array [1, 2, 3] type='3 * union[int64]'>\r\n```\r\n\r\nIf anything like a `union[int64]` came up in an operation that can return unions, it _should have_ been eliminated by an internal call to `simplify`:\r\n\r\n```python\r\n>>> fixed_it = ak.Array(bad_union.layout.simplify())\r\n>>> fixed_it\r\n<Array [1, 2, 3] type='3 * int64'>\r\n```\r\n\r\nThat's why I'd like to find out which of your steps is producing this union of only one type (a record in your example). That one operation is probably not calling `simplify` as it should.\r\n\r\nWith your data, though, it should be possible to turn\r\n\r\n```python\r\n>>> df_Pangenome = df.Pangenome\r\n>>> ak.type(df_Pangenome):\r\n20 * var * union[{\"Gene\": option[string],  \"Annotation\": option[string]}]\r\n```\r\n\r\ninto something usable by\r\n\r\n```python\r\n>>> fixed = ak.Array(\r\n...     ak.layout.ListArray64(\r\n...         df_Pangenome.layout.starts,\r\n...         df_Pangenome.layout.stops,\r\n...         ak.layout.IndexedArray64(\r\n...             df_Pangenome.layout.content.index,\r\n...             df_Pangenome.layout.content.contents[0],\r\n...         )\r\n...     )\r\n... )\r\n... \r\n>>> ak.type(fixed)\r\n20 * var * {\"Gene\": option[string],  \"Annotation\": option[string]}\r\n```\r\n\r\nbut I haven't tested the above (I _simulated_ the Python outputs) because your example is not reproducible (no access to `df`, for example).\r\n\r\n[More on UnionArrays here](https://awkward-array.readthedocs.io/en/latest/ak.layout.UnionArray.html).",
     "createdAt":"2021-01-05T18:29:40Z",
     "number":263080,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"nongiga"
        },
        "body":"You know what, I am really sorry, I reproduced the example wrong.\r\n\r\nLet's add another field called GeneNum.\r\n\r\nSo the union appears only after concatenation, and it looks like this:\r\n```\r\n20* union[{\"Gene\": option[string], \"Annotation\": option[string], \"GeneNum': option[string]}, {\"Gene\": option[string], \"Annotation\": option[string], \"GeneNum\":?int64}]\r\n```\r\nSo it seems that the problem is that pyarrow reads some of the csv's into one type and others into a different type, and that's where the problem originates from. I will try to produce a solution here since it might become a common problem. My guess is that the solution is either loading one csv as an example and setting a schema or merging all the csvs together and only then turning into an ak.Array and unflattering.\r\n\r\nEdit:\r\n\r\nAlright so problem solved but I encountered another problem with concatenate upon saving. Again I am not sure how to reproduce it but I think it might be a simple fix. Let me know if you want more information and I'll give it to you.\r\n\r\n\r\nSo I have this original code that is the same as above (with the addition that I set the conversion options in advance, in the following manner):\r\n\r\n```\r\nopt=csv.ConvertOptions(column_types=[('Accessory Fragment', pa.string()),( 'Accessory Order with Fragment', pa.string() )],\r\n        strings_can_be_null=True)\r\n roary_pa=csv.read_csv(filename, opt)\r\n```\r\nBut then when I run it, I get the following error:\r\n\r\n```\r\n----------------------------------------\r\nArrowInvalidTraceback (most recent call last)\r\n<ipython-input-262-eaff77291036> in <module>\r\n     46 df=ak.with_field(df, Pangenome, where='Pangenome')\r\n     47 \r\n---> 48 ak.to_parquet(df, datadir+\"/datastruct/up_to_pangenome.parquet\")\r\n\r\n~/opt/miniconda3/envs/server-env/lib/python3.7/site-packages/awkward/operations/convert.py in to_parquet(array, where, explode_records, list_to32, string_to32, bytestring_to32, **options)\r\n   2662     layout = to_layout(array, allow_record=False, allow_other=False)\r\n   2663     iterator = batch_iterator(layout)\r\n-> 2664     first = next(iterator)\r\n   2665 \r\n   2666     if \"schema\" not in options:\r\n\r\n~/opt/miniconda3/envs/server-env/lib/python3.7/site-packages/awkward/operations/convert.py in batch_iterator(layout)\r\n   2646                         list_to32=list_to32,\r\n   2647                         string_to32=string_to32,\r\n-> 2648                         bytestring_to32=bytestring_to32,\r\n   2649                     )\r\n   2650                 )\r\n\r\n~/opt/miniconda3/envs/server-env/lib/python3.7/site-packages/awkward/operations/convert.py in to_arrow(array, list_to32, string_to32, bytestring_to32)\r\n   2221             )\r\n   2222 \r\n-> 2223     return recurse(layout, None, False)\r\n   2224 \r\n   2225 \r\n\r\n~/opt/miniconda3/envs/server-env/lib/python3.7/site-packages/awkward/operations/convert.py in recurse(layout, mask, is_option)\r\n   1937                     len(offsets) - 1,\r\n   1938                     [None, pyarrow.py_buffer(offsets)],\r\n-> 1939                     children=[content_buffer],\r\n   1940                 )\r\n   1941             else:\r\n\r\n~/opt/miniconda3/envs/server-env/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.Array.from_buffers()\r\n\r\n~/opt/miniconda3/envs/server-env/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.Array.validate()\r\n\r\n~/opt/miniconda3/envs/server-env/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Length spanned by list offsets (5305) larger than values array (length 5304)\r\n```\r\n\r\nSince it is an issue with the offsets/values I figured it must be either an issue with concatenate or unflatten. So I mended the code so that all the lines are concatenated by pandas and only then converted to AwkwardArray and unflattened. In this manner I skipped the concatenation step:\r\n\r\n```\r\ndatadir=os.getcwd()\r\n\r\n# get names of csv files\r\ndirnames=np.char.add(np.asarray(datadir+\"/pangenome/\", dtype=str), np.asarray(df.CaseNum,dtype=str))\r\nfilenames=np.char.add(dirnames,np.asarray('/gene_presence_absence.csv', dtype=str))\r\n\r\n# keep track over which of the files exist\r\nisfile=[os.path.isfile(fn) for fn in filenames]\r\n\r\n# loop through all existing csv files and get their data into a list of panda structs\r\nroary_pd= [pd.read_csv(fn) for fn in filenames[isfile]]\r\n\r\n# get line count in each file for unflattening\r\ncount_r= [len(rpd.index) for rpd in roary_pd]\r\ncount=np.zeros((len(ilp),),dtype=int) # all line counts are zero\r\ncount[isfile]=r_count # except for records that have a matching csv file\r\n\r\n# merge csvs together in pandas\r\nmerged=pd.concat(roary_pd, ignore_index=True)\r\n\r\n#convert from pandas to arrow\r\npar=pa.Table.from_pandas(merged_rpd)\r\n\r\n# convert from arrow to AwkwardArray\r\npangenome_data=ak.from_arrow(par)\r\n\r\n# unflatten according to csv line counts to fit the main array/df\r\npangenome_data=ak.unflatten(pangneome_data,count)\r\n\r\n# assign pangenome data to array\r\ndf=ak.with_fields(df, pangenome_data, where='Pangenome)\r\nak.to_parquet(df, datadir+\"/up_to_pangenome.parquet\")\r\n```\r\n\r\nFor the process without ak.concatenate I get the type:\r\n```\r\n4 * {\"NewRandomID\": int64, \"RandomID\": int64, \"CaseNum\": int64, \"Seqplates\": option[var * string], \"Isolates\": var * {\"IsoNum\": float64, \"RowNum\": int64, \"SampleDate\": string, \"SeqSubDirName\": string, \"LowQualityThrs\": int64, \"Freezer_pos\": string, \"Used\": int64, \"SiteString\": string, \"Seqname\": string, \"IsSameStrain\": bool}, \"Pangenome\": var * {\"Gene\": option[string],  \"Annotation\": option[string]}}\r\n```\r\n\r\nFor the process with concatenate I get the type:\r\n```\r\n4 * {\"NewRandomID\": int64, \"RandomID\": int64, \"CaseNum\": int64, \"Seqplates\": option[var * string], \"Isolates\": var * {\"IsoNum\": float64, \"RowNum\": int64, \"SampleDate\": string, \"SeqSubDirName\": string, \"LowQualityThrs\": int64, \"Freezer_pos\": string, \"Used\": int64, \"SiteString\": string, \"Seqname\": string, \"IsSameStrain\": bool}, \"Pangenome\": var * {\"Gene\": option[string],  \"Annotation\": option[string]}}\r\n```\r\n\r\nSo both look identical.\r\n\r\nAdditionally, converting using to_arrow gives no errors, only the next step does.",
        "createdAt":"2021-01-06T07:31:27Z",
        "number":264206
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Knowing that the bad union is being produced by `concatenate` gives me somewhere to look. I'll try to reproduce it on smaller examples.\r\n\r\nAs for the second error, Arrow is complaining that the content of a list has unreachable elements, which is allowed in Awkward Array. That's a separate issue (and I know that Arrow doesn't complain about it _right away_; maybe I should try saving every type to Parquet in the tests). I'll look into that, too.",
        "createdAt":"2021-01-06T13:41:58Z",
        "number":264764
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-01-05T17:59:48Z",
  "number":641,
  "title":"to_parquet cannot convert union struct",
  "url":"https://github.com/scikit-hep/awkward/discussions/641"
 },
 {
  "author":{
   "login":"picassopotato"
  },
  "body":"Hi, sorry if this is a very basic question but.. I'm trying to get things to work and I'm following the https://hsf-training.github.io/hsf-training-uproot-webpage/ guide. The only context is that I have a file with a TTree containing several TBranches. I want to make a dict holding the branches and their arrays, but I get this:\r\n\r\n```>>> import uproot\r\n>>> f = uproot.open(\"uproot-tutorial-file.root\")\r\n>>> branches = f[\"Events\"].arrays()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 1120, in arrays\r\n    arrays,\r\n  File \"/usr/local/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3430, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/usr/local/lib/python3.7/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/usr/local/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3407, in basket_to_array\r\n    branch,\r\n  File \"/usr/local/lib/python3.7/site-packages/uproot/interpretation/numerical.py\", line 115, in final_array\r\n    output = library.finalize(output, branch, self, entry_start, entry_stop)\r\n  File \"/usr/local/lib/python3.7/site-packages/uproot/interpretation/library.py\", line 483, in finalize\r\n    if isinstance(array, awkward.layout.Content):\r\nAttributeError: module 'awkward' has no attribute 'layout'\r\n```\r\n\r\nTo get here I just did pip install uproot. I have uproot 4.0.1 and awkward 0.14.0. Would you please lend a hand?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"The problem is mixing Uproot 4 with Awkward 0. Update pip packages with\r\n\r\n```bash\r\npip uninstall awkward awkward0 awkward1 uproot uproot3 uproot4\r\n```\r\n\r\nenough times that you get rid of all existing versions, then\r\n\r\n```bash\r\npip install awkward uproot\r\n```\r\n\r\nand if you _need_ old interfaces, you can also install\r\n\r\n```bash\r\npip install awkward0 uproot3    # the old ones\r\n```\r\n\r\nYou can use old and new in the same Python process, but be sure your versions are all to the right of the name-change transition line:\r\n\r\n![](https://user-images.githubusercontent.com/1852447/105192214-7c166400-5afd-11eb-94be-0ae9df79e611.png)",
     "createdAt":"2021-01-20T15:51:55Z",
     "number":296497,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"picassopotato"
     },
     "body":"Oh sorry, no idea why I had awkward0. Or, awkward but in version 0? But thank you that worked, and thanks for the swift repsonse!",
     "createdAt":"2021-01-20T15:58:18Z",
     "number":296521,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"For more context, this is part of a major Awkward+Uproot upgrade that completed in December 2020. The error you saw was because names were switched, and the names were switched because some analyses might be depending on the old interfaces. We want to allow gradual adoption by the ability to import both old and new in the same Python process, which wouldn't be possible if the version number was switched.\r\n\r\nUnfortunately for pip, you have to upgrade packages manually. If something brings in an old version of some package as a dependency, it won't get upgraded unless some package explicitly requires a new version or you manually upgrade it. Technically (i.e. from pip's perspective), Uproot 4 does not require Awkward Array, so that it is more portable, so that's why you didn't get it automatically.\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/105201235-41fd9000-5b06-11eb-83c8-7e2ee19f85c4.png)\r\n",
     "createdAt":"2021-01-20T16:03:20Z",
     "number":296533,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"picassopotato"
     },
     "body":"\"If something brings in an old version of some package as a dependency\" yes this turned out to be the issue, installing uproot-methods caused awkward to downgrade. I've seen that vector (https://vector.readthedocs.io/en/latest/index.html) is the recommended way to handle Lorentz vectors, but I'm a little bit lost. I have a TTree with TLorentzVectors and I just need to get things like .Pt() and .Phi(). Do I need to recreate those functions from fX fY fZ and fE? Do you have a good guide for this sort of thing? Thanks!",
     "createdAt":"2021-01-24T11:48:28Z",
     "number":306085,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"picassopotato"
        },
        "body":"@jpivarski sorry to be a bother but may I kindly remind you of this?",
        "createdAt":"2021-01-27T10:27:46Z",
        "number":314002
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Actually, it's been starred in my inbox for days. It prompted me to talk with the author of Vector about trading some of our responsibilities. I'll be working on the Vector package personally to get it into shape for analysis. My work on that will start in March and will probably take about a month.\r\n\r\nFor the short-term, writing functions for simple things like `pt` and `phi` is an option (see https://github.com/scikit-hep/uproot3-methods/blob/master/uproot3_methods/classes/TLorentzVector.py).\r\n\r\nAnother option is to use Coffea's vector classes: https://coffeateam.github.io/coffea/modules/coffea.nanoevents.methods.vector.html?highlight=vector",
        "createdAt":"2021-01-27T13:12:25Z",
        "number":314405
       },
       {
        "author":{
         "login":"picassopotato"
        },
        "body":"Awesome, thanks!",
        "createdAt":"2021-01-27T13:36:03Z",
        "number":314560
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":4
  },
  "createdAt":"2021-01-20T15:47:28Z",
  "number":660,
  "title":"AttributeError: module 'awkward' has no attribute 'layout'",
  "url":"https://github.com/scikit-hep/awkward/discussions/660"
 },
 {
  "author":{
   "login":"mova"
  },
  "body":"Hi there!\r\nI'm trying to optimize the speed of building an ak.Array and I'm trying to use numba with the ak.array builder.\r\nIn this case, i have to pass the signature. What is the type of the ArrayBuilder?\r\nI tried both `ak.ArrayBuilder.numba_type` and `ak.ArrayBuilder.numba_type` without success.\r\n```\r\n@numba.njit(\r\n    (numba.types.int64, numba.types.float32[:, :, :], ak.ArrayBuilder.numba_type)\r\n)\r\ndef map_calo_to_hits(\r\n    eventNumber: int, caloimg: np.ndarray, builder: ak.ArrayBuilder\r\n):\r\n    ...\r\n```\r\nThis results in \r\n```\r\n    134     (numba.types.int64, numba.types.float32[:, :, :], ak.ArrayBuilder.numba_type)\r\n    135 )\r\n--> 136 def map_calo_to_hits(\r\n    137     eventNumber: int, caloimg: np.ndarray, builder: ak.ArrayBuilder\r\n    138 ):\r\n\r\n~/fgsim/.tox/py38/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func)\r\n    219             with typeinfer.register_dispatcher(disp):\r\n    220                 for sig in sigs:\r\n--> 221                     disp.compile(sig)\r\n    222                 disp.disable_compile()\r\n    223         return disp\r\n\r\n~/fgsim/.tox/py38/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs)\r\n     30         def _acquire_compile_lock(*args, **kwargs):\r\n     31             with self:\r\n---> 32                 return func(*args, **kwargs)\r\n     33         return _acquire_compile_lock\r\n     34 \r\n\r\n~/fgsim/.tox/py38/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig)\r\n    837         # Use counter to track recursion compilation depth\r\n    838         with self._compiling_counter:\r\n--> 839             args, return_type = sigutils.normalize_signature(sig)\r\n    840             # Don't recompile if signature already exists\r\n    841             existing = self.overloads.get(tuple(args))\r\n\r\n~/fgsim/.tox/py38/lib/python3.8/site-packages/numba/core/sigutils.py in normalize_signature(sig)\r\n     44         check_type(return_type)\r\n     45     for ty in args:\r\n---> 46         check_type(ty)\r\n     47 \r\n     48     return args, return_type\r\n\r\n~/fgsim/.tox/py38/lib/python3.8/site-packages/numba/core/sigutils.py in check_type(ty)\r\n     38     def check_type(ty):\r\n     39         if not isinstance(ty, types.Type):\r\n---> 40             raise TypeError(\"invalid type in signature: expected a type \"\r\n     41                             \"instance, got %r\" % (ty,))\r\n     42 \r\n\r\nTypeError: invalid type in signature: expected a type instance, got <property object at 0x2b993f4142c0>\r\n```",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Here's a way to find out: compile a function and look at its overloaded types.\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import numba as nb\r\n>>> @nb.njit\r\n... def do_nothing(builder, array):\r\n...   pass\r\n... \r\n>>> do_nothing.overloads.keys()\r\nodict_keys([])\r\n```\r\n\r\n(Nothing yet because we haven't compiled anything yet.)\r\n\r\n```python\r\n>>> do_nothing(ak.ArrayBuilder(), ak.Array([[1, 2, 3], [], [4, 5]]))\r\n>>> do_nothing.overloads.keys()\r\nodict_keys([\r\n    (ak.ArrayBuilderType(None),    # <---- the ArrayBuilder is opaque\r\n     ak.ArrayView(                 # <---- the Array is specialized\r\n         ak.ListArrayType(\r\n             array(int64, 1d, C),\r\n             ak.NumpyArrayType(array(int64, 1d, A), none, {}),\r\n             none, {}),\r\n         None, ())\r\n    )\r\n])\r\n```\r\n\r\nCalling it once compiles the function for the given argument types, and what you see above are the Numba types for the ArrayBuilder and the Array. The ArrayBuilder type is simple: it's opaque to Numba so that it can be filled with any type of data, determined at runtime. The Array type is specialized for the data that it contains so that it can generate faster code to iterate over inputs.\r\n\r\nYou can also access these types from the objects:\r\n\r\n```python\r\n>>> ak.ArrayBuilder().numba_type\r\nak.ArrayBuilderType(None)\r\n\r\n>>> ak.Array([[1, 2, 3], [], [4, 5]]).numba_type\r\nak.ArrayView(\r\n    ak.ListArrayType(\r\n        array(int64, 1d, C),\r\n        ak.NumpyArrayType(array(int64, 1d, A), none, {}),\r\n        none, {}),\r\n    None, ())\r\n```\r\n\r\nThe ArrayBuilder's Numba type is [defined here](https://github.com/scikit-hep/awkward-1.0/blob/91eaafa8635b5a7e1b610c0e9a8709ffab3ee227/src/awkward/_connect/_numba/builder.py#L30-L37), and its only parameter (showing up as `None` in this example) is for custom `behavior`, which is really only needed to pass through to the output, since you can't snapshot an ArrayBuilder inside of a JIT'ed function and therefore be able to use any custom behaviors. (Custom behaviors can define Numba specializations for Arrays, however. We're working on some for [Vector](https://github.com/scikit-hep/vector).) The code to build a type for an Array is spread over the whole `ak._connect._numba` submodule. Also, note that this is a hidden submodule because the way Awkward Arrays get JIT'ed is an internal detail, not part of the public API.\r\n\r\nThat said, knowing the types of ArrayBuilder and Array doesn't help you make anything faster. As shown above, Numba compiles the function the first time it is called, when the full types of its arguments are known. Performing this compilation a millisecond earlier when the function is defined doesn't help anything. Maybe if you're defining the function nested within some other context so it gets redefined (and therefore recompiled) repeatedly, try to put it at global scope so that it gets defined exactly once per Python process. Numba also has `cache=True` to save the compiled function to avoid recompiling it when the Python process restarts, but that doesn't help if you're deploying a function to remote machines (since it caches it on the hard drive of the machine where it runs). Also, I haven't seen compilations that are slow enough to bother.\r\n\r\nIf it's runtime you're thinking about, there are things you can do. The first is to try to build the output without ArrayBuilder, if possible. ArrayBuilder is untyped\u2014like Python, it has to discover the types of objects at runtime\u2014and therefore it's slower compiled code than compiled code can be when its type is known. Right now, @ianna is working on a TypedArrayBuilder, which is more constrained in needing to know its type before being filled, and therefore should provide performance advantages.\r\n\r\nFor the time being, building an Awkward Array out of `ak.layout.*` primitives will be faster than letting ArrayBuilder do it, though it's more work to set it up and ensure that it's working. Here's an example of building a jagged array in Numba with ArrayBuilder:\r\n\r\n```python\r\nIn [1]: import awkward as ak\r\n   ...: import numpy as np\r\n   ...: import numba as nb\r\n\r\nIn [2]: @nb.njit\r\n   ...: def build_with_ArrayBuilder(builder, num_lists, ave_length):\r\n   ...:     for i in range(num_lists):\r\n   ...:         builder.begin_list()\r\n   ...:         count = np.random.poisson(ave_length)\r\n   ...:         for j in range(count):\r\n   ...:             builder.real(np.random.normal(0, 1))\r\n   ...:         builder.end_list()\r\n   ...:     return builder\r\n   ...: \r\n\r\nIn [3]: %%timeit\r\n   ...: \r\n   ...: array = build_with_ArrayBuilder(ak.ArrayBuilder(), 10000000, 10).snapshot()\r\n   ...: \r\n   ...: \r\n8.09 s \u00b1 11.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nand here's an example of building it manually, by constructing the same structure out of `ak.layout.ListOffsetArray` and `ak.layout.NumpyArray`. Since the sizes of the output arrays have to be known and allocated before the can be filled, the program has to be rearranged to do the pass that fills `offsets` before the pass that fills `content` (but you avoid unnecessary guesses at allocation sizes). Note that in this case, Numba is only making flat NumPy arrays, which is what it was designed for.\r\n\r\n```python\r\nIn [4]: @nb.njit\r\n   ...: def build_manually(num_lists, ave_length):\r\n   ...:     offsets = np.empty(num_lists + 1, np.int64)\r\n   ...:     offsets[0] = 0\r\n   ...:     for i in range(num_lists):\r\n   ...:         count = np.random.poisson(ave_length)\r\n   ...:         offsets[i + 1] = offsets[i] + count\r\n   ...:     content = np.empty(offsets[-1], np.float64)\r\n   ...:     for i in range(num_lists):\r\n   ...:         start = offsets[i]\r\n   ...:         stop = offsets[i + 1]\r\n   ...:         for j in range(start, stop):\r\n   ...:             content[j] = np.random.normal(0, 1)\r\n   ...:     return offsets, content\r\n   ...: \r\n\r\nIn [5]: %%timeit\r\n   ...: \r\n   ...: offsets, content = build_manually(10000000, 10)\r\n   ...: array = ak.Array(\r\n   ...:     ak.layout.ListOffsetArray64(\r\n   ...:         ak.layout.Index64(offsets),\r\n   ...:         ak.layout.NumpyArray(content),\r\n   ...:     ),\r\n   ...: )\r\n   ...: \r\n   ...: \r\n3.88 s \u00b1 12.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nIt's about a factor of 2 in this case, though the use of random number algorithms (which adds to both cases) might be significant.\r\n\r\nAs it turns out, the answer is yes: computing the random numbers accounts for nearly all of the time in the second case:\r\n\r\n```python\r\nIn [8]: @nb.njit\r\n   ...: def just_random_numbers(num_lists, ave_length):\r\n   ...:     out1 = 0     # to make sure LLVM doesn't compile them away\r\n   ...:     out2 = 0.0\r\n   ...:     for i in range(num_lists):\r\n   ...:         out1 = np.random.poisson(ave_length)\r\n   ...:         for j in range(out1):\r\n   ...:             out2 = np.random.normal(0, 1)\r\n   ...:     return out1, out2\r\n   ...: \r\n\r\nIn [9]: %%timeit\r\n   ...: \r\n   ...: just_random_numbers(10000000, 10)\r\n   ...: \r\n   ...: \r\n3.42 s \u00b1 13.6 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nso really, building manually would be _much_ faster than the ArrayBuilder if not for the random numbers. If the other parts of your program are significantly faster than generating random numbers, you can gain a lot from manually building the output, rather than using ArrayBuilder. If not, then don't optimize this when something else is your bottleneck.\r\n\r\nBy the way, I'm giving a tutorial on Numba tomorrow: https://github.com/jpivarski-talks/2021-02-03-pyhep-numba-tutorial",
     "createdAt":"2021-02-02T15:56:00Z",
     "number":331302,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"mova"
        },
        "body":"Thank you so much, that helped a lot.\r\nSorry for the very late reply.",
        "createdAt":"2021-05-10T16:11:43Z",
        "number":719328
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-02-02T09:07:21Z",
  "number":696,
  "title":"Speeding up Numba JIT-compiled functions that use ArrayBuilder",
  "url":"https://github.com/scikit-hep/awkward/discussions/696"
 },
 {
  "author":{
   "login":"nikoladze"
  },
  "body":"When i filter complex awkward array, e.g. containing Records, then typically this will result in an indexed arrray\r\n\r\n```pycon\r\n>>> import numpy as np\r\n>>> import awkward as ak\r\n>>> array = ak.zip({\"a\" : np.random.rand(100), \"b\" : np.random.rand(100)})\r\n>>> skim = array[array.a > 0.5]\r\n>>> skim.layout\r\n<IndexedArray64>\r\n    <index><Index64 i=\"[6 7 9 12 20 ... 93 94 95 96 99]\" offset=\"0\" length=\"49\" at=\"0x55a7a345b4a0\"/></index>\r\n    <content><RecordArray>\r\n        <field index=\"0\" key=\"a\">\r\n            <NumpyArray format=\"d\" shape=\"100\" data=\"0.191062 0.280678 0.32714 0.459306 0.169022 ... 0.613331 0.88532 0.240763 0.469416 0.857204\" at=\"0x55a7a391e070\"/>\r\n        </field>\r\n        <field index=\"1\" key=\"b\">\r\n            <NumpyArray format=\"d\" shape=\"100\" data=\"0.191364 0.641833 0.749943 0.076706 0.0817349 ... 0.170089 0.28631 0.594311 0.637529 0.488793\" at=\"0x55a7a38bb0e0\"/>\r\n        </field>\r\n    </RecordArray></content>\r\n</IndexedArray64>\r\n```\r\n\r\nThis is actually a great feature, since it allows me to do fast things with complex arrays without creating huge and expensive copies.\r\n\r\nHowever, sometimes it might be useful to actually create a \"compressed\" copy where the indices or masks are applied all the way down. For example when i want to loop over several files, filter them and concatenate the filtered results.\r\n\r\nIt seems `ak.to_arrow` does something like that, so `ak.from_arrow(ak.to_arrow(...))` might be a way to do it, but maybe there is another way?\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I've been thinking about this, too, since it's an operation that you'd want to have happen right before pickling an array, for instance. It wouldn't be too different in spirit from [ak.materialized](https://awkward-array.readthedocs.io/en/latest/_auto/ak.materialized.html) or [ak.repartition](https://awkward-array.readthedocs.io/en/latest/_auto/ak.repartition.html), which gives you logically the same array, but changed internally for performance reasons.\r\n\r\nI think the right name for such an operation would be \"to pack,\" as in [Pascal packed arrays](https://www.tutorialspoint.com/pascal/pascal_packed_array.htm), [packing booleans into bits in NumPy](https://numpy.org/doc/stable/reference/generated/numpy.packbits.html), or [Parquet bit-packing of small integers](https://aaaaaaron.github.io/2018/08/22/Parquet-encoding-definitions-official/#Run-Length-Encoding-Bit-Packing-Hybrid-RLE-3). (The alliteration of \"packing for pickle\" suggests itself.) It's not \"compression\" because it's not an entirely different encoding, just trimming off the unused parts.\r\n\r\nSuch a feature could be done entirely at the Python level (using the internal `ak._util.recursively_apply` helper function), though it would require a thorough understanding of what is allowed and what is not allowed in [each Content subclass](https://awkward-array.readthedocs.io/en/latest/ak.layout.Content.html).\r\n\r\nAt the moment, [ak.to_arrow](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_arrow.html) mixes the operations of packing and conversion to Arrow. An operation that only packs (`ak.packed`, to use the same tense as [ak.materialized](https://awkward-array.readthedocs.io/en/latest/_auto/ak.materialized.html) and [ak.partitioned](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partitioned.html)?) would be more lightweight than `ak.from_arrow(ak.to_arrow(...))` because missing data has to be packed into validity bits for Arrow\u2014[IndexedOptionArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedOptionArray.html) can be more efficient for large missing objects, like large records, and even 8-bit booleans would need to be converted into 1-bit booleans and back again. However, if you don't have any missing data, `ak.from_arrow(ak.to_arrow(...))` wouldn't necessarily be _expensive_, but obviously not the most logically clean way to do it.\r\n\r\nBottom line: I agree with you that `ak.packed` is a well-motivated operation. I don't have near-term plans to write it, and I would be open to contributions, but if it became a blocker for something else, that would prompt me to write it. Are you interested? We could also consider this Discussion an open invitation to anyone else who wants to try it. (A good metric would be to measure the file size of pickled arrays from typical analyses and show how much smaller they can be!)",
     "createdAt":"2021-02-04T17:40:30Z",
     "number":338726,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"nikoladze"
     },
     "body":"I agree `ak.packed` would probably be the better naming for this (i had [np.ma.MaskedArray.compressed](https://numpy.org/doc/stable/reference/generated/numpy.ma.MaskedArray.compressed.html#numpy.ma.MaskedArray.compressed) in mind). At the Moment i have no strong need for functionality like this. It already works with `ak.to_arrow` and therefore also with `ak.to_parquet`.",
     "createdAt":"2021-02-05T12:00:07Z",
     "number":341248,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-02-04T16:27:22Z",
  "number":701,
  "title":"How to compress/skim indexed or masked arrays",
  "url":"https://github.com/scikit-hep/awkward/discussions/701"
 },
 {
  "author":{
   "login":"aminnj"
  },
  "body":"Hi,\r\n\r\nI'm using Python 3.7.3 and awkward 1.0.2 and I see that `ak.singletons()` only adds a dimension when there is a missing element. E.g.,\r\n```python\r\nak.singletons([2,0,1])\r\n# <Array [2, 0, 1] type='3 * int64'>\r\nak.singletons([2,None,1])\r\n# <Array [[2], [], [1]] type='3 * var * int64'>\r\n```\r\nwhich means I have two cases to deal with when using one array as an index for another. \r\n\r\nThe first case (which doesn't give me my desired output) where the first array has inner arrays of size at least 1 (and thus each value in the second index array is defined):\r\n```python\r\nvals = ak.Array([[43, 15, 10.5], [11.5], [50, 5]])\r\nidx = ak.Array([2, 0, 1])\r\nvals[ak.singletons(idx)]\r\n# <Array [[50, 5], [43, 15, 10.5], [11.5]] type='3 * var * float64\r\n# but I was expecting to get [[10.5],[11.5],[5]]\r\n```\r\nAnd another case (which does give me my desired output) where the first array has at least one empty inner array (and thus at least one missing element in the indexing array):\r\n```python\r\nvals = ak.Array([[43, 15, 10.5], [], [50, 5]])\r\nidx = ak.Array([2, None, 1])\r\nvals[ak.singletons(idx)]\r\n# <Array [[10.5], [], [5]] type='3 * var * float64'>\r\n```\r\nIs this intended? If so, is there a more awkward-like way of doing `vals[idx]`?\r\n\r\nThanks!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"The semantics of this\u2014what it's _supposed_ to do\u2014could be better thought-out. It was intended for cases in which missing values are present because it was for handling the output of [ak.argmin](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argmin.html) and [ak.argmax](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argmax.html). (Here are some early thoughts about the problem: #203.) The problem is basically this:\r\n\r\n```python\r\n>>> # We want to find the max of one_quantity and look at the corresponding quantity in \"another.\"\r\n>>> one_quantity = ak.Array([[1, 2, 3], [], [5, 4]])\r\n>>> another = ak.Array([[1.1, 2.2, 3.3], [], [5.5, 4.4]])\r\n\r\n>>> # As of Awkward 1, argmax gives one dimensional output with Nones, not singletons/empties.\r\n>>> # (This is required for conformance with NumPy.)\r\n>>> ak.argmax(one_quantity, axis=1)\r\n<Array [2, None, 0] type='3 * ?int64'>\r\n\r\n>>> # Applying that to \"another\" is not what we want, since it applies to the first dimension.\r\n>>> # It rearranges the lists, rather than picking out the corresponding element from each list.\r\n>>> another[ak.argmax(one_quantity, axis=1)]\r\n<Array [[5.5, 4.4], None, [1.1, 2.2, 3.3]] type='3 * option[var * float64]'>\r\n\r\n>>> # ak.singletons gets us to that other format.\r\n>>> ak.singletons(ak.argmax(one_quantity, axis=1))\r\n<Array [[2], [], [0]] type='3 * var * int64'>\r\n\r\n>>> # And we get what we want: the corresponding value from each list.\r\n>>> another[ak.singletons(ak.argmax(one_quantity, axis=1))]\r\n<Array [[3.3], [], [5.5]] type='3 * var * float64'>\r\n\r\n>>> # Which we then have to pair with ak.firsts to turn it back into flat-with-missing-values.\r\n>>> ak.firsts(another[ak.singletons(ak.argmax(one_quantity, axis=1))])\r\n<Array [3.3, None, 5.5] type='3 * ?float64'>\r\n```\r\n\r\nHowever, that's non-obvious and verbose. I don't remember where I first saw it, but some users of Awkward found a much simpler solution:\r\n\r\n```python\r\n>>> # The 'keepdims' argument was included for NumPy compatibility...\r\n>>> ak.argmax(one_quantity, axis=1, keepdims=True)\r\n<Array [[2], [None], [0]] type='3 * var * ?int64'>\r\n\r\n>>> # But it gives us nearly what ak.singletons does, which simplifies the max-by-another problem.\r\n>>> another[ak.argmax(one_quantity, axis=1, keepdims=True)]\r\n<Array [[3.3], [None], [5.5]] type='3 * var * ?float64'>\r\n\r\n>>> # And then we can do the \"ak.firsts\" thing with a simple slice.\r\n>>> another[ak.argmax(one_quantity, axis=1, keepdims=True)][:, 0]\r\n<Array [3.3, None, 5.5] type='3 * ?float64'>\r\n```\r\n\r\nSo in truth, `ak.singletons` and `ak.firsts` aren't really needed to solve the problem they were invented for, and their semantics weren't carefully thought through for other problems.\r\n\r\nFor instance, I wonder if you can get what you want by turning a regular axis created with `np.newaxis` into an irregular one with [ak.from_regular](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_regular.html):\r\n\r\n```python\r\n>>> vals = ak.Array([[43, 15, 10.5], [11.5], [50, 5]])\r\n>>> idx = ak.Array([2, 0, 1])\r\n\r\n>>> # np.newaxis makes a new axis with length 1.\r\n>>> idx[:, np.newaxis]\r\n<Array [[2], [0], [1]] type='3 * 1 * int64'>\r\n\r\n>>> # But we're about to do a jagged slice, so turn the length-1 dimension into a variable-length one.\r\n>>> ak.from_regular(idx[:, np.newaxis])\r\n<Array [[2], [0], [1]] type='3 * var * int64'>\r\n\r\n>>> # Use this to slice \"vals\". It picks out the nth item from each list.\r\n>>> vals[ak.from_regular(idx[:, np.newaxis])]\r\n<Array [[10.5], [11.5], [5]] type='3 * var * float64'>\r\n\r\n>>> # It works exactly the same way if you have any missing data.\r\n>>> idx = ak.Array([2, None, 1])\r\n>>> vals[ak.from_regular(idx[:, np.newaxis])]\r\n<Array [[10.5], [None], [5]] type='3 * var * ?float64'>\r\n\r\n>>> # And that shape is exactly what you want if you're going to be removing this dimension.\r\n>>> vals[ak.from_regular(idx[:, np.newaxis])][:, 0]\r\n<Array [10.5, None, 5] type='3 * ?float64'>\r\n```\r\n\r\nSo maybe `ak.singletons` and `ak.firsts` should be deprecated, and these slicing tricks should be advertised in tutorials on the https://awkward-array.org site. (I _really_ need to write more of that documentation.) If you agree, I'll convert this Issue into a Discussion and then it would remain visible for others to comment on the proposed deprecation and benefit from the ad-hoc documentation above.",
     "createdAt":"2021-02-06T19:45:55Z",
     "number":345236,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"nsmith-"
        },
        "body":"I think here is where I pointed out that keepdims needed to return an irregular axis to be used as a slicer: https://github.com/scikit-hep/awkward-1.0/issues/434 and its nice to see now there is a utility to convert between regular and irregular with `ak.from_regular`",
        "createdAt":"2021-02-10T17:06:21Z",
        "number":357277
       },
       {
        "author":{
         "login":"drahnreb"
        },
        "body":"Slicing can be hard. Ragged slicing can be harder. This might be a bit off-topic regarding `ak.singletons` and `ak.firsts` deprecation but I think a slicing abstraction layer e.g. an awkward-style `np.take` that extends to unique challenges in the ragged space, like #370, could combine and simplify a few common or interesting slicing applications.\r\n\r\nApart from the discussed scenarios, I see useful extensions like e.g. multidimensional index slicing\r\n```python \r\nidxs = [0, 1, 0, 2, 1]\r\nak.singletons(ak.pad_none(a, ak.num(a))[np.arange(len(a)), idxs])\r\n```\r\nor \"ragged-to-ragged slicing\" instead of semi-internal `layout` fiddling (where I mutate `starts` and `stops` indices). ",
        "createdAt":"2021-02-10T23:19:00Z",
        "number":358233
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"We're heading a little in that direction\u2014the [ak.mask](https://awkward-array.readthedocs.io/en/latest/_auto/ak.mask.html) syntax deliberately makes it look like a slice because it's almost a slice:\r\n\r\n```python\r\n>>> array = ak.Array(range(10))\r\n>>> array\r\n<Array [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] type='10 * int64'>\r\n>>> array[array % 2 == 0]\r\n<Array [0, 2, 4, 6, 8] type='5 * int64'>\r\n>>> array.mask[array % 2 == 0]\r\n<Array [0, None, 2, None, ... 6, None, 8, None] type='10 * ?int64'>\r\n```\r\n\r\nand I was thinking about a `pick` that also acts as an easier slice: https://github.com/scikit-hep/awkward-1.0/issues/203#issuecomment-615509590\r\n\r\nI'd like to avoid the word `take`, though, because it already has a meaning. @nsmith- mentioned below the usefulness of some shorthands. If they can be expressed entirely in terms of existing operations, maybe they could go into a new operations submodule:\r\n\r\n```\r\nsrc/awkward\r\n\u2514\u2500\u2500 operations\r\n \u00a0\u00a0 \u251c\u2500\u2500 convert.py\r\n \u00a0\u00a0 \u251c\u2500\u2500 derived.py     <---- ?\r\n \u00a0\u00a0 \u251c\u2500\u2500 describe.py\r\n \u00a0\u00a0 \u251c\u2500\u2500 reducers.py\r\n \u00a0\u00a0 \u2514\u2500\u2500 structure.py\r\n```",
        "createdAt":"2021-02-11T22:35:13Z",
        "number":361135
       }
      ],
      "totalCount":3
     }
    },
    {
     "author":{
      "login":"aminnj"
     },
     "body":"Thanks for the detailed answer! I need to remember to use `keepdims=True` when the index array comes from other awkward operations. The `np.newaxis` trick works for when I have a handmade index array (that might use -1 instead of None for when the collection is empty).\r\n\r\nMaybe `ak.singletons`/`ak.firsts` is still useful to keep around, given that they can convert the output of `argmax`/`argmin` after they have been computed (at least, with the above alternatives advertised somewhere)? I've also found `ak.firsts` to be a nicer alternative to `ak.pad_none(a, 1)[:,0]` since it turns inner `[]` into `None`)",
     "createdAt":"2021-02-06T20:00:48Z",
     "number":345237,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"nsmith-"
        },
        "body":"I want to +1 the use of `ak.firsts` as shorthand for `ak.pad_none(a, 1)[:, 0]` which is somewhat common. In fact, it would be nice to be able to optionally ignore bounds in slices like this, returning `None` when out of bounds.",
        "createdAt":"2021-02-10T17:08:53Z",
        "number":357302
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"But you did raise an important point, that `ak.singletons` and `ak.firsts` are not well thought-through. They don't do what you expected for non-option-type arrays, so they ought to be fixed (given a simple, easily describable rule for what they should do in all circumstances) or removed. Removing is definitely less work!\r\n\r\nI'll convert this into a Discussion to let others weigh in on it. Now would be a good time to schedule them for removal in 1.2.0 or 1.3.0 (see [Roadmap](https://github.com/scikit-hep/awkward-1.0#roadmap)).",
     "createdAt":"2021-02-06T20:12:24Z",
     "number":345238,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"nsmith-"
        },
        "body":"Nick shows clearly an inconsistency for `ak.singletons` but is there one also for `ak.firsts`?",
        "createdAt":"2021-02-10T17:10:48Z",
        "number":357310
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"I currently use `ak.firsts` to handle the case where I have a `bool` array on which I wish to evaluate something _like_ `ak.where`, except yielding `None` where no items are `True`, and yielding only one value for the inner axis. \r\n\r\n<!-- If I just wanted `np.where`, then I'd have to count and pad the array first(?):\r\n```python\r\nmask = ...\r\nsize_dim_1 = ak.max(ak.count(mask, axis=-1))\r\npadded = padded = ak.pad_none(mask, padded, axis=1)\r\nregularised = ak.fill_none(padded, False)\r\ni, j = ak.where(regularised)\r\n```\r\n\r\nBut because I want to include `None` values where the mask is entirely false for the inner axis, and I don't want repeated values per index, I do this: -->\r\n\r\n```python\r\nlocal_nonzero = ak.local_index(is_pid_i)[is_pid_i]\r\nj = ak.firsts(local_nonzero)\r\n```\r\n\r\nI wonder if there's a better way to do this that doesn't require local_index, but I suspect not because somewhere the local indices need to be computed. Clearly, without `firsts`, I'd need to `pad_none` and then slice.",
     "createdAt":"2021-04-23T15:03:48Z",
     "number":650443,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"**Conclusion:** `ak.singletons` and `ak.firsts` will be kept, but their implementation in Awkward version 2.0 should probably be a rewrite without guarantee of backward compatibility, so that they can be better suited to the cases they were applied to, as opposed to the `ak.argmin`/`argmax` case.\r\n\r\nSee #1189 for an example of something that wasn't well thought-through.\r\n\r\n`ak.firsts` has an `axis` but `ak.singletons` doesn't. Should it? (Can it?)",
     "createdAt":"2022-01-04T20:37:44Z",
     "number":1908293,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":5
  },
  "createdAt":"2021-02-06T05:49:23Z",
  "number":710,
  "title":"Removing the 'ak.singletons' and 'ak.firsts' functions: any complaints?",
  "url":"https://github.com/scikit-hep/awkward/discussions/710"
 },
 {
  "author":{
   "login":"Dominic-Stafford"
  },
  "body":"I've encountered two (possibly related) bugs when using negative axis indices in local_index (with awkward version 1.1.0rc2). The first is that `ak.local_index(jets, axis=-1)`, where \"jets\" comes from the coffea Nanoevents schema, gives an error:\r\n`*** ValueError: axis == -1 exceeds the min depth == 1 of this array`\r\nHowever `ak.local_index(jets, axis=1)` and `ak.local_index(jets.pt, axis=-1)` both work and give the same output as I would expect from `ak.local_index(jets, axis=-1)`.  I wasn't able to directly reproduce this problem outside of a coffea processor, as it didn't seem to occur for simpler classes like lorentz vectors. If you think this actually a problem with coffea's Nanoevents rather than an awkard problem I can make an issue there.\r\n\r\nThe second issue, which I found while trying to reproduce the first, is that for an array containing only strings, local index seems to find another axis:\r\n```\r\n>>> arr = ak.from_iter([[\"a\", \"b\", \"c\"], [], [\"d\", \"e\"]])\r\n>>> ak.local_index(arr, axis=-1)\r\n<Array [[[0], [0], [0]], [], [[0], [0]]] type='3 * var * var * int64'>\r\n>>> ak.local_index(arr, axis=-2)\r\n<Array [[0, 1, 2], [], [0, 1]] type='3 * var * int64'>\r\n```\r\nIf the array is made of integers, the output is what I would expect:\r\n```\r\n>>> arr = ak.from_iter([[1, 2, 3], [], [4, 5]])\r\n>>> ak.local_index(arr, axis=-1)\r\n<Array [[0, 1, 2], [], [0, 1]] type='3 * var * int64'>\r\n```\r\nAnd if I mix ints and strings, I get the same error as for the jets:\r\n```\r\n>>> arr = ak.from_iter([[\"a\", \"b\", \"c\"], [], [\"d\", 1]])\r\n>>> ak.local_index(arr, axis=-1)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/afs/desy.de/user/s/stafford/.local/lib/python3.8/site-packages/awkward/operations/structure.py\", line 1771, in local_index\r\n    out = layout.localindex(axis)\r\nValueError: axis == -1 exceeds the min depth == 1 of this array\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.1.0rc2/src/libawkward/Content.cpp#L1708)\r\n```\r\n\r\nThanks,\r\n\r\nDominic",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I suspect that the error is correct for your first issue, though I'm open to improvements in the phrasing of the message. Is it because `jets` has fields with different depths? When you say `jets.pt`, you get a jagged array of numbers\u2014no record structures anywhere\u2014but `jets` is a record that might contain fields with more dimensions than `pt`. The `axis=-1` makes the function apply to the innermost dimension of all fields of the record(s), which might be different depths for different fields.\r\n\r\n```python\r\n>>> array = ak.Array([{\"x\": [1, 2, 3], \"y\": [[1, 2, 3], [], [4, 5]]}])\r\n>>> print(ak.num(array, axis=-1))\r\n[{x: 3, y: [3, 0, 2]}]\r\n>>> print(ak.local_index(array, axis=-1))\r\n[{x: [0, 1, 2], y: [[0, 1, 2], [], [0, 1]]}]\r\n```\r\n\r\nWhereas in NumPy, negative `axis` is a convenience (you don't have to check `array.ndim` to choose an `axis` from the other end), in Awkward Array, it's the only way to express some things. In the above example, `axis=-1` means `axis=1` for *x* and `axis=2` for *y*.\r\n\r\n```python\r\n>>> print(ak.num(array.x, axis=1), ak.num(array.y, axis=2))\r\n[3] [[3, 0, 2]]\r\n>>> print(ak.local_index(array.x, axis=1), ak.local_index(array.y, axis=2))\r\n[[0, 1, 2]] [[[0, 1, 2], [], [0, 1]]]\r\n```\r\n\r\nIn your case, it might be that `jets` has some fields that can't be mutually computed at any `axis=-1` level. (It depends on what's in the `jets` record, which is why it was hard to reproduce.) If you have a set of fields that you want to narrow in on, you could do a [nested projection](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#nested-projection), like this:\r\n\r\n```python\r\n>>> jets[[\"pt\", \"eta\", \"phi\", \"m\"]]\r\n```\r\n\r\nwhich would keep kinematics while dropping the subjet structure or associated leptons or whatever it is that has deeper jaggedness and is preventing the `ak.local_index`. On the other hand, if you're only trying to get a local index, you only need one field, such as `jets.pt`.\r\n\r\n----------------------------\r\n\r\nAs for the second issue, the extra dimension on the strings is the list that is the string itself. Strings are not special objects, they're a special interpretation of lists, but only some functions do special things with them. (Try looking at their `layout`: you'll see that the \"stringiness\" is just a parameterization of the ListArray/ListOffsetArray.) `ak.local_index`, in particular, is generic/naive with respect to strings\u2014there's no mention of strings [in the documentation](https://awkward-array.readthedocs.io/en/latest/_auto/ak.local_index.html).\r\n\r\nBut maybe this is the wrong behavior? I could convert this issue into a Discussion if these two topics are things you want others to chime in on.",
     "createdAt":"2021-02-08T19:21:05Z",
     "number":353626,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"Dominic-Stafford"
     },
     "body":"Thank you for the explanation. I agree these things aren't really bugs, but feel it would be good to have more error messages/documentation to make the user aware of them, and I agree this issue should be converted to a discussion so other people can suggest how they would expect it to behave.\r\n\r\nFor the first issue, having looked at the jets again, I see there is a field (`muonIdxG`) with `ndim`=3, while the other fields have `ndim`=2, as you suggested. If it's not too hard to implement, it might be nice if the error message in cases like these was something like \"Cannot compute negative index of RecordArray with different depths\" to make this a little clearer.\r\n\r\nFor the second issue, I hadn't realised awkward was aware that strings are lists, as this is not the case in numpy. I feel the current implementation would probably surprise a lot of people, but also some more experienced users might want to be able to access the strings in this way. Maybe one could add a flag for whether to consider strings as lists, or just objects in the array, either in the `local_index` function, or maybe as an option when producing an Array (with the default to just consider them as objects)?\r\n\r\nAlso, maybe index=-1 shouldn't be the default for `local_index`, as a new user expecting it to just behave as in numpy might experience similar pitfalls? However, I guess this is the option people are mostly likely to use, so it would be useful if other people have an opinion on this.",
     "createdAt":"2021-02-09T12:23:19Z",
     "number":353627,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I'll make this a discussion. There are things to do here, but it would be good to get more input.\r\n\r\nNumPy has two ways of representing strings: as fixed-width bytes (unencoded or UTF-32) and as Python objects. The wasted space in fixed-width strings is severe enough that Pandas defaults to Python objects. However, Python objects can only be used at the Python level (no calculations in C++ without making the C++ layer depend on Python headers). Both of these dtypes can be _passed through_ an Awkward Array, but there are many places where I need to do something with the dtype of an array and only support a reasonable set (booleans, numbers, now including complex, and hopefully soon date-times). Fixed-width bytes and Python object pointers are not included in that set, so as a user, it might work at first, but soon you'd run into something unsupported.\r\n\r\nBut considering that both fixed-width bytestrings and Python objects are highly wasteful, I think we should keep using variable-length strings, as Arrow and Parquet do, but make more methods aware of them. Since negative index handling happens in one place, we could make a string's internal dimension never contribute to the \"number of dimensions\" used when calculating a negative `axis`. That can become one of the specializations associated with `__array__ = \"string\"`.\r\n",
     "createdAt":"2021-02-09T16:28:21Z",
     "number":353628,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"For this and other reasons, I've implemented the above in #737. Now the depth of an array of strings is the same as the depth of an array of numbers:\r\n\r\n```python\r\n>>> ak.Array([\"one\", \"two\", \"three\"])\r\n<Array ['one', 'two', 'three'] type='3 * string'>\r\n>>> ak.Array([\"one\", \"two\", \"three\"]).layout.purelist_depth\r\n1\r\n>>> ak.Array([[\"one\", \"two\"], [\"three\"]]).layout.purelist_depth\r\n2\r\n>>> ak.Array([1, 2, 3]).layout.purelist_depth\r\n1\r\n>>> ak.Array([[1, 2], [3]]).layout.purelist_depth\r\n2\r\n```\r\n\r\nThe only implementations of functions that depended on this have been made _simpler_ as a result of fixing them. This was a good thing to do (and to do it now before many functions use strings).",
        "createdAt":"2021-02-12T22:08:36Z",
        "number":364159
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"PR #737 fixes your original issue:\r\n\r\n```python\r\n>>> arr = ak.from_iter([[\"a\", \"b\", \"c\"], [], [\"d\", \"e\"]])\r\n>>> ak.local_index(arr, axis=-1)\r\n<Array [[0, 1, 2], [], [0, 1]] type='3 * var * int64'>\r\n```",
     "createdAt":"2021-02-12T22:10:13Z",
     "number":364165,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"Dominic-Stafford"
        },
        "body":"Thank you! This seems to be a nice way to fix that behaviour",
        "createdAt":"2021-02-16T10:55:09Z",
        "number":373233
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":4
  },
  "createdAt":"2021-02-08T12:19:53Z",
  "number":716,
  "title":"Definition of negative 'axis' in 'ak.local_index' and strings (and asking for better error messages).",
  "url":"https://github.com/scikit-hep/awkward/discussions/716"
 },
 {
  "author":{
   "login":"mariogeiger"
  },
  "body":"Hi,\r\n\r\nI'm using heterogeneous data structures to store my experiment results and I prefer to use `pytorch` as `numpy`. So my data contain `torch.Tensor` objects. How can I load my data into `awkward`?\r\n\r\n```python\r\ndata = [\r\n    dict(\r\n        x='abc',\r\n        y=torch.tensor([1.0, 2.0]),\r\n    ),\r\n    dict(\r\n        x='efg',\r\n        y=torch.tensor([3.0, 4.0]),\r\n    ),\r\n]\r\nak.from_iter(data)\r\n# ValueError: cannot convert tensor(1.) (type Tensor) to an array element\r\n```\r\n\r\nBest regards,\r\nMario",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"mariogeiger"
     },
     "body":"My solution so far:\r\n```python\r\ndef deepmap(fun, data):\r\n    if isinstance(data, (list, tuple, set, frozenset)):\r\n        return type(data)(to_numpy(x) for x in data)\r\n\r\n    if isinstance(data, dict):\r\n        return {key: to_numpy(x) for key, x in data.items()}\r\n\r\n    return fun(data)\r\n\r\nto_numpy = partial(deepmap, lambda x: x.numpy() if isinstance(x, torch.Tensor) else x)\r\n\r\nak.from_iter(to_numpy(data))\r\n```",
     "createdAt":"2021-02-13T08:54:27Z",
     "number":364794,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"We intend to write code that recognizes all the major array-like types, and currently we're working on JAX, not pytorch yet. This [Consortium for Python Data API Standards](https://data-apis.org) may help make that a one step thing.\r\n\r\nFor the moment, all of these array like types can be converted to NumPy, albeit manually as in your solution, and NumPy arrays can be viewed directly as Awkward Arrays. There is some [documentation on that](https://awkward-array.org/how-to-convert-numpy.html). Passing a numpy array directly to the [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html) constructor or (equivalently but more explicitly) the [ak.from_numpy](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_numpy.html) function would avoid a conversion of the data into and back out of python objects, which can be slow and use up memory. Don't use [ak.from_iter](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_iter.html) unless your data are already Python objects, which they are not in this case.\r\n\r\nOther than that, your solution of deeply iterating over the Python dicts containing pytorch Tensors is a good one. That has to be explicit, because only you know how you have structured the data at that level. That can be considered just a normal part of Python bookkeeping.",
     "createdAt":"2021-02-13T17:56:56Z",
     "number":366331,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-02-13T08:17:34Z",
  "number":739,
  "title":"Load data that contain pytorch tensors",
  "url":"https://github.com/scikit-hep/awkward/discussions/739"
 },
 {
  "author":{
   "login":"deeplook"
  },
  "body":"First, hats off for a fantastic package! Now I'm still very new to it, and while trying to apply it especially to GeoJSON data I wonder if there is some recommended way I haven't discovered yet to flatten/reduce/search/collect (many terms seem to apply) to a nested tree into a flat list, in my case for example GeoJSON `features` or even `geometry` fields which can exist on multiple levels, depending on the general structure and feature type (e.g. Polygon vs MultiPolygon). I've looked at https://awkward-array.org/how-to-math-reducing.html?highlight=reduce, but it's empty. Any pointers are much welcome!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I really need to write those tutorials, but I need to do a lot of things! Here are some quick hints for how to get started, though the actual answer to your question will depend on exactly what you want to do.\r\n\r\nStarting with the bike routes (loading them as on [what-is-awkward](https://awkward-array.org/what-is-awkward.html)),\r\n\r\n```python\r\n>>> bikeroutes\r\n<Record ... [-87.7, 42], [-87.7, 42]]]}}]} type='{\"type\": string, \"crs\": {\"type\"...'>\r\n```\r\n\r\nIn this particular GeoJSON file, all of the geometry elements are `\"MultiLineString\"`. I can check that using square-bracket syntax to get the `\"type\"` field\u2014I can't use the dot syntax because [ak.Array.type](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#ak-array-type) is a property of `ak.Arrays`.\r\n\r\n```python\r\n>>> bikeroutes[\"features\", \"geometry\", \"type\"]\r\n<Array ['MultiLineString', ... ] type='1061 * string'>\r\n>>> bikeroutes[\"features\", \"geometry\", \"type\"] == \"MultiLineString\"\r\n<Array [True, True, True, ... True, True, True] type='1061 * bool'>\r\n>>> ak.all(bikeroutes[\"features\", \"geometry\", \"type\"] == \"MultiLineString\")\r\nTrue\r\n```\r\n\r\nIf they differed, you'd be able to pick out one type with\r\n\r\n```python\r\n>>> only_multiline = bikeroutes[\"features\", \"geometry\", \"type\"] == \"MultiLineString\"\r\n>>> bikeroutes[\"features\", \"geometry\", \"coordinates\", only_multiline]\r\n<Array [[[[-87.8, 41.9], ... [-87.7, 42]]]] type='1061 * var * var * var * float64'>\r\n```\r\n\r\nOnce selected, the structure of the coordinates becomes more regular. In this case, the last \"var\" is always two elements, longitude and latitude. It can simplify things further along if we separate them early.\r\n\r\n```python\r\n>>> longitude = bikeroutes[\"features\", \"geometry\", \"coordinates\", only_multiline][:, :, :, 0]\r\n>>> latitude = bikeroutes[\"features\", \"geometry\", \"coordinates\", only_multiline][:, :, :, 1]\r\n>>> longitude, latitude\r\n(<Array [[[-87.8, -87.8, ... -87.7, -87.7]]] type='1061 * var * var * float64'>,\r\n <Array [[[41.9, 41.9, 41.9, ... 42, 42, 42]]] type='1061 * var * var * float64'>)\r\n```\r\n\r\nSome slices can't be put in the same square brackets, such as alternating advanced indexes and basic indexes (because in trying to generalize what NumPy does, one of its rules is ambiguous in a non-rectilinear context), so we just put them in separate square brackets, as above.\r\n\r\nAnyway, now that we have arrays with type `var * var * float64`, we can **reduce** those variable-length lists in a variety of ways. We could pick the first or last of each list:\r\n\r\n```python\r\n>>> longitude[:, :, 0]\r\n<Array [[-87.8], [-87.7], ... [-87.8], [-87.7]] type='1061 * var * float64'>\r\n>>> longitude[:, :, -1]\r\n<Array [[-87.8], [-87.7], ... [-87.8], [-87.7]] type='1061 * var * float64'>\r\n```\r\n\r\nor we could take the mean of each list:\r\n\r\n```python\r\n>>> ak.mean(longitude, axis=-1)\r\n<Array [[-87.8], [-87.7], ... [-87.8], [-87.7]] type='1061 * var * ?float64'>\r\n```\r\n\r\nThere are a lot of reducers and reducer-like functions:\r\n\r\n   * [ak.count](https://awkward-array.readthedocs.io/en/latest/_auto/ak.count.html): the number of elements (not to be confused with [ak.num](https://awkward-array.readthedocs.io/en/latest/_auto/ak.num.html), which interprets `axis` differently from a reducer).\r\n   * [ak.count_nonzero](https://awkward-array.readthedocs.io/en/latest/_auto/ak.count_nonzero.html): the number of elements that are not equal to zero or False.\r\n   * [ak.sum](https://awkward-array.readthedocs.io/en/latest/_auto/ak.sum.html): adds values with identity 0.\r\n   * [ak.prod](https://awkward-array.readthedocs.io/en/latest/_auto/ak.prod.html): multiplies values with identity 1.\r\n   * [ak.any](https://awkward-array.readthedocs.io/en/latest/_auto/ak.any.html): reduces with logical or, \"true if *any* members are non-zero.\"\r\n   * [ak.all](https://awkward-array.readthedocs.io/en/latest/_auto/ak.all.html): reduces with logical and, \"true if *all* members are non-zero.\"\r\n   * [ak.min](https://awkward-array.readthedocs.io/en/latest/_auto/ak.min.html): minimum value; empty lists result in None.\r\n   * [ak.max](https://awkward-array.readthedocs.io/en/latest/_auto/ak.max.html): maximum value; empty lists result in None.\r\n   * [ak.argmin](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argmin.html): integer position of the minimum value; empty lists result in None.\r\n   * [ak.argmax](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argmax.html): integer position of the maximum value; empty lists result in None.\r\n   * [ak.moment](https://awkward-array.readthedocs.io/en/latest/_auto/ak.moment.html): the \"nth\" moment of the distribution; ``0`` for sum, ``1`` for mean, ``2`` for variance without subtracting the mean, etc.\r\n   * [ak.mean](https://awkward-array.readthedocs.io/en/latest/_auto/ak.mean.html): also known as the average.\r\n   * [ak.var](https://awkward-array.readthedocs.io/en/latest/_auto/ak.var.html): variance about the mean.\r\n   * [ak.std](https://awkward-array.readthedocs.io/en/latest/_auto/ak.std.html): standard deviation about the mean.\r\n   * [ak.covar](https://awkward-array.readthedocs.io/en/latest/_auto/ak.covar.html): covariance of two datasets.\r\n   * [ak.corr](https://awkward-array.readthedocs.io/en/latest/_auto/ak.corr.html): correlation of two datasets (covariance normalized to variance).\r\n   * [ak.linear_fit](https://awkward-array.readthedocs.io/en/latest/_auto/ak.linear_fit.html): linear fits, possibly very many of them.\r\n   * [ak.softmax](https://awkward-array.readthedocs.io/en/latest/_auto/ak.softmax.html): the softmax function of machine learning.\r\n\r\nand I'll be adding `ak.median` soon (#741). (Any of these with the same name as a NumPy function have the same behavior as that NumPy function, and if you're using NumPy 1.17.0 or later, `np.mean` can be substituted for `ak.mean`.)\r\n\r\nThe `axis` parameter is important: `axis=-1` means the deepest lists. That's why it reduced an array of type `1061 * var * var * float64` to an array of type `1061 * var * ?float64` (the question mark, [meaning option-type](https://datashape.readthedocs.io/en/latest/types.html#option-missing-data), is to allow for empty lists, whose mean is `None`).\r\n\r\nIf instead of `axis=-1`, we averaged over the first level lists,\r\n\r\n```python\r\n>>> ak.mean(longitude, axis=0)\r\n<Array [[-87.7, -87.7, ... -87.6, -87.6]] type='7 * var * ?float64'>\r\n```\r\n\r\nNotice that we have only 7 results. That's because the longest list in the first `var` dimension is 7, and here, the first element is an average over all the firsts of the inner lists, the second element is an average over all the seconds of the inner lists, etc. You can see that by looking at the multiplicity of each of these elements:\r\n\r\n```python\r\n>>> ak.num(ak.mean(longitude, axis=0))\r\n<Array [1980, 113, 97, 42, 167, 87, 3] type='7 * int64'>\r\n```\r\n\r\nThere are a lot of firsts, fewer seconds, even fewer thirds, and only 3 lists with all 7 elements. These are the lists of discontiguous paths: most paths are contiguous (having only one section), only only a few have a lot of discontiguous sections.\r\n\r\nWe don't have an explicit \"group by,\" but this is becoming possible because of some new functions like [ak.argsort](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argsort.html), [ak.run_lengths](https://awkward-array.readthedocs.io/en/latest/_auto/ak.run_lengths.html), and [ak.unflatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.unflatten.html). See the `ak.run_lengths` documentation for an example of how to put those three together to do a group-by. Admittedly, that's the opposite of \"reducing,\" but it might be what you meant by \"collecting.\"\r\n\r\nAnother \"opposite of reduction\" is padding. In some cases, you'll want to pad a dimension to equal lengths, rather than reducing the dimension away. Suppose, for instance, we want to take a mean over path segments but pad the number of path segments to 7. That is, starting from\r\n\r\n```python\r\n>>> ak.mean(longitude, axis=-1)\r\n<Array [[-87.8], [-87.7], ... [-87.8], [-87.7]] type='1061 * var * ?float64'>\r\n```\r\n\r\nwe can look at the number of reduced elements in each,\r\n\r\n```python\r\n>>> ak.num(ak.mean(longitude, axis=-1))\r\n<Array [1, 1, 1, 1, 1, 1, ... 1, 1, 1, 1, 1, 1] type='1061 * int64'>\r\n```\r\n\r\nwhich looks like 1 everywhere, but actually there are a few that are more than 1:\r\n\r\n```python\r\n>>> ak.max(ak.num(ak.mean(longitude, axis=-1)))\r\n7\r\n```\r\n\r\nThe [ak.pad_none](https://awkward-array.readthedocs.io/en/latest/_auto/ak.pad_none.html) function will pad them up to a given length:\r\n\r\n```python\r\n>>> ak.pad_none(ak.mean(longitude, axis=-1), 7)\r\n<Array [[-87.8, None, None, ... None, None]] type='1061 * var * ?float64'>\r\n>>> ak.num(ak.pad_none(ak.mean(longitude, axis=-1), 7))\r\n<Array [7, 7, 7, 7, 7, 7, ... 7, 7, 7, 7, 7, 7] type='1061 * int64'>\r\n```\r\n\r\nNote that if you also `clip=True`, then the length of each list is no longer variable: we pad up to 7 and also clip any that might be more than 7, so the type is now regular: there are exactly 7 in each.\r\n\r\n```python\r\n>>> ak.pad_none(ak.mean(longitude, axis=-1), 7, clip=True)\r\n<Array [[-87.8, None, None, ... None, None]] type='1061 * 7 * ?float64'>\r\n>>> # or 3 or whatever\r\n>>> ak.pad_none(ak.mean(longitude, axis=-1), 3, clip=True)\r\n<Array [[-87.8, None, None, ... None, None]] type='1061 * 3 * ?float64'>\r\n```\r\n\r\nNow this is ready to be converted into a NumPy array with [ak.to_numpy](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_numpy.html),\r\n\r\n```python\r\n>>> ak.to_numpy(ak.pad_none(ak.mean(longitude, axis=-1), 7, clip=True))\r\nmasked_array(\r\n  data=[[-87.78982910437593, --, --, --, --, --, --],\r\n        [-87.74850140064592, --, --, --, --, --, --],\r\n        [-87.61668302288385, --, --, --, --, --, --],\r\n        [-87.61225128042686, --, --, --, --, --, --],\r\n        [-87.61658216320194, --, --, --, --, --, --],\r\n        [-87.70704523129784, --, --, --, --, --, --],\r\n        [-87.70086075786773, --, --, --, --, --, --],\r\n        [-87.62536362791651, --, --, --, --, --, --],\r\n        [-87.62367290319223, --, --, --, --, --, --],\r\n        [-87.62215393620382, --, --, --, --, --, --],\r\n        [-87.60498403377314, --, --, --, --, --, --],\r\n        [-87.67312384621376, --, --, --, --, --, --],\r\n        [-87.69709189597117, --, --, --, --, --, --],\r\n        [-87.54187541007825, --, --, --, --, --, --],\r\n        [-87.69098199150667, --, --, --, --, --, --],\r\n        [-87.63376205750455, --, --, --, --, --, --],\r\n        [-87.59003505243615, --, --, --, --, --, --],\r\n        [-87.60663406256548, --, --, --, --, --, --],\r\n        [-87.59210358472923, --, --, --, --, --, --],\r\n        [-87.62592114179975, --, --, --, --, --, --],\r\n        [-87.71816696431299, --, --, --, --, --, --],\r\n        [-87.6565763901438, --, --, --, --, --, --],\r\n        [-87.64863900594533, --, --, --, --, --, --],\r\n        [-87.64907619265722, --, --, --, --, --, --],\r\n        [-87.69539301998199, --, --, --, --, --, --],\r\n        [-87.60919371688361, --, --, --, --, --, --],\r\n        [-87.64676773058504, --, --, --, --, --, --],\r\n        [-87.61508257726894, --, --, --, --, --, --],\r\n        [-87.61109823582429, --, --, --, --, --, --],\r\n        [-87.60794901995541, --, --, --, --, --, --],\r\n        [-87.66069859146353, --, --, --, --, --, --],\r\n        [-87.6446851348852, --, --, --, --, --, --],\r\n        [-87.64438008141994, --, --, --, --, --, --],\r\n        [-87.63347281118922, --, --, --, --, --, --],\r\n        [-87.64716591424734, --, --, --, --, --, --],\r\n        [-87.65677825277427, --, --, --, --, --, --],\r\n        [-87.65703560508626, --, --, --, --, --, --],\r\n        [-87.63102881317302, --, --, --, --, --, --],\r\n        [-87.54420184515567, --, --, --, --, --, --],\r\n        [-87.64399093900948, --, --, --, --, --, --],\r\n        [-87.63153167004569, --, --, --, --, --, --],\r\n        [-87.65729258714634, --, --, --, --, --, --],\r\n        [-87.6736296226221, --, --, --, --, --, --],\r\n        [-87.64647514696313, --, --, --, --, --, --],\r\n        [-87.5670347436087, --, --, --, --, --, --],\r\n        [-87.63445542054468, --, --, --, --, --, --],\r\n        [-87.64666649778596, --, --, --, --, --, --],\r\n        [-87.6353988580254, --, --, --, --, --, --],\r\n        [-87.69208330758644, --, --, --, --, --, --],\r\n        [-87.67987472541603, --, --, --, --, --, --],\r\n        [-87.6288880473052, --, --, --, --, --, --],\r\n        [-87.63133498484497, --, --, --, --, --, --],\r\n        [-87.56911419280497, --, --, --, --, --, --],\r\n        [-87.65713893195229, --, --, --, --, --, --],\r\n        [-87.63982575795488, --, --, --, --, --, --],\r\n        [-87.64968309800122, --, --, --, --, --, --],\r\n        [-87.66627785087064, --, --, --, --, --, --],\r\n        [-87.60768991764432, --, --, --, --, --, --],\r\n        [-87.63986340460846, --, --, --, --, --, --],\r\n        [-87.63017721238154, --, --, --, --, --, --],\r\n        [-87.64271285534649, --, --, --, --, --, --],\r\n        [-87.63692635382557, --, --, --, --, --, --],\r\n        [-87.68840658206535, --, --, --, --, --, --],\r\n        [-87.68639424460982, --, --, --, --, --, --],\r\n        [-87.67971630848649, --, --, --, --, --, --],\r\n        [-87.68134923819026, --, --, --, --, --, --],\r\n        [-87.6652102372879, --, --, --, --, --, --],\r\n        [-87.63905032174591, --, --, --, --, --, --],\r\n        [-87.69822984437612, --, --, --, --, --, --],\r\n        [-87.70758832634793, --, --, --, --, --, --],\r\n        [-87.60824261786335, --, --, --, --, --, --],\r\n        [-87.68502518903846, --, --, --, --, --, --],\r\n        [-87.67666348420444, --, --, --, --, --, --],\r\n        [-87.65184998823406, --, --, --, --, --, --],\r\n        [-87.65576943439689, --, --, --, --, --, --],\r\n        [-87.67502226068834, --, --, --, --, --, --],\r\n        [-87.67495423682854, --, --, --, --, --, --],\r\n        [-87.77024768348971, --, --, --, --, --, --],\r\n        [-87.76474227896196, --, --, --, --, --, --],\r\n        [-87.67440580109745, --, --, --, --, --, --],\r\n        [-87.62062959129157, --, --, --, --, --, --],\r\n        [-87.6192656193883, --, --, --, --, --, --],\r\n        [-87.67955436986004, --, --, --, --, --, --],\r\n        [-87.65761633777541, --, --, --, --, --, --],\r\n        [-87.54792339554031, --, --, --, --, --, --],\r\n        [-87.75194652296219, --, --, --, --, --, --],\r\n        [-87.6964643882578, --, --, --, --, --, --],\r\n        [-87.64773169995742, --, --, --, --, --, --],\r\n        [-87.70701265034963, --, --, --, --, --, --],\r\n        [-87.6631859770511, --, --, --, --, --, --],\r\n        [-87.66325805923087, --, --, --, --, --, --],\r\n        [-87.68273844008236, --, --, --, --, --, --],\r\n        [-87.71189753652736, --, --, --, --, --, --],\r\n        [-87.64385660800737, --, --, --, --, --, --],\r\n        [-87.62861981584953, --, --, --, --, --, --],\r\n        [-87.65582427736987, --, --, --, --, --, --],\r\n        [-87.64399888515813, --, --, --, --, --, --],\r\n        [-87.75517298463835, --, --, --, --, --, --],\r\n        [-87.76290638279215, --, --, --, --, --, --],\r\n        [-87.71399913924206, --, --, --, --, --, --]],\r\n  mask=[[False,  True,  True, ...,  True,  True,  True],\r\n        [False,  True,  True, ...,  True,  True,  True],\r\n        [False,  True,  True, ...,  True,  True,  True],\r\n        ...,\r\n        [False,  True,  True, ...,  True,  True,  True],\r\n        [False,  True,  True, ...,  True,  True,  True],\r\n        [False,  True,  True, ...,  True,  True,  True]],\r\n  fill_value=1e+20)\r\n```\r\n\r\nthough perhaps we don't like [NumPy masked arrays](https://numpy.org/doc/stable/reference/maskedarray.html) (the equivalent of Awkward's option-type, when applied to numbers). Padding is often paired with [ak.fill_none](https://awkward-array.readthedocs.io/en/latest/_auto/ak.fill_none.html), which replaces missing values with a given value.\r\n\r\n```python\r\n>>> ak.fill_none(ak.pad_none(ak.mean(longitude, axis=-1), 7, clip=True), 0)\r\n<Array [[-87.8, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='1061 * 7 * float64'>\r\n```\r\n\r\nNow we could use [ak.to_numpy](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_numpy.html) or even plain [np.asarray](https://numpy.org/doc/stable/reference/generated/numpy.asarray.html) to get a NumPy array. Some libraries automatically coerce their arguments into NumPy arrays (in which case, you don't need to [np.asarray](https://numpy.org/doc/stable/reference/generated/numpy.asarray.html) explicitly), but some don't.\r\n\r\n```python\r\n>>> np.asarray(ak.fill_none(ak.pad_none(ak.mean(longitude, axis=-1), 7, clip=True), 0))\r\narray([[-87.7898291 ,   0.        ,   0.        , ...,   0.        ,\r\n          0.        ,   0.        ],\r\n       [-87.7485014 ,   0.        ,   0.        , ...,   0.        ,\r\n          0.        ,   0.        ],\r\n       [-87.61668302,   0.        ,   0.        , ...,   0.        ,\r\n          0.        ,   0.        ],\r\n       ...,\r\n       [-87.75517298,   0.        ,   0.        , ...,   0.        ,\r\n          0.        ,   0.        ],\r\n       [-87.76290638,   0.        ,   0.        , ...,   0.        ,\r\n          0.        ,   0.        ],\r\n       [-87.71399914,   0.        ,   0.        , ...,   0.        ,\r\n          0.        ,   0.        ]])\r\n```\r\n\r\nI hope this is enough to get you started!",
     "createdAt":"2021-02-17T15:35:30Z",
     "number":378053,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"deeplook"
        },
        "body":"@jpivarski Thank you! That's enough to drown in!",
        "createdAt":"2021-02-18T21:39:22Z",
        "number":382444
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-02-17T12:54:33Z",
  "number":745,
  "title":"How to flatten/reduce/search/collect fields into a flat list",
  "url":"https://github.com/scikit-hep/awkward/discussions/745"
 },
 {
  "author":{
   "login":"rkansal47"
  },
  "body":"Hi, is it possible to sum over records using ak.sum() while respecting the records' 'add' behaviour? Specifically I'm using the [PtEtaPhiMLorentzVector](https://coffeateam.github.io/coffea/api/coffea.nanoevents.methods.vector.PtEtaPhiMLorentzVector.html) record from Coffea and I am seeing that performing an ak.sum() over an array of of Vectors is not equivalent to summing them manually (i.e. with the '+' operation). \r\n\r\ne.g. with such an array:\r\n`\r\ntest_vecs = ak.zip({\r\n        \"pt\": [1, 2],\r\n        \"eta\": [0.1, 0.2],\r\n        \"phi\": [0.1, 0.2],\r\n        \"mass\": [0, 0]\r\n        }, with_name=\"PtEtaPhiMLorentzVector\")\r\n`\r\n\r\n`ak.sum(test_vecs, axis=0)` just sums each individual fields' arrays whereas I'm hoping to get the same output as doing `test_vecs[0] + test_vecs[1]` (which would add them using the defined behaviour). ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Currently, it is not, though I can see how that would be nice. The reducers (like sum) are handled deep in C++ to make `axis != -1` possible, but behaviors like `PtEtaPhiMLorentzVector` are only known to the Python layer.\r\n\r\nIn fact, it might be necessary to _prevent_ `ak.sum` from descending into records because it just adds components like this, and that would be undesirable (and surprising) for non-Cartesian coordinates like this.\r\n\r\nIt may be possible to get what you want\u2014sums of variable numbers of Lorentz vectors without having to pick out each one\u2014but it's not going to be an easy fix. @nsmith- asked about this a while ago.",
     "createdAt":"2021-02-21T14:15:55Z",
     "number":390858,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"nsmith-"
     },
     "body":"As a workaround for lack of `ak.sum` support, all the coffea vector objects have a `.sum()` method.  See https://coffeateam.github.io/coffea/api/coffea.nanoevents.methods.vector.LorentzVector.html#coffea.nanoevents.methods.vector.LorentzVector.sum\r\nIn your example, however, `test_vecs.sum()` fails due to #758. For a jagged test_vecs:\r\n```\r\n>>> test2 = ak.unflatten(test_vecs, [2])\r\n>>> test2.sum()[0]\r\n<LorentzVectorRecord ... y: 0.497, z: 0.503, t: 3.05} type='LorentzVector[\"x\": f...'>\r\n>>> test2.sum()[0].x\r\n2.955137320960509\r\n```",
     "createdAt":"2021-02-21T15:09:24Z",
     "number":390957,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-02-21T06:36:05Z",
  "number":761,
  "title":"ak.sum over records?",
  "url":"https://github.com/scikit-hep/awkward/discussions/761"
 },
 {
  "author":{
   "login":"williamnash"
  },
  "body":"Hello, I just started using Awkward array for HEP and had a quick question about the best way to reconfigure my existing code to use the package. \r\n\r\nI am starting with a flat tree which contains objects which link to each other via preset indices (e.g. a reco muon will contain an index for the gen muon it is matched to). I haven't yet found a simple way to combine the two jagged arrays of gen particles and muons in such a way that I can select only those gen muons among all the gen particles. For instance if I start with \r\n\r\n\r\n```python\r\ngen_particles = ak.Array([[a, b, c], [a, b, c, d, e], [a, b, c]])\r\nmuon_gen_ids = ak.Array([[0,1],[1,3], [0]])\r\n```\r\n\r\nI want select the subset of gen particles with ids which belong to the list of muon_gen_ids. In this case I want the array\r\n\r\n```python\r\ngen_muons = ak.Array([[a,b],[b,d], [a]])\r\n```\r\nI know how to select for the `gen_particle_ids` using the `local_index()` function, and imagine the solution would involve the `mask()` function, but haven't figure our a solution that doesn't involve a for-loop (which would be slower than my current implementation of just indexing into the array). Any suggestions?\r\n\r\nThanks!\r\nWill\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"As it turns out, it's easier than you think:\r\n\r\n```python\r\n>>> gen_particles = ak.Array([[\"a\", \"b\", \"c\"], [\"a\", \"b\", \"c\", \"d\", \"e\"], [\"a\", \"b\", \"c\"]])\r\n>>> muon_gen_ids = ak.Array([[0, 1], [1, 3], [0]])\r\n>>> gen_particles[muon_gen_ids]\r\n<Array [['a', 'b'], ['b', 'd'], ['a']] type='3 * var * string'>\r\n```\r\n\r\nWhat you want is to use the ids as a slice. This is like [NumPy's advanced indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html#advanced-indexing), but generalized to the non-rectilinear case.\r\n\r\n(Thanks for the very clear example and desired result! It helped me understand your problem quickly.)",
     "createdAt":"2021-03-26T23:07:31Z",
     "number":536365,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"williamnash"
        },
        "body":"Wow super slick. Thank you!",
        "createdAt":"2021-03-26T23:34:43Z",
        "number":536455
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-03-26T22:39:33Z",
  "number":792,
  "title":"Using an index based predefined mask",
  "url":"https://github.com/scikit-hep/awkward/discussions/792"
 },
 {
  "author":{
   "login":"HenryDayHall"
  },
  "body":"So sometimes calling `dict(ak_array)` gives an error.\r\n\r\n```python\r\n\r\nIn [10]: import awkward as ak\r\n    ...: print(ak.__version__)\r\n    ...: dog = ak.from_iter([[1., 2.], [5.]])\r\n    ...: pets = ak.zip({\"dog\": dog[np.newaxis]}, depth_limit=1)\r\n1.1.2\r\n\r\nIn [11]: dict(pets)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-11-f7ecb9a598b4> in <module>\r\n----> 1 dict(pets)\r\n\r\n~/Programs/anaconda3/envs/tree/lib/python3.7/site-packages/awkward/highlevel.py in __getitem__(self, where)\r\n   1765             2\r\n   1766         \"\"\"\r\n-> 1767         return ak._util.wrap(self._layout[where], self._behavior)\r\n   1768\r\n   1769     def __setitem__(self, where, what):\r\n\r\nValueError: scalar Record can only be sliced by field name (string); try \"0\"\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.1.2/src/libawkward/array/Record.cpp#L198)\r\n\r\n```\r\nIt is more than possible I have done something odd with my packages, so let me know if you don't reproduce it. Also apologies if it's already been reported.\r\n\r\nSide note for future readers; if you got here with the same goal I had then you wanted the list of column names, it's `ak.fields(pets)`.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I don't know what you want `dict(ak_array)` to do. Technically, what happens is that the `dict` constructor iterates over what it's given and tries to cast each item as a 2-tuple to use as key-value pairs. Some Awkward Arrays will have that structure and some won't.\r\n\r\nAre you wanting to turn the Awkward Array into Python lists and dicts? That's [ak.to_list](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_list.html) or `ak_array.tolist()` as a method (following NumPy's [naming convention](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.tolist.html)).\r\n\r\n```python\r\n>>> pets\r\n<Array [{dog: [[1, 2], [5]]}] type='1 * {\"dog\": var * var * float64}'>\r\n>>> pets.tolist()\r\n[{'dog': [[1.0, 2.0], [5.0]]}]\r\n```",
     "createdAt":"2021-03-31T16:38:48Z",
     "number":553581,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"HenryDayHall"
        },
        "body":"Oh that's cool, yes, that is what I was hopping to get. Thanks.",
        "createdAt":"2021-03-31T17:52:12Z",
        "number":553862
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-03-31T14:34:20Z",
  "number":798,
  "title":"Calling dict() on some ak.Array object",
  "url":"https://github.com/scikit-hep/awkward/discussions/798"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@ianna fixed the data type of `ak.argsort` for empty arrays (it should always be integer, but was float in some cases). PR #803.\r\n\r\nIt fixes code like\r\n\r\n```python\r\ngood_leptons = good_leptons[ak.argsort(good_leptons.pt)]\r\n```\r\n\r\nin which `good_leptons` consists of empty lists.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.2.1'>1.2.1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-04-07T17:16:33Z",
  "number":804,
  "title":"1.2.1",
  "url":"https://github.com/scikit-hep/awkward/discussions/804"
 },
 {
  "author":{
   "login":"drahnreb"
  },
  "body":"Descriptors for class `PartitionedArray` and `IrregularlyPartitionedArray` are not accessible via:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> ak.__version__\r\n1.2.0\r\n>>> array = ak.repartition([{\"x\": x, \"y\": x * 10} for x in range(10)], 2)\r\n>>> array.layout\r\n<IrregularlyPartitionedArray>\r\n    <partition start=\"0\" stop=\"2\">\r\n        <RecordArray length=\"2\">\r\n            <field index=\"0\" key=\"x\">\r\n                <NumpyArray format=\"l\" shape=\"2\" data=\"0 1\" at=\"0x00010f038200\"/>\r\n            </field>\r\n            <field index=\"1\" key=\"y\">\r\n                <NumpyArray format=\"l\" shape=\"2\" data=\"0 10\" at=\"0x00010f03a200\"/>\r\n            </field>\r\n        </RecordArray>\r\n    </partition>\r\n    <partition start=\"2\" stop=\"4\">\r\n        <RecordArray length=\"2\">\r\n            <field index=\"0\" key=\"x\">\r\n                <NumpyArray format=\"l\" shape=\"2\" data=\"2 3\" at=\"0x00010f038200\"/>\r\n            </field>\r\n            <field index=\"1\" key=\"y\">\r\n                <NumpyArray format=\"l\" shape=\"2\" data=\"20 30\" at=\"0x00010f03a200\"/>\r\n            </field>\r\n        </RecordArray>\r\n    </partition>\r\n</IrregularlyPartitionedArray>\r\n\r\n>>> ak.partitions(array)\r\n[2,2]\r\n>>> array.partitions\r\n\r\nAttributeError: no field named 'partitions'\r\n```\r\nAccording to the docs, this should be a valid repartition? \r\n\r\nThe [tests](https://github.com/scikit-hep/awkward-1.0/blob/main/tests/test_0056-partitioned-array.py) do not cover `ak.repartition` with partitioned array methods, but following one test leads to:\r\n```python\r\n>>> one = ak.from_iter([[1.1, 2.2, 3.3], [], [4.4, 5.5]], highlevel=False)\r\n>>> two = ak.from_iter([[6.6], [], [], [], [7.7, 8.8, 9.9]], highlevel=False)\r\n>>> array = ak.partition.IrregularlyPartitionedArray([one, two])\r\n>>> array.layout\r\n\r\nAttributeError: no field named 'layout'\r\n```\r\n\r\nAccording to the docs, I would expect this to work as\r\n> [...] it should behave identically to a non-partitioned array [...]",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Both of these are problems with mixing high-level and low-level arrays.\r\n\r\nIn the first example, `array` is a high-level array whose `layout` is partitioned. The fact that `array` is partitioned is not visible from the `array` level; you'd only know it if you delved into the `layout` (or used [ak.partitions](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partitions.html) to get the length of each). To access the actual partitions, you could do `array.layout.partitions`, but this would be a low-level view.\r\n\r\nIn the second example, you created a low-level `IrregularlyPartitionedArray`, which has no `layout` because it is a layout. If wrapped in an `ak.Array` constructor, it would behave like an unpartitioned high-level array.\r\n\r\nYou might be trying to iterate over partitions. There isn't actually a function for that (other than using the numbers from [ak.partitions](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partitions.html) to make slices in a `for` loop; see below). I've been wondering if there need to be more tools like this.\r\n\r\n```python\r\n>>> array = ak.repartition(np.arange(100), 10)\r\n>>> # high-level array\r\n>>> array\r\n<Array [0, 1, 2, 3, 4, ... 95, 96, 97, 98, 99] type='100 * int64'>\r\n>>> # low-level partitions\r\n>>> array.layout.partitions\r\n[\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"0 1 2 3 4 5 6 7 8 9\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"10 11 12 13 14 15 16 17 18 19\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"20 21 22 23 24 25 26 27 28 29\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"30 31 32 33 34 35 36 37 38 39\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"40 41 42 43 44 45 46 47 48 49\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"50 51 52 53 54 55 56 57 58 59\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"60 61 62 63 64 65 66 67 68 69\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"70 71 72 73 74 75 76 77 78 79\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"80 81 82 83 84 85 86 87 88 89\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"90 91 92 93 94 95 96 97 98 99\" at=\"0x562fa91d9a70\"/>\r\n]\r\n>>> # number of entries in each partition\r\n>>> ak.partitions(array)\r\n[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\r\n>>> # cumbersome way to iterate over partitions\r\n>>> start = 0\r\n>>> for count in ak.partitions(array):\r\n...     stop = start + count\r\n...     print(repr(array[start:stop]))\r\n...     start = stop\r\n... \r\n<Array [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] type='10 * int64'>\r\n<Array [10, 11, 12, 13, 14, ... 16, 17, 18, 19] type='10 * int64'>\r\n<Array [20, 21, 22, 23, 24, ... 26, 27, 28, 29] type='10 * int64'>\r\n<Array [30, 31, 32, 33, 34, ... 36, 37, 38, 39] type='10 * int64'>\r\n<Array [40, 41, 42, 43, 44, ... 46, 47, 48, 49] type='10 * int64'>\r\n<Array [50, 51, 52, 53, 54, ... 56, 57, 58, 59] type='10 * int64'>\r\n<Array [60, 61, 62, 63, 64, ... 66, 67, 68, 69] type='10 * int64'>\r\n<Array [70, 71, 72, 73, 74, ... 76, 77, 78, 79] type='10 * int64'>\r\n<Array [80, 81, 82, 83, 84, ... 86, 87, 88, 89] type='10 * int64'>\r\n<Array [90, 91, 92, 93, 94, ... 96, 97, 98, 99] type='10 * int64'>\r\n```\r\n\r\nThe biggest difference that partitions make is that every Awkward operation applies separately to each partition, returning a new partitioned array. the statement in the documentation is that these are not interface-visible differences (in the high-level view), but can be performance differences.",
     "createdAt":"2021-04-07T20:38:16Z",
     "number":581844,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"drahnreb"
        },
        "body":"Thanks, that makes a lot of sense. I was actually recursing on the low-level side for numba. While partitioning for chunked calculations I tried to iterate over the partitions on the wrong level. Must have been too late at night\u2026",
        "createdAt":"2021-04-08T02:44:23Z",
        "number":582638
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-04-07T20:20:21Z",
  "number":807,
  "title":"Using a PartitionedArray, accessing its properties",
  "url":"https://github.com/scikit-hep/awkward/discussions/807"
 },
 {
  "author":{
   "login":"Superharz"
  },
  "body":"I have two large nested awkward arrays ```a``` and ```b```.\r\nI want to calculate ```a - b```.\r\nThis operation takes about 30Gb of my RAM. However, the CPU is only at 10% and it seems like only 4 of my 16 logical cores are used.\r\nMy question: Is it possible to speed up the calculation and if yes: How?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Awkward Array does not automatically parallelize, and since most operations do not release the [Python GIL](https://realpython.com/python-gil/) (yet), a single process is not going to be able to take advantage of multiple cores.\r\n\r\nTo answer your question, I'd have to know a lot more about what `a`, `b`, and `-` are (if this is not using 100% of one CPU, they're probably not numbers, and so `-` is some [overloaded behavior](https://awkward-array.readthedocs.io/en/latest/ak.behavior.html)). However, I would guess that `a` and `b` are being lazily loaded from a file to use so much memory and not be CPU-bound.",
     "createdAt":"2021-04-08T21:19:50Z",
     "number":586808,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"Superharz"
        },
        "body":"Thanks for your explanation.\r\nI think you are right. The array I used was too big for my RAM. I found this out by flattening the the array with ```axis = None```.\r\nI tried it again with smaller arrays ```a``` and ```b``` and flattened them to load them completly into my RAM. The later substraction ``` a - b``` happened almost instant.",
        "createdAt":"2021-04-08T23:18:28Z",
        "number":587135
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-04-08T21:08:01Z",
  "number":810,
  "title":"How to speed up calculations",
  "url":"https://github.com/scikit-hep/awkward/discussions/810"
 },
 {
  "author":{
   "login":"lukasheinrich"
  },
  "body":"I'm working on a awkward-array integration with `vector` and `awkward` for `pylhe`\r\n\r\nThis is useful exercise for building up peharps a more complete event data model for ATLAS as well (@nikoladze @matthewfeickert @kratsg @gordonwatts @alexander-held @henryiii )\r\n\r\nThe raw arry (ignoring references/links that come layer is this (filtering for briefness)\r\n\r\n```python\r\n>>> events = ak.from_iter({\r\n  'particles': [\r\n      {k if k!= 'e' else 'E':getattr(p,k) for k in pylhe.LHEParticle.fieldnames if k in ['px','py','pz','e']} for p in e.particles \r\n  ],\r\n  'eventinfo':  {k:getattr(e.eventinfo,k) for k in pylhe.LHEEventInfo.fieldnames if k in ['nparticles','weight']}\r\n  }\r\n  \r\n  for e in pylhe.readLHEWithAttributes(skhep_testdata.data_path('pylhe-testfile-pr29.lhe'))\r\n)\r\n>>> ak.type(events)\r\n791 * {\"particles\": var * {\"px\": float64, \"py\": float64, \"pz\": float64, \"E\": float64}, \"eventinfo\": {\"nparticles\": float64, \"weight\": float64}}\r\n```\r\n\r\nand layout\r\n\r\n```python\r\n>>> events.layout.contents[0].content.setparameter('__record__','Particle')\r\n... events.layout.contents[1].setparameter('__record__','EventInfo')\r\n... events.layout.setparameter('__record__','Event')\r\n... ak.type(events)\r\n791 * Event[\"particles\": var * Particle[\"px\": float64, \"py\": float64], \"eventinfo\": EventInfo[\"nparticles\": float64]]\r\n```\r\n\r\nI can now give names to the records via \r\nwhich captures the nesting: \r\n\r\n* The Datatset is an array of events\r\n* The Event consists of EventInfo and an variable array of Particles\r\n* Particles consist of momentum components\r\n\r\nregistering the following dummy behaviors\r\n\r\n```python\r\nclass Particle(ak.Record):\r\n    pass\r\n\r\nclass ParticleArray(ak.Array):\r\n    pass\r\n\r\nclass Event(ak.Record):\r\n    pass\r\n\r\nclass EventArray(ak.Array):\r\n    pass\r\n\r\nclass EventInfo(ak.Record):\r\n    pass\r\n\r\nclass EventInfoArray(ak.Array):\r\n    pass\r\n\r\nak.behavior['Particle'] = Particle\r\nak.behavior['*','Particle'] = ParticleArray\r\n\r\nak.behavior['Event'] = Event\r\nak.behavior['*','Event'] = EventArray\r\n\r\nak.behavior['EventInfo'] = EventInfo\r\nak.behavior['*','EventInfo'] = EventInfoArray\r\n```\r\n\r\nI can get any slice into the array to instantiate\r\n\r\n```python\r\n>>> events[:].particles\r\n<ParticleArray [[{px: -0.315, py: -0.63, ... E: 9.35}]] type='791 * var * Partic...'>\r\n```\r\n\r\n\r\n```python\r\n>>> events[0]\r\n<Event ... nparticles: 9, weight: 0.0751}} type='Event[\"particles\": var * Partic...'>\r\n```\r\n\r\n**and** the trivial slice gives me an `EventArray`\r\n\r\n```python\r\n>>> events[:]\r\n<EventArray [{particles: [, ... weight: 1.29e-05}}] type='791 * Event[\"particles...'>\r\n```\r\n\r\n** but **  printing out `events` on its own doesn't pick up the toplevel behavior\r\n\r\n```python\r\n>>> events\r\n<Array [{particles: [, ... weight: 1.29e-05}}] type='791 * Event[\"particles\": va...'>\r\n```\r\n\r\nis there a way to \"re-instantiate\" the toplevel object so that it also becomes an `EventArray`?\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"lukasheinrich"
     },
     "body":"the solution seems to be to either do \r\n\r\n`ak.Array(events)` which should be zero-copy  or to `events[:]` as above",
     "createdAt":"2021-04-12T17:12:36Z",
     "number":601165,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"(I finally read this.) Both `ak.Array(events)` and `events[:]` are zero-copy.\r\n\r\nInterestingly enough, `events[:,]` is not zero-copy\u2014the zero-copyness is an implementation detail that is currently different between a pure range slice and a one-element tuple containing a range slice (unless I've already addressed that and just don't remember). So although `events[:]` is not going to be turned from a zero-copy operation to one that does a copy, `ak.Array(events)` is a cleaner way to express the intention.\r\n\r\n(Particularly since for Python lists, for which the copy/no-copy distinction is visible beyond performance, as Python lists are mutable, the syntax `some_list[:]` is _supposed to_ copy the list.)",
        "createdAt":"2021-04-12T17:58:36Z",
        "number":601339
       },
       {
        "author":{
         "login":"lukasheinrich"
        },
        "body":"thanks - I'll mark this as an answer!",
        "createdAt":"2021-04-12T18:50:34Z",
        "number":601566
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Oh, I read this, but I didn't read it carefully enough: don't use `setparameters` directly, as there's an ancient issue to remove it: #177. Use the high-level function [ak.with_parameter](https://awkward-array.readthedocs.io/en/latest/_auto/ak.with_parameter.html), which would give you a new object anyway.\r\n\r\nI had misread this, thinking that you changed the `ak.behavior` after creating the array, which is another way you can get into this situation.",
        "createdAt":"2021-04-12T19:08:07Z",
        "number":601661
       },
       {
        "author":{
         "login":"lukasheinrich"
        },
        "body":"how would I achieve the scenario above where I need to set multiple parameters at different nesting levels?",
        "createdAt":"2021-04-12T19:17:44Z",
        "number":601708
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"For something as complex as this, the easiest way would be to build the whole structure from [ak.layout.Content](https://awkward-array.readthedocs.io/en/latest/ak.layout.Content.html) subclass instances, which each have a `parameter` argument to their constructors (instead of building the thing and then having to dig down to find it again).\r\n\r\nAlso, you should be aware that [ak.from_iter](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_iter.html) is the slow path, especially if the data start as arrays (I suppose they don't in your case because you have to parse LHE files as text).\r\n\r\nI think you want to have 6 growable arrays (a class that allocates a NumPy array and a pointer to \"next entry,\" fills that array until it is full, then allocates and copies data to a new NumPy array that is 1.5\u00d7 larger):\r\n\r\n   * entry offsets: integers starting with 0 (one longer than the number of events), collecting a cumulative sum of the number of particles per event\r\n   * px, py, pz, E, all as flat (no event boundaries) floating-point arrays,\r\n   * weight as a floating-point array (no need for nparticles, as users can get that using [ak.num](https://awkward-array.readthedocs.io/en/latest/_auto/ak.num.html)).\r\n\r\nThen you can put these together as flat (no event boundaries) record arrays with the [ak.layout.RecordArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.RecordArray.html) constructor and break that into lists by wrapping it in an [ak.layout.ListOffsetArray64](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html). Adding the eventinfo is another [ak.layout.RecordArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.RecordArray.html) around that. By building it up in pieces, you'll have an opportunity to set the parameters at each stage.\r\n\r\nHaving said all of that, I just noticed that you can also use [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html), passing a `name` argument to the [begin_record](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html#ak-arraybuilder-begin-record) or [record](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html#ak-arraybuilder-record) method. That would certainly be simpler, though [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html) is the performance cost hidden inside [ak.from_iter](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_iter.html) (it has to iterate over Python objects\u2014that's the problem, which might not matter for you because you're coming from text files, anyway).",
        "createdAt":"2021-04-12T19:34:18Z",
        "number":601778
       },
       {
        "author":{
         "login":"lukasheinrich"
        },
        "body":"yeah the speed is probably limited by text IO/XML parsing  but this seems to work nicely\r\n\r\n```python\r\nbuilder = ak.ArrayBuilder()\r\nfor e in pylhe.readLHEWithAttributes(skhep_testdata.data_path('pylhe-testfile-pr29.lhe')):\r\n    with builder.record(name = 'Event'):\r\n        builder.field('eventinfo')\r\n        with builder.record(name =  'EventInfo'):\r\n            for fname in e.eventinfo.fieldnames:\r\n                builder.field(fname).real(getattr(e.eventinfo,fname))\r\n        builder.field('particles')\r\n        with builder.list():\r\n            for p in e.particles:\r\n                with builder.record(name = 'Particle'):\r\n                    builder.field('px').real(1.2)\r\narr = builder.snapshot()\r\narr\r\n```",
        "createdAt":"2021-04-12T20:10:23Z",
        "number":601935
       }
      ],
      "totalCount":6
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-04-12T13:30:12Z",
  "number":818,
  "title":"pick up behavior without slice after setting `parameters` manually",
  "url":"https://github.com/scikit-hep/awkward/discussions/818"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"Like 1.3.0rc3, but also fixes issue #819: PR #820.\r\n\r\nSee release notes for 1.3.0rc1, 1.3.0rc2, 1.3.0rc3. This release comes _after_ those.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.2.2'>1.2.2</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-04-12T21:34:18Z",
  "number":823,
  "title":"1.2.2",
  "url":"https://github.com/scikit-hep/awkward/discussions/823"
 },
 {
  "author":{
   "login":"claudiopascoal"
  },
  "body":"Hi,\r\nMy name is Cl\u00e1udio, from the LZ collaboration.\r\ni) I have two trees in which one is data and the second tree is Monte-Carlo.\r\nii) Then, from each tree I select only some branches and create a class 'awkward.highlevel.Array' in the following way:\r\n        arrays = tree1.arrays(variables)\r\niii) I would like to add Monte-Carlo branches to the arrays variables which contains data branches. Before, in uproot3, we could use:\r\n      arrays.update(tree2.arrays(mc_variables))\r\nbut this method is no longer available in uproot4.\r\nWhat is the solution for this?\r\n\r\nThanks",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Actually, this is an Awkward Array question, though I should start by saying that if you need to keep using Awkward 0/Uproot 3 for any reason, you can\r\n\r\n```bash\r\npip install awkward0 uproot3\r\n```\r\n\r\nand these packages can be imported into the same Python session with the new libraries.\r\n\r\nAs for updating an array, there is this syntax:\r\n\r\n```python\r\narrays[\"new_field_name\"] = new_field\r\n```\r\n\r\nand there's this, using [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html):\r\n\r\n```python\r\nnew_arrays = ak.zip({\"field1\": field1, \"field2\": field2, ...}, depth_limit=1)\r\n```\r\n\r\nHere's an example if you don't know or don't want to explicitly iterate over the fields using [ak.unzip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.unzip.html) and [ak.fields](https://awkward-array.readthedocs.io/en/latest/_auto/ak.fields.html):\r\n\r\n```python\r\n>>> a = ak.Array([{\"a\": 1}, {\"a\": 2}, {\"a\": 3}])\r\n>>> bc = ak.Array([{\"b\": 1, \"c\": 1}, {\"b\": 2, \"c\": 2}, {\"b\": 3, \"c\": 3}])\r\n>>> ak.zip(dict(zip(ak.fields(a), ak.unzip(a)), **dict(zip(ak.fields(bc), ak.unzip(bc)))))\r\n<Array [{a: 1, b: 1, c: 1}, ... b: 3, c: 3}] type='3 * {\"a\": int64, \"b\": int64, ...'>\r\n```\r\n\r\n(New versions of Python have fancy syntax for merging dicts, but I'm using old syntax that's guaranteed to work.)",
     "createdAt":"2021-04-13T18:47:04Z",
     "number":606294,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"claudiopascoal"
        },
        "body":"It works well, thanks!",
        "createdAt":"2021-04-23T16:59:20Z",
        "number":650921
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-04-13T18:37:15Z",
  "number":830,
  "title":"Merging arrays in uproot4",
  "url":"https://github.com/scikit-hep/awkward/discussions/830"
 },
 {
  "author":{
   "login":"ryanwclark1"
  },
  "body":"I will preface this by saying I am exploring the library and I'm not a current user and have only skimmed the documentation.  Does awkward have a straight forward approach for handling XML?  I found the from json/parquet/arrow functions and although I didn't see an explicit from_xml didn't know if someone with more experience with the library could point me in the right direction.  Thank you in advance.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"It currently doesn't, though XML is a natural way to express nestedness relationships. You haven't missed anything in the docs, though.\r\n\r\nYou might want to consider building Awkward Arrays from XML using [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html). That's how the JSON is implemented (though in C++, but Python should be fast enough if the source format is text).",
     "createdAt":"2021-04-18T21:27:07Z",
     "number":627613,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-04-18T15:10:08Z",
  "number":841,
  "title":"Use with XML?",
  "url":"https://github.com/scikit-hep/awkward/discussions/841"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"[ak.flatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.flatten.html) flattens one level of list depth by default:\r\n\r\n```python\r\n>>> array = ak.Array([[[1, 2, 3], []], [], [[4, 5]]])\r\n>>> ak.flatten(array)\r\n<Array [[1, 2, 3], [], [4, 5]] type='3 * var * int64'>\r\n>>> ak.flatten(ak.flatten(array))\r\n<Array [1, 2, 3, 4, 5] type='5 * int64'>\r\n```\r\n\r\nThis is what users of functional programming languages (LISPs, Scala, Spark) would expect:\r\n\r\n```scala\r\nscala> val array = List(List(List(1, 2, 3), List()), List(), List(List(4, 5)))\r\narray: List[List[List[Int]]] = List(List(List(1, 2, 3), List()), List(), List(List(4, 5)))\r\n\r\nscala> array.flatten\r\nres0: List[List[Int]] = List(List(1, 2, 3), List(), List(4, 5))\r\n\r\nscala> array.flatten.flatten\r\nres1: List[Int] = List(1, 2, 3, 4, 5)\r\n```\r\n\r\nbut it has come up more than once as a surprise. Users in our community expect `ak.flatten` to \"completely flatten,\" which is an option, but not the default one:\r\n\r\n```python\r\n>>> ak.flatten(array, axis=None)\r\n<Array [1, 2, 3, 4, 5] type='5 * int64'>\r\n```\r\n\r\nIn part, this may be because other dimension-reducing operations have `axis=None` as a default, such as the reducers:\r\n\r\n```python\r\n>>> ak.sum(array)   # no argument, assumes you want to completely sum\r\n15\r\n>>> ak.sum(array, axis=0)\r\n<Array [[5, 7, 3], []] type='2 * var * int64'>\r\n>>> ak.sum(array, axis=1)\r\n<Array [[1, 2, 3], [], [4, 5]] type='3 * var * int64'>\r\n>>> ak.sum(array, axis=2)\r\n<Array [[6, 0], [], [9]] type='3 * var * int64'>\r\n```\r\n\r\nActually, I would have had the default axis for reducers be `axis=-1`, but NumPy forced the default to be `axis=None`. (I don't see `ak.flatten` as being \"similar to\" `ak.sum` et al.)\r\n\r\nIf there's a strong consensus\u2014i.e. a lot of people \"+1\"ing this or otherwise chiming in\u2014then we can change the default. That will be rough, since the name \"`ak.flatten`\" is a good one that I don't want to change and a switch in default without renaming the function is going to break somebody's code, possibly in subtle ways (i.e. wrong answer, rather than an error message like \"this function doesn't exist anymore, use the new name with the new default\").\r\n\r\nHere's how it could be done, if there's a groundswell of support for it: I can immediately make the default argument a dummy object that, if detected, will raise a warning saying, \"`ak.flatten default axis is changing from 1 to None in version 1.4.0 (2021-08-01); please specify an explicit axis for now\" and then change the default at that time. This is referring to the [scheduled semi-major releases](https://github.com/scikit-hep/awkward-1.0#roadmap) in which breaking changes are allowed. Anyone who upgrades Awkward Array between now and August will get the message and adjust. Anyone who doesn't\u2014people who have already upgraded to Awkward 1.x but do not upgrade again in this 3.5 month window\u2014won't see the message and risk getting an error due to a default behavior changing under them.\r\n\r\nAs an alternative, we could leave `ak.flatten` as it is and introduce `ak.ravel` (a NEP 18 overload of [np.ravel](https://numpy.org/doc/stable/reference/generated/numpy.ravel.html)), which is just `ak.flatten` with `axis=None`. That doesn't help people who are incorrectly guessing that `ak.flatten` completely flattens\u2014it's a different function name to remember\u2014but it seems appropriate to me because \"`np.ravel`\" is the NumPy word for \"complete flatten,\" and it doesn't conflict with the usage of \"flatten\" in functional programming.\r\n\r\nSo, let me know what you think! This thread was prompted by #832, and @masonproffitt and @agoose77, if you want to point others to this to avoid letting this thread die in obscurity, please do so.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Hey @jpivarski, this is a great summary of where things stand.\r\n\r\nI think that it would be worth overloading `np.ravel` for Awkward arrays regardless of what the equivalent Awkward function is actually called. As to whether to do this with a new Awkward function or not, I suppose is a matter or taste. I would be tempted to introduce an `ak.ravel` function in order to keep API parity with NumPy. \r\n\r\nConcerning `ak.flatten`, as a non-fp user, the name is non-intuitive; it's not \"flattening\" the array, rather it is expanding/dropping one (preferred) dimension. However, I can see that Scala does use the axis=1 convention, and therefore if this name is \"standard\", then it would be best to keep `axis=1`. I would go as far as to say that if `np.ravel` has a similarly named Awkward counterpart, then most users will reach for *that* rather than `flatten`, and the point becomes moot.",
     "createdAt":"2021-04-20T09:01:14Z",
     "number":634065,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"masonproffitt"
        },
        "body":"Interestingly, I would argue that `ravel` is the non-intuitive name. To me, it sounds like the exact opposite behavior (\"tangling things up\"). Removing a dimension is exactly what flattening is: I think of flattening a 3D box to a rectangle or flattening a rectangle to a line. Even in the [documentation for `np.ravel`](https://numpy.org/doc/stable/reference/generated/numpy.ravel.html), the behavior of the function is explained as returning a \"flattened\" array.\r\n\r\n[Numpy's `flatten`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.flatten.html) uses the `axis=None` behavior (though without an actual `axis` parameter) which was the primary reason I suggested this change for `ak.flatten`. I had actually never heard of `np.ravel` before that issue that I made (#832). I've always used `flatten` in both numpy and awkward, so I would not be so sure that users would go to `ravel` instead of `flatten`. Personally I would expect the opposite, especially since `awkward0.JaggedArray` has a `flatten`, although there the default is `axis=0` but the meaning is different such that it corresponds to `ak.flatten`'s `axis=1`.",
        "createdAt":"2021-04-20T10:09:28Z",
        "number":634375
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Ah, well in terms of etymology, ravel is entirely unintuitive, but here we are :face_with_head_bandage: \r\n\r\nAs `flatten` is an `ndarray` method rather than top-level function, I think it is likely used less frequently (though I have no data to support this claim). __I suppose this raises the question of whether Awkward should be NumPy-like, or classical FP-like.__ I have been using Awkward from the vantage point of a NumPy user, so that guides my opinion on what is \"intuitive\" for \"new\" users.  \r\n\r\nTo fully align with NumPy conventions, I would probably want to make `flatten` and `ravel` aliases of the same function implementing the existing `flatten(axis=None)` behaviour, and introduce a new function e.g. `expand`, `drop`, `simplify`, `relax`. These are _terrible_ names; the point here is the motivation for doing it :)\r\n",
        "createdAt":"2021-04-20T10:16:04Z",
        "number":634421
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-04-19T19:44:07Z",
  "number":845,
  "title":"Changing the default ak.flatten axis from axis=1 to axis=None: any complaints?",
  "url":"https://github.com/scikit-hep/awkward/discussions/845"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"Same as [1.3.0rc4](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.3.0rc4). Deploying as a full release because issue #859 was encountered twice (before as #836), so it's a common problem.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.2.3'>1.2.3</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-05-10T14:58:47Z",
  "number":861,
  "title":"1.2.3",
  "url":"https://github.com/scikit-hep/awkward/discussions/861"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@ianna fixed class names in the documentation (PR #860).\r\n\r\n@agoose77 fixed `ak.is_none` (PR #864).\r\n\r\n@drahnreb made Parquet lazy-loading only build Forms for requested columns (PR #867).\r\n\r\n@agoose77 and @drahnreb together made idioms like `x[\"z\"] = None` work correctly by assigning a column of Nones (PR #880).\r\n\r\n@jpivarski fixed some union-simplification issues in getitem-field (PR #870). Also fixed various bugs to enable Numba JIT-compiled functions to iterate over VirtualArrays created by lazy-reading Parquet (PR #871). Fixed a relic of the `ptr().get()` \u2192 `data()` transition for kernel interfaces (PR #877). Fixed some null-type issues with Arrow/Parquet columns (PR #878). And fixed matrix multiplication of a vector as an Awkward record (PR #868).\r\n\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.3.0'>1.3.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-06-01T17:15:23Z",
  "number":888,
  "title":"1.3.0",
  "url":"https://github.com/scikit-hep/awkward/discussions/888"
 },
 {
  "author":{
   "login":"agoose77"
  },
  "body":"Hi @jpivarski, I thought I'd place this here for a wider discussion.\r\n\r\nSomething I am encountering quite a lot at the moment is the need to decide upon (and require) a particular array structure for a routine. With NumPy, I'd often be able to assume that an operation will vectorize over the n-th dimension, and leading dimensions just modify the output structure. There doesn't seem to be a way to do this in Awkward unless the operation is a ufunc. \r\n\r\nIn my particular case, I have a lookup function that maps from a record to another record, which I want to perform at the innermost dimension. Where the RecordArray has only one dimension, this is simply\r\n\r\n```python3\r\nLOOKUP_TABLE = ak.from_numpy(np.random.uniform(size=(10, 10, 10, 3)))\r\n\r\ndef transform(addr, highlevel=True):\r\n    transformed = LOOKUP_TABLE[\r\n        addr[\"x\"],\r\n        addr[\"y\"],\r\n        addr[\"z\"],\r\n    ]\r\n    return ak.zip(\r\n        {\r\n            \"u\": transformed[:, 0],\r\n            \"v\": transformed[:, 1],\r\n            \"w\": transformed[:, 2],\r\n        },\r\n        highlevel=highlevel,\r\n    )\r\n```\r\n\r\nWith additional structure, e.g. several of these addresses per-event (in uproot terminology), this mapping no longer works, because jagged indices are treated differently to regular ones. \r\n\r\n```python3\r\ndata  = ak.zip({\r\n    'x': [[0,1,2], [3], [4,5,6,7]],\r\n    'y': [[8,9,0], [8], [7,8,5,3]],\r\n    'z': [[5,4,3], [9], [1,1,2,5]],\r\n}, with_name=\"coord\")\r\n```\r\n```pycon\r\n>>> data.type\r\n3 * var * {\"x\": int64, \"y\": int64, \"z\": int64}\r\n>>> transform(data)\r\n```\r\n```pytb\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-682-4ad728482488> in <module>\r\n----> 1 transform(data)\r\n\r\n<ipython-input-678-5aa8c791c389> in transform(addr, highlevel)\r\n      1 def transform(addr, highlevel=True):\r\n----> 2     transformed = LOOKUP_TABLE[\r\n      3         addr[\"x\"],\r\n      4         addr[\"y\"],\r\n      5         addr[\"z\"],\r\n\r\n/opt/texat-venv/lib/python3.9/site-packages/awkward/highlevel.py in __getitem__(self, where)\r\n    972         have the same dimension as the array being indexed.\r\n    973         \"\"\"\r\n--> 974         return ak._util.wrap(self.layout[where], self._behavior)\r\n    975 \r\n    976     def __setitem__(self, where, what):\r\n\r\nValueError: cannot fit jagged slice with length 3 into RegularArray of size 10\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.2.2/src/libawkward/array/RegularArray.cpp#L1537)\r\n```\r\n\r\n## Flattening the Array\r\nBecause this operation does not care about the structure at all, we can completely flatten the address, and then completely unflatten the result. One way to do this is to actually invoke `ak.flatten`:\r\n```python3\r\ndef apply_flat(array, func):\r\n    sizes = []\r\n\r\n    # Flatten array\r\n    flattened = array\r\n    for i in range(array.ndim - 1):\r\n        sizes.append(ak.num(flattened))\r\n        flattened = ak.flatten(flattened)\r\n        \r\n    # Compute result on flattened array\r\n    result = func(flattened)\r\n    \r\n    # Unflatten the result\r\n    for s in reversed(sizes):\r\n        result = ak.unflatten(result, s)\r\n        \r\n    return result\r\n```\r\n\r\nThis approach is not elegant, and may involve several copies of the data (due to `flatten`). \r\n\r\n## Operating on the Contents\r\nAn alternative solution is to operate directly on the underlying buffers (contents). In this example, we can require that any array must have records with scalar fields, such that the `RecordArray` fields are flat buffers. Then, it looks something like\r\n```python3\r\ndef apply_to_record(array, record, func):\r\n    def getfunction(layout, depth):\r\n        if not isinstance(layout, ak.layout.RecordArray):\r\n            return\r\n\r\n        if layout.parameter(\"__record__\") != record:\r\n            return\r\n\r\n        return lambda: func(layout)\r\n\r\n    content = ak._util.recursively_apply(\r\n        ak.operations.convert.to_layout(array), getfunction\r\n    )\r\n    return ak._util.wrap(content, ak._util.behaviorof(array))\r\n```\r\nwhere we call the transform upon the layout which has the appropriate record name. This works by reusing the parent/outer layouts, but replacing the underlying record itself.\r\n```pycon\r\n>>> apply_to_record(data, \"coord\", lambda x: transform(x, highlevel=False))\r\n<Array [[{u: 0.227, v: 0.0513, ... w: 0.689}]] type='3 * var * {\"u\": float64, \"v...'>\r\n```\r\nIt seems like this is done [in several places in coffea](https://github.com/CoffeaTeam/coffea/blob/2baae94028c38b59f0eb52127d8fb92840dbf23d/coffea/jetmet_tools/CorrectedJetsFactory.py).\r\nThe drawback is that the (private) `recursive_apply` function requires us to operate on the full contents of the buffers, even if only one element is every actually indexed onto. \r\n\r\nThere might be a nicer way to do this, I'd appreciate any discussion on the topic!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Maybe the appropriate jagged array to use as a slice could be built with [ak.local_index](https://awkward-array.readthedocs.io/en/latest/_auto/ak.local_index.html)?",
     "createdAt":"2021-06-08T15:25:52Z",
     "number":840658,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I don't *think* so - I want NumPy-like advanced indexing, but with a jagged structure. I don't believe that we currently provide any mechanism to request that, so one either has to drop the jagged requirement, or find an alternative like one of the three suggestions here?",
        "createdAt":"2021-06-09T07:37:19Z",
        "number":843991
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Here's another approach (you mentioned Numba in Gitter). You can write a Numba function that doesn't care how many levels deep an array is, as long as it can resolve that depth during compilation. I just got help on this on the [numba/numba-dev Gitter](https://gitter.im/numba/numba-dev) (thanks, @seibert!), since my attempt to do it using [nb.generated_jit](https://numba.pydata.org/numba-doc/dev/user/generated-jit.html) ran into an unimplemented feature. The reason for using `nb.generated_jit` instead of `nb.jit` is that you get to write type-dependent Python code that only runs at compile-time\u2014basically, like template metaprogramming in C++, but with a much easier language to do that in (Python).\r\n\r\nThe `nb.generated_jit` approach raises NotImplementedError when you construct a recursion on types, but [nb.extending.overload](http://numba.pydata.org/numba-doc/latest/extending/overloading-guide.html) does nearly the same thing (lets you write type-dependent Python code at compile-time) without running into the unimplemented case. (This may be a situation in which `nb.generated_jit`, an older function in Numba, needs to be updated to work as well as `nb.extending.overload`.) The drawback of `nb.extending.overload` is that it has to wrap an existing function: its purpose is to overload functions, such as SciPy routines. So with that said, here's how to write a function that adds one to an Awkward Array in place, for any list-depth:\r\n\r\n```python\r\n>>> import numba as nb\r\n>>> import numpy as np\r\n>>> import awkward as ak\r\n>>> \r\n>>> def dummy(x):\r\n...     pass\r\n... \r\n>>> @nb.extending.overload(dummy)\r\n... def _add_one(data):\r\n...     if data.type.ndim == 1:\r\n...         print(f\"Typing THEN branch: ndim={data.type.ndim}\")\r\n...         def impl(data):\r\n...             tmp = np.asarray(data)   # zero-copy wrap as NumPy, so that it can be changed in-place\r\n...             for i in range(len(tmp)):\r\n...                 tmp[i] += 1\r\n...         return impl\r\n...     else:\r\n...         print(f\"Typing ELSE branch: ndim={data.type.ndim}\")\r\n...         def impl(data):\r\n...             for x in data:\r\n...                 dummy(x)\r\n...         return impl\r\n... \r\n>>> @nb.njit\r\n... def add_one(data):\r\n...     print(\"Actually running\")\r\n...     return dummy(data)\r\n... \r\n>>> array = ak.Array([[[[[1], []]], [[], [[2, 3]]]], []])\r\n>>> add_one(array)\r\nTyping ELSE branch: ndim=5\r\nTyping ELSE branch: ndim=4\r\nTyping ELSE branch: ndim=3\r\nTyping ELSE branch: ndim=2\r\nTyping THEN branch: ndim=1\r\nActually running\r\n>>> array\r\n<Array [[[[[2], []]], [[], [[3, 4]]]], []] type='2 * var * var * var * var * int64'>\r\n```\r\n\r\nThe `dummy` function is because we have to overload something. You can see the function recursing in the typing pass, before anything actually runs. The resulting compiled code is not recursive: it is specialized for the exact data types of the array (i.e. all levels could be inlined by the compiler, in principle).\r\n\r\nNormally, we don't update Awkward Arrays in place because it's hard to know which arrays share buffers: an update might have long-distance effects. So here's an example that doesn't change the array in-place. Unfortunately, [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html) is dynamically typed (even in Numba) and it will slow down the process. (TypedArrayBuilder should fix this, once it's implemented as a Numba extension.)\r\n\r\n```python\r\n>>> def dummy(data, builder):\r\n...     pass\r\n... \r\n>>> @nb.extending.overload(dummy)\r\n... def _add_one(data, builder):\r\n...     if data.type.ndim == 1:\r\n...         def impl(data, builder):\r\n...             for x in data:\r\n...                 builder.integer(x + 1)\r\n...             return builder\r\n...         return impl\r\n...     else:\r\n...         def impl(data, builder):\r\n...             for x in data:\r\n...                 builder.begin_list()\r\n...                 dummy(x, builder)\r\n...                 builder.end_list()\r\n...             return builder\r\n...         return impl\r\n... \r\n>>> @nb.njit\r\n... def add_one(data, builder):\r\n...     return dummy(data, builder)\r\n... \r\n>>> builder = add_one(\r\n...     ak.Array([[[[[1], []]], [[], [[2, 3]]]], []]),\r\n...     ak.ArrayBuilder(),\r\n... )\r\n>>> builder.snapshot()\r\n<Array [[[[[2], []]], [[], [[3, 4]]]], []] type='2 * var * var * var * var * int64'>\r\n```\r\n",
     "createdAt":"2021-06-08T19:32:38Z",
     "number":841889,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I even used the overload feature the other day, and completely failed to make the connection that it would side step the problem with recursion type resolution. Thanks for sharing this, looking forward to trying it out! ",
        "createdAt":"2021-06-08T19:40:25Z",
        "number":841916
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"So, I've concluded that this approach is probably the *best*. Once the `TypedArrayBuilder` arrives in `main`, the performance cost of using `numba` will hopefully disappear.  Here's a fun way to use Jim's code, with a decorator; minimising the boilerplate for multiple jitted functions:\r\n\r\n```python3\r\ndef njit_at_dim(dim=1):\r\n    def wrapper(impl_dim):\r\n        def token(data, builder):\r\n            pass\r\n\r\n        def impl_nd(data, builder):\r\n            for inner in data:\r\n                builder.begin_list()\r\n                token(inner, builder)\r\n                builder.end_list()\r\n            return builder\r\n\r\n        @nb.extending.overload(token)\r\n        def dispatch(data, builder):\r\n            if data.type.ndim == dim:\r\n                return impl_dim\r\n            else:\r\n                return impl_nd\r\n\r\n        @nb.njit\r\n        def jitted(data, builder):\r\n            return token(data, builder)\r\n\r\n        return jitted\r\n    return wrapper\r\n```\r\nOne can then decorate the function that should act on the given dimension, passing in the appropriate dimension to the decorator, or omitting it to use the default (`dim=1`):\r\n```python3\r\n@njit_at_dim()\r\ndef add_one(data, builder):\r\n    for x in data:\r\n        builder.integer(x + 1)\r\n    return builder\r\n```\r\n\r\n```python3\r\n>>> data = ak.Array([\r\n...     [\r\n...         [1, 2, 3],\r\n...         [4, 5]\r\n...     ],\r\n...     [\r\n...         [6, 7],\r\n...         [8]\r\n...     ]\r\n... ])\r\n>>> add_one(data, ak.ArrayBuilder()).snapshot()\r\n<Array [[2, 3, 3, 3, 3, 3, ... 1, 1, 1, 1, 1]] type='1138 * var * int64'>\r\n```\r\n\r\nOne could go further with this if the function weren't required to be callable by other jitted functions; the builder `snapshot` could be called automatically upon exit, and instantiated such that the user wouldn't need to pass it in as an argument.",
        "createdAt":"2021-06-09T08:46:47Z",
        "number":844323
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Thinking about TypedArrayBuilder got me thinking, \"If you want to maintain _the same_ structure, you don't need to build it from scratch: you can reuse the `offsets` and such from the input array,\" which got me thinking, \"But that would just be a ufunc, and we have the infrastructure to handle that in ufuncs already,\" which got me thinking that I should provide an example of using Numba to make a ufunc.\r\n\r\nSo, this is a special case (you want to maintain structure), but it's much simpler to write and doesn't involve the slow-down related to ArrayBuilder.\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import numba as nb\r\n>>> @nb.vectorize([nb.int64(nb.int64), nb.float64(nb.float64)])\r\n... def add_one(x):\r\n...     return x + 1\r\n... \r\n>>> add_one(ak.Array([[[1, 2], []], [], [[3]]]))\r\n<Array [[[2, 3], []], [], [[4]]] type='3 * var * var * int64'>\r\n>>> add_one(ak.Array([[[1.1, 2.2], []], [], [[3.3]]]))\r\n<Array [[[2.1, 3.2], []], [], [[4.3]]] type='3 * var * var * float64'>\r\n```\r\n\r\nOne caveat: unlike normal Numba, you have to specify all of the signatures you'll want this new ufunc, `add_one`, to work for, because otherwise it won't be recognized as a ufunc and Awkward Array's `__array_ufunc__` method won't get called. Those are in the `nb.vectorize` decorator (`nb.int64(nb.int64)` means \"expect one `int64` in, return an `int64`\"; ufuncs can have multiple arguments, like `nb.float64(nb.float64, nb.float64)`).",
        "createdAt":"2021-06-09T13:34:11Z",
        "number":845717
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"This is probably a great place to put it! The only caveat is that for arrays which have been shortened by indexing (mainly record arrays), the underlying memory is a lot larger than what is actually required. ",
        "createdAt":"2021-06-09T17:59:41Z",
        "number":847080
       }
      ],
      "totalCount":4
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-06-08T13:08:08Z",
  "number":902,
  "title":"Best way to operate upon innermost dimension",
  "url":"https://github.com/scikit-hep/awkward/discussions/902"
 },
 {
  "author":{
   "login":"agoose77"
  },
  "body":"# Context\r\nIn #917 a new `axis` parameter is added to constrain the scope of `fill_none` to a particular layout depth. This should make the function more predictable. `axis` may be set to `None`, in which case `fill_none` will recurse to all axes.\r\n\r\nHowever, as this new behaviour is breaking, it must be opted into explicitly, i.e. with `ak.fill_none(array, axis=...)`. Once the deprecation period elapses, we will need to choose a default axis. For context, the *current* behaviour without an axis is to find the *first option axis*, starting from 0, and remove the option type. \r\n\r\n# New behaviour\r\nFor an example of the new behaviour, consider the following layout:\r\n```python\r\nlayout = (\r\n    # current axis 0\r\n    ak.layout.RegularArray(\r\n        # current axis 1\r\n        ak.layout.IndexedOptionArray64(\r\n            ak.layout.Index64([0, 1, -1, 2, 3]),\r\n            ak.layout.ListOffsetArray64(\r\n                # current axis 2\r\n                ak.layout.Index64([0, 3, 6, 9, 12]),\r\n                ak.layout.IndexedOptionArray64(\r\n                    ak.layout.Index64([0, 1, 2, 3, 4, 5, -1, 7, 8, 9, 10, 11]),\r\n                    ak.layout.NumpyArray(np.arange(12)),\r\n                ),\r\n            ),\r\n        ),\r\n        2,\r\n    )\r\n)\r\n```\r\nThis layout has the corresponding list representation:\r\n```python\r\n[\r\n    [\r\n        [0, 1, 2],\r\n        [3, 4, 5],\r\n    ],\r\n    [\r\n        None,\r\n        [None, 7, 8],\r\n    ],\r\n]\r\n```\r\n\r\nIf we call `fill_none` with `axis=0`, then it will try to remove any `None`s in the zeroth axis:\r\n```python3\r\n>>> ak.fill_none(layout, 1, axis=0).tolist()\r\n... [\r\n...     [\r\n...         [0, 1, 2],\r\n...         [3, 4, 5],\r\n...     ],\r\n...     [\r\n...         None,\r\n...         [None, 7, 8],\r\n...     ],\r\n... ]\r\n```\r\nThis array does not have any `None`s in the zeroth axis (because a bare `RegularArray` produces sublists of fixed size), and so it returns the same layout.\r\n\r\nNow, if we call `fill_none` with `axis=1`, it will remove the `None`s in the first axis:\r\n```python3\r\n>>> ak.fill_none(layout, 1, axis=1).tolist()\r\n... [\r\n...     [\r\n...         [0, 1, 2],\r\n...         [3, 4, 5],\r\n...     ],\r\n...     [\r\n...         1,\r\n...         [None, 7, 8],\r\n...     ],\r\n... ]\r\n```\r\nThis *does* change the layout, because of the  `IndexedOptionArray64` that the `RegularyArray` builds the sublists from.\r\n\r\nFinally, we have the `axis=None` behaviour. Clearly, all levels of `None` have been removed.\r\n```python3\r\n>>> ak.fill_none(layout, 1, axis=None).tolist()\r\n... [\r\n...     [\r\n...         [0, 1, 2],\r\n...         [3, 4, 5],\r\n...     ],\r\n...     [\r\n...         1,\r\n...         [1, 7, 8],\r\n...     ],\r\n... ]\r\n```\r\n\r\n# Default `axis`?\r\nSo, the question remains, what should `ak.fill_none(layout, <value>)` do? There are some options:\r\n1. Let `axis=0` for symmetry with `ak.is_none`\r\n1. Let `axis=1` for symmetry with `ak.pad_none` (and some other existing functions)\r\n2. Let `axis=-1` for \"the most likely\" scenario that users are filling with scalar values\r\n\r\nThis Discussion is a welcome place for users to provide their opinions on what the new defaults should be. [As @jpivarski says](https://github.com/scikit-hep/awkward-1.0/pull/917#issuecomment-861031080), now is the time to decide it whilst it's not depended upon!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I don't think `axis=1` is a good choice. That's the default for `ak.pad_none` because that function rarely makes sense at `axis=0`, unlike `ak.fill_none`.\r\n\r\nFor me, I see the choice as being between `axis=0` (because I feel like defaults should have minimal scope, with non-defaults expanding outward) and `axis=None` (because a literal reading of \"fill with Nones\" sounds like you're going to do it everywhere, on all levels).\r\n\r\n@lgray, @nsmith-, and @raymondEhlers, you have all been involved in issues that mentioned `ak.fill_none` in the past. Do you have opinions on the default `axis`?",
     "createdAt":"2021-06-15T18:26:44Z",
     "number":874591,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"nsmith-"
        },
        "body":"My only concern with `axis` values other than `-1` is the proliferation of union arrays. Along those lines, I'm surprised that https://awkward-array.readthedocs.io/en/latest/_auto/ak.singletons.html did not get an `axis` parameter before `fill_none`, since I assume for the most part `fill_none` and `pad_none` is being used to massage an array towards being rectilinear. I guess once `fill_none` is in place, broadcasting can take care of some of the work, e.g.\r\n```python\r\n>>> ak.Array([[1, 2, 3], None, [4, 5]]) * ak.Array([[1, 2, 3], [4, 5], [6, 7]])\r\n<Array [[1, 4, 9], None, [24, 35]] type='3 * option[var * int64]'>\r\n>>> ak.Array([[1, 2, 3], 1, [4, 5]]) * ak.Array([[1, 2, 3], [4, 5], [6, 7]])\r\n<Array [[1, 4, 9], [4, 5], [24, 35]] type='3 * var * int64'>\r\n```",
        "createdAt":"2021-06-15T19:40:02Z",
        "number":875039
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Yes, the existing version of `fill_none` will introduce a number of `Union`s. I suppose this is really where careful usage of `axis` is always better than blindly calling `fill_none` with no axis.",
        "createdAt":"2021-06-15T19:59:14Z",
        "number":875098
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"If any choice of `axis` is less likely to lead to new UnionArrays, then that choice is better. UnionArrays cause troubles down the road.\r\n\r\nI see now what you mean, @nsmith-, about `axis=-1` being the least likely to lead to UnionArrays, because a user is most likely to pass a number as the fill value and maybe not notice that some lists are missing as well. I hadn't considered `axis=-1`, but maybe we should be considering this function as a thing like [ak.values_astype](https://awkward-array.readthedocs.io/en/latest/_auto/ak.values_astype.html) (which _only_ operates at `axis=-1`; `ak.fill_none` could _default_ to that but have more general options).\r\n\r\nThe point about UnionArrays makes me more negative about `axis=None`. By definition, there isn't any fill value that would make multilple levels non-union.",
        "createdAt":"2021-06-15T20:15:18Z",
        "number":875140
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"@jpivarski @nsmith this is something that I was just thinking about. \r\n\r\nI don't think the argument against `axis != -1` holds though; I do agree that filling on those axes will produce `UnionArrays`, but only if the user passes a scalar value. The issue is not which axis is being used, rather whether the user is using `fill_axis` propertly; `fill_none` with scalars is effectively a shorthand for union for outer axes. Despite this paragraph, I also agree that `-1` is *probably* a better option because I suspect scalar fill values are more common (and therefore, `axis = -1`) :upside_down_face: \r\n\r\nThis now makes me strongly want to drop `axis=None`. Sometimes it's quite easy to lose sight of the problem that you're *actually* trying to solve. I don't think `axis=None` has any *real* value at all. ",
        "createdAt":"2021-06-15T20:30:09Z",
        "number":875175
       },
       {
        "author":{
         "login":"raymondEhlers"
        },
        "body":"Just to add my experience: grepping around my repos, I see that I almost always use `fill_none` with `pad_none` to make arrays rectilinear, filling in some scalar value. In the other cases, I was trying to get rid of all option types (which is to say, I was expecting `axis=None`). But in practice, I think `axis=-1` would have been enough to get what I wanted (plus, this unique case is only due to the option types that used to occur when writing parquet files).\r\n\r\nStepping back, this discussion made me realize that I wasn't really aware of the details of the current behavior until reading about them here. Naively, I had expected it to be `axis=None`. However, given the discussion above about unions and that `axis=-1` would have practically fulfilled my use cases, `axis=-1` seems reasonable to me",
        "createdAt":"2021-06-15T20:36:21Z",
        "number":875199
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I agree: the main use is to make arrays rectilinear (and non-option-type). `axis=None` can't do that: you'd have to call `ak.fill_none` with different fill values for each level if you had missing values at multiple levels.\r\n\r\nOn the other hand, I don't think it's bad that `axis=None` was implemented. Union arrays are valid arrays, and sometimes someone would want one (and deal with the fact that not everything works for them). But it's clearer that it should not be the default.\r\n\r\nMost users probably have missing values at only one level, which is why the weird behavior the function used to have worked. Using `axis=-1` means you usually don't have to think about the type of the fill value: it would almost always be a number (some sort of leaf type). Having less to think about is a big plus.\r\n\r\nAll of this discussion has been great\u2014I don't think either of us were considering `axis=-1` before, but this is looking more like the right default. Without having this conversation, we very likely would have picked a bad default.\r\n\r\nDoes anyone here have objections to `axis=-1`?",
        "createdAt":"2021-06-15T20:55:55Z",
        "number":875287
       }
      ],
      "totalCount":6
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-06-15T08:57:04Z",
  "number":920,
  "title":"Choosing the default axis for `fill_none`",
  "url":"https://github.com/scikit-hep/awkward/discussions/920"
 },
 {
  "author":{
   "login":"nikoladze"
  },
  "body":"I'd like to read a nested array lazily from parquet files and attach behavior to some of the record fields. For example if i would have\r\n\r\n```python\r\narray = ak.zip(\r\n    {\r\n        \"A\" : ak.zip(\r\n            {\r\n                \"x\" : [[1, 2], [], [3, 4, 5]],\r\n                \"y\" : [[10, 20], [], [30, 40, 50]]\r\n            }\r\n        ),\r\n        \"B\" : ak.zip(\r\n            {\r\n                \"x\": [[1, 2, 3, 4], [5], [6]],\r\n                \"y\": [[10, 20, 30, 40], [50], [60]]\r\n            }\r\n        )\r\n    }, depth_limit=1\r\n)\r\n\r\nak.to_parquet(array, \"array.parquet\")\r\n```\r\n\r\ni want to load this with\r\n\r\n```python\r\nlazy = ak.from_parquet(\"array.parquet\", lazy=True)\r\n```\r\n\r\nNow, for example if i want to set the `__record__` parameter of the \"A\" record i could do something like\r\n\r\n```python\r\nlazy[\"A\"] = ak.with_name(lazy.A, \"AClass\")\r\n```\r\n\r\nbut that will materialize the virtual array under \"A\". Skimming over the code for `ak.from_parquet` it seems manually reconstructing that VirtualArray will be quite involved.\r\n\r\nIs there a straightforward solution for this?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"## Setting the top-level `__record__`\r\nThe reason that this happens is that after setting the `__record__` parameter, `with_name` re-visits the entire layout and simplifies any union types. This is useful, because it removes redundant unions, but has the side-effect of materialising the virtual array.\r\n\r\nI wonder @jpivarski if it is sensible to implement a flag for `recursively_apply` to prevent it from recursing into virtual layouts. In this particular instance, if a virtual layout has not been materialised by the first `getfunction` then we shouldn't either. Of course, we don't need a flag for this; the second `getfunction` can add a case to handle it.\r\n\r\nIn the mean-time, unless Jim has a better solution, you can implement a non-simplifying variant of `with_name`. You can also just modify the layout directly, but this is cleaner!\r\n```python3\r\ndef with_name_lazy(array, name, highlevel=True, behavior=None):\r\n    def getfunction(layout):\r\n        if isinstance(layout, ak.layout.RecordArray):\r\n            parameters = dict(layout.parameters)\r\n            parameters[\"__record__\"] = name\r\n            return lambda: ak.layout.RecordArray(\r\n                layout.contents,\r\n                layout.recordlookup,\r\n                len(layout),\r\n                layout.identities,\r\n                parameters,\r\n            )\r\n        else:\r\n            return None\r\n\r\n    out = ak._util.recursively_apply(\r\n        ak.operations.convert.to_layout(array), getfunction, pass_depth=False\r\n    )\r\n    if highlevel:\r\n        return ak._util.wrap(out, ak._util.behaviorof(array, behavior=behavior))\r\n    else:\r\n        return out\r\n```\r\n\r\nNow, we should be able to detect whether a virtual array has been generated with `layout.peek_cache`. However, this doesn't seem to return anything for the arrays lazily loaded by `from_parquet`. I think this is because of how the caching is setup - I'm not an expert here yet! If you wrap the result with your own cache, e.g.\r\n```python3\r\ncache = {}\r\ncached = ak.with_cache(lazy, cache)\r\n```\r\nThen you can create a simplifying variant of the `with_name` function that does not materialise virtual arrays:\r\n```python3\r\ndef with_name_lazy(array, name, highlevel=True, behavior=None):\r\n    def getfunction(layout):\r\n        if isinstance(layout, ak.layout.RecordArray):\r\n            parameters = dict(layout.parameters)\r\n            parameters[\"__record__\"] = name\r\n            return lambda: ak.layout.RecordArray(\r\n                layout.contents,\r\n                layout.recordlookup,\r\n                len(layout),\r\n                layout.identities,\r\n                parameters,\r\n            )\r\n        else:\r\n            return None\r\n\r\n    out = ak._util.recursively_apply(\r\n        ak.operations.convert.to_layout(array), getfunction, pass_depth=False\r\n    )\r\n\r\n    def getfunction2(layout):\r\n        if isinstance(layout, ak.layout.VirtualArray) and layout.peek_cache is None:\r\n            return None\r\n\r\n        if isinstance(layout, ak._util.uniontypes):\r\n            return lambda: layout.simplify(merge=True, mergebool=False)\r\n        else:\r\n            return None\r\n\r\n    out2 = ak._util.recursively_apply(out, getfunction2, pass_depth=False)\r\n\r\n    if highlevel:\r\n        return ak._util.wrap(out2, ak._util.behaviorof(array, behavior=behavior))\r\n    else:\r\n        return out2\r\n```",
     "createdAt":"2021-06-17T08:46:24Z",
     "number":882137,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"nikoladze"
     },
     "body":"Thanks for the quick answer! I should have mentioned that i already tried the \"non-simplifying\" variant with only the first call of `recursively_apply`. That's already better, but the part that descends down until the `RecordArray` still materializes the offsets. I'm checking the cache with `lazy._caches`:\r\n\r\n```python\r\nlazy = ak.from_parquet(\"array.parquet\", lazy=True)\r\nnamed_lazy_A = with_name_lazy(lazy.A, \"AClass\") # using the function that does not have the simplify call\r\nlazy._caches\r\n```\r\n```\r\nOut[6]:\r\n({'ak.from_parquet:0:off:A.list.item.x:A[0]': <ListOffsetArray64>\r\n     <offsets><Index64 i=\"[0 2 2 5]\" offset=\"0\" length=\"4\" at=\"0x7f1494000100\"/></offsets>\r\n     <content><RecordArray length=\"5\">\r\n         <field index=\"0\" key=\"x\">\r\n             <NumpyArray format=\"l\" shape=\"5\" data=\"1 2 3 4 5\" at=\"0x7f1494000080\"/>\r\n         </field>\r\n     </RecordArray></content>\r\n </ListOffsetArray64>},)\r\n```\r\n",
     "createdAt":"2021-06-17T10:46:41Z",
     "number":882607,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"nikoladze"
     },
     "body":"I also attempted to rebuild the `VirtualArray` layout and modify the form, but that did not seem to work - the generator does not put the record name there:\r\n\r\n```python\r\nimport awkward as ak\r\nimport json\r\n\r\nlazy = ak.from_parquet(\"array.parquet\", lazy=True)\r\n\r\ndef rebuild_virtual_with_name(layout, name):\r\n    generator = layout.generator\r\n    args_list = list(generator.args)\r\n    form = args_list[3]\r\n    form_dict = json.loads(form.tojson())\r\n    form_dict[\"content\"][\"parameters\"][\"__record__\"] = \"AClass\"\r\n    new_form = ak.forms.Form.fromjson(json.dumps(form_dict))\r\n    args_list[3] = new_form\r\n    return ak.Array(\r\n        ak.layout.VirtualArray(\r\n            ak.layout.ArrayGenerator(\r\n                generator.callable,\r\n                args=tuple(args_list),\r\n                form=new_form,\r\n            )\r\n        )\r\n    )\r\n\r\nlazy_A = rebuild_virtual_with_name(lazy.A.layout, \"AClass\")\r\n```\r\n\r\nif i now try to access fields of `lazy_A` - i get `ValueError: generated array does not conform to expected form:` and one can see the generated form does not have the parameters.",
     "createdAt":"2021-06-17T11:02:27Z",
     "number":882667,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Ah, I originally (mis)read that you wanted to set the `__record__` of the non-virtual `RecordArray`. Whoops!\r\n\r\n I think that the answer here depends upon how generic you want to be. Ultimately, I don't think that there's any way to do this besides deferring the call to `with_name`. If you don't want to write the logic to traverse a form dict, you can just replace the virtual layout with one that hides the form information, and just assumes the virtual layout has a length. If you wanted to do this \"properly\", you could predict what this function would do to the form, and update that information. \r\n\r\nHere's a simple implementation of the former:\r\n```python\r\nfrom collections.abc import Iterable\r\n\r\n\r\ndef _deferred_with_name_lazy(layout, name, where):\r\n    inner = ak.materialized(layout, highlevel=False)\r\n    return with_name_lazy(inner, name, where, highlevel=False)\r\n\r\n\r\ndef with_name_lazy(base, name, where, highlevel=True, behavior=None):\r\n    # Recursively move toward target\r\n    if isinstance(where, str):\r\n        where = [where]\r\n\r\n    def getfunction(layout, depth, where):\r\n        # If we reach a record array\r\n        if isinstance(layout, ak.layout.RecordArray):\r\n            # If we have no paths left to traverse\r\n            if len(where) == 0:\r\n                # Then add the parameter\r\n                parameters = dict(layout.parameters)\r\n                parameters[\"__record__\"] = name\r\n                return lambda: ak.layout.RecordArray(\r\n                    layout.contents,\r\n                    layout.recordlookup,\r\n                    len(layout),\r\n                    layout.identities,\r\n                    parameters,\r\n                )\r\n            # Otherwise, continue recursion\r\n            else:\r\n                return where[1:]\r\n\r\n        # If we hit a virtual array, we don't want to materialise it,\r\n        # so we return another virtual array that hides\r\n        elif isinstance(layout, ak.layout.VirtualArray):\r\n            return lambda: ak.virtual(\r\n                _deferred_with_name_lazy,\r\n                (layout, name, where),\r\n                length=len(layout),\r\n                highlevel=False,\r\n            )\r\n        else:\r\n            return where\r\n\r\n    out = ak._util.recursively_apply(\r\n        ak.operations.convert.to_layout(array), getfunction, pass_user=True, user=where\r\n    )\r\n    if highlevel:\r\n        return ak._util.wrap(out, ak._util.behaviorof(array, behavior=behavior))\r\n    else:\r\n        return out\r\n```\r\n\r\nThen, to use this we have\r\n```python\r\nannotated = with_name_lazy(lazy, \"AClass\", \"A\")\r\n```\r\nor for deeper `RecordArray`s,\r\n```python\r\n\r\nannotated = with_name_lazy(lazy, \"AClass\", (\"A\", \"child_of_A\"))\r\n```",
        "createdAt":"2021-06-17T11:25:22Z",
        "number":882743
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"This project has a long history of reorganizing methods to stop them from materializing VirtualArrays. The ultimate solution will be to integrate Awkward Array and Dask ([see this talk](https://github.com/jpivarski-talks/2021-05-21-dasksummit-awkward-collection)) because Dask has a `.compute()` method for the user to be able to say \"now is the time to compute.\"\r\n\r\nBut until that day comes, we can continue to fix specific operations that we feel should not cause a materialization. If this is a problem with `simplify_union` triggering a materialization, that can be fixed. The mergeability is checked by each Content class, and [VirtualArray::mergeable](https://github.com/scikit-hep/awkward-1.0/blob/2b6f3ccea3ec9ca53f9b704dd4522973a35c4f08/src/libawkward/array/VirtualArray.cpp#L807-L810) simply materializes and then uses generated array to check mergeability. If the mergeability check were moved to the Form classes, rather than the Content classes, then `VirtualArray::mergeable` would be able to use its Form to do this check instead of materializing. (That's how other tests have been made non-materializing, by moving tests that do not depend on a full array into Forms.)",
        "createdAt":"2021-06-17T14:57:48Z",
        "number":883828
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"This is effectively *two* problems:\r\n1. the existing `with_name` implementation eagerly materialises `VirtualArray`s because it visits the entire layout\r\n2. `with_name` needs to defer its operation if it encounters a `VirtualArray` during its visit.\r\n\r\nI don't think (2) is something we necessarily want to be getting involved with because it opens a whole can of worms w.r.t deferring operations & modifying the form to reflect the pending change etc.\r\n\r\n(1) definitely seems like something we could look at, because all we really need to do is make sure that we don't travel any farther than the depth of the layout that was renamed. ",
        "createdAt":"2021-06-18T12:34:36Z",
        "number":887685
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I'll suggest an alternative.",
        "createdAt":"2021-06-18T13:19:32Z",
        "number":887858
       }
      ],
      "totalCount":4
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"@nikoladze, maybe you can avoid `with_name` by constructing a new array? There's new documentation on that:\r\n\r\nhttps://awkward-array.org/how-to-create-constructors.html#content-recordarray\r\n\r\nSince your array contains VirtualArrays, you need to keep the original ak.Array in scope until you've made a new ak.Array, because the VirtualArrays only keto weak reference to any attached caches:\r\n\r\nhttps://awkward-array.org/how-to-create-constructors.html#content-virtualarray\r\n\r\nIf your use of `with_name` attached new arrays more deeply than the top level, it would have had to materialize anyway: in order to zip nested lists, it would have needed the `offset` arrays of the other lists, to be sure it's broadcasted correctly.",
     "createdAt":"2021-06-18T13:19:15Z",
     "number":887852,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"nikoladze"
     },
     "body":"@jpivarski, @agoose77, thanks a lot for your detailed answers! I did not really think about using the existing virtual array to generate a new virtual array, but maybe that is the way to go here. But i think i haven't quite understood how the nesting of virtual arrays works. I tried the following:\r\n\r\n```python\r\nlazy = ak.from_parquet(\"array.parquet\", lazy=True)\r\n\r\nleaf_form = {\r\n    \"class\": \"NumpyArray\",\r\n    \"itemsize\": 8,\r\n    \"format\": \"l\",\r\n    \"primitive\": \"int64\",\r\n}\r\n\r\ndef get_virtual_content(virtual_col):\r\n\r\n    def generate():\r\n        return ak.materialized(virtual_col).layout.content\r\n\r\n    return ak.layout.VirtualArray(\r\n        ak.layout.ArrayGenerator(\r\n            generate,\r\n            form=ak.forms.Form.fromjson(json.dumps(leaf_form))\r\n        )\r\n    )\r\n\r\ndef get_virtual_record(lazy_col, with_name):\r\n    form = ak.forms.Form.fromjson(json.dumps({\r\n        \"class\": \"ListOffsetArray64\",\r\n        \"offsets\": \"i64\",\r\n        \"content\": {\r\n            \"class\": \"RecordArray\",\r\n            \"contents\": {\r\n                \"x\": leaf_form,\r\n                \"y\": leaf_form,\r\n            }\r\n        },\r\n        \"parameters\": {\r\n            \"__record__\": with_name\r\n        }\r\n    }))\r\n\r\n    def generate():\r\n        return ak.layout.ListOffsetArray64(\r\n            ak.materialized(lazy_col.x).layout.offsets,\r\n            ak.layout.RecordArray(\r\n                [\r\n                    get_virtual_content(lazy_col.x.layout),\r\n                    get_virtual_content(lazy_col.y.layout)\r\n                ],\r\n                [\"x\", \"y\"]\r\n            ),\r\n            parameters={\"__record__\": with_name}\r\n        )\r\n\r\n    return ak.Array(\r\n        ak.layout.VirtualArray(\r\n            ak.layout.ArrayGenerator(generate, length=len(lazy_col), form=form)\r\n        )\r\n    )\r\n\r\nlazy_A = get_virtual_record(lazy.A, \"AClass\")\r\n```\r\n\r\nHowever if i now access e.g `lazy_A.x` it seems also the virtual arrays for the `x` and `y` leaf nodes get materialized. I think it happens in the constructor of `ak.layout.RecordArray` ... \r\n\r\nIn general i was hoping to get a more straightforward solution to read a `coffea.nanoevents` - like array. Parquet already allows storing the nested data, awkward array has `ak.from_parquet` with `lazy=True`, so in principle that already brings us halfway there. The next step would then to attach names to the collections.\r\n\r\nTagging @nsmith- maybe he has some ideas as well.\r\n\r\n",
     "createdAt":"2021-06-21T13:06:27Z",
     "number":897698,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"If the tree structure is VirtualArray \u2192 RecordArray \u2192 {x: NumpyArray, y: NumpyArray}, then you can't materialize _x_ without materializing _y_. A VirtualArray is all-or-nothing; it is either materialized or not.\r\n\r\nTherefore, the following is a common pattern: RecordArray \u2192 {x: VirtualArray \u2192 NumpyArray, y: VirtualArray \u2192 NumpyArray}. Accessing _x_ won't necessarily access _y_. NanoEvents uses this pattern, as does `ak.from_buffers` and `ak.from_parquet`.\r\n\r\nHere, for example, is a utility class (that @nsmith- wrote) to insert VirtualForms into the field of every RecordForm (_under_ the record, not _over_ it). https://github.com/scikit-hep/awkward-1.0/blob/346662d894df1ecab7318ba4c2bf8225ac7b94c6/src/awkward/operations/convert.py#L4792-L4813",
        "createdAt":"2021-06-21T13:33:32Z",
        "number":897829
       },
       {
        "author":{
         "login":"nikoladze"
        },
        "body":"mhh - i thought i already did `RecordArray \u2192 {x: VirtualArray \u2192 NumpyArray, y: VirtualArray \u2192 NumpyArray}` since my `get_virtual_content` function returns a virtual array?\r\n\r\nIn any case - since `ak.from_buffers` does it right i could also use that (with a mapping that makes use of the existing lazy array) - the following seems to work:\r\n\r\n```python\r\nimport awkward as ak\r\n\r\nlazy = ak.from_parquet(\"array.parquet\", lazy=True)\r\n\r\ndef get_leaf_form(form_key):\r\n    return {\r\n        \"class\": \"NumpyArray\",\r\n        \"itemsize\": 8,\r\n        \"format\": \"l\",\r\n        \"primitive\": \"int64\",\r\n        \"form_key\": form_key,\r\n    }\r\n\r\ndef get_collection_form(col_name, with_name):\r\n    return {\r\n        \"class\": \"ListOffsetArray64\",\r\n        \"offsets\": \"i64\",\r\n        \"content\": {\r\n            \"class\": \"RecordArray\",\r\n            \"contents\": {\r\n                \"x\": get_leaf_form(f\"{col_name}.x\"),\r\n                \"y\": get_leaf_form(f\"{col_name}.y\"),\r\n            },\r\n            \"parameters\": {\r\n                \"__record__\": with_name,\r\n            }\r\n        },\r\n        \"form_key\": f\"{col_name}.x\",\r\n    }\r\n\r\nform = {\r\n    \"class\": \"RecordArray\",\r\n    \"contents\": {\r\n        \"A\": get_collection_form(\"A\", \"AClass\"),\r\n        \"B\": get_collection_form(\"B\", \"BClass\"),\r\n    }\r\n}\r\n\r\nclass Mapping:\r\n\r\n    def __getitem__(self, key):\r\n        part, array_key, what = key.split(\"-\")\r\n        array = ak.materialized(lazy[tuple(array_key.split(\".\"))])\r\n        if what == \"offsets\":\r\n            return array.layout.offsets\r\n        elif what == \"data\":\r\n            return array.layout.content\r\n\r\nlazy_with_names = ak.from_buffers(form, len(lazy), Mapping(), lazy=True)\r\n```",
        "createdAt":"2021-06-21T13:58:52Z",
        "number":897963
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I was basing my assessment on the fact that your `get_virtual_record` describes a RecordForm that contains the NumpyForms.\r\n\r\nUsing `ak.from_buffers` directly works.\r\n\r\nHere is a complete example, in case it helps:\r\n\r\n```python\r\nimport cachetools\r\n\r\nimport awkward as ak\r\nimport numpy as np\r\n\r\ndef generate_x():\r\n    print(\"generating x\")\r\n    return ak.Array([1.1, 2.2, 3.3, 4.4, 5.5])\r\n\r\ndef generate_y():\r\n    print(\"generating y\")\r\n    return ak.Array([10, 20, 30, 40, 50])\r\n\r\nform_x = ak.forms.Form.from_numpy(np.dtype(np.float64))\r\nform_y = ak.forms.Form.from_numpy(np.dtype(np.int64))\r\n\r\ncache = cachetools.LRUCache(np.inf)\r\n\r\nrecordarray = ak.layout.RecordArray(\r\n    [\r\n        ak.layout.VirtualArray(\r\n            ak.layout.ArrayGenerator(generate_x, length=5, form=form_x),\r\n            ak.layout.ArrayCache(cache),\r\n        ),\r\n        ak.layout.VirtualArray(\r\n            ak.layout.ArrayGenerator(generate_y, length=5, form=form_y),\r\n            ak.layout.ArrayCache(cache),\r\n        ),\r\n    ],\r\n    [\"x\", \"y\"]\r\n)\r\n\r\narray = ak.Array(recordarray)\r\n```\r\n\r\n(such a thing could also be set up using `ak.virtual`, but I'm going the low-level route because I think this is meant to be integrated into something else low-level).",
        "createdAt":"2021-06-21T14:40:28Z",
        "number":898230
       },
       {
        "author":{
         "login":"nsmith-"
        },
        "body":"Sorry to be so late to the party. It seems to me the magic in NanoEvents is that the form can be manipulated to add `__record__` and any other parameter at any level before passing to `ak.from_buffers`, which will dutifully copy them into the materialized layout: https://github.com/scikit-hep/awkward-1.0/blob/346662d894df1ecab7318ba4c2bf8225ac7b94c6/src/awkward/operations/convert.py#L4356\r\n\r\nYou start from `ak.from_parquet` where you have no opportunity to manipulate the form. Ideally the parquet schema would carry this information and https://github.com/scikit-hep/awkward-1.0/blob/346662d894df1ecab7318ba4c2bf8225ac7b94c6/src/awkward/operations/convert.py#L3146 would read it and act as appropriate. I don't know if arrow schemas have such a `parameters` concept.",
        "createdAt":"2021-07-01T21:48:24Z",
        "number":953448
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Arrow and Parquet have string-based metadata, which is where the parameters should eventually go (but haven't yet).",
        "createdAt":"2021-07-01T22:57:45Z",
        "number":953619
       }
      ],
      "totalCount":5
     }
    }
   ],
   "totalCount":5
  },
  "createdAt":"2021-06-17T07:48:23Z",
  "number":934,
  "title":"How to attach `__record__` parameters to arrays from lazy-loaded parquet files without materializing?",
  "url":"https://github.com/scikit-hep/awkward/discussions/934"
 },
 {
  "author":{
   "login":"HDembinski"
  },
  "body":"Awkward arrays have a minimal amount of methods, instead a lot of functionality that one would expect to be provided by attributes and methods is provided by free functions, e.g. `ak.fields`. I am personally also an advocate of functional programming, but I believe that this design harms the library, since \r\n\r\n1) Python users do not expect to use functional programming. Python is multi-paradigm, but the object orientation is strong, with everything being an object. The awkward design fundamentally goes against what is generally considered Pythonic.\r\n2) The argument given in the docs in favor of using free functions is to keep the attribute name space of the Array object free, since record-array-like arrays expose their fields as attributes. This root cause is a poor design choice, since it does not always work anyway, the record could be named so that is does not map to a valid attribute name. It also creates two ways of accessing the field instead of offering the One Way To Do It as demanded by the Zen of Python. It is not worth sacrificing all that to save typing two quotes and a bracket.\r\n\r\nA Pythonic design would make record-array like awkward arrays behave like dicts, so that I can access the fields with `.keys()` and generally use the dict interface.\r\n\r\nAs it is designed now, awkward arrays do not feel Pythonic at all, which means that someone used to Python has to look everything up in the docs instead of being able to rely on common patterns that are established in the Python world.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"I only have five minutes to throw in my two-cents here; I agree that some of the design decisions made by Awkward are \"non-Pythonic\", but I think they are necessary. Note that this is a third-party perspective rather than a core maintainer's.\r\n\r\nIn the domain that the Scientific Python stack occupy, NumPy+Pandas are effectively DSLs. As such, NumPy is also not incredibly Pythonic by your metric; the array interface doesn't require arrays to implement methods, it only describes compatability with the high level APIs. \r\n\r\nOne of the core features of Awkward is making structure low cost (both physically, and mentally). That is why fields move through indices in the `__getitem__` operation. The intended consequence (I believe) is that users can add structure to better data e.g. [here](https://nbviewer.jupyter.org/github/jpivarski-talks/2020-04-08-eic-jlab/blob/master/2020-04-08-eic-jlab-EVALUATED.ipynb)\r\n\r\nWith all of this structure, there are really three options:\r\n1. Only support `__getitem__`\r\n2. Support both `__getitem__` and `__getattribute__`\r\n3. Create a proxy attribute e.g. `array._.x`\r\n\r\nIf you require the `.` namespace for Awkward methods (1), something like `events.kaons.id` becomes `events['kaons']['id']`. Personally, I find that harder to read (and there is a reason that people write object wrappers over structured JSON).\r\n\r\nThe proxy method *could* work, but it feels a bit awkward, and I could see it going wrong `events._.kaons._.id`.\r\n\r\nI don't think there is anything wrong with a library deviating from Pythonic standard, because a lot of what the ecosystem does is non-Pythonic; Numba requires you to write low level loops, and in conventional NumPy code, classes are frequently inapplicable to data analyses where the class effectively changes with every operation. That's why Awkward behaviours are interesting - they provide a much more lightweight solution to OO programming in the space. In fact, you could go as far as to say that the behaviour paradigm is true OO - instead of every type having a suite of (NumPy) methods that originate from a data member (e.g. a class with a list attribute exposing all the list methods), Awkward only implements the interface that the behaviour designer intends.",
     "createdAt":"2021-06-17T12:13:09Z",
     "number":882901,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-06-17T11:44:17Z",
  "number":936,
  "title":"Awkward array design issues",
  "url":"https://github.com/scikit-hep/awkward/discussions/936"
 },
 {
  "author":{
   "login":"alpetukhov"
  },
  "body":"Hello everyone,\r\n\r\nI'm trying to get the particle track data from the _.root_ files with the `uproot`, but I guess the question would be more appropriate here.\r\n\r\nThe `Array` I'm getting from the _.root_ file with the `iterate()` function is structured like:\r\n```\r\n[Event [ Hit variable [ Hit number ] ] ]\r\n```\r\nand I want to get a structure that is \r\n```\r\n[ Event [ Hit number [ hit variable ] ] ]\r\n```\r\n\r\nI suppose it would be more clear with an example. I've got\r\n```\r\n[\r\n  [ 'hit_x' : [ 0, 1, 0],\r\n    'hit_y' : [12, 0.3, 7],\r\n    'hit_z': [3, 8, 6]],\r\n  \r\n  ['hit_x' : [ 1, 1],\r\n    'hit_y' : [3, 7],\r\n    'hit_z': [0,4]]\r\n]\r\n```\r\n\r\nand I want to get\r\n\r\n```\r\n[\r\n  [[0, 12, 3], [1, 0.3, 8], [0, 7, 6]],\r\n  [[1, 3, 0], [1, 7, 4]]\r\n]\r\n```\r\n\r\nSo not only I want to transpose, I want to\r\n\r\n1. Drop the names of the hit variables in the `Array`\r\n2. Deal with the fact that number of track is not fixed so the innermost dimension in the initial `Array` is changing\r\n***\r\nI've tried the most obvious thing - the `numpy` `transpose` function:\r\n```\r\nnp.transpose(hit_array)\r\n```\r\n\r\nBut got the following error:\r\n```\r\nTypeError: no implementation found for 'numpy.transpose' on types that implement __array_function__: [<class 'awkward.highlevel.Array'>]\r\n```\r\n\r\n\r\nSo what would be the most appropriate way to do this transformation? I'm doing it so I can write the resulting array as a 'hit_variables' column of the pandas dataset to do some more transformations and write it to the _.hdf_ file. This is only the part of the whole transformation process which is quite memory consuming and I'm to to this python way to handle big datases so I want to make sure I would create as little intermediate data as possible.\r\n\r\nThanks in advance,\r\nAleksandr\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Given that you're using uproot, I believe you're obtaining arrays of the form\r\n```python3\r\n>>> arr = ak.Array([\r\n...   { 'hit_x' : [ 0, 1, 0],\r\n...     'hit_y' : [12, 0.3, 7],\r\n...     'hit_z': [3, 8, 6]},\r\n...\r\n...   {'hit_x' : [ 1, 1],\r\n...     'hit_y' : [3, 7],\r\n...     'hit_z': [0,4]}\r\n... ])\r\n```\r\nIf you just wanted the records to have scalar fields, then one way to achieve what you want is to re-zip the array, i.e.\r\n```python3\r\n>>> fields = ak.unzip(arr)\r\n>>> names = ak.fields(arr)\r\n>>> rezipped = ak.zip(dict(zip(names, fields)))\r\n>>> rezipped.tolist()\r\n[[{'hit_x': 0, 'hit_y': 12.0, 'hit_z': 3},\r\n  {'hit_x': 1, 'hit_y': 0.3, 'hit_z': 8},\r\n  {'hit_x': 0, 'hit_y': 7.0, 'hit_z': 6}],\r\n [{'hit_x': 1, 'hit_y': 3.0, 'hit_z': 0},\r\n  {'hit_x': 1, 'hit_y': 7.0, 'hit_z': 4}]]\r\n```\r\nThen you can unzip the records \r\n```python3\r\n>>> ak.unzip(rezipped)\r\n(<Array [[0, 1, 0], [1, 1]] type='2 * var * int64'>,\r\n <Array [[12, 0.3, 7], [3, 7]] type='2 * var * float64'>,\r\n <Array [[3, 8, 6], [0, 4]] type='2 * var * int64'>)\r\n```\r\n\r\nHowever, you are asking about how to concatenate the fields as columns. To do this, we do not need to re-zip the array. First, we need to unzip the original array:\r\n```python3\r\n>>> x, y, z = ak.unzip(arr)\r\n```\r\nWhat we want to do is call `np.stack`, but currently Awkward does not implement support for it. Instead, we can add a new innermost dimension to our array, and then call `ak.concatenate`:\r\n```python3\r\n>>> new_dim = (..., np.newaxis)\r\n>>> transposed = ak.concatenate((x[new_dim], y[new_dim], z[new_dim]), axis=-1)\r\n>>> transposed.tolist()\r\n[[[0.0, 12.0, 3.0], [1.0, 0.3, 8.0], [0.0, 7.0, 6.0]],\r\n [[1.0, 3.0, 0.0], [1.0, 7.0, 4.0]]]\r\n```\r\n\r\nNote that this will create intermediate data because `concatenate` creates a copy. In general this is pretty unavoidable; Awkward does have constructs to reference existing arrays, but when your array data size is similar to the size of the index, it's probably not worth doing (and is likely slower, because it adds another layer of indirection for computation).\r\n\r\nThere might be a faster way to do this (if that matters), depending upon how the data are serialised & deserialised; if `hit_x`, `hit_y`, and `hit_z` are different branches, then I believe that they are deserialised as separate arrays. If this weren't the case, then the fields of the record array would just be strided views on one big array. In that case, to obtain the transpose one would just take a `RegularArray` over the underlying data.",
     "createdAt":"2021-06-22T09:19:48Z",
     "number":903475,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"@agoose77's answer is exactly what I would have said: this is a good way of solving the problem. The only thing I would add for @alpetukhov is that this problem is not really \"transposing,\" since a record of three fields is a very different thing from a list of length three. `ak.unzip` breaks down the record structure and `ak.concatenate` builds up a list structure, and that's why this works. `:)`",
        "createdAt":"2021-06-22T13:30:12Z",
        "number":904865
       },
       {
        "author":{
         "login":"alpetukhov"
        },
        "body":"Thanks for such a swift reply! That was exactly what I needed.\r\nOne small question though. I don't want to hardcode the variables, so is it memory safe to do something like this?\r\n```\r\n>>> new_dim = (..., np.newaxis)\r\n>>> fields_to_transpose = [x[new_dim] for x in ak.unzip(arr)]\r\n>>> transposed = `ak.concatenate(fields_to_transpose,` axis=-1)\r\n```",
        "createdAt":"2021-06-23T12:22:35Z",
        "number":909813
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I'm not sure what you're asking by \"memory safe,\" but the above safely avoids Python for loops over large datasets. We assume that the number of fields in a record is small (a few thousand or less), which is not expensive to iterate over (`for x in ak.unzip(arr)`), but the number of items in an array is large (~billions). The above does not iterate over the array.\r\n\r\nThe `ak.unzip` and `x[..., np.newaxis]` doesn't even copy all the data (though it occurs to me that this particular slice _could_ be optimized more than it is), but the `ak.concatenate` does need to copy data, since it's collating it from many sources into a single output array. (Concatenation of more than two sources is better optimized than it used to be\u2014it used to do it two arrays at a time, but now it copies everything into the output array in a single pass.)\r\n\r\nIf by \"memory safe,\" you mean memory leaks and such, there aren't any known ways to cause memory leaks, though a few bugs involving memory leaks have been fixed in the past. Memory leaks are bugs that we should fix, not an indicator that you're doing anything wrong.",
        "createdAt":"2021-06-23T12:47:40Z",
        "number":909959
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-06-22T08:56:41Z",
  "number":945,
  "title":"Transpose a part of an Array",
  "url":"https://github.com/scikit-hep/awkward/discussions/945"
 },
 {
  "author":{
   "login":"alpetukhov"
  },
  "body":"Hello everyone, \r\n\r\nI'm going through the _root_ files with the `uproot.iterate()` function, so I've created an array to store the results\r\n```python\r\nhits_store = ak.Array([])\r\n```\r\nthen on every step I'm doing some transormation and appending the result to that array\r\n```python\r\nhits_store = ak.concatenate([hits_store, result])\r\n```\r\n\r\n\r\n***\r\nI want to get a `DataFrame` that looks something like this\r\n```\r\n               hits  tracks\r\n0          [[1, 2]]      -1\r\n1  [[1, 2], [3, 4]]      -2\r\n2          [[5, 6]]      -3\r\n```\r\n\r\nSo if I have an array that looks like this:\r\n```python\r\narr_hand = ak.Array([\r\n    [[1, 2]], \r\n    [[1, 2], [3, 4]], \r\n    [[5, 6]]\r\n])\r\n```\r\nI need to make it an `ApacheArrow` and then pass it to the `DataFrame` constructor (as I've read in [Conversion through Apache Arrow](https://awkward-array.org/how-to-convert-pandas.htmll))\r\n```python\r\narrow_hand = ak.to_arrow(arr_hand)\r\ndata_hand = {'hits': arrow_hand, 'tracks' : [-1, -2, -3]}\r\ndf_hand = pd.DataFrame(data_hand)\r\n```\r\n\r\nBut the problem arises them I'm trying to run the `ak.to_arrow()` on array that was created with `ak.concatenate()`\r\n```python\r\narr_1 = ak.Array([\r\n    [[1, 2]],\r\n    [[1, 2], [3, 4]]\r\n])\r\narr_2 = ak.Array([\r\n       [[5, 6]]\r\n])\r\narr_conc = ak.concatenate([arr_1, arr_2])\r\nak.to_arrow(arr_conc)\r\n```\r\nThe `arr_conc` looks the same as `arr_hand` but the last line fails with the following error:\r\n```\r\nArrowInvalid                              Traceback (most recent call last)\r\n<ipython-input-74-dc961b3eeec4> in <module>\r\n----> 1 ak.to_arrow(arr_conc)\r\n\r\n/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/awkward/operations/convert.py in to_arrow(array, list_to32, string_to32, bytestring_to32)\r\n   2224             )\r\n   2225 \r\n-> 2226     return recurse(layout, None, False)\r\n   2227 \r\n   2228 \r\n\r\n/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/awkward/operations/convert.py in recurse(layout, mask, is_option)\r\n   1933             )\r\n   1934             if mask is None:\r\n-> 1935                 arrow_arr = pyarrow.Array.from_buffers(\r\n   1936                     pyarrow.large_list(content_type),\r\n   1937                     len(offsets) - 1,\r\n\r\n/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib.Array.from_buffers()\r\n\r\n/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib.Array.validate()\r\n\r\n/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: First or last list offset out of bounds\r\n```\r\n***\r\nIs this the correct way to store the results during the iteration? If so, why does the `ak.to_arrow()` fails?\r\n\r\nThanks in advance,\r\nAleksandr\r\n\r\nP.S. Sorry if I'm flooding the GitHub discussions. Tell me if there's more appropriate place for such questions.\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"As a diagnostic step, call\r\n\r\n```python\r\nak.validity_error(arr_conc)\r\n```\r\n\r\nto see if the array is malformed. That's what Arrow is saying, but it would be good to corroborate it with our own error message.",
     "createdAt":"2021-06-24T11:46:29Z",
     "number":914664,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"alpetukhov"
        },
        "body":"```python\r\nak.validity_error(arr_conc)\r\n```\r\n\r\nyields nothing and\r\n\r\n```python\r\nprint(ak.validity_error(arr_conc))\r\n```\r\n\r\nyields `None`\r\n",
        "createdAt":"2021-06-24T12:14:29Z",
        "number":915353
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I just tried reproducing your example and I get no errors. What are your versions?\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import pyarrow as pa\r\n>>> ak.__version__\r\n'1.3.0'\r\n>>> pa.__version__\r\n'4.0.1'\r\n>>> arr_1 = ak.Array([\r\n...     [[1, 2]],\r\n...     [[1, 2], [3, 4]]\r\n... ])\r\n>>> arr_2 = ak.Array([\r\n...        [[5, 6]]\r\n... ])\r\n>>> arr_conc = ak.concatenate([arr_1, arr_2])\r\n>>> ak.to_arrow(arr_conc)\r\n<pyarrow.lib.LargeListArray object at 0x7fc776f09760>\r\n[\r\n  [\r\n    [\r\n      1,\r\n      2\r\n    ]\r\n  ],\r\n  [\r\n    [\r\n      1,\r\n      2\r\n    ],\r\n    [\r\n      3,\r\n      4\r\n    ]\r\n  ],\r\n  [\r\n    [\r\n      5,\r\n      6\r\n    ]\r\n  ]\r\n]\r\n>>> import pandas as pd\r\n>>> pd.DataFrame({\"hits\": ak.to_arrow(arr_conc)})\r\n               hits\r\n0          [[1, 2]]\r\n1  [[1, 2], [3, 4]]\r\n2          [[5, 6]]\r\n```",
        "createdAt":"2021-06-24T12:23:54Z",
        "number":915395
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-06-24T08:27:26Z",
  "number":956,
  "title":"Appending arrays during uproot.iterate and writing them to pandas",
  "url":"https://github.com/scikit-hep/awkward/discussions/956"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@ianna, @agoose77, and @jpivarski added to or corrected the documentation (PRs #891, #897, #925, #931, #943, #947, #951).\r\n\r\n@agoose77 added `ak.ptp` (PR #890), added support for Arrow FixedSizeListType (PR #907), implemented `ak.packed` (PRs #918, #912, #937), fixed up `ak.from_parquet` (PRs #923, #935) and `ak.unflatten` (PR #922), corrected `nbytes` (PR #928), updated the deprecation system (PR #929), fixed `ak.with_cache` (PR #942), and did internal refactoring (PR #952).\r\n\r\n@ianna added support for date-times (PRs #835, #915, #916), renamed TypedArrayBuilder \u2192 LayoutBuilder, thus completing it as it doesn't need to have a high-level interface (PR #924).\r\n\r\n@ioanaif started the C++ \u2192 Python refactoring project with help from @agoose77 and @jpivarski (PRs #884, #895, #896, #914, #957, #958, #962).\r\n\r\n@henryiii fixed an inexplicable bug in manylinux1 (PR #955), made the tests directory-independent (PR #953), and updated CMake infrastructure (PR #954). I think this will be the first deployment using GitHub Actions, rather than Azure!\r\n\r\n@jpivarski fixed random bugs (PRs #901, #904, #950) and made `__getitem__` return Python `str` when they're strings (PR #874), pinned `jaxlib<0.1.68` because of an intermittent MacOS segfault (PR #963; see connected discussion).\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.4.0rc1'>1.4.0rc1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-06-25T17:49:04Z",
  "number":964,
  "title":"1.4.0rc1",
  "url":"https://github.com/scikit-hep/awkward/discussions/964"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"Identical to 1.4.0rc6.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.4.0'>1.4.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-07-02T16:53:53Z",
  "number":978,
  "title":"1.4.0",
  "url":"https://github.com/scikit-hep/awkward/discussions/978"
 },
 {
  "author":{
   "login":"artlbv"
  },
  "body":"(I name this issue as the task I need to do, but there is probably a better more technical name for this)\r\n\r\nI'm using awkward arrays as spit out by uproot4 from \"flat\" ROOT files.\r\n\r\nThe problem is the following:\r\n* I have two object awkward arrays, e.g. for some selected gen-particles (`genbs`) and reco-jets (`l1jets`).\r\n* I would like to store the smallest `deltaR` of the reco jets to the gen particles as another \"property\" of the reco jets\r\n\r\nFor the `deltaR` computation I use [cell 82 from this tutorial notebook](https://github.com/jpivarski-talks/2021-06-14-uproot-awkward-columnar-hats/blob/main/evaluated/3-awkward-array.ipynb):\r\n\r\n```python\r\njs, bs = ak.unzip(ak.cartesian((l1jets, genbs)))\r\ndR = js.deltaR(bs)\r\n```\r\n\r\nA small \"snippet\" is in this screenshot (don't know how to insert that here otherwise)\r\n![image](https://user-images.githubusercontent.com/4972492/125514066-95aae6fc-5937-4fec-aecb-a491773964fc.png)\r\n\r\nNow what I would like to do is to create a new array/feauture e.g. `min_dR` in the `l1jets` that would correspond to the smallest dR of the jet to the gen particles in this event.\r\n\r\nIf I'd knew there are always e.g. 2 gen particles in an event (like in the selection above), I could in principle just slice the `dR` with a step of 2 and with offsets `i` for gen particle `i = 0,1`. \r\n```python\r\ndR_b1 = dR[:,0::2]\r\ndR_b2 = dR[:,1::2]\r\n```\r\nGiven that I actually  care about the dR being smaller than some threshold, I thought could then just do:\r\n```python\r\n(dR_b1 < 0.4) & (dR_b2 < 0.4)\r\n```\r\nbut that gives error `ValueError: in ListOffsetArray64, cannot broadcast nested list`.\r\n\r\nThis will in general not work when I have an a-priori unknown length of both of the input arrays.\r\n\r\nSo I was wondering whether it would be possible to somehow slice/reshape the arrays using the per-event number of input objects, etc to create a smallest deltaR array for the reco jets. Alternatively, the `deltaR < thr` result would also be fine to store.\r\n\r\nAre there any built-in features that would allow this? Or an elegant way to achieve it without too much hacking.\r\n\r\n\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"A few weeks after that tutorial, I gave [another tutorial that answered your question](https://github.com/jpivarski-talks/2021-07-06-pyhep-uproot-awkward-tutorial). (When writing it, I decided to go into more detail on these sorts of examples, but when I was presenting it, I didn't get to that part, so it's not in the video.)\r\n\r\n[This is the relevant section.](https://nbviewer.jupyter.org/github/jpivarski-talks/2021-07-06-pyhep-uproot-awkward-tutorial/blob/main/evaluated/uproot-awkward-tutorial.ipynb#Example:-reconstructed/generator-level-matching-with-%CE%94R)\r\n\r\nSomething that I was going to do when presenting it live and didn't get a chance to: try swapping the `abs(g.pdgId) == 13` with `abs(g.pdgId) != 13` and see how the distribution of \u0394R changes, once you have this notebook loaded and have evaluated cells up to this point.\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/125521428-8d206d2f-d2bb-42ec-9b5e-1f3de6eb5371.png)\r\n\r\nThe idea is that you pass `nested=True` when computing [ak.cartesian](https://awkward-array.readthedocs.io/en/latest/_auto/ak.cartesian.html), which creates a new level of nesting in which the combinations are grouped by the left item. That is, if you compute the Cartesian product of `l1jets` and `genbs` with `nested=True`, the new level of list-nesting will have all `l1jets[..., 0]` in the first new list, all `l1jets[..., 1]` in the second new list, and so on.\r\n\r\nThat extra level of nesting gives you structure to \"reduce\" over. In this case, you want to reduce by [ak.min](https://awkward-array.readthedocs.io/en/latest/_auto/ak.min.html) or [ak.argmin](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argmin.html); i.e. find the jet-gen pair with the smallest \u0394R. If instead of finding the \"best match,\" you were only interested in whether there was \"any match within a \u0394R cut,\" the reducer you'd use would be [ak.any](https://awkward-array.readthedocs.io/en/latest/_auto/ak.any.html).\r\n\r\nReducing these nested lists, of which there is one for each of the `l1jets`, leaves you with an array that has one inner item per `l1jet` (each list gets reduced to one value). That gives you a \"best `gen`\" and \"best \u0394R\" for each `l1jet`, and that's an array that you can use with the `l1jets` data. (Hint: if you have a new field with the right number of items at each level, you can assign it to an existing record: [ak.Array.setitem](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#ak-array-setitem).)\r\n\r\nSo that's the idea; hopefully, the notebook will be a help. If that's too difficult to follow, I also [worked through the same question again using Numba](https://nbviewer.jupyter.org/github/jpivarski-talks/2021-07-06-pyhep-uproot-awkward-tutorial/blob/main/evaluated/uproot-awkward-tutorial.ipynb#Reconstructed/generator-level-matching-in-Numba) instead of Awkward array-at-a-time functions. If might be easier to read the nested \"for\" loops in Numba, and since it's JIT-compiled, it's feasible to use at large scales.",
     "createdAt":"2021-07-13T20:49:55Z",
     "number":1000892,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"artlbv"
        },
        "body":"Worked like a charm, thanks a lot Jim!\r\n\r\n```python\r\njet_gen = ak.cartesian({\"l1jets\": l1jets, \"gen\": genjets}, nested=True)\r\njs, gs = ak.unzip(jet_gen)\r\ndR = js.deltaR(gs)\r\n\r\nl1jets[\"min_dR_genjet\"] = ak.min(dR, axis = -1)\r\nbest_dR = ak.argmin(dR, axis=-1, keepdims=True)\r\nl1jets[\"partonFlavour\"] = jet_gen[best_dR].gen.partonFlavour\r\n```\r\n\r\n_Stupid me was for some reason watching and reading your tutorial from PyHEP20*20*_",
        "createdAt":"2021-07-14T12:46:36Z",
        "number":1003665
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-07-13T20:05:29Z",
  "number":996,
  "title":"Store smallest deltaR result as new feature",
  "url":"https://github.com/scikit-hep/awkward/discussions/996"
 },
 {
  "author":{
   "login":"ruoyu0088"
  },
  "body":"Some JSON file use Object instead Array to save data rows, for example:\r\n\r\n```python\r\njson_str = '''\r\n{\r\n\"a\":{\"name\":\"name1\", \"value\":10, \"data\":[1, 2, 3], \"key\":\"check\"},\r\n\"b\":{\"name\":\"name2\", \"value\":20, \"data\":[2, 3, 4]},\r\n\"c\":{\"name\":\"name3\", \"value\":30, \"data\":[9, 3, 4, 10]}\r\n}\r\n'''\r\n```\r\n\r\n`from_json()` returns a Record:\r\n\r\n```python\r\nimport awkward as ak\r\ndata = ak.from_json(json_str)\r\nak.to_list(data)\r\n```\r\n\r\n```\r\n{'a': {'name': 'name1', 'value': 10, 'data': [1, 2, 3], 'key': 'check'}, \r\n'b': {'name': 'name2', 'value': 20, 'data': [2, 3, 4]}, \r\n'c': {'name': 'name3', 'value': 30, 'data': [9, 3, 4, 10]}}\r\n```\r\n\r\nbut what I really want is:\r\n\r\n```python\r\nimport json\r\ndata2 = ak.from_json(json.dumps([val | {'id':key} for key, val in json.loads(json_str).items()]))\r\nak.to_list(data2)\r\n```\r\n\r\n```\r\n[{'name': 'name1', 'value': 10, 'data': [1, 2, 3], 'key': 'check', 'id': 'a'}, \r\n{'name': 'name2', 'value': 20, 'data': [2, 3, 4], 'key': None, 'id': 'b'}, \r\n{'name': 'name3', 'value': 30, 'data': [9, 3, 4, 10], 'key': None, 'id': 'c'}]\r\n```\r\n\r\nAre there any method that can convert from `data` to `data2` without modify the original JSON data?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"There is nothing wrong (and indeed, it is sometimes favourable) with using Python routines to manipulate your JSON data. When loading anything from Python or JSON, Awkward already has to perform a loop, so there is no harm in doing it yourself, particularly when it's cleaner to do so. Awkward is designed to deal with columnar data, i.e. many rows, few columns, whereas this scheme has many columns, single rows. When `ak.from_json` builds this array, it effectively creates multiple versions of the same layout \u2014 one for each key. \r\n\r\nRather than doing all of this work, we can directly use a JSON reading library (such as `json`, but also `orjson` etc.) to load the data:\r\n```python3\r\nimport json\r\n\r\ndata = json.loads(json_str)\r\n```\r\n\r\nThen we can restructure it. Python has [long guaranteed](https://docs.python.org/2/library/stdtypes.html#dictionary-view-objects) that iterating over the various `dictview`s such as `keys` and `values` will visit each item in the same order, so we can now use `ak.with_field` to add a new `id` column to our iterator of rows:\r\n```python3\r\narray = ak.with_field(data.values(), data.keys(), \"id\")\r\n```\r\n\r\nOne could also do this by first creating the array from the values, and then calling `with_field`. The above code is slightly smaller, because `with_field` can convert a Python object to an `Array` if necessary.\r\n\r\n\r\n",
     "createdAt":"2021-07-15T08:57:41Z",
     "number":1007719,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I thought I'd add just a little more:\r\n\r\nThe one thing that you want to avoid doing is making a record with a large number of fields (more than 1000, say) because different fields are different array buffers, and working with lots of little arrays gives you none of the performance benefit of working with arrays. This should be avoided even as an intermediate step: it's better to iterate over data in Python than to make a million little arrays (which then get iterated over in Python).\r\n\r\nSince each of your top-level fields points to structures that are all the same type (in your example), you'll want the same-type data to ask be in shared fields. The problem is just to turn the dict that you start with into a list (i.e. dropping top-level field names) before converting it into an Awkward Array. This is what `dict.values` does.\r\n\r\n```python\r\nak.from_iter(json.loads(json_str).values())\r\n```\r\n\r\n(Okay, so some of the values of your top-level fields differ in that they have a \"check\" subfield, but this week be read in as though \"check\" were a missing value on all of the other records, and you can work with it in Awkward Array as an option-type.)",
     "createdAt":"2021-07-15T13:12:17Z",
     "number":1008613,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-07-15T05:59:16Z",
  "number":1002,
  "title":"How to convert a Record to an Array of it's fields?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1002"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"(It must seem rather late to even be talking about this.)\r\n\r\nWe've been keeping Python 2 support in Awkward Array to prevent overconstrained dependencies in legacy systems. For instance, if your collaboration software stack isn't Python 3 ready, a Python 3-only Awkward Array would prevent you from using it with experiment-specific software. Maybe I'm overly sensitive to this issue from too many bad experiences, but I think that time spent trying to bolt together formally incompatible versions of packages to enable some calculation is a waste of time.\r\n\r\nDropping Python 2 support in Awkward Array wouldn't mean that old versions of Awkward Array can't be used with Python 2, but it would mean that Python 2 users would not get any Awkward Array updates (bug-fixes and features). Since language features and library features are usually unrelated, I've wanted to keep them decoupled.\r\n\r\nHowever, two things have changed. It sounds like LHC collaboration software is fully or almost fully out of the Python 2 jungle\u2014specifically, I've heard that CMSSW_12_0_0 will be fully Python 3. (Let me know about other LHC experiments and any collaboration software stacks you're familiar with.) The second thing is that compatibility constraints on the _distribution_ side are starting to get unmanageable. Yesterday, @henryiii [said](https://github.com/scikit-hep/awkward-1.0/pull/1004#issue-690853518),\r\n\r\n> Binary distribution will become much harder quite rapidly for Python 2 - pip, packaging, wheel, and auditwheel have already dropped support, manylinux1 (the only remaining place with Python 2) dies Jan 1, 2022, pybind11 drops support roughly Jan 1, 2022, cibuildwheel drops it tomorrow, etc.\r\n\r\nIt's the fall of Rome for Python 2: all the infrastructure is coming apart. For a while, supporting Python 2 could be justified because it didn't cost us much effort and maybe made things easier for users working in tight environments; soon it _would_ cost us effort and maybe doesn't matter anymore for users.\r\n\r\nAwkward Array should drop Python 2 support this year, before this manylinux1 cut-off on Jan 1, 2022, maybe a month before: Dec 1, 2021. That date [corresponds to version 1.9.0](https://github.com/scikit-hep/awkward-1.0#roadmap). After that threshold, the minimum Python version would be 3.6. (Supporting Python 3.5 is almost as hard as 2.7, but I have not seen any issues from 3.6 onward.)\r\n\r\nLet me know if this would cause any problems for you or if you'd be happier with an even earlier cut-off date!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"A naive question from me - are there any minor version constraints on LHC usage? Or is dropping 3.5 acceptable for the reason that it isn't constrained?",
     "createdAt":"2021-07-16T14:37:43Z",
     "number":1013519,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"When latecomers jump from Python 2 to 3, they generally jump over the early minor versions of Python 3, straight into 3.7 or 3.8.\r\n\r\nActually, I think have statistics from PyHEP 2020...\r\n\r\n![](https://raw.githubusercontent.com/jpivarski-talks/2021-02-24-reload-statistics/main/PLOTS-pyhep/pyhep-python-2-vs-3.svg)\r\n\r\nAs it turns out, no, I don't have data on Python _minor_ version. I might be able to get that from pip download statistics, but I don't think I've extracted that field from the BigTable source.\r\n\r\nAnd the Python 2 vs 3 is probably very different this year than it was one year ago.",
        "createdAt":"2021-07-16T14:48:50Z",
        "number":1013561
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Version [1.7.0](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.7.0) is the last to support Python 2 and 3.5.\r\n\r\nA branch called [save/1.7.x-end-of-Python-2-and-3.5](https://github.com/scikit-hep/awkward-1.0/tree/save/1.7.x-end-of-Python-2-and-3.5) has been created for bug-fixes in the 1.7.x line.\r\n\r\nThe next `main` branch version will be 1.8.0rc1.\r\n\r\nNow we should feel free to rip out any Python 2.7 or 3.5-supporting infrastructure (as long as those PRs target the `main` branch, naturally).",
     "createdAt":"2021-12-02T21:42:42Z",
     "number":1740493,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-07-16T14:35:12Z",
  "number":1010,
  "title":"Dropping Python 2 support",
  "url":"https://github.com/scikit-hep/awkward/discussions/1010"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"PR #694 silently introduced a new slicing rule: if slicing with variable-length dimensions but _one_ dimension has regular length 1, it would be expanded to fit the array you're slicing, to allow this:\r\n\r\n```python\r\narray = ak.Array(\r\n    [\r\n        [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]],\r\n        [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]],\r\n    ]\r\n)\r\nslicer = ak.Array([[3, 4], [0, 1, 2, 3]])\r\nassert array[slicer[:, np.newaxis]].tolist() == [\r\n    [[3, 4], [8, 9], [13, 14]],\r\n    [[15, 16, 17, 18], [20, 21, 22, 23], [25, 26, 27, 28]],\r\n]\r\n```\r\n\r\nThe rule was introduced because it simplifies expressions in this tutorial that was never presented: https://github.com/jpivarski-talks/2021-02-09-propose-scipy2021-tutorial/blob/main/prep/million-song.ipynb (it was a proposal that wasn't accepted, and hence wasn't developed beyond its currently rough state).\r\n\r\nThe rule was justified because length-1 dimensions are expanded to fit the dimensions of other arrays in broadcasting, and NumPy advanced indexing broadcasts all arrays in a tuple before applying them as a slice, and I wanted a similar effect. However, there are several differences from the NumPy case: Awkward advanced indexing involves a single nested array, which has to match dimensions with the array you're slicing, whereas NumPy advanced indexing involves a tuple of arrays that get broadcasted together; in no case is the array used as a slice broadcasted with the array being sliced. The analogy is a little weak.\r\n\r\nMoreover, the new rule has led to some erroneous cases, described in #1022, especially https://github.com/scikit-hep/awkward-1.0/issues/1022#issuecomment-884237836. (The length-1 dimensions were _supposed_ to be made with `np.newaxis`, but there are other ways to \"accidentally\" make length-1 dimensions.) Although the rule has been in the codebase for 6 months, it has never been advertised and probably hasn't been triggered until now because it relies on a mixture of regular and variable-length axes. (@agoose77 uses regular and mixed dimensions a lot.)\r\n\r\nTherefore, _I will be removing this feature,_ restoring the \"you need all dimensions in the slicer to be regular or variable-length\" behavior that predated this rule. Slices like the above are still possible, but are more laborious to set up:\r\n\r\n```python\r\nassert array[[[[3, 4], [3, 4], [3, 4]], [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]]].tolist() == [\r\n    [[3, 4], [8, 9], [13, 14]],\r\n    [[15, 16, 17, 18], [20, 21, 22, 23], [25, 26, 27, 28]],\r\n]\r\n```\r\n\r\n(The `[3, 4]` is replicated as many times as the length of the first list in `array` and the `[0, 1, 2, 3]` is replicated as many times as the second list in `array`. If this needs to be a common idiom, we can find other ways to construct it; it's `slicer[[[0, 0, 0], [1, 1, 1]]]`.)\r\n\r\nThis is not something that would be easy to turn into a deprecation cycle\u2014note to self: I should not introduce slicing rules lightly!\u2014but it turns something that is currently poorly defined into an error message. If you see this error message:\r\n\r\n> ValueError: slice items can have all fixed-size dimensions (to follow NumPy's slice rules) or they can have all var-sized dimensions (for jagged indexing), but not both in the same slice item\r\n\r\nIn something that worked in `awkward>=1.1.0rc2,<=1.4.0` and doesn't work in `awkward>1.4.0`, then it is likely this. If this is still a need (originally issue #492), we'll find a different solution, some function that modifies the slicer to fit the array to be sliced, rather than implicitly taking regular length-1 dimensions to do that automatically.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"PR #1028 removes the slice feature, as described above. It's an almost exact inverse of #694.",
     "createdAt":"2021-07-21T17:17:36Z",
     "number":1034039,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-07-21T15:34:36Z",
  "number":1027,
  "title":"Reverting #694",
  "url":"https://github.com/scikit-hep/awkward/discussions/1027"
 },
 {
  "author":{
   "login":"agoose77"
  },
  "body":"### Description of new feature\n\nBy name, `ak.unzip` is the logical inverse of `ak.zip`. However, at the moment, the API is not symmetrical; `ak.zip` can produce tuples or named records from the input argument, whereas `ak.unzip` always returns a tuple of arrays. \r\n\r\nIf this can be improved upon, then one option is to return a mapping or a tuple according to the array type. This *would* work, but I'm not convinced that returning different types is the best design.\r\n\r\nPerhaps a better solution would be to return two tuples; one for the keys, and one for values? \r\n```python3\r\nfields, values = ak.unzip(array)\r\n```\r\n\r\nAlthough users would still need to handle the two cases differently (named records vs tuples), this would be as simple as testing `len(keys)` (or just `bool(keys)` if one is being Pythonic). This is also true of using `ak.fields` with `ak.unzip`, so I don't think it's a strong counterargument.\r\n\r\nThis issue proposes to clean-up `ak.unzip`, but clearly it would entail a breaking change. ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"ak.zip and ak.unzip were not intended to be _perfect_ inverses of each other. ak.flatten and ak.unflatten are not perfect opposites either, and for the same reason: to invert ak.zip, you need both ak.fields and ak.unzip; to invert ak.unflatten, you need both ak.num and ak.flatten.\r\n\r\nPersonally, I feel like it's enough to document this. As a user, I wouldn't expect perfect symmetry. (Though I'm not a user; actual users are invited to chime in below!) I would, however, want convenience, and having to unpack a pair of `fields` and `values` from ak.unzip when I usually only want the values would be a stumbling block (and at this point, a breaking change). Interfaces that return a tuple to unpack presuppose that it's going to be used as a separate line of code, as a statement, since in Python 3 only assignment statements can unpack tuples, which is great for a demonstrative example, but it interferes with using a function in an expression. (Note: [Python 2 could unpack tuples in function arguments!](https://www.python.org/dev/peps/pep-3113/))\r\n\r\nThe main use-case for ak.unzip is to turn the \"everything is in one array\" result from something like ak.cartesian or ak.combinations into \"parts of that calculation are in different arrays so that I can use them in separate formulas.\" At times, I've thought that ak.cartesian and ak.combinations had the wrong interface\u2014that they should return results in split form\u2014but adding ak.unzip eliminated that need, since it can be a two-part idiom. Unlike an extra function argument in ak.cartesian and ak.combinations, a separate ak.unzip function can be used in other contexts as well. That, anyway, was the motivation.",
     "createdAt":"2021-08-02T15:58:15Z",
     "number":1121048,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I'm just adding to this after some time has elapsed. It occurs to me now that we have `ak.fields` which is a dict-like metaphor for the keys of the `RecordArray`. I wonder whether `ak.unzip` should really be `ak.values` and `ak.unzip`. However, at this juncture, this would be a major API breakage for \"aesthetics\", which is clearly poor motivation. I think I was using `ak.unzip` with `ak.fields` in a particularly niche case of re-zipping some fields, e.g. with jagged `RecordArray`s from uproot. This is the only use-case that I have encountered thus far.\r\n\r\nSo, I think I'm happy with the current API for now!\r\n\r\nAdditionally, I do miss the extending unpacking that Py2 supported, although I can see the motivations for removing it.",
        "createdAt":"2021-12-23T17:15:29Z",
        "number":1864488
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-08-02T14:59:45Z",
  "number":1043,
  "title":"Should `ak.unzip` return both the field names and the values?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1043"
 },
 {
  "author":{
   "login":"pfackeldey"
  },
  "body":"### Version of Awkward Array\r\n\r\n'1.4.0'\r\n\r\n### Description and code to reproduce\r\n\r\nDear awkward developers,\r\n\r\nwe noticed an unexpected behaviour in fancy indexing empty dimensions before and after `ak.to_numpy` call.\r\nI think the following snippet makes it a bit more clear:\r\n\r\n```python\r\n# before\r\nak.to_numpy(ak.Array([[1], [2]])[[]])\r\n>> array([], shape=(0, 0), dtype=int64)\r\n\r\n# after\r\nak.to_numpy(ak.Array([[1], [2]]))[[]]\r\n>> array([], shape=(0, 1), dtype=int64) # <-- this is what we expected\r\n```\r\nThe shape discrepancy is unexpected for us. \r\nWhat is your take, is this behaviour intended?\r\n\r\nBest, Peter",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"It's because `ak.Array([[1], [2]])` is an array of variable-length lists and `np.array([[1], [2]])` is an array of fixed-length lists:\r\n\r\n```python\r\n>>> ak.Array([[1], [2]])                   # note the \"var\"\r\n<Array [[1], [2]] type='2 * var * int64'>\r\n>>> \r\n>>> ak.Array(np.array([[1], [2]]))         # now it's regular\r\n<Array [[1], [2]] type='2 * 1 * int64'>\r\n```\r\n\r\nThe empty slice maintains var on the var-type array and the regular length of the regular array:\r\n\r\n```python\r\n>>> ak.Array([[1], [2]])[[]]\r\n<Array [] type='0 * var * int64'>\r\n>>> \r\n>>> ak.Array(np.array([[1], [2]]))[[]]\r\n<Array [] type='0 * 1 * int64'>\r\n```\r\n\r\nAnd this is preserved when converting to NumPy. Var-type arrays can only be converted to NumPy if they _happen to be regular in the instance you're trying to convert_. (Regular arrays can always be converted to NumPy.) The lengths of the inner dimensions of a var-type array are determined from what those list lengths happen to be in the particular instance. Here, they happen to be zero because they're empty.\r\n\r\n```python\r\n>>> ak.to_numpy(ak.Array([[1], [2]])[[]])\r\narray([], shape=(0, 0), dtype=int64)\r\n>>> \r\n>>> ak.to_numpy(ak.Array(np.array([[1], [2]]))[[]])\r\narray([], shape=(0, 1), dtype=int64)\r\n```\r\n\r\nIt's unfortunate that we have to have this subtlety\u2014that you, as user, have to be aware of the distinction between var-type and regular-type nested lists, but this is because only the regular-type can perfectly correspond to NumPy's behavior; var-type is new territory.\r\n\r\n~~Let me know if I should reopen this, but I think it answers your question.~~ Actually, no, I'm going to convert it to a Q&A discussion so that it remains visible. Others might encounter this, and I don't want to bury it by making it a closed issue.",
     "createdAt":"2021-08-04T13:39:04Z",
     "number":1130324,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"pfackeldey"
        },
        "body":"Thanks for the detailed reply! Alright I think I got the idea. Is `var-type` then always converted to an empty dimension, when converting to NumPy? I guess there is no correct \"default\", since this is new territory...",
        "createdAt":"2021-08-06T09:11:02Z",
        "number":1138347
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"No, var-type is converted to however many list elements a particular array has. For instance,\r\n\r\n```python\r\n>>> ak.to_numpy(ak.Array([[], [], []]))\r\narray([], shape=(0, 0), dtype=float64)\r\n>>> ak.to_numpy(ak.Array([[1], [1], [1]]))\r\narray([[1],\r\n       [1],\r\n       [1]])\r\n>>> ak.to_numpy(ak.Array([[1, 2], [1, 2], [1, 2]]))\r\narray([[1, 2],\r\n       [1, 2],\r\n       [1, 2]])\r\n>>> ak.to_numpy(ak.Array([[1, 2, 3], [1, 2, 3], [1, 2, 3]]))\r\narray([[1, 2, 3],\r\n       [1, 2, 3],\r\n       [1, 2, 3]])\r\n```\r\n\r\nBut it's entirely possible for a var-type to have an inconsistent number of list elements (which is the point of it, after all). In that case, it can't be converted into NumPy:\r\n\r\n```python\r\n>>> ak.to_numpy(ak.Array([[1, 2, 3], [1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 3]]))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/miniconda3/lib/python3.8/site-packages/awkward/operations/convert.py\", line 204, in to_numpy\r\n    return to_numpy(array.layout, allow_missing=allow_missing)\r\n  File \"/home/jpivarski/miniconda3/lib/python3.8/site-packages/awkward/operations/convert.py\", line 332, in to_numpy\r\n    return to_numpy(array.toRegularArray(), allow_missing=allow_missing)\r\nValueError: in ListOffsetArray64, cannot convert to RegularArray because subarray lengths are not regular\r\n```\r\n\r\n---------------------------\r\n\r\nActually, that first example (`ak.to_numpy(ak.Array([[], [], []]))`) should be converted into a NumPy array of shape `(3, 0)`, rather than `(0, 0)`. That might actually be a bug. It's possible to express an array of shape `(3, 0)` with Awkward Array:\r\n\r\n```python\r\n>>> ak.Array(ak.layout.RegularArray(\r\n...     ak.layout.NumpyArray(np.array([])),\r\n...     size=0,\r\n...     zeros_length=3,\r\n... ))\r\n<Array [[], [], []] type='3 * 0 * float64'>\r\n```\r\n\r\nThat's what the var-type list `ak.Array([[], [], []])` should have been converted into. It could be that the conversion code is missing the `zeros_length` argument. (The `zeros_length` argument was added later to patch up missing expressiveness: the ability to say that an array of regular, zero-length lists is not empty. In NumPy, the `shape` can be given any tuple of nonnegative integers, but Awkward Array has to construct these out of nested tree nodes, which complicates some corner-cases.)",
        "createdAt":"2021-08-06T12:05:04Z",
        "number":1139013
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I've written up the bug as https://github.com/scikit-hep/awkward-1.0/issues/1048.",
        "createdAt":"2021-08-06T12:10:43Z",
        "number":1139038
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-08-04T13:24:41Z",
  "number":1047,
  "title":"Empty slicing behaviour before and after `ak.to_numpy` is different",
  "url":"https://github.com/scikit-hep/awkward/discussions/1047"
 },
 {
  "author":{
   "login":"jrueb"
  },
  "body":"### Version of Awkward Array\n\n1.4.0 (and git main) \n\n### Description and code to reproduce\n\nWhen inserting a virtual array into an array created with `ak.from_buffers` with `lazy=True`, the generator function of the virtual array gets called right away. This however happens only for the first virtual array being inserted. I would expect the generator function not being called at all, as the virtual arrays content is not required at this point.\r\nCode example:\r\n```python\r\nimport numpy as np\r\nimport awkward as ak\r\n\r\n\r\ndef some_function1(length):\r\n    print(\"called1\")\r\n    return np.ones(length)\r\n\r\n\r\ndef some_function2(length):\r\n    print(\"called2\")\r\n    return np.ones(length)\r\n\r\n\r\narray = ak.from_buffers(*ak.to_buffers(ak.Array({\"something\": [1,2,3]})), lazy=True)\r\narray[\"virtual\"] = ak.virtual(some_function1, (len(array),), length=len(array))\r\narray[\"virtual\"] = ak.virtual(some_function2, (len(array),), length=len(array))\r\n\r\n```\r\nThis will print `called1`, even though it shouldn't. If `lazy=False` in the `ak.from_buffers` call, it doesn't print anything, like it should.\r\n\r\n\r\nIn the same context i noticed a continuous increase in memory usage, in case the array is also used as part of the `args` parameter. To me this looks a lot like something isn't being freed. Code example:\r\n```python\r\nimport os\r\nimport psutil\r\nimport gc\r\nimport numpy as np\r\nimport awkward as ak\r\n\r\n\r\ndef some_function(length, something):\r\n    print(\"called\")\r\n    return np.ones(length)\r\n\r\n\r\nwhile True:\r\n    gc.collect()\r\n    big = ak.Array({\"something\": np.random.rand(100000000)})\r\n    array = ak.from_buffers(*ak.to_buffers(big), lazy=True)\r\n    array[\"virtual\"] = ak.virtual(some_function, (len(array), array), length=len(array))\r\n    array[\"virtual\"] = ak.virtual(some_function, (len(array), array), length=len(array))\r\n    print(psutil.Process(os.getpid()).memory_info().rss)\r\n```\r\nThe resident set size, which is printed, will continue to increase with every loop. This does not happen (there is almost no change to the number) if `array[\"virtual\"]` is only set once. It also does not happen if `big` instead of `array` is used in the `args` parameter.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"RE the lazy aspect to this question, I assume what's happening here is that `ak.with_field` is materialising the virtual array when broadcasting. ",
     "createdAt":"2021-08-09T14:56:36Z",
     "number":1150050,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"> RE the lazy aspect to this question, I assume what's happening here is that `ak.with_field` is materialising the virtual array when broadcasting.\r\n\r\nThat's right.\r\n\r\nAssigning an array `x` as a new field of another array `y` necessarily evaluates any generators in `x` and `y` because it needs to broadcast them to see how to restructure `x` to fit `y`. That's so that something like\r\n\r\n```python\r\narray_of_3vectors[\"mass\"] = 125.0\r\n```\r\n\r\ncan set a `\"mass\"` field with the same value in all records in `array_of_3vectors`, regardless of whether `array_of_3vectors` is an array of records, an array of lists of records, or something deeper. Similarly,\r\n\r\n```python\r\narray_of_lists_of_lists[\"new_field\"] = array_of_lists\r\n```\r\n\r\nwill broadcast each element from the shallower `array_of_lists` into the corresponding `array_of_lists_of_lists`.\r\n\r\nThis is a fundamental problem with our design of lazy arrays: the idea is that it will be materialized when needed, but it can be hard for users to guess when it's needed. (Maybe you didn't know that field-assignment broadcasts, for instance.) There's a long history of bug-fixes for lazy arrays being materialized \"too early,\" but without a clear definition of when _is_ \"too early.\" In some cases, it was obvious: some code was just checking something that could be determined from type alone, such as the value of the `\"__record__\"` parameter. Materializing a VirtualArray node for that would be wrong, because the `\"__record__\"` parameter is expressed in the VirtualArray's Form, and we can search down that tree instead. Other cases have been more borderline: should a slice materialize a VirtualArray? It's a calculation on an array, but most people didn't expect it to materialize an array, especially when the slice was just picking out one field (e.g. `my_array.field`). Now a slice of a VirtualArray creates a new VirtualArray that would cascade through evaluating the slice, then evaluating the original array.\r\n\r\nBut for broadcasting, we can't delay that. To delay a calculation, we have to be able to say what its Form is going to be without actually performing the calculation. There are even some slices in which that's not possible: the Form of the output depends on the specific values of the array. (That is, if some lists are empty or well-aligned or something, the output would have a different Form than if they weren't, and in order to make a new VirtualArray, we have to predict the Form it would have upon evaluation.)\r\n\r\nDask solves the lazy array problem differently, and that's where we're directing new effort on this:\r\n\r\n![](https://raw.githubusercontent.com/jpivarski-talks/2021-05-21-dasksummit-awkward-collection/main/degrees-of-transparency-2.svg)\r\n\r\nOne difference is that a Dask collection has a `.compute()` method, so you get to say when it's the right time to start a calculation. As a consequence, the Dask DAG can be a different thing from an ordinary array (i.e. `.compute()` returns a different type, mapping DAG \u2192 in-memory array), so it doesn't need to support all the same operations that an ordinary array supports. Perhaps there would be no way to ask a DAG what the array type will be without computing it, for instance. But this would relax constraints on what can go into the DAG so that any operation can be lazy.\r\n\r\nDevelopment of an Awkward-Dask collection is starting next month.\r\n\r\n-----------------\r\n\r\nAs for solving your problem: are you trying to make a record array with virtual fields? Instead of assigning new fields into an existing object, you could create the array all at once by calling the `ak.Array` constructor:\r\n\r\n```python\r\narray = ak.Array({\"field1\": ak.virtual(...), \"field2\": ak.virtual(...)})\r\n```\r\n\r\nor `ak.zip`, limited to the non-virtual, list-like dimensions:\r\n\r\n```python\r\narray = ak.zip({\"field1\": ak.virtual(...), \"field2\": ak.virtual(...)}, depth_limit=1)\r\n```\r\n\r\nIf the fields were _lists of virtual_ data (i.e. the list offsets are not virtual, but the contents are), then you could use `depth_limit=2` to zip those lists together without materializing the virtual data. `depth_limit=1` is equivalent to the `ak.Array` constructor.\r\n\r\n",
     "createdAt":"2021-08-09T15:25:31Z",
     "number":1150051,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jrueb"
        },
        "body":"I forgot to mention this: The `array` in my case is a NanoEvents generated by Coffea. I don't think the `ak.Array` constructor is usable here. I would have to loop over all the fields, possibly recursively, most likely breaking something about the structure.\r\n\r\nSomething I still don't understand and which made me really sure this a bug in laziness is that the second insertion does not cause the function to be called. If the first call is on purpose, I would assume the second insertion to have the same behavior. Why doesn't the second array need to be broadcasted?\r\nIf this is really how it is supposed to be, I can workaround this by just inserting a virtual array that doesn't need any heavy computation first, followed by my virtual arrays that actually need the heavy computation. Only the first one will be called right away anyway. But this seemed too arbitrary to me.\r\n\r\nShould I maybe create a separate issue for the memory leak? I thought these two are probably connected because I only could reproduce the leak in this exact same context.",
        "createdAt":"2021-08-09T16:07:46Z",
        "number":1150234
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"> Why doesn't the second array need to be broadcasted?\r\n\r\nBroadcasting depends on all arrays involved in the broadcast. In each line like\r\n\r\n```python\r\narray[\"virtual\"] = ak.virtual(some_function1, (len(array),), length=len(array))\r\n```\r\n\r\nthere are two arrays: the `array` on the left-hand side and the `ak.virtual(...)` on the right-hand side. The left-hand side is changed by this operation\u2014it contains virtual arrays and they're materialized by the broadcast, after which they hold data in a cache. It happens to be the case in this instance that\r\n\r\n```python\r\narray[\"virtual2\"] = ak.virtual(some_function2, (len(array),), length=len(array))\r\n```\r\n\r\n(or assigning to the same field name) doesn't follow a code path that evaluates the right-hand side because the left-hand side is different now, but that's a fragile situation that I wouldn't rely on.\r\n\r\nYou can always collect many arrays and pass them all to an `ak.Array` constructor or `ak.zip`, even if you had to go through a loop or recursive function to find them: this is a common functional pattern, and it works well with Awkward Arrays because\u2014apart from this field-assignment and the internal caching of VirtualArrays\u2014they're immutable. In fact, nearly every function in the Awkward codebase is a recursive walk through an unknown tree, returning a new tree with this functional pattern. I would especially recommend it if you have a large number of fields. The broadcasting in a field-assignment is a performance cost (even potential broadcasting, as it has to check to see if broadcasting is necessary).\r\n\r\nAbout the memory leak, let me look at that again. I think I know what it is (and just edited away a lot of speculation). I'll post in the next reply.",
        "createdAt":"2021-08-09T16:58:30Z",
        "number":1150447
       }
      ],
      "totalCount":2
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"The key thing about the memory leak is that it happens when a lazy array is used as a function argument but not when a non-lazy array is used as a function argument. I think what's happening is that Python can't see reference cycles like A \u2192 B \u2192 A when A is a Python object and B is a C++ object. All of the mallocs and frees are appropriately paired up (because the C++ code uses `std::shared_ptr` throughout), but that doesn't work for cycles.\r\n\r\n@nsmith- found a similar issue when a VirtualArray's cache contained the VirtualArray itself. The cache is a Python mapping (to let you use `cachetools`, a plain `dict`, etc.) but the VirtualArray and its components (ArrayGenerator and ArrayCache) are C++ objects that were designed to be independent of Python. When they wrap a Python object, their `std::shared_ptr` calls `PyDECREF`, so mallocs and frees are paired up, but Python's garbage collector can't traverse the object graph through a C++ object to know that a `std::shared_ptr` points to something that contains a Python object. Not only has Python's garbage collector not been instrumented to do this, but also the type information about what needs to be traversed and how to interpret it does not exist after compile-time. You need a language that can reflect all of its types to be able to do that.\r\n\r\nOur solution with the VirtualArray cache problem was to make the ArrayCache hold only a weak reference to the Python cache object, and then connect the strong reference through `ak.Array` (a hidden attribute named `_caches`). It works: the cycle only goes through Python objects and the garbage collector is able to detect cycles and delete them, but at a considerable cost in complexity. The bug-fix to make this reference weak (#427) was followed by many other bug-fixes to deal with the fact that it is weak and the reference can get lost (#428, #479, #560).\r\n\r\nWhat you've found is the same story, but with ArrayGenerator's hold on Python objects as arguments to the function, rather than ArrayCache's hold on a Python object as a MutableMapping. I think @nsmith- predicted yet another way of forming cycles: by the ArrayGenerator's function holding a closure to the array itself, but holding arguments is effectively the same. That sounded unlikely to come up in practice: why would the function to fill in an array need to reference the holder where that function would go? Furthermore, in a purely immutable world, it wouldn't be possible to set up that situation: you wouldn't be able to reference a VirtualArray in one of the arguments that constructs the VirtualArray because it hasn't been made yet. @nsmith-'s example of a closure referencing itself is possible because Python's global namespaces are mutable: that's how you can write a recursive function\u2014Python doesn't attempt to evaluate a reference to `f` inside the definition of `f` until `f` is called, which is after it has been fully defined. ([See this](https://stackoverflow.com/a/23830790/1623645) if you're interested.) Your example of the arguments referencing itself is only possible because of field-assignment, which is another exception from the general rule against mutability. If you weren't assigning\r\n\r\n```python\r\narray[\"virtual\"] = something_involving(array)\r\n```\r\n\r\nbut building it all at once with the `ak.Array` constructor or `ak.zip`, then it wouldn't be possible to make this cyclic reference.\r\n\r\nJust as in the vagueness about when a lazy array materializes, this memory leak is a consequence of a bad design decision: if we implemented the array nodes in Python, rather than C++, then none of this would have ever come up. As the attempt to patch VirtualArray caches shows, trying to patch this with weak references will bring in its own problems. Just as the vagueness of lazy array materialization will be solved by relying instead on Dask, the problems introduced by C++ are being addressed by refactoring Awkward Array's middle layer from C++ to Python. There's a [talk on that](https://indico.cern.ch/event/1032972/); it's an [ongoing project](https://github.com/scikit-hep/awkward-1.0/pulls?q=is%3Apr+%22C%2B%2B+refactoring%22), but the gist is below:\r\n\r\n![](https://raw.githubusercontent.com/jpivarski-talks/2021-05-26-awkward-refactoring-irishepas/main/awkward-1-0-layers-problem.svg)\r\n\r\nWe replace the C++ box with a Python box:\r\n\r\n![](https://raw.githubusercontent.com/jpivarski-talks/2021-05-26-awkward-refactoring-irishepas/main/awkward-1-0-layers-1.svg)\r\n\r\nThe motivation described in that talk was for JAX (hence the \"differentiated kernels\" that becomes possible), but many other issues boiled down to the same thing. JAX and Dask can both be supported better if those libraries' tracer objects can \"see\" down to the level of kernels, but the long-standing problem of reference cycles would also be fixed by letting Python's garbage collector \"see\" down to this level. Kernel functions, by the way, do zero memory management (purely borrowed references).\r\n\r\nSo the memory leak you found is real, but our fix for it is going to be this refactoring.",
     "createdAt":"2021-08-09T17:38:28Z",
     "number":1150628,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Someday, I should give a talk on \"Python/C++ integration: just because you can doesn't mean you should.\" The tooling is great (e.g. pybind11), but the problems are architectural, six steps ahead of the actual coding and hard to predict. I certainly didn't see this far ahead.\r\n\r\nSometimes, of course, you _have to_ include C++ in a Python project for performance or interoperability with another C++ codebase. But then, the lessons would be like the lessons about mutability from functional programming: if we can't be purists, at least we can learn to be wary and only introduce mutability\u2014or C++ integration\u2014when necessary, knowing which problems to look out for when doing so. With mutability, we have to worry about the distinction between copying and viewing. With C++ integration, we have to worry about integrating memory management. Just matching shared pointers to `PyINCREF` and `PyDECREF` (which pybind11 does automatically, by the way) is not enough: reference cycles are important, too.\r\n",
        "createdAt":"2021-08-09T17:57:45Z",
        "number":1150698
       },
       {
        "author":{
         "login":"nsmith-"
        },
        "body":"Yet another example of creating reference cycles is by creating a virtual array that closes over a highlevel array using the same cache: https://github.com/CoffeaTeam/coffea/pull/487\r\nThere are so many ways to make cycles \ud83d\ude22 ",
        "createdAt":"2021-08-09T18:42:32Z",
        "number":1150860
       }
      ],
      "totalCount":2
     }
    },
    {
     "author":{
      "login":"jrueb"
     },
     "body":"Thanks for the insightful discussion. i decided to give the `ak.Array` constructor a chance as @jpivarski suggested and it turned out to actually solve not only the first part but both parts of the issue, even with NanoEvents. Everything put into a simple class looks like this:\r\n\r\n```python\r\nclass VirtualArrayCopier:\r\n    def __init__(self, array):\r\n        self.data = {f: array[f] for f in ak.fields(array)}\r\n        self.behavior = array.behavior\r\n\r\n    def __setitem__(self, key, value):\r\n        self.data[key] = value\r\n\r\n    def get(self):\r\n        array = ak.Array(self.data)\r\n        array.behavior = self.behavior\r\n        return array\r\n\r\n    def wrap_with_copy(self, func):\r\n        @functools.wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            return func(self.get(), *args, **kwargs)\r\n        return wrapper\r\n```\r\n\r\nBy using the `__setitem__` method followed by the `get` method, one can add fields of virtual arrays without accidentally triggering the materialization. At the same time using the `data` attribute (or simpler `wrap_with_copy`) inside the `args` parameter, one avoids the cyclic reference. Probably there still are some things to improve here.",
     "createdAt":"2021-08-10T20:11:20Z",
     "number":1155798,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":4
  },
  "createdAt":"2021-08-09T14:41:51Z",
  "number":1053,
  "title":"Assigning virtual arrays as a field to an array materializes the virtual arrays",
  "url":"https://github.com/scikit-hep/awkward/discussions/1053"
 },
 {
  "author":{
   "login":"rocurley"
  },
  "body":"### Version of Awkward Array\n\n1.4.0\n\n### Description and code to reproduce\n\n**Code**\r\n```python\r\nimport pyarrow as pa\r\nimport awkward as ak\r\n\r\ntable1 = pa.Table.from_pydict({\"key\": [True]})\r\ntable2 = pa.Table.from_pydict({\"\": [True]})\r\ntable3 = pa.Table.from_pydict({\"\": [True], \"key\": [False]})\r\nprint(ak.from_arrow(table1))\r\nprint(ak.from_arrow(table2))\r\nprint(ak.from_arrow(table3))\r\n```\r\n**Output**\r\n```\r\n[{key: True}]\r\n[True]\r\n[{'': True, key: False}]\r\n```\r\n**Expected Output**\r\n```\r\n[{key: True}]\r\n[{'':True}]\r\n[{'': True, key: False}]\r\n```",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"I can reproduce this, and the `to_pydict` representation of these tables is valid. I assume we support empty field names (Jim may correct me), because the third test case works.",
     "createdAt":"2021-08-20T20:40:08Z",
     "number":1222732,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"The cause of this behaviour appears to be https://github.com/scikit-hep/awkward-1.0/blob/533f3852b5528543ad6b9c81d8f7d6fede69cb4c/src/awkward/operations/convert.py#L2902\r\n\r\nThis was introduced in https://github.com/scikit-hep/awkward-1.0/pull/606/commits/6dca8621e75c28b4bb36524ad90c7ad5c9299c46\r\n\r\nI don't know *why* we do this \u2014 it reminds me of the lazy column reading with Parquet, but a cursory glance doesn't ring any bells.",
     "createdAt":"2021-08-20T20:51:47Z",
     "number":1222733,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"What part of that commit is the problem?\r\n\r\nArrow Tables and Parquet files require named fields, but Awkward Arrays can be non-records. For non-records, we use the empty string as a field name, so the empty string is treated differently. I don't know if that's the problem here. I hadn't considered that empty string column names would be desirable in a context with real records (more than one field, more than a formality to fit the technology).",
     "createdAt":"2021-08-21T15:08:35Z",
     "number":1222734,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"> What part of that commit is the problem?\r\n> \r\n> Arrow Tables and Parquet files require named fields, but Awkward Arrays can be non-records. For non-records, we use the empty string as a field name, so the empty string is treated differently. I don't know if that's the problem here. I hadn't considered that empty string column names would be desirable in a context with real records (more than one field, more than a formality to fit the technology).\r\n\r\nIt's exactly this - we use the presence of only an empty key to implicitly mean \"this is an (Awkward) non-record array\". @rocurley in this case (I assume) wants to be able to load such a table and maintain the structure. \r\n\r\nRegardless, this cannot be \"solved\" - any convention that applies meaning to the key will ultimately collide with some value chosen by a particularly determined user. But, maybe an Awkward prefix would be better than an empty key?Perhaps this issue should become should be a discussion, to welcome other Arrow users' opinions.",
     "createdAt":"2021-08-21T15:28:57Z",
     "number":1222735,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Then what it boils down to us that we wanted a space larger than the set of all possible records; we wanted records and non-records, so we took a corner out of the space of records to represent the non-records. Now those records are not representable. We could have picked a special-sounding key, like \"AwkwardFieldNotARecord,\" but it seemed like a better idea to use a special value (the only string of length zero) for this special meaning.\r\n\r\nChanging it now would touch a lot of code\u2014it wouldn't be clean and easy, though that's not the strongest agreement against it. The strongest argument is that some people have saved files with the one policy, but if new versions of Awkward Array use a different policy for interpreting single-key, empty key name records, the existing files will be interested differently\u2014it would not be backward compatible.\r\n\r\nThere is something that we _could_ do: we could introduce another convention to encode the single-key, empty key name case in yet another structure. That is, if you really wanted to save this Awkward Array:\r\n\r\n```\r\n{\"\": XYZ}\r\n```\r\n\r\nthen we would put it in a file containing\r\n\r\n```\r\n{\"\": {\"\": XYZ}}\r\n```\r\n\r\nOkay, so what if you wanted the latter? Well, we'd save it in s file that contained three nested, single-key, empty key name records. A single rule could shift everything down by one, and even though this breaks existing uses of these structures, I doubt anyone's using them seriously.\r\n\r\nDoing so, we'd get the whole space that we want: all records and non-records. The space of records is infinite, and that means that we can make a bijection to wither infinite space with strictly more things in it\u2014this is [Hilbert's hotel](https://en.wikipedia.org/wiki/Hilbert%27s_paradox_of_the_Grand_Hotel?wprov=sfla1).\r\n\r\nWe could have done this uniformly with _all_ structures, but if we did, then the Parquet view of all Awkward Arrays would be strange. This way, only strange structures get a strange encoding. (Strange enough, though, that I expect it would be raised as another bug report, when discovered!)\r\n\r\nI think we should make this a discussion, but let's hear back from @rocurley first.",
     "createdAt":"2021-08-22T11:51:16Z",
     "number":1222736,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"> Changing it now would touch a lot of code\u2014it wouldn't be clean and easy, though that's not the strongest agreement against it. The strongest argument is that some people have saved files with the one policy, but if new versions of Awkward Array use a different policy for interpreting single-key, empty key name records, the existing files will be interested differently\u2014it would not be backward compatible.\r\n\r\nThat's a really important point that I failed to mention yesterday \u2014 any change to the convention here breaks reading old files. As you say, we'd want a strong case to be made in order to do that. ",
     "createdAt":"2021-08-22T12:08:48Z",
     "number":1222737,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"rocurley"
     },
     "body":"Hey folks!\r\n\r\nSo the context for how I encountered this is that I'm trying to read/write data in a very similar format to an Awkward Array from/to parquet, and I've got a [property test](https://hypothesis.readthedocs.io/en/latest/index.html) set up to generate random data and round-trip it. Hypothesis likes to generate \"smaller\" values, such as the empty string, and so it discovered this corner case.\r\n\r\nPractically speaking, it was pretty easy to solve this issue on my end: I just made it an error to make a column with an empty string as a name. Since my column names are directly human generated anyway, that wasn't much of a problem. Given that, I arguably no longer have a horse in this race. The reason I reported this as a bug anyway is that it felt like it made the library less predictable, which is something I value.\r\n\r\nI've been thinking about what circumstances could cause somebody to run into this outside the context of a test. I think the most likely case is if you're using it more as a dict than as a struct: your column names are generated at runtime. Maybe something like \"I'm recording the visitor counts per hour to each subdomain of my website (including the empty subdomain) and using awkward array to save it to parquet\". What does that person want? I think, mostly, they don't want the special case, but that's not on the table for backwards compatibility. Failing that, I think they want reliable round-tripping through parquet. The Hilbert hotel approach will get them that, so that's nice. On the other hand, you could imagine that they're writing the data to parquet so that some other system can consume it: in that case they want the current behavior.\r\n\r\nNot really sure what would be best here!",
     "createdAt":"2021-08-22T16:24:11Z",
     "number":1222738,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"The Awkward \u2192 Parquet \u2192 Awkward round trip can be made an identity function, which is more predictable, through the Hilbert Hotel encoding, but that makes the Awkward \u2192 Parquet step more complex and less predictable. The reason we're not wrapping everything in `{\"\":` is because we want the Parquet to be understandable in other (non-Awkward) programs. We can't have both Awkward \u2192 Parquet and Awkward \u2192 Parquet \u2192 Awkward both be simple because Parquet encodes a different space of data structures than Awkward.\r\n\r\nSince this was motivated by hypothesis testing the edge cases, not a real use-case, I think we shouldn't change it yet.\r\n\r\nAs for the possibility of a real use-case in which records are treated more like dicts than structs: that's an antipattern we should discourage. Field names in C structs are not generated because they get \"compiled in,\" there's a presumption that they're not dynamic (and can't be, unless you've got a JIT compiler). Trying to make them dynamic would make the performance worse, rather than better. Record names in Awkward Arrays are like that, too: each field creates its own memory buffer (and data type, and array infrastructure), so if they're dynamically created, the performance would be worse; it would make a lot more sense in that case to use a Python dict than an Awkward Array.\r\n\r\nOn the other hand, I should point out that a \"mapping\" data structure is a future feature (#780), but the structure is not a record with dynamically generated field names: it's two equal length arrays of keys and values (thus enforcing that the keys all have the same type as each other in one compact array, rather than an array and type for each key), in which the keys are sorted for binary searches. Uproot generates arrays of this type when it encounters a C++ `std::map` in a ROOT file, but there isn't anything in Awkward Array yet to use it for fast lookups.\r\n\r\nI'll make this a discussion.",
     "createdAt":"2021-08-23T14:31:44Z",
     "number":1222739,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":8
  },
  "createdAt":"2021-08-20T20:16:01Z",
  "number":1069,
  "title":"`from_arrow` doesn't preserve empty string column name",
  "url":"https://github.com/scikit-hep/awkward/discussions/1069"
 },
 {
  "author":{
   "login":"agoose77"
  },
  "body":"## TL;DR\r\n1. Awkward adds an extra \"advanced\" indexing routine that shadows the existing NumPy advanced indexing.\r\n2. It is hard to write code that uses this without adding multiple `ak.from_regular` calls to ensure the index is jagged.\r\n3. To fix this, we need to make the advanced Awkward-like (jagged) indexing more explicit (or drop NumPy-like indexing altogether)\r\n\r\n## Motivation\r\n\r\n### NumPy Compatability\r\nAwkward Array supports both `ak.Array`s and `np.ndarray`s in its high-level API, and in many of the array/layout methods. To maintain compatibility with NumPy, parts of the Awkward API that overload the NumPy API (e.g. `ak.sum`) behave differently according to whether they are given \"NumPy-like\" (regular) arrays or jagged arrays. There are several places that this happens:\r\n- Array indexing (`ak.Array.__getitem__`)\r\n- Array broadcasting (`ak.broadcast_arrays`)\r\n\r\n### Indexing mechanisms (`array[...]` subscript operator)\r\n`ak.Array.__getitem__` supports both the NumPy advanced indexing mechanism, and an Awkward-specific one. Although not strictly part of the NumPy API (It isn't a ufunc, or overloaded by `__array_function__`), the behaviour of the array subscript operator is a constrained by the social contract that `Array`s behave like `ndarray`s. So, to support both kinds of indexing, we switch between the implementations according to the kind of array that is passed into the index, i.e. whether it is entirely jagged, or entirely regular.\r\n\r\n*Awkward-like Indexing*\r\n```pycon\r\n>>> x = ak.Array( [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]])\r\n>>> ix = ak.Array([[0, 1], [0, 1], [0, 1], [0, 2]]) \r\n>>> x[ix].tolist()\r\n[[0, 1], [3, 4], [6, 7], [9, 11]]\r\n```\r\nvs *NumPy-like Indexing*\r\n```pycon\r\n>>> x[ak.to_regular(ix)].tolist()\r\n[[[0, 1, 2], [3, 4, 5]],\r\n [[0, 1, 2], [3, 4, 5]],\r\n [[0, 1, 2], [3, 4, 5]],\r\n [[0, 1, 2], [6, 7, 8]]]\r\n```\r\n\r\nI will subsequently refer to indexing with jagged (`var` dimension) arrays as \"awkward-like indexing\", whilst indexing with regular arrays is \"NumPy-like indexing\".\r\n\r\nWhilst it is reasonable for Awkward `Array`s to behave differently to NumPy `ndarray`s, a regular *Awkward* (non-`var`) `Array` is conceptually a (stricter) sub-type of a jagged *Awkward* (`var`) `Array`. By a hand-wavy application of [Liskov's substitution principle](https://en.wikipedia.org/wiki/Liskov_substitution_principle), it should be possible to substitute the former for the latter. However, because we use jaggedness to request Awkward-like semantics, code that interacts with regular arrays nearly always does not work for irregular ones. \r\n\r\n### Intent vs Structure\r\nPart of the problem here is that we mix the *intent* of the operation (e.g. perform advanced Awkward indexing) with the structure (e.g. this array has a dimension of fixed size). Whilst this satisfies some notion of compatibility (indexing with NumPy-like arrays should behave like NumPy), it makes it harder to reason about what code will do without actually running it.\r\n\r\nFor example, consider using `argmax` of one array to index another. Here is the first code one might write:\r\n```python3\r\ndef y_by_max_x(x, y):\r\n    ix = ak.argmax(x, axis=-1, keepdims=True)\r\n    return y[ix]\r\n```\r\nNow, from the rules of indexing, we cannot know if this will succeed for any arrays `x` and `y`, because if `x` is regular, then `keepdims=True` will give `ix` a constant `* 1` dimension. If `x` is jagged in the final dimension, `ix` will be given `* var` in the last dimension. Not only that, we need to be careful that the leading dimensions of `x` are all jagged, because otherwise the index will still fail. Thus, the \"safe\" code becomes\r\n```python3\r\ndef y_by_max_x(x, y):\r\n    ix = ak.argmax(x, axis=-1, keepdims=True)\r\n    for i in range(1, ix.ndim):\r\n        ix = ak.from_regular(ix, axis=i)\r\n    return y[ix]\r\n```\r\n\r\nThis is a bit of a contrived example, but it reflects the fact that just by wanting to index using Awkward-like semantics, we have to add boilerplate to assert our intent (ahead of time).\r\n\r\n## Proposals\r\nThe take-away of the above section is that I would like to separate the different kinds of advanced indexing such that there cannot be any ambiguity. In an ideal world, we wouldn't change the meaning of an operation according to the array dimension types *at all*, but as aforementioned, we do this for compatibility. \r\n\r\nMaking a change to solve this will allow the reader to reason about the code, and remove the need for so many guards. @jpivarski and I have had a few conversations on this already, and the following ideas were discussed. For completeness, I include ideas that aren't really strongly supported!\r\n\r\n1. Add `ak.Array.var` that \"only\" performs Awkward-like indexing:\r\n   - Should this only accept arrays (i.e. not slices, integer indices etc)?\r\n   - Established convention e.g. `.loc` in Pandas\r\n   - Should we *remove* this behaviour from `ak.Array.__getitem__`?\r\n2. Change the indexing type on a per-array basis, e.g. `y = ak.with_awkward_semantics(y)` such that `y[ix]` always performs Awkward-like indexing. This would propagate through most operations (but what happens with `ak.zip`?)\r\n   - This could be done by setting a custom behaviour that calls `.var` in (1)!\r\n3. Add a special index object that signals intent, e.g. `y[ak.jagged[ix]]` or `y[ix.jagged]`\r\n   - This is quite numpy-like (see https://numpy.org/neps/nep-0021-advanced-indexing.html proposal)\r\n\r\nPinging @nsmith whom @jpivarski mentioned had thoughts on an `ak.Array.pick`-like function in the past",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"So, onto discussion. My opinions:\r\n\r\n* (1) is perhaps the easiest to implement, and is very explicit, but:\r\n  - If we want to \"solve\" the question of ambiguity, one would nearly always need to use `.var` (which is a lot of extra typing, and visual noise) and, drop jagged indexing from `[...]`, otherwise the same problem exists but in reverse.\r\n* (2) allows the user to make a one-off decision, but\r\n  - it *disables* NumPy-like advanced indexing, which is probably useful (albeit less often than Awkward-like).\r\n  - it still requires the user to defensively call `with_awkward_semantics` in examples like the one above\r\n  - it has a non-local effect (e.g. indexing operation at line 8 depends on something at line 0).\r\n* (3) has interesting aesthetics, (there's something nice about having indexing and masking being treated as different characterisations of the same operation) but there is precedent to make this a method (2) because we already have `ak.Array.mask`",
     "createdAt":"2021-08-25T22:33:07Z",
     "number":1236386,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-08-25T22:25:02Z",
  "number":1076,
  "title":"Should `ak.Array` have a `.var` accessor for jagged indexing?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1076"
 },
 {
  "author":{
   "login":"jrueb"
  },
  "body":"I would like to check if my array is masked. If it is masked, I need to apply some extra logic. Example:\r\n```python\r\na = ak.unflatten([1,2,3,4,5], [2,2,1])\r\nb = ak.mask(a, a != 1)\r\n```\r\nNow if I call a function on `a` or `b` I would like to be able to check if the input array had `ak.mask` called on it or not. i can think of several solutions, but they all seem a bit too complicated. I was looking for something like `ak.is_masked(arr, axis)` but was unable to find it.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"We don't as-yet have a routine to do this. A \"hacky\" way is to check the type string, but if you want to specify an axis, we should write a proper visitor to do this. Your proposed function might look something like this:\r\n\r\n```python3        \r\ndef is_masked(array, axis=1):\r\n    # To support Py 2, we can't use nonlocal\r\n    state = {\r\n        \"posaxis\": axis, \r\n        \"is_mask\": False\r\n    }\r\n\r\n    def apply(layout, depth):\r\n        posaxis = state['posaxis'] = layout.axis_wrap_if_negative(state['posaxis'])\r\n        if (depth == posaxis + 1) and isinstance(layout, ak._util.optiontypes):\r\n            state['is_mask'] = True\r\n\r\n    layout = ak.to_layout(array)\r\n    ak._util.recursive_walk(layout, apply)\r\n\r\n    return state['is_mask']\r\n```\r\n    \r\nIt's not perfect (we don't exit out early once we find the right axis), but it demonstrates the point.",
     "createdAt":"2021-08-27T15:25:34Z",
     "number":1244946,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"The type that gets returned is not a string, it's an object, and inspecting that is not hacky.\r\n\r\n```python\r\n>>> isinstance(b.type.type.type, ak.types.OptionType)\r\nTrue\r\n```\r\n\r\nThat could be a recursive walk, depending on what your requirements are for wanting to find these things is. For instance, will you always be wanting to ask the question at the same `axis` depth? Would you be going through record structures? @agoose77's solution is like a general-purpose function we might add to the library, maybe with a name like `is_optiontype`, as some of the ways you can get option types is not through masking (IndexedOptionArray). Maybe that recursive function should also operate on Type objects, since you're asking a question about types? (And then maybe it should be a single implementation on Types, starting from the `array.type` if given an `array`, so that it won't materialize VirtualArrays...)\r\n\r\nThe downside of using the Type object as I've presented it above is that I didn't think it was being widely used and I'm taking advantage of that to change the interface for Awkward v2. Nested types will, in the future, be accessed as `.content`, rather than `.type`, to conform to how it's done in arrays and Forms. So in v2, the above would have to be\r\n\r\n```python\r\n>>> isinstance(b.type.content.content, ak.types.OptionType)\r\nTrue\r\n```\r\n\r\nand any recursive function would be descending on `.content`, rather than `.type`.",
        "createdAt":"2021-08-27T16:48:14Z",
        "number":1245310
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-08-27T14:54:03Z",
  "number":1077,
  "title":"How to check if an array is masked?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1077"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"This is the same as [1.5.0rc2](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.5.0rc2); most of the changes went into [1.5.0rc1](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.5.0rc1).\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.5.0'>1.5.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-09-12T21:58:27Z",
  "number":1090,
  "title":"1.5.0",
  "url":"https://github.com/scikit-hep/awkward/discussions/1090"
 },
 {
  "author":{
   "login":"imdinu"
  },
  "body":"I would like to be able to do array operations across different fields but I can't figure out how.  Let's take the mean as an example. Here I have irregular size arrays for the *x* and *y* fields across different entries, but for each individual entry the length of those two arrays is the same. How could I compute the element-wise mean between \"x\" and \"y\" for each entry?\r\n```py\r\ndata = [\r\n    {\"x\": np.array([1, 2, 3]), \"y\": np.array([2, 3, 4])}, \r\n    {\"x\": np.array([5, 6]), \"y\": np.array([6, 7])}, \r\n    {\"x\": np.array([0, 0, 0, 0]), \"y\":np.array([1, 1, 1, 1])}\r\n]\r\narr = ak.Array(data)\r\n```\r\nThe the desired result would be:\r\n```py\r\nxy_mean = [\r\n    {\"avg\": [1.5, 2.5, 3.5]},\r\n    {\"avg\": [5.5, 6.5]},\r\n    {\"avg\": [0.5, 0.5, 0.5, 0.5]}\r\n]\r\n```\r\n> **Note:** `ak.mean(arr)` throws `ValueError` for any *axis* argument value except **None**\r\n\r\nIs there any way of achieving this without iterating over each entry? Perhaps some clever uses of `ak.behavior`?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"imdinu"
     },
     "body":"### Easy solution actually\r\nOk, I have tinkered around a bit more and realised the answer is really straight forward. I could just use vectorized addition and division:\r\n\r\n```py\r\n# vectorized operations\r\nxy_mean = (arr[\"x\"]+arr[\"y\"])/2\r\n```\r\n\r\nThis is easily readable and scales very well compared to iterating over each row:\r\n```py\r\n# iterative approach\r\ndef iterate(arr):\r\n    return ak.Array(\r\n        [np.mean([arr[\"x\"][i], arr[\"y\"][i]], axis=0) \r\n        for i in range(len(arr.layout))])\r\n```\r\n### Time comparison\r\n![timing](https://user-images.githubusercontent.com/27980953/133943746-7a03bc43-ac2c-4ad7-a056-b84454b5c8e8.png)\r\n\r\n### **Suggestion**\r\n Wouldn't it make sense to have this be the behaviour of `ak.mean(arr, axis=1)` in this case?\r\n",
     "createdAt":"2021-09-19T21:49:00Z",
     "number":1354680,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"This is a good solution, and you've shown what a difference vectorization makes!\r\n\r\nAs for making it the behavior of `ak.mean`, none of the Awkward functions compute across fields\u2014record fields are not considered a dimension. What `axis=1` does mean is to average over (variable length) lists within an array. It wouldn't be possible for it to mean both.\r\n\r\nIf you have a situation in which it would be convenient to not have to manually write the names of the fields (maybe because you don't know them, it because they're different in different datasets and you want to write general code), there's `ak.fields` to give you a list of key names and `ak.unzip` to operate over the fields themselves.\r\n\r\n```python\r\nsum(ak.unzip(arr))/len(ak.fields(arr))\r\n```\r\n\r\n(Untested: I hope that Python's `sum` will take the first two items to start the summation and not need or assume an identity element, since it's not zero in this case.)",
        "createdAt":"2021-09-20T02:57:56Z",
        "number":1355233
       },
       {
        "author":{
         "login":"imdinu"
        },
        "body":"Thank you for your response. I understand the point about fields not being a considered a dimension. Sometimes it's a bit hard to wrap my head around those irregular data structures \ud83d\ude05.\r\n\r\nI have tested your suggestion and  it works just fine. It appears to be roughly 20% slower than doing the operations explicitly, but both methods have the same order of complexity so it is not a noticeable difference. (Both run in a about a few milliseconds for 20K entries)",
        "createdAt":"2021-09-20T08:45:13Z",
        "number":1356009
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I suspect you can _marginally_ improve the performance by specifying a `start` value for `sum`; it uses `0` by default for the start value, implying `n+1` summations. To be clear, this is not an *important* optimization; the overhead of one additional field addition doesn't scale with the array elements, and is only \"significant\" for a small number fields.\r\n\r\nNevertheless, for fun, you can do this in a single line using the new walrus operator in Python 3.8+, or with the functional libraries `operator` and `functools`:\r\n\r\n### Walrus\r\n```python3\r\nsum((x := ak.unzip(arr))[1:], start=x[0]) / len(x)\r\n```\r\n\r\n### `operator` and `functools`\r\n```python3\r\nfunctools.reduce(operator.add, ak.unzip(arr)) / len(ak.fields(arr))\r\n```\r\n\r\nIn this case, I don't think the one-liners necessarily improve readability, so I'd just introduce a function\r\n```python3\r\ndef sum_fieldwise(arr):\r\n  head, *tail = ak.unzip(arr)\r\n  return sum(tail, start=head)\r\n```",
        "createdAt":"2021-09-20T09:16:08Z",
        "number":1356122
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-09-19T20:34:59Z",
  "number":1100,
  "title":"How could I apply a function of multiple irregular fields without iterating along the axis?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1100"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@chrisburr made the build respect `CMAKE_ARGS` if it's set by the environment: PR #1091.\r\n\r\n@jpivarski fixed `mask_identity=False`, so that it never returns an option type: PR #1108. Also fixed parameters, which truncated floats to ints in its string representation: PR #1114.\r\n\r\n@agoose77 added a check to `from_json` to not attempt to find very large strings as filenames: PR #1085.\r\n\r\n**The v2 project:**\r\n\r\n@ianna added sorting: PR #1072, reducers: PR #1099, argsort: PR #1092.\r\n\r\n@stormiestsin added `validityerror`: PR #1101.\r\n\r\n@jpivarski added a TypeTracer: PRs #1109, #1110.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.5.1'>1.5.1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-10-14T20:47:49Z",
  "number":1115,
  "title":"1.5.1",
  "url":"https://github.com/scikit-hep/awkward/discussions/1115"
 },
 {
  "author":{
   "login":"ivirshup"
  },
  "body":"I'm a little confused about the versioning of this project.\r\n\r\nWhat is v2? Is there an imminent set of breaking changes? And is this described somewhere? All I could find is the roadmap which only has `1.x` releases planned.\r\n\r\nTrying to figure out if we should be expecting things to break, and what those things might be!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"`v2` is foremostly a rewrite of the middle C++ layer of Awkward Array in pure Python. Originally, this C++ layer was intended to be usable by other C++ software (e.g. ROOT, FastJet), but it was later realised that it presented some challenges. Among these, integration with other tools such as Jax was made more difficult by the opaqueness of the C++ layer. In addition to fixing a design problem, `v2` is also being used to re-think the lazy-loading & parquet routines in Awkward. There is an [on-going project](https://iris-hep.org/projects/awkward-dask.html) to replace the `VirtualArray` and `PartitionedArray` features with a separate third-party (`dask-awkward`) library that can leverage the expertise and resources behind the Dask project to scale Awkward analysis code to clusters.\r\n\r\n@jpivarski may correct me here, but if you're a high-level API user, that is, any public `ak.*` method, then the intent with the v1\u2192v2 transition is that it should not hugely affect your code because the focus is updating this mid level API. In particular, this is *not* like the v0\u2192v1 transition. Only with things like the removal of partitioned/lazy arrays (`ak.partitioned`) and removal of Parquet from core (at least, I assume we are dropping from core. This might not be the case) will you notice anything hugely disruptive. I don't know what the policy is on API breakages, but I'll leave Jim to address those!",
     "createdAt":"2021-11-16T15:15:19Z",
     "number":1652093,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"What @agoose77 wrote is correct: a design goal of Awkward 2.0 is to avoid changes to the high-level interface, which includes the functions named `ak.this` and `ak.that`, as well as the `ak.Array`, `ak.Record`, and `ak.ArrayBuilder` classes.\r\n\r\nTherefore, upgrading from version 1.x to version 2.0 will require minimal, if any, changes to data analyst user code. What's changing is the view that you see if you look at `.layout` of an `ak.Array`, with names like `ListOffsetArray64` and `NumpyArray`: those classes are currently C++ classes, and they're being replaced by Python classes. So, for instance, `ListOffsetArray32`, `ListOffsetArrayU32`, and `ListOffsetArray64` (three templated types in C++) are being replaced by a single `ListOffsetArray` in Python. We may provide the three old names as aliases to ease the transition, but if you make heavy use _of these layout classes_, some of the methods are changing to make the internal codebase more maintainable, and you'd have to adjust to that.\r\n\r\nThe main motivation for this refactoring ([given in detail here](https://indico.cern.ch/event/1032972/)) is to enable third-party Python libraries to be able to see more of the Awkward internals, in particular Dask. This is related to the biggest change from version 1.x to 2.0: the VirtualArray and PartitionedArray node types are being dropped in favor of an Awkward-aware Dask collection ([in development here](https://github.com/ContinuumIO/dask-awkward/), through a collaboration with Anaconda). That is, Awkward's built-in lazy arrays are being replaced by industry-standard lazy arrays. Awkward's lazy arrays were a perpetual source of trouble (see, for example, #230, #400, #432, #479, #541, #560, #597, #603, #655, #679, #783, #865, #899, #940, #1052...), largely because it evaluates the array \"when some information is needed,\" rather than \"when the user asks for it\" (Dask's interface, which requires users to type `.compute()`):\r\n\r\n<img src=\"https://raw.githubusercontent.com/jpivarski-talks/2021-05-21-dasksummit-awkward-collection/main/degrees-of-transparency-2.svg\" width=\"600px\">\r\n\r\n(slide from the [Dask Summit](https://summit.dask.org/schedule/presentation/24/dask-in-high-energy-physics-community/), [slides here](https://github.com/jpivarski-talks/2021-05-21-dasksummit-awkward-collection)).\r\n\r\nReplacing Awkward's built-in lazy arrays with Dask _is_ a big change for [Coffea's NanoEvents](https://coffeateam.github.io/coffea/notebooks/nanoevents.html), which is thoroughly virtual. The biggest part of the Awkward v1 \u2192 v2 transition will be to upgrade NanoEvents, and we'll give the Coffea team a good transition period for that work. On the plus side, it will allow Dask to be more thoroughly integrated into Coffea analyses than it currently is.\r\n\r\nAnother thing that will be different from the Awkward v0 \u2192 v1 transition is that names will not change in PyPI/conda-forge: there won't be any `awkward2` package as there was `awkward0` and `awkward1`. The downside of this is that you won't be able to load both Awkward version 1.x and 2.0 in the same Python process\u2014the motivation for multiple names in 0.x and 1.0\u2014but the high-level user changes would be much smaller or nonexistent in this case, so there's less motivation for being able to adopt it gradually. The name-change caused a lot more problems than I expected, so we won't be doing that again. (Also, whereas Awkward 0.x and 1.x were totally different libraries, 1.x and 2.x are more of a Ship of Theseus.)\r\n\r\nThis GitHub repo will need a new name, since scikit-hep/awkward-1.0 won't make sense anymore. scikit-hep/awkward-0.x was originally named scikit-hep/awkward-array. To be super-careful about the possibility of old links, I'm thinking the name of this repo should not become scikit-hep/awkward-array, but instead be scikit-hep/awkward, a never-before-used name. (GitHub automatically forwards links when a repo changes its name, but we don't want old links to point to the new repo, even though those old links would have to be years old. Google has already updated.)\r\n\r\nNote that arrays pickled or otherwise saved through [ak.to_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_buffers.html)/[ak.from_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_buffers.html) (e.g. [this tutorial](https://awkward-array.org/how-to-convert-buffers.html)) will be interchangeable between versions 1.0.1+ and 2.0+. While they can't be loaded in the same Python process, they can be serialized and deserialized between the two versions (or sent through an Arrow buffer without touching disk, etc.). Awkward [version 1.0.1](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.0.1), the minimum for pickle compatibility, is from Dec 14, 2020.)\r\n\r\nI'll be talking more about the pros and cons (mostly cons) of Awkward 1.x's mixed C++/Python codebase at ACAT: [Lessons learned in Python-C++ integration](https://indico.cern.ch/event/855454/contributions/4605044/). Short story\u2014lots of linking issues: #209, #211, #217, #233, #281, #316, #430, #476, #483, #562, #774, #778.\r\n\r\nIf you have any concerns about the performance impact of replacing C++ code with Python code, \r\n\r\n<img src=\"https://user-images.githubusercontent.com/1852447/142030610-0cb4899c-8a68-464c-a4b5-73bbb3ea7a39.png\" width=\"600px\">\r\n\r\n(from a 2020 SciPy talk) was repeated with today's v1, v2, and the equivalent Python:\r\n\r\n   * Awkward v1: **2.50 seconds** (yay! it's faster than in 2020, but that may either be improvements in Awkward or differences in testing, maybe order of operations and CPU cache... I don't have the original testing code)\r\n   * Awkward v2: **1.67 seconds**: not slower in exactly the same test; in fact, it's 50% faster (more NumPy, less custom kernel)\r\n   * Equivalent Python: **136 seconds** (almost exactly the same as in 2020, despite a new Python interpreter)\r\n\r\nThe main point of this performance test is to demonstrate that we're not putting the parts that scale with array size in pure Python: they're still in compiled code.\r\n\r\nAlso, feature requests that would have required a whole new algorithm are being implemented in version 2.0 only instead of v1 and v2 because it's easier to write once than write and port. For instance, #838 needed the `__repr__` to be rewritten to satisfy a different set of constraints, and `.tolist()` was replaced with an algorithm that's 40\u00d7 faster than the one in v1. These are, technically, user-visible changes, but not part of a public API.",
        "createdAt":"2021-11-16T17:21:29Z",
        "number":1653026
       },
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"Thanks for the very informative responses!\r\n\r\nWe're looking at including support for awkward in [`anndata`](https://github.com/theislab/anndata) (https://github.com/theislab/anndata/pull/647), and this all sounds quite manageable!",
        "createdAt":"2021-11-17T10:39:35Z",
        "number":1657152
       }
      ],
      "totalCount":2
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"A bit more context\u2014the graphic below comes from a talk I'm writing.\r\n\r\n![awkward-timeline](https://user-images.githubusercontent.com/1852447/142273956-915a48c7-2b53-4e0a-ba5a-69b38a42ff80.png)\r\n\r\nI've also been counting lines of code and I estimate that we're 75% done with _writing_ v2. (Assuming that \"lines of code\" is proportional to time, that means the v2 green/development bar will continue for another 2 months.)\r\n\r\nDuring the overlap period (both v1 and v2 are yellow):\r\n\r\n   * there will be 1.x.y and 2.0.0rcZ releases in PyPI and conda-forge, both under the name `awkward` (not repeating the \"name switch\" mistake)\r\n   * bugs will be fixed in 1.x.y, but new features will go into 2.0.0rcZ only (already doing that now, to some extent)\r\n   * 2.0.0rcZ will be the main branch in GitHub and 1.x.y will be a special branch\r\n   * the overlap period will be at least 4 months long, like v0 and v1, driven by the time needed to adapt NanoEvents and Uproot to using Dask for laziness, rather than Awkward v1 VirtualArrays\r\n\r\nAfter the overlap period (v1 is gray and v2 is yellow):\r\n\r\n   * 2.0.0 (no rc) will be released; `pip install awkward` would get that one by default (same for conda-forge)\r\n   * the 1.x.y branch will not be deleted; bug-fixes in 1.x.y will be possible, but rare\r\n\r\nDropping Python 2.7 support will happen before the v1 \u2192 v2 transition, since that's driven by loss of tooling at the end of this year (#1010).\r\n\r\nFYI @henryiii, @nsmith-, @lgray, @ioanaif, @ianna, @agoose77, @stormiestsin ",
     "createdAt":"2021-11-17T20:24:20Z",
     "number":1660704,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-11-16T14:26:52Z",
  "number":1151,
  "title":"What is v2?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1151"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"**The 1.7.x series is the last to support Python 2.7 and Python 3.5. From 1.8.0 onward, only Python 3.6 and above will be supported.**\r\n\r\n@ianna fixed a division-by-zero bug: #1168 and #1169.\r\n\r\n@jpivarski applied the 1.7.0 deprecation (see [Roadmap](https://github.com/scikit-hep/awkward-1.0#roadmap)): #1118. Enabled codecov: #1120. Updated Azure's Windows VM: #1129. Fixed order in RecordForm.contents property: #1166. Fixed Parquet reading for files with zero RecordBatches: #1170.\r\n\r\nVersion 2 development:\r\n\r\n@ioanaif: #1082, #1116, #1130, #1137, #1145, #1147, #1148, #1161, \r\n\r\n@ianna: #1111, #1141, #1140, #1138, #1142, #1149, #1150, #1164, \r\n\r\n@ctrl-stormy: #1135, \r\n\r\n@jpivarski: #1117, #1119, #1121, #1122, #1123, #1124, #1125, #1131, #1132, #1134, #1143, #1146, #1154, #1153, #1156, #1159, #1160, #1162, #1165, \n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.7.0'>1.7.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-12-02T21:30:21Z",
  "number":1171,
  "title":"1.7.0",
  "url":"https://github.com/scikit-hep/awkward/discussions/1171"
 },
 {
  "author":{
   "login":"fleble"
  },
  "body":"Dear experts,\r\n\r\nFor various reasons, I wrote a class inheriting from ak.Array implementing a few methods. For instance:\r\n```python\r\nclass Wrapper(ak.Array):\r\n    def __init__(self, data):\r\n        super().__init__(data)\r\n\r\n    def test(self):\r\n        print(\"OK\")\r\n\r\nak_array = Wrapper([[1,2], [], [3]])\r\nak_array.test()\r\n```\r\n\r\nThis worked with version 1.5.1 of awkward array, but does not work anymore with version 1.7.0. The error is the following:\r\n```\r\n  File \"/home/fleble/miniconda3/envs/svj/lib/python3.8/site-packages/awkward/highlevel.py\", line 1129, in __getattr__\r\n    raise AttributeError(\r\nAttributeError: no field named 'test'\r\n```\r\n\r\nIndeed, the type of `ak_array` is `<class 'awkward.highlevel.Array'>`.\r\n\r\nIs there a way to write a class deriving from ak.Array or should this be prohibited?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"This change was caused by https://github.com/scikit-hep/awkward-1.0/commit/ccd753757c8a7173ba6f7e4d855e0bceb82a759c.\r\n\r\nI don't know what @jpivarski expects to happen with custom array classes in this context. A solution that _is_ guaranteed to work in future is the `__array__` parameter:\r\n```python3\r\nak.behavior['wrapper_cls'] = Wrapper\r\n\r\nak_array = ak.with_parameter(\r\n  [[1,2], [], [3]],\r\n   \"__array__\", \r\n  \"wrapper_cls\"\r\n)\r\n```\r\n\r\nAwkward uses the behaviours mechanism to allow an association between methods and data, without binding them together explicitly. This mechanism extends to _records_, i.e. collections of arrays, which can provide OO interfaces by defining a particular behaviour class for the record. For more information, see here: https://awkward-array.readthedocs.io/en/latest/ak.behavior.html",
     "createdAt":"2021-12-07T16:22:39Z",
     "number":1767173,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"grst"
        },
        "body":"How does this extend to a case where I have a wrapper class that requires an additional parameter, e.g. \r\n\r\n```python\r\nimport awkward._v2 as ak\r\n\r\n@ak.behaviors.mixins.mixin_class(ak.behavior)\r\nclass Wrapper(ak.Array):\r\n   def __init__(self, array, params):\r\n       self._params = params\r\n       super().__init__(array)\r\n\r\n   def do_something_with_params(self):\r\n       print(f\"These are the params: {self._params}\")\r\n```\r\n\r\nWhen I do `ak.with_name(array, name=\"Wrapper\")`, how would I pass that additional parameter? \r\n\r\nCC @ivirship\r\n",
        "createdAt":"2022-09-02T11:06:26Z",
        "number":3535483
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"@grst in general, you would be encouraged not to do this. Awkward makes new versions of arrays all over the place, and it does this by assuming that arrays are layouts + behaviour dictionary. This is intentional; the behaviour mechanism is like a lightweight view over an existing array, so we don't want any additional mechanisms for configuration besides the array itself. \r\n\r\nYou might use the existing parameters method to attach metadata to an array. I don't believe we have any strong policy about what can be put in there yet (ping @jpivarski). Additionally, I believe that most operations that would cause these parameters to be lost would also lead to the `__array__` parameter that associates the wrapper with the particular array to be lost. The reverse is perhaps not true though; we might somewhere remove only the `__array__` parameter. This likely wouldn't be a problem in this case, though.\r\n\r\nCould you perhaps expand on your use case here? :)",
        "createdAt":"2022-09-02T11:21:27Z",
        "number":3535576
       },
       {
        "author":{
         "login":"grst"
        },
        "body":"Our use case is related to https://github.com/scverse/anndata/pull/647: AnnData is a container with several matrices that are [aligned to each other](https://anndata.readthedocs.io/en/latest/_images/anndata_schema.svg) and can be numpy arrays, sparse arrays, pandas data frames, or now also awkward arrays. \r\n\r\nWe are implementing \"views\" with a copy-on-write mechanism for `AnnData`. When slicing an `AnnData` object, it becomes an `AnnDataView`, when a user tries to modify the view, it is implicitly copied. \r\n\r\n```python\r\n# X = main matrix, obsm = matrices aligned to axis 0 (\"observations\") of X\r\n>>> adata= anndata.AnnData(X = np.ones((4,4)), obsm={\"test\": np.ones((4, 2))})\r\n>>> adata\r\nAnnData object with n_obs \u00d7 n_vars = 4 \u00d7 4\r\n    obsm: 'test'\r\n\r\n# Let's create a view by slicing\r\n>>> view = adata[:3, ]\r\n>>> view \r\nView of AnnData object with n_obs \u00d7 n_vars = 3 \u00d7 4\r\n    obsm: 'test'\r\n\r\n# Let's update the \"test\" matrix in `.obsm`\r\n>>> view.obsm[\"test\"]\r\nArrayView([[1., 1.],\r\n           [1., 1.],\r\n           [1., 1.]])\r\n>>> view.obsm[\"test\"][1, :] = 42\r\n/home/sturm/tmp/ipykernel_57822/3480696444.py:1: ImplicitModificationWarning: Trying to modify attribute `.obsm` of view, initializing view as actual.\r\n  view.obsm[\"test\"][1, :] = 42\r\n\r\n# Now `view` is a regular AnnData object and not a view anymore. \r\n>>> view\r\nAnnData object with n_obs \u00d7 n_vars = 3 \u00d7 4\r\n    obsm: 'test\r\n```\r\n\r\nWith the changes introduced in the PR above, `.obsm` could contain an AwkwardArray: \r\n```python\r\n>>> adata.obsm[\"awk\"] = ak.Array([{'a': 0, 'b': 1}] * 4)\r\n```\r\n\r\nWhen we change that array, we need to \"notify\" the `AnnDataView` that holds the view of the awkward array, to turn itself into a copy. \r\n```python\r\n# Now we need to trigger the `ImplicitModificationWarning` and make an actual copy\r\n>>> adata.obsm[\"awk\"][\"b\"] = np.array([42, 42, 42, 42])\r\n```\r\n\r\nThe way we currently solve this is to have a `View` Class for each data type (e.g. [`SparseCSRView`](https://github.com/scverse/anndata/blob/f7edc67ed238990256617f06122a4e0270730a0c/anndata/_core/views.py#L96-L99)) with a [`_SetItemMixin`](https://github.com/scverse/anndata/blob/f7edc67ed238990256617f06122a4e0270730a0c/anndata/_core/views.py#L17-L35) that overrides the `__setitem__` function. This overridden `__setitem__` function [makes use of a reference to the AnnData view](https://github.com/scverse/anndata/blob/f7edc67ed238990256617f06122a4e0270730a0c/anndata/_core/views.py#L44) to trigger the copy. This reference is what I would want to pass as parameter to the `AwkwardArrayView` which I [tried implementing as a \"behavior\"](https://github.com/scverse/anndata/blob/f7edc67ed238990256617f06122a4e0270730a0c/anndata/_core/views.py#L163-L170). \r\n\r\nI hope I managed to give enough context without going too much into anndata-specific details. LMK if anything is unclear. \r\n\r\n\r\n\r\n",
        "createdAt":"2022-09-02T11:52:59Z",
        "number":3535849
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I'm sure I could find it in the PR, but it would be easier for me to ask here - does `adata.obsm[\"awk\"] = ak.Array([{'a': 0, 'b': 1}] * 4)` store the `ak.Array` or the underlying layout in your `AnnData` container?\r\n\r\nAssuming it stores the `ak.Array`, then on the face of it, parameters would seem useful here. We have a restriction that `parameters` needs to be JSON serialisable. So, I suggest a weak registry:\r\n\r\n```python\r\nimport awkward._v2 as ak\r\nimport weakref\r\n\r\n_registry = weakref.WeakValueDictionary()\r\n\r\n\r\nANNDATA_KEY_PARAMETER = \"_anndata_listener_key\"\r\n\r\ndef find_listener(key: str):\r\n    return _registry[key]\r\n\r\n\r\ndef compute_key(array: ak.Array, listener) -> str:\r\n    return f\"target-{id(listener)}\"\r\n\r\n\r\ndef register_listener(array: ak.Array, listener):\r\n    key = compute_key(array, listener)\r\n    _registry[key] = listener\r\n    return ak.with_parameter(array, ANNDATA_KEY_PARAMETER, key)\r\n\r\n\r\nclass ArrayView(ak.Array):\r\n    def _handle_missing_key(self, name, value):\r\n        pass\r\n\r\n    def _handle_invalid_key(self, name, value, key):\r\n        pass\r\n\r\n    def __setitem__(self, name, value):\r\n        key = self.layout.parameter(ANNDATA_KEY_PARAMETER)\r\n        if key is None:\r\n            self._handle_missing_key(name, value)\r\n        else:\r\n            try:\r\n                ref = find_listener(key)\r\n            except KeyError:\r\n                self._handle_invalid_key(name, value, key)\r\n            else:\r\n                ref.notify(name, value, self)\r\n        super().__setitem__(name, value)\r\n\r\n\r\nak.behavior['*', 'ArrayView'] = ArrayView\r\n\r\n\r\nclass Listener:\r\n    def notify(self, name, value, notifier):\r\n        print(f\"Notified by {notifier} for {name}={value!r}\")\r\n\r\n# Associate target with array\r\nlistener = Listener()\r\n\r\narray = register_listener(\r\n    ak.zip({'x': [[1, 2, 3], [4, 5, ]]}, with_name=\"ArrayView\"),\r\n    listener\r\n)\r\n\r\narray['x'] = array.x * 2\r\n```\r\n\r\nWould something like this work?",
        "createdAt":"2022-09-02T12:48:40Z",
        "number":3536257
       },
       {
        "author":{
         "login":"grst"
        },
        "body":"> 'm sure I could find it in the PR, but it would be easier for me to ask here - does adata.obsm[\"awk\"] = ak.Array([{'a': 0, 'b': 1}] * 4) store the ak.Array or the underlying layout in your AnnData container?\r\n\r\nCorrect, it stores the `ak.Array`!\r\n\r\n@ivirshup, what do you think of this approach? Looks reasonable to me! ",
        "createdAt":"2022-09-02T13:13:49Z",
        "number":3536455
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"There's no limitation on the scope of what goes into `parameters` except\r\n\r\n  * dunder keys (`__array__`, `__record__`, `__doc__`, ...) are reserved, since they have some active meaning (interpreted by Awkward internals)\r\n  * parameters have to be JSON (so that they can be serialized and saved forever or transmitted to workers on a network).\r\n\r\nVirtually every operation takes an array and makes a new array from it: a lot of attention is given in the code to pass on parameters through all the operations in which the output is \"the same array\" in some sense. Since you want metadata that follows the array around, this sounds like the right place to put it.\r\n\r\nBut also, you're using this to make weak references, which are very transient, non-JSON-like things. That argues against making them parameters. @agoose77's solution of using parameters to store keys of a weak reference registry is a good one, though if you ever serialize these arrays, they'll retain keys that point to a registry that no longer exists. If you're ever reading and writing or transmitting arrays with these keys, be sure to distinguish between keys relevant for this Python process with its global registry of transient weak references and some other one in another Python process. (Maybe always strip the keys out before serializing?)\r\n\r\nOn the other other hand, maybe this is a job for the new ak.Array-level metadata proposed in #1391. (I just mentioned this issue a few minutes ago, and it's high in our priority list.) This metadata would follow \"the same array\" around as ak.Array instances beget new ak.Array instances, like parameters, but it wouldn't be part of the serialized array, wouldn't have the JSON restriction, and therefore could hold transients like weak references. As in the discussion a few minutes ago, we want this mechanism to be general enough that it supports a variety of use-cases. (The original use-case is just to not lose xarray metadata, if we ever convert to or from xarrays.)",
        "createdAt":"2022-09-02T17:43:56Z",
        "number":3539002
       }
      ],
      "totalCount":6
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Thanks, @agoose77, for answering this! You made all the points I wanted to except the \"why\".\r\n\r\nYou can make subclasses of `ak.Array` and `ak.Record`, but you have to assign them through `ak.behavior`. There's a big write-up of that here:\r\n\r\nhttps://awkward-array.readthedocs.io/en/latest/ak.behavior.html\r\n\r\nWhen you do array manipulations, you end up creating new array objects all the time\u2014every slice is a new array object from Python's point of view (though the data buffers themselves share a lot of memory for efficiency). The `ak.behavior` is a set of rules for applying your subclasses to new array objects depending on \"parameters\" in the array, so even if your data type is buried deeply in a data structure (and there are _no_ Python classes associated with it), when it comes to the surface through a slice or something, it will be wrapped by the right Python class (which _must_ be a subclass of `ak.Array` or `ak.Record`, depending on what you're doing).\r\n\r\n> This change was caused by [ccd7537](https://github.com/scikit-hep/awkward-1.0/commit/ccd753757c8a7173ba6f7e4d855e0bceb82a759c).\r\n\r\nOh, yeah, the old behavior worked by accident. Before that fix, it wasn't checking `ak.behavior`, which let the class be stale (and therefore wrong) in some cases and allowed your `Wrapper` without going through the `ak.behavior` mechanism, @fleble. The new version revealed unintended behavior that you were using. Switching to `ak.behavior` would make it more stable anyway: if you have records inside the Array that need to be interpreted a particular way, this would wrap them _any_ time they come to the surface through a slice, not just when you explicitly call `Wrapper`.",
     "createdAt":"2021-12-07T16:31:35Z",
     "number":1767246,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"fleble"
     },
     "body":"@agoose77 and @jpivarski thank you very much for your answers. I had completely missed the point of `ak.behavior`. I see it now. Thank you very much for the very clear, thorough and very fast reply!\r\n\r\nIf my class also defines some attributes, and a class inherits from another one, I figured out I can proceed like this:\r\n```python\r\nclass Wrapper0(ak.Array):\r\n    def __init__(self, data):\r\n        ak.behavior[\"Wrapper0\"] = Wrapper0\r\n        if \"__array__\" not in ak.parameters(data):\r\n            data = ak.with_parameter(\r\n                data,\r\n                \"__array__\",\r\n                \"Wrapper0\",\r\n            )\r\n        super().__init__(data)\r\n        self.attr0 = \"attr0\"\r\n\r\n    def test0(self):\r\n        print(\"0\")\r\n\r\n\r\nclass Wrapper1(Wrapper0):\r\n    def __init__(self, data):\r\n        ak.behavior[\"Wrapper1\"] = Wrapper1\r\n        if \"__array__\" not in ak.parameters(data):\r\n            data = ak.with_parameter(\r\n                data,\r\n                \"__array__\",\r\n                \"Wrapper1\",\r\n            )\r\n        super().__init__(data)\r\n        self.attr1 = \"attr1\"\r\n\r\n    def test1(self):\r\n        print(self.fields)\r\n\r\n\r\ndata = {\r\n    \"Jet_pt\": [[1,2], [], [3]],\r\n    \"MET\": [1, 2, 3],\r\n}\r\nak_array = Wrapper1(ak.Array(data))\r\n\r\nak_array.test0()\r\nak_array.test1()\r\nprint(ak_array.attr0)\r\nprint(ak_array.attr1)\r\n```\r\n\r\n(posting in case it is of interest for someone or in case you have a comment about it).\r\n\r\nThank you very much again.",
     "createdAt":"2021-12-07T18:47:45Z",
     "number":1768104,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I would advise against registering the behaviour on the initialiser of your classes. It is better to do this outside of the class, e.g.\r\n\r\n```python3\r\nclass Wrapper0(ak.Array):\r\n    def test0(self):\r\n        print(\"0\")\r\n\r\n\t@property\r\n    def attr0(self):\r\n        return \"attr0\"\r\n\r\nak.behavior['Wrapper0'] = Wrapper0\r\n\r\n\r\nclass Wrapper1(Wrapper0):\r\n\r\n    def test1(self):\r\n        print(self.fields)\r\n\r\n\t@property\r\n    def attr1(self):\r\n        return \"attr1\"\r\n\r\nak.behavior['Wrapper1'] = Wrapper1\r\n\r\n\r\ndata = {\r\n    \"Jet_pt\": [[1,2], [], [3]],\r\n    \"MET\": [1, 2, 3],\r\n}\r\nak_array = ak.with_parameter(ak.zip(data, depth_limit=1), \"__array__\", \"Wrapper1\")\r\n\r\nak_array.test0()\r\nak_array.test1()\r\nprint(ak_array.attr0)\r\nprint(ak_array.attr1)\r\n\r\n```\r\n\r\nHowever, there's more to `ak.behavior` than just `__array__`. If your behaviour corresponds to operations on fields, i.e. if you want to define a class for a particular collection of fields, there's a very similar parameter called `__record__`. This allows you to specify behavior for singular records as well as arrays of records (RecordArrays).\r\n\r\n \r\n```python3\r\nclass MyRecord(ak.Record):\r\n    def what_am_i(self):\r\n        print(\"I am a record\")\r\n\r\nak.behavior['MyRecord'] = MyRecord\r\n\r\n\r\nclass MyRecordArray(ak.Array):\r\n    def what_am_i(self):\r\n        print(\"I am an array of record\")\r\n\r\nak.behavior['*', 'MyRecord'] = MyRecordArray\r\n\r\n\r\n\r\ndata = {\r\n    \"Jet_pt\": [[1,2], [], [3]],\r\n    \"MET\": [1, 2, 3],\r\n}\r\nak_array = ak.zip(data, with_name=\"MyRecord\", depth_limit=1)\r\n```\r\nThen we have\r\n```pycon\r\n>>> ak_array.what_am_i()\r\nI am an array of record\r\n>>> ak_array[0].what_am_i()\r\nI am a record\r\n```\r\n\r\nIt is useful to have separate classes for records and arrays when you can't use the same logic e.g., if you are subscripting one of your fields:\r\nThis works for arrays\r\n```pycon\r\n>>> ak_array.MET[0] # OK\r\n1\r\n```\r\nBut fails for records\r\n```pycon\r\n>>> ak_array[0].MET[0] # NOT OK\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-27-75468926cd2d> in <module>\r\n      1 ak_array.MET[0] # OK\r\n----> 2 ak_array[0].MET[0] # NOT OK\r\n\r\nTypeError: 'int' object is not subscriptable\r\n```\r\nHowever, often your methods / properties work equally well upon records and arrays. In these cases, there's a useful `mixin_class` decorator to reduce the amount of boilerplate:\r\n```python3\r\n@ak.mixin_class(ak.behavior) # Register with ak.behavior for arrays and records!\r\nclass MyRecord:\r\n    def what_am_i(self):\r\n        print(\"I am an array / record\")\r\n\r\n\r\ndata = {\r\n    \"Jet_pt\": [[1,2], [], [3]],\r\n    \"MET\": [1, 2, 3],\r\n}\r\nak_array = ak.zip(data, with_name=\"MyRecord\", depth_limit=1)\r\n```\r\nThen we have\r\n```pycon\r\n>>> ak_array.what_am_i()\r\nI am an array / record\r\n>>> ak_array[0].what_am_i()\r\nI am an array / record\r\n```\r\n\r\nIn terms of naming, you don't really want `Record` or `Wrapper` in your names. Instead, something meaningful e.g. `Point3D` that represents the kind of object you have.",
        "createdAt":"2021-12-08T15:05:04Z",
        "number":1773276
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":3
  },
  "createdAt":"2021-12-07T15:56:05Z",
  "number":1177,
  "title":"Wrapping ak.Array",
  "url":"https://github.com/scikit-hep/awkward/discussions/1177"
 },
 {
  "author":{
   "login":"fleble"
  },
  "body":"Dear experts,\r\n\r\nI cannot find a straightforward way to make a calculation using ak array operations for a particular task. I was wondering whether I am missing something or you could point me to the relevant functions to do such a task. You can find below the description of the problem.\r\n\r\nI have the following two ak arrays, for instance:\r\n```python\r\nak_array_1 = ak.Array([[1, 6], [2]])\r\nak_array_2 = ak.Array([[0, 4, 7], [2]])\r\n```\r\nAnd for each element of `ak_array_2`, I would like to compute the minimum distance to the corresponding nested list in array `ak_array_1`.\r\nFor instance here, the output would be:\r\n```\r\n[[1, 2, 1], [0]]\r\n```\r\n0 is closest to 1 and the difference is 1, 4 is closest to 6 and the difference is 2 etc...\r\nIn my use case, the number of elements is very large in axis 0, but not in axis 1 (length < 10).\r\n\r\nI found a solution for it (see below), but it is quite convoluted, and I was wondering if there exists a simpler solution for it\r\n```python\r\nfor idx in range(ak.max(ak.num(ak_array_2, axis=1), axis=0)):\r\n    ak_array_2_masked = ak.mask(ak_array_2, ak.count(ak_array_2, axis=1)>idx)\r\n    ak_array_2_one_index = ak_array_2_masked[:, idx][:, np.newaxis]\r\n    ak_array_2_broadcasted = ak.broadcast_arrays(ak_array_2_one_index, ak_array_1)[0]\r\n    distances = abs(ak_array_2_broadcasted - ak_array_1)\r\n\r\n    output = ak.Array([ak.min(distances, axis=1)])\r\n    if idx == 0:\r\n        output_array = output\r\n    else:\r\n        output_array = ak.concatenate((output_array, output), axis=0)\r\n\r\n# Inverting axis 0 and 1\r\noutput_array = ak.from_regular(ak.to_numpy(output_array).T)\r\n\r\n# Restoring the jagged structure\r\nfilter_ = ak.fill_none((output_array != None), False)\r\noutput_array = output_array[filter_]\r\nprint(output_array)\r\n```\r\n\r\n(I wrote the loop over axis 1 because axis 0 has large number of elements.)\r\n\r\nI have a more general concern: in complicated cases I have the impression that `for` loops are easy to write while it becomes quite challenging to write correct array operations. I was wondering what the strategy should be. Is there any way to fall back on `for` loops over axis 0, still in a quite efficient way?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"On the larger point of what's easier, for-loops or array-oriented expressions, that's something I've been thinking about since the beginning (my original plans, before 2018, presumed that users would _only_ want explicit loops, albeit in functional map/reduce form). Generally speaking, some things are easier to read (and construct) in an array-oriented way, others are easier with explicit loops. None of this is about what's easier for the computer, it's entirely about what fits with the human mind, and everybody has different subjective ideas about \"easy\" and \"hard.\" Sometimes it's due to background\u2014what you're more familiar with is going to look easier\u2014but that's not 100% of it: there are some programming paradigms that I can't get used to, no matter how long I work with them.\r\n\r\nSo here are two solutions to your problem, and I'll let you decide what's best for you.\r\n\r\n## The array-oriented way\r\n\r\nThe thing you probably needed to know is that [ak.cartesian](https://awkward-array.readthedocs.io/en/latest/_auto/ak.cartesian.html) has a `nested` parameter, which preserves the original list structure when computing a Cartesian product. Your problem then fits an \"explode-flat-reduce\" pattern, which is common enough that I had been talking about it in ancient times ([2016\u20122017](https://github.com/diana-hep/femtocode#explode-operations)). The Cartesian product is the \"explode\" operation.\r\n\r\nFirst, the difference between default `nested=False`:\r\n\r\n```python\r\n>>> ak.cartesian([ak_array_2, ak_array_1]).tolist()\r\n[[(0, 1), (0, 6), (4, 1), (4, 6), (7, 1), (7, 6)], [(2, 2)]]\r\n```\r\n\r\nand `nested=True`:\r\n\r\n```python\r\n>>> ak.cartesian([ak_array_2, ak_array_1], nested=True).tolist()\r\n[[[(0, 1), (0, 6)], [(4, 1), (4, 6)], [(7, 1), (7, 6)]], [[(2, 2)]]]\r\n```\r\n\r\nWith `nested=True`, there's an extra level of list depth, and the nested lists follow the structure of the first array argument, `ak_array_2`. (The `nested` parameter can take more specific arguments than `False` and `True`, which could let you write the two arrays in their original order, `ak_array_1` before `ak_array_2`, but I think that would only complicated it.)\r\n\r\nNow you want to find differences between the lefts and rights of those 2-tuples. That would be a \"flat\" operation, one that doesn't change any nesting structure, just does elementwise math. To do that, we pull the lefts and rights of the tuples into two arrays,\r\n\r\n```python\r\n>>> a2, a1 = ak.unzip(ak.cartesian([ak_array_2, ak_array_1], nested=True))\r\n>>> a2\r\n<Array [[[0, 0], [4, 4], [7, 7]], [[2]]] type='2 * var * var * int64'>\r\n>>> a1\r\n<Array [[[1, 6], [1, 6], [1, 6]], [[2]]] type='2 * var * var * int64'>\r\n```\r\n\r\nbecause that's what makes it possible to put them both in a formula.\r\n\r\n```python\r\n>>> abs(a2 - a1)\r\n<Array [[[1, 6], [3, 2], [6, 1]], [[0]]] type='2 * var * var * int64'>\r\n```\r\n\r\nNow you want a \"reduce\" (or \"implode\") operation: one that finds the minimum in each list. [ak.min](https://awkward-array.readthedocs.io/en/latest/_auto/ak.min.html) will do that if you give it the right `axis` parameter. (For the _deepest_ lists, the most common case, that's `axis=-1`.)\r\n\r\n```python\r\n>>> ak.min(abs(a2 - a1), axis=-1)\r\n<Array [[1, 2, 1], [0]] type='2 * var * ?int64'>\r\n```\r\n\r\nAnd there you go\u2014that's the answer. The integer type is \"optional\" (question mark), meaning there could be `None` values in the result if any lists were empty. If you don't want that, use `mask_identity=False` (in which case, the minimum of an empty list would be infinity for floating point types and MAXINT for integer types, rather than `None`).\r\n\r\n### Recap\r\n\r\nThe above came with a lot of explanation, but the full thing, from start to finish, is just\r\n\r\n```python\r\na2, a1 = ak.unzip(ak.cartesian([ak_array_2, ak_array_1], nested=True))\r\nresult = ak.min(abs(a2 - a1), axis=-1)\r\n```\r\n\r\ntwo lines. (It can't be comfortably done in one line\u2014anything I can think of to shorten it further would only obfuscate the meaning.)\r\n\r\n## The imperative way\r\n\r\nAs you pointed out, you can do it with for loops. To do it _at scale_ (i.e. fast enough for large datasets), you'll want JIT-compilation: Numba. You probably also needed to know about [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html), an imperative way to make Awkward Arrays, or at least [ak.unflatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.unflatten.html), a way to add nested list structure to to an originally flat array.\r\n\r\nFirst, let's do it outside of Numba to get the logic right.\r\n\r\n```python\r\n>>> builder = ak.ArrayBuilder()\r\n>>> for list1, list2 in zip(ak_array_1, ak_array_2):\r\n...     builder.begin_list()\r\n...     for y in list2:\r\n...         best = None\r\n...         for x in list1:\r\n...             if best is None or abs(x - y) < best:\r\n...                 best = abs(x - y)\r\n...         builder.append(best)\r\n...     builder.end_list()\r\n... \r\n>>> result = builder.snapshot()\r\n>>> result\r\n<Array [[1, 2, 1], [0]] type='2 * var * int64'>\r\n```\r\n\r\nThis code is also following some common (imperative) patterns: you compute a running extremum (min or max) by setting a \"best so far\" variable to some neutral value, like `None`, and update it whenever a more extreme value appears. Since we're minimizing, we could have set the initial `best` to infinity for floating point or MAXINT for integer types, and the logic would be the same without the `if best is None` guard. This is the same choice we had in `ak.min`, about whether to set `mask_identity`.\r\n\r\nUnlike the array-oriented solution, which constructed a Cartesian product, this solution iterates over a Cartesian product. Like the array-oriented solution, there's an asymmetry between `ak_array_1` and `ak_array_2`: you want the output to have the structure of `ak_array_2`, so that has to be the outer loop (just as it had to be the first in `ak.cartesian`'s argument list).\r\n\r\nThe above works, but if you have any large datasets, you'll find that iteration over Awkward Arrays is not fast. Awkward Arrays have to do a lot of indirection to produce a given element as a Python object\u2014it would be faster to just turn them into Python built-in objects with [ak.to_list](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_list.html) and iterate over those (which Python has optimized as much as it can) than to iterate over them as Awkward Arrays. But it would be some 100's of times faster to loop over compiled, low-level values than even Python built-ins, so that's what we'll do.\r\n\r\nFirst, get [Numba](https://numba.pydata.org/). That's the JIT-compiler. Awkward Array has Numba extensions so that we can iterate over our arrays in a Numba JIT-compiled function, but _only_ iterate: the array-oriented functions like `ak.concatenate`, advanced slicing, etc. are _not_ available in the JIT-compiled code. (It would be an enormous amount of work: extending Numba is like porting to another language.) So the choice is either/or: array-oriented code outside of Numba, imperative code inside of Numba.\r\n\r\nWe can put the entirety of the for loop in a JIT-compiled function, since it's just iterating over the array structure, and ArrayBuilder operations have also been extended in Numba (less efficiently: building NumPy outputs would be faster, if your output were rectilinear). Constructing the ArrayBuilder and taking its `snapshot` are not possible in the JIT-compiled function because they involve runtime types. With these constraints, the solution becomes:\r\n\r\n```python\r\n>>> @nb.jit\r\n... def compute(builder, array1, array2):\r\n...     for list1, list2 in zip(array1, array2):\r\n...         builder.begin_list()\r\n...         for y in list2:\r\n...             best = None\r\n...             for x in list1:\r\n...                 if best is None or abs(x - y) < best:\r\n...                     best = abs(x - y)\r\n...             builder.append(best)\r\n...         builder.end_list()\r\n...     return builder\r\n... \r\n>>> builder = compute(ak.ArrayBuilder(), ak_array_1, ak_array_2)\r\n>>> result = builder.snapshot()\r\n>>> result\r\n<Array [[1, 2, 1], [0]] type='2 * var * int64'>\r\n```\r\n\r\nAll the caveats I've been describing should be taken as a warning: arbitrary Python code is not possible in Numba, just a numeric subset. They've documented the supported features [here](https://numba.pydata.org/numba-doc/dev/reference/pysupported.html) and [here](https://numba.pydata.org/numba-doc/dev/reference/numpysupported.html). Python, as a language, wasn't designed to be statically compiled (that would be **Julia**), but if you're using Python and you need a way \"out\" to do some imperative number-crunching at scale, Numba's a great way to do it.\r\n\r\n(I once [gave a tutorial](https://youtu.be/X_BJrmofRWQ) on good ways of using Numba. Generally, JIT-compile small functions and only gradually add bells and whistles, so that you know which ones aren't allowed when you hit them. For instance, in the above, we'd be tempted to use ArrayBuilder's `list()` context manager to simplify the `begin_list()`/`end_list()` construction, but context managers aren't in Numba's list of supported Python features. If that wasn't added in a small step, it would be hard to know from the error message that it's the context manager, not something else, that was wrong.)\r\n\r\n## Conclusion\r\n\r\nFor this particular case, I happen to think that the array-oriented solution is easier. However, it relies on \"just knowing some stuff,\" like the existence of the `nested` parameter in `ak.cartesian`, and maybe also familiarity with the explode-flat-reduce pattern, to notice that this is an example of it. There are more complex problems where I'd favor Numba (or Julia), but since it can go either way, I think it's a good idea to keep both doors open.\r\n\r\nBesides, there's no reason why a workflow couldn't include an array-oriented piece that feeds into an imperative piece that feeds into another array-oriented piece, etc. It's a big deal that the same data structure can be used for both, which wouldn't be the case with non-columnar data.",
     "createdAt":"2021-12-19T19:38:31Z",
     "number":1844003,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"fleble"
        },
        "body":"@jpivarski A huge thank you for this very detailed and clear explanation! This is very helpful.\r\n\r\n> Generally speaking, some things are easier to read (and construct) in an array-oriented way, others are easier with explicit loops. None of this is about what's easier for the computer, it's entirely about what fits with the human mind, and everybody has different subjective ideas about \"easy\" and \"hard.\"\r\n\r\nI agree with that statement! I need to get more acquainted with the array-oriented approach and all its potential.\r\n\r\nThank you very much for providing the example with Numba and `ak.ArrayBuilder` and pointing to the supported features with Numba.",
        "createdAt":"2021-12-19T20:59:39Z",
        "number":1844152
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-12-18T11:15:39Z",
  "number":1190,
  "title":"Calculating min distance between two jagged arrays having different jagged structures",
  "url":"https://github.com/scikit-hep/awkward/discussions/1190"
 },
 {
  "author":{
   "login":"pzelasko"
  },
  "body":"Hi there!\r\n\r\nI'm building a library for speech data representation and processing for deep learning called [lhotse](https://github.com/lhotse-speech/lhotse). One of the aspects of speech data is its variable length, so features describing utterances will always have one dynamic dimension. I think it is very difficult to find a project that supports storing ragged arrays like these efficiently, and I was very intrigued to find about awkward array.\r\n\r\nMy question is, what is the preferred format for incremental writing of awkward arrays? I saw mentions of HDF5, arrow, feather, and parquet in the docs, but the examples all seemed to assume that the full array is known ahead of time. This is not the case with iterative feature extraction; we might have 10.000 hours of speech and want to store the arrays as we compute them for later model training.\r\n\r\nI appreciate your help and suggestions.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"For incremental writing, probably Parquet would be best: it would be in batches, but each batch could be a row group. Unfortunately, we don't have an interface set up to keep an output file handle open and write one row group at a time, though it wouldn't be a big modification of what we have.\r\n\r\nOn the other hand, I'm reluctant to add that interface because we're in the process of rewriting the file-writing for Awkward 2.0, so such a modification would have a short life.\r\n\r\nInstead of writing row groups, what about writing a Parquet dataset as a collection of files? That's a standard, well-recognized format (see [Arrow docs](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html)), especially if the collection of files has a METADATA file. The [ak.to_parquet](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_parquet.html) docs describe how to use `ak.to_parquet.dataset` (note the dot: it's a nested function), which wraps a set of already-written Parquet files into a dataset by adding the METADATA.\r\n\r\nWould that work for you: one chunk (as much as will fit in memory) per file, and then treating those files as a dataset?",
     "createdAt":"2021-12-23T15:13:01Z",
     "number":1863935,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"pzelasko"
        },
        "body":"Thanks for getting back to me so quickly. I've been away for some time. \r\n\r\nYour suggestion sounds interesting, although it's a non-trivial modification of what we're doing at the moment: keeping the file handle open for writing, and writing one-by-one. Do you expect the Parquet solution to be roughly as fast as, or faster than, HDF5 for random reads?",
        "createdAt":"2022-01-06T03:28:31Z",
        "number":1916320
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I wasn't suggesting keeping a file handle open\u2013the method described above is to put chunks in separate files in the same directory, then adding a metadata file to that directory with a slightly different command. Then all the files in the directory act as one array.\r\n\r\nThe method of chunks-as-row-groups, rather than chunks-as-files, would require an open file handle, but that method hasn't been implemented. Only the chunks-as-files has.\r\n\r\nHDF5 has a lot more ways of representing rectilinear arrays, some of which can be faster for random reads, particularly if your access has some kind of pattern (i.e. not entirely random, but not necessarily left-to-right sequential, either). Parquet only has a chunk structure, so it's _only_ good for left-to-right sequential access. But Parquet lets you store complex data structures in a columnar way, which HDF5 does not, so it wouldn't be a quantitative question of speed but a categorical question of what data types you have. I assumed that your data are not rectilinear arrays of numbers from the fact that you're using Awkward Array. If your data are rectilinear, I'd use NumPy and HDF5; otherwise, I'd use Awkward and Parquet. I can't think of any cases in which you'd want to cross them.\r\n\r\n (Unless you're using `ak.to_buffers` to bypass HDF5's limitations on rectilinear shapes: https://awkward-array.org/how-to-convert-buffers.html#saving-awkward-arrays-to-hdf5 . I also wouldn't consider HDF5's compound types\u2013they're not columnar.)",
        "createdAt":"2022-01-06T04:26:35Z",
        "number":1916464
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-12-23T02:27:07Z",
  "number":1191,
  "title":"Storing awkward arrays incrementally",
  "url":"https://github.com/scikit-hep/awkward/discussions/1191"
 }
]
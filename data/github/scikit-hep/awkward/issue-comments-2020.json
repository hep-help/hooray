[
 {
  "author_association":"MEMBER",
  "body":"Any progress here?\r\n\r\nYou might want to fast-forward to the latest version of master, since the `FillableArray` \u2192 Awkward arrays conversion no longer has any unimplemented cases. Any `None` (`null`) values are represented by an `IndexedOptionArray` and any differences in types at the same level are represented by a `UnionArray`. If you had to do any work-arounds for missing types in the past, you no longer need to do them. Most of the unfinished bits are on the Python side, so you can write C++ unimpeded.",
  "created_at":"2020-01-20T18:15:02Z",
  "id":576384041,
  "issue":30,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NjM4NDA0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-20T18:15:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for pinging me on this, Jim! I kind of forgot to remove WIP from the title, but the autochecks seem happy, and test_PR026 seems to work with the most recent updates. What's your procedure for finalizing merge requests?",
  "created_at":"2020-01-20T20:36:51Z",
  "id":576423048,
  "issue":30,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NjQyMzA0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-20T20:36:51Z",
  "user":"MDQ6VXNlcjI2OTc1NTMw"
 },
 {
  "author_association":"MEMBER",
  "body":"First, merge your branch to the latest master. That should trigger a test, but if it doesn't, I'll do that manually. Since you've given me the go-ahead, when the tests pass, I'll squash-and-merge it.\r\n\r\nKeep in mind that a squash-and-merge is a lossy procedure: your local git repo will have the only record of the steps you took to complete the PR; GitHub will only have the before and after states. The PR will appear in the commit history as a single commit.",
  "created_at":"2020-01-20T21:01:42Z",
  "id":576429005,
  "issue":30,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NjQyOTAwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-20T21:01:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The branch was already up-to-date with master (aha! c205c991a22bab528b1e5e435029b1052a3f5c7e), so I'm just testing it again. If it passes, I'll merge it.",
  "created_at":"2020-01-20T22:55:34Z",
  "id":576453920,
  "issue":30,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NjQ1MzkyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-20T22:55:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2020-01-04T19:34:10Z",
  "id":570813192,
  "issue":41,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MDgxMzE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-04T19:34:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This will go into 0.1.40 in a moment, once I finish off #40.",
  "created_at":"2020-01-04T19:34:58Z",
  "id":570813247,
  "issue":41,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MDgxMzI0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-04T19:34:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna I just finished some major structural and name changes (which could be hard to merge against if I had left them for later) and now I'm going to put in some stubs for the `flatten` operation, as an indication of how to get started on this. I hope to have it ready for you early next week.",
  "created_at":"2020-01-04T20:19:11Z",
  "id":570816427,
  "issue":42,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MDgxNjQyNw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-04T20:19:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @ianna I just finished some major structural and name changes (which could be hard to merge against if I had left them for later) and now I'm going to put in some stubs for the `flatten` operation, as an indication of how to get started on this. I hope to have it ready for you early next week.\r\n\r\n@jpivarski - would you like me to commit the code to this PR?",
  "created_at":"2020-01-06T08:10:06Z",
  "id":571045571,
  "issue":42,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MTA0NTU3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-06T08:10:06Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Do you have code (somewhere offline)?\r\n\r\nEither we could collaborate on this PR, or (probably better) I could use it to make the stubs, close it off, and you could start a new PR on top of it with the implementations that fill the stubs.",
  "created_at":"2020-01-06T11:15:54Z",
  "id":571101999,
  "issue":42,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MTEwMTk5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-06T11:15:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If you have some time today, maybe I can set up these stubs in a collaborative call, so that I can talk you through what needs to be added and why. I'll be able to start as early as 7:30am U.S. Central time, maybe 8am.",
  "created_at":"2020-01-06T11:27:51Z",
  "id":571105421,
  "issue":42,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MTEwNTQyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-06T11:27:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> If you have some time today, maybe I can set up these stubs in a collaborative call, so that I can talk you through what needs to be added and why. I'll be able to start as early as 7:30am U.S. Central time, maybe 8am.\r\n\r\nYes, let's try this",
  "created_at":"2020-01-06T12:58:58Z",
  "id":571129053,
  "issue":42,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MTEyOTA1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-06T12:58:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The shortest reproducer is:\r\n```python\r\ncheck = ak.Array([{\"x\": b\"b\"}, {\"x\": b\"b\"}, {\"x\": b\"a\"}, {\"x\": b\"b\"}])\r\nprint(check)\r\nprint(ak.tolist(check))\r\nprint(ak.tolist(ak.fromiter(ak.tolist(check))))\r\n```\r\n\r\nwhich gives:\r\n```\r\n[{x: b'b'}, {x: b'b'}, {x: b'a'}, {x: b'b'}]\r\n[{'x': [98]}, {'x': [98]}, {'x': [97]}, {'x': [98]}]\r\n[{'x': [98]}, {'x': [98]}, {'x': [97]}, {'x': [98]}]\r\n```",
  "created_at":"2020-01-05T15:48:31Z",
  "id":570923530,
  "issue":43,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MDkyMzUzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-05T15:48:50Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and you probably don't need a test of `RawArrayOf<T>`. This kind of array is strictly one-dimensional, so `flatten` would always return an error. That's a simple enough case that it can get away without a test.",
  "created_at":"2020-01-07T14:54:17Z",
  "id":571620369,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MTYyMDM2OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-07T14:54:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I just put in an example of a `ListArray` that demonstrates its generality. What's allowed and what isn't allowed is [described here](https://github.com/scikit-hep/awkward-1.0/wiki/ListArray.md).\r\n\r\nWhen you enter the `flatten` implementation, you don't know if the array is in a legal state; the policy is to check as many rules as you need to perform the operation (during the operation).\r\n\r\n`ListArray` is sufficiently general that it will need a new [cpu-kernel](https://github.com/scikit-hep/awkward-1.0/tree/master/include/awkward/cpu-kernels). That's part of what makes this function a good introduction.  :)",
  "created_at":"2020-01-07T16:05:09Z",
  "id":571651783,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MTY1MTc4Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-07T16:05:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Uh-oh: I didn't intend to merge the whole master branch into this branch, but if I'm reading it right, that's what I did with 5dde95c2da437a110bc9f66422f257c4382ec5c8. If that's a problem, you can roll back that commit\u2014I only did it to make sure that the `VERSION_INFO` was ahead of the latest release.",
  "created_at":"2020-01-08T21:10:38Z",
  "id":572258325,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MjI1ODMyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-08T21:10:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Uh-oh: I didn't intend to merge the whole master branch into this branch, but if I'm reading it right, that's what I did with [5dde95c](https://github.com/scikit-hep/awkward-1.0/commit/5dde95c2da437a110bc9f66422f257c4382ec5c8). If that's a problem, you can roll back that commit\u2014I only did it to make sure that the `VERSION_INFO` was ahead of the latest release.\r\n\r\nNo problem, I'll update my branch ",
  "created_at":"2020-01-08T21:15:13Z",
  "id":572260006,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MjI2MDAwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-08T21:15:13Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - it's not finished yet. I just wanted to bring it up-to-date with my local branch. I'm a bit confused with the indices as you can see from the tests :-)\r\n",
  "created_at":"2020-01-09T15:57:33Z",
  "id":572625239,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MjYyNTIzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-09T15:57:33Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - Could you, please, have a look? I'm not sure if I'm going in the right direction. Besides, I need some help here. If I retrieve the indices for the sliced array, I need an offset:\r\n```diff\r\n     array2 = array[2:-1]\r\n-    assert awkward1.tolist(array2.flatten()) == [3.3, 4.4, 5.5]\r\n+    #assert awkward1.tolist(array2.flatten()) == [3.3, 4.4, 5.5]\r\n```\r\n```python\r\n        array2 = array[2:-1]\r\n>       assert awkward1.tolist(array2.flatten()) == [3.3, 4.4, 5.5]\r\nE       assert [0.0, 1.1, 2.2] == [3.3, 4.4, 5.5]\r\nE         At index 0 diff: 0.0 != 3.3\r\nE         Full diff:\r\nE         - [0.0, 1.1, 2.2]\r\nE         + [3.3, 4.4, 5.5]\r\n```",
  "created_at":"2020-01-10T11:50:13Z",
  "id":573005611,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MzAwNTYxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-10T11:50:13Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"First of all, with the `ListArray` defined as it is in the test code (`[[0.0, 1.1, 2.2], [], [5.5], [8.8]]` and _not_ `[[0.0, 1.1, 2.2], [], [3.3, 4.4], [5.5], [6.6, 7.7, 8.8, 9.9]]`), the assertion should be false because `array[2:-1]` of _this array_ is `[[5.5]]` and its flattening is `[5.5]`. It's not an interesting example. (Your `ListArray` example differs from the `ListOffsetArray` example because it's missing the `3:5` subarray and the `6:10` subarray has been shortened to `8:9`.)\r\n\r\nTo see this, try running the pure Python `flatten` function from the top of the test file:\r\n\r\n```python\r\n>>> flatten(awkward1.tolist(array))\r\n[0.0, 1.1, 2.2, 5.5, 8.8]\r\n>>> flatten(awkward1.tolist(array[2:-1]))\r\n[5.5]\r\n```\r\n\r\n## Bigger troubles\r\n\r\nHowever, you're going to have more difficulty when you get to the next block:\r\n\r\n```python\r\ncontent = awkward1.layout.NumpyArray(numpy.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=numpy.int64))\r\nstarts  = awkward1.layout.Index64(numpy.array([4, 999, 0, 0, 1, 7]))\r\nstops   = awkward1.layout.Index64(numpy.array([7, 999, 1, 4, 5, 10]))\r\narray   = awkward1.layout.ListArray64(starts, stops, content)\r\nassert awkward1.tolist(array) == [[4, 5, 6], [], [0], [0, 1, 2, 3], [1, 2, 3, 4], [7, 8, 9]]\r\n```\r\n\r\nThis one not only has gaps, but they're out of order and overlap. Maybe you should concentrate on this one first because of its generality.\r\n\r\n## Suggested strategy\r\n\r\nThere are many ways to implement these functions, and I see that you're following a procedure for `ListArray` that is similar to what was done for `ListOffsetArray`. Here's a better idea: instead of trying to \"fix\" the `ListArray` with a new `starts` and `stops` (you can't because of the gaps, overlaps, and out-of-orderness), write a kernel that performs the flattening as an integer array that will be passed on to the next level down.\r\n\r\nFor instance, with the first `ListArray`:\r\n\r\n```python\r\n>>> awkward1.tolist(array)\r\n[[0.0, 1.1, 2.2], [], [5.5], [8.8]]\r\n>>> flatten(awkward1.tolist(array))\r\n[0.0, 1.1, 2.2, 5.5, 8.8]\r\n```\r\n\r\nYour kernel can create an `Index64` named `nextcarry` that looks like this: `[0, 1, 2, 5, 8]`. Then when you call\r\n\r\n```c++\r\nstd::shared_ptr<Content> out = content_.get()->carry(nextcarry)\r\n```\r\n\r\nthat `out` array will be `[0.0, 1.1, 2.2, 5.5, 8.8]`. \"Carrying\" is an operation that I added to support `getitem`; it does exactly the same thing as this:\r\n\r\n```python\r\n>>> content[[0, 1, 2, 5, 8]]\r\n<NumpyArray format=\"d\" shape=\"5\" data=\"0 1.1 2.2 5.5 8.8\" at=\"0x55f493797580\"/>\r\n```\r\n\r\n(\"gather\" in SIMD and [numpy.take](https://docs.scipy.org/doc/numpy/reference/generated/numpy.take.html) in NumPy). It doesn't matter what kind of an array `content_` is\u2014it could be records or more jagged arrays or whatever.\r\n\r\nBack when all of these things were implemented exclusively in NumPy calls (see [old implementation of flatten](https://github.com/scikit-hep/awkward-array/blob/3442c51ed5dafb7d94f828c6cdc07659f9c03244/awkward/array/jagged.py#L1391-L1417)), we used this trick everywhere: an array of non-negative integers is a partial application of a function on any type of data structure. I came up with [this explanation why](https://github.com/scikit-hep/awkward-1.0/blob/master/docs/theory/arrays-are-functions.pdf): considering element-selection from an array (i.e. passing an integer between square brackets) as a function call, treating that array of length `n`, element type `D` as a function `[0, n) \u2192 D`, and gather/numpy.take is function composition over those function calls:\r\n\r\n```\r\n(   [0, m) \u2192 [0, n)   ) \u2218 (   [0, n) \u2192 D   ) = (   [0, m) \u2192 D   )\r\n```\r\n\r\nFunction composition is associative, so we can apply it whenever we like. In `getitem`, I had to recursively apply an operation that had to carry information from one recursive step to the next, much like carrying a digit in longhand addition, so this operation is called `carry`. It's only used internally, but `ListArray::flatten` would make good use of it.\r\n\r\nSo, given an array like\r\n\r\n```python\r\ncontent = awkward1.layout.NumpyArray(numpy.array([0.0, 1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9]))\r\nstarts  = awkward1.layout.Index64(numpy.array([4, 999, 0, 0, 1, 7]))\r\nstops   = awkward1.layout.Index64(numpy.array([7, 999, 1, 4, 5, 10]))\r\narray   = awkward1.layout.ListArray64(starts, stops, content)\r\nassert awkward1.tolist(array) == [[4.4, 5.5, 6.6], [], [0.0], [0.0, 1.1, 2.2, 3.3], [1.1, 2.2, 3.3, 4.4], [7.7, 8.8, 9.9]]\r\n```\r\n\r\nThe task of your new cpu-kernel would be to generate this `Index64`:\r\n\r\n```\r\n[4, 5, 6, 0, 0, 1, 2, 3, 1, 2, 3, 4, 7, 8, 9]\r\n```\r\n\r\nand then pass that `Index64` as \"`nextcarry`\" to\r\n\r\n```c++\r\ncontent_.get()->carry(nextcarry)\r\n```\r\n\r\nto get\r\n\r\n```python\r\n>>> numpy.asarray(content[[4, 5, 6, 0, 0, 1, 2, 3, 1, 2, 3, 4, 7, 8, 9]])\r\narray([4.4, 5.5, 6.6, 0. , 0. , 1.1, 2.2, 3.3, 1.1, 2.2, 3.3, 4.4, 7.7, 8.8, 9.9])\r\n```\r\n\r\nActually, you'll need two cpu-kernels: the first one adds up all the `stop[i] - start[i]` to determine the length of `nextcarry` before you can allocate it. Then the second cpu-kernel fills it with integers.",
  "created_at":"2020-01-10T15:45:17Z",
  "id":573088356,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MzA4ODM1Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-10T15:45:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - thanks, using the offsets fixed the test. Please, review the PR. ",
  "created_at":"2020-01-10T17:42:01Z",
  "id":573134605,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MzEzNDYwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-10T17:42:01Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, I'd prefer to open a new PR for ```axis != 0```. \r\n\r\nThe tests with NumpyArray work, but I couldn't figure out how to test it on the stride tricks - are they allowed?\r\n```python\r\n>>> a = np.arange(1,10).reshape(3,3)\r\n>>> a\r\narray([[1, 2, 3],\r\n       [4, 5, 6],\r\n       [7, 8, 9]])\r\n>>> np.lib.stride_tricks.as_strided(a, shape=a.shape[::-1], strides=a.strides[::-1])\r\narray([[1, 4, 7],\r\n       [2, 5, 8],\r\n       [3, 6, 9]])\r\n```",
  "created_at":"2020-01-13T11:36:19Z",
  "id":573622142,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MzYyMjE0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-13T11:37:14Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The examples I have in the inline comment, by slicing a e-dimensional array, should provide enough extreme examples of strides.\r\n\r\nThe `stride_trucks` function allows you to create arbitrary shape/stride combinations, some of which could even raise segfaults when iterating over the array. (It's one of the few ways to raise a segfault in pure Python.) But slicing can also make some extreme shape/stride combinations, all of which are legal.\r\n\r\nThe only shape/stride combination I can think of that you can't make with slicing, but can with `stride_tricks` is an axis with `shape != 0` and `stride == 0`. This effectively duplicates a value from the original array `shape` times. Is used to broadcast a scalar or single-valued dimension to match a dimension of length `shape` without actually copying data.\r\n\r\nBut you don't need the low-level `stride_tricks` to do this: there are `broadcast` functions that create such examples more safely.",
  "created_at":"2020-01-13T12:44:53Z",
  "id":573645689,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MzY0NTY4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-13T12:44:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> The examples I have in the inline comment, by slicing a e-dimensional array, should provide enough extreme examples of strides.\r\n> \r\n> The `stride_trucks` function allows you to create arbitrary shape/stride combinations, some of which could even raise segfaults when iterating over the array. (It's one of the few ways to raise a segfault in pure Python.) But slicing can also make some extreme shape/stride combinations, all of which are legal.\r\n> \r\n> The only shape/stride combination I can think of that you can't make with slicing, but can with `stride_tricks` is an axis with `shape != 0` and `stride == 0`. This effectively duplicates a value from the original array `shape` times. Is used to broadcast a scalar or single-valued dimension to match a dimension of length `shape` without actually copying data.\r\n> \r\n> But you don't need the low-level `stride_tricks` to do this: there are `broadcast` functions that create such examples more safely.\r\n\r\nThanks, @jpivarski! I think, I'm done with this PR. What are the next steps? ",
  "created_at":"2020-01-13T17:08:17Z",
  "id":573768786,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3Mzc2ODc4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-13T17:08:17Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I just resolved the merge conflict (version number), removed the `[WIP]` from the name, and the tests have just passed. Are there any intermediate commit states you think are important to save before I squash and merge? If not, then the intermediate history will be lost on GitHub, preserved only in our local clones of the repo.\r\n\r\nBeyond that, I think it would be time to add `axis != 0`, which would be a good introduction to what recursion looks like on columnar data. The first case to add would be `axis > 0`, for which the Python-only `flatten` is a good illustration. Test examples for this case should be pretty deep, like 3 levels or more. It can be particularly tricky crossing the boundary from Awkward Array nodes like `ListArray` and `RegularArray` (each node is at most one \"dimension\") into `NumpyArray` nodes (can be more than one dimension). Some examples should have multidimensional `NumpyArrays`, in which the `axis` picks out one of the NumPy dimensions.\r\n\r\nThe `axis < 0` case, counting from `-1` being the innermost dimension, is hard even to define. For example, is this structure\r\n\r\n![example-hierarchy](https://user-images.githubusercontent.com/1852447/72277720-0cbb8980-35f8-11ea-968e-71abd0960f8a.png)\r\n\r\ntwo-dimensional or three-dimensional (assuming the `NumpyArrays` here are one-dimensional)? Following the `\"x\"` branch, it's a `ListOffsetArray64` of a one-dimensional `NumpyArray`, which looks two-dimensional. Following the `\"y\"` branch, it's a `ListOffsetArray64` of `ListOffsetArray64` of a one-dimensional `NumpyArray`, which looks three-dimensional.\r\n\r\nFor a structure like this, should `axis=-1` mean that we should flatten the nested array inside `\"y\"`? If so, then there would be no way to deal with `\"x\"`. In other cases (where `\"x\"` and `\"y\"` are both multidimensional, perhaps to different degrees), such an `axis=-1` notation would provide ways of flattening that wouldn't be possible (in one step) if `axis=-1` weren't defined this way.\r\n\r\nIf, instead, `axis=-1` means we should only flatten above the `RecordArray`: removing the outermost `ListOffsetArray64` in this example, then it would always be easier to make sense of: it would succeed or fail in the same cases that you've already defined in this PR. However, that would make it impossible to flatten inside of a `RecordArray`\u2014in one step. (The user would have to project the record into its components, flatten each, and recombine them. That sounds annoying, but it's a special case that probably doesn't come up often.)\r\n\r\nI'm inclined to define `axis=-1` to be above the `RecordArray`. Since all operations should adhere to the same conventions, this means that subsequent operations should also only descend to the first `RecordArray` when determining what `axis=-1` means. There would need to be a new virtual method, `list_depth`, beside `minmax_depth` to do this counting and convert negative `axis` values into non-negative ones before recursing. All of that would be the subject of the new PR that adds `axis != 0` to `flatten`.",
  "created_at":"2020-01-13T17:42:45Z",
  "id":573783659,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3Mzc4MzY1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-13T17:45:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"When you're sure that intermediate commits don't have to be preserved, give me the okay and I'll squash-and-merge. Thanks!",
  "created_at":"2020-01-13T17:50:08Z",
  "id":573786681,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3Mzc4NjY4MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-13T17:50:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> When you're sure that intermediate commits don't have to be preserved, give me the okay and I'll squash-and-merge. Thanks!\r\n\r\nThanks, yes, you can squash-and-merge it.",
  "created_at":"2020-01-13T21:45:39Z",
  "id":573886195,
  "issue":45,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3Mzg4NjE5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-13T21:45:39Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Even stranger outcomes:\r\n```python\r\nak.tolist(ak.Array([{'x': 3, 'extreme': {'pt': 3.3, 'charge': -1, 'iso': 100}}, {'x': 3, 'what': 3}]))\r\n```\r\ngives:\r\n```\r\n[{'x': 3, 'extreme': {'pt': 3.3, 'charge': -1, 'iso': 100}, 'what': None}]\r\n```",
  "created_at":"2020-01-09T00:30:08Z",
  "id":572323774,
  "issue":47,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MjMyMzc3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-09T00:30:08Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"The incorrect behavior is that missing fields are supposed to be filled with `None`, but weren't (backward in time in your first example, forward in time in the second). The `FillableArray` is supposed to ensure that the `length` of each field is equal to the `length` of the record itself, and without those `None` values, it isn't. If the fields of a `RecordArray` do not have equal lengths, the `RecordArray` is said to have the length of the shortest of its fields. (This allows filling processes to fill arrays gradually and have the presented array be eventually consistent.)\r\n\r\nWhat happened here is that an old `while` loop was turned into a `for` loop without getting rid of the `i++` at the end, so `i++` was happening twice. The `None`-padding _was only performed for even-numbered fields!_\r\n\r\nFixed for both `RecordFillable` and `TupleFillable`:\r\n\r\n```diff\r\ndiff --git a/src/libawkward/fillable/RecordFillable.cpp b/src/libawkward/fillable/RecordFillable.cpp\r\nindex 5a00255..e6ebce0 100644\r\n--- a/src/libawkward/fillable/RecordFillable.cpp\r\n+++ b/src/libawkward/fillable/RecordFillable.cpp\r\n@@ -389,7 +389,6 @@ namespace awkward {\r\n         if (contents_[i].get()->length() != length_ + 1) {\r\n           throw std::invalid_argument(std::string(\"record field \") + util::quote(keys_[i], true) + std::string(\" filled more than once\"));\r\n         }\r\n-        i++;\r\n       }\r\n       length_++;\r\n       begun_ = false;\r\ndiff --git a/src/libawkward/fillable/TupleFillable.cpp b/src/libawkward/fillable/TupleFillable.cpp\r\nindex 2d4e53d..b8a55d5 100644\r\n--- a/src/libawkward/fillable/TupleFillable.cpp\r\n+++ b/src/libawkward/fillable/TupleFillable.cpp\r\n@@ -243,7 +243,6 @@ namespace awkward {\r\n         if (contents_[i].get()->length() != length_ + 1) {\r\n           throw std::invalid_argument(std::string(\"tuple index \") + std::to_string(i) + std::string(\" filled more than once\"));\r\n         }\r\n-        i++;\r\n       }\r\n       length_++;\r\n       begun_ = false;\r\n```\r\n\r\nWell, that was dumb. Thanks for catching it! PR #48, will be deployed as 0.1.47.",
  "created_at":"2020-01-09T15:37:47Z",
  "id":572616369,
  "issue":47,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MjYxNjM2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-09T15:37:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This came up while writing CHEP proceedings. Originally, the testing code raised an error because it attempted to apply the `Point` class at both the `RecordArray` and the `Record` levels (since layout types specify the types of their elements, a `Record` has only one type to speak of, and it's the same as the `RecordArray`'s element type).",
  "created_at":"2020-01-10T13:53:06Z",
  "id":573043847,
  "issue":49,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3MzA0Mzg0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-10T13:53:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Resolved by #81.",
  "created_at":"2020-01-15T01:50:23Z",
  "id":574457966,
  "issue":50,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDQ1Nzk2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-15T01:50:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"See #69 for further discussion of how to interpret negative `axis`.",
  "created_at":"2020-01-14T15:54:38Z",
  "id":574242180,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDI0MjE4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T15:54:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The `axis` parameter for `count` (a special-case reducer introduced in PR #81) should be handled in the same way as the `axis` parameter for `flatten`. This issue (#51) should add `axis > 0` and `axis < 0` cases to both `flatten` and `count`.",
  "created_at":"2020-01-15T01:15:31Z",
  "id":574450173,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDQ1MDE3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-15T01:15:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna I added a lot of issues yesterday, which cover the gap from the present to the [minimum viable product for users](https://github.com/scikit-hep/awkward-1.0/milestone/4) that is expected at the end of the 6-month sprint. (Every item in the [README checklist for the 6-month sprint](https://github.com/scikit-hep/awkward-1.0#checklist-of-features-for-the-six-month-sprint) has been translated into an issue.)\r\n\r\nI've assigned the array operations to you (`flatten` and `count` being the first two) and the new array types, Python front-end, and Numba implementation to me. That's completely negotiable.\r\n\r\nIf you're wondering where to get started, it's this one (issue #51), which we discussed before. Looking ahead to operations like the reducers in #69 has consequences for how negative `axis` should be interpreted, so it would be good to read ahead about what's coming.",
  "created_at":"2020-01-15T15:23:49Z",
  "id":574709541,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDcwOTU0MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-15T15:23:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @ianna I added a lot of issues yesterday, which cover the gap from the present to the [minimum viable product for users](https://github.com/scikit-hep/awkward-1.0/milestone/4) that is expected at the end of the 6-month sprint. (Every item in the [README checklist for the 6-month sprint](https://github.com/scikit-hep/awkward-1.0#checklist-of-features-for-the-six-month-sprint) has been translated into an issue.)\r\n> \r\n> I've assigned the array operations to you (`flatten` and `count` being the first two) and the new array types, Python front-end, and Numba implementation to me. That's completely negotiable.\r\n> \r\n> If you're wondering where to get started, it's this one (issue #51), which we discussed before. Looking ahead to operations like the reducers in #69 has consequences for how negative `axis` should be interpreted, so it would be good to read ahead about what's coming.\r\n\r\nThanks, @jpivarski - Excellent! I thought it was a plan for the end of February :-)\r\n\r\nI'm done with **NumpyArray** for ```axis != 0```, not yet ```axis == -1```.  I need to read the issues for the latter.\r\nI am adding more extensive tests and will open a PR either later today or tomorrow morning.",
  "created_at":"2020-01-15T15:39:34Z",
  "id":574716917,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDcxNjkxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-15T15:39:34Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, good! With your work on `NumpyArray`, however, I put in some corrections to your initial implementation (PR #45) as part of adding `IndexedArray::flatten` yesterday (PR #81). Specifically, this:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/428c3db9745af34798f3f970ffa48e2db009485b/src/libawkward/array/NumpyArray.cpp#L628-L634\r\n\r\nIt's a potential merging hazard!",
  "created_at":"2020-01-15T15:43:49Z",
  "id":574718893,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDcxODg5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-15T15:43:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Oh, good! With your work on `NumpyArray`, however, I put in some corrections to your initial implementation (PR #45) as part of adding `IndexedArray::flatten` yesterday (PR #81). Specifically, this:\r\n> \r\n> https://github.com/scikit-hep/awkward-1.0/blob/428c3db9745af34798f3f970ffa48e2db009485b/src/libawkward/array/NumpyArray.cpp#L628-L634\r\n> \r\n> It's a potential merging hazard!\r\n\r\nNo problem, I'll start with a clean area.",
  "created_at":"2020-01-15T15:46:14Z",
  "id":574720037,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDcyMDAzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-15T15:46:14Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Finished in PR #82 (@lgray). I'm going to finish issue #53 in the same PR before merging it, though.",
  "created_at":"2020-01-15T21:26:20Z",
  "id":574863490,
  "issue":52,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDg2MzQ5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-15T21:26:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Awesome!",
  "created_at":"2020-01-15T22:22:55Z",
  "id":574884994,
  "issue":52,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDg4NDk5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-15T22:22:55Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Finished in PR #82.",
  "created_at":"2020-01-16T02:07:07Z",
  "id":574946904,
  "issue":53,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDk0NjkwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-16T02:07:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"And `FillableUnion` \u2192 `UnionArray`.\r\n\r\nWith that, all JSON documents can be converted to Awkward Arrays.",
  "created_at":"2020-01-14T14:52:20Z",
  "id":574210501,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDIxMDUwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T14:52:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Might need to update [UnionArray wiki page](https://github.com/scikit-hep/awkward-1.0/wiki/UnionArray.md).",
  "created_at":"2020-01-14T15:00:12Z",
  "id":574214406,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDIxNDQwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T15:00:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Finished by #84 (except for Numba implementation).",
  "created_at":"2020-01-16T23:45:39Z",
  "id":575399744,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTM5OTc0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-16T23:45:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"After a long conversation with Nick, we decided that this functionality should become part of the VirtualArray. I'm updating this on #57.",
  "created_at":"2020-02-21T21:45:29Z",
  "id":589850336,
  "issue":55,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4OTg1MDMzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-21T21:45:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This would not be a `Content` type, but something that can hold `Contents` as chunks. Thus, ChunkedArray is always the outermost layer (root of the tree) and can't be nested.\r\n\r\nThere's also a use-case for chunks at the leaves, which might be handled by an alternative to NumpyArray someday. But that is not this issue.",
  "created_at":"2020-03-18T05:31:58Z",
  "id":600433017,
  "issue":56,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDQzMzAxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-18T05:31:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Since this array type can only be used at the root of a structure, unlike [Arrow's ChunkedArray](https://arrow.apache.org/docs/cpp/arrays.html#chunked-arrays), it should have a different name to avoid confusion with Arrow's. This root-level chunking is called\r\n\r\n   * Record batches (Arrow)\r\n   * Row groups (Parquet)\r\n   * Clusters (ROOT)\r\n   * Partitions (Spark)\r\n\r\nI think \"partitions\" is the least jargony, the least likely to be confused for something else. So the name of this Awkward type will be **PartitionedArray**.\r\n\r\n(@nsmith- @lgray)",
  "created_at":"2020-04-13T18:43:33Z",
  "id":613035847,
  "issue":56,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMzAzNTg0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T18:43:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Outcome of a long conversation with @nsmith-, in which we went through all the consequences of the following:\r\n\r\n   * VirtualArrays need to take a callable `Closure` C++ instance as an argument. The `Closure` abstract class will be instantiated for Python as a `PyClosure` (with appropriate data-translation and reference counting) and as a `SlicedClosure` (see below). In principle, pure-C++ closures would be supported, but that's not the main intent or first implementation.\r\n   * All materialized data would be stored in a `Cache` C++ instance, similarly special-cased as `PyCache`, which supports the `MutableMapping` interface in Python. No data would be attached to the VirtualArray itself, as this has been a cause of memory leaks in Awkward0. (\"Leaking\" in the sense that the user didn't know where the memory was being held. By centralizing everything in a cache, we can cap it or explicitly flush it.)\r\n   * All operations on a VirtualArray (except one) cause it to be materialized, which means the output of the operation is not a VirtualArray, but a node corresponding to the computed output.\r\n   * The one exception is single-level getitem (which is why #55 was closed; VirtualArray will take over the proposed operation of SlicedArray). If a single-level getitem is received, the `SliceItem` is put in a `SlicedClosure` to make a new VirtualArray. In principle, repeated getitem attempts make larger `SlicedClosures`, but we don't want to compose them into a single `SlicedClosure` because (a) repeated filtering would be rare in a physics analysis\u2014it complicates the analysis because each array of booleans would have to have a different length\u2014and (b) such a composition would be independently applied at each VirtualArray in a RecordArray of many fields, duplicating effort and memory usage. It's better to let each element of the `SlicedClosure` chain point to the same `shared_ptr<int64_t>` for all fields in a RecordArray of VirtualArrays.\r\n\r\nA `getitem_at` and `getitem(SliceArray64)` with `advanced` are not single-level slices, but `SliceRange`, `SliceField`, `SliceFields`, etc. are. `SliceNewAxis` can be applied without materializing the VirtualArray and `SliceEllipsis` requires materialization because we don't know how deep it goes. It's only a single-level slice if the `tail` is empty.\r\n\r\n   * When a VirtualArray makes a new VirtualArray with a new `SlicedClosure`, it is not assigned any cache. If the sliced VirtualArray is repeatedly accessed, it will always reevaluate the slice (though the unsliced data may be fetched from its cache, depending on eviction policies).\r\n   * Attempts to cache sliced VirtualArrays should be a separate application, which is why it's important for `SlicedClosures` to be inspectable externally, in Python. Each of the `SliceItem` types will have to be convertible back into their Python equivalents.\r\n   * Persistent keys should be `parameters`, maybe named `\"__persistent_key__\"`.\r\n   * VirtualArrays in Numba can either materialize upon entry into the JIT-compiled function (bad for naive users, since that might mean `for event in events` of a lazy `events` is super-slow) or they have to contain a full layout description (all nodes but no lengths). That can wait.",
  "created_at":"2020-02-21T22:23:57Z",
  "id":589862540,
  "issue":57,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4OTg2MjU0MA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-02-21T22:23:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I should do ByteMaskedArray before BitMaskedArray, as preparation. BitMaskedArray is just like it but with the additional complication of having to deal with bits. Probably just generate bytes when needed, leaving direct bit-manipulation as a later optimization.\r\n\r\nAs soon as ByteMaskedArray is ready, reducers should get a `semigroup` parameter to turn placeholder-identities into `None` (expected for min/max, and probably necessary before #70).",
  "created_at":"2020-02-13T19:08:01Z",
  "id":585922734,
  "issue":58,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NTkyMjczNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-13T19:08:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"At that time, also push `None` values in a reduction down to just above the primitive level with ByteMaskedArray, and use that to propagate `Nones` in the result. But it will be much easier with a ByteMaskedArray than an IndexedOptionArray.",
  "created_at":"2020-02-13T19:48:15Z",
  "id":585940266,
  "issue":58,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NTk0MDI2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-13T19:48:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Finished in PR #86.",
  "created_at":"2020-01-21T05:18:16Z",
  "id":576522108,
  "issue":60,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NjUyMjEwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-21T05:18:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Done in PR #89. `ak.where` \u2194 `np.where` is the first demonstrating example.",
  "created_at":"2020-01-24T23:29:49Z",
  "id":578341988,
  "issue":61,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3ODM0MTk4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-24T23:29:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In both Python and Numba.",
  "created_at":"2020-01-14T15:23:35Z",
  "id":574226632,
  "issue":62,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDIyNjYzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T15:23:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@mhedges This is a port of what you implemented in Awkward 0.x. I'll be following the example you set. (Thanks!)",
  "created_at":"2020-01-14T18:19:34Z",
  "id":574306162,
  "issue":63,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDMwNjE2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T18:19:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Implemented by PR #92.",
  "created_at":"2020-01-28T21:51:19Z",
  "id":579474171,
  "issue":63,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTQ3NDE3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-28T21:51:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"And also `ak.FillableArray`.",
  "created_at":"2020-01-14T15:24:22Z",
  "id":574226966,
  "issue":64,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDIyNjk2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T15:24:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This would get superseded by #95.",
  "created_at":"2020-02-16T05:32:48Z",
  "id":586671369,
  "issue":64,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NjY3MTM2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-16T05:32:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Implemented by PR #92.",
  "created_at":"2020-01-28T21:50:55Z",
  "id":579473995,
  "issue":65,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTQ3Mzk5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-28T21:50:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Putting here an example of the pyarrow behavior:\r\n```python\r\nIn [1]: import pyarrow as pa\r\n\r\nIn [2]: pa.array(range(5))\r\nOut[2]:\r\n<pyarrow.lib.Int64Array object at 0x112289c90>\r\n[\r\n  0,\r\n  1,\r\n  2,\r\n  3,\r\n  4\r\n]\r\n\r\nIn [3]: pa.array(range(5)).take(pa.array([1, None, 2]))\r\nOut[3]:\r\n<pyarrow.lib.Int64Array object at 0x1122dd130>\r\n[\r\n  1,\r\n  null,\r\n  2\r\n]\r\n```",
  "created_at":"2020-01-16T20:43:42Z",
  "id":575337705,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTMzNzcwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-16T20:43:42Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"pyarrow doesn't support it, but a logical extension should also do this:\r\n\r\n```python\r\n>>> py.array(range(5)).compress(py.array([False, True, None, None, True])\r\n[\r\n   1,\r\n   null,\r\n   null,\r\n   4\r\n]\r\n```\r\n\r\nOf course, \"[compress](https://docs.scipy.org/doc/numpy/reference/generated/numpy.compress.html)\" is a terrible name, and pyarrow's [compress](https://arrow.apache.org/docs/python/generated/pyarrow.compress.html) function does the more logical thing: lossless compression. However, when these are used in `__getitem__` without special names like `take` and `compress`, the above is what a user would expect.",
  "created_at":"2020-02-06T20:37:30Z",
  "id":583099118,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzA5OTExOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-06T20:37:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Step 1 is done (in PR #111):\r\n\r\n```python\r\n>>> ak.Array(range(5))[ak.Array([1, None, 2])]\r\n<Array [1, None, 2] type='3 * ?int64'>\r\n```",
  "created_at":"2020-02-07T22:51:35Z",
  "id":583650293,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzY1MDI5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-07T22:51:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Step 2 is done (also in PR #111):\r\n\r\n```python\r\n>>> ak.Array(range(5))[ak.Array([False, True, None, None, True])]\r\n<Array [1, None, None, 4] type='4 * ?int64'>\r\n```",
  "created_at":"2020-02-08T01:59:51Z",
  "id":583689189,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzY4OTE4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-08T01:59:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"And all the jagged slices:\r\n\r\n```python\r\n>>> array = ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5], [6.6], [7.7, 8.8, 9.9]])\r\n```\r\n```python\r\n>>> ak.tolist(array[[[0, -1], [], [], [0, 0, 0], [-1, -2, -3]]])\r\n[[1.1, 3.3], [], [], [6.6, 6.6, 6.6], [9.9, 8.8, 7.7]]\r\n```\r\n```python\r\n>>> ak.tolist(array[[[0, None, -1], [None], [], [0, None, 0], [-1, -2, -3]]])\r\n[[1.1, None, 3.3], [None], [], [6.6, None, 6.6], [9.9, 8.8, 7.7]]\r\n```\r\n```python\r\n>>> ak.tolist(array[[[0, -1], None, [], [], None, [0, 0, 0], [-1, -2, -3]]])\r\n[[1.1, 3.3], None, [], [], None, [6.6, 6.6, 6.6], [9.9, 8.8, 7.7]]\r\n```\r\n```python\r\n>>> ak.tolist(array[[[0, None, -1], None, [None], [], None, [0, 0, 0], [-1, -2, -3]]])\r\n[[1.1, None, 3.3], None, [None], [], None, [6.6, 6.6, 6.6], [9.9, 8.8, 7.7]]\r\n```",
  "created_at":"2020-02-09T06:00:32Z",
  "id":583808217,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzgwODIxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-09T06:00:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"And jagged mask (almost forgot the most important case!):\r\n\r\n```python\r\n>>> ak.tolist(array[[[False, False, True], [], [True, True], [False], [True, False, True]]])\r\n[[3.3], [], [4.4, 5.5], [], [7.7, 9.9]]\r\n```",
  "created_at":"2020-02-09T07:36:38Z",
  "id":583814755,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzgxNDc1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-09T07:36:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This can also have `None`:\r\n\r\n```python\r\n>>> ak.tolist(array[[[False, False, True], None, [], None, [True, True], [False], [True, False, True]]])\r\n[[3.3], None, [], None, [4.4, 5.5], [], [7.7, 9.9]]\r\n```",
  "created_at":"2020-02-09T07:38:03Z",
  "id":583814849,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzgxNDg0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-09T07:38:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Getting `None` values in the inner layer (correctly across jagged boundaries) was more difficult, but it's done now:\r\n\r\n```python\r\n>>> ak.tolist(array[[[False, True, None], [None], [None, True], [False], [True, False, True]]])\r\n[[2.2, None], [None], [None, 5.5], [], [7.7, 9.9]]\r\n```\r\n\r\nYou can even do them at both levels.  `:)`\r\n\r\n```python\r\n>>> ak.tolist(array[[[False, True, None], None, [None], None, [None, True], [False], [True, False, True]]])\r\n[[2.2, None], None, [None], None, [None, 5.5], [], [7.7, 9.9]]\r\n```\r\n\r\nSo this issue is closed. The `tests/test_PR111_jagged_and_masked_getitem.py` is much more extensive.",
  "created_at":"2020-02-09T10:46:43Z",
  "id":583830535,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzgzMDUzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-09T10:46:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Most expected users of Awkward \u2194 Arrow conversion are using Python, not C++. Writing a converter in C++ is not a performance consideration, but an accessibility one\u2014it allows pure C++ programs to exchange structured data with other programs in the Arrow ecosystem. However, writing it would mean spinning off a separate package, since Awkward can't take on Arrow as a dependency; the Awkward-Arrow package would have to depend on both Awkward and Arrow, which is too much to deal with right now.\r\n\r\nFor the time being, Awkward \u2194 Arrow conversion will be a Python function. (And that Python function can continue to exist as a fallback if the Awkward-Arrow package isn't accessible).",
  "created_at":"2020-03-04T12:47:56Z",
  "id":594500560,
  "issue":68,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NDUwMDU2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-04T12:47:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"On second thought, there's this: https://github.com/apache/arrow/blob/master/docs/source/format/CDataInterface.rst\r\n\r\nWe may have a minimal-dependency way to consume and produce Arrow buffers after all. (Need to check on the status of that from Arrow.)",
  "created_at":"2020-03-13T02:38:38Z",
  "id":598520012,
  "issue":68,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODUyMDAxMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-03-13T02:38:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It looks promising.",
  "created_at":"2020-03-13T06:19:58Z",
  "id":598569470,
  "issue":68,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODU2OTQ3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-13T06:19:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"My reading of this ([Arrow JIRA ticket](https://issues.apache.org/jira/browse/ARROW-7912) and [pull request](https://github.com/apache/arrow/pull/6040)) is that this human-readable specification is the entirety of the C interface. There's no code other than what we see on the instructions page. We're supposed to copy its [struct definitions](https://github.com/apache/arrow/blob/master/docs/source/format/CDataInterface.rst#structure-definitions) into our project, populate them according to the rules on the page, and that's it: the in-memory buffer we've just made is an Arrow buffer. It would be nice to see an example of wrapping that buffer in `pyarrow` and verifying that the data can be round-tripped, but once we (I or someone else) figure out how to do that, we can submit such an example as a PR to [apache/arrow](https://github.com/apache/arrow).",
  "created_at":"2020-03-16T18:45:20Z",
  "id":599701626,
  "issue":68,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5OTcwMTYyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-16T18:45:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I can't make it an \"assignment,\" but @trickarcher is actively working on this.",
  "created_at":"2020-04-17T23:44:49Z",
  "id":615512367,
  "issue":68,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTUxMjM2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T23:44:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In #51 and [here](https://github.com/scikit-hep/awkward-1.0/pull/45#issuecomment-573783659), I said that negative `axis` should count upward from the first `RecordArray`; that is, `axis=-1` would mean the outermost `ListOffsetArray64` in\r\n\r\n![example-hierarchy](https://user-images.githubusercontent.com/1852447/72277720-0cbb8980-35f8-11ea-968e-71abd0960f8a.png)\r\n\r\ninstead of the innermost `ListOffsetArray64` (pointed to by `contents[\"y\"]`). Although that seems like a good choice for `flatten`, _it would be a bad choice for reducers_. It's quite common to want to reduce the innermost level of a tree, even if they're inside of a `RecordArray`. In fact, awkward 0.x _always_ applies to the innermost level; it was written before [I realized that other levels were conceptually possible](https://github.com/scikit-hep/awkward-array/issues/166#issuecomment-514802933).\r\n\r\nSo negative `axis` parameters should count up from the leaves of the tree, passing right through any `RecordArrays`. The leaves of the tree might be at different levels, so a negative `axis` defined this way would do things that aren't possible with a positive `axis` (unlike NumPy).\r\n\r\n**This has consequences for `flatten`:** the `axis` should not be interpreted different ways in different functions, so `flatten` should count up from the leaves of the tree, too!",
  "created_at":"2020-01-14T16:02:13Z",
  "id":574245738,
  "issue":69,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDI0NTczOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T16:02:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Worth mentioning: the existence of reducers will also enable several other essential functions (though they are not themselves reducers):\r\n\r\n```python\r\ndef moment(self, n, weight=None):\r\n   \"Compute the n-th moment of an array with optional weight.\"\r\n    with self.numpy.errstate(invalid=\"ignore\"):\r\n        if weight is None:\r\n            return self.numpy.true_divide((self**n).sum(), self.count())\r\n        else:\r\n            return self.numpy.true_divide(((self * weight)**n).sum(), (self * 0 + weight).sum())\r\n\r\ndef mean(self, weight=None):\r\n   \"Compute the mean (average) of an array with optional weight.\"\r\n    with self.numpy.errstate(invalid=\"ignore\"):\r\n        if weight is None:\r\n            return self.numpy.true_divide(self.sum(), self.count())\r\n        else:\r\n            return self.numpy.true_divide((self * weight).sum(), (self * 0 + weight).sum())\r\n\r\ndef var(self, weight=None, ddof=0):\r\n   \"Compute the variance of an array with optional weight and possibly reduce it with a number of degrees of freedom.\"\r\n    with self.numpy.errstate(invalid=\"ignore\"):\r\n        if weight is None:\r\n            denom = self.count()\r\n            one = self.numpy.true_divide(self.sum(), denom)\r\n            two = self.numpy.true_divide((self**2).sum(), denom)\r\n        else:\r\n            denom (self * 0 + weight).sum()\r\n            one = self.numpy.true_divide((self * weight).sum(), denom)\r\n            two = self.numpy.true_divide(((self * weight)**2).sum(), denom)\r\n        if ddof != 0:\r\n            return (two - one**2) * denom / (denom - ddof)\r\n        else:\r\n            return two - one**2\r\n\r\ndef std(self, weight=None, ddof=0):\r\n   \"Compute the standard deviation of an array with optional weight and possibly reduce it with a number of degrees of freedom.\"\r\n    with self.numpy.errstate(invalid=\"ignore\"):\r\n        return self.numpy.sqrt(self.var(weight=weight, ddof=ddof))\r\n```",
  "created_at":"2020-01-14T16:06:39Z",
  "id":574247739,
  "issue":69,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDI0NzczOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T16:06:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think for argmax indeed it makes much more sense to reduce the dimension by 1, reflecting the behavior of max.",
  "created_at":"2020-01-16T21:20:23Z",
  "id":575352106,
  "issue":70,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTM1MjEwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-16T21:20:23Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Also relevant: the new `ak.singletons` function in #198. This is needed to turn the `ak.argmax` output into something that can select one object from each list.\r\n\r\nMaybe there will also need to be a convenient syntax for it, since the examples in the tutorial are looking like:\r\n\r\n```python\r\n#  select <- to singletons <- argmax <- the cut\r\nevents.pions[ak.singletons(ak.argmax(abs(events.pions.vtx), axis=1))]\r\n```",
  "created_at":"2020-04-06T20:56:12Z",
  "id":610032308,
  "issue":70,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMDAzMjMwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-06T20:56:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"What about `isna`? I feel like `dropna` could just be a composition of `isna` and slicing.",
  "created_at":"2020-01-14T17:59:00Z",
  "id":574298078,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDI5ODA3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T17:59:00Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> What about `isna`? I feel like `dropna` could just be a composition of `isna` and slicing.\r\n\r\nThis is an excellent point. I had intended to make an issue for `isna` but got lost among all the items. As you point out, if we had an `isna`, we wouldn't need a `dropna` because slicing does that already.\r\n\r\nOr... hold that thought. There might be cases inside of `RecordArrays` where it would be very difficult to use slicing. At least, `isna` should be a first priority and we'll see if `dropna` is really needed after that.",
  "created_at":"2020-01-14T18:15:03Z",
  "id":574304387,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDMwNDM4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T18:15:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Implemented by PR #92 (and touched-up by 960baf57f43a638cbd46032d20cf9d972243f9ae). `ak.isna` and `ak.notna` were needed for Pandas.\r\n\r\nI'll close this to see if the need for `ak.dropna` ever comes up in practice.",
  "created_at":"2020-01-28T21:59:30Z",
  "id":579477719,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTQ3NzcxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-28T21:59:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'd like to re-open this to make sure `ak.isna` has an axis argument, as discussed in https://github.com/scikit-hep/awkward-array/issues/227",
  "created_at":"2020-01-31T18:52:11Z",
  "id":580862728,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDg2MjcyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-31T18:52:11Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"This was finished in PR #155.",
  "created_at":"2020-03-17T21:21:10Z",
  "id":600307405,
  "issue":72,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDMwNzQwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T21:21:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I'm done with ```axis==0```, I think. The auto generated ```Content``` gets padded  without assertions. Here are some thoughts on implementation, please, comment.\r\n\r\n * The ```length``` in ```pad(length, axis)``` can be interpreted as a desired ```length``` of the array, e.g. ```tolength```, or as a padding with ```length``` so that ```array.length() + length == array.pad(length).length()```. I've implemented the latter.\r\n\r\n * The ```NumpyArray``` padding can be implemented extending its shape or not. The first is similar to numpy padding and can be extended to padding with any value. The latter adds correct number of ```None```'s at ```axis```. I'd think, this is desired behaviour, but I've implemented both.\r\n\r\nFor example:\r\n```python\r\narray = NumpyArray([ 0,  1,  2,  3,  4, 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],[2, 3, 5], [15, 5, 1], 0)\r\nassert list(array) == [[[ 0,  1,  2,  3,  4], [ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]],\r\n                            [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]]]\r\nassert list(array.pad(3,0)) == [[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]], [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]], [[None, None, None, None, None], [None, None, None, None, None], [None, None, None, None, None]], [[None, None, None, None, None], [None, None, None, None, None], [None, None, None, None, None]], [[None, None, None, None, None], [None, None, None, None, None], [None, None, None, None, None]]]\r\nassert list(array.pad(3,1)) == [[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [None, None, None, None, None], [None, None, None, None, None], [None, None, None, None, None]],\r\n                                    [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29], [None, None, None, None, None], [None, None, None, None, None], [None, None, None, None, None]]]\r\nassert list(array.pad(3,2)) == [[[ 0,  1,  2,  3,  4, None, None, None], [ 5,  6,  7,  8,  9, None, None, None], [10, 11, 12, 13, 14, None, None, None]],\r\n                                    [[15, 16, 17, 18, 19, None, None, None], [20, 21, 22, 23, 24, None, None, None], [25, 26, 27, 28, 29, None, None, None]]]\r\n```\r\nvs\r\n\r\n```python\r\nassert list(array.pad_no_shape(3,0)) == [[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]], [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]], None, None, None]\r\nassert list(array.pad_no_shape(3,1)) == [[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], None, None, None], [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29], None, None, None]]\r\nassert list(array.pad_no_shape(3,2)) == [[[0, 1, 2, 3, 4, None, None, None], [5, 6, 7, 8, 9, None, None, None], [10, 11, 12, 13, 14, None, None, None]], [[15, 16, 17, 18, 19, None, None, None], [20, 21, 22, 23, 24, None, None, None], [25, 26, 27, 28, 29, None, None, None]]]\r\n```",
  "created_at":"2020-02-17T17:58:04Z",
  "id":587104159,
  "issue":73,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NzEwNDE1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-17T17:58:04Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't know that NumPy had a `pad` function, but [there it is](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). That function seems to be for giving image processing applications a little more room for edge effects. It doesn't even have an `axis` parameter.\r\n\r\nOur `pad` was introduced because some physics analyses needed it and it was designed for that use-case ([here is the old code](https://github.com/scikit-hep/awkward-array/blob/a2645fdaed1a6997c4677ae47cbb2cd0663e8a21/awkward/array/jagged.py#L1843-L1892)). Perhaps it should be called `rpad` to avoid confusion with `numpy.pad`. (I just changed the poster to say `rpad`!)\r\n\r\nThe way I defined it before, my `axis=0` was your `axis=1`. However, I think your definition makes more sense and we should go with that. It's the `pad_no_shape` version of the above that we should be using. I see that your `pad` (without `_no_shape`) does what `numpy.pad` does, and I'm sorry for the confusion.\r\n\r\nSo I think your `pad_no_shape` is the definition we should use, with your definition of `axis`. The next step would be to replace your implementation that generates UnionArrays with one that generates IndexedOffsetArrays. When all NumpyArrays are one-dimensional, this allows you to add the padding without ever modifying the array that's getting padded. For instance:\r\n\r\n```python\r\n>>> import awkward1 as ak, numpy as np\r\n>>> from awkward1.layout import *\r\n>>> array = ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]]).layout\r\n>>> array\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 3 3 5]\" offset=\"0\" at=\"0x5643ef230880\"/></offsets>\r\n    <content><NumpyArray format=\"d\" shape=\"5\" data=\"1.1 2.2 3.3 4.4 5.5\" at=\"0x5643ef232890\"/></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\n```python\r\n>>> padindex = Index64(np.array([0, 1, 2, -1, -1, -1, 3, 4, -1]))\r\n>>> padoffsets = Index64(np.array([0, 3, 6, 9]))\r\n>>> padarray = ListOffsetArray64(padoffsets, IndexedOptionArray64(padindex, array.content))\r\n>>> padarray\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 3 6 9]\" offset=\"0\" at=\"0x5643ef14e860\"/></offsets>\r\n    <content><IndexedOptionArray64>\r\n        <index><Index64 i=\"[0 1 2 -1 -1 -1 3 4 -1]\" offset=\"0\" at=\"0x5643ef236bc0\"/></index>\r\n        <content><NumpyArray format=\"d\" shape=\"5\" data=\"1.1 2.2 3.3 4.4 5.5\" at=\"0x5643ef232890\"/></content>\r\n    </IndexedOptionArray64></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\n```python\r\n>>> ak.tolist(padarray)\r\n[[1.1, 2.2, 3.3], [None, None, None], [4.4, 5.5, None]]\r\n```\r\n\r\nSince we were padding elements of this NumpyArray (and not between dimensions of a multidimensional `shape`), we can just take them as they are. The `padindex` is counting consecutively everywhere there isn't a `None`; we only needed to introduce the IndexedOptionArray and replace the ListOffsetArray.\r\n\r\nIn addition, users see the distinction between a variable-dimension array that happens to have identical lengths an a truly regular-dimension array. Therefore, this function should have a different behavior (either as a parameter or as two versions of the function) for \"pad and clip,\" which always returns a RegularArray, and \"pad at least...\" which always returns a ListOffsetArray.\r\n\r\nWith the same example as above, consider padding _and clipping_ to length 2:\r\n\r\n```python\r\n>>> padindex2 = Index64(np.array([0, 1, -1, -1, 3, 4]))\r\n>>> padarray2 = RegularArray(IndexedOptionArray64(padindex2, array.content), 2)\r\n>>> padarray2\r\n<RegularArray size=\"2\">\r\n    <content><IndexedOptionArray64>\r\n        <index><Index64 i=\"[0 1 -1 -1 3 4]\" offset=\"0\" at=\"0x5643ef220ff0\"/></index>\r\n        <content><NumpyArray format=\"d\" shape=\"5\" data=\"1.1 2.2 3.3 4.4 5.5\" at=\"0x5643ef232890\"/></content>\r\n    </IndexedOptionArray64></content>\r\n</RegularArray>\r\n```\r\n\r\n```python\r\n>>> ak.tolist(padarray2)\r\n[[1.1, 2.2], [None, None], [4.4, 5.5]]\r\n```\r\n\r\nAlthough at the level of Python lists this looks the same, high-level users will see the difference in type (these are the strings you get from `Type::tostring`):\r\n\r\n```python\r\n>>> ak.typeof(padarray)\r\nvar * ?float64\r\n>>> ak.typeof(padarray2)\r\n2 * ?float64\r\n```",
  "created_at":"2020-02-18T17:00:13Z",
  "id":587565354,
  "issue":73,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NzU2NTM1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-18T17:00:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"From https://github.com/scikit-hep/awkward-1.0/pull/99#discussion_r376347495\r\n\r\n> @jpivarski - please, comment if I understand correctly what `argsort` suppose to do for #74\r\n> Would you rather I used a different sorting algorithm then quick sort? Its average time complexity is `O(n log(n))`, but the worst case is `O(n^2)`. Its worst space complexity is `O(log(n))`\r\n\r\nThe `argsort` function produces an array of integers that, if applied as a slice, would sort the array. For any functions that have \"arg\" versions, we should implement the \"arg\" version first because it usually makes the \"non-arg\" version trivial. Here's a NumPy example:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> array = np.array([6.6, 3.3, 4.4, 1.1, 4.4, 5.5])\r\n>>> index = np.argsort(array)\r\n>>> index\r\narray([3, 1, 2, 4, 5, 0])\r\n>>> array[index]\r\narray([1.1, 3.3, 4.4, 4.4, 5.5, 6.6])\r\n```\r\n\r\nIn the above, if we had an `argsort` already written, the `sort` would be just `getitem(Slice({ SliceArray(argsort_result) }))` or something, maybe `carry(argsort_result)`. (Note: this suggests that it should be implemented as an `argsort64` function returning an `Index64` taking no `axis` parameter, so that it can be internally reused, much like `count`.)\r\n\r\nThe externally visible `argsort` and `sort` functions would take an `axis` parameter, which, for NumPy, determines which direction the sorting is performed:\r\n\r\n```python\r\n>>> array = np.array([[6.6, 3.3, 4.4],\r\n...                   [1.1, 4.4, 5.5]])\r\n>>> np.argsort(array, axis=0)   # compare each top with each bottom\r\narray([[1, 0, 0],\r\n       [0, 1, 1]])\r\n>>> np.argsort(array, axis=1)   # compare left-to-right\r\narray([[1, 2, 0],\r\n       [0, 1, 2]])\r\n```\r\n\r\nThat makes sense when the array is rectilinear. _Maybe_ we could do the same thing with variable-length arrays:\r\n\r\n```python\r\n>>> array = ak.Array([[6.6, 3.3, 4.4],\r\n...                   [1.1, 4.4]])\r\n>>> ak.argsort(array, axis=0)   # compare top-to-bottom, missing goes first\r\n<Array [[1, 0], [0, 1, 0]] type='2 * var * int64'>\r\n>>> ak.argsort(array, axis=1)   # compare left-to-right\r\n<Array [[1, 2, 0], [0, 1]] type='2 * var * int64'>\r\n```\r\n\r\nbut I don't know how or when that would actually be useful. The physics use-case for sorting is either `axis=-1` or at least at the purelist depth. I can't imagine why we'd ever want `axis=0` above, sorting across elements that might not exist because some inner lists are shorter than others.\r\n\r\nMaybe this function shouldn't have an `axis` parameter, but something with a new name, like `depth`. It would always sort horizontally, but `depth` controls at what level the sorting occurs.\r\n\r\n```python\r\n>>> array = ak.Array([[6.6, 3.3, 4.4],\r\n...                   [1.1, 4.4]])\r\n>>> ak.argsort(array, depth=0)   # lexographically compare [6.6, 3.3, 4.4] against [1.1, 4.4]\r\n<Array [1, 0] type='2 * int64'>\r\n>>> ak.argsort(array, depth=1)   # sort within each list\r\n<Array [[1, 2, 0], [0, 1]] type='2 * var * int64'>\r\n```\r\n\r\n_This_ would be useful because the jagged getitem rule (issue #67, ongoing PR #111) would be able to use this result to either sort which list goes first or internally sort each list, depending on the value of `depth`.\r\n\r\nSo I'd say, let's implement this with a `depth` but no `axis` parameter. An `axis` can always be added later if someone really has a use for it, but the value of `depth` is already evident. To do this in stages, implement `depth=-1` first, since that one doesn't need to introduce a lexicographic ordering on structures, something that should be defined generally.\r\n\r\nAbout which sorting algorithm to use, the main use-case is sorting a large number of small lists, not a single large list. Therefore, scaling badly with _n_ is not as much of an issue as set-up and tear-down costs. Often, the data are already sorted and a physicist just wants to make sure they're sorted, so the running time shouldn't be catastrophic for already-sorted data. (I remember from somewhere that there's a sorting algorithm that has bad time characteristics for already-sorted data.)",
  "created_at":"2020-02-07T12:43:26Z",
  "id":583374226,
  "issue":74,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzM3NDIyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-07T12:43:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - thanks! Here is a test of ```argsort``` implemented in Python:\r\n\r\n```python\r\n>>> print(array.constructor)\r\n<bound method NumpyArray.constructor of <NumpyArray>\r\n    <ptr>5.0 5.1 4.2 6.2 9.7 -0.7 0.6 4.9 5.7 7.7 -0.1 8.2 7.7 5.5 8.5 5.7 5.4 4.3 1.2 4.6 4.8 3.9 4.2 8.4 9.5 3.1 2.2 0.9 8.0 3.6 3.6 9.6 2.6 1.7 4.0 6.6 10.1 5.5 0.8 4.6 6.2 -0.2 6.4 6.7 3.2 3.1 8.5 5.3 7.4 7.8 9.0 5.3 3.1 5.8 2.7 8.3 9.0 0.5 4.5 1.6 9.6 6.2 5.0 2.3 7.2</ptr>\r\n    <shape>21 3</shape>\r\n    <strides>3 1</strides>\r\n    <offset>2</offset>\r\n</NumpyArray>>\r\n>>> print(list(array))\r\n[[4.2, 6.2, 9.7], [-0.7, 0.6, 4.9], [5.7, 7.7, -0.1], [8.2, 7.7, 5.5], [8.5, 5.7, 5.4], [4.3, 1.2, 4.6], [4.8, 3.9, 4.2], [8.4, 9.5, 3.1], [2.2, 0.9, 8.0], [3.6, 3.6, 9.6], [2.6, 1.7, 4.0], [6.6, 10.1, 5.5], [0.8, 4.6, 6.2], [-0.2, 6.4, 6.7], [3.2, 3.1, 8.5], [5.3, 7.4, 7.8], [9.0, 5.3, 3.1], [5.8, 2.7, 8.3], [9.0, 0.5, 4.5], [1.6, 9.6, 6.2], [5.0, 2.3, 7.2]]\r\n>>> indices = array.argsort()\r\n>>> print(list(indices))\r\n[5, 41, 10, 57, 6, 38, 27, 18, 59, 33, 26, 63, 32, 54, 52, 45, 25, 44, 29, 30, 21, 34, 22, 2, 17, 58, 19, 39, 20, 7, 62, 47, 51, 16, 37, 13, 8, 15, 53, 61, 40, 3, 42, 35, 43, 64, 48, 9, 12, 49, 28, 11, 55, 23, 46, 14, 56, 50, 24, 60, 31, 4, 36]\r\n>>> sorted_array = []\r\n>>> for i in range(len(indices)):\r\n...     sorted_array.append(array.ptr[indices[i]])\r\n... \r\n>>> print(list(sorted_array))\r\n[-0.7, -0.2, -0.1, 0.5, 0.6, 0.8, 0.9, 1.2, 1.6, 1.7, 2.2, 2.3, 2.6, 2.7, 3.1, 3.1, 3.1, 3.2, 3.6, 3.6, 3.9, 4.0, 4.2, 4.2, 4.3, 4.5, 4.6, 4.6, 4.8, 4.9, 5.0, 5.3, 5.3, 5.4, 5.5, 5.5, 5.7, 5.7, 5.8, 6.2, 6.2, 6.2, 6.4, 6.6, 6.7, 7.2, 7.4, 7.7, 7.7, 7.8, 8.0, 8.2, 8.3, 8.4, 8.5, 8.5, 9.0, 9.0, 9.5, 9.6, 9.6, 9.7, 10.1]\r\n>>> \r\n\r\n```",
  "created_at":"2020-02-07T14:29:14Z",
  "id":583412373,
  "issue":74,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzQxMjM3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-07T14:29:14Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"We would want it to sort within arrays, like\r\n\r\n```python\r\n>>> array.argsort(depth=-1)\r\n[[0, 1, 2], [0, 1, 2], [2, 0, 1], [2, 1, 0], [2, 1, 0], [1, 0, 2], [1, 2, 0], [2, 0, 1], [1, 0, 2], [0, 1, 2], [1, 0, 2], [2, 0, 1], [0, 1, 2], [0, 1, 2], [1, 0, 2], [0, 1, 2], [2, 1, 0], [1, 0, 2], [1, 2, 0], [0, 2, 1], [1, 0, 2]]\r\n```\r\n\r\nThe primary use-case of this is to sort particles within an event (which is why an algorithm that scales well with _n_ is not as important as the constant-time part of set-up and tear-down). Also, operations shouldn't eliminate structure like this\u2014they should do _something_ with it.",
  "created_at":"2020-02-07T14:34:59Z",
  "id":583414915,
  "issue":74,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzQxNDkxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-07T14:34:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This was a frequently requested feature (see [old issue](https://github.com/scikit-hep/awkward-array/issues/84), also frequently requested in person).",
  "created_at":"2020-01-14T16:56:59Z",
  "id":574271873,
  "issue":75,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NDI3MTg3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-14T16:56:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Fixed in PR #89.",
  "created_at":"2020-01-24T23:28:45Z",
  "id":578341780,
  "issue":75,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3ODM0MTc4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-24T23:28:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"A basic `ak.concatenate` has been implemented in #92, though it will be improved when #90 exists. It also doesn't have an `axis`, but that's coming.",
  "created_at":"2020-01-25T23:32:13Z",
  "id":578453249,
  "issue":76,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3ODQ1MzI0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-25T23:32:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Note: it's not just tuples.\r\n\r\n```python\r\n>>> awkward.JaggedArray.zip({\"x\": first, \"y\": second}).tolist()\r\n[[{'x': 1, 'y': 1.1}, {'x': 2, 'y': 2.2}, {'x': 3, 'y': 3.3}],\r\n [],\r\n [{'x': 4, 'y': 4.4}, {'x': 5, 'y': 5.5}]]\r\n```",
  "created_at":"2020-03-10T22:16:59Z",
  "id":597348059,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM0ODA1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T22:16:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The `ak.zip([array1, array2, array3])` and `ak.zip({\"key1\": array1, \"key2\": array2, \"key3\": array3})` functions should probably take a `depthlimit` parameter to indicate how many levels of jaggedness to try to combine.\r\n\r\n`depthlimit=0` would be a synonym for the RecordArray constructor, which creates top-level fields with whatever jaggedness the fields have unreconciled. `depthlimit=None` should go all the way down, broadcasting if possible. `depth=1` would broadcast the first levels together, resulting in a ListArray(RecordArray(...)), `depth=2` would produce ListArray(ListArray(RecordArray(...))), etc.",
  "created_at":"2020-03-10T22:21:28Z",
  "id":597349514,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM0OTUxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T22:21:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I had intended to create a PR for this, but accidentally committed to master (too distracted). So instead of linking to a PR, here's the one commit that finished it: 380055244e414f0ce1e6d2446498608a0588fced",
  "created_at":"2020-03-11T15:16:01Z",
  "id":597693176,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzY5MzE3Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "laugh":1,
   "total_count":2
  },
  "updated_at":"2020-03-11T15:16:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The [test passed](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=1564&view=results), so I'm closing this.",
  "created_at":"2020-03-11T15:35:55Z",
  "id":597704134,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzcwNDEzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-11T15:35:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"If I recall, we were going to keep such operations as module methods like `ak.cross(array, array)` rather than as member functions to minimize namespace clashes?  Alternatively we could spend some time finding a synonym for cross.",
  "created_at":"2020-01-17T15:24:18Z",
  "id":575670328,
  "issue":78,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTY3MDMyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-17T15:24:18Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"That's right, at the `ak.Array` level, these are not methods but are in the global namespace like `ak.cross`. But for the layout objects (not the data-analyst level), they're methods for convenience. In C++, it's convenient to make them virtual methods because the compiler ensures that we have a method defined (though it might only be a `runtime_error(\"FIXME\")` stub).",
  "created_at":"2020-01-17T16:00:13Z",
  "id":575685017,
  "issue":78,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTY4NTAxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-17T16:00:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Here's an interesting observation: cross is a composition of broadcasting and zip.\r\nConsider the following numpy arrays:\r\n```python\r\na = numpy.arange(24).reshape(6, 4)\r\nb = numpy.array(['a', 'b'] * 6).reshape(6, 2)\r\n```\r\nThen the awkward0 operation `out = a.cross(b)` is equivalent to:\r\n```python\r\ni0, i1 = numpy.broadcast_arrays(a[:, :, None], b[:, None, :])\r\nrectype = [('i0', i0.dtype), ('i1', i1.dtype)]\r\nout = numpy.fromiter(zip(i0.flatten(), i1.flatten()), dtype=rectype).reshape(6, 2*4)\r\n```\r\nThe `ak.zip` would simplify the last line.\r\nFor nested cross, `out = a.cross(b, nested=True)` is even simpler:\r\n```python\r\ni0 = a\r\ni1 = numpy.broadcast_to(b[:, None, :], shape=(6, 4, 2))\r\n```\r\n\r\nIn fact, with the current awkward1 master, nested cross is effectively implemented:\r\n```python\r\na = ak.Array([[0, 1, 2], [3, 4], [5], []])\r\nb = ak.Array([[1], [2, 3], [4], [5, 6, 7]])\r\na[:, :, None] < b[:, None, :]\r\n```\r\nis the same as awkward0's\r\n```python\r\na = ak0.fromiter([[0, 1, 2], [3, 4], [5], []])\r\nb = ak0.fromiter([[1], [2, 3], [4], [5, 6, 7]])\r\nab = a.cross(b, nested=True)\r\nab.i0 < ab.i1\r\n```\r\nNon-nested cross will need `ak.flatten`.\r\nNotice that in fact the left-broadcasting on `var` works nicely here, e.g. the following are equivalent:\r\n```python\r\na[:, :, None] < b[:, None, :]\r\na < b[:, None]\r\n```\r\nand a simple switch of where we insert newaxis gives us `b.cross(a, nested=True)`:\r\n```python\r\na[:, None] < b\r\n```",
  "created_at":"2020-01-29T03:36:05Z",
  "id":579577077,
  "issue":78,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTU3NzA3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-29T03:38:45Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"`ak.zip` is #77, which is related to #156 (so much so that I just labeled them duplicates). Just cross-referencing here because this uses zip.",
  "created_at":"2020-03-10T22:35:40Z",
  "id":597353923,
  "issue":78,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM1MzkyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T22:35:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- I wasn't planning to, but in the end, I used your method to implement `cross`, with minor modifications (more low-level functions than `__getitem__`).\r\n\r\nThanks! Your observation really helped!",
  "created_at":"2020-03-12T01:36:36Z",
  "id":597963032,
  "issue":78,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5Nzk2MzAzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T01:36:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'd like to propose an additional `axis=` argument to all combinatorics functions, defaulting as usual to `-1`.  For example,\r\n```python\r\na = ak.fromiter([[['000', '001'], ['010']], [['100', '101'], ['110', '111']]])\r\nassert a.choose(2).tolist() == [[[('000', '001')], []], [[('100', '101')], [('110', '111')]]]\r\nassert a.choose(2, axis=1).tolist() == [[(['000', '001'], ['010'])], [(['100', '101'], ['110', '111'])]]\r\n# axis 0 would typically be a bad idea, but valid\r\n```\r\n",
  "created_at":"2020-01-17T15:33:31Z",
  "id":575674191,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTY3NDE5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-17T15:33:31Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Actually I just realized that right now `axis=1` is the default in awkward0.  The `axis=-1` here would be `a.copy(content=a.content.choose(2))` in awkard0",
  "created_at":"2020-01-17T15:36:38Z",
  "id":575675389,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTY3NTM4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-17T15:36:38Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know whether we need/want the same `axis` default for all functions. For something like `flatten`, the usual case is `axis=0`, but for a reducer, the usual case is `axis=-1`. The first most important thing is that `axis` works the same way/means the same thing for all functions. _Maybe_ they should also have the same default. Maybe not. I don't know how confusing it would be to users if they have different defaults. (Or _no_ defaults? No, that's too extreme.)",
  "created_at":"2020-01-17T16:03:29Z",
  "id":575686307,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTY4NjMwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-17T16:03:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In the end, there are two functions, `ak.pandas.df` (returns one DataFrame) and `ak.pandas.dfs` (returns several DataFrames). Apart from heterogeneous data (UnionArrays), any Awkward array can be expressed as a _set of DataFrames_, each with a different jagged structure. For example, one table of muons, one table of jets, etc. The `ak.pandas.dfs` function is more general and less convenient. The `ak.pandas.df` function uses Pandas to merge all the DataFrames, and we can pass on the \"how\" join modifier.\r\n\r\nHere's an example:\r\n\r\n```python\r\n>>> array = awkward1.Array(\r\n    [\r\n     [\r\n      [{\"x\": 0.0, \"y\": []}, {\"x\": 1.1, \"y\": [1]}, {\"x\": 2.2, \"y\": [2, 2]}],\r\n      [],\r\n      [{\"x\": 3.3, \"y\": [3, 3, 3]}, {\"x\": 4.4, \"y\": [4, 4, 4, 4]}]\r\n     ],\r\n     [],\r\n     [\r\n      [{\"x\": 5.5, \"y\": [5, 5, 5, 5, 5]}]\r\n     ]\r\n    ])\r\n```\r\n\r\nThe `x` and `y` columns can't be expressed in the same DataFrame because they have different jaggedness. (This would also be true if they had the same _depth_ of jaggedness but different index values.)\r\n\r\n```python\r\n>>> ak.pandas.dfs(array)[0]\r\n                              x\r\nentry subentry subsubentry     \r\n0     0        0            0.0\r\n               1            1.1\r\n               2            2.2\r\n      2        0            3.3\r\n               1            4.4\r\n2     0        0            5.5\r\n>>> ak.pandas.dfs(array)[1]\r\n                                           y\r\nentry subentry subsubentry subsubsubentry   \r\n0     0        1           0               1\r\n               2           0               2\r\n                           1               2\r\n      2        0           0               3\r\n                           1               3\r\n                           2               3\r\n               1           0               4\r\n                           1               4\r\n                           2               4\r\n                           3               4\r\n2     0        0           0               5\r\n                           1               5\r\n                           2               5\r\n                           3               5\r\n                           4               5\r\n```\r\n\r\nBut there are natural ways to combine them, but one must make some choices (the normal issues involved in joining relational tables). For full power, the user should call `pd.merge` explicitly, but having a singular `ak.pandas.df` for easy cases is more convenient than always dealing with the plurality of `ak.pandas.dfs` results.\r\n\r\n```python\r\n>>> ak.pandas.df(array)  # how=\"inner\" (default); the x associated with y=[] is gone\r\n                                             x  y\r\nentry subentry subsubentry subsubsubentry        \r\n0     0        1           0               1.1  1\r\n               2           0               2.2  2\r\n                           1               2.2  2\r\n      2        0           0               3.3  3\r\n                           1               3.3  3\r\n                           2               3.3  3\r\n               1           0               4.4  4\r\n                           1               4.4  4\r\n                           2               4.4  4\r\n                           3               4.4  4\r\n2     0        0           0               5.5  5\r\n                           1               5.5  5\r\n                           2               5.5  5\r\n                           3               5.5  5\r\n                           4               5.5  5\r\n\r\n>>> ak.pandas.df(array, how=\"outer\")  # the x associated with y=[] needs placeholders\r\n                                             x    y\r\nentry subentry subsubentry subsubsubentry          \r\n0     0        0           NaN             0.0  NaN\r\n               1           0               1.1  1.0\r\n               2           0               2.2  2.0\r\n                           1               2.2  2.0\r\n      2        0           0               3.3  3.0\r\n                           1               3.3  3.0\r\n                           2               3.3  3.0\r\n               1           0               4.4  4.0\r\n                           1               4.4  4.0\r\n                           2               4.4  4.0\r\n                           3               4.4  4.0\r\n2     0        0           0               5.5  5.0\r\n                           1               5.5  5.0\r\n                           2               5.5  5.0\r\n                           3               5.5  5.0\r\n                           4               5.5  5.0\r\n```",
  "created_at":"2020-03-10T21:12:19Z",
  "id":597320143,
  "issue":80,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzMyMDE0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T21:12:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - ```NumpyArray::count``` is implemented as:  \r\n```python\r\ndef test_count_numpy_array():\r\n    array = awkward1.layout.NumpyArray(numpy.arange(2*3*5, dtype=numpy.int64).reshape(2, 3, 5))\r\n    assert awkward1.tolist(array) == [[[ 0,  1,  2,  3,  4], [ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]],\r\n                                      [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]]]\r\n    assert awkward1.tolist(array.count()) == [3, 3]\r\n```\r\nshould introducing ```NumpyArray::count(axis)``` produce:\r\n```python\r\n    assert awkward1.tolist(array.count(1)) == [5, 5]\r\n```\r\nor\r\n```python\r\n    assert awkward1.tolist(array.flatten()) == [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]]\r\n    assert awkward1.tolist(array.flatten().count()) == [5, 5, 5, 5, 5, 5]\r\n    assert awkward1.tolist(array.count(1)) == [5, 5, 5, 5, 5, 5]\r\n\r\n    assert awkward1.tolist(array.flatten(1)) == [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]]\r\n    assert awkward1.tolist(array.flatten(1).count()) == [15, 15]\r\n    assert awkward1.tolist(array.count(2)) == [15, 15]\r\n\r\n    assert awkward1.tolist(array.flatten(2)) == [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\r\n    assert awkward1.tolist(array.flatten(2).count()) == [30]\r\n    assert awkward1.tolist(array.count(3)) == [30]\r\n```",
  "created_at":"2020-01-16T12:11:11Z",
  "id":575124158,
  "issue":83,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTEyNDE1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-16T12:11:11Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Here's a small Python function that defines the behavior of `count`:\r\n\r\n```python\r\ndef count(data, axis=0):\r\n    if axis < 0:\r\n        raise NotImplementedError(\"axis < 0 is much harder for untyped data...\")\r\n    if isinstance(data, list):\r\n        if axis == 0:\r\n            if all(isinstance(x, list) for x in data):\r\n                return [len(x) for x in data]\r\n            else:\r\n                raise ValueError(\"cannot count the lengths of non-lists\")\r\n        else:\r\n            return [count(x, axis - 1) for x in data]\r\n    else:\r\n        raise ValueError(\"cannot count {0} objects\".format(type(data)))\r\n```\r\n\r\nand so\r\n\r\n```python\r\nassert count([[[1, 2, 3], []], [[4, 5]], [[6], [7, 8, 9]]], axis=0) == [2, 1, 2]\r\nassert count([[[1, 2, 3], []], [[4, 5]], [[6], [7, 8, 9]]], axis=1) == [[3, 0], [2], [1, 3]]\r\n```\r\n\r\nTo answer your question about the `NumpyArray` example,\r\n\r\n```python\r\nassert count(numpy.arange(2*3*5, dtype=numpy.int64).reshape(2, 3, 5).tolist()) == [3, 3]\r\nassert count(numpy.arange(2*3*5, dtype=numpy.int64).reshape(2, 3, 5).tolist(), axis=1) == [[5, 5, 5], [5, 5, 5]]\r\n```\r\n\r\nBy composing this with the `flatten` Python function, you can get the normative behavior for your other examples.",
  "created_at":"2020-01-16T14:19:13Z",
  "id":575171872,
  "issue":83,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTE3MTg3Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-16T14:19:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, using the definition of `axis` that goes all the way down to the leaves of the data structure, we can define the `elif isinstance(data, dict)` (representing Awkward `Records`) case as well:\r\n\r\n```python\r\ndef flatten(data, axis=0):\r\n    if axis < 0:\r\n        raise NotImplementedError(\"axis < 0 is much harder for untyped data...\")\r\n    if isinstance(data, list):\r\n        if axis == 0:\r\n            if all(isinstance(x, list) for x in data):\r\n                return sum(data, [])\r\n            else:\r\n                raise ValueError(\"cannot concatenate non-lists\")\r\n        else:\r\n            return [flatten(x, axis - 1) for x in data]\r\n    elif isinstance(data, dict):\r\n        return {n: flatten(x, axis) for n, x in data.items()}   # does not reduce axis!\r\n    else:\r\n        raise ValueError(\"cannot flatten {0} objects\".format(type(data)))\r\n\r\ndef count(data, axis=0):\r\n    if axis < 0:\r\n        raise NotImplementedError(\"axis < 0 is much harder for untyped data...\")\r\n    if isinstance(data, list):\r\n        if axis == 0:\r\n            if all(isinstance(x, list) for x in data):\r\n                return [len(x) for x in data]\r\n            else:\r\n                raise ValueError(\"cannot count the lengths of non-lists\")\r\n        else:\r\n            return [count(x, axis - 1) for x in data]\r\n    elif isinstance(data, dict):\r\n        return {n: count(x, axis) for n, x in data.items()}   # does not reduce axis!\r\n    else:\r\n        raise ValueError(\"cannot count {0} objects\".format(type(data)))\r\n```\r\n\r\nNote that passing through a `RecordArray` doesn't reduce the `axis` by one, as it does for passing through lists. A layer of records is not a \"dimension.\"\r\n\r\nThe following are list-like and reduce `axis` in recursion:\r\n\r\n   * `ListArray`\r\n   * `ListOffsetArray`\r\n   * `RegularArray`\r\n   * each item of a `NumpyArray`'s `shape`\r\n   * in principle, `EmptyArray`, but it doesn't have any content to pass the operation on to (so it's an error if `axis != 0` by the time it gets here)\r\n\r\nand the following are not list-like; they do not reduce the `axis` in recursion:\r\n\r\n   * `RecordArray` (elements have RecordType)\r\n   * `IndexedArray` (elements have the `content`'s type) and `IndexedOptionArray` (elements have OptionType of their `content`'s type)\r\n\r\nFinally, the following are not even arrays\u2014they're scalars, for which a lot of these operations are nonsensical. (They descend from `Content` so that `getitem` can return arrays or scalars with a single return type.)\r\n\r\n   * `Record` (an element of a `RecordArray`)\r\n   * `None` (a possible return value from OptionType)\r\n   * a `NumpyArray` with an empty `shape`.\r\n",
  "created_at":"2020-01-16T14:27:09Z",
  "id":575175565,
  "issue":83,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTE3NTU2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-16T14:29:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, it's more or less done. I'll add more tests later today.",
  "created_at":"2020-01-29T11:30:48Z",
  "id":579714238,
  "issue":83,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTcxNDIzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-29T11:30:48Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - please, review.",
  "created_at":"2020-01-29T14:05:59Z",
  "id":579770310,
  "issue":83,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTc3MDMxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-29T14:05:59Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - thanks! please, go ahead.",
  "created_at":"2020-01-29T15:29:44Z",
  "id":579809875,
  "issue":83,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTgwOTg3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-29T15:29:44Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @jpivarski - thanks! please, go ahead.\r\n\r\nactually, let me fix a few things you've commented earlier...",
  "created_at":"2020-01-29T15:35:50Z",
  "id":579815177,
  "issue":83,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTgxNTE3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-29T15:35:50Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - ok, ```RecordArray``` and ```UnionArray``` will be done in a separate PR.",
  "created_at":"2020-01-29T16:22:19Z",
  "id":579838067,
  "issue":83,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTgzODA2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-29T16:22:19Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I'll merge it now.",
  "created_at":"2020-01-29T16:27:00Z",
  "id":579840441,
  "issue":83,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTg0MDQ0MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-29T16:27:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@lgray Now `UnionArray` with its `Identities` is done, so any data that can be filled with a `FillableArray` can be snapshotted (i.e. any JSON can become an Awkward Array). As soon as the tests pass, I'm going to merge and deploy this.",
  "created_at":"2020-01-16T23:45:08Z",
  "id":575399625,
  "issue":84,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTM5OTYyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-16T23:45:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski excellent!",
  "created_at":"2020-01-17T14:49:42Z",
  "id":575656215,
  "issue":84,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3NTY1NjIxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-17T14:49:42Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"This will be superseded by #95.",
  "created_at":"2020-02-16T05:32:07Z",
  "id":586671324,
  "issue":85,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NjY3MTMyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-16T05:32:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Moving the OptionType check before the ListType/RegularType check (in PR #89) fixes this.",
  "created_at":"2020-01-23T16:00:07Z",
  "id":577746068,
  "issue":88,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3Nzc0NjA2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-23T16:00:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"```python\r\na = ak.Array([[1.1,  2.2, 3.3], [], [4.4, 5.5], [6.6, 7.7, 8.8, 9.9]])\r\nb = ak.Array([[100, None, 300], [],       None, [600, 700, 800, 900]])\r\nak.tolist(a + b)\r\n```\r\n\r\nis also good.",
  "created_at":"2020-01-23T16:01:06Z",
  "id":577746526,
  "issue":88,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3Nzc0NjUyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-23T16:01:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Same for\r\n\r\n```python\r\na = ak.Array([[1.1, None, 3.3], [], [4.4, 5.5], [6.6, 7.7, 8.8, 9.9]])\r\nb = ak.Array([[100,  200, 300], [],       None, [600, 700, 800, 900]])\r\n```",
  "created_at":"2020-01-23T16:02:40Z",
  "id":577747250,
  "issue":88,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3Nzc0NzI1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-23T16:02:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The original example,\r\n\r\n```python\r\nimport awkward1 as ak\r\na = ak.Array([None, [None, 1], []])\r\nb = ak.Array([[1], [2, 3], [4]])\r\na + b\r\n```\r\n\r\nhas the issue that the last subarray has length 0 in `a` but length 1 in `b`. Before ba891f9164d27748b5dab3b105a277d59ebdd768, it tried to broadcast this, but now it's just an error.\r\n\r\nWith\r\n\r\n```python\r\nimport awkward1 as ak\r\na = ak.Array([None, [None, 1], [5]])\r\nb = ak.Array([[1], [2, 3], [4]])\r\na + b\r\n```\r\n\r\nit works fine: result is\r\n\r\n```\r\n<Array [None, [None, 4], [9]] type='3 * option[var * ?int64]'>\r\n```",
  "created_at":"2020-01-23T16:53:10Z",
  "id":577770214,
  "issue":88,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3Nzc3MDIxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-23T16:53:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Such a function would also improve `ak.concatenate`, which is now defined as a UnionArray. This may be a good general way to do thing: structure-manipulation functions create UnionArrays and the UnionArrays try to simplify themselves however they can.",
  "created_at":"2020-01-25T23:30:07Z",
  "id":578453106,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3ODQ1MzEwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-25T23:30:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Implemented in PR #93.",
  "created_at":"2020-02-03T21:47:40Z",
  "id":581634676,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MTYzNDY3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-03T21:47:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Done in PR #110. There are two nice examples of it here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/0ab2c573b8237e71e213572a8686bf7a04e2e66d/tests/test_PR110_various_cleanups.py#L78-L115",
  "created_at":"2020-02-06T00:13:02Z",
  "id":582674537,
  "issue":91,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MjY3NDUzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-06T00:13:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"FYI @mhedges, this is where I'm incorporating your work. Unlike last time, I'm going to make all `ak.Arrays` subclasses of the Pandas extension type if Pandas is importable. There's a startup cost associated with this, but it's not too bad:\r\n\r\n| | slow computer | medium computer | fast computer |\r\n|:-|:-:|:-:|:-:|\r\n| only Pandas | 1.5 sec | 0.35 sec | 0.25 sec |\r\n| only Numba | 1.0 sec | 0.35 sec | 0.20 sec |\r\n| both Pandas and Numba | 2.0 sec | 0.60 sec | 0.35 sec |\r\n\r\nwhere \"slow computer\" is a Chromebook. Having to wait 2 seconds for `awkward1` to import on a computer like that shouldn't be too much of a surprise.",
  "created_at":"2020-01-25T20:58:15Z",
  "id":578442506,
  "issue":92,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3ODQ0MjUwNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-25T20:58:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"**Update:** as it turns out, we don't need to `import pandas` just to make `ak.Array` inherit from `pandas.api.extensions.ExtensionArray`. You can change base classes at runtime! ([In Python 2.7 and 3.2+](https://stackoverflow.com/a/9639512/1623645).)\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/cc7bf633ba0f4dd3d96a2990bcf16b15192c4dfc/awkward1/_pandas.py#L46-L55\r\n\r\nSo we can avoid the 2 second startup time on slow computers. The Numba startup time can be avoided by registering it as a [Numba entry point](https://numba.pydata.org/numba-doc/dev/extending/entrypoints.html), which I haven't done yet, but it's available as long as I require `numba>=0.46.0`.",
  "created_at":"2020-01-25T23:46:22Z",
  "id":578454084,
  "issue":92,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3ODQ1NDA4NA==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2020-01-25T23:46:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith-: The internal `called_by_module` function\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/db6938d82a41be90266945909244181c65b49a7b/awkward1/highlevel.py#L12-L19\r\n\r\nwill ensure that Matplotlib is not allowed to iterate over an array, while still letting everybody else do it. It's a blacklist against bad actors.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/db6938d82a41be90266945909244181c65b49a7b/awkward1/highlevel.py#L72-L78\r\n\r\nI already need it for Pandas: Pandas uses `__array__` on a subset of array elements to print them to the screen. For this case, I don't mind returning NumPy object arrays, because they're necessarily small (or the printing would be a bottleneck, anyway). But in general, `ak.Array` should only be coerced to a NumPy array if it can do so as a non-object array, preserving structure.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/db6938d82a41be90266945909244181c65b49a7b/awkward1/highlevel.py#L96-L106\r\n\r\nThis _does_ feel like a hack, I know, but it's only applied to the high level object (`ak.Array`). It exists because these third-party libraries should have introduced a protocol that doesn't name-clash with a preexisting one, or they assumed that their `__iter__`/`__array__` needs would be the same as anybody else's.\r\n\r\nThe `inspect` module might have a performance penalty, but these are methods that shouldn't be used in tight loops, anyway.",
  "created_at":"2020-01-26T00:48:53Z",
  "id":578457778,
  "issue":92,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3ODQ1Nzc3OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-26T00:48:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- I ended up inhibiting Matplotlib by correctly implementing `np.size` and `np.atleast_1d` with reasonable definitions for Awkward arrays.\r\n\r\n   * `np.size` only gives a value if the axis in question is a RegularArray. `plt.hist` queries them all (`axis=None`), which will raise an error suggesting `ak.flatten` or `ak.tonumpy` if it contains any jagged arrays.\r\n   * `np.atleast_1d` has to be able to cast all its arguments as NumPy arrays (of at least one dimension). If any Awkward arrays fail `ak.tonumpy`, this will fail.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/a9b166ef490be2cd2e98ace5092d57d13563235a/tests/test_PR089_numpy_functions.py#L169-L181\r\n\r\nSo... not a hack! (This probably requires `numpy>=1.17` though. Users of Matplotlib with older NumPy versions might get the old failure, but a situation in which the old failure gradually recedes into the past is acceptable, I think.)\r\n\r\nHowever, `ak.Array.__array__` still checks to see if it's being called by Pandas and creates a NumPy object array only for Pandas. (All other cases attempt `ak.tonumpy`.)",
  "created_at":"2020-01-28T21:36:10Z",
  "id":579467765,
  "issue":92,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTQ2Nzc2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-28T21:38:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@mhedges This is a complete implementation of Awkward-in-Pandas, derived heavily from your work in Awkward 0. It also avoids the 2 second startup time by having `PandasMixin` inherit from `PandasNotImportedYet` until the first time its `dtype` is queried, then it retroactively inherits from `pandas.api.extensions.ExtensionArray` (for all existing arrays, too).\r\n\r\nThe only loophole I see is that someone might do this:\r\n\r\n```python\r\npd.DataFrame({\"column\": [... lists of lists ...]}, dtype=\"awkward1\")\r\n```\r\n\r\nand it will say that `\"awkward1\"` has not been registered as a dtype _if_ the import-on-demand has not been triggered yet. I'll put that off to see if it becomes an issue in practice.\r\n\r\nThe full Pandas test suite works:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/a9b166ef490be2cd2e98ace5092d57d13563235a/tests/test_PR090_as_pandas_extension.py#L195-L196",
  "created_at":"2020-01-28T21:45:43Z",
  "id":579471630,
  "issue":92,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTQ3MTYzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-28T21:45:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"This is awesome! So cool to see it come together.  That's pretty slick move to wait to import from pandas until runtime",
  "created_at":"2020-01-29T15:00:39Z",
  "id":579795043,
  "issue":92,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTc5NTA0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-29T15:00:39Z",
  "user":"MDQ6VXNlcjE4NjcyNTEy"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - unless you have more comments, I think, I'm done with this PR. I managed to loose an ```awkward1/signatures/identities_8cpp.xml``` file in a merge.",
  "created_at":"2020-02-04T16:36:24Z",
  "id":581997906,
  "issue":96,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MTk5NzkwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-04T16:36:24Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Great! I would have named the argument to the NumpyArray constructors something more generic than `count` because I'm expecting these constructors to be used in other contexts later. I'll do that later, though.\r\n\r\nI think I lost the `identities_8cpp.xml` file, too... I'll make sure to regenerate all the signatures at some point.\r\n\r\nI'm ready to merge: give me the thumbs-up and I'll do it. (To make sure I don't collide with some last-minute changes.)",
  "created_at":"2020-02-04T16:53:46Z",
  "id":582007205,
  "issue":96,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MjAwNzIwNQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-02-04T16:53:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I would have named the argument to the NumpyArray constructors something more generic than count because I'm expecting these constructors to be used in other contexts later. I'll do that later, though.\r\n\r\nDone in 007e5610b9337cf5e225bbae59f8305e619bb896",
  "created_at":"2020-02-04T22:21:59Z",
  "id":582144609,
  "issue":96,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MjE0NDYwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-04T22:21:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!\r\n\r\nThe files in `awkward1/signatures` are generated by manually running doxygen, and for now at least, I want that to be manual and in source control. These files are needed to specify signatures of header files in Python for the Numba implementation. If new types are used in the arguments, `awkward1/_numba/cpu.py` has to be updated, so I only want to update the signatures when I'm ready to update Numba. Synchronization between the true signatures and the signatures represented in XML are only a problem if Numba uses something that changes without updating XML\u2014in this growing phase, we're adding new ones, rather than changing old ones. (Changing old ones is unlikely in the future, too, because there's so much boilerplate that has to be the same for C++, Numba, and eventually CUDA or OpenCL.)\r\n\r\nIt would probably be better to generate all of that boilerplate with a script, and that script can communicate the correct signatures to Python, rather than inferring them through the fact that doxygen can parse `.h` syntax. But one thing at a time...",
  "created_at":"2020-01-29T17:25:30Z",
  "id":579867838,
  "issue":97,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU3OTg2NzgzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-29T17:25:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I do not see any warnings or errors in the build.",
  "created_at":"2020-01-30T11:43:25Z",
  "id":580215032,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDIxNTAzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-30T11:43:25Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll have to check it when I get in, because there _were_ a few in operations.cpp.\r\n\r\nAlso, you may want to change the name of this pill request.",
  "created_at":"2020-01-30T12:03:41Z",
  "id":580221633,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDIyMTYzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-30T12:03:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Looking at the [last Python 3.7, 32-bit test](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=1054&view=logs&jobId=14431b54-d949-5b45-5c5a-199a6962c9d9&j=14431b54-d949-5b45-5c5a-199a6962c9d9&t=0a366409-8179-5e51-c0fe-d3dc52b6ba4f), I see these warnings from Visual Studio:\r\n\r\n```\r\n2020-01-30T11:34:09.0342649Z Microsoft (R) Build Engine version 15.9.21+g9802d43bc3 for .NET Framework\r\n2020-01-30T11:34:09.0343483Z Copyright (C) Microsoft Corporation. All rights reserved.\r\n2020-01-30T11:34:09.0343737Z \r\n2020-01-30T11:34:09.4825119Z   Checking Build System\r\n2020-01-30T11:34:09.6213315Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:34:09.8561716Z   Content.cpp\r\n2020-01-30T11:34:10.5684550Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:34:11.6208142Z   getitem.cpp\r\n2020-01-30T11:34:11.6624677Z   identities.cpp\r\n2020-01-30T11:34:11.7332313Z   Identities.cpp\r\n2020-01-30T11:34:11.8898799Z   operations.cpp\r\n2020-01-30T11:34:12.2951423Z D:\\a\\1\\s\\src\\cpu-kernels\\operations.cpp(127): warning C4244: '=': conversion from 'T' to 'C', possible loss of data [D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\awkward-cpu-kernels-objects.vcxproj]\r\n2020-01-30T11:34:12.2967992Z           with\r\n2020-01-30T11:34:12.2968954Z           [\r\n2020-01-30T11:34:12.2969553Z               T=int64_t\r\n2020-01-30T11:34:12.2970106Z           ]\r\n2020-01-30T11:34:12.2970657Z           and\r\n2020-01-30T11:34:12.2971211Z           [\r\n2020-01-30T11:34:12.2971747Z               C=int32_t\r\n2020-01-30T11:34:12.2972302Z           ]\r\n2020-01-30T11:34:12.3003505Z   D:\\a\\1\\s\\src\\cpu-kernels\\operations.cpp(133): note: see reference to function template instantiation 'Error awkward_listarray_flatten_scale<int32_t,int64_t>(C *,C *,const T *,const C *,const C *,int64_t,int64_t,int64_t)' being compiled\r\n2020-01-30T11:34:12.3004586Z           with\r\n2020-01-30T11:34:12.3004760Z           [\r\n2020-01-30T11:34:12.3006332Z               C=int32_t,\r\n2020-01-30T11:34:12.3015414Z               T=int64_t\r\n2020-01-30T11:34:12.3019796Z           ]\r\n2020-01-30T11:34:12.3020153Z D:\\a\\1\\s\\src\\cpu-kernels\\operations.cpp(128): warning C4244: '=': conversion from 'T' to 'C', possible loss of data [D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\awkward-cpu-kernels-objects.vcxproj]\r\n2020-01-30T11:34:12.3021382Z           with\r\n2020-01-30T11:34:12.3021765Z           [\r\n2020-01-30T11:34:12.3028695Z               T=int64_t\r\n2020-01-30T11:34:12.3032720Z           ]\r\n2020-01-30T11:34:12.3032988Z           and\r\n2020-01-30T11:34:12.3033859Z           [\r\n2020-01-30T11:34:12.3034242Z               C=int32_t\r\n2020-01-30T11:34:12.3042298Z           ]\r\n2020-01-30T11:34:12.3051761Z D:\\a\\1\\s\\src\\cpu-kernels\\operations.cpp(127): warning C4244: '=': conversion from 'T' to 'C', possible loss of data [D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\awkward-cpu-kernels-objects.vcxproj]\r\n2020-01-30T11:34:12.3052263Z           with\r\n2020-01-30T11:34:12.3052432Z           [\r\n2020-01-30T11:34:12.3052563Z               T=int64_t\r\n2020-01-30T11:34:12.3052713Z           ]\r\n2020-01-30T11:34:12.3056491Z           and\r\n2020-01-30T11:34:12.3061461Z           [\r\n2020-01-30T11:34:12.3061967Z               C=uint32_t\r\n2020-01-30T11:34:12.3066680Z           ]\r\n2020-01-30T11:34:12.3071257Z   D:\\a\\1\\s\\src\\cpu-kernels\\operations.cpp(136): note: see reference to function template instantiation 'Error awkward_listarray_flatten_scale<uint32_t,int64_t>(C *,C *,const T *,const C *,const C *,int64_t,int64_t,int64_t)' being compiled\r\n2020-01-30T11:34:12.3074616Z           with\r\n2020-01-30T11:34:12.3078488Z           [\r\n2020-01-30T11:34:12.3078758Z               C=uint32_t,\r\n2020-01-30T11:34:12.3079626Z               T=int64_t\r\n2020-01-30T11:34:12.3080023Z           ]\r\n2020-01-30T11:34:12.3087167Z D:\\a\\1\\s\\src\\cpu-kernels\\operations.cpp(128): warning C4244: '=': conversion from 'T' to 'C', possible loss of data [D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\awkward-cpu-kernels-objects.vcxproj]\r\n2020-01-30T11:34:12.3091967Z           with\r\n2020-01-30T11:34:12.3092249Z           [\r\n2020-01-30T11:34:12.3092396Z               T=int64_t\r\n2020-01-30T11:34:12.3092553Z           ]\r\n2020-01-30T11:34:12.3097076Z           and\r\n2020-01-30T11:34:12.3101492Z           [\r\n2020-01-30T11:34:12.3104128Z               C=uint32_t\r\n2020-01-30T11:34:12.3104337Z           ]\r\n2020-01-30T11:34:12.3472381Z   util.cpp\r\n2020-01-30T11:34:12.7634225Z   Generating Code...\r\n2020-01-30T11:34:12.9753627Z   Index.cpp\r\n2020-01-30T11:34:14.0167956Z   awkward-cpu-kernels-objects.vcxproj -> D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\awkward-cpu-kernels-objects.dir\\Release\\awkward-cpu-kernels-objects.lib\r\n2020-01-30T11:34:14.1293420Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:34:14.2033314Z   awkward-cpu-kernels-static.vcxproj -> D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\Release\\awkward-cpu-kernels-static.lib\r\n2020-01-30T11:34:14.2307013Z   Iterator.cpp\r\n2020-01-30T11:34:14.7652179Z   Slice.cpp\r\n2020-01-30T11:34:15.4430062Z   EmptyArray.cpp\r\n2020-01-30T11:34:16.1074344Z   IndexedArray.cpp\r\n2020-01-30T11:34:16.9356028Z   ListArray.cpp\r\n2020-01-30T11:34:17.7094159Z   ListOffsetArray.cpp\r\n2020-01-30T11:34:18.4777061Z   None.cpp\r\n2020-01-30T11:34:19.0961373Z   NumpyArray.cpp\r\n2020-01-30T11:34:19.5621681Z D:\\a\\1\\s\\src\\libawkward\\array\\NumpyArray.cpp(693): warning C4244: 'initializing': conversion from 'int64_t' to 'ssize_t', possible loss of data [D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\awkward-objects.vcxproj]\r\n2020-01-30T11:34:19.5624319Z D:\\a\\1\\s\\src\\libawkward\\array\\NumpyArray.cpp(710): warning C4244: 'initializing': conversion from 'int64_t' to 'ssize_t', possible loss of data [D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\awkward-objects.vcxproj]\r\n2020-01-30T11:34:20.0293191Z   Record.cpp\r\n2020-01-30T11:34:20.7136389Z   RecordArray.cpp\r\n2020-01-30T11:34:21.4486147Z   RegularArray.cpp\r\n2020-01-30T11:34:22.2111553Z   UnionArray.cpp\r\n2020-01-30T11:34:22.9764391Z   BoolFillable.cpp\r\n2020-01-30T11:34:23.5506396Z   Fillable.cpp\r\n2020-01-30T11:34:24.0527480Z   FillableArray.cpp\r\n2020-01-30T11:34:24.5828354Z   FillableOptions.cpp\r\n2020-01-30T11:34:24.9770832Z   Float64Fillable.cpp\r\n2020-01-30T11:34:25.5495020Z   Generating Code...\r\n2020-01-30T11:34:38.5708317Z   Compiling...\r\n2020-01-30T11:34:38.5712446Z   GrowableBuffer.cpp\r\n2020-01-30T11:34:39.1186775Z   Int64Fillable.cpp\r\n2020-01-30T11:34:39.6864029Z   ListFillable.cpp\r\n2020-01-30T11:34:40.2349211Z   OptionFillable.cpp\r\n2020-01-30T11:34:40.7904155Z   RecordFillable.cpp\r\n2020-01-30T11:34:41.4669696Z   StringFillable.cpp\r\n2020-01-30T11:34:42.0887312Z   TupleFillable.cpp\r\n2020-01-30T11:34:42.6972208Z   UnionFillable.cpp\r\n2020-01-30T11:34:43.2968242Z   UnknownFillable.cpp\r\n2020-01-30T11:34:43.8621706Z   json.cpp\r\n2020-01-30T11:34:44.4764712Z   root.cpp\r\n2020-01-30T11:34:45.0472018Z   ArrayType.cpp\r\n2020-01-30T11:34:45.5622860Z   ListType.cpp\r\n2020-01-30T11:34:46.1117652Z   OptionType.cpp\r\n2020-01-30T11:34:46.6586402Z   PrimitiveType.cpp\r\n2020-01-30T11:34:47.2361639Z   RecordType.cpp\r\n2020-01-30T11:34:47.8650691Z   RegularType.cpp\r\n2020-01-30T11:34:48.4088655Z   Type.cpp\r\n2020-01-30T11:34:49.0768685Z   UnionType.cpp\r\n2020-01-30T11:34:49.6631375Z   UnknownType.cpp\r\n2020-01-30T11:34:50.2100864Z   Generating Code...\r\n2020-01-30T11:34:57.4223131Z   Compiling...\r\n2020-01-30T11:34:57.4224467Z   util.cpp\r\n2020-01-30T11:34:58.1228147Z   Generating Code...\r\n2020-01-30T11:34:59.4253097Z   awkward-objects.vcxproj -> D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\awkward-objects.dir\\Release\\awkward-objects.lib\r\n2020-01-30T11:34:59.4650800Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:34:59.6294893Z   awkward-static.vcxproj -> D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\Release\\awkward-static.lib\r\n2020-01-30T11:34:59.6687732Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:34:59.7483091Z   pyawkward.cpp\r\n2020-01-30T11:35:28.7586843Z      Creating library D:/a/1/s/build/temp.win32-3.7/Release/Release/layout.lib and object D:/a/1/s/build/temp.win32-3.7/Release/Release/layout.exp\r\n2020-01-30T11:35:28.9039575Z   Generating code\r\n2020-01-30T11:36:03.5068033Z   Finished generating code\r\n2020-01-30T11:36:04.1747497Z   layout.vcxproj -> D:\\a\\1\\s\\build\\lib.win32-3.7\\layout.cp37-win32.pyd\r\n2020-01-30T11:36:04.2788832Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:36:05.6657674Z   test_PR019_use_json_library.cpp\r\n2020-01-30T11:36:05.8704110Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:36:05.9169780Z   test_PR016_finish_getitem_for_rawarray.cpp\r\n2020-01-30T11:36:06.2660396Z      Creating library D:/a/1/s/build/temp.win32-3.7/Release/Release/PR019.lib and object D:/a/1/s/build/temp.win32-3.7/Release/Release/PR019.exp\r\n2020-01-30T11:36:06.2692547Z   PR019.vcxproj -> D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\Release\\PR019.exe\r\n2020-01-30T11:36:06.3228195Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:36:06.3970117Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:36:06.4191992Z   Auto build dll exports\r\n2020-01-30T11:36:06.9832904Z      Creating library D:/a/1/s/build/temp.win32-3.7/Release/Release/awkward.lib and object D:/a/1/s/build/temp.win32-3.7/Release/Release/awkward.exp\r\n2020-01-30T11:36:07.0021242Z   awkward.vcxproj -> D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\Release\\awkward.dll\r\n2020-01-30T11:36:07.0744900Z   test_PR030_recordarray_in_numba.cpp\r\n2020-01-30T11:36:07.0770338Z      Creating library D:/a/1/s/build/temp.win32-3.7/Release/Release/PR016.lib and object D:/a/1/s/build/temp.win32-3.7/Release/Release/PR016.exp\r\n2020-01-30T11:36:07.0773874Z   PR016.vcxproj -> D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\Release\\PR016.exe\r\n2020-01-30T11:36:07.1049625Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n2020-01-30T11:36:07.1408036Z   Auto build dll exports\r\n2020-01-30T11:36:07.2541013Z      Creating library D:/a/1/s/build/temp.win32-3.7/Release/Release/awkward-cpu-kernels.lib and object D:/a/1/s/build/temp.win32-3.7/Release/Release/awkward-cpu-kernels.exp\r\n2020-01-30T11:36:07.4392439Z      Creating library D:/a/1/s/build/temp.win32-3.7/Release/Release/PR030.lib and object D:/a/1/s/build/temp.win32-3.7/Release/Release/PR030.exp\r\n2020-01-30T11:36:07.4397722Z   PR030.vcxproj -> D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\Release\\PR030.exe\r\n2020-01-30T11:36:07.4462533Z   awkward-cpu-kernels.vcxproj -> D:\\a\\1\\s\\build\\temp.win32-3.7\\Release\\Release\\awkward-cpu-kernels.dll\r\n2020-01-30T11:36:07.5159743Z   Building Custom Rule D:/a/1/s/CMakeLists.txt\r\n```\r\n\r\nThere's something remaining in NumpyArray.cpp lines 693 and/or 710, as well as the operations.cpp errors that I'll be testing now.",
  "created_at":"2020-01-30T15:10:35Z",
  "id":580298807,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDI5ODgwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-30T15:10:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, the NumpyArray.cpp ones were easy, so I fixed those, too.",
  "created_at":"2020-01-30T15:14:38Z",
  "id":580300665,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDMwMDY2NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-01-30T15:14:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"And everything just broke for Python 3.6 and above because Pandas 1.0 has just been released. Apparently, what I developed for Pandas 0.24 has incompatibilities with 1.0. I'll add those fixes to this PR.",
  "created_at":"2020-01-30T15:24:38Z",
  "id":580305324,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDMwNTMyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-30T15:24:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - here is a first step towards adding the compiler flags: https://github.com/scikit-hep/awkward-1.0/commit/db85e88c15295d81a067d69ba7188251630225c6\r\nThe list of flags is a guess. I can only test clang on MacOS.",
  "created_at":"2020-01-30T16:48:42Z",
  "id":580346603,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDM0NjYwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-30T16:48:42Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna I want to merge this as soon as it passes tests, so that I can merge it into my active branch. The commit you're referring to, db85e88, is that on this branch or another? If you want it in this PR, please add it quickly.",
  "created_at":"2020-01-30T18:02:19Z",
  "id":580379270,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDM3OTI3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-30T18:02:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @ianna,\r\n\r\nI want you to take a look at [studies/flatten.py](https://github.com/scikit-hep/awkward-1.0/blob/study/flatten/studies/flatten.py), which is a simplified implementation of all of the `Content` classes. They have just enough structure (`__getitem__` for single-int and start-stop range, `__iter__`, two printable representations for debugging) to be useful but not so much as to hide the big picture.\r\n\r\nThe constructors are full of assertions that define what is allowed and what is not allowed for each array type. I'm thinking of transforming this into better documentation than what we have in the wiki. Walking through this actually revealed a bug in ListOffsetArray (my C++ implementation of its `__getitem__` for start-stop range).\r\n\r\nAdditionally, they can all be randomly generated. The randomly generated arrays are guaranteed to be valid (though the constructors will complain if they're not). This is an easy way of making a lot of test cases, though some of them can be prohibitively large (sometimes the structure grows exponentially\u2014just kill the process with control-C and roll the dice again...).\r\n\r\nIf you do encounter a problematic implementation, it helps to have a Python prompt open to investigate it. You can run this with\r\n\r\n```bash\r\npython -i study/flatten.py\r\n```\r\n\r\nto get an interactive prompt after it either finishes or fails with an exception. The last attempt is called `array`: see the testing loop for more information. (In Python, variable scope is not limited to a block; it's limited to functions, and there are no functions in the test. `array` has the last value it did before the exception, which is great for interactively debugging it.)\r\n\r\nYou can\r\n\r\n```python\r\nprint(array.constructor)\r\n```\r\n\r\nto get the string value of a constructor for a problematic array, which you can then copy-paste into `studies/flatten.py` for further tests, without randomization (so that you can focus on a failing case).\r\n\r\nFor the sake of example, I've implemented `count` in a rowwise way (like the function in the main tests, but more complete):\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ca981b7607473e5e842eb9c73635417b1157cd14/studies/flatten.py#L600-L623\r\n\r\nAnd I've implemented it in a columnar way. When the columnar versions all work, you can translate them into C++ for the main codebase.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ca981b7607473e5e842eb9c73635417b1157cd14/studies/flatten.py#L625-L747\r\n\r\nThe tests are right below that. It's just one little script.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ca981b7607473e5e842eb9c73635417b1157cd14/studies/flatten.py#L736-L747\r\n\r\nI think you should figure out the logic of `flatten` in this simplified environment before working on it in C++. I believe that will save a lot of time. (It has for me, by a large factor.)\r\n",
  "created_at":"2020-01-31T23:43:53Z",
  "id":580958297,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDk1ODI5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-31T23:43:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and be sure to use Python 3.",
  "created_at":"2020-01-31T23:50:17Z",
  "id":580959599,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDk1OTU5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-31T23:50:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks, @jpivarski - this is really helpful and quite simple. I think, I've checked all C++ implementations of *::count against this study. ",
  "created_at":"2020-02-03T14:25:00Z",
  "id":581438761,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MTQzODc2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-03T14:25:00Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I'm not sure if I should start a new PR or try to merge it in your branch. It's still work in progress: https://github.com/scikit-hep/awkward-1.0/compare/study/flatten...study/flatten-v1?expand=1",
  "created_at":"2020-02-03T16:31:45Z",
  "id":581500160,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MTUwMDE2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-03T16:31:45Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"You can take over this branch. I've copied out versions of the classes for documentation, so you can do whatever you need to these.",
  "created_at":"2020-02-03T16:33:50Z",
  "id":581501088,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MTUwMTA4OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-02-03T16:33:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I'm not sure why this is failing. Am I missing something?\r\n```python\r\nTraceback (most recent call last):\r\n  File \"studies/flatten.py\", line 1068, in <module>\r\n    assert rowwise == columnar\r\nAssertionError\r\n>>> print(array.constructor)\r\n<bound method ListArray.constructor of <ListArray>\r\n    <starts>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</starts>\r\n    <stops>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1</stops>\r\n    <content><RegularArray>\r\n        <content><RecordArray>\r\n            <length>4</length>\r\n        </RecordArray></content>\r\n        <size>4</size>\r\n    </RegularArray></content>\r\n</ListArray>>\r\n>>> print(list(array))\r\n[[[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]], [[(), (), (), ()]]]\r\n>>> print(array.flatten())\r\n<IndexedArray>\r\n    <index>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</index>\r\n    <content><RegularArray>\r\n        <content><RecordArray>\r\n            <length>4</length>\r\n        </RecordArray></content>\r\n        <size>4</size>\r\n    </RegularArray></content>\r\n</IndexedArray>\r\n>>> print(list(flatten(array)))\r\n[[(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()]]\r\n>>> print(list(array.flatten()))\r\n[[(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()]]\r\n>>> \r\n```",
  "created_at":"2020-02-07T17:01:25Z",
  "id":583501057,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzUwMTA1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-07T17:01:25Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"```python\r\n>>> print(array.constructor())\r\n```\r\n\r\n(You were printing the function itself, not the result of executing the function.)",
  "created_at":"2020-02-07T17:08:42Z",
  "id":583504423,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzUwNDQyMw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-02-07T17:08:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I wonder why there is an assert when the resulting lists look identical:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"studies/flatten.py\", line 1087, in <module>\r\n    assert rowwise == columnar\r\nAssertionError\r\n>>> print(*flatten(array))\r\n[3.8, 9.2] [2.9, 1.5] [4.5, 7.4] [4.5, 4.6] [5.3, 11.2] [3.6, 6.9] [-0.9, 3.8]\r\n>>> print(*array.flatten())\r\n[3.8, 9.2] [2.9, 1.5] [4.5, 7.4] [4.5, 4.6] [5.3, 11.2] [3.6, 6.9] [-0.9, 3.8]\r\n>>> print(array.constructor())\r\nListArray([7, 1], [9, 6], RegularArray(RawArray([8.4, -3.6, 4.5, 7.4, 4.5, 4.6, 5.3, 11.2, 3.6, 6.9, -0.9, 3.8, 6.9, -4.0, 3.8, 9.2, 2.9, 1.5]), 2))\r\n\r\n```",
  "created_at":"2020-02-07T22:09:12Z",
  "id":583637636,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzYzNzYzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-07T22:09:12Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> I wonder why there is an assert when the resulting lists look identical\r\n\r\nDon't you have to wrap one of those as a `list`?",
  "created_at":"2020-02-07T22:40:19Z",
  "id":583647236,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzY0NzIzNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-02-07T22:40:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I need your help here. Could you, please, have a look? I cannot figure out why occasionally I get a nested ```columnar``` list. I've forced the ```axis = 0``` for the test and it looks like I get correct results (if the nesting is ignored). \r\n```python\r\n>>> print(array.constructor())\r\nRegularArray(IndexedOptionArray([13, 9, 13, 4, 8, 3, 15, -1, 16, 2, 8], NumpyArray([2.1, 8.4, 7.4, 1.6, 2.2, 3.4, 6.2, 5.4, 1.5, 3.9, 3.8, 3.0, 8.5, 6.9, 4.3, 3.6, 6.7, 1.8, 3.2], [18], [1], 1)), 3)\r\n>>> print(*array)\r\n[4.3, 3.8, 4.3] [3.4, 3.9, 2.2] [6.7, None, 1.8]\r\n>>> print(rowwise)\r\n[4.3, 3.8, 4.3, 3.4, 3.9, 2.2, 6.7, None, 1.8]\r\n>>> print(columnar)\r\n[[4.3, 3.8, 4.3, 3.4, 3.9, 2.2, 6.7, None, 1.8]]\r\n```",
  "created_at":"2020-02-13T16:27:39Z",
  "id":585845242,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NTg0NTI0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-13T16:27:39Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The code that is directly being called by this is the following:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e669c06a3c5c8fac4529203c50f4ec1d8e329281/studies/flatten.py#L933-L937\r\n\r\nwhich wraps the result in a RegularArray in one case (increasing its list-depth) and not in another case (not increasing its list-depth). The two cases are things that would be invisible to the user\u2014whether there are unreachable elements in the RegularArray's `content` or not. It looks like this:\r\n\r\n```python\r\n>>> # no visible difference between these two\r\n>>> list(RegularArray(RawArray([0.0, 1.1, 2.2, 3.3, 4.4, 5.5]), 3))\r\n[[0.0, 1.1, 2.2], [3.3, 4.4, 5.5]]\r\n>>> list(RegularArray(RawArray([0.0, 1.1, 2.2, 3.3, 4.4, 5.5, 6.6]), 3))\r\n[[0.0, 1.1, 2.2], [3.3, 4.4, 5.5]]\r\n\r\n>>> # but this behavior creates a visible difference\r\n>>> list(RegularArray(RawArray([0.0, 1.1, 2.2, 3.3, 4.4, 5.5]), 3).flatten())\r\n[0.0, 1.1, 2.2, 3.3, 4.4, 5.5]\r\n>>> list(RegularArray(RawArray([0.0, 1.1, 2.2, 3.3, 4.4, 5.5, 6.6]), 3).flatten())\r\n[[0.0, 1.1, 2.2, 3.3, 4.4, 5.5]]\r\n```\r\n\r\nWe should talk sometime about this more generally. I'll contact you by email.",
  "created_at":"2020-02-13T17:36:41Z",
  "id":585879499,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NTg3OTQ5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-13T17:36:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna, can this branch be deleted? Does it contain anything valuable that's not on other branches?",
  "created_at":"2020-03-10T05:18:53Z",
  "id":596908225,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjkwODIyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T05:18:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Same for `awkward_compiler_flags` and `feature/PR083-numpyarray-flatten-bugfix`. Do we need them? I want to make sure that the number of stale branches doesn't get out of hand. Thanks!\r\n",
  "created_at":"2020-03-10T05:21:20Z",
  "id":596908799,
  "issue":99,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjkwODc5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T05:21:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Finished in PR #106.",
  "created_at":"2020-02-03T23:20:19Z",
  "id":581667051,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MTY2NzA1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-03T23:20:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"**Maybe...** It might be better to force arrays of X to have a different name in `classes` than records of X.",
  "created_at":"2020-02-04T21:29:38Z",
  "id":582124169,
  "issue":101,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MjEyNDE2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-04T21:29:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Done in PR #110. There's a nice example of it here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/0ab2c573b8237e71e213572a8686bf7a04e2e66d/tests/test_PR110_various_cleanups.py#L20-L63",
  "created_at":"2020-02-06T00:11:27Z",
  "id":582674150,
  "issue":101,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MjY3NDE1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-06T00:11:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - all reported failures are in:\r\n```\r\n================================== FAILURES ===================================\r\n______________________ TestConstructors.test_from_dtype _______________________\r\n\r\nself = <tests.test_PR090_as_pandas_extension.TestConstructors object at 0x0000027E1088A710>\r\ndata = <Array [[], [0], [0, ... 94, 95, 96, 97, 98]] type='100 * option[var * int64]'>\r\n\r\n    def test_from_dtype(self, data):\r\n        # construct from our dtype & string dtype\r\n        dtype = data.dtype\r\n    \r\n        expected = pd.Series(data)\r\n        result = pd.Series(list(data), dtype=dtype)\r\n        self.assert_series_equal(result, expected)\r\n    \r\n        result = pd.Series(list(data), dtype=str(dtype))\r\n        self.assert_series_equal(result, expected)\r\n    \r\n        # gh-30280\r\n        \r\n>       expected = pd.DataFrame(data).astype(dtype)\r\n\r\nC:\\hostedtoolcache\\windows\\Python\\3.6.8\\x64\\lib\\site-packages\\pandas\\tests\\extension\\base\\constructors.py:69: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nC:\\hostedtoolcache\\windows\\Python\\3.6.8\\x64\\lib\\site-packages\\pandas\\core\\frame.py:474: in __init__\r\n    arrays, columns = to_arrays(data, columns, dtype=dtype)\r\nC:\\hostedtoolcache\\windows\\Python\\3.6.8\\x64\\lib\\site-packages\\pandas\\core\\internals\\construction.py:484: in to_arrays\r\n    data = [tuple(x) for x in data]\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\n.0 = <generator object Array.__iter__ at 0x0000027E1404D8E0>\r\n\r\n>   data = [tuple(x) for x in data]\r\nE   TypeError: 'NoneType' object is not iterable\r\n\r\nC:\\hostedtoolcache\\windows\\Python\\3.6.8\\x64\\lib\\site-packages\\pandas\\core\\internals\\construction.py:484: TypeError\r\n============= 1 failed, 250 passed, 1 skipped in 70.45s (0:01:10) =============\r\n\r\n```",
  "created_at":"2020-01-31T08:48:41Z",
  "id":580641995,
  "issue":102,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDY0MTk5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-31T08:48:41Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"It's that `gh-30280` test that Pandas added in version 1.0. one of the things that tests does is use the data directly in a `DataFrame` constructor, with and without translating it to a list first, and expect them to be the same. There's no way we can satisfy that with our data structures, so I ended up having to disable that one test. The disabling is in master, I piggybacked it on the pull request you did yesterday to fix integer issues. So merge from master.",
  "created_at":"2020-01-31T11:56:33Z",
  "id":580704979,
  "issue":102,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MDcwNDk3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-01-31T11:56:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Done in PR #107.",
  "created_at":"2020-02-04T21:22:28Z",
  "id":582121349,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MjEyMTM0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-04T21:22:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"There wasn't actually anything to fix. That `out` quoted above is not a UnionArray.\r\n\r\nBut we have a test that goes through this path now, and that's good. (It was broken before.)",
  "created_at":"2020-02-04T22:40:44Z",
  "id":582151166,
  "issue":108,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MjE1MTE2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-04T22:40:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@lukasheinrich: see 795ff7b2707ffef45930e7421235f9b70dc97ff9\r\n\r\nThis PR will be closed soon. It's just more time-effective to get a batch done at once.",
  "created_at":"2020-02-04T21:51:04Z",
  "id":582132637,
  "issue":109,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MjEzMjYzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-04T21:51:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@lukasheinrich Actually, we can't put a template in the .h file without introducing a compilation-dependence. Now see 7a06cac348260dfac8151315010a51517e0b430b\r\n\r\nI had forgotten that there was already a `byteptr()` function that all of these could use. If you have some other type not in the list, you can `*reinterpret_cast<T*>(byteptr())` to get it.",
  "created_at":"2020-02-04T22:06:32Z",
  "id":582139023,
  "issue":109,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MjEzOTAyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-04T22:06:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Note to self: pyawkward.cpp can turn NumPy masked arrays into unmasked arrays _without warning_. You have to catch those cases and send them to the appropriate `SliceMasked64`.\r\n\r\n**Edit:** fixed.",
  "created_at":"2020-02-06T22:00:18Z",
  "id":583131786,
  "issue":111,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzEzMTc4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-07T22:49:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"   * [x] Note to self: `SliceMasked` creates an IndexedOptionArray; be sure to `simplify` it on exit.",
  "created_at":"2020-02-08T14:51:29Z",
  "id":583743188,
  "issue":111,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4Mzc0MzE4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-09T02:17:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"   * [x] Note to self: you haven't tested slice items after a missing/jagged item, yet.",
  "created_at":"2020-02-09T01:35:28Z",
  "id":583794710,
  "issue":111,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4Mzc5NDcxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-09T02:48:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"   * [x] Note to self: you haven't tested Python lists to jagged slice in Python.",
  "created_at":"2020-02-09T02:54:59Z",
  "id":583799055,
  "issue":111,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4Mzc5OTA1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-09T05:50:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"   * [x] Remember to reinstate the `requirements-tests.txt`!",
  "created_at":"2020-02-09T12:04:47Z",
  "id":583837418,
  "issue":111,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzgzNzQxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-09T12:09:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's fixed; this PR is done. I'll merge it in the morning.",
  "created_at":"2020-02-09T12:18:39Z",
  "id":583838594,
  "issue":111,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4MzgzODU5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-09T12:18:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You probably want to merge with master soon. I split up `src/pyawkward.cpp` into a lot of little files. (It doesn't help with compilation speeds, but it is now possible to compile a library that depends on Awkward such that they can share data in Python.)",
  "created_at":"2020-02-11T04:47:14Z",
  "id":584476612,
  "issue":114,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NDQ3NjYxMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-02-11T04:47:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - please, check if the test `tests/test_PR114_rpad_operation.py` gives correct output.\r\n",
  "created_at":"2020-02-26T13:26:26Z",
  "id":591424720,
  "issue":114,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MTQyNDcyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-26T13:26:26Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I finally found the time to review your updates. Let me know by email when would be good for a long in-person code review next week. My schedule looks free.",
  "created_at":"2020-02-28T20:09:24Z",
  "id":592706693,
  "issue":114,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MjcwNjY5Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-02-28T20:09:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"replaced by https://github.com/scikit-hep/awkward-1.0/pull/132",
  "created_at":"2020-03-05T17:42:58Z",
  "id":595357352,
  "issue":114,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTM1NzM1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T17:42:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna, can this branch be deleted? Does it contain anything valuable that's not on other branches?",
  "created_at":"2020-03-10T05:19:23Z",
  "id":596908347,
  "issue":114,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjkwODM0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T05:19:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Oh, sorry, I thought I have deleted all of them :-)",
  "created_at":"2020-03-12T18:29:11Z",
  "id":598350927,
  "issue":114,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODM1MDkyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T18:29:11Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"   * [x] Make `util::Reducer` an object so that passing arbitrary functions would someday be possible.",
  "created_at":"2020-02-13T23:54:42Z",
  "id":586029846,
  "issue":115,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NjAyOTg0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-14T19:26:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Now I'm thinking that `count` is **not** a reducer. If it were a reducer with identity `0` and binary operation `(tally, next) \u2192 tally + 1`, then an array like\r\n\r\n```python\r\n[[[  2,   3,   5,   7,  11],\r\n  [ 13,  17,  19,  23,  29],\r\n  [ 31,  37,  41,  43,  47]],\r\n [[ 53,  59,  61,  67,  71],\r\n  [ 73,  79,  83,  89,  97],\r\n  [101, 103, 107, 109, 113]]]\r\n```\r\n\r\nwould have a `count(axis=0)` of\r\n\r\n```python\r\n[[1+1, 1+1, 1+1, 1+1, 1+1],\r\n [1+1, 1+1, 1+1, 1+1, 1+1],\r\n [1+1, 1+1, 1+1, 1+1, 1+1]]\r\n```\r\n\r\nbut we want an operation that returns\r\n\r\n```python\r\n[3, 3]\r\n```\r\n\r\nThe difference is that reducers go all the way down to the leaves, but we want a \"count\" operation that tells us the sizes of inner lists without being sensitive to anything deeper than the chosen `axis`, to fill the role of the frequently used `.counts` attribute of Awkward0. (And distinguishing between \"count\" and \"counts\" is not good. Too many people have already made that mistake, myself included.)\r\n\r\nSo we need to distinguish between the operation that returns `[3, 3]` (i.e. Awkward0's `.counts`) and the reducer that returns `[[2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2]]` for `axis=0`. Perhaps the word \"size\" is correct, because `np.size` does something like this in the rectilinear context:\r\n\r\n```python\r\n>>> np.size(that_array, axis=0)\r\n2\r\n>>> np.size(that_array, axis=1)\r\n3\r\n>>> np.size(that_array, axis=2)\r\n5\r\n```\r\n\r\nbut, following that logic, what people would need most of the time would be `axis=1`, which would be a little awkward to have to keep in mind. Also, Pandas makes heavy use of `np.size`, and returning surprising things from it (an array instead of an integer) might break Pandas.\r\n\r\nMaybe `ak.sizes`? The pluralization would avoid conflict with the NumPy function (and therefore operability of Pandas) and it strongly suggests an array result. Also, `axis=0` returning `[3, 3]` wouldn't be a surprise: it's not the size/length _of_ `axis=0`, it's the sizes _in_ `axis=0`. The highest possible `axis` would be one less than the highest possible `axis` for `np.size`, but it would be the same as the highest possible `axis` for `flatten`.\r\n\r\n@ianna: I'm going to make a new issue for renaming what we're currently calling \"count\" to \"sizes\" (i.e. the `count64` and `count` virtual methods).\r\n\r\n@nsmith- and @masonproffitt: you might have some opinions on this. `array.counts` is one of the most frequently used array properties in Awkward0. The spelling of this would change to `ak.sizes(array)` with a default of `axis=0`, and it would be extended to allow `axis != 0` (with the usual rules for counting backward from the leaves if the depth doesn't branch).",
  "created_at":"2020-02-15T17:41:17Z",
  "id":586623849,
  "issue":115,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NjYyMzg0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-15T17:41:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As a follow-up of the above, the need for a distinction between the reducer `count` and the new `sizes` operation (what Awkward0 called `.counts`) is illustrated by `axis=None`:\r\n\r\n   * `ak.count(array, axis=None)` returns the total number of leaf values, counting each leaf of the tree (in record fields and nested lists).\r\n   * `ak.sizes(array)` returns an array of subarray lengths, which is independent of how deep the objects they contain are. `ak.sizes` doesn't even have an `axis=None`.\r\n\r\nThe purpose of `ak.count` is to be used as a denominator in pseudo-reducers, like\r\n\r\n```python\r\ndef mean(array, axis=None):\r\n    with numpy.errstate(invalid=\"ignore\"):\r\n        return awkward1.sum(array, axis=axis) / awkward1.count(array, axis=axis)\r\n```\r\n",
  "created_at":"2020-02-15T19:11:58Z",
  "id":586632086,
  "issue":115,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NjYzMjA4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-15T19:11:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Compilation of all sources from scratch (12 core machine):\r\n\r\n```\r\nreal\t1m47.566s\r\nuser\t1m44.996s\r\nsys\t0m4.160s\r\n```\r\n\r\nLinking only (no files touched):\r\n\r\n```\r\nreal\t0m28.955s\r\nuser\t0m28.361s\r\nsys\t0m1.152s\r\n```\r\n\r\n1/3 of the compilation time is spent only making the `awkward1.layout` extension module. It's unclear that refactoring this into several extension modules would make an impact on full compilations (depends on whether linking is nonlinear or making separate `.so` files can be done in parallel\u2014linking one is single-threaded), but it would definitely make an impact on partial recompilations, in which only the classes in one of the smaller `.so` files are touched.\r\n\r\nAlso, calling types, fillables, etc. \"layouts\" is an abuse of terminology. Only the `Content` subclasses are layouts.",
  "created_at":"2020-02-10T13:46:52Z",
  "id":584129913,
  "issue":116,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NDEyOTkxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-10T13:46:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm away from my big computer right now, but I don't think this solved the compilation speed issue. Nevertheless, it was a necessary refactoring.",
  "created_at":"2020-02-11T00:11:15Z",
  "id":584421009,
  "issue":116,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NDQyMTAwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-11T00:11:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@glass-ships and @lukasheinrich:\r\n\r\nYou both want to create projects that depend on Awkward. While I understand that @lukasheinrich isn't currently considering round-tripping the data through Python, you should both be aware that I've tested compiling a dependent Python module against Awkward, and it works in MacOS and Linux. (I haven't figured out a CMake issue in Windows, yet.)\r\n\r\nHere's an example, which I should also link from the README.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/tree/feature/PR116-refactor-pyawkward/dependent-project\r\n\r\nThe dependent project has either the static or dynamic libraries as a compile-time dependency. (The example shows it for static: just remove the `-static` in [CMakeLists.txt](https://github.com/scikit-hep/awkward-1.0/blob/feature/PR116-refactor-pyawkward/dependent-project/CMakeLists.txt).) It can then use `std::shared_ptr<ak::Content>` as an argument or a return type from its functions, and when the dependent project is compiled as a Python module with pybind11, pybind11 links those C++ objects to the right Python objects.\r\n\r\nSome caveats:\r\n\r\n   * The Windows issue I referred to, but it would probably just involve having access to a Windows machine or help from a Windows expert.\r\n   * You're compiling against a particular version of Awkward, so your dependent project will be pinned to that version.\r\n   * Fortunately or unfortunately, depending on your point of view, you're also compiling against a particular version of pybind11. I've asked about this on the [pybind11 Gitter](https://gitter.im/pybind/Lobby), and it sounds like there are intervals of compatibility, which might or might not be related to semantic version number. The reason this is \"fortunate\" is because it gives you a nice Python exception, rather than a random segfault deep in an Awkward routine because the C++ layout is different. When I release a version of Awkward, it should always include the pybind11 version number in the release notes so that dependent projects compiling against Awkward X.Y know that they need pybind11 version Z.\r\n\r\nCan the linking be purely dynamic? I'm not sure. I would consider that a valuable goal, but I don't know if it can be achieved.\r\n\r\n@henryiii may also have some good ideas about this.",
  "created_at":"2020-02-11T00:26:16Z",
  "id":584424971,
  "issue":116,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NDQyNDk3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-11T00:26:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"To confirm that this refactoring didn't do a thing for compilation time, here's compilation of all sources from scratch (same 12 core machine).\r\n\r\n```\r\nreal\t2m1.092s\r\nuser\t1m58.334s\r\nsys\t0m5.153s\r\n```\r\n\r\nIt's about 10 seconds longer.\r\n\r\nLinking only (no files touched):\r\n\r\n```\r\nreal\t0m31.471s\r\nuser\t0m30.691s\r\nsys\t0m1.359s\r\n```\r\n\r\nIt's about 2 seconds longer.",
  "created_at":"2020-02-11T16:41:23Z",
  "id":584728555,
  "issue":116,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NDcyODU1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-11T16:41:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"For what it's worth, there's a new `branch_depth` method on all `Content` subclasses that reducers needed for their logic. As it turns out, `minmax_depth == purelist_depth` wasn't right for that case; I had to implement different behavior depending on whether any records or unions cause branches.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/395def2a9481bbf07d30d1a37a7875ec619ceba5/src/libawkward/Content.cpp#L77-L97\r\n\r\nIt may be that my original `minmax_depth` and `purelist_depth` are irrelevant. If, for example, there are two RecordArrays in the tree and one splits the min/max into `(2, 3)` while the other unsplits them into `(4, 4)`, we still want to treat that as a branching depth, only allowing negative `axis` up to the first branch point. The new `branch_depth` method returns a boolean (\"is the depth ever different?\") and the minimum depth (\"what's the largest possible `-axis`?\").\r\n\r\nImplementing `axis` for `sizes` might need to do something _similar_ to that. This is definitely something to work out in a Python-based study.",
  "created_at":"2020-02-15T19:03:50Z",
  "id":586631428,
  "issue":117,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NjYzMTQyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-15T19:03:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Another possible option is `ak.length` (or plural), one advantage is there is no `np.length` or `pd.length`, making it a bit more orthogonal.",
  "created_at":"2020-02-15T21:38:04Z",
  "id":586643696,
  "issue":117,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NjY0MzY5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-15T21:38:04Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"True, and we should consider that. Internally in Awkward, however, `length` is always used as the number of elements in an array and `size` is used as the number of elements in each subarray of a RegularArray. The fact that we use this terminology internally isn't a strong argument for presenting a different terminology externally.\r\n\r\nThe lack of any antecedent in NumPy could be good or bad: it provides no guidance but it also doesn't misguide. The fact that `np.size(array, axis=X)` returns the number of elements at level `X` and we want `ak.sizes(array, axis=X)` to return the number of elements in each subarray at level `X + 1` is potentially an issue that another word would help with.\r\n\r\nI've got to think about it.\r\n\r\n(One thing, though, it definitely needs to be plural: `ak.lengths`.)",
  "created_at":"2020-02-15T21:48:23Z",
  "id":586644431,
  "issue":117,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4NjY0NDQzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-15T21:48:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Oh, I remember that. It was implemented as described above, then I extended it to an extra depth to return a one dimensional NumpyArray as in `counts(axis)` from `study/flatten.py`\r\n\r\nEither `lengths` or `sizes` is less confusing then `counts`.\r\n\r\nIntuitively, I\u2019d say `length` is constant while `size` isn\u2019t, but it\u2019s from Java/C++ world.",
  "created_at":"2020-02-17T07:02:36Z",
  "id":586845913,
  "issue":117,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4Njg0NTkxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-17T12:41:55Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Some changes in definition, implemented in #152: (1) I shifted the meaning of `axis` by one, so that it's always describing lengths of the specified axis, rather than one deeper. So in my original example,\r\n\r\n```python\r\n[[[  2,   3,   5,   7,  11],\r\n  [ 13,  17,  19,  23,  29],\r\n  [ 31,  37,  41,  43,  47]],\r\n [[ 53,  59,  61,  67,  71],\r\n  [ 73,  79,  83,  89,  97],\r\n  [101, 103, 107, 109, 113]]]\r\n```\r\n\r\n   * `axis=0` now returns `2`, the scalar length of the array;\r\n   * `axis=1` now returns `[3, 3]` (what the old `axis=0` would have returned)\r\n   * `axis=2` now returns `[[5, 5, 5], [5, 5, 5]]` (what the old `axis=1` would have returned).\r\n   * no `axis` values that correspond to an axis in the array (this one has only three: `0`, `1`, and `2`) are illegal, although `axis=0` is qualitatively different from the rest. (That's a recurring theme in these operations.)\r\n\r\nMajor change number (2) is that I've changed the name again, from \"sizes\" to \"num\". This will allow physicists to write very readable code, like\r\n\r\n```python\r\nak.num(muons)     # implicit axis=1\r\n```\r\n\r\nand\r\n\r\n```python\r\nmuons[ak.num(muons) >= 2]\r\n```\r\n\r\nThere is no collision in the NumPy namespace with a function named \"num\", and it's far from any of NumPy's existing concepts (unlike \"size\").",
  "created_at":"2020-03-07T04:57:58Z",
  "id":596047227,
  "issue":117,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjA0NzIyNw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-03-07T04:57:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Slow laptop with all tests in:\r\n\r\n```\r\nreal    0m50.159s\r\nuser    0m49.790s\r\nsys     0m0.679s\r\n```\r\n\r\nSame laptop with Numba tests removed (16 out of 261):\r\n\r\n```\r\nreal    0m4.444s\r\nuser    0m4.238s\r\nsys     0m0.564s\r\n```\r\n\r\n`:)`",
  "created_at":"2020-02-19T12:32:30Z",
  "id":588202398,
  "issue":118,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU4ODIwMjM5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-19T12:32:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I really don't know what I can do about union types. I can propagate some sort of `MultiViewType` to each of its uses, with a `MultiViewModel` that keeps a pointer to the tag and can instantiate as the right kind of thing, but there would be cases in which a number would be a multiview (because as far as the type-checking is concerned, it _could be_ a number, even if this happens to be impossible, given a set of `tags`). Users expect numbers to behave like numbers: they have to support addition, numerical functions, etc., which is an expansive set of operations. For a multiview to look like a number, it would have to support all of those operations, too, though it might raise exceptions for the cases in which it's not actually a number. But how would we implement _everything_ that's legal for a number on multiview? (It would be a simple implementation: if the `tag` says it's really a number, apply the operation; otherwise, raise exception. The problem is registering behaviors for that wide-open set of operations.)",
  "created_at":"2020-02-24T20:50:32Z",
  "id":590542012,
  "issue":118,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MDU0MjAxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-24T20:50:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This PR eliminated 24,000 lines of code.   `:)`",
  "created_at":"2020-02-25T21:20:28Z",
  "id":591074892,
  "issue":118,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MTA3NDg5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-25T21:20:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nope, forgot about the moved directory; now this PR has eliminated 28,000 lines of code.",
  "created_at":"2020-02-25T21:39:14Z",
  "id":591083050,
  "issue":118,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MTA4MzA1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-25T21:39:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Some baseline compilation times on a slow (Chromebook) computer:\r\n\r\n   * full compilation time in release mode\r\n\r\n```\r\nreal    3m20.192s\r\nuser    3m10.504s\r\nsys     0m12.654s\r\n```\r\n\r\n   * linking-only in release mode\r\n\r\n```\r\nreal    0m52.832s\r\nuser    0m50.578s\r\nsys     0m3.352s\r\n```\r\n\r\n   * full compilation in debug mode\r\n\r\n```\r\nreal    4m49.084s\r\nuser    4m45.499s\r\nsys     0m14.281s\r\n```\r\n\r\n   * linking-only in debug mode\r\n\r\n```\r\nreal    0m8.494s\r\nuser    0m6.615s\r\nsys     0m1.735s\r\n```\r\n\r\nThat's an interesting trade-off.",
  "created_at":"2020-02-29T16:07:43Z",
  "id":592960550,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5Mjk2MDU1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-02-29T16:07:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@henryiii I've fully merged in #130 and tested it, including ArrayBuilder in Numba (which uses ctypes to access libawkward.so).",
  "created_at":"2020-03-01T01:14:31Z",
  "id":593025711,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MzAyNTcxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-01T01:14:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Remember, we need to manually squash to 2-3 commits then rebase and merge, since we do a large renaming and several edits; if we just squash and merge git will loose the history tracking on the python files in src/awkward1. Let me know when you are ready and I can do it.",
  "created_at":"2020-03-02T04:11:29Z",
  "id":593209743,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MzIwOTc0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-02T04:11:29Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"@henryiii It's ready. Please do it.",
  "created_at":"2020-03-02T04:13:23Z",
  "id":593210051,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MzIxMDA1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-02T04:13:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I copied this branch as [save/PR129-henryiii-build-procedure](https://github.com/scikit-hep/awkward-1.0/tree/save/PR129-henryiii-build-procedure).",
  "created_at":"2020-03-02T04:30:07Z",
  "id":593213376,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MzIxMzM3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-02T04:30:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski and @henryiii - nice work, local builds are a lot faster! Thanks!\r\n",
  "created_at":"2020-03-04T09:10:23Z",
  "id":594405994,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NDQwNTk5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-04T09:10:23Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ok, the failures in the tests for negative axis are expected:\r\n```python\r\n>       assert awkward1.tolist(array.sizes(-1)) == [3, 3, 3, 0, 5, 2, 4, 4]\r\nE       RuntimeError: FIXME: negative axis not implemented yet\r\n\r\n```",
  "created_at":"2020-03-04T20:02:39Z",
  "id":594802507,
  "issue":132,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NDgwMjUwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-04T20:03:02Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, I'm finished with code migration. Please, review the PR when you have time.\r\n\r\nThe `rpad` operation at `axis = 0` is either the preserves the original array type or returns an option of it.\r\nThe `rpad` operation at `axis = 1` return type is:\r\n```python\r\n'var * float64' == 'var * ?float64'\r\n```",
  "created_at":"2020-03-05T17:18:45Z",
  "id":595346222,
  "issue":132,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTM0NjIyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T17:18:45Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna I'm reviewing it now.",
  "created_at":"2020-03-06T12:13:34Z",
  "id":595742053,
  "issue":132,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTc0MjA1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T12:13:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, all your comments are addressed. ",
  "created_at":"2020-03-06T22:31:33Z",
  "id":595992732,
  "issue":132,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTk5MjczMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T22:31:33Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I agree that it's done. I just added a high-level interface and interacted with it on the command line. It behaved in a natural way, except for one detail:\r\n\r\n```python\r\n>>> array = ak.Array([1, 2, 3, 4, 5])\r\n>>> ak.rpad(array, 7, axis=0)\r\n<Array [1, 2, 3, 4, 5, None, None] type='7 * ?int64'>\r\n>>> ak.rpad(array, 3, axis=0)\r\n<Array [1, 2, 3, 4, 5] type='5 * int64'>\r\n>>> ak.rpad(array, 3, axis=0, clip=True)\r\n<Array [1, 2, 3] type='3 * ?int64'>\r\n```\r\n\r\nIf the `axis` is `0` and the target length is less than the actual length of the array, the type is not an option type (`int64` above, rather than `?int64`). For consistency\u2014to avoid surprises in scripts\u2014I'm going to want to make that an option type as well. `axis > 0` does not have this issue:\r\n\r\n```python\r\n>>> array = ak.Array([[1, 2], [3], [4, 5]])\r\n>>> ak.rpad(array, 2)\r\n<Array [[1, 2], [3, None], [4, 5]] type='3 * var * ?int64'>\r\n>>> ak.rpad(array, 1)\r\n<Array [[1, 2], [3], [4, 5]] type='3 * var * ?int64'>\r\n>>> ak.rpad(array, 1, clip=True)\r\n<Array [[1], [3], [4]] type='3 * 1 * ?int64'>\r\n```\r\n\r\nbecause we can't know the lengths of inner lists before deciding what the structure of the output is going to look like.\r\n\r\nHowever, we should leave this unfixed for now (maybe make an issue). The \"UnmaskedArray\" class would be ideally suited to solve this: instead of returning a `shallow_copy` from `Content::rpad_axis0`, we would return\r\n\r\n```c++\r\nstd::make_shared<UnmaskedArray>(Identities::none(), util::Parameters(), shallow_copy());\r\n```\r\n\r\nwhere UnmaskedArray gives something option type, but none of the values are actually `None`. It's a formality we can deal with later.\r\n\r\nBut other than that formality, this is great; I think we should merge it! Give me an indication that you're really done (that you don't have any local changes left to submit) and I'll merge it.\r\n\r\nThanks a lot!",
  "created_at":"2020-03-09T13:58:34Z",
  "id":596539699,
  "issue":132,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjUzOTY5OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-03-09T13:58:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks, @jpivarski! Yes, please, merge it.\r\n",
  "created_at":"2020-03-09T14:03:51Z",
  "id":596542667,
  "issue":132,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjU0MjY2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-09T14:03:51Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll do that. Do you want to call on Skype and talk about what's next?",
  "created_at":"2020-03-09T14:04:59Z",
  "id":596543283,
  "issue":132,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjU0MzI4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-09T14:04:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I'll do that. Do you want to call on Skype and talk about what's next?\r\n\r\nYes, please :-) \r\n",
  "created_at":"2020-03-09T14:05:50Z",
  "id":596543783,
  "issue":132,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjU0Mzc4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-09T14:05:50Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm online now.",
  "created_at":"2020-03-09T14:06:05Z",
  "id":596543905,
  "issue":132,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjU0MzkwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-09T14:06:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"So it wasn't really the Identities themselves, just the assertions (see comment above). The reason I've never hit this before is that I'm only now compiling in Debug mode (to get faster _recompilations_). Most of the assertions were added early in the project and weren't tested. These were \"wrong\" in that they had off-by-one errors in their allowed range\u2014an easy mistake to make.\r\n\r\nNow all the tests are in and we're back to the level of testing we had before the change in build procedure (a little over 2000 assertions). I'll merge this as soon as the tests are green.",
  "created_at":"2020-03-02T23:29:32Z",
  "id":593676813,
  "issue":133,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MzY3NjgxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-02T23:29:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Also, this was a first test-run of the new localbuild.py script. It's a fantastic debugging workflow!",
  "created_at":"2020-03-02T23:30:30Z",
  "id":593677135,
  "issue":133,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5MzY3NzEzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-02T23:30:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It was a mistake I had previously fixed for non-tuple RecordArrays, and it was right next to my mistake for the tuple RecordArrays. It would not have been fixed by transitioning `setidentities` \u2192 `withidentities`.\r\n\r\nGood catch, Windows!",
  "created_at":"2020-03-03T20:02:30Z",
  "id":594141341,
  "issue":133,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NDE0MTM0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-03T20:02:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This was based on a misunderstanding. Close it to reduce clutter.",
  "created_at":"2020-03-03T18:41:06Z",
  "id":594103738,
  "issue":134,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NDEwMzczOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-03T18:41:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Similar to the [old one](https://github.com/scikit-hep/awkward-array/blob/master/awkward/persist.py), but it doesn't have to be exactly the same. It really should be in C++, and that already limits the possibility of compatibility.\r\n\r\nAlso, Awkward1 has a different set of node types than Awkward0.",
  "created_at":"2020-03-18T05:33:46Z",
  "id":600433483,
  "issue":136,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDQzMzQ4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-18T05:33:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #348 added a serialization protocol in Python. There are two new functions, [ak.to_arrayset](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_arrayset.html) and [ak.from_arrayset](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_arrayset.html), which turn an Awkward Array into a schema and a collection of flat arrays (an \"arrayset\"):\r\n\r\n```python\r\n>>> original = ak.Array([[1, 2, 3], [], [4, 5]])\r\n>>> form, container, num_partitions = ak.to_arrayset(original)\r\n>>> form\r\n{\r\n    \"class\": \"ListOffsetArray64\",\r\n    \"offsets\": \"i64\",\r\n    \"content\": {\r\n        \"class\": \"NumpyArray\",\r\n        \"itemsize\": 8,\r\n        \"format\": \"l\",\r\n        \"primitive\": \"int64\",\r\n        \"form_key\": \"node1\"\r\n    },\r\n    \"form_key\": \"node0\"\r\n}\r\n>>> container\r\n{'node0-offsets': array([0, 3, 3, 5], dtype=int64),\r\n 'node1': array([1, 2, 3, 4, 5])}\r\n>>> print(num_partitions)\r\nNone\r\n```\r\n\r\nUnless there's a strong call for a C++ version, the serialization protocol in Python will be considered good enough.",
  "created_at":"2020-07-27T23:42:35Z",
  "id":664693669,
  "issue":136,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDY5MzY2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T23:42:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@henryiii Since it looks like you're not working on this branch at the moment, I'm going to use it to try to fix the Windows part. That seems to be orthogonal to what you're doing. I'll be temporarily turning off all deployment builds except Windows, so we probably can't work on it at the same time. Let me know if this affects your plans.",
  "created_at":"2020-03-05T16:13:38Z",
  "id":595312633,
  "issue":142,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTMxMjYzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T16:13:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think I nearly had it, fixed the one mistake, so macOS and Linux should work when they come back on.\r\n\r\n(Sorry, forgot to skip ci!)",
  "created_at":"2020-03-05T17:00:05Z",
  "id":595336630,
  "issue":142,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTMzNjYzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T17:00:05Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Because you are using your own personal Azure instead of the Scikit-HEP one, I can't cancel the build. Feel free to do so.",
  "created_at":"2020-03-05T17:02:34Z",
  "id":595337901,
  "issue":142,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTMzNzkwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T17:02:34Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"@henryiii I'm done with WOWW. Anything more that you want to do with the PR is up to you; let me know when I should merge it.\r\n\r\nI'd prefer to use the Scikit-HEP Azure. That's another thing that we should switch over to. Not today, though.",
  "created_at":"2020-03-05T17:16:15Z",
  "id":595345027,
  "issue":142,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTM0NTAyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T17:16:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"After tests pass (are they running?), if you could trigger a deploy just to make sure everything works and so I can see what the output of the `auditwheel show` is, that should be great.",
  "created_at":"2020-03-05T17:18:40Z",
  "id":595346184,
  "issue":142,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTM0NjE4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T17:18:40Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I think the tests are waiting on Yana's \"final cleanup\" in PR #132 to finish. Just a minute or two...",
  "created_at":"2020-03-05T17:22:01Z",
  "id":595347716,
  "issue":142,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTM0NzcxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T17:22:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"**To Jim:** you're in the middle of working on merge (`merge`, `mergeable`, and `reverse_merge`).\r\n\r\nAll three masked array types have these functions defined; IndexedOptionArray, ByteMaskedArray, BitMaskedArray, and UnmaskedArray are all aware of each other. However, all the other array types have to check for the three masked array types wherever they check for IndexedOptionArray in their merging tests.",
  "created_at":"2020-03-14T19:59:59Z",
  "id":599128549,
  "issue":143,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5OTEyODU0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-14T19:59:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The following names have been changed:\r\n\r\n   * `FillableArray` \u2192 `ArrayBuilder`\r\n   * `FillableOptions` \u2192 `ArrayBuilderOptions`\r\n   * `fillable` (in any other context) \u2192 `builder`\r\n\r\nIt's still early days and @henryiii found the name \"fillable\" to suggest an already allocated array to which we're inserting values, rather than a thing that grows. I agreed. I also didn't like the way that \"Array\" suggests that it's the same sort of thing as \"ListArray\", \"RecordArray\", etc. It's not\u2014you have to `snapshot` for that. That's why there was a name change.\r\n\r\nSorry for the lack of notice!",
  "created_at":"2020-03-04T22:42:41Z",
  "id":594905806,
  "issue":145,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NDkwNTgwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-04T22:42:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"No worries, that makes a lot of sense. thanks the context as well! ",
  "created_at":"2020-03-04T22:55:40Z",
  "id":594913700,
  "issue":145,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NDkxMzcwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-04T22:55:40Z",
  "user":"MDQ6VXNlcjI2OTc1NTMw"
 },
 {
  "author_association":"MEMBER",
  "body":"The latest release and master are not far apart\u2014usually there's no difference between them.\r\n\r\nAbout this error, you gave it a Python list of two NumPy arrays. That pattern isn't convertible to an array, but I suppose it could be. Do you want it to mean a regular array with shape `(2, 3)` or a jagged array of length `2` that happens to have `3` elements in each subarray? It maybe you intended to write\r\n\r\n```python\r\nak.Array([[1, 2, 3], [4, 5, 6]])\r\n```\r\n\r\n(which gives you a jagged array) or\r\n\r\n```python\r\nak.Array(np.array([[1, 2, 3], [4, 5, 6]]))\r\n```\r\n\r\n(which gives you a regular array).\r\n\r\nIt's not too early to submit bug reports like this. This constructor goes through\r\n\r\n```python\r\nawkward1.operations.convert.fromiter\r\n```\r\n\r\nCheck that and see if it's what you want. You might want to add another case, it a better error message.",
  "created_at":"2020-03-05T11:31:44Z",
  "id":595182941,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTE4Mjk0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T11:32:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the quick answer, as always!\r\n\r\nMy MWE was a bit too minimal ;) I am dealing with differing array lengths and even nested structures, so wrapping it into a `numpy.array` doesn't work as it yields `dtype=object`:\r\n\r\n```python\r\n[ins] In [20]: a = ak.Array(np.array([[1,2,3], [3,4,5,6]]))                \r\n\r\n[ins] In [21]: a                                                           \r\nOut[21]: ---------------------------------------------------------------------------\r\n...\r\n...\r\nValueError: Numpy format \"O\" cannot be expressed as a PrimitiveType\r\n```\r\n\r\nBut coming back to this `numpy` vs `list` interpretation, I think intuitively I'd expect that something like `[np.array(...), np.array(...)]` would help even more since it carries type information, so I'd expect to behave exactly like `[list(), list()]` with less type inference logic behind the scenes.",
  "created_at":"2020-03-05T16:53:05Z",
  "id":595333062,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTMzMzA2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T16:53:05Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Now that you've put different lengths into each subarray, it can't be a \"normal\" NumPy array and NumPy represents it with dtype `\"O\"` (an array of pointers to Python objects; useless for accelerating numerical code). So\r\n\r\n```python\r\nak.Array(np.array([[1,2,3], [3,4,5,6]]))\r\n```\r\n\r\ncan't be an allowed procedure\u2014NumPy has already broken the data before we get to it. On the other hand, maybe what we should do with NumPy `\"O\"` arrays is just treat them like Python lists and iterate over them? It's all a question of what you want the interface to do.\r\n\r\nYou're right that\r\n\r\n```python\r\nak.Array(np.array([[1, 2, 3], [4, 5, 6]])     # internally calls fromnumpy\r\n```\r\n\r\nis more lightweight than\r\n\r\n```python\r\nak.Array([[1, 2, 3], [4, 5, 6]])              # internally calls fromiter\r\n```\r\n\r\nThe first casts the NumPy array (without even copying it, using [fromnumpy](https://github.com/scikit-hep/awkward-1.0/blob/a1b7cb7239530c8c38b96fdf1cb03d7217288050/src/awkward1/operations/convert.py#L20-L50)) and the second iterates and performs type-inference on the fly (using [fromiter](https://github.com/scikit-hep/awkward-1.0/blob/a1b7cb7239530c8c38b96fdf1cb03d7217288050/src/awkward1/operations/convert.py#L52-L60)). However, they result in different types:\r\n\r\n```python\r\n>>> ak.typeof(ak.Array(np.array([[1, 2, 3], [4, 5, 6]])))\r\n2 * 3 * int64\r\n>>> ak.typeof(ak.Array([[1, 2, 3], [4, 5, 6]]))\r\n2 * var * int64\r\n```\r\n\r\nThe first array has `size=3` burned into its type, whereas the second is `size=var`. It happens to have equal sizes in this case, but since it's not part of its type, it behaves differently\u2014it behaves as though the sizes are variable (i.e. it's jagged).\r\n\r\nWhen I say, define what you'd _like_ it to do, I mean open a PR and propose a behavior for a case that currently errors-out. If you want NumPy dtype `\"O\"` to defer to `fromiter`, that's adding a one line check in `fromnumpy`. If you want lists of NumPy arrays to also iterate, then that's a one line check in `fromiter`. (Maybe two lines\u2014I exaggerate.)",
  "created_at":"2020-03-05T17:10:05Z",
  "id":595342209,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTM0MjIwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T17:10:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes I agree ;) I will think about it.\r\n\r\nFor numpy `O`-dtypes I wouldn't care too much to be honest. I am just a bit confused about the behaviour of lists (or iterators) of numpy arrays.\r\n\r\nI'll walk through the internals of `awkward1` with the debugger, I still need to learn how it works before I can make meaningful suggestions, besides high-level API design requests :wink: ",
  "created_at":"2020-03-05T17:24:51Z",
  "id":595349035,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTM0OTAzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-05T17:24:51Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know if you're still thinking about this. It's okay to come back to it later.",
  "created_at":"2020-03-10T21:23:40Z",
  "id":597324816,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzMyNDgxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T21:23:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Sorry for the late response:  yes I am still thinking about it. Meanwhile I set up the dev environment and playing around with it and have more questions and ideas already \ud83d\ude09 \r\n\r\nI think in first place it would be nice if the `fromiter` method would first look up potential valid (not `\"O\"`) dtypes, so those could be inferred. This would solve that `[[1,2,3], [4,5,6]]` would create the same awkward array as `[np.array([1,2,3]), np.array([4,5,6])]`.",
  "created_at":"2020-03-10T21:30:59Z",
  "id":597327794,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzMyNzc5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T21:30:59Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"One thing we should be careful to keep in mind is that `var * float64` is a different type from `N * float64` for some integer `N`\u2014a difference that data analyst users must be aware of\u2014and constructor signatures involving NumPy arrays are for producing `N` types while non-NumPy iterables are for producing `var` types. If NumPy arrays are _inside_ a variable-length container like a Python list, they should probably be treated as generic iterables (i.e. produce `var` types). The rules for whether `var` types or `N` types are produced should be as guessable as possible.\r\n\r\nIf you want to follow this particular rabbit hole, the Python-side `fromiter` function is actually pretty short:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e60faf1bf1815b44124a71f444c969194118481d/src/awkward1/operations/convert.py#L52-L60\r\n\r\nIt goes straight into C++ (where iteration over the Python objects was considerably faster, even though they have to do the same Python introspection). It's here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e60faf1bf1815b44124a71f444c969194118481d/src/python/layout/content.cpp#L512-L564\r\n\r\nI'm a little surprised that a NumPy array doesn't count as a `py::iterable`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e60faf1bf1815b44124a71f444c969194118481d/src/python/layout/content.cpp#L553\r\n\r\nI must have been assuming that it would when I wrote that. However, we can add an explicit test for `py::array` and then iterate over it as though it were a `py::iterable`.\r\n\r\nAnything produced by `ak::ArrayBuilder` is of `var` type, so that would handle my concern above.",
  "created_at":"2020-03-10T21:52:43Z",
  "id":597335854,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzMzNTg1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T21:52:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"The `var` and `N` type thing makes of course perfect sense, I totally forgot about the implications. I just started using `pybind11` in serious projects so my knowledge is very limited. No idea why a NumPy array is not < `py::iterable`, the public docs are also very sparse on it \ud83d\ude09 https://pybind11.readthedocs.io/en/stable/reference.html#_CPPv48iterable",
  "created_at":"2020-03-10T23:22:12Z",
  "id":597367419,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM2NzQxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T23:22:12Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"The specific issue you were asking about has been resolved, so I'll be closing this.\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> import numpy as np\r\n\r\n>>> ak.Array([[1, 2, 3], [4, 5, 6, 7]])\r\n<Array [[1, 2, 3], [4, 5, 6, 7]] type='2 * var * int64'>\r\n\r\n>>> ak.Array([np.array([1, 2, 3]), np.array([4, 5, 6, 7])])\r\n<Array [[1, 2, 3], [4, 5, 6, 7]] type='2 * var * int64'>\r\n```",
  "created_at":"2020-03-11T23:40:56Z",
  "id":597933567,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzkzMzU2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-11T23:40:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you Jim!",
  "created_at":"2020-03-12T06:46:47Z",
  "id":598032699,
  "issue":146,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODAzMjY5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T06:46:47Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I think it's a shallow copy (could be made explicit with copy.copy(ak.behavior), and you could skip this step if `user_supplied` is None (assuming you are then okay with changing ak.behavior also changing here - otherwise you always need a copy).",
  "created_at":"2020-03-06T16:04:13Z",
  "id":595836682,
  "issue":149,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTgzNjY4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T16:04:13Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"This Mapping applies overrides if they exist and defaults if they don't, without a shallow copy every time. (It happens quite frequently, before every `ak.Array` constructor, and the number of behavioral overrides may be large.)\r\n\r\nAlso, it enforces the equivalence between missing keys and `None` values, so that an application can't accidentally rely on the distinction. Thus, it is always possible for overrides to explicitly opt out of certain defaults, by overriding them with `None`.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/5a16540bf171814a2f405e66bc0d8359a32f80ce/src/awkward1/_util.py#L34-L63\r\n\r\nThe `items` method is explicitly implemented because it is used a lot\u2014much more than `__iter__`.\r\n\r\nAlso note that this is not public API (it's in `awkward1._util`).",
  "created_at":"2020-03-06T21:37:16Z",
  "id":595975738,
  "issue":149,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTk3NTczOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T21:37:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Note for testing: the swallowed error occurs when calling a method of a mixin causes an unrelated attribute error. So if `.mass` is called, but that throws an attribute error for some other reason (forgetting a subpackage import in my case).",
  "created_at":"2020-03-06T16:49:39Z",
  "id":595857841,
  "issue":150,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTg1Nzg0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T16:49:39Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Using `where in dir(type(self))` seems to stop the error swallowing and it working for me at the moment. A note as to why this check is needed would be a good idea, too, as I don't think I wold have known why it was being done if it wasn't for the fact it wasn't doing what it was supposed to be doing.",
  "created_at":"2020-03-06T16:53:50Z",
  "id":595859756,
  "issue":150,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTg1OTc1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T16:54:54Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm working on this problem and am unable to reproduce it. How is your situation different?\r\n\r\n```python\r\nclass Common(object):\r\n    @property\r\n    def notbroken(self):\r\n        return 3.14\r\n\r\n    @property\r\n    def broken(self):\r\n        raise ValueError(\"broken!\")\r\n\r\nclass Dummy(awkward1.Record, Common):\r\n    pass\r\n\r\ndef test():\r\n    behavior = {}\r\n    behavior[\"Dummy\"] = Dummy\r\n\r\n    array = awkward1.Array([{\"x\": 1}, {\"x\": 2}, {\"x\": 3}], behavior=behavior)\r\n    array.layout.setparameter(\"__record__\", \"Dummy\")\r\n\r\n    print(array[1].broken)\r\n```",
  "created_at":"2020-03-06T20:15:23Z",
  "id":595947321,
  "issue":150,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTk0NzMyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T20:15:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I can't even reproduce it with an Array (or with methods instead of properties, or reversing the order of inheritance...).\r\n\r\n```python\r\nclass Common(object):\r\n    @property\r\n    def notbroken(self):\r\n        return 3.14\r\n\r\n    @property\r\n    def broken(self):\r\n        raise ValueError(\"broken!\")\r\n\r\nclass Dummy(awkward1.Array, Common):\r\n    pass\r\n\r\ndef test():\r\n    behavior = {}\r\n    behavior[\"Dummy\"] = Dummy\r\n\r\n    array = awkward1.Array([{\"x\": 1}, {\"x\": 2}, {\"x\": 3}], behavior=behavior)\r\n    array.layout.setparameter(\"__array__\", \"Dummy\")\r\n\r\n    print(array[:].broken)\r\n```",
  "created_at":"2020-03-06T20:21:47Z",
  "id":595949558,
  "issue":150,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTk0OTU1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T20:21:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It only swallows AttributeErrors, since that causes Python to keep looking. Just change ValueError to AttributeError.\r\n\r\n",
  "created_at":"2020-03-06T20:35:23Z",
  "id":595954468,
  "issue":150,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTk1NDQ2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T20:35:23Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'd show an example, but I've already applied the fix listed at the top so both of them work for me (but you can tell a small difference in the traceback).",
  "created_at":"2020-03-06T20:36:39Z",
  "id":595954883,
  "issue":150,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTk1NDg4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T20:37:54Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Got it:\r\n\r\n```python\r\nclass Dummy(awkward1.Record):\r\n    @property\r\n    def broken(self):\r\n        raise AttributeError(\"I'm broken!\")\r\n\r\ndef test():\r\n    behavior = {}\r\n    behavior[\"Dummy\"] = Dummy\r\n\r\n    array = awkward1.Array([{\"x\": 1}, {\"x\": 2}, {\"x\": 3}], behavior=behavior)\r\n    array.layout.setparameter(\"__record__\", \"Dummy\")\r\n\r\n    print(array[1].broken)\r\n```\r\n```\r\nE               AttributeError: no field named 'broken'\r\n```",
  "created_at":"2020-03-06T20:37:17Z",
  "id":595955102,
  "issue":150,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTk1NTEwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T20:37:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2020-03-06T20:37:24Z",
  "id":595955143,
  "issue":150,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTk1NTE0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T20:37:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, I get:\r\n\r\n```\r\nIn [5]: test()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-5-fbd55f77ab7c> in <module>\r\n----> 1 test()\r\n\r\n<ipython-input-4-48f554f51928> in test()\r\n     18     array.layout.setparameter(\"__record__\", \"Dummy\")\r\n     19\r\n---> 20     print(array[1].broken)\r\n     21\r\n\r\n~/git/scikit-hep/vector/.env/lib/python3.7/site-packages/awkward1/highlevel.py in __getattr__(self, where)\r\n    220     def __getattr__(self, where):\r\n    221         if where in dir(self.__class__):\r\n--> 222             return super(Record, self).__getattribute__(where)\r\n    223         else:\r\n    224             if where in self._layout.keys():\r\n\r\n<ipython-input-4-48f554f51928> in broken(self)\r\n      6     @property\r\n      7     def broken(self):\r\n----> 8         raise AttributeError(\"broken!\")\r\n      9\r\n     10 class Dummy(awkward1.Record, Common):\r\n\r\nAttributeError: broken!\r\n```\r\n\r\nWhich is much better.",
  "created_at":"2020-03-06T20:39:04Z",
  "id":595955738,
  "issue":150,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NTk1NTczOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-06T20:40:23Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to leave this open to do `flatten` as well. It's coupled: some of the tests had to be disabled because `flatten` depended on `count64`, and now those functions are gone. I don't want to put skipped tests into master.",
  "created_at":"2020-03-07T05:00:25Z",
  "id":596047396,
  "issue":152,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjA0NzM5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-07T05:00:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"See https://github.com/scikit-hep/awkward-1.0/issues/117#issuecomment-596047227 for an explanation of why \"num\" is a good name (e.g. `ak.num(muons) >= 2`).",
  "created_at":"2020-03-07T05:01:51Z",
  "id":596047500,
  "issue":152,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NjA0NzUwMA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-03-07T05:01:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"nicely done!",
  "created_at":"2020-03-10T08:48:31Z",
  "id":596969929,
  "issue":152,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5Njk2OTkyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T08:48:31Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I do not know how to handle `NumpyArray` of `PyObjects`, e.g. `format_ == O`, so I thought I'd use a `-1000` instead of `None` in the tests for `NumpyArray`.\r\n\r\nI think, the `fillna` operation could take a `from` and a `to` arguments.",
  "created_at":"2020-03-11T19:16:03Z",
  "id":597819143,
  "issue":155,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzgxOTE0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-11T19:16:03Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, I was just thinking about getting to work on #77, the `ak.zip` method, which might be what you're talking about here. That one would be a good first issue.\r\n\r\nThe signature\r\n\r\n```python\r\nak.Array({'a': [1,2,3], 'b': [4,5,6,7,8]})\r\n```\r\n\r\nshould be forbidden, since the data between parentheses is actually a Record. Let's call that a bug-fix: it shouldn't turn a dict into an array of its keys. The error message should probably point out that if you want a record, you can build a record:\r\n\r\n```python\r\nak.Record({'a': [1,2,3], 'b': [4,5,6,7,8]})\r\n```\r\n\r\nbut that constructor doesn't invoke `fromiter` yet. That's explicitly a FIXME:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e60faf1bf1815b44124a71f444c969194118481d/src/awkward1/highlevel.py#L173-L186\r\n",
  "created_at":"2020-03-10T22:15:34Z",
  "id":597347281,
  "issue":156,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM0NzI4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T22:28:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm marking this as a duplicate because it's very similar to what the `ak.zip` function (#77) should provide. It should be a different function from the `ak.Array` constructor because it's doing a very different thing with the data it is given.",
  "created_at":"2020-03-10T22:34:23Z",
  "id":597353525,
  "issue":156,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM1MzUyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T22:34:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes I see, thanks for the feedback!",
  "created_at":"2020-03-10T23:06:34Z",
  "id":597362942,
  "issue":156,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM2Mjk0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T23:06:34Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to open a PR to handle these constructor issues. I'm not sure how much it will try to address, but there will be a place to look.\r\n\r\n   * The `ak.Array` and `ak.Record` constructor bugs (iterating over the dict keys and implementing that FIXME)\r\n   * Make `fromiter` iterate over `py::array` (mentioned in #146)\r\n   * Possibly, but not necessarily, `ak.zip`.\r\n",
  "created_at":"2020-03-10T23:17:58Z",
  "id":597366071,
  "issue":156,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM2NjA3MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-03-10T23:17:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I really should have linked these issues to the PR. The signature you were asking for is available in `ak.zip`, but it's not an `ak.Array` signature.\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> array = ak.zip({\"x\": ak.Array([[1, 2], [], [3]]), \"y\": ak.Array([1.1, 2.2, 3.3])})\r\n\r\n>>> print(array)\r\n[{x: [1, 2], y: 1.1}, {x: [], y: 2.2}, {x: [3], y: 3.3}]\r\n\r\n>>> ak.typeof(array)\r\n3 * {\"x\": var * int64, \"y\": float64}\r\n```\r\n\r\nSince Python lists are \"array-like,\" they can be used for small examples:\r\n\r\n```python\r\n>>> array = ak.zip({\"x\": [[1, 2], [], [3]], \"y\": [1.1, 2.2, 3.3]})\r\n\r\n>>> print(array)\r\n[{x: [1, 2], y: 1.1}, {x: [], y: 2.2}, {x: [3], y: 3.3}]\r\n\r\n>>> ak.typeof(array)\r\n3 * {\"x\": var * int64, \"y\": float64}\r\n```\r\n\r\nIt would be confusing to allow this syntax in the `ak.Array` constructor because that constructor argument is interpreted as the inverse of `ak.list`:\r\n\r\n```python\r\n>>> not_array = ak.Record({\"x\": [[1, 2], [], [3]], \"y\": [1.1, 2.2, 3.3]})\r\n>>> print(not_array)\r\n{x: [[1, 2], [], [3]], y: [1.1, 2.2, 3.3]}\r\n>>> ak.typeof(not_array)\r\n{\"x\": var * var * int64, \"y\": var * float64}\r\n\r\n>>> ak.Array({\"x\": [[1, 2], [], [3]], \"y\": [1.1, 2.2, 3.3]})\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward1/highlevel.py\", line 34, in __init__\r\n    raise TypeError(\"could not convert dict into an awkward1.Array; try awkward1.Record\")\r\nTypeError: could not convert dict into an awkward1.Array; try awkward1.Record\r\n```",
  "created_at":"2020-03-11T23:50:35Z",
  "id":597936084,
  "issue":156,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzkzNjA4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-11T23:50:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@tamasgal\r\n\r\n```python\r\nak.Record({\"x\": 1, \"y\": 2.2})\r\n```\r\n\r\nnow produces a record and\r\n\r\n```python\r\nak.Array({\"x\": 1, \"y\": 2.2})\r\n```\r\n\r\nraises an error. `ak.Array` and `ak.Record` are mutually exclusive and each error message points you to the other class.",
  "created_at":"2020-03-10T23:39:07Z",
  "id":597372179,
  "issue":157,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM3MjE3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-10T23:39:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"```python\r\narray = awkward1.Array([numpy.array([1, 2, 3]), numpy.array([4, 5, 6, 7])])\r\nassert str(awkward1.typeof(array)) == \"2 * var * int64\"\r\nassert awkward1.tolist(awkward1.tolist(array)) == [[1, 2, 3], [4, 5, 6, 7]]\r\n```\r\n\r\nThe implementation is cheap: when we encounter a `py::array` in `fromiter`, we call `tolist()` on it and deal with it as an iterable. The numbers it contains have `numpy.integer`, `numpy.floating`, and `numpy.bool_` type (which aren't the same as Python `int`, `float`, and `bool`, which is an annoyance when converting to JSON as well!), so these are now recognized and converted as well.\r\n\r\nA better implementation might avoid converting NumPy arrays into Python lists and iterate over it in C++, but we're just establishing behavior here; optimizing later.\r\n\r\nAt least I sorted the type checks to put the most common first. The NumPy type checks have to `import numpy`, so they're last.",
  "created_at":"2020-03-11T00:11:23Z",
  "id":597380395,
  "issue":157,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM4MDM5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-11T00:11:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Can't sort the type checks arbitrarily, or else `True`/`False` might be recognized as `int` and `bytes` as `str`.",
  "created_at":"2020-03-11T00:27:22Z",
  "id":597384372,
  "issue":157,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM4NDM3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-11T00:27:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"That was quick, as always! Thanks I'll test tomorrow \ud83d\ude03 ",
  "created_at":"2020-03-11T01:31:04Z",
  "id":597398439,
  "issue":157,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzM5ODQzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-11T01:31:04Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"That last commit also removes a \"skip\" from the all-important \"Numba cpointers\" test (#118) and revealed a few other places where `awkward1._numba` had to be changed to `awkward1._connect._numba`. Thus, I have to merge this PR to make master fully work with Numba. I want to do that as soon as these tests pass; `ak.zip` can go in later.",
  "created_at":"2020-03-11T13:10:55Z",
  "id":597623019,
  "issue":157,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzYyMzAxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-11T13:12:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@henryiii's class as a JupyterBook: https://henryiii.github.io/compclass-book/week1/0_IntroductionAndLogin",
  "created_at":"2020-03-12T15:33:43Z",
  "id":598254362,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODI1NDM2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T15:58:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"See my comment here: <https://github.com/scikit-hep/scikit-hep-tutorials/issues/1>. \r\n\r\nFor a single package, the nbsphinx extensions works well for examples (See [boost-histogram's examples](https://boost-histogram.readthedocs.io/en/latest/notebooks/xarray.html)). For cross-package tutorials, I think JupyterBook would be the best option.",
  "created_at":"2020-03-12T15:58:06Z",
  "id":598267495,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODI2NzQ5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T15:58:06Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Note: GitBook is actually the name of an abandoned Open Source software package that I used to make Modern CMake, the CLI11 tutorials, and a few more, and has been replaced by a service. We've been having to migrate away from GitBook; for example the LHCb starter kit was converted to a different technology a couple of months ago.",
  "created_at":"2020-03-12T16:00:15Z",
  "id":598268610,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODI2ODYxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T16:00:46Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't want to get into the details in the meeting, but I think cross-package tutorials _and_ JupyterBook-based Awkward-only tutorials are a good idea. For the time being, I'm trying to figure out a strategy for the Awkward-only tutorials. I'll be doing a big documentation push pretty soon, and I want to go in the right direction.\r\n\r\nDoxygen will be involved, definitely for C++ and probably for Python. (That doxypypy looks pretty good. I don't want to have to remember to add autodoc stubs in reST files; I've forgotten it too many times in Uproot docs.)\r\n\r\nThe main thing will be nugget-sized \"how tos\" and maybe one \"getting started\" tutorial. I'd kinda like the how-tos, tutorials, and reference docs (Doxygen output) to be together\u2014maybe in the same format, same site, and/or same toolchain.\r\n\r\nI'll take a look at JupyterBooks.",
  "created_at":"2020-03-12T16:04:49Z",
  "id":598270995,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODI3MDk5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T16:04:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, [JupyterBooks isn't popular](https://trends.google.com/trends/explore?date=all&q=Wikibook,GitBook,JupyterBook). As long as it isn't in danger of going away the way StackOverflow Documentation did, that's the important point.\r\n\r\nBut supposing the community loses interest in JupyterBook, then we'd be left with documentation in a bunch of Jupyter notebooks, which isn't a major problem. There will be other ways to automatically convert it as documentation. (Contrast this to all the Google Code wiki markup I wrote to document old projects. It's [kinda readable as Markdown](https://github.com/jpivarski/svgfig/wiki)...)\r\n\r\nI know for a fact, though, that nobody reads the Binder tutorial on Uproot. The fact that nobody complained when it was broken for months is a strong indication. Binder isn't the solution: it takes too long to load.",
  "created_at":"2020-03-12T16:51:16Z",
  "id":598295455,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODI5NTQ1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T16:51:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thebelab is nice. It takes 20 seconds to start a kernel for interactivity, which is longer than casual readers will wait, but it doesn't take you away from the page the way a Binder link would.\r\n\r\nMaybe the following would work:\r\n\r\n   * Host all documentation in the same repo as Awkward itself, i.e. this scikit-hep/awkward-1.0 site (whose name will change to scikit-hep/awkward-array). [This GitHub blog](https://github.blog/2016-08-22-publish-your-project-documentation-with-github-pages/) says that I can do it with a docs folder, which I have. The awkward-array.org domain is open, but maybe https://scikit-hep.org/awkward-1.0 would just work (once some content is there).\r\n   * The C++ and Python reference documentation (API) from Doxygen, sourced by Doxygen-style comments (C++) and docstrings (Python and doxypypy). Doxygen generates static HTML that I'd host as-is. On the plus side, it would _look_ like standard Doxygen, and that helps people who are familiar with that. (Reference documentation is for programmers.)\r\n   * Use JupyterBook to generate a Jekyll site on github.io. Everything would be vanilla: no customizations, though Thebelab should be turned on. Jekyll must have an option for including static HTML (from Doxygen) in the same site that is generated from JupyterBook.\r\n   * I would prefer writing the text in Jupytext with \"percent\" format, instead of in-browser notebooks. Then I can use Emacs and Atom/Hydrogen for writing (which I highly prefer). I'd generate the output with `jupyter-book build --execute`.\r\n   * It would be nice to include automated testing\u2014the Uproot docs have gotten out of date, from time to time. Since the source files for all of these documents are Jupyter notebooks, there must be an automated way to run them and at least verify that they raise no exceptions. (And I suppose I could put assertions in hidden cells.) Note: in Jupytext's \"percent\" format (*.pct.py), testing for exceptions is just a matter of running the script. All the non-Python metadata is in comments.\r\n   * I'd build the HTML locally while writing, but in general, the Doxygen build and maybe Jupyter notebook check should be run automatically in an Azure pipeline when a PR is pushed to master. That would be a third pipeline, beside the build/test triggered by commits to all branches and the deploy triggered by a new tag.\r\n   * The content would be divided into \"Awkward Data Analysts,\" \"Awkward Framework Builders,\" and \"Awkward Developers.\" They should probably be separate books, with three links on the GitHub front-page. The \"Awkward Data Analysts\" book would be dominated by (exclusively?) example-driven how-tos, which don't fit a book format very well: it would be a long list in the left-column. The incremental search in JupyterBook is nice, if users notice it.",
  "created_at":"2020-03-12T17:39:11Z",
  "id":598324908,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODMyNDkwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T18:37:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"JupyterBook is fairly new. And it is basically vanilla, except for optional cell customizations, such as the ability to hide a cell (like a setup cell) in the HTML output; I don't think you'd need those and they wouldn't have a strong effect anyway, and they are just normal cell metadata.\r\n\r\nThe good thing about JupyterBook, besides the fact you can use the Jupyter Notebooks directly, even directly view them on Github, is that it is not the only Notebook to static site system. nbsphinx for Sphinx does the same thing, so if JupyterBook goes under, we could switch to Sphinx fairly quickly. Some other static site generators are gaining support for notebooks too. It's just currently the nicest and is now an official part of the Jupyter organization. (I think this happened recently!)\r\n\r\nI think the outline you've described above is very much what I was thinking, though I'd use GitHub Actions, as it's a little simpler and you don't need the release mechanism. I also would avoid checking in the generated html into the source (which I currently am doing for the demo site, but I think I can work around that) - it would be force pushed to a gh-pages branch or similar. I'll be playing with this soon(ish) for a tutorial setup and will update you when I have it working.",
  "created_at":"2020-03-12T18:02:27Z",
  "id":598339022,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODMzOTAyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T18:04:23Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'd do it in GitHub Actions if I could move all CI/CD to GitHub Actions. I don't know that I need to do that migration _first_, though.\r\n\r\nReading about this has introduced me to Jupytext, which is great! The plain files that Jupytext uses are more version control-friendly and I wouldn't have to launch a JupyterLab every time I want to edit something (which is a bottleneck for me).\r\n\r\nnbsphinx (which I hadn't known about before, either) starts from ipynb notebooks, but a Jupytext \u2192 nbsphinx \u2192 readthedocs can be a fallback plan if JupyterBook disappears.\r\n\r\nI don't want to save HTML output in GitHub either. At least not the main scikit-hep/awkward-1.0 repository. Maybe the documentation build process can send whatever Jekyll/GitHub Pages needs to a repo that's not meant to be edited directly.\r\n\r\nThe [JupyterBook documentation suggests Netlify](https://jupyterbook.org/guide/publish/netlify.html), which could be a good repository for static HTML, if that's what the documentation build process produces. I used to use AWS buckets for that: static HTML hosting is free. But as much as possible, I'd like this to be unconnected from personal accounts (such as my personal Azure account) and on shared accounts (like the scikit-hep organization). Maybe [Zeit](https://zeit.co/). They look friendly.",
  "created_at":"2020-03-12T18:17:05Z",
  "id":598345502,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODM0NTUwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T18:28:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've been using Azure for wheels and GHA for docs and tests, and that's been working well. They all run on pretty much the same Microsoft backends and are the configs are mostly the same except for a 1:1 term mapping. I don't think there's harm in selecting the one that best for a particular job. It's only been in the last year or so that we could run on a single system; it used to be a different system per supported OS at least. Note there is no setup at all for GHA, since it's built in and always available. You already have an actions tab across the top of all your repositories. It just runs based on the presence of files `.github/workflows/*.yml`.\r\n\r\nJupytext sounded very promising, yes, I just heard about it recently. Launching JupyterLab is a pain.\r\n\r\nIRIS-HEP has a dual repo setup for hosting, that does work.\r\n\r\nI haven't looked into Netlify.",
  "created_at":"2020-03-12T18:44:28Z",
  "id":598357235,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODM1NzIzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T18:45:56Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Note for related projects (vector, boost-histogram, etc): This strategy is probably a bit different, since Awkward is using Doxygen instead of Sphinx. If Sphinx is already being used, then nbsphinx is probably the correct choice.",
  "created_at":"2020-03-12T18:48:14Z",
  "id":598358812,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODM1ODgxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-12T18:48:14Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"From @kratsg: look into \"[breathe](https://pypi.org/project/breathe/)\" and \"[exhale](https://pypi.org/project/exhale/)\". It turns Doxygen into Sphinx.",
  "created_at":"2020-03-18T13:24:19Z",
  "id":600620366,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDYyMDM2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-18T13:24:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"also see @cranmer's jupyterbook\r\n\r\nhttps://cranmer.github.io/madminer-tutorial/intro",
  "created_at":"2020-03-18T15:11:03Z",
  "id":600678949,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDY3ODk0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-18T15:11:03Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"MEMBER",
  "body":"Also mine: https://henryiii.github.io/compclass I've built GHA to handle running the notebooks and producing the JupyterBook, and no output is saved in the repo. Repo here: https://github.com/henryiii/compclass",
  "created_at":"2020-03-18T15:32:35Z",
  "id":600691256,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDY5MTI1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-18T15:33:27Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"This is pretty much figured out:\r\n\r\n   * Doxygen for C++\r\n   * Sphinx for Python\r\n   * JupyterBooks for tutorials\r\n\r\nOf these, the first two are done.",
  "created_at":"2020-04-02T02:01:50Z",
  "id":607578303,
  "issue":158,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNzU3ODMwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T02:01:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In this world, `argcross` is defined in terms of `cross`! Everything here is done; when the tests pass, we merge and deploy.",
  "created_at":"2020-03-11T22:43:07Z",
  "id":597916317,
  "issue":159,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5NzkxNjMxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-11T22:43:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and the reason why I'm writing this: #70 (argmin and argmax) has this issue as a prerequesite. This issue has to be resolved before work can start on that.",
  "created_at":"2020-03-13T02:37:02Z",
  "id":598519633,
  "issue":161,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5ODUxOTYzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-13T02:37:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's a bug\u2014a future bug, not a present one, but still.",
  "created_at":"2020-03-14T15:03:37Z",
  "id":599075853,
  "issue":162,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5OTA3NTg1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-14T15:03:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The documentation says this is an existing feature, but it is not. It should be implemented soon.",
  "created_at":"2020-04-08T12:45:34Z",
  "id":610937692,
  "issue":163,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMDkzNzY5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-08T12:45:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The new commit, creates two methods in the `cpu-kernels` to `get` and `set` contents. However, in doing this I was forced to break the `extern \"C\"` interface which is against the current design.",
  "created_at":"2020-03-16T10:17:39Z",
  "id":599453884,
  "issue":164,
  "node_id":"MDEyOklzc3VlQ29tbWVudDU5OTQ1Mzg4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-16T10:17:39Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Strangely, I see your comment about merge conflicts in my email, but not here. (I don't fully understand how GitHub comments and email mix.)\r\n\r\nAnyway, yes, there have been two PRs since you started this PR, and you'll have to merge them in. These changes include one or two places where buffers were directly accessed because they predated this PR. You should be able to see them in the diffs.\r\n\r\nGoing forward, I'm going to be using the `IndexOf<T>::getitem` and `setitem` functions, but I can't use any new functions that this PR introduces until this PR gets merged. (I'm pretty sure the `IndexOf<T>::getitem` and `setitem` predate this PR, though...)",
  "created_at":"2020-03-17T13:00:48Z",
  "id":600057473,
  "issue":164,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDA1NzQ3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T13:00:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I am sorry,  that was a dumb mistake on my part.\r\n\r\nI think from now on you can easily use `Index<T>::getitem` and `setitem` functions. Every other new function that I introduced just aids the separation between CPU and GPU, so you should be good to use those two functions even without my PR.",
  "created_at":"2020-03-17T13:06:26Z",
  "id":600060256,
  "issue":164,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDA2MDI1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T13:06:26Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I am done with the PR. I don't know how to trigger the `CI`. It would be helpful if I could check whether all builds are passing. I have checked it locally on my Linux system and the respective C++ tests pass.",
  "created_at":"2020-03-17T14:33:22Z",
  "id":600104343,
  "issue":164,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDEwNDM0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T14:34:11Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"I hadn't noticed that\u2014the CI isn't triggering. Maybe because your repository is a fork: I went into Azure to try to invoke it manually, and I could only choose from branches and commits on the main repo, not forks. If so, that's quite a deficiency of Azure. At the very least, I should be able to _manually_ run it. I'm looking into it. (I do want to run a suite of tests before merging, even though I'm pretty sure they'll all pass.)",
  "created_at":"2020-03-17T14:44:52Z",
  "id":600110217,
  "issue":164,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDExMDIxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T14:44:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Wait a minute: now the CI builds are visible here? Maybe I just needed to create a main repo branch; creating the duplicate PR seems to have been superfluous.",
  "created_at":"2020-03-17T15:08:03Z",
  "id":600122829,
  "issue":164,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDEyMjgyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T15:08:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yup, every test has passed. I guess you can merge it into master if everything's okay.",
  "created_at":"2020-03-17T15:23:32Z",
  "id":600131114,
  "issue":164,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDEzMTExNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T15:23:32Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Great to see that this has started! I don't think it's necessary, however, to implement all three sorting algorithms unless it's something that comes for free from `#include <algorithm>` or something. We can leave the `axis=0` sorting unimplemented in C++, since it's qualitatively different from the rest and can be implemented at the Python level by deferring to NumPy.\r\n\r\nThe interesting thing is `sort` and `argsort` of deeply nested lists, `axis=1` and greater. So far, no one has asked for sorting structures, such as\r\n\r\n```python\r\n>>> [3, 5, 2] < [4, 1]\r\nTrue\r\n```\r\n\r\nso it would be fine to raise an error if the requested `axis` is not a 1-D NumpyArray, a RawArray, or an IndexedArray that can be evaluated (`carry(index_)`) to a NumpyArray or RawArray. EmptyArrays can also be sorted (trivially). IndexedOptionArrays count as a structure, unless we want to adopt a convention of `None` always being last (or first, but I think last would be better, if handled at all).\r\n\r\nNote that Python 3 (unlike Python 2) doesn't like to sort objects of different types, including `None`:\r\n\r\n```python\r\n>>> sorted([5, 3, 4])\r\n[3, 4, 5]\r\n>>> sorted([5, 3, 4, None])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: '<' not supported between instances of 'NoneType' and 'int'\r\n>>> sorted([5, 3, 4, []])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: '<' not supported between instances of 'list' and 'int'\r\n```\r\n\r\nThat's another argument for not handling these cases.\r\n\r\nNumpyArrays, RawArrays, EmptyArrays, and evaluated IndexedArrays have something in common: they're all the deepest possible axis, `axis=-1`, which should be the default, once negative axis is handled. (Note to self: I need to implement that right away, so that this PR can be completed!) `axis=-1` is also the default `axis` for `numpy.sort` and `numpy.argsort`, which is great, because then we can make `awkward1.sort`/`awkward1.argsort` a drop-in replacement (in the [NEP18](https://numpy.org/neps/nep-0018-array-function-protocol.html) sense).\r\n\r\nNumPy has a definition of sorting with `axis != -1` that is _not_ sorting structures:\r\n\r\n```python\r\n>>> numpy.sort([[3.3, 2.2, 4.4], [1.1, 5.5, 3.3]], axis=-1)\r\narray([[2.2, 3.3, 4.4],\r\n       [1.1, 3.3, 5.5]])\r\n>>> numpy.sort([[3.3, 2.2, 4.4], [1.1, 5.5, 3.3]], axis=-2)\r\narray([[1.1, 2.2, 3.3],\r\n       [3.3, 5.5, 4.4]])\r\n```\r\n\r\nIt's similar to the way that reducers with `axis != -1` is handled:\r\n\r\n```\r\n>>> numpy.min([[3.3, 2.2, 4.4], [1.1, 5.5, 3.3]], axis=-1)\r\narray([2.2, 1.1])\r\n>>> numpy.min([[3.3, 2.2, 4.4], [1.1, 5.5, 3.3]], axis=-2)\r\narray([1.1, 2.2, 3.3])\r\n```\r\n\r\nHaving gone through that exercise:\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> ak.min([[3.3, 2.2, 4.4], [1.1, 5.5]], axis=-1)\r\n<Array [2.2, 1.1] type='2 * ?float64'>\r\n>>> ak.min([[3.3, 2.2, 4.4], [1.1, 5.5]], axis=-2)\r\n<Array [1.1, 2.2, 4.4] type='3 * ?float64'>\r\n```\r\n\r\nI can say that it's really complicated: you have to pass down and update a `nextcarry`, `nextparents`, and `nextstarts` at every list level.\r\n\r\n**So don't implement sorting for axis != -1 right now. But do set up the infrastructure so that it would be possible in the future.**\r\n\r\nThat is, follow the structure of `reduce_next`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/bf44d0244d6eec460c00837afea09a42761f1d45/src/libawkward/array/ListOffsetArray.cpp#L984-L1104\r\n\r\nbut don't fill in all the cases. This kind of operation, for which `axis=-1` is the natural default (like reducers and unlike most others, such as `rpad` or `fillna`) does not use `axis_wrap_if_negative` (so I'm off the hook for implementing that). Instead, it has a common entry function, like `Content::reduce`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/bf44d0244d6eec460c00837afea09a42761f1d45/src/libawkward/Content.cpp#L79-L113\r\n\r\nthat passes down `negaxis` instead of `axis`. (For these types of operations, it's more natural to think in terms of the negative axis, so when the user supplies `-1`, we call that `1`.) The `parents` is an array of integers with the same length as the current level that indicates which outer list each element belongs to. For instance,\r\n\r\n```python\r\n[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]\r\n```\r\n\r\nmeans that the 15 elements at this level are nested inside of\r\n\r\n```python\r\n[[. . . . .] [. . . . .] [. . . . .]]\r\n```\r\n\r\nSo `parents` is like bottom-up pointers containing the same information as the top-down `starts` pointers. For the same array, `starts` is\r\n\r\n```python\r\n[0, 5, 10]\r\n```\r\n\r\nFor most reducers, I only needed `parents`, but last night I added `argmin` and `argmax`, which needed `starts` so that the `arg` integers could be relative to the start of the current list, rather than being absolute numbers. (Since you're doing `argsort`, you'll need it, too.) The two of these have all the information that would be needed for handling any `axis`; the `parents` are only monotonically increasing for `axis=-1`.\r\n\r\nJust to reiterate, you do not need to implement `axis != -1`, but create and use the `parents` and `starts` similar to the `reduce` and `reduce_next` methods, even though they're overkill for this case. There are huge blocks of `reduce_next` that do not need a parallel yet\u2014for example,\r\n\r\n```c++\r\nif (!branchdepth.first  &&  negaxis == branchdepth.second)\r\n```\r\n\r\nis only for `axis != -1`, and this is more than half of the function. The `else` clause,\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/bf44d0244d6eec460c00837afea09a42761f1d45/src/libawkward/array/ListOffsetArray.cpp#L1071-L1103\r\n\r\ndoesn't have a `nextcarry` because the relevant contents are contiguous (it has a `getitem_range_nowrap` instead), the `nextparents` are relatively simple, and the `nextstarts` are simply `util::make_starts(offsets_)`.\r\n\r\nYou can have ListArray and RegularArray just call `toListOffsetArray64()` and handle all the list stuff in `ListOffsetArrayOf<T>::sort_next`. Don't handle option types yet (IndexedOptionArray, ByteMaskedArray, BitMaskedArray) because I'll be correcting the definition by solving issue #166; it's currently not correct. UnionArrays and RecordArrays are relatively simple; see the `reduce_next` examples.",
  "created_at":"2020-03-17T15:58:05Z",
  "id":600150510,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDE1MDUxMA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-03-17T15:58:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"All the commits I just made were for setting up documentation sites. That's been settled: Doxygen will go to awkward-array.readthedocs.org (under [static_](https://awkward-array.readthedocs.io/en/latest/_static/), which can't be changed for technical reasons related to Sphinx) and the tutorials go to GitHub Pages ([here](https://scikit-hep.org/awkward-1.0/index.html)). Merging that shouldn't affect your work.\r\n\r\nHowever, I'm about to make a big change, source-file-wise, by enforcing an 80-character limit everywhere. I just checked over the changes here and it looks like you only have stubs: that should still be easy to merge. But if you have more substantial changes in your local git repo, then it could be difficult. I'll start working on that 80-character limit in another PR and not merge it until I hear back from you about whether that would present any difficulties.\r\n\r\nSo let me know; thanks!",
  "created_at":"2020-03-19T20:38:47Z",
  "id":601405364,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTQwNTM2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-19T20:38:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> All the commits I just made were for setting up documentation sites. That's been settled: Doxygen will go to awkward-array.readthedocs.org (under [static_](https://awkward-array.readthedocs.io/en/latest/_static/), which can't be changed for technical reasons related to Sphinx) and the tutorials go to GitHub Pages ([here](https://scikit-hep.org/awkward-1.0/index.html)). Merging that shouldn't affect your work.\r\n> \r\n> However, I'm about to make a big change, source-file-wise, by enforcing an 80-character limit everywhere. I just checked over the changes here and it looks like you only have stubs: that should still be easy to merge. But if you have more substantial changes in your local git repo, then it could be difficult. I'll start working on that 80-character limit in another PR and not merge it until I hear back from you about whether that would present any difficulties.\r\n> \r\n> So let me know; thanks!\r\n\r\nThanks for letting me know! No worries, I'm ok with merging conflicts. This PR will be easy to merge - all these are new member functions. Git merge usually messes a bit `cpu-kernels`. To avoid this I'll place the new functions at the end when I merge master to my branch.",
  "created_at":"2020-03-20T06:59:41Z",
  "id":601560666,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTU2MDY2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-20T06:59:41Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Let's use this \"convert to draft\" GitHub feature instead of the \"`[WIP]`\" naming convention. The WIP-bot counts as an unfinished test, so all the passing commits are yellow dots, not clearly indicating which ones actually passed all the real tests\u2014I've never really liked it, for that reason, and now \"convert to draft\" looks like an acceptable alternative. The draft feature also provides more protection against accidentally clicking the \"merge\" button.\r\n\r\nThat will be in the CONTRIBUTING.md that's still in my head.\r\n\r\nI've seen your commits. Do you feel like you're making progress, or is there some issue you want to talk about? If so, we can arrange a time.",
  "created_at":"2020-04-16T12:46:14Z",
  "id":614630314,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNDYzMDMxNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-16T12:46:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Let's use this \"convert to draft\" GitHub feature instead of the \"`[WIP]`\" naming convention. The WIP-bot counts as an unfinished test, so all the passing commits are yellow dots, not clearly indicating which ones actually passed all the real tests\u2014I've never really liked it, for that reason, and now \"convert to draft\" looks like an acceptable alternative. The draft feature also provides more protection against accidentally clicking the \"merge\" button.\r\n> \r\n> That will be in the CONTRIBUTING.md that's still in my head.\r\n> \r\n> I've seen your commits. Do you feel like you're making progress, or is there some issue you want to talk about? If so, we can arrange a time.\r\n\r\nHi Jim,\r\nI think, I'm making progress. I'm debugging the sorting ranges definition based on relative indices for multi dimensional arrays. It is fun :-)\r\nI did not know that making an existing PR to a draft PR feature has been implemented. Let's use it! ",
  "created_at":"2020-04-16T13:15:33Z",
  "id":614645570,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNDY0NTU3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-16T13:15:33Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I don't have Windows accessible to test the build - is it expected that it fails?\r\n\r\n```\r\n    Generating Code...\r\n  Not able to access compiler path (on Windows), using CMake default\r\n  Traceback (most recent call last):\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py\", line 257, in <module>\r\n      main()\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py\", line 240, in main\r\n      json_out['return_val'] = hook(**hook_input['kwargs'])\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py\", line 181, in build_wheel\r\n      return _build_backend().build_wheel(wheel_directory, config_settings,\r\n    File \"C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-build-env-u5s2z4k6\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 212, in build_wheel\r\n      return self._build_with_temp_dir(['bdist_wheel'], '.whl',\r\n    File \"C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-build-env-u5s2z4k6\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 198, in _build_with_temp_dir\r\n      self.run_setup()\r\n    File \"C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-build-env-u5s2z4k6\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 143, in run_setup\r\n      exec(compile(code, __file__, 'exec'), locals())\r\n    File \"setup.py\", line 137, in <module>\r\n      setup(name = \"awkward1\",\r\n    File \"C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-build-env-u5s2z4k6\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 144, in setup\r\n      return distutils.core.setup(**attrs)\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\distutils\\core.py\", line 148, in setup\r\n      dist.run_commands()\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\distutils\\dist.py\", line 966, in run_commands\r\n      self.run_command(cmd)\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\distutils\\dist.py\", line 985, in run_command\r\n      cmd_obj.run()\r\n    File \"C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-build-env-u5s2z4k6\\overlay\\Lib\\site-packages\\wheel\\bdist_wheel.py\", line 223, in run\r\n      self.run_command('build')\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\distutils\\cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\distutils\\dist.py\", line 985, in run_command\r\n      cmd_obj.run()\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\distutils\\command\\build.py\", line 135, in run\r\n      self.run_command(cmd_name)\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\distutils\\cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\distutils\\dist.py\", line 985, in run_command\r\n      cmd_obj.run()\r\n    File \"C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-build-env-u5s2z4k6\\overlay\\Lib\\site-packages\\setuptools\\command\\build_ext.py\", line 87, in run\r\n      _build_ext.run(self)\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\distutils\\command\\build_ext.py\", line 340, in run\r\n      self.build_extensions()\r\n    File \"setup.py\", line 42, in build_extensions\r\n      self.build_extension(x)\r\n    File \"setup.py\", line 84, in build_extension\r\n      subprocess.check_call([\"cmake\", \"--build\", build_dir] + build_args)\r\n    File \"C:\\hostedtoolcache\\windows\\Python\\3.8.2\\x64\\lib\\subprocess.py\", line 364, in check_call\r\n      raise CalledProcessError(retcode, cmd)\r\n  subprocess.CalledProcessError: Command '['cmake', '--build', 'build\\\\temp.win-amd64-3.8\\\\Release', '--config', 'Release', '--', '/m', '/m']' returned non-zero exit status 1.\r\n  Building wheel for awkward1 (PEP 517): finished with status 'error'\r\n  ERROR: Failed building wheel for awkward1\r\nFailed to build awkward1\r\nCleaning up...\r\n```",
  "created_at":"2020-04-20T15:32:06Z",
  "id":616630028,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjYzMDAyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T15:32:06Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Builds on Windows should work.\r\n\r\nI looked at the full log and found that the first error is this:\r\n\r\n```\r\n2020-04-20T14:13:47.6069252Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-854ndqim\\src\\libawkward\\array\\NumpyArray.cpp(2355): error C2146: syntax error: missing ')' before identifier 'or' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-854ndqim\\build\\temp.win-amd64-3.8\\Release\\awkward-objects.vcxproj]\r\n```\r\n\r\nI don't immediately see the error when I look at this line in your branch, but it happens in the middle of the argsort function:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/51b0be556633954bdd12c077aed1c03357f23703/src/libawkward/array/NumpyArray.cpp#L2314-L2481\r\n\r\nand there are `#ifdef _MSC_VER` switches in there that could, in principle, cause a syntax error on Windows that doesn't show up on other systems.",
  "created_at":"2020-04-20T16:35:15Z",
  "id":616669286,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjY2OTI4Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-20T16:35:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Builds on Windows should work.\r\n> \r\n> I looked at the full log and found that the first error is this:\r\n> \r\n> ```\r\n> 2020-04-20T14:13:47.6069252Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-854ndqim\\src\\libawkward\\array\\NumpyArray.cpp(2355): error C2146: syntax error: missing ')' before identifier 'or' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-854ndqim\\build\\temp.win-amd64-3.8\\Release\\awkward-objects.vcxproj]\r\n> ```\r\n> \r\n> I don't immediately see the error when I look at this line in your branch, but it happens in the middle of the argsort function:\r\n> \r\n> https://github.com/scikit-hep/awkward-1.0/blob/51b0be556633954bdd12c077aed1c03357f23703/src/libawkward/array/NumpyArray.cpp#L2314-L2481\r\n> \r\n> and there are `#ifdef _MSC_VER` switches in there that could, in principle, cause a syntax error on Windows that doesn't show up on other systems.\r\n\r\nI'll try to find a PC :-)",
  "created_at":"2020-04-20T16:38:04Z",
  "id":616671056,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjY3MTA1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T16:38:04Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Incidentally, these \"Windows ifdefs\" can also be tested with a 32-bit version of Linux. For faster debugging on systems with these alternative `format_` definitions, I use one of these Docker images (I can't remember which):\r\n\r\n```\r\n% docker images\r\nREPOSITORY                          TAG                 IMAGE ID            CREATED             SIZE\r\ni386/ubuntu                         latest              f015d121c3ab        3 months ago        64.8MB\r\nnickcis/fedora-32                   latest              5cf360091dd7        4 years ago         212MB\r\n32bit/debian                        latest              ec51524b3826        5 years ago         272MB\r\n```\r\n\r\nIt seems that I used `i386/ubuntu` most recently. Looking in my command history, I mounted a directory on my host computer (for persistence) while running the 32-bit Docker image and installed a 32-bit version of Miniconda through the Docker image into the persistent directory. It complained that my host is 64 bit, but that's because Docker images _do_ leak information about the host, and I could safely disregard this warning. I installed CMake, GCC, and NumPy using Conda to ensure that I got 32-bit versions of all of them, and git cloned awkward1 into the persistent volume.\r\n\r\nWith all that set up, the compiler goes into the `#ifdefs` that are triggered by Windows OR 32-bit, and had faster turn-around time on debugging them.\r\n\r\nFor anything that actually depends on Windows, as opposed to these 32-bit `format` definitions, I don't have a good workflow. I've never been able to run Windows in a virtual machine with compilers. When something truly Windows-related fails, Azure is the only way I have to debug it, too.",
  "created_at":"2020-04-20T16:45:47Z",
  "id":616675721,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjY3NTcyMQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-20T16:45:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I'll try to find a PC :-)\r\n\r\nFinding a PC with properly set-up compilers is the hard part. Microsoft makes Windows virtual machines available, but the purpose is to test web pages on their Edge web browser, not compiling C++ programs. I haven't found a virtual machine with Visual Studio and all that. If you're looking for laptops, particularly a laptop owned by somebody who actually does C++ work, you may have more success.",
  "created_at":"2020-04-20T16:49:03Z",
  "id":616677699,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjY3NzY5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T16:49:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > I'll try to find a PC :-)\r\n> \r\n> Finding a PC with properly set-up compilers is the hard part. Microsoft makes Windows virtual machines available, but the purpose is to test web pages on their Edge web browser, not compiling C++ programs. I haven't found a virtual machine with Visual Studio and all that. If you're looking for laptops, particularly a laptop owned by somebody who actually does C++ work, you may have more success.\r\n\r\nWell... it turns out the Windows compiler does not define alternative tokens for logical operators.\r\n\r\nBTW, thanks a lot for cleaning the MacOS warnings! I've updated to master and **all** the warnings are gone.\r\n",
  "created_at":"2020-04-21T14:44:53Z",
  "id":617225476,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzIyNTQ3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-21T14:44:53Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> BTW, thanks a lot for cleaning the MacOS warnings! I've updated to master and all the warnings are gone.\r\n\r\n(Thank @veprbl!)",
  "created_at":"2020-04-21T14:54:07Z",
  "id":617231166,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzIzMTE2Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-21T14:54:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, I'm nearly done with this PR. There are a few irregularities I'd need to fix (see the comments in the test), but I would like to discuss it beforehand. Would you have time tomorrow? Thanks",
  "created_at":"2020-04-22T15:55:50Z",
  "id":617866926,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzg2NjkyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-22T15:55:50Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - I think, I'm nearly done with this PR. There are a few irregularities I'd need to fix (see the comments in the test), but I would like to discuss it beforehand. Would you have time tomorrow? Thanks\r\n\r\nSure: 11am Chicago == 18:00 Geneva on Skype?",
  "created_at":"2020-04-22T16:03:13Z",
  "id":617871543,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzg3MTU0Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-22T16:03:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - please, ignore the `recurse` method for the time being. I need it to test the arrays shape consistency. Since the logic hasn't been reviewed yet, I decided to keep it for now. I'll remove it when the review comments are addressed. Thanks!",
  "created_at":"2020-05-11T16:18:17Z",
  "id":626804143,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNjgwNDE0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-11T16:18:17Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I intend to review this, but first I rebooted the Windows tests, as they all timed out at 1 hour. If it's not some transient problem with Azure (i.e. if it happens again), then it's likely a pointer bug that gets missed by the other two platforms. That's happened to me before, and by debugging it on Windows, I found a hidden pointer bug. I just wish we had a better way to run Windows than compiling the whole project every time in Azure.",
  "created_at":"2020-05-13T17:18:44Z",
  "id":628130634,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODEzMDYzNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-05-13T17:18:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - All tests pass! I've finished with the code cleanup. I'll wait for you to review it. Thanks!",
  "created_at":"2020-05-14T16:04:09Z",
  "id":628732236,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODczMjIzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T16:04:09Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Great! I'll review it, possibly tomorrow. Thanks!",
  "created_at":"2020-05-14T16:14:31Z",
  "id":628737970,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODczNzk3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T16:14:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - please, let me know if I need to change the `sorting.cpp`. Thanks.",
  "created_at":"2020-06-15T07:32:19Z",
  "id":643956146,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0Mzk1NjE0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-15T07:32:19Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to check this over and make any small changes to finish it up. (For instance, a `git merge master` was needed to fix linker errors on Linux.) Thanks for taking it this far! It represents an enormous amount of work.",
  "created_at":"2020-06-15T12:22:52Z",
  "id":644099384,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDA5OTM4NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-06-15T12:22:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's started: https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=1643&view=results\r\n\r\nIf there isn't an easier way to do this, I'll have to rethink the process. @henryiii, did you know that Azure won't trigger (and can't be manually triggered) on fork PRs? I had to make this a branch on the main repo to get it to work.",
  "created_at":"2020-03-17T15:05:57Z",
  "id":600121693,
  "issue":169,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDEyMTY5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T15:05:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Close this duplicate PR. The CI is working on the original PR, and that's the one I'd like to merge, given the choice.",
  "created_at":"2020-03-17T15:08:35Z",
  "id":600123166,
  "issue":169,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDEyMzE2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T15:08:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think this just changed a few days ago, and now you have to add an explicit request for a PR build. I just had someone fix this in CLI11.",
  "created_at":"2020-03-17T15:09:55Z",
  "id":600123945,
  "issue":169,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDEyMzk0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T15:10:09Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, looks like you explicitly mention PRs in the test file. It should be fine, you might be seeing hiccups in Azure starting.",
  "created_at":"2020-03-17T15:11:57Z",
  "id":600125039,
  "issue":169,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDEyNTAzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-17T15:11:57Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a \"good first issue\" because it's a fairly easy exercise in writing tests, but we would need some `ak.from_parquet`, first. Such a function doesn't exist. Maybe that should be a \"new feature\" issue. It might be covered under #68.",
  "created_at":"2020-04-17T23:43:09Z",
  "id":615511833,
  "issue":172,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTUxMTgzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T23:43:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> There's no `astype`, and a general one might want to change such things as non-optiontype arrays into UnmaskedArrays or filter/rearrange the order of record fields. Before we need something like that, though, it would be useful to simply change the leaf types (like what NumPy's `astype` does). Should it be named `ak.withleaftype(array, np.int32)`?\r\n\r\nor `ak.leaf_astype(array, np.int32)`?",
  "created_at":"2020-03-18T09:14:05Z",
  "id":600507575,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDUwNzU3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-18T09:14:05Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Maybe. The data analyst-level Python names tend to avoid underscores because they slow down typing. Also, whereas you or I think of these nodes as leaves of a tree, that's not the first metaphor that might come to mind during data analysis. A data analysis is more likely to be thinking of these as \"the numbers,\" though some of them are boolean or text characters.",
  "created_at":"2020-03-18T12:44:58Z",
  "id":600601968,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDYwMTk2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-18T12:44:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"#346 was merged.",
  "created_at":"2020-08-06T17:09:55Z",
  "id":670059137,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MDA1OTEzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-06T17:09:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Old references to this same issue: #85 (first consideration) and #95 (was going to include UnionArray, but I saw how hard that would be).",
  "created_at":"2020-12-22T01:25:21Z",
  "id":749285768,
  "issue":174,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0OTI4NTc2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-22T01:25:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Ultimately, the reason we want a RedirectArray is to allow nodes in a tree to point to \"ancestor\" or \"cousin\" nodes in the same tree. Nodes that are strictly \"descendants\" or thought of as being in another tree are already covered by normal `content` inclusion.\r\n\r\n## Option 1: a new RedirectArray node type\r\n\r\nThis was my original thought, but it's going to require rethinking of some basic things, like how to build Forms or Types of trees with redirects. There will need to be placeholder objects and breadcrumbs in the recursive functions to avoid infinite loops. (All of those \"`seen\"` sets and dicts in Awkward0 recursive functions should probably be called \"breadcrumbs.\")\r\n\r\nAlso, RedirectArray would be the one and only mutable node type. One mutable exception in a world of immutability could have unforeseen consequences.\r\n\r\n## Option 2: every node has a weak_ptr to parent, RedirectArray is a path with `\"..\"` in it\r\n\r\nGiving every node a `std::weak_ptr` to its parent would be less of a can of worms than the above since `content` or `contents` can only be assigned in a node's constructor (because they're immutable), this assignment can be a variation on `shallow_copy` with the exception that it would replace the `parent` of the node that it is given with a new `parent`.\r\n\r\nWith access to node parents, RedirectArray doesn't have to be a pointer, it can be a path, like a filename path that includes `\"..\"` or some other kind of `up` token. This path would need `up` tokens, strings (to follow RecordArrays down) and integers (to follow UnionArrays down). The `up` tokens don't have to be a special type: they are only useful at the beginning of a path so they could be replaced by an integer. A path would then be a number of steps up followed by mixed strings and integers to step down.\r\n\r\nThis might be the only solution here that can work in Numba: the `ArrayView` expands a tree out as an array of integers, which becomes an infinite array if the type is cyclic. (It doesn't matter whether the data are cyclic, although electrons with a \"closest photon\" and photons with a \"closest electron\" is a \"cousin\" relationship that can easily be cyclic.)\r\n\r\nBeing a path also makes it explicit that RedirectArrays are only for links within a tree. If we're ever linking \"outside\" of a tree, that's equivalent to just adding a new `content`.\r\n\r\n## Option 3: we can already do it with VirtualArrays\r\n\r\nNodes are immutable and we want to insert node A as the `content` of node B, but node A might not exist yet when we're building node B. [Scala's solution to that is lazy val](https://blog.codecentric.de/en/2016/02/lazy-vals-scala-look-hood/), which attaches an attribute to a class without evaluating it until it is accessed.\r\n\r\nWe have lazy semantics: VirtualArrays. So we can already make cyclic dependencies exactly the way that Scala does it.\r\n\r\n```python\r\nimport numpy as np\r\nimport awkward1 as ak\r\n\r\nvalue = ak.layout.NumpyArray(np.array([1.23, 3.21, 9.99, 3.14, 2.71, 5.55, 8.00, 9.00, 0.00]))\r\nleft_index = ak.layout.Index64(np.array([1, 3, 5, 7, -1, -1, -1, -1, -1]))\r\nright_index = ak.layout.Index64(np.array([2, 4, 6, -1, 8, -1, -1, -1, -1]))\r\n\r\nhold = None\r\ndef generate():\r\n    return hold\r\n\r\nvirtual = ak.layout.VirtualArray(ak.layout.ArrayGenerator(generate))\r\n\r\nleft = ak.layout.IndexedOptionArray64(left_index, virtual)\r\nright = ak.layout.IndexedOptionArray64(right_index, virtual)\r\n\r\nrecords = ak.layout.RecordArray({\"value\": value, \"left\": left, \"right\": right})\r\nhold = records\r\ntree = records[0]\r\n```\r\n\r\nNow `ak.to_list(tree)` is\r\n\r\n```python\r\n{'left': {'left': {'left': {'left': None, 'right': None, 'value': 9.0},\r\n                   'right': None,\r\n                   'value': 3.14},\r\n          'right': {'left': None,\r\n                    'right': {'left': None, 'right': None, 'value': 0.0},\r\n                    'value': 2.71},\r\n          'value': 3.21},\r\n 'right': {'left': {'left': None, 'right': None, 'value': 5.55},\r\n           'right': {'left': None, 'right': None, 'value': 8.0},\r\n           'value': 9.99},\r\n 'value': 1.23}\r\n```\r\n\r\nIf I had slipped up and made a cyclic value reference in this cyclic type reference, the graph would not be a tree and `ak.to_list` or `ak.to_json` would try to build an infinite structure and then crash. Perhaps that's what `ak.to_json` should do, but `ak.to_list` should create a cyclic reference among the Python objects. That would require adding breadcrumbs to the `tolist` implementation.\r\n\r\nA lot of methods would have to be protected, because a cyclic dependence could appear anywhere. That was the reason I wanted to create an explicit RedirectArray in Awkward1: the fact that Awkward0's cyclic references could be anywhere because they were just Python references meant that everything had to be protected. It also meant that two objects could be equal without this being apparent because of where the check starts. Suppose that one node A contains another node B, which contains A. I could have another B that contains A that contains B, which is an identical graph, but depending on where you break it to make the check finite, it could appear to be a different graph.\r\n\r\nIn particular, calling a function like `ak.type` on the `tree` above causes an infinite loop and eventually a segfault. Rather than being good news that we already have a solution for cyclic references, it's bad news because it's an unprotected way to get a segfault in Python.\r\n\r\nHowever, I'm going to ignore it because fixing it would be hard and it's something that only an expert would think of trying. (Insert grim reaper GIF here.)\r\n\r\n## Option 4: the array is a plain array of integers, but high-level behaviors interpret it as references\r\n\r\nInstead of making this a low-level thing, perhaps the low-level array could just be integers, but we add a high-level behavior to interpret it as references. This solves the issue of not having to rewrite all the low-level methods to be recursion-aware. It gives the redirects that second-class status that I wanted, so that A \u2192 B \u2192 redirect(A) is different from B \u2192 A \u2192 redirect(B). It can also work in Numba because high-level behaviors can be overriden in Numba.\r\n\r\nHowever, the main place where redirects act is in `__getitem__`, and `__getitem__` is a deeply recursive C++ function. Adding a behavioral override for `__getitem__` would mean getting information about `behavior` into the C++ call. It will be hard to find a way to do that without making the C++ codebase depend on pybind11.\r\n\r\nAlso, this puts the responsibility for dealing with references into every behavioral override. That might be putting too much work on framework-developers for something that could be handled once for all.\r\n\r\n## Semi-conclusion: I'm thinking of going with Option 2.\r\n\r\nThoughts, @nsmith-?",
  "created_at":"2020-05-16T12:43:25Z",
  "id":629640098,
  "issue":178,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTY0MDA5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-16T12:43:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"For now, after implementing the new masked getitem behavior in #322 I was able to implement option 4 in NanoEvents.\r\nIt works like this: a `NanoEventsFactory` object is constructed, and the `factory.events()` method constructs the `events` array. As long as the factory object is alive, it holds a reference to the array so subsequent calls can return the cached copy. A unique key is generated when the factory is instantiated, and the `NanoEventsFactory` class holds a class member that is a weak value dictionary mapping this key to the factory instance. All collections in the `events` have a parameter `events_key` that can be used by the `NanoEventsFactory.get_events(key)` classmethod to retrieve the parent `events` that the collection belongs to. As long as users _only mask entries_ in the events and child collections (never reducing the length) then the cross-references can be computed, masking entries where the calling collection no longer has any information.",
  "created_at":"2020-07-09T19:04:25Z",
  "id":656298938,
  "issue":178,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NjI5ODkzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-09T19:04:25Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"In a revision of NanoEvents, I started using the `behavior` attribute of high-level arrays to carry a sticky reference to an initial array, which the derived arrays can reference into using integers in a high-level behavior method. So, in summary, option 4 still seems to work reasonably well.",
  "created_at":"2020-08-18T18:20:58Z",
  "id":675638143,
  "issue":178,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTYzODE0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T18:20:58Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Current thinking:\r\n\r\n   * JupyterBooks and Doxygen do not \"play well\" together. Clicking from JupyterBooks to Doxygen on the same site causes Doxygen to render incorrectly until it is reloaded. Apparently, the CSS or Javascript is not being loaded the first time. It has nothing to do with any customizations I've made\u2014this always happens, and only when clicking _from_ JupyterBooks _to_ Doxygen.\r\n   * So we'll host them on different sites.\r\n      * Maybe Doxygen on ReadTheDocs (through MkDocs, because that passes on static HTML well)?\r\n      * Maybe JupyterBooks in the GitHub `docs/` directory, because GitHub is well integrated with Jekyll.\r\n\r\nKeep those two far apart! If the error still happens, then we just leave it as-is and don't make many links from JupyterBooks to Doxygen. They'll be siloed.",
  "created_at":"2020-03-19T01:40:29Z",
  "id":600941828,
  "issue":180,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMDk0MTgyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-19T01:40:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is not final. I'm merging into master to see if GitHub Pages will pick up the Jekyll content in the docs/ directory.",
  "created_at":"2020-03-19T16:10:22Z",
  "id":601268305,
  "issue":180,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTI2ODMwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-19T16:10:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It already does: [in the documentation](https://awkward-array.readthedocs.io/en/latest/_auto/ak.mask.html),\r\n\r\n```python\r\n>>> array = ak.Array([[[0, 1, 2], [], [3, 4], [5]], [[6, 7, 8], [9]]])\r\n>>> good = (array % 2 == 1)\r\n>>> good\r\n<Array [[[False, True, False], ... [True]]] type='2 * var * var * bool'>\r\n>>> ak.mask(array, good)\r\n<Array [[[None, 1, None], ... None], [9]]] type='2 * var * var * ?int64'>\r\n```",
  "created_at":"2020-04-08T12:44:27Z",
  "id":610937131,
  "issue":181,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMDkzNzEzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-08T12:44:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hmmm. `awkward.cpython-37m-darwin.so` is not a file that should exist: the extension modules are named `layout`, `types`, and `_io`, and the non-Python dynamically loadable libraries are `libawkward-cpu-kernels` and `libawkward`. (The script is aware that they end in `.dylib`, rather than `.so`.)\r\n\r\nHave you tried the full installation command below? (And maybe with `--no-cache`, as there might be some broken pip cache now.)\r\n\r\n```bash\r\npip install .[test,dev]\r\n```\r\n\r\nIf this turns out to be necessary, I shouldn't be recommending the version without `[test,dev]`. (I'm unfamiliar with installing a local directory with pip\u2014it's not something I do often, and I don't have a Mac to test directly on.)\r\n\r\nAnother question: why are you compiling from source? Is there something wrong with the wheels?",
  "created_at":"2020-03-19T16:29:29Z",
  "id":601278701,
  "issue":182,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTI3ODcwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-19T16:29:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I am looking to help out in the project. Not sure what yet, but more python-side (with numba!) than c++, if I can help it.",
  "created_at":"2020-03-19T16:45:17Z",
  "id":601287047,
  "issue":182,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTI4NzA0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-19T16:45:17Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, that's great!\r\n\r\nIn that case, I suggest the [Development Workflow](https://github.com/scikit-hep/awkward-1.0#development-workflow) instructions. Pip triggers a complete recompilation with every install and there seemed to be no way around that. (It starts by copying files into a clean directory, so it appears to be a new set of C++ sources every time. Pip just wasn't designed for incremental development.) The `localbuild.py` script compiles once (or incrementally if you change C++, but you won't) and copies everything from `src/awkward1` to `awkward1` so that you can run it locally. Be sure to edit the `src/awkward1/*` files and not the importable ones that get overwritten. The `localbuild.py` script also has a `--pytest` parameter so that the edit-test cycle is as simple as possible. This doesn't install it system-wide, though.\r\n\r\nI'm working on the documentation now and hope to clarify the install options on the main page.",
  "created_at":"2020-03-19T17:02:48Z",
  "id":601300440,
  "issue":182,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTMwMDQ0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-19T17:02:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> pip install .[test,dev]\r\n\r\nran OK, but I don't think actually installed in the end; but `localbuild` worked. I can always add the path to PYTHONPATH or link the build dir into site-packages, that'll work.",
  "created_at":"2020-03-19T17:07:32Z",
  "id":601302816,
  "issue":182,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTMwMjgxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-19T17:07:32Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I think this means that the \"pip install\" build is okay as far as we know (I'd want to try it fresh on a new environment to know if it's really not installing), but I shouldn't recommend the version without `[test,dev]`. Having too many install options is clutter, anyway.",
  "created_at":"2020-03-20T21:29:05Z",
  "id":601918542,
  "issue":182,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTkxODU0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-20T21:29:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The 79 character limit will be less destructive to readability if I introduce some aliases:\r\n\r\n   * [x] `std::shared_ptr<Content>` \u2192 `ContentPtr` (1435 cases)\r\n   * [x] `std::vector<std::shared_ptr<Content>>` \u2192 `ContentPtrVec` (75 cases)\r\n   * [x] `std::shared_ptr<Builder>` \u2192 `BuilderPtr` (503 cases)\r\n   * [x] `std::shared_ptr<Identities>` \u2192 `IdentitiesPtr` (163 cases)\r\n   * [x] `std::shared_ptr<SliceItem>` \u2192 `SliceItemPtr` (142 cases)\r\n   * [x] `std::shared_ptr<Type>` \u2192 `TypePtr` (128 cases)\r\n   * [x] `std::shared_ptr<util::RecordLookup>` \u2192 `util::RecordLookupPtr` (55 cases)\r\n\r\nAnd maybe also\r\n\r\n   * [x] Replace all `typedefs` with `using`.",
  "created_at":"2020-03-19T22:13:20Z",
  "id":601441561,
  "issue":183,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTQ0MTU2MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-03-20T11:25:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I just turned 10,000 lines of code into 40,000 lines of code.  `:)`",
  "created_at":"2020-03-21T00:06:15Z",
  "id":601960356,
  "issue":183,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTk2MDM1Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2020-03-21T00:06:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"**Before:**\r\n\r\n![before](https://user-images.githubusercontent.com/1852447/77215618-11625980-6ae3-11ea-9dda-5a0706003cf2.png)\r\n\r\n**After:**\r\n\r\n![after](https://user-images.githubusercontent.com/1852447/77215622-1921fe00-6ae3-11ea-9866-8d17e3e131ce.png)\r\n\r\n**Done!**",
  "created_at":"2020-03-21T00:44:13Z",
  "id":601967504,
  "issue":183,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTk2NzUwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-21T00:44:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm calling it a bug because it's not optional, though I suppose that's stretching the terminology.",
  "created_at":"2020-03-20T21:27:19Z",
  "id":601917989,
  "issue":184,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwMTkxNzk4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-20T21:27:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I want to work on this. I think a specialized C++ method which concatenates two arrays at a time with Python calling it repeatedly for all input arrays should do it.\r\n\r\nWriting it in Python as a recursive function:\r\n```python\r\ndef concat(arrays, axis=0, mergebool=True, highlevel=True):\r\n    if axis != 0:\r\n        res = [subarr for subarr in arrays[0]]\r\n        for arr in arrays[1:]:\r\n            for i in range(len(arrays[0])):\r\n                res[i] = concat([res[i], arr[i]], axis=axis-1, mergebool=mergebool, highlevel=highlevel)\r\n        return ak.Array(res)\r\n    else:\r\n        return ak.concatenate(arrays, mergebool=mergebool, highlevel=highlevel)\r\n```\r\n```python\r\n>>> x = ak.Array([[[1, 2, 3], [4, 5]], [[6, 7]]])\r\n>>> y = ak.Array([[[], [8, 9]], [[10, 11]]])\r\n>>> x, y\r\n(<Array [[[1, 2, 3], [4, 5]], [[6, 7]]] type='2 * var * var * int64'>,\r\n <Array [[[], [8, 9]], [[10, 11]]] type='2 * var * var * int64'>)\r\n\r\n>>> concat([x, y], axis = 0).tolist()\r\n[[[1, 2, 3], [4, 5]], [[6, 7]], [[], [8, 9]], [[10, 11]]]\r\n\r\n>>> concat([x, y], axis = 1).tolist()\r\n[[[1, 2, 3], [4, 5], [], [8, 9]], [[6, 7], [10, 11]]]\r\n\r\n>>> concat([x, y], axis = 2).tolist()\r\n[[[1, 2, 3], [4, 5, 8, 9]], [[6, 7, 10, 11]]]\r\n```\r\nTo use a iterative solution instead of a recursive one we will need to track the index at each depth. At each depth we need to rewrite the Array's `Index` and `Content` and at `depth=axis` we need to call `merge`, not sure about the `Identities` and `Parameters` part yet, maybe I'll figure it out as I start working.\r\nHopefully no time pressure on this task as well.",
  "created_at":"2020-04-17T22:46:17Z",
  "id":615496042,
  "issue":184,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTQ5NjA0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T22:46:17Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"This is the correct behavior, and you're right that it should be recursive. The identities should be `Identities::none()`, so that's easy, and the parameters should probably be the parameters of the first array. I should think about what the policy should be when combining arrays with different parameters, and maybe introduce a function that merges them in a standardized way.\r\n\r\nYou're also right that this needs to be a custom C++ function\u2014the [old solution in pure Python](https://github.com/scikit-hep/awkward-array/blob/a2645fdaed1a6997c4677ae47cbb2cd0663e8a21/awkward/array/jagged.py#L1643-L1726) was very complicated and only worked for `axis=1`. However, any operation that involves more than one array needs a Python part to do the broadcasting:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ef401a044dc6b5b48443caa9223d1e3d6aecec78/src/awkward1/_util.py#L382\r\n\r\nBroadcasting in Awkward Array not only involves expanding dimensions of length 1 as it does for NumPy, but also dealing with IndexedArrays, RecordArrays, UnionArrays, and the rest. It's written in Python because testing for all of these types would involve a lot of `dynamic_casts` in C++, but it's more manageable in Python. For example, if one array is\r\n\r\n   * ListOffsetArray \u2192 IndexedArray \u2192 ListArray \u2192 RecordArray,\r\n\r\nand another is\r\n\r\n   * IndexedArray \u2192 ListArray \u2192 IndexedArray \u2192 ListOffsetArray \u2192 RecordArray,\r\n\r\nthen they have equivalent structure without even needing to \"broadcast\" in the NumPy sense. Checking for all of those cases in C++ would be a mess! (The `merge` function behind concatenate for `axis=0` has to deal with this complexity at one level of depth, and it's [quite messy](https://github.com/scikit-hep/awkward-1.0/blob/ef401a044dc6b5b48443caa9223d1e3d6aecec78/src/libawkward/array/ListArray.cpp#L739-L1000). It exists to concentrate that mess in one place, so that it can be used elsewhere.)\r\n\r\nTherefore, I don't think this is a \"good first issue.\" I'd appreciate the help, but I think this one would be too much without more background in the internals.",
  "created_at":"2020-04-17T23:31:18Z",
  "id":615508038,
  "issue":184,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTUwODAzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T23:31:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Can this feature be reprioritized? Axis-1 (or deeper) concatenate can be very useful for some common operations, e.g. merging lepton collections.",
  "created_at":"2020-07-24T19:16:30Z",
  "id":663692492,
  "issue":184,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY5MjQ5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-24T19:16:30Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"That would be reasonable. I'll get to it soon after dealing with Arrow 1.0, which seems to have broken everything.",
  "created_at":"2020-07-24T19:18:23Z",
  "id":663693237,
  "issue":184,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY5MzIzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-24T19:18:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna I was just thinking about this: what would you think about taking this on? It would use the negative axis you just finished. Do you see what it's supposed to do?\r\n\r\n(Personally, I don't know what it ought to do if the desired axis is deeper than a record layer. Maybe we could exclude that case.)",
  "created_at":"2020-07-25T05:26:37Z",
  "id":663812391,
  "issue":184,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzgxMjM5MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-07-25T05:26:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This wasn't linked to the new PR, #539, so it didn't automatically close. But it's done!",
  "created_at":"2020-11-17T14:02:21Z",
  "id":728947953,
  "issue":184,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODk0Nzk1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T14:02:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've tested it on ReadTheDocs and all is well. Now I just need to write that documentation.",
  "created_at":"2020-03-26T03:10:45Z",
  "id":604203143,
  "issue":187,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNDIwMzE0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-26T03:10:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to close this PR off now because it's really about the front page documentation. The next PR will be about Python docstrings.",
  "created_at":"2020-03-26T18:22:38Z",
  "id":604597689,
  "issue":187,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNDU5NzY4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-26T18:22:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Close this off for now so that the `ak.Array` documentation will be public while I write the rest.",
  "created_at":"2020-03-28T12:06:04Z",
  "id":605438303,
  "issue":188,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNTQzODMwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-28T12:06:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"## This PR will wait until Numba 0.49 is released on PyPI.\r\n\r\nAs soon as that happens, I'll `git merge master` it, test it, and if all goes well, accept it.",
  "created_at":"2020-03-26T21:30:07Z",
  "id":604697115,
  "issue":189,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNDY5NzExNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-26T21:30:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The first Numba release candidate was just released:\r\n\r\nhttps://numba.pydata.org/numba-doc/dev/release-notes.html\r\n\r\nNote to self: since Numba is an optional dependency, it can't be pinned to a minimal version the way that Numpy is. I should explicitly check the version in any functions that _use_ Numba (not the entry point\u2014that gets called even if a user doesn't use it, which can be annoying if they have Numba 0.48 for reasons that have nothing to do with Awkward). So the check shouldn't be in `register()`, but it should be in something that gets called before every _user-initiated_ action.",
  "created_at":"2020-03-28T14:29:03Z",
  "id":605454752,
  "issue":189,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNTQ1NDc1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-28T14:29:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"[Numba 0.49 is now out](http://numba.pydata.org/numba-doc/latest/release-notes.html#version-0-49-0), so it's time for this PR to be released. I've updated it to master and this CI run will determine if we're getting the new Numba on Azure. The old Numba would cause errors.",
  "created_at":"2020-04-17T16:37:01Z",
  "id":615346266,
  "issue":189,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTM0NjI2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T16:37:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It works! Time to merge and deploy.",
  "created_at":"2020-04-17T18:26:22Z",
  "id":615396579,
  "issue":189,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTM5NjU3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T18:26:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Merge this so that these docs can go to the website while I work on the remainder.",
  "created_at":"2020-03-30T01:56:22Z",
  "id":605742048,
  "issue":190,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNTc0MjA0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-30T01:56:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The same could be said of ak.tojson: it takes a string to be a file name, not a `FILE`.",
  "created_at":"2020-03-28T21:36:07Z",
  "id":605522722,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNTUyMjcyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-28T21:36:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ideally there should be a path to use arbitrary python file-like objects, not just C-files. I understand that might come with quite a performance penalty for local files, and you might  want to split the loading code.",
  "created_at":"2020-03-31T12:49:48Z",
  "id":606606058,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjYwNjA1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-31T12:49:48Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"In that case, I guess it would be using the RapidJSON function that converts to strings; that would be the \"performance penalty.\" It must also be possible to convince RapidJSON to output an iterable of strings (that can be consumed in a memory-efficient way by Python), but I haven't delved into it.",
  "created_at":"2020-03-31T12:57:18Z",
  "id":606609844,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjYwOTg0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-31T12:57:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"It is a general point though - I would love to see all of the IO work for remote file-systems via python. That will often amount to processing in-memory bytes objects, but hopefully can use the existing read-block API to avoid having to hold entire files in memory.",
  "created_at":"2020-03-31T12:59:56Z",
  "id":606611165,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjYxMTE2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-31T12:59:56Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Maybe the right place to do it is in ChunkedArray (issue #56, which is soon-to-do on my list).\r\n\r\nAwkward0 had a ChunkedArray, and it's been causing problems in people's analyses because it's not correctly implemented for all methods. Making it a `Content` type like everything else (i.e. that can be nested anywhere) was too ambitious, and it only really makes sense to have chunkability at the root and leaves of the tree. At the root, you get partitioning for parallel processing, and at the leaves, you get paging for I/O systems.\r\n\r\nMy plan is to add ChunkedArray at the root (an alternative NumpyArray for chunked leaves would be left for indefinitely later: paged I/O is good, but you can usually concatenate all of those 1-d arrays before using them in an analysis). Since this defines the partitioning, it might not be necessary to also iterate over chunks of JSON output because each chunk of a ChunkedArray can be written to JSON in its entirety and that's the chunked stream.\r\n\r\nIf the system wants the sequence of JSON chunks to collectively form a single JSON, that just means replacing the last `]` with `,` and the first `[` with nothing...",
  "created_at":"2020-03-31T13:07:58Z",
  "id":606615201,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjYxNTIwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-31T13:07:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> If the system wants the sequence of JSON chunks to collectively form a single JSON, that just means replacing the last ] with , and the first [ with nothing...\r\n\r\n(or a line terminator, since line-deliminated JSON is extremely common)",
  "created_at":"2020-03-31T13:11:36Z",
  "id":606617336,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjYxNzMzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-31T13:11:36Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"For future reference:\r\n\r\n   * The many-JSON case is done in #543. They can be separated by a line terminator, though they don't have to be (JSON syntax defines its own end-of-object).\r\n   * Passing in a Python object with a `read` method has not been done.\r\n\r\nSuch an implementation should not read in the whole file and pass it as a string to the existing method\u2014it should read in the data and let RapidJSON parse it in small steps. That's fine: the SAX-like interface and StringStream that we're already using from RapidJSON is well-suited to this style of interface.\r\n\r\nWhat it would take for the latter: a variant of `do_parse`\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/68087850eabafd39f0c0de37ed96487fa44cc69f/src/libawkward/io/json.cpp#L662-L704\r\n\r\nthat repeatedly calls `reader.Parse` without erroring out if the parsing is not done for a given batch of string data and a variant of `rj::StringStream` in\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/68087850eabafd39f0c0de37ed96487fa44cc69f/src/libawkward/io/json.cpp#L706-L716\r\n\r\nthat is configured to call the Python object's `read` function for data. Such a thing would use pybind11, so it belongs in the `src/python` directory. (References to pybind11 are not allowed in the `src/libawkward` codebase.) There are other objects like this, which are called from `src/libawkward` but defined in `src/python`, such as the `PyArrayGenerator` and `PyArrayCache` in https://github.com/scikit-hep/awkward-1.0/blob/main/src/python/virtual.cpp . We use a Java-like interface: abstract class in `src/libawkward`, concrete class in `src/python`.",
  "created_at":"2020-12-11T22:36:42Z",
  "id":743463094,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ2MzA5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T22:36:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This would be a \"good first issue\" for developers who want to delve into C++. This function has a straightforward output\u2014booleans at some level of depth\u2014but more importantly, it only involves one input array and therefore avoids broadcasting.\r\n\r\nThere are a number of rules to follow when adding an operation in C++, such as never accessing a buffer's data directly\u2014it has to go into a new cpu-kernels function to keep the structure organization code separate from the numeric computation code, which will eventually be have a gpu-kernels backend as well. If anyone starts on this issue, it will force me to write a CONTRIBUTING.md to spell out what's allowed and what isn't.",
  "created_at":"2020-04-17T23:40:22Z",
  "id":615510971,
  "issue":193,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTUxMDk3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T23:40:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I have begin working on this and have a few thoughts which might need corrections and/or suggestions.\r\n\r\nWe'll have a `is_none` function with the arguments `axis` and `depth`. Till `axis != depth` we recursively call `is_none` on `contents` incrementing `depth` until `axis = depth` similar to many other functions.\r\nWhen `axis = depth` if we are currently an `IndexedOptionArray` we return `NumpyArray(bytemask())` otherwise we return a `NumpyArray` of the same length filled with `False`.\r\n\r\nNow the issue is that there is no `NumpyArray` constructor for boolean values ([here](https://github.com/scikit-hep/awkward-1.0/blob/master/src/libawkward/array/NumpyArray.cpp#L27)). There is a constructor right below which can allow us to do something like `NumpyArray(bytemask(), \"Bool\")`. I can't think of anything with the same convenience. Can this be added ? If so, then what should be it's name as \"b\" is reserved for `int8` right now.\r\n\r\nCurrently, calling `ak.is_none(ak.Array([None, 1, 2, 3])).dtype` returns `dtype('bool')`. I am assuming this behavior needs to be maintained. \r\n\r\nAs far as the `CONTRIBUTING.md` is concerned maybe you can start writing the `How-it-works tutorials for developers` section, a main outline of the architecture along with an explanation of the various Array types can at least give potential contributors a head start before this section is fully written. The previous readme page([here](https://github.com/scikit-hep/awkward-1.0/blob/0.2.5/README.md)) was helpful compared to right now where it would be a nightmare for a newcomer to figure things out independently.",
  "created_at":"2020-04-22T01:58:21Z",
  "id":617501512,
  "issue":193,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzUwMTUxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-22T01:58:21Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"If the NumpyArray constructor is given a NumPy array with `np.bool` dtype, the NumpyArray will have boolean type. There's no reason to express that separately.\r\n\r\nGiving this function depth would go through `awkward1._util.recursively_apply`. That takes a function that tells `recursively_apply` whether it should keep descending or to compute something on the current level and return. The full set of array nodes with option type are in `awkward1._util.optiontype`, and they all have `boolmask` methods.\r\n\r\nDoing so would turn the output of this function from a NumPy array (which can have dtype `np.bool` or `np.bool_`) into an Awkward Array (which can have depth, and its dtype is a dummy to satisfy Pandas). I think that would be okay because the circumstances that require NumPy output, such as Pandas, ask for `axis=0`, which makes an Awkward Array that is immediately convertible into a NumPy array (i.e. its `__array__` method doesn't complain).\r\n\r\nI agree that I need documentation, but I'm also on the hook for updating the Uproot project to output Awkward Arrays. That's supposed to be done by the end of April, which isn't going to happen because the two new array node types it needs, PartitionedArray and VirtualArray, are still in progress (well, the latter one is). I know that writing documentation enables contributors, but updating Uproot enables users, since it's the primary way physicists obtain arrays. Right now, Awkward1 has quite a lot developed, but few users because they're still getting Awkward0 arrays from Uproot.\r\n\r\nI'll help you with spot problems and can probably manage to write that CONTRIBUTING.md, but such a file would be about coding practice, not an explanation of how the library works.",
  "created_at":"2020-04-22T11:36:27Z",
  "id":617723136,
  "issue":193,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzcyMzEzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-22T11:36:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This issue keeps popping up, so I'm moving it to the November project. As per the discussion on #226, it will have a Python implementation. The `awkward1._util.recursively_apply` utility would make short work of it.",
  "created_at":"2020-10-30T22:03:28Z",
  "id":719820303,
  "issue":193,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgyMDMwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:03:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This one will be next, following up on PR #582.",
  "created_at":"2020-12-09T01:08:20Z",
  "id":741368908,
  "issue":193,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MTM2ODkwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-09T01:08:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"These name changes are significant enough that I should stop this PR right now and merge it. That way, it will all be in one commit on master.",
  "created_at":"2020-03-30T13:51:10Z",
  "id":606011890,
  "issue":194,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjAxMTg5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-30T13:51:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski Hi, I didn't mean to disturb. Can you check your email ? I will delete this message soon.",
  "created_at":"2020-03-30T14:12:42Z",
  "id":606024203,
  "issue":195,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjAyNDIwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-30T14:12:42Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski Hi, I didn't mean to disturb. Can you check your email ? I will delete this message soon.\r\n\r\nI'm checking my email. (That's how I got this message, after all.)",
  "created_at":"2020-03-30T14:33:07Z",
  "id":606036514,
  "issue":195,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjAzNjUxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-30T14:33:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Did you receive any mails lately ? I sent one 17 hours ago related to the GSoC project.\r\n",
  "created_at":"2020-03-30T14:39:38Z",
  "id":606040496,
  "issue":195,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjA0MDQ5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-30T14:39:38Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"> Did you receive any mails lately ? I sent one 17 hours ago related to the GSoC project.\r\n\r\nI did, but it's a long one and will take me some time to get through all the materials and questions. I was looking for a long enough block to dedicate to it.",
  "created_at":"2020-03-30T14:43:18Z",
  "id":606042732,
  "issue":195,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjA0MjczMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-30T14:43:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Fair enough. Hopefully, you can get through them by the end of the day. I shall send a draft proposal within the next 2 hours. If you could review that it would be great. \r\nThank You",
  "created_at":"2020-03-30T14:53:15Z",
  "id":606048613,
  "issue":195,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNjA0ODYxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-03-30T14:53:15Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Cleaned up doxygen warnings (only one kind of warning left; don't know how to fix it, but the documentation it generates looks okay).",
  "created_at":"2020-04-02T01:18:02Z",
  "id":607567279,
  "issue":197,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNzU2NzI3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T01:18:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, fixed those, too. Doxygen and Sphinx run without warnings.",
  "created_at":"2020-04-02T01:33:29Z",
  "id":607571061,
  "issue":197,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwNzU3MTA2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T01:33:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"`ak.combinations` is fine by me, I originally picked `choose` as it is N-choose-k which gives the number of combinations and is slightly shorter, but sadly numpy has stole our name.  For `cross`, how about `ak.cartesian_product`? I might consider this acceptable verbosity since anyway `cross` can be implemented using broadcasting tricks.",
  "created_at":"2020-04-02T18:26:07Z",
  "id":608027910,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODAyNzkxMA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-02T18:26:07Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think ak.cross -> ak.outer should work well (np.outer is ak.cross for rectilinear data structures).\r\n\r\nak.choose -> ak.combinations is fine to me and very clear. I like the connection to itertools.",
  "created_at":"2020-04-02T18:30:42Z",
  "id":608030312,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODAzMDMxMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-02T18:30:42Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"> I think ak.cross -> ak.outer should work well (np.outer is ak.cross for rectilinear data structures).\r\n\r\n[np.outer](https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html) doesn't make tuples; it actually multiplies, so it's different. itertools.product, however, makes tuples.",
  "created_at":"2020-04-02T18:32:58Z",
  "id":608031562,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODAzMTU2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T18:32:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah - true it really is the product. I think in the same style, `ak.cartesian` should work well, as Nick was suggesting. I don't think we need to add `_product` onto everything.",
  "created_at":"2020-04-02T18:36:56Z",
  "id":608033540,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODAzMzU0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T18:36:56Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"We're back from the GitHub outage! I'm thinking of going with\r\n\r\n   * `choose` \u2192 `combinations`\r\n   * `diagonal` \u2192 `replacement`\r\n   * `cross` \u2192 `product`\r\n\r\nThe first two seem pretty uncontroversial, and for the third, we have to decide between\r\n\r\n   * confusing `product` with multiplication (maybe tab-complete would show that there's both `prod` and `product`?)\r\n   * lack of symmetry with combinations/getting names from itertools. (Sorry, Descartes.)\r\n\r\nThanks for the super-informal survey. I think I'll wait on closing that PR until tomorrow, in case there are any other comments, but I want to be ready to press \"go\" if it's fine.",
  "created_at":"2020-04-02T21:45:39Z",
  "id":608107004,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODEwNzAwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T21:45:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm making a few more last-minute renamings on #201 in case anyone would like to chime in. Speak now...",
  "created_at":"2020-04-02T22:30:48Z",
  "id":608125276,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODEyNTI3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T22:30:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I will merge all the name changes enacted in #201 tomorrow unless somebody complains.",
  "created_at":"2020-04-03T01:40:59Z",
  "id":608182075,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODE4MjA3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-03T01:40:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> confusing product with multiplication (maybe tab-complete would show that there's both prod and product?)\r\n\r\nI think this is perfectly acceptable:\r\n* In my opinion `prod` and `product` do the \"obvious\" thing once you know `awkward1` is aiming to copy `itertools` and `numpy`\r\n* It's unlikely that using the wrong one would give a sensible result",
  "created_at":"2020-04-03T05:06:54Z",
  "id":608232035,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODIzMjAzNQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-03T05:06:54Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"NONE",
  "body":"+1 for `ak.cartesian()`, following the style of `np.dot()`, `np.inner()`, `np.outer()`, and `np.kron()`. I agree that `combinations` and `replacement` seem perfectly fine.",
  "created_at":"2020-04-03T07:53:12Z",
  "id":608286826,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODI4NjgyNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-03T07:53:12Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"It is possible for `ak.product` and `ak.cartesian` to both exist as synonyms (a fact that, if suspected, is easily checked if one of them `is` the other). I'm holding out the possibility that `ak.tolist` will someday be added as a synonym for `ak.to_list`. In that case, the question is whether `ak.product` would exist first with the option to add `ak.cartesian` or vice-versa.\r\n\r\n(I'm not going to add synonyms right away because they're easy to add and hard to remove. Only if there's a large number of people who want it, after the library has been in use for a while.)\r\n\r\nSo instead of asking who's in favor of `ak.cartesian`, is anyone against `ak.product`? Is the existence of that function a problem that later adding an `ak.cartesian` synonym wouldn't fix?",
  "created_at":"2020-04-03T11:37:35Z",
  "id":608385711,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODM4NTcxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-03T11:37:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think `ak.product` is too close to `ak.prod`, in letter space and that makes them want to be close in concept space. If some one just types 'product' programming away they're probably gonna expect multiplication first, if they're coming from numpythonic background. It will make sense to itertools users, but that might not be the largest cross section of people this package encounters.\r\n\r\n`ak.cartesian` at least has that first level of \"what exactly do they mean by cartesian??\",  and it'll show up quickly when searching in the repository for functions.",
  "created_at":"2020-04-03T13:26:54Z",
  "id":608432652,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODQzMjY1Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-03T13:27:05Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"How does everyone else feel if I make it `ak.cartesian` first with the option of possibly later adding `ak.product`? With `ak.cartesian` being first, this also means that all the documentation and tutorials will say `ak.cartesian`, leading users in that direction. There would also be some resistance against adding a synonym for the reason Lindsey described, and just because synonyms are hard to get rid of.\r\n\r\nI personally preferred agreement with itertools, but I'm willing to go with majority opinion (as I was with the [underscores between words](https://github.com/scikit-hep/awkward-array/issues/240) and the [80-character width](https://github.com/scikit-hep/awkward-1.0/pull/183) if the community is clear).\r\n\r\n**If I don't hear a complaint,** I'll change `ak.product` to `ak.cartesian`. **If there isn't consensus,** I will leave it as `ak.product` (because I get the deciding vote if the community's opinion isn't strongly one way).",
  "created_at":"2020-04-03T14:04:25Z",
  "id":608451852,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODQ1MTg1Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":3,
   "total_count":3
  },
  "updated_at":"2020-04-03T14:04:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It sounds like everybody's okay with `ak.cartesian` and some are actively for it. I'll change to `ak.cartesian`; that will be the primary (and probably only) name for this function.",
  "created_at":"2020-04-03T15:46:47Z",
  "id":608515849,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODUxNTg0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-03T15:46:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Going, going, gone!\r\n\r\n(It's `ak.cartesian`.)",
  "created_at":"2020-04-03T17:29:54Z",
  "id":608566470,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODU2NjQ3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-03T17:29:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm not sure if you've been seeing this, but it was asked for.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/docs/0198-tutorial-documentation-1/studies/how-to-questions-survey.md\r\n\r\nThanks for making an issue!",
  "created_at":"2020-04-02T20:05:12Z",
  "id":608075210,
  "issue":200,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODA3NTIxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T20:05:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Interestingly, `ak.concatenate` with `axis=-1` would be implementable with `ak.stack(ak.zip([a, b, c]))`.  For other axes, I think the more generalized pandas `pivot` method would need to be implemented: it can map an arbitrary set column index levels to new row index levels.",
  "created_at":"2020-04-03T19:30:57Z",
  "id":608619679,
  "issue":200,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODYxOTY3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-03T19:30:57Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Something to consider:\r\n\r\n   * `rpad` \u2192 `padna`\r\n\r\nIt can't just be \"pad\" because of [np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). We (with @ianna) added an \"r\" because the `None` values go on the right, not the left. But really, unequal length lists always align left like this\u2014it has to do with the fact that the leftmost objects can be identified with each other while the rightmost cannot. Putting \"r\" for \"right\" in the name is (1) a little obscure; I wouldn't guess \"right\", and (2) begs the question, \"why not left?\" I don't want to entertain that question.\r\n\r\nSo instead of saying which side should get padded, since this function isn't actually different in that regard, let's use the disambiguator to say what it should be padded with. NumPy's pad fills with zeros; Pandas's [pd.Series.str.pad](https://pandas.pydata.org/pandas-docs/version/0.22.0/generated/pandas.Series.str.pad.html) pads with spaces. The fact that Awkward fills with `None` is a bit unusual\u2014the other libraries don't have the opportunity to do that.\r\n\r\nSo maybe this or\r\n\r\n   * `rpad` \u2192 `pad_none` and\r\n   * `fillna` \u2192 `fill_none`\r\n\r\nThis would be more in keeping with the use of snake_case and whole words, which was a request from many people (https://github.com/scikit-hep/awkward-array/issues/240). @nsmith- suggested `fillna` for [agreement with Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html), but that's a method on DataFrame, not a free function like ours, and the \"na\" comes from R, which doesn't really have an equivalent in Python.",
  "created_at":"2020-04-02T22:29:29Z",
  "id":608124762,
  "issue":201,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODEyNDc2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T22:29:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Ach! But ak.Array _must_ have a method named `isna` or else it can't be a Pandas column!\r\n\r\nOkay, so there's a difference between free functions and methods:\r\n\r\n   * [x] function `ak.pad_none`\r\n   * [x] function `ak.fill_none`\r\n   * [x] function `ak.is_none` (no function `ak.not_none`; that's gratuitous)\r\n   * [x] function `ak.to_list`\r\n   * [x] method `ak.Array.isna` because Pandas requires it\r\n   * [x] method `ak.Array.dropna` might be required for performance in Pandas\r\n   * [x] method `ak.Array.fillna` might be required for performance in Pandas\r\n   * [x] method `ak.Array.tolist` to be like NumPy; it's just natural to type\r\n   * [x] method `ak.Array.tojson` for symmetry\r\n\r\nIf the People ask for it, synonyms `ak.tolist` \u2192 `ak.to_list` and `ak.tojson` \u2192 `ak.to_json` can be later added, but only if it's a true desire path.\r\n\r\nTo do:\r\n\r\n   * [x] Document the ak.Array methods that exist only for Pandas compatibility.\r\n",
  "created_at":"2020-04-02T22:44:01Z",
  "id":608129816,
  "issue":201,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODEyOTgxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-03T01:12:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hmm. NumPy has an np.product, but [it's going to be deprecated](https://stackoverflow.com/a/49864286/1623645). That could count as an argument against it\u2014a NumPy function of that name technically exists\u2014but it could count as an argument for it\u2014NumPy will never use that name again, for fear of conflicts with old code.",
  "created_at":"2020-04-02T22:51:53Z",
  "id":608132379,
  "issue":201,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODEzMjM3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T22:51:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"   * [ak.covar](https://awkward-array.readthedocs.io/en/latest/_auto/ak.covar.html) is not quite the same thing as [np.cov](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html) (both do correlations)\r\n   * [ak.corr](https://awkward-array.readthedocs.io/en/latest/_auto/ak.corr.html) is not quite the same thing as [np.corrcoef](https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html) (both do correlations)\r\n\r\nAs long as they're not quite the same, I'll keep their different names. That, at least, will prevent misidentification by accident. The NumPy functions seem to be designed for a single large matrix, while the Awkward functions are reducer-like, which favors computing many small covariances and correlations (e.g. tracking or vertex-finding or something).\r\n\r\nAlso, the ak.covar name has symmetry with ak.var, and they are much closer in behavior than np.cov is to np.var.",
  "created_at":"2020-04-02T23:16:20Z",
  "id":608140433,
  "issue":201,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODE0MDQzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-02T23:16:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Manually document\r\n\r\n   * [x] ak.numba.register\r\n   * [x] ak.pandas.register\r\n   * [x] ak.pandas.df\r\n   * [x] ak.pandas.dfs\r\n   * [x] ak.numexpr.evaluate\r\n   * [x] ak.numexpr.re_evaluate\r\n   * [x] ak.autograd.elementwise_grad\r\n",
  "created_at":"2020-04-02T23:21:08Z",
  "id":608142231,
  "issue":201,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODE0MjIzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-03T00:38:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"**Final names:**\r\n\r\n| old | new |\r\n|:------|:------|\r\n| `ak.choose` | `ak.combinations` |\r\n| `ak.diagonal` | `ak.replacement` |\r\n| `ak.cross` | `ak.cartesian` |\r\n| `ak.rpad` | `ak.pad_none` |\r\n| `ak.fillna` | `ak.fill_none` |\r\n| `ak.isna` | `ak.is_none` |\r\n| `ak.notna` | _gone!_ |\r\n| `ak.Array.to_list` | `ak.Array.tolist` |\r\n| `ak.Array.to_json` | `ak.Array.tojson` |\r\n\r\n`ak.covar` and `ak.corr` are unchanged, as are the methods that are required for Pandas.",
  "created_at":"2020-04-03T17:48:28Z",
  "id":608575354,
  "issue":201,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYwODU3NTM1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-03T17:48:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"No need: `np.minimum` and `np.maximum` are ufuncs, so it already works.\r\n\r\n```python\r\n>>> one = ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]])\r\n>>> two = ak.Array([[1, 3, 2], [], [4, 6]])\r\n>>> np.minimum(one, two)\r\n<Array [[1, 2.2, 2], [], [4, 5.5]] type='3 * var * float64'>\r\n>>> np.maximum(one, two)\r\n<Array [[1.1, 3, 3.3], [], [4.4, 6]] type='3 * var * float64'>\r\n```",
  "created_at":"2020-04-08T12:42:32Z",
  "id":610936181,
  "issue":202,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMDkzNjE4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-08T12:42:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a \"good first issue\" because it's pure Python, wrapping up the singleton/firsts business in the advanced part of this tutorial:\r\n\r\nhttps://github.com/jpivarski/2020-04-08-eic-jlab#readme\r\n\r\ninto a more convenient function. I'm favoring the `array.pick[...]` approach, which would use `ak.singletons` to expand the output of `ak.argmax` into the right form for jagged integer indexing, and then the result of that would be passed through `ak.firsts` to produce the result.\r\n\r\nThis is all done manually in the demo, making \"select one jagged array by the maxima of another\" seem more complicated than it is. The higher-level function would wrap up that technique in an easy-to-use interface.",
  "created_at":"2020-04-17T23:36:03Z",
  "id":615509590,
  "issue":203,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTUwOTU5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T23:36:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- I don't know about `array.pick[...]` anymore. It does seem to be more general than specialized `ak.minby` and `ak.maxby`, but is it how a user would expect to do things?",
  "created_at":"2020-10-23T20:21:03Z",
  "id":715571427,
  "issue":203,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNTU3MTQyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-23T20:21:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Honestly, give the discussion in #434 and elsewhere it seems clear we want `minby` and `maxby` because we keep talking about it :)\r\nSomething something desire path...",
  "created_at":"2020-10-23T20:25:20Z",
  "id":715573133,
  "issue":203,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNTU3MzEzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-23T20:25:20Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"`ak.minby`/`ak.maxby` is more guessable, certainly if you see it in a list or set of tab-completions.",
  "created_at":"2020-10-23T20:33:44Z",
  "id":715576651,
  "issue":203,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNTU3NjY1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-23T20:33:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Or maybe this could be fixed in one swoop by having RecordArray always create an IndexedArray instead of [carrying](https://github.com/scikit-hep/awkward-1.0/blob/8215351f09de3773b14318815538ec0faaeb6235/src/libawkward/array/RecordArray.cpp#L439-L454), or only carry if the number of fields is small (1?).\r\n\r\nSince that affects everything with RecordArray in it, the unit tests would quickly identify any problems with that.\r\n\r\nA potential problem: if `carry` can potentially return an IndexedArray, any procedure that wraps carried data inside another IndexedArray should immediately call `simplify_option` to ensure that we don't create IndexedArrays of IndexedArrays. (In a realistic analysis, that might give rise to very deep chains of IndexedArrays of IndexedArrays.) Previous uses of `carry` assumed that the node type doesn't change, and now it would be able to, so if I do this, I should review what is done with every carried array.",
  "created_at":"2020-04-08T17:29:51Z",
  "id":611089765,
  "issue":204,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMTA4OTc2NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-08T17:29:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This was completed by PR #261.",
  "created_at":"2020-07-20T17:55:09Z",
  "id":661242085,
  "issue":204,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTI0MjA4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T17:55:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think you're right: this is a copy-paste error. I checked for additional errors of the same kind, but didn't find any. If this passes CI, I'll release a new bugfix version.\r\n\r\nThanks!",
  "created_at":"2020-04-10T18:51:50Z",
  "id":612168033,
  "issue":205,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjE2ODAzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-10T18:51:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks, Jim! A votre sant\u00e9 \ud83c\udf7b ",
  "created_at":"2020-04-10T19:07:12Z",
  "id":612174719,
  "issue":205,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjE3NDcxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-10T19:07:12Z",
  "user":"MDQ6VXNlcjc2NDkxMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for pointing this out! I'll be pushing a version 0.2.12 soon; that one should be okay.\r\n\r\nBy the way, is there a reason you're compiling from source? What combination of OS and Python version do you have\u2014is it failing to find a binary wheel for you?",
  "created_at":"2020-04-11T13:28:47Z",
  "id":612421466,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQyMTQ2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T13:28:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"When [this build](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=1801&view=logs&j=4a2b32f2-8889-5075-9c47-2644e2b42452) is done, version 0.2.12 will be [on PyPI](https://pypi.org/project/awkward1/).",
  "created_at":"2020-04-11T13:31:25Z",
  "id":612421968,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQyMTk2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T13:31:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski I maintain `awkward1` in nixpkgs https://github.com/NixOS/nixpkgs/blob/master/pkgs/development/python-modules/awkward1/default.nix , was trying to bump the version to see if it fixes the tests.",
  "created_at":"2020-04-11T14:35:31Z",
  "id":612435201,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQzNTIwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T14:35:31Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The failure I am dealing with right now:\r\nhttps://hydra.nixos.org/job/nixpkgs/trunk/python37Packages.awkward1.x86_64-linux\r\n\r\nMy last time I bisected the problem to find that https://github.com/NixOS/nixpkgs/commit/aecc14b52b08b59f57595a9968cf22def0f9bb7b was where awkward1 first broke for us.",
  "created_at":"2020-04-11T14:42:41Z",
  "id":612436670,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQzNjY3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T14:42:41Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"You will have to bump the Awkward version number: 0.1.38 will not work on recent versions of Numba.\r\n\r\n   1. Early this year, Numba changed the name of the `numpy_arange_1` function, which they considered not part of the public API. I was using it, it broke, and I adjusted. (I can't easily find the version where I fixed it, but that's moot for reasons below.)\r\n   2. I [showed how I was using Numba's API](https://github.com/scikit-hep/awkward-1.0/blob/master/docs-demo-notebooks/2020-01-22-numba-demo-EVALUATED.ipynb) on January 22, 2020, including a list of all the functions I'm using. Not all of them had been declared as public API.\r\n   3. This prompted [Numba to do a survey](https://github.com/numba/numba/issues/5214) of who is using what in their Numba extensions.\r\n   4. They used this information to reorganize their libraries into more clearly defined public and private.\r\n   5. Meanwhile I [completely rewrote my Numba extension](https://github.com/scikit-hep/awkward-1.0/pull/118), using what I had learned from their feedback. Not a single line of the original exists\u2014trying to get Awkward 0.1.38 to work is not useful.\r\n   6. With the new Numba extensions API, the names are different again. When [Numba 0.49](https://groups.google.com/a/continuum.io/forum/#!topic/numba-users/U9ijrsVU6eU) is no longer a release candidate, that will become the minimal version Awkward will accept.\r\n   7. PR #189 is waiting for Numba 0.49 to become a regular release; when it does, I'll be ready with the new names. Until then, I should keep using the old names.",
  "created_at":"2020-04-11T15:14:30Z",
  "id":612443367,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQ0MzM2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T15:14:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Awkward 0.2.12 is [on PyPI](https://pypi.org/project/awkward1/). Does it work for you?",
  "created_at":"2020-04-11T15:15:03Z",
  "id":612443520,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQ0MzUyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T15:15:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Awkward 0.2.12 is [on PyPI](https://pypi.org/project/awkward1/). Does it work for you?\r\n\r\nYes, this issue is resolved by 0.2.12. Thank you!\r\n\r\n> You will have to bump the Awkward version number: 0.1.38 will not work on recent versions of Numba.\r\n\r\nI've pushed a bump to 0.2.12, and it seems to work fine on linux. However I still get errors on macOS\r\nhttps://gist.github.com/veprbl/60e1ffb1374a5ae617651ef57675d320\r\nLooks like some RTTI problem.",
  "created_at":"2020-04-11T15:49:21Z",
  "id":612451163,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQ1MTE2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T15:49:21Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"What is RTTI? I don't recognize this \"missing boxer for subtype...\" error.",
  "created_at":"2020-04-11T15:56:48Z",
  "id":612452485,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQ1MjQ4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T15:56:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \r\nhttps://en.wikipedia.org/wiki/Run-time_type_information\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/7bf1434f28fdb669a0e1cc4fbd1c11aa840458d3/src/python/layout/content.cpp#L122",
  "created_at":"2020-04-11T15:58:28Z",
  "id":612452745,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQ1Mjc0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T15:58:28Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for finding it\u2014I didn't recognize my own error message. (I was searching the web for it!)\r\n\r\nSo it looks like `dynamic_cast` will always fail if the compiler has RTTI disabled, and that's probably what's happening here. I'm looking into ways of ensuring that RTTI is always enabled in CMake. In my continuous integration, it's apparently enabled in all three systems: Linux, MacOS, and Windows.\r\n\r\nWould you be able to test from a GitHub branch? It would be time-consuming to have to push PyPI versions just to find out if a compiler option is on. (Thanks!)\r\n",
  "created_at":"2020-04-11T17:30:25Z",
  "id":612472029,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQ3MjAyOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-11T17:30:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"All of the requirements that are used by setup.py are in the tarball (not the wheels, but I don't think it needs to be there). I could never convince setuptools to include requirements-dev.txt, so I inlined them in setup.py.",
  "created_at":"2020-04-11T13:26:13Z",
  "id":612420942,
  "issue":208,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQyMDk0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T13:26:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"ffae43aa45d99e32d9242f21544f10cd8529b282 fails: https://gist.github.com/989007ff77cd929ed3aaf1e985374c8b\r\n\r\nOn my side I've reverted to llvmlite 0.30.0 and numba 0.46.0 and doing a bisect of awkward between 0.1.38 and master. I'm getting some intermediate revisions at which tests fail to collect because of:\r\n```\r\nModuleNotFoundError: No module named 'pybind11_tests'\r\n```\r\nI do \"git submodules update --recursive\", but that doesn't help. Are you familiar with this issue?",
  "created_at":"2020-04-11T19:32:27Z",
  "id":612497524,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQ5NzUyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T19:35:37Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I've been working continuously on this. I think that the RTTI issue are actually related to the visibility warnings I've been trying to get rid of on MacOS because those visibility warnings are all related to `typeinfo`.\r\n\r\nSince pybind11 requires \"hidden\" visibility, all of the projects that I build have \"hidden\" visibility: libawkward-cpu-kernels, libawkward, and the Python extension modules. Symbols that actually do need to be visible are explicitly labeled with `__attribute__((visibility(\"default\")))` (through a macro). It looks like Clang also has `__attribute__((type_visibility(\"default\")))`, and I'm trying to figure out how to use that.\r\n\r\nSince debugging through CI is extremely painstaking, I've installed Clang (for Linux) and I'm trying to use that. (Sorry it's taking a while.)\r\n\r\nAs for\r\n\r\n```\r\nModuleNotFoundError: No module named 'pybind11_tests'\r\n```\r\n\r\nNo, I'm not familiar with that error. We should not be running pybind11_tests\u2014I don't know why it's trying to on your system.",
  "created_at":"2020-04-11T19:43:52Z",
  "id":612499573,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjQ5OTU3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T19:43:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"a390222a03435ec073bbb1541123d4de215e9709 fails: https://gist.github.com/b4b14ccaeeacafb57c884c282d29574d",
  "created_at":"2020-04-11T20:30:14Z",
  "id":612507574,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUwNzU3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T20:30:14Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"a49f5b90f0fdd25c1fa8689cc41f3c685c3d2179 fails: https://gist.github.com/veprbl/1aa32f4a436aebf35e1e3d30362c1559\r\n1b99eba1ff3e2e48c2c9f8ab4608240c4178f5da fails: https://gist.github.com/99576ea7fc34e2c7cdd1758a408e4efe",
  "created_at":"2020-04-11T20:40:04Z",
  "id":612509260,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUwOTI2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T20:40:04Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I've given up trying to run Clang on my computer and I'm just debugging through continuous integration. All of the tests pass, but the MacOS tests have a lot of warnings about not being able to link `typeinfo` symbols. As I said above, I highly suspect that's relevant.",
  "created_at":"2020-04-11T20:43:43Z",
  "id":612509911,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUwOTkxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T20:43:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"a502209cc4ca17bf8067f5a81aaed71bec682447 fails: https://gist.github.com/ce9ffea8cb4d8a34aa539605a41a729c",
  "created_at":"2020-04-11T20:47:18Z",
  "id":612510473,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUxMDQ3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T20:47:18Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski I think the clang idea is not a bad one. I rebuilt python38 (not python37 like I did macOS) and awkward's dependencies with clang on linux and managed to produce a single failure: https://gist.github.com/dd9f4889bede80de6f65231e407affa3 I don't think it's the relevant one, though.",
  "created_at":"2020-04-11T21:00:31Z",
  "id":612512647,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUxMjY0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:00:31Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"23600ea3e666c2eb8fb5b49f9ec43e91b0680ad9 fails: https://gist.github.com/20dc37fb15b4739403bfa29987739bb5",
  "created_at":"2020-04-11T21:04:15Z",
  "id":612513265,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUxMzI2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:04:15Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right, that error isn't relevant. However, I wasn't able to get clang to link on Linux\u2014there are so many `undefined reference` errors that I can't scroll through them. I just don't know what needs to be done to get clang to work in place of gcc.",
  "created_at":"2020-04-11T21:04:50Z",
  "id":612513367,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUxMzM2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:05:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I had been selecting clang by setting the environment variable `CXX` to its full path. Now I'm trying to set `CMAKE_CXX_COMPILER` to clang's full path. I'm thinking that maybe setting only `CXX` might have swapped the compiler without also swapping the linker.\r\n\r\nWell, there's a zillion linker errors again.",
  "created_at":"2020-04-11T21:09:01Z",
  "id":612514013,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUxNDAxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:09:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Am I right in thinking that I shouldn't use `ld` when compiling with clang? Because on my testing laptop, it compiles with clang (no problems) and links with `ld` with many errors.",
  "created_at":"2020-04-11T21:12:06Z",
  "id":612514510,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUxNDUxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:12:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Could you show me your errors?",
  "created_at":"2020-04-11T21:14:10Z",
  "id":612514830,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUxNDgzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:14:10Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"The end of the errors is\r\n\r\n```\r\n/home/pivarski/miniconda3/bin/ld: operations.cpp:(.text.startup+0x14): undefined reference to `std::ios_base::Init::~Init()'\r\n/home/pivarski/miniconda3/bin/ld: libawkward-cpu-kernels-static.a(reducers.cpp.o): in function `_GLOBAL__sub_I_reducers.cpp':\r\nreducers.cpp:(.text.startup+0xd): undefined reference to `std::ios_base::Init::Init()'\r\n/home/pivarski/miniconda3/bin/ld: reducers.cpp:(.text.startup+0x14): undefined reference to `std::ios_base::Init::~Init()'\r\n/home/pivarski/miniconda3/bin/ld: libawkward-cpu-kernels-static.a(util.cpp.o): in function `_GLOBAL__sub_I_util.cpp':\r\nutil.cpp:(.text.startup+0xd): undefined reference to `std::ios_base::Init::Init()'\r\n/home/pivarski/miniconda3/bin/ld: util.cpp:(.text.startup+0x14): undefined reference to `std::ios_base::Init::~Init()'\r\nclang-10: error: linker command failed with exit code 1 (use -v to see invocation)\r\nCMakeFiles/test0030.dir/build.make:85: recipe for target 'test0030' failed\r\nmake[2]: *** [test0030] Error 1\r\nCMakeFiles/Makefile2:120: recipe for target 'CMakeFiles/test0030.dir/all' failed\r\nmake[1]: *** [CMakeFiles/test0030.dir/all] Error 2\r\nMakefile:140: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```\r\n\r\nbut the beginning is beyond my (giant) scroll buffer.",
  "created_at":"2020-04-11T21:14:47Z",
  "id":612514942,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUxNDk0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:14:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think it's complaining about every symbol in the project.",
  "created_at":"2020-04-11T21:15:08Z",
  "id":612514991,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUxNDk5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:15:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The result of my bisect:\r\ne9f6ba0ca393ee37ed1b5dabd85ab0e2df8e341c is the first bad commit\r\nI had to do \"rm -rf pybind11/tests\" to fix the collection error.\r\nThis time I did not bother updating submodules:\r\n```\r\n# git submodule status\r\n e43e1cc01ae6d4e4e5ba10557a057d7f3d5ece0d pybind11 (v2.3.0)\r\n f54b0e47a08782a6131cc3d60f94d038fa6e0a51 rapidjson (v1.1.0)\r\n```",
  "created_at":"2020-04-11T21:52:53Z",
  "id":612520944,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUyMDk0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:52:53Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"> I had to do \"rm -rf pybind11/tests\" to fix the collection error.\r\n\r\nI realize now why you were seeing the pybind11_tests error: I've always run pytest with an explicit target:\r\n\r\n```bash\r\npytest -vv tests\r\n```",
  "created_at":"2020-04-11T21:56:09Z",
  "id":612521411,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUyMTQxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:56:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> The result of my bisect:\r\n> [e9f6ba0](https://github.com/scikit-hep/awkward-1.0/commit/e9f6ba0ca393ee37ed1b5dabd85ab0e2df8e341c) is the first bad commit\r\n\r\nThat's a very interesting result, because it did change the build procedure. A monolithic Python extension module was broken into three extension modules. But that's unrelated to the warnings that I have been trying to get rid of all this time: the warnings come before any Python modules are compiled.\r\n\r\nSo now I have to think about how I could possibly test this. That refactoring was a big change, made long ago.",
  "created_at":"2020-04-11T21:58:28Z",
  "id":612521762,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUyMTc2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T21:58:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> So now I have to think about how I could possibly test this. That refactoring was a big change, made long ago.\r\n\r\nAnd somehow e9f6ba0ca393ee37ed1b5dabd85ab0e2df8e341c doesn't have any of those warnings we were chasing after:\r\nhttps://gist.github.com/4cddda4da02e6e7e14ca04e27c11f46d",
  "created_at":"2020-04-11T22:02:37Z",
  "id":612522436,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUyMjQzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T22:02:37Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"@veprbl This last commit probably doesn't fix the linker MacOS warnings, but it might fix the error that you see.\r\n\r\nIt could be that `typeinfo` was lost when an object created in one of the 3 extension modules was accessed in another, since these were distinct .dylib files and we're having trouble with `typeinfo` symbols anyway. With only 1 extension module, it's definitely the same one and it should see its own symbols.\r\n\r\nLet me know how it fares. Thanks!",
  "created_at":"2020-04-11T23:16:11Z",
  "id":612533005,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjUzMzAwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-11T23:16:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"ce9c2fb2bea2489eb05048c2630070d52c70b09c passes: https://gist.github.com/80118ce327992a5615f6ff9e3b2822e0",
  "created_at":"2020-04-12T16:37:52Z",
  "id":612642765,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY0Mjc2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-12T16:37:52Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"@veprbl Sorry about the noise on #210: this was a good chance to get CI working for PRs from forks.\r\n\r\nOnce it ran, I was getting errors from both MacOS and Windows ([here](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=1816&view=results)). Your changes managed to get MacOS on Azure to see the RTTI error\u2014it might have been an accident that prevented it from materializing, something like a race in which symbols from the \"layout\" extension module overrode symbols from the \"types\" extension module in one case and vice-versa in the other. If we were that close to a bug, then it's too brittle to allow.\r\n\r\nFor Windows, I learned rather early on that you can't link a shared library to a shared library: the dependency must be a static library. I don't remember the details, but that was the conclusion I came to about 6 months ago. Now (in #210) they're failing with segfaults. I'd rather not revisit that.\r\n\r\nThere is one combination that works for everybody: the current state of #209. I have un-refactored the 3 extension modules into 1 extension module, and I think I can live with that. The refactoring didn't live up to its goals: [linker time wasn't improved](https://github.com/scikit-hep/awkward-1.0/pull/116) and the difficulty compiling a dependent C++ project was not related to how many Python extension modules the project was split into. (In fact, it might have suffered from the same kind of brittleness as you encountered, but I haven't been testing it on MacOS.)\r\n\r\nAlso, I really wanted to split into a lot of extension modules, but it could only be 3 because of various C++ constraints. Now I the `ak.layout`, `ak.types`, and `ak._io` modules are actually Python modules that pass objects from `ak._ext`. I could break them down into smaller, more logical pieces, now that the C++ constraints are not limiting the problem. The only ugly thing is that the names of these classes include `_ext` in their names, but maybe that's a good reminder to know which are pure Python and which are not.\r\n\r\nSo I'm favoring this solution (#209 over #210), since it seems to solve all the constraints, but I should check that \"dependent project\" again, first.",
  "created_at":"2020-04-12T18:24:04Z",
  "id":612656643,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY1NjY0Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-12T18:24:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@veprbl The dependent project had some minor bugs as a result of not being in continuous integration (for other reasons: I can't find the installed paths on the Azure box), but otherwise it seemed to work flawlessly. I'm happy with restoring the monolithic Python extension.\r\n\r\nMeanwhile, what were the things you did with `RPATH`?\r\n\r\n```cmake\r\n  if(APPLE)\r\n    set_target_properties(layout PROPERTIES BUILD_WITH_INSTALL_RPATH TRUE INSTALL_RPATH \"@loader_path\")\r\n  else()\r\n    set_target_properties(layout PROPERTIES BUILD_RPATH_USE_ORIGIN TRUE)\r\n  endif()\r\n```\r\n\r\nAs I understand it, `RPATH` is the Darwin equivalent of Linux's `LD_LIBRARY_PATH`. Would the above be a good idea to apply independently of whether I build a monolithic Python extension module or not? Because if it is, I'll try adding it to this PR before merging and deploying a new version.",
  "created_at":"2020-04-12T18:42:08Z",
  "id":612658904,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY1ODkwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-12T18:42:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"\r\n> As I understand it, `RPATH` is the Darwin equivalent of Linux's `LD_LIBRARY_PATH`.\r\n\r\nNo, this is not correct. The `DYLD_LIBRARY_PATH` is Darwin's equivalent of the `LD_LIBRARY_PATH`.\r\n\r\n> Would the above be a good idea to apply independently of whether I build a monolithic Python extension module or not? Because if it is, I'll try adding it to this PR before merging and deploying a new version.\r\n\r\nRPATH can be set for executables or libraries that dynamically link to other libraries. If you have such cases, you may need it.\r\n\r\n> Meanwhile, what were the things you did with `RPATH`?\r\n> \r\n> ```cmake\r\n>   if(APPLE)\r\n>     set_target_properties(layout PROPERTIES BUILD_WITH_INSTALL_RPATH TRUE INSTALL_RPATH \"@loader_path\")\r\n>   else()\r\n>     set_target_properties(layout PROPERTIES BUILD_RPATH_USE_ORIGIN TRUE)\r\n>   endif()\r\n> ```\r\n\r\nI don't have a reference for this, this is based mostly on my (possibly incomplete) understanding of the RPATH handling in CMake. Some info can be found on CMake wiki [1].\r\n\r\nThe idea is the following: Because you are moving libraries to their final destinations in setup.py using setuptools, the CMake has no idea about the final destinations for the libraries, so setting a specific absolute RPATH is not an option. The only choice left would be to use a relative RPATH, which would make libraries \"relocatable\". But one thing about RPATH is that you don't just set them to something like \"./../foo/lib\", but instead have to specify it using $ORIGIN like \"$ORIGIN/../foo/lib\". Presumably, all that's needed for this to work in CMake is to set `BUILD_RPATH_USE_ORIGIN` to `TRUE`. And this would work on Linux and some BSD's, but on macOS things are a bit more sophisticated, it has multiple different keywords that it understands: `@loader_path`, `@executable_path` and `@rpath`, so this, I guess, doesn't fit into the standard $ORIGIN definition, so I manually set `INSTALL_RPATH` to `\"@loader_path\"`. Finally, CMake has this quite unique feature where RPATH is not just set with linker flags, but can also be updated with new RPATH (e.g. see [2]) when the binaries are installed (copied from the build directory to their prefixes in the `CMAKE_INSTALL_PATH`). Since we are aiming for relocatable binaries, I just let it make linker do it since the beginning by setting `BUILD_WITH_INSTALL_RPATH` to `TRUE`.\r\n\r\n[1] https://gitlab.kitware.com/cmake/community/-/wikis/doc/cmake/RPATH-handling\r\n[2] https://github.com/NixOS/patchelf : AFAIK patchelf is not used by CMake, but, I believe, it well illustrates what is being done",
  "created_at":"2020-04-13T06:38:26Z",
  "id":612774090,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjc3NDA5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T06:38:26Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"> RPATH can be set for executables or libraries that dynamically link to other libraries. If you have such cases, you may need it.\r\n\r\nThanks for the explanation! This sounds important for all of the libraries that would need to be linked against, including libawkward-cpu-kernels and libawkward, so I've applied this technique to each. The number of properties applied to every library is growing, so I may need to wrap them up in a CMake function someday.\r\n\r\nAlso, thanks for the thorough review. I appreciate it!",
  "created_at":"2020-04-13T11:42:56Z",
  "id":612866015,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjg2NjAxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T11:42:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Note: all of the MacOS warnings are about `typeinfo` in these three classes:\r\n\r\n```\r\ntypeinfo for awkward::IndexedArrayOf<long long, true>\r\ntypeinfo for awkward::ListOffsetArrayOf<long long>\r\ntypeinfo name for awkward::ListOffsetArrayOf<long long>\r\n```",
  "created_at":"2020-04-13T12:55:15Z",
  "id":612886392,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjg4NjM5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T12:55:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The One Weird Trick was based on https://stackoverflow.com/questions/307352/g-undefined-reference-to-typeinfo and eternally springing hope.\r\n\r\nMaybe it's relevant that it's only three classes that have `typeinfo` link errors, and the warnings are new.",
  "created_at":"2020-04-13T13:45:14Z",
  "id":612904648,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjkwNDY0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T13:45:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Bisecting through Azure's history, the MacOS `typeinfo` warnings were first seen in 79cc9a75b464b54ad04cbde505c967cc74699b74, which is a very useful hint. This is when methods that return one specific `Content` subclass (`toListOffsetArray64` and `toIndexedOptionArray64`) were given this as a return type, rather than the more generic `ContentPtr`. Apparently, that's a problem for Cling or MacOS. I will revert that change, since it wasn't really needed and the `dynamic_casts` that it would have made unnecessary haven't been removed yet, anyway.",
  "created_at":"2020-04-13T14:08:43Z",
  "id":612913737,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjkxMzczNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T14:08:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is probably going to do it. Even though the virtual destructors were irrelevant for these `typeinfo` warnings, I'm going to leave them in the headers because they're trivial (no code) and the immediate information they give (the fact that they're trivial) is useful at the class-definition level, so they can be an exception to the rule that all implementations go in .cc files.\r\n\r\nIn case I don't mention it before closing it, this PR dealt with two issues that at first seemed related, but in the end were not:\r\n\r\n   * `typeinfo` mismatch in different Python extension modules, solved by ce9c2fb2bea2489eb05048c2630070d52c70b09c, whcih combines Python extension modules into a single module. This problem could be relevant again for dependent projects that produce Awkward Arrays; if so, follow that up in issue #211.\r\n   * Linker warnings about the visibility of `typeinfo` symbols. This issue was first raised by @ianna in an email, and I've been wanting to fix it because I don't know if there could be downstream implications. (That's why I immediately thought there might be a connection between these two issues.)\r\n\r\nBoth of these affected only MacOS (or just Clang?) and the first issue was intermittent (not failing in Azure until #210).\r\n\r\nThe first CI run with e2ddce3066ea05f935006a5d44ce31c2feb3af12 is complete; it fixed some but not all of the warnings. Definitely the right track.",
  "created_at":"2020-04-13T14:38:27Z",
  "id":612926058,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjkyNjA1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T14:38:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Now the remaining warnings are:\r\n\r\n```\r\nCompiling libawkward:\r\n\r\nawkward::IndexedArrayOf<int, false>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, false>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, false>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, true>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, true>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, true>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, false>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, false>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, false>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, true>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, true>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, true>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<unsigned int, false>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<unsigned int, false>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<unsigned int, false>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\n\r\nCompiling the Python module:\r\n\r\nawkward::IndexedArrayOf<int, false>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, false>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, false>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, true>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, true>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<int, true>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, false>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, false>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, false>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, true>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, true>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<long long, true>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<unsigned int, false>::mergeable(std::__1::shared_ptr<awkward::Content> const&, bool) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<unsigned int, false>::merge(std::__1::shared_ptr<awkward::Content> const&) const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\nawkward::IndexedArrayOf<unsigned int, false>::simplify_optiontype() const\r\n    wants typeinfo for awkward::IndexedArrayOf<long long, true>\r\n```",
  "created_at":"2020-04-13T14:39:02Z",
  "id":612926340,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjkyNjM0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T14:39:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Or more simply:\r\n\r\n```\r\nCompiling libawkward:\r\n\r\nIndexedArray::mergeable(shared_ptr<Content> const&, bool) const\r\n    wants typeinfo for IndexedArray\r\nIndexedArray::merge(shared_ptr<Content> const&) const\r\n    wants typeinfo for IndexedArray\r\nIndexedArray::simplify_optiontype() const\r\n    wants typeinfo for IndexedArray\r\n\r\nCompiling the Python module:\r\n\r\nIndexedArray::mergeable(shared_ptr<Content> const&, bool) const\r\n    wants typeinfo for IndexedArray\r\nIndexedArray::merge(shared_ptr<Content> const&) const\r\n    wants typeinfo for IndexedArray\r\nIndexedArray::simplify_optiontype() const\r\n    wants typeinfo for IndexedArray\r\n```",
  "created_at":"2020-04-13T14:44:47Z",
  "id":612928875,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjkyODg3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T14:44:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm not going to be able to solve these last few warnings; maybe I'll get back to them someday.\r\n\r\nOnce these tests pass, I'm merging the PR.",
  "created_at":"2020-04-13T16:32:46Z",
  "id":612976534,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjk3NjUzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T16:32:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry that the tests didn't start automatically. Apparently, my Azure Pipelines aren't set up to run on PRs from forks. I'm going to look that up.\r\n\r\nIt's running now because I made a branch called [copy/veprbl/pr/test_shared](https://github.com/scikit-hep/awkward-1.0/tree/copy/veprbl/pr/test_shared).",
  "created_at":"2020-04-12T17:11:43Z",
  "id":612647228,
  "issue":210,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY0NzIyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-12T17:12:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"/AzurePipelines help",
  "created_at":"2020-04-12T17:37:50Z",
  "id":612650690,
  "issue":210,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY1MDY5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-12T17:37:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"<samp>\n<b>Supported commands</b><br>\r\n<ul type=\"none\"><li><b>help:</b></li><ul type=\"none\"><li>Get descriptions, examples and documentation about supported commands</li><li><b>Example: </b>help \"command_name\"</li></ul><li><b>list:</b></li><ul type=\"none\"><li>List all pipelines for this repository using a comment.</li><li><b>Example: </b>\"list\"</li></ul><li><b>run:</b></li><ul type=\"none\"><li>Run all pipelines or specific pipelines for this repository using a comment. Use this command by itself to trigger all related pipelines, or specify specific pipelines to run.</li><li><b>Example: </b>\"run\" or \"run pipeline_name, pipeline_name, pipeline_name\"</li></ul><li><b>where:</b></li><ul type=\"none\"><li>Report back the Azure DevOps orgs that are related to this repository and org</li><li><b>Example: </b>\"where\"</li></ul></ul><br>\r\nSee <a href=\"https://go.microsoft.com/fwlink/?linkid=2056279\">additional documentation.</a>\n</samp>",
  "created_at":"2020-04-12T17:37:54Z",
  "id":612650701,
  "issue":210,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY1MDcwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-12T17:37:54Z",
  "user":"MDM6Qm90MzY3NzE0MDE="
 },
 {
  "author_association":"MEMBER",
  "body":"/AzurePipelines run buildtest-awkward",
  "created_at":"2020-04-12T17:39:19Z",
  "id":612650879,
  "issue":210,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY1MDg3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-12T17:39:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"<samp>\nAzure Pipelines could not run because the pipeline triggers exclude this branch/path.<br>\r\n\n</samp>",
  "created_at":"2020-04-12T17:39:25Z",
  "id":612650894,
  "issue":210,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY1MDg5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-12T17:39:25Z",
  "user":"MDM6Qm90MzY3NzE0MDE="
 },
 {
  "author_association":"MEMBER",
  "body":"/AzurePipelines run buildtest-awkward",
  "created_at":"2020-04-12T17:57:22Z",
  "id":612653307,
  "issue":210,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY1MzMwNw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-12T17:57:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"<samp>\nAzure Pipelines successfully started running 1 pipeline(s).<br>\r\n\n</samp>",
  "created_at":"2020-04-12T17:57:29Z",
  "id":612653319,
  "issue":210,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjY1MzMxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-12T17:57:29Z",
  "user":"MDM6Qm90MzY3NzE0MDE="
 },
 {
  "author_association":"MEMBER",
  "body":"Wow\u2014this is incredibly detailed! Thanks!\r\n\r\nIn the pybind11 issue that you linked, there was a suggestion to set dlopen flags to `os.RTLD_NOW | os.RTLD_GLOBAL` instead of `os.RTLD_NOW` before loading the extension module. Does this work as a work-around?\r\n\r\nIn #209, I'm combining all of Awkward's extension modules into one, but this problem would reemerge for dependent projects written in C++ with Python interfaces on MacOS. Based on your description of the above, I think this would be a problem only when dependent project _produce_ arrays (though that's probably the more important use-case for dependent projects).\r\n\r\nTo answer your question,\r\n\r\n> That we then attempt to `box` (why do even need the `box`, didn't we just successfully return `ak::Content` from the `awkward1._io.fromjson`?)\r\n\r\nThe motivation for `box` is that some outputs are not `Content` subclasses on the Python side. A zero-dimensional NumpyArray should become a Python number and `ak::None` should become Python None. I don't think JSON \u2192 Awkward for only a number or only JSON `null` are supported; if so, those cases would require a `box`.\r\n\r\nOn the other hand, maybe stronger boxing can protect the arrays across boundaries... In our design, these array nodes be immutable (#176 and #177) and are inexpensive to copy (compared to the array buffers). If this is a problem in the future, I could add a protocol to replace the `ak::Content` subclasses with generic objects (_not_ copying the buffers, but everything else), pass them over the boundary, and reconstruct `ak::Content` subclasses on the other side. Then each library\u2014Awkward and its dependent projects\u2014would each have its own private symbols, though each dependent project would have to know about this stronger boxing/unboxing protocol.",
  "created_at":"2020-04-13T13:36:17Z",
  "id":612901113,
  "issue":211,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjkwMTExMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T13:36:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Cross-linking some discussion from the ITK library dealing with this issue a few years ago (it looks like you've already gone down much the same path in 209, but could help w/ the separate module issue here):\r\n\r\n- https://insightsoftwareconsortium.atlassian.net/browse/ITK-3490 (esp the linked article:  http://www.russellmcc.com/posts/2013-08-03-rtti.html)\r\n- https://review.source.kitware.com/%23/c/21805/7/\r\n- https://review.source.kitware.com/%23/c/21755/6/",
  "created_at":"2020-04-13T15:48:30Z",
  "id":612956933,
  "issue":211,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMjk1NjkzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-13T15:48:30Z",
  "user":"MDQ6VXNlcjMyNzcwNg=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> In the pybind11 issue that you linked, there was a suggestion to set dlopen flags to `os.RTLD_NOW | os.RTLD_GLOBAL` instead of `os.RTLD_NOW` before loading the extension module. Does this work as a work-around?\r\n\r\nI did not check, but I believe it would work. Enabling `RTLD_GLOBAL` is bound to cause some obscure platform-dependent bugs in the future, so I don't think we need to investigate this.",
  "created_at":"2020-04-13T18:59:47Z",
  "id":613042832,
  "issue":211,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxMzA0MjgzMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-13T18:59:47Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know how much of this is still valid, but since this looks most useful as a search result, I'm going to close it.",
  "created_at":"2020-10-30T22:05:49Z",
  "id":719821035,
  "issue":211,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgyMTAzNQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-10-30T22:05:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nope.",
  "created_at":"2020-04-16T13:44:13Z",
  "id":614661782,
  "issue":212,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNDY2MTc4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-16T13:44:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We haven't needed it yet. More of these things happen bottom-up thorough nesting structures than top-down through schemas, anyway. In fact, some of the \"wants\" described above have been filled in different ways. So I'm closing this as obsolete.",
  "created_at":"2020-12-11T22:41:16Z",
  "id":743464632,
  "issue":213,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ2NDYzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T22:41:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for looking into this!\r\n\r\nYour solution does appear to be correct: I think all the array nodes involved in `ak.combinations` are RegularArray, ListArray, and ListOffsetArray. It is a more \"local\" change than the one I had been considering in https://github.com/scikit-hep/awkward-1.0/issues/204#issuecomment-611089765, which would have made every operation that rearranges a RecordArray insert an IndexedArray.\r\n\r\nOne thing, though, is that immediately after creating an IndexedArray from some operation, we must call `simplify_optiontype` on it. So instead of\r\n\r\n```c++\r\ncontents.push_back(std::make_shared<IndexedArray64>(\r\n        Identities::none(),\r\n        util::Parameters(),\r\n        Index64(ptr, 0, totallen),\r\n        content_));\r\n```\r\n\r\nwe want\r\n\r\n```c++\r\nstd::shared_ptr<IndexedArray64> indexedarray =\r\n    std::make_shared<IndexedArray64>(\r\n        Identities::none(),\r\n        util::Parameters(),\r\n        Index64(ptr, 0, totallen),\r\n        content_));\r\ncontents.push_back(indexedarray.get()->simplify_optiontype();\r\n```\r\n\r\nThis prevents us from making IndexedArray of IndexedArray, which can get out of hand as the user composes operations.\r\n\r\nAlso, since it's an intended performance improvement, we can't take it without doing a performance test. It's intended to speed up calculations such as `In[155]` of [this notebook](https://github.com/jpivarski/2020-04-08-eic-jlab/blob/master/2020-04-08-eic-jlab-EVALUATED.ipynb), which combines all pairs of photons to look for those with the right mass for\r\n\r\n<img src=\"https://render.githubusercontent.com/render/math?math=\\pi^0 \\to \\gamma \\gamma\">\r\n\r\ndecays. It was computationally expensive because it builds copies of a lot of particle records with a lot of fields, noticeable as a slight delay on my laptop and straining the ability of whatever free service Binder runs on.\r\n\r\nEither you or I could do such a performance test, but if it's me, it will take a while to get to it. To make the computer struggle more (a better performance test, as it tests further out on the scaling curve), we could remove the requirement that the particles be photons: the `events.prt.pdg == Particle.from_string(\"gamma\").pdgid` filter. The subsequent steps (computing the mass and filtering based on them) is still interesting, but as a part that should be timed separately.\r\n\r\nThe reason is that this is a performance _tradeoff_, rather than an absolute improvement. With `carray` instead of IndexedArray, data are copied into the combinations record. With IndexedArray instead of `carry`, a new level of indirection is placed in the combinations record, and subsequent operations must go through this layer of indirection forevermore. If the \"`content_`\" (above) is already something lightweight, such as NumpyArray, then it's better to `carry`, but if it's something heavyweight, like a RecordArray with lots of fields, then it's better to use IndexedArray **if** subsequent operations won't be using all of those fields. Instead of copying all the fields once, we're forevermore going through an extra layer of indirection on the fields we do use.\r\n\r\nThat's why it would be interesting to see how the speed of (1) the `ak.combinations` and (2) the subsequent mass calculation compare (a) with `carry` and (b) with IndexedArray. That would show us what this tradeoff costs in a typical case.\r\n\r\nIf the results of such a test favors IndexedArray, then we still might only do it if the `content_` is not a NumpyArray.",
  "created_at":"2020-04-17T21:56:26Z",
  "id":615481792,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTQ4MTc5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T21:56:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I will attempt to write the performance tests. \r\nWhat kind of testing should we do ? A couple of simple tests measuring time using `std::chrono` for a couple of different cases as mentioned above or use a tool like perf/Callgrind/GoogleBenchmark ?\r\nThis is the first issue tagged with the performance tag so I don't know what's expected.\r\n\r\nI am a bit overwhelmed studying the internals of this package so I will take some time to complete it.",
  "created_at":"2020-04-17T22:26:32Z",
  "id":615491098,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTQ5MTA5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T22:26:32Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"> I am a bit overwhelmed studying the internals of this package so I will take some time to complete it.\r\n\r\nI'm pleasantly surprised that you took it on.  `:)`\r\n\r\nThe test would be at the level of Python, since the data are prepared in Python (cell 155 of the notebook I referenced above). To do so, you don't need to learn any more internals of Awkward, just to find the relevant cells for extracting and preparing the data in the form used in cell 155.",
  "created_at":"2020-04-17T22:34:55Z",
  "id":615493349,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTQ5MzM0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T22:34:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"To evaluate the tradeoff of the original and the new implementation I did two benchmarks. The first finds the combinations of photons for the right mass as suggested above (Cell 155, [here](https://github.com/jpivarski/2020-04-08-eic-jlab/blob/master/2020-04-08-eic-jlab-EVALUATED.ipynb)). This is an organic example and shows the benefit so easily that I felt no need to make the test harder. \r\n\r\n```python\r\nevts = events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid][:length]\r\n\r\n%%timeit        #TEST 1\r\npizero_candidates = ak.combinations(evts, 2, with_name=\"pair\")\r\n\r\n%%timeit       #TEST 2\r\npizero_candidates = ak.combinations(evts, 2, with_name=\"pair\")\r\npizero = pizero_candidates[pizero_candidates.mass(0, 0) - 0.13498 < 0.000001]\r\npizero[\"px\"] = pizero.slot0.px + pizero.slot1.px\r\npizero[\"py\"] = pizero.slot0.py + pizero.slot1.py\r\npizero[\"pz\"] = pizero.slot0.pz + pizero.slot1.pz\r\npizero[\"p\"] = np.sqrt(pizero.px**2 + pizero.py**2 + pizero.pz**2)\r\n```\r\n|Test         |         10               |100                |1000                  |10000                   |\r\n|------------|----------------------|------------------|---------------------|-----------------------|\r\n|TEST 1: Original|288 \u00b5s \u00b1 520 ns |2.93 ms \u00b1 48.6 \u00b5s|40.7 ms \u00b1 101 \u00b5s|409 ms \u00b1 3.48 ms|\r\n|TEST 1:Indexed|90.7 \u00b5s \u00b1 122 ns |191 \u00b5s \u00b1 273 ns|1.24 ms \u00b1 1.15 \u00b5s|12.7 ms \u00b1 36.7 \u00b5s|\r\n|TEST 1:Indexed + Simplify|93.2 \u00b5s \u00b1 275 ns|195 \u00b5s \u00b1 322 ns|1.25 ms \u00b1 8.17 \u00b5s|12.9 ms \u00b1 8.39 \u00b5s|\r\n|TEST 2: Original|11.6 ms \u00b1 57.9 \u00b5s|20.5 ms \u00b1 101 \u00b5s|112 ms \u00b1 1.24 ms | 1.29 s \u00b1 1.47 ms|\r\n|TEST 2: Indexed|26.6 ms \u00b1 41.8 \u00b5s |30.6 ms \u00b1 66.8 \u00b5s|69.9 ms \u00b1 606 \u00b5s|537 ms \u00b1 6.35 ms|\r\n|TEST 2: Indexed + Simplify|26.8 ms \u00b1 234 \u00b5s|31.1 ms \u00b1 368 \u00b5s|73.5 ms \u00b1 979 \u00b5s|538 ms \u00b1 6.72 ms \r\n\r\nThe reason I have also measured the timing difference with and without `simplify_optiontype()` is that while playing around with all these versions I think I was able to create a testcase where there was a significant difference between the two versions(2x ish). I am no longer able to create such a testcase so most probably I just confused myself doing something incorrect. If you think of something that can trigger such a time difference please let me know. If there are such cases we can also consider applying `simplify_optiontype()` only when RTTI check shows `content_` is an `IndexedArray`.\r\n\r\nComing back to the benchmark. In the 2<sup>nd</sup> test the advantage of `carry` is demonstrated. \r\n```python\r\narr = ak.Array([[_ for _ in range(length)]])\r\n\r\n%%timeit         #TEST 3\r\nx = ak.combinations(arr, 2)\r\n\r\n%%timeit         #TEST 4\r\nx = ak.combinations(arr, 2)\r\nak.sum(x)\r\n```\r\n|Test         |         10               |100                |1000                  |10000                   |\r\n|------------|----------------------|------------------|---------------------|-----------------------|\r\n|TEST 3: Original|20.4 \u00b5s \u00b1 45.3 ns|56.1 \u00b5s \u00b1 308 ns|4.05 ms \u00b1 40.8 \u00b5s|624 ms \u00b1 2.71 ms\r\n|TEST 3: Indexed|65.8 \u00b5s \u00b1 372 ns|97.7 \u00b5s \u00b1 114 ns|3.23 ms \u00b1 28.9 \u00b5s|409 ms \u00b1 2.22 ms\r\n|TEST 3: Indexed + Simplify| 70.5 \u00b5s \u00b1 181 ns|104 \u00b5s \u00b1 688 ns|3.84 ms \u00b1 27 \u00b5s|416 ms \u00b1 2.1 ms \r\n|TEST 4: Original|62.1 \u00b5s \u00b1 66.5 ns|104 \u00b5s \u00b1 110 ns|4.73 ms \u00b1 18.3 \u00b5s|679 ms \u00b1 2.04 ms\r\n|TEST 4: Indexed|197 \u00b5s \u00b1 226 ns|289 \u00b5s \u00b1 912 ns|9.26 ms \u00b1 7 \u00b5s|1.35 s \u00b1 895 \u00b5s\r\n|TEST 4: Indexed + Simplify| 204 \u00b5s \u00b1 706 ns|299 \u00b5s \u00b1 3.28 \u00b5s|11.3 ms \u00b1 194 \u00b5s|1.37 s \u00b1 18.5 ms\r\n\r\nI have a suggestion.\r\nIn both (original, new) implementations we are making assumptions about how the user will use `combinations` and what arguments will be passed to it potentially causing some problems. Instead of that how about passing this choice as an argument to the combinations function. For a `RecordArray` the default choice will be the new method and for `ListArray` let the default choice be the carry method. When combinations is called on a `RecordArray` the new choice is passed as a parameter to the `combinations(content_)`.\r\n\r\n\r\n\r\n\r\n",
  "created_at":"2020-04-20T00:38:25Z",
  "id":616253313,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjI1MzMxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T01:04:40Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! This is helpful. Replacing `carry` with IndexedArray on a large RecordArray is a clear improvement (409 ms vs 13 ms is 40\u00d7), and the subsequent mass calculation is not negatively affected (1290 - 409 ms vs 537 - 12 ms is 1.6\u00d7, which is essentially the same, though it oddly fluctuated in favor of more indirection).\r\n\r\nWhen it's pure numbers, not a large RecordArray, avoiding `carry` while calculating combinations is hardly noticeable (624 ms vs 416 ms is only 1.5\u00d7), but even a downstream calculation like \"sum\" is made much worse (679 - 624 ms vs 1370 - 416 ms is 17\u00d7 worse).\r\n\r\nSince we don't know how applications will use these functions, we can't optimize anything as small as a factor of 2. What's a factor of 2 better for one application will be a factor of 2 worse for another. But the 40\u00d7 improvement for RecordArrays and the 17\u00d7 worsening for non-RecordArrays are significant enough to affect nearly all applications and we can make decisions based on them.\r\n\r\nIt should not be a user-configurable parameter because that would blow up the complexity too much. (Giving physicists knobs to tune performance will almost surely lead to anti-tuned performance: most will use them incorrectly.) Even doing \"smart\" things like using IndexedArrays when the number of fields is greater than a certain number will likely lead to breaking someone's analysis in surprising ways. Whatever rule this follows, it has to follow it predictably and consistently.\r\n\r\nSince the important thing is whether the arrays are RecordArrays, perhaps instead of changing ak.combinations, as you've done, the best thing is for `RecordArray::carry` to always generate an IndexedArray instead of carrying through to its fields. This is what I was considering here: https://github.com/scikit-hep/awkward-1.0/issues/204#issuecomment-611089765\r\n\r\nThat should have the same performance improvement as you've shown above (it would provide the gain seen in tests 1 and 2 and not affect tests 3 and 4). I'm just not sure if it will crash: until now, I've always assumed that the return value of `carry` has the same type as the input. I don't know what will happen if `RecordArray::carry` can return an `IndexedArray`. (Note: it doesn't have to simplify because such a step cannot create IndexedArray of IndexedArray.)",
  "created_at":"2020-04-20T01:53:15Z",
  "id":616267510,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjI2NzUxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T01:53:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I'm not sure that I understand the last two paragraphs above. Right now when combinations is called on `RecordArrays` ([here](https://github.com/scikit-hep/awkward-1.0/blob/master/src/libawkward/array/RecordArray.cpp#L946)). It calls combinations on all its `content`. Hence, there is no `RecordArray::carry` called in this function.\r\n\r\nWhich shows a flaw in this implementation. Since all the fields have the same structure it's quite wasteful to regenerate the combinations mask every single time (Unless the overhead of computing this is very less which I have not measured).  We should generate this mask in the local scope of `combinations(RecordArray)` and then change all internal Arrays to indexed arrays.\r\n\r\nWe will revert the changes here back to the original `carry` implementation.",
  "created_at":"2020-04-20T22:38:37Z",
  "id":616848387,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjg0ODM4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T22:42:52Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"> I'm not sure that I understand the last two paragraphs above. Right now when combinations is called on `RecordArrays` ([here](https://github.com/scikit-hep/awkward-1.0/blob/master/src/libawkward/array/RecordArray.cpp#L946)). It calls combinations on all its `content`. Hence, there is no `RecordArray::carry` called in this function.\r\n\r\nActually, no, if program flow enters the `RecordArray::combinations` method, then we're not computing combinations on this level. If each field of a record is a simple number, then we're not computing them on that level, either (`depth` counts list depth, not RecordArray or IndexedArray depth) and that would just be a user error (no calculation performed). The only way that program flow could descend into each of those fields in `contents_` and not end up raising an error is if each of the fields contains a list, and then we should compute them separately because each of those lists could have different lengths. We need to compute all the fields separately because they can and usually will produce different results.\r\n\r\nWhat I was actually talking about was replacing RecordArray's `carry` implementation, which eagerly rearranges every field:\r\n\r\nhttps://github.com/Ellipse0934/awkward-1.0/blob/2b243ef8abbb8abf80628633f5d5a4dcaa9437d8/src/libawkward/array/RecordArray.cpp#L439-L454\r\n\r\nwith an implementation that instead just returns an IndexedArray of that RecordArray, lazily rearranging every field. In other words,\r\n\r\n```c++\r\n  const ContentPtr\r\n  RecordArray::carry(const Index64& carry) const {\r\n    IdentitiesPtr identities(nullptr);\r\n    if (identities_.get() != nullptr) {\r\n      identities = identities_.get()->getitem_carry_64(carry);\r\n    }\r\n    return std::make_shared<IndexedArray64>(identities,\r\n                                            Parameters::none(),\r\n                                            carry,\r\n                                            shallow_copy());\r\n  }\r\n```\r\n\r\ninstead of replacing the _call_ to `carry` in `ListArray::combinations`, `ListOffsetArray::combinations`, and `RegularArray::combinations`. That should make the general situation like your test 2 (RecordArrays are lazily rearranged by `combinations`) and your test 3 (non-RecordArrays are eagerly rearranged by `combinations`), the best of both worlds, by factors of 40\u00d7and 20\u00d7.\r\n\r\nIt would do a lot of other things as well, and I'm not 100% sure that the result will be error-free. But assuming that it's error-free or the errors can be fixed, it might be the best policy, not just for `combinations`, but for all operations.",
  "created_at":"2020-04-20T23:03:12Z",
  "id":616855820,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjg1NTgyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T23:03:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I think I understood some of what I misunderstood. The first being how a `RecordArray` is structured, and the other which I have still not quite understood.\r\n\r\nHow is `RecordArray::carry` called from `RecordArray::combinations(...)`. If `axis = 0` then `combinations_axis0` is called which does not call it, otherwise for all  `content[i].type::combinations` is called which only calls carry on its own type and not the parent. But definitely it's being called because the behavior changed when I copy pasted the code you wrote above.\r\n\r\nOn my machine rerunning the benchmark for length  1000: \r\n- TEST 1:  1.91 ms\r\n- TEST 2:  65.3 ms\r\n- TEST 3:  5.95 ms\r\n- TEST 4: 6.67 ms\r\n\r\nThese results are as you predicted are the best of both cases.\r\nHowever some test crashed from a segmentation fault and there are some weird artifacts which I cannot understand. \r\n\r\nSo, right now either you can accept the current PR or close it and later this topic should be revisited. I think I'll work on the `is_none()` now as it seems within my reach.",
  "created_at":"2020-04-21T14:31:46Z",
  "id":617217514,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzIxNzUxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-21T14:31:46Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"I half-expected segfaults when replacing RecordArray's `carry` implementation with an IndexedArray. Until now, I'd always assumed that the type that the type of `xyz.get()->carry()` is equal to the type of `xyz`, and this change breaks that assumption. I'll have to carefully review all the uses of carried data to see where the assumption is being relied upon.\r\n\r\nThanks for trying it! (There was a chance it could have gone without any errors.)",
  "created_at":"2020-04-21T14:36:31Z",
  "id":617220317,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzIyMDMxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-21T14:36:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Closing PR as it seems like we may have a better implementation in the future. \r\nAs it is I dislike the PR in its current state.",
  "created_at":"2020-04-21T14:55:58Z",
  "id":617232246,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzIzMjI0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-21T14:55:58Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"It's a \"bug\" because it _will_ be a blocker for the GPU backend.",
  "created_at":"2020-04-17T22:31:47Z",
  "id":615492537,
  "issue":215,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNTQ5MjUzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-17T22:31:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thank you for looking into this! I'll support your effort as much as I can. I see that the linker warnings on MacOS are gone; that's a good indication of improvement.\r\n\r\nSince the issue is with vtables, don't worry about libawkward-cpu-kernels, since this library has no classes or any external symbols that aren't `extern \"C\"`.\r\n\r\nThe one test failure is my fault, from PR #212. Apparently, a segfault from Numba snuck through\u2014I'm going to fix that.",
  "created_at":"2020-04-19T22:44:19Z",
  "id":616237887,
  "issue":217,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjIzNzg4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-19T22:44:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski Thanks! Before this is ready, I still need to look into providing a test for the second change (perhaps, with -Wweak-template-vtables), I will look into that tomorrow or next weekend.",
  "created_at":"2020-04-19T23:07:21Z",
  "id":616240724,
  "issue":217,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjI0MDcyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-19T23:07:21Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Turns out -Wweak-template-vtables doesn't catch the problem addressed by the second commit. I couldn't find any other simple way either. Let's call it ready.",
  "created_at":"2020-04-20T21:33:17Z",
  "id":616821760,
  "issue":217,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjgyMTc2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T21:33:17Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I noticed that the Windows builds were raising (a lot of) warnings about this use of `extern template`, so I excluded Visual Studio. After all, this was intended to correct something Cling does.\r\n\r\nI also noticed that the localbuild.py script was failing for users/organizations other than \"scikit-hep\" and in Slice.h, the fix was applied to `SliceArrayOf` but not `SliceMissingOf` and `SliceJaggedOf`. (Admittedly, they were far from each other in the file.)\r\n\r\nAfter this test runs without warnings (in MacOS and Windows), I'll be merging this PR. Thank you for lending your time and your expertise!",
  "created_at":"2020-04-20T22:38:16Z",
  "id":616848257,
  "issue":217,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjg0ODI1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T22:38:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The big issue is that I don't understand how `list` gets reference counted in Numba (it has strange longevity between Numba functions and somebody else is incrementing it). This PR is a redesign of the past week's work without using `list`. Instead, the PartitionedViewModel maintains a \"current\" ArrayViewModel that gets updated when crossing boundaries. This is more efficient for sequential access than the old version, though the same for random access. (Don't do it!)",
  "created_at":"2020-04-20T05:32:20Z",
  "id":616320675,
  "issue":218,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjMyMDY3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T05:32:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That's true, and I punted on supporting Numba with Python 3.5. Since Numba is not an explicit dependency of Awkward, I couldn't put it in the requirements.txt, but the \"dev requirements\" includes\r\n\r\n```\r\nnumba>=0.49.0;python_version>=\"3.6\r\n```\r\n\r\nWhat you're seeing is that\r\n\r\n   * Numba moved the locations of most of the classes and functions useful for extensions like ours to present a more stable public API. This was, in a large part, inspired by a [talk I gave to the Numba group](https://nbviewer.jupyter.org/github/scikit-hep/awkward-1.0/blob/master/docs-demo-notebooks/2020-01-22-numba-demo-EVALUATED.ipynb#4.2-Survey-of-Numba-functions-used) which led to them [conducting a survey](https://github.com/numba/numba/issues/5214) of all the major users of Numba extensions to figure out which are the important functions to present in an API. Since everything moved, our minimum Numba version is now 0.49, after the move.\r\n   * The last NumPy version that can run on Python 3.5 is 1.14 (see [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html)).\r\n   * The earliest NumPy version that Numba 0.49 can use is 1.15 (see [release notes](https://numba.pydata.org/numba-doc/dev/release-notes.html#version-0-49-0-apr-16-2020)).\r\n\r\nHence, Numba won't work in Awkward for Python 3.5. QED.\r\n\r\nI ought to put some specific check in, to provide a clearer warning to the user.",
  "created_at":"2020-04-20T14:00:24Z",
  "id":616573180,
  "issue":219,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjU3MzE4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T14:00:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I changed the name of this issue because what I plan to do with this is to make Awkward complain on entry (the `awkward1._connect._numba.register` function) if the Numba version is less than 0.49, which would always be true for Python 3.5.\r\n\r\nIt won't complain on import (so that the mere existence of an old Numba doesn't break Awkward), but when you attempt to use an array in a Numba function.",
  "created_at":"2020-04-20T14:03:56Z",
  "id":616575351,
  "issue":219,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjU3NTM1MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-20T14:03:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Alright, yes I was aware of these changes and the dependency issues. Just wanted to let you know that this bubbles up in an inconvenient way, but thanks for the summary!\r\n\r\n> I ought to put some specific check in, to provide a clearer warning to the user.\r\n\r\nOK that sounds fine!",
  "created_at":"2020-04-20T14:04:30Z",
  "id":616575681,
  "issue":219,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjU3NTY4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T14:04:30Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Fixed in PR #227.",
  "created_at":"2020-04-30T11:26:28Z",
  "id":621774260,
  "issue":219,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMTc3NDI2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-30T11:26:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks Jim!",
  "created_at":"2020-04-30T11:27:18Z",
  "id":621774617,
  "issue":219,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMTc3NDYxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-30T11:27:18Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I've verified that this ~~is correct~~ works for me.",
  "created_at":"2020-04-20T15:11:54Z",
  "id":616617317,
  "issue":220,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjYxNzMxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T15:12:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The demo explicitly requires an old version of Awkward (what was current at the time of the demo, which is entirely replaced now) and implicitly requires a <0.49 version of Numba, because we didn't know the change was coming.",
  "created_at":"2020-04-20T15:02:50Z",
  "id":616611671,
  "issue":221,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjYxMTY3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T15:02:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes, I think I also tried Numba 0.47 and Awkward1==0.1.92 and 0.1.38 but I could not get it work. I did not make the effort tot install the mid-version though \ud83d\ude48 \r\n\r\nAnyways, is there a new method to invoke this functionality or is this currently under reconstruction?\r\n\r\nEdit: forgot to mention that I also looked at the tests but I could not find an obvious way how to utilise the Numba functionality. I am of course aware that the documentation is currently under development! Sorry for being impatient \ud83d\ude09 ",
  "created_at":"2020-04-20T15:11:51Z",
  "id":616617277,
  "issue":221,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjYxNzI3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T15:15:38Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I just checked your code sample, copied from the notebook.\r\n\r\n```python\r\n# Works for the layout nodes, but not the high-level ak.Array wrapper yet.\r\n```\r\n\r\nNow the inverse is true\u2014it only works for ak.Array and not the layout nodes. The Awkward-Numba interface was entirely rewritten after that talk, responding to feedback from the Numba team. (Now we're not even passing layout nodes through Numba; it's more of a pointer starting from the root of the ak.Array because this avoids copying large structures repeatedly.)\r\n\r\nHere is a better demo to start from, presented one or two weeks ago: https://github.com/jpivarski/2020-04-08-eic-jlab#readme I froze function names before this new demo, so it's more forward-looking.",
  "created_at":"2020-04-20T15:17:18Z",
  "id":616620696,
  "issue":221,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjYyMDY5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T15:17:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Indeed, omg that's a bit embarrassing. Of course I first tried to naively use them in Numba, which failed (surely my mistake) then searched for the docs and fixated on the Layout-stuff.\r\n\r\nThanks! Also for the new demo :)",
  "created_at":"2020-04-20T15:30:21Z",
  "id":616628948,
  "issue":221,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjYyODk0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T15:30:21Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"What you're showing is consistent with the original `fit` array being malformed\u2014having `starts` and `stops` that point beyond its `contents`. The `repr` will fail if this is the case. You might want to try\r\n\r\n   * [ak.is_valid](https://awkward-array.readthedocs.io/en/latest/_auto/ak.is_valid.html): returns False if malformed.\r\n   * [ak.validity_error](https://awkward-array.readthedocs.io/en/latest/_auto/ak.validity_error.html): returns a string describing the error if malformed.\r\n\r\nBoth of these accept `exception=True` to halt execution if you have a malformed array. Perhaps validity checking should be part of `repr`, but it's a potentially expensive operation because it has to scan all fields of an array, which might be a big \"events\" dataset with lots of fields.\r\n\r\nThe second question is how did it get into this state? I hadn't thought of passing an Awkward0 array (from Uproot) directly into the `ak.Array` constructor; there's a\r\n\r\n   * [ak.from_awkward0](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_awkward0.html): converts from Awkward0 to Awkward1\r\n\r\nfunction for that, but it shouldn't be allowed to do crazy things like this. I'd like to see this file.\r\n\r\nFinally, you might not be interested in\r\n\r\n   * [ak.count](https://awkward-array.readthedocs.io/en/latest/_auto/ak.count.html): a _reducer_ that follows NumPy reduction rules for each `axis`\r\n\r\nrather than\r\n\r\n   * [ak.num](https://awkward-array.readthedocs.io/en/latest/_auto/ak.num.html): returns the number of values at some level of depth.\r\n\r\nFor example,\r\n\r\n```python\r\n>>> array = ak1.Array([[[0, 0, 0, 0, 0],\r\n...                     [0, 0, 0],\r\n...                     [0, 0, 0, 0]],\r\n...                    [[0, 0],\r\n...                     [0, 0, 0, 0, 0]]])\r\n>>> ak1.count(array, axis=None)\r\n19\r\n>>> print(ak1.count(array, axis=0))\r\n[[2, 2, 1, 1, 1],\r\n [2, 2, 2, 1, 1],\r\n [1, 1, 1, 1]]\r\n>>> print(ak1.count(array, axis=1))\r\n[[3, 3, 3, 2, 1],\r\n [2, 2, 1, 1, 1]]\r\n>>> print(ak1.count(array, axis=2))\r\n[[5, 3, 4],\r\n [2, 5]]\r\n```\r\n\r\nAll `axis != -1` (i.e. `axis != 2` for this doubly jagged array) count laterally across objects. If you have a jagged array of particles in events, `axis = 0` asks questions like, \"How many first particles are there, added up across all events?\" and \"How many second particles are there?\"\r\n\r\nThe `ak.num` function gives you something you can use in cuts, like\r\n\r\n```python\r\n>>> print(ak1.num(array, axis=0))\r\n2\r\n>>> print(ak1.num(array, axis=1))\r\n[3, 2]\r\n>>> print(ak1.num(array, axis=2))\r\n[[5, 3, 4], [2, 5]]\r\n```\r\n",
  "created_at":"2020-04-20T17:36:29Z",
  "id":616705482,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjcwNTQ4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T17:36:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Regarding the new title, it's this line\r\n\r\n```python\r\nfit = ak1.Array(uproot.open(my_file)[\"E/Evt/trks/trks.fitinf\"].array())\r\n```\r\n\r\nthat I'm worried might be producing invalid arrays, and it's probably taking the `ak.from_iter` path.",
  "created_at":"2020-04-20T17:58:12Z",
  "id":616717237,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjcxNzIzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T17:58:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I just noticed that you do, in fact, provide data. Triaging now.",
  "created_at":"2020-04-20T18:01:45Z",
  "id":616719120,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjcxOTEyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T18:01:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I was wrong. `ak.from_iter` is producing valid arrays; `ak.count` with `axis=0` is producing invalid ones.\r\n\r\n```python\r\n>>> ak.validity_error(ak.count(fit, axis=2))\r\n>>> ak.validity_error(ak.count(fit, axis=1))\r\n>>> ak.validity_error(ak.count(fit, axis=0))\r\n'at layout (ListArray64): start[i] > stop[i] at i=20'\r\n```\r\n\r\nand we see crazy values in the ListArray's `starts` and `stops` by looking at the low-level layout:\r\n\r\n```python\r\n>>> ak.count(fit, axis=0).layout\r\n<ListArray64>\r\n    <starts>\r\n        <Index64 i=\"[0 17 34 51 68 ... 510 520 530 540 549]\" offset=\"0\" length=\"56\"/>\r\n    </starts>\r\n    <stops>\r\n        <Index64 i=\"[17 28 42 59 76 ... 1 1 1 1 1]\" offset=\"0\" length=\"56\"/>\r\n    </stops>\r\n    <content>\r\n        <NumpyArray format=\"l\" shape=\"331\" data=\"10 10 10 10 10 ... 8 8 8 8 8\"/>\r\n    </content>\r\n</ListArray64>\r\n```\r\n\r\n(Sometimes those 500-ish numbers are like `94008244174849`.) Looking into it.",
  "created_at":"2020-04-20T18:09:47Z",
  "id":616722985,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjcyMjk4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T18:09:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Sorry my issue was probably too cluttered with lots of unecessary information. \r\n\r\nBut the main problem is that both `ak.count` and `ak.count_nonzero` seem to provide invalid outputs when used with `axis=0` and `axis=1` when the data had 2 levels of nesting.",
  "created_at":"2020-04-20T18:57:21Z",
  "id":616745953,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjc0NTk1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T18:57:21Z",
  "user":"MDQ6VXNlcjQ3MTExMDg3"
 },
 {
  "author_association":"MEMBER",
  "body":"In fact, all reducers do; it shows up when `axis` is less than the nesting depth and there are trailing empty lists in the output. The `starts` and `stops` of the trailing lists were left uninitialized, rather than being set to equal values (indicating an empty list).\r\n\r\nI just fixed it. Actually, the new system is quite nice to debug! Since there are for loops at the deepest level, I can insert print statements. `:)`",
  "created_at":"2020-04-20T19:00:14Z",
  "id":616747306,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjc0NzMwNg==",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2020-04-20T19:00:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Oh great! Thank you very much for the prompt reply! I will test the fix :)\r\n\r\nLooking forward to the official release of version 1 of `awkward1` :)",
  "created_at":"2020-04-20T19:04:10Z",
  "id":616749399,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjc0OTM5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T19:04:10Z",
  "user":"MDQ6VXNlcjQ3MTExMDg3"
 },
 {
  "author_association":"MEMBER",
  "body":"It will be in the next release, which will probably wait for #216. I'm deploying releases a little less often than I used to because these include a lot of binaries and PyPI is instituting a quota.\r\n\r\nWhat you want for your analysis/framework is probably not `ak.count` with `axis=0`. I don't know exactly what you want to do, but it probably involves `ak.num`.",
  "created_at":"2020-04-20T19:18:43Z",
  "id":616756233,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjc1NjIzMw==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2020-04-20T19:18:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"What I needed for my analysis is `ak.count` with `axis=2`, which is already working fine. But when I noticed that `ak.count` with `axis=0` and `axis=1` seemed buggy, I thought I should report it. That's all :)\r\nSo basically I am not stuck in my analysis, so no worries :)",
  "created_at":"2020-04-21T09:05:19Z",
  "id":617053332,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzA1MzMzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-21T09:05:19Z",
  "user":"MDQ6VXNlcjQ3MTExMDg3"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, good. You might want to write `axis=-1`, which will always be the last one, no matter how deep.\r\n\r\nNegative axis has been implemented for reducers and will be for everything else, but some of them rely on a soon-to-be-written function.\r\n\r\nThanks for reporting the issue!",
  "created_at":"2020-04-21T11:36:01Z",
  "id":617124396,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzEyNDM5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-21T11:36:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Looking good so far!",
  "created_at":"2020-04-20T21:37:00Z",
  "id":616823426,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjgyMzQyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T21:37:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The \"failed jobs\" are actually me killing them to make room for tests of PR #217, which I'm checking for an imminent merge.",
  "created_at":"2020-04-20T21:41:09Z",
  "id":616825458,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjgyNTQ1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T21:41:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm using GitHub drafts now, instead of `[WIP]`.",
  "created_at":"2020-04-20T21:42:39Z",
  "id":616826206,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNjgyNjIwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-20T21:42:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The CI is failing because it can't import `pyarrow`. I'll add that in the `.ci` stuff, if you don't mind.",
  "created_at":"2020-04-24T06:59:21Z",
  "id":618839279,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxODgzOTI3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-24T06:59:21Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"You have a lot of comments above that I need to get to, but for pyarrow dependencies, the tests should do \"pytest.importorskip(pyarrow)\" to optionally run Arrow tests (like Numba), and you should add pyarrow to the CI (like Numba).",
  "created_at":"2020-04-24T12:27:45Z",
  "id":618979288,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxODk3OTI4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-24T12:27:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Main thing:\r\n\r\n> Probably what we should do is schedule a pair-programming session. I've done that a few times with Atom TeleType with Skype/Zoom and it's worked very well.\r\n> \r\n> Monday or Tuesday of next week? How late would be okay for you? (I'm not trying to avoid getting up early; I'm trying to avoid morning meetings because that's prime time for CERN-U.S. meetings. I'm in a project that has takes a lot of that time, but if you name an hour-long slot that works for you, I'll skip it to help you out here.)\r\n\r\nPick a time and let's talk. Also, try to get a copy of Atom with Teletype if that's not what you usually use because that will make communication much easier. I'll send you a Zoom link by email or Gitter.\r\n\r\nAnd again, thanks for looking into this! You're on the right track\u2014we're in the details, which is a good place to be.  `:)`",
  "created_at":"2020-04-24T13:46:43Z",
  "id":619020743,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxOTAyMDc0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-24T13:46:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks, I'll look into it. As far as time goes, I usually stay awake till 3AM so, I think anytime between 12AM IST to 2AM IST works for me. And can we hold this on a Tuesday? Looking forward to it. Also, should I go ahead and try to fix up some of the issues you mentioned here, or do you want to address it in the meeting itself?",
  "created_at":"2020-04-24T14:19:15Z",
  "id":619039012,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxOTAzOTAxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-24T14:19:15Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"That is very late.  `:)`  Midnight IST is 1:30pm in Chicago, so that definitely works for both of us. If you like, we could start an hour or two earlier without conflicting with my morning meetings. Let me know if you'd like to move it, but otherwise I'll meet you then.\r\n\r\nAlso, we should be clear about the date. Let's say we meet at 12:01am IST on Tuesday morning (i.e. you stay up late on Monday night). That's Monday afternoon for me. If this isn't the day you meant, let me know.\r\n\r\nI'm sending you the Zoom link on Gitter, and we can continue scheduling there, if there's anything to clear up.",
  "created_at":"2020-04-24T14:47:09Z",
  "id":619056245,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxOTA1NjI0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-24T14:47:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hey Jim, can you manually trigger the CI like when you did for the one PR I submitted long ago. I don't know why the ci isn't triggering automatically.\r\n\r\nAlso, now `to_arrow` is entirely zero-copy. Everything is being formed from buffers. I need to sort out the IndexedArray, as Arrow hasn't implemented the feature to apply masks on Arrow Arrays, and the making a DictionaryArray `from_buffers` is somewhat complicated in the sense that it requires a data buffer which if I try to make after converting them to Arrow Arrays becomes weirder because each Arrow is a list of Buffers in itself. I'll ask the Arrow guys for an opinion on how to proceed with this.",
  "created_at":"2020-04-30T16:41:50Z",
  "id":621968611,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMTk2ODYxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-30T16:41:50Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yup, the build passes now. All that's left is to really nitpick the details and run tests on some larger datasets. I have added some elementary tests, which should be a good proof of concept.",
  "created_at":"2020-05-10T14:41:56Z",
  "id":626338766,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNjMzODc2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-10T14:41:56Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! I'll check this out and review it on Monday morning.",
  "created_at":"2020-05-10T18:49:27Z",
  "id":626371787,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNjM3MTc4Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2020-05-10T18:49:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry that I have to close this for technical reasons; it continues in PR #263.",
  "created_at":"2020-05-13T18:44:11Z",
  "id":628175502,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODE3NTUwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T18:44:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I am not sure how `index()` is implemented but if it's using `==`, I assume that it chokes on the `ValueError` which is raised when comparing two arrays with different shapes.\r\n\r\n```python\r\nIn [1]: import awkward1 as ak\r\n\r\nIn [2]: a = ak.Array([1,2,3])\r\n\r\nIn [3]: b = ak.Array([1,2,3])\r\n\r\nIn [4]: a == b\r\nOut[4]: <Array [True, True, True] type='3 * bool'>\r\n\r\nIn [5]: bool(a == b)\r\nOut[5]: True\r\n\r\nIn [6]: c = ak.Array([1,2,3,4])\r\n\r\nIn [7]: a == c\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-2997aacb48a8> in <module>\r\n----> 1 a == c\r\n```\r\n\r\n",
  "created_at":"2020-04-21T15:23:11Z",
  "id":617248947,
  "issue":225,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzI0ODk0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-21T15:23:11Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't realize that ak.Array had inherited `index` from `collections.abc.Sequence`, but it shouldn't. As @tamasgal pointed out, it uses `==`, which a NumPy-like thing would use to produce an _array_ of results, which satisfies the truthiness because it's not an empty array.\r\n\r\nI'm going to remove `Sequence` as a mixin from ak.Array. After all, NumPy arrays aren't `Sequences` (for the same reason).\r\n\r\n```python\r\n>>> isinstance(np.array([1, 2, 3]), collections.abc.Sequence)\r\nFalse\r\n```\r\n\r\nThanks again!",
  "created_at":"2020-04-21T15:26:22Z",
  "id":617250887,
  "issue":225,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzI1MDg4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-21T15:26:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yep, the catch is really that the truthiness of an array is not an `all(...)`.\r\n\r\n@zinebaly I'd recommend to implement a numba JITted function, that should be fairly straight forward.",
  "created_at":"2020-04-21T15:42:32Z",
  "id":617255903,
  "issue":225,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzI1NTkwMw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-21T15:42:32Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"NONE",
  "body":"Yep, the catch is really that the truthiness of an array is not an `all(...)`.\r\n\r\n@zinebaly I'd recommend to implement a numba JITted function, that should be fairly straight forward.",
  "created_at":"2020-04-21T15:52:31Z",
  "id":617258137,
  "issue":225,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzI1ODEzNw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-04-21T15:52:31Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"@zinebaly You could do it with Numba. Just thinking through how this could be done in array-at-a-time operations...\r\n\r\n```python\r\nhaystack = ak.Array([[1, 2], [0], [1, 2, 3], [], [4, 5, 6], [1, 2, 3]])\r\nneedle = ak.Array([1, 2, 3])\r\n\r\nak.num(haystack, axis=1) == ak.num(needle, axis=0)\r\n# [False, False, True, False, True, True]\r\n# Where are the lengths the same? Note the different axis depths.\r\n\r\nsamelength = haystack.mask[ak.num(haystack, axis=1) == ak.num(needle, axis=0)]\r\n# [None, None, [1, 2, 3], None, [4, 5, 6], [1, 2, 3]]\r\n# Unlike a cut (boolean array in square brackets), a mask maintains the original\r\n# indexing by putting 'None' placeholders in the output array.\r\n\r\nneedle[np.newaxis]\r\n# <Array [[1, 2, 3]] type='1 * 3 * int64'>\r\n# Standard NumPy trick for telling it which dimension you want to broadcast. (You\r\n# want [1, 2, 3] for every element of samelength, not 1 for the first, 2 for the\r\n# second, which is what implicit broadcasting would do here.)\r\n\r\nsamelength == needle[np.newaxis]\r\n# [None, None, [True, True, True], None, [False, False, False], [True, True, True]]\r\n# The alternative, haystack == needle[np.newaxis], would have failed because the\r\n# 3-element needle can't be broadcasted to some of the haystack elements, as they\r\n# have different lengths. The mask prevents operations that you don't want to do.\r\n\r\ngood = ak.all(samelength == needle[np.newaxis], axis=1)\r\n# [None, None, True, None, False, True]\r\n# The 'all' and 'any' reducers come up in a lot of these things.\r\n\r\nak.where(good)\r\n# (<Array [2, 5] type='2 * int64'>,)\r\n# We actually found all of them, but 'index' only wants the first one.\r\n\r\nak.where(good)[0][0]\r\n# 2\r\n```\r\n\r\nak.Array will quietly cease to be a Sequence in #216. It's just too minor of a change at this early stage to make a whole PR out of it.\r\n\r\nI'm closing this PR.",
  "created_at":"2020-04-21T16:37:27Z",
  "id":617273799,
  "issue":225,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzI3Mzc5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-21T16:37:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I see that I wrote that \"it should probably become a C++ method,\" though now I'm not sure why.\r\n\r\nThere are some functions that really must be written in C++ because of their algorithmic complexity (anything that would be hard to write in NumPy calls); there are others that really must be written in Python because of their type-complexity (anything involving more than one array argument or a relationship between one node and the next node deep).\r\n\r\nThis one could go either way: I don't see any difficulties with writing a method for each class and I don't see any difficulties with writing a closure for `awkward1._util.recursively_apply`. The latter would involve less code, as it would only have to define a behavior for the option-type nodes; the behavior for other nodes follows a pattern. If you want to do it in C++, it could be a good learning exercise, since that would expose you to all the node types.\r\n\r\nOriginally, I had envisioned all operations being implemented in C++, but now I don't see that as being necessary. Users in C++ are not doing data analysis; they're connecting Awkward to other services that are written in C++, so they need the data structures, but not all the operations. Of course, if an operation is easier to implement in C++, then we'd take advantage of that fact.\r\n\r\n> As far as I can tell only IndexedOptionArray is capable of storing None at it's own level (?).\r\n\r\nThe full set of node types that can store `None` at their own level (called \"option-type\" nodes) is:\r\n\r\n   * [IndexedOptionArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedOptionArray.html): the first and most flexible. Any sufficiently complex operation eventually turns an option-type array into IndexedOptionArray to avoid deep copying (similar to the way that complex operations turn ListOffsetArrays into the more general ListArrays).\r\n   * [ByteMaskedArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ByteMaskedArray.html): behaves like NumPy's MaskedArray (though it can store any data type, unlike NumPy's MaskedArray). Beyond compatibility with NumPy, this one is primarily intended for the [ak.mask](https://awkward-array.readthedocs.io/en/latest/_auto/ak.mask.html) function, since that can be performed zero-copy with a ByteMaskedArray.\r\n   * [BitMaskedArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.BitMaskedArray.html): Arrow's mechanism for option-types; it exists for compatibility only. Just about any operation turns it into a ByteMaskedArray or an IndexedOptionArray.\r\n   * [UnmaskedArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.UnmaskedArray.html): can't actually store `None` values, but it declares its type as potentially storing `None` values. This is so that we can present uniform high-level types on a collection of datasets (like partitions in a PartitionedArray) but not unnecessarily generate empty masks. All data from Arrow is option-type, for instance, but if none of the values are `None`, they don't generate a bit-mask. UnmaskedArray is for those arrays that could have masks, but don't.\r\n\r\nThe `awkward1._util.optiontype` tuple has the full set of classes with option-type, and implementations in Python check this with `isinstance` to know if they're allowed to call `bytemask()`. In C++, you're writing the method separately for each class, so you do option-type things on the option-type array nodes.\r\n\r\nIn C++, I don't see a need for an `isnone_axis0`. Some of the other operations have an `_axis0` method on `Content` (the superclass) because they do a different thing at `axis=0` than other `axis` values. That's not true of `is_none`: it's regular at all levels of recursion.",
  "created_at":"2020-04-22T13:21:45Z",
  "id":617775589,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxNzc3NTU4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-22T13:21:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> I see that I wrote that \"it should probably become a C++ method,\" though now I'm not sure why.\r\n\r\nThat's alright I am treating this primarily as a learning exercise with the work done as a nice side product. I really don't mind if this PR gets rejected after passing review if the other implementation is more in the style of this library.\r\n\r\nWhile I complained a tad about there being no \"how-to-develop\" documentation I really appreciate the effort you put into replying to my queries promptly and in great detail. ",
  "created_at":"2020-04-23T02:15:38Z",
  "id":618136374,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxODEzNjM3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-23T02:15:38Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski Just wanted to clarify something.\r\n```python\r\n>>> a = ak.Array([None, [1, None], [2, 3]])\r\n>>> ak.is_num(a, axis = 1)\r\n[None, [False, True], [False, False]]\r\n``` \r\n^Is that the correct behavior ? I mean to skip over `None` elements when `axis != depth`. It seems this is the general behavior for these type of functions.\r\nOr should it throw an error since `None` has no depth.",
  "created_at":"2020-04-23T22:26:27Z",
  "id":618703378,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxODcwMzM3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-23T22:26:27Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, this is correct. By specifying an axis, we only want to know about None values at that specific depth, passing through all other structures, even if they also contain Nones. You've identified the right pattern.",
  "created_at":"2020-04-23T22:47:48Z",
  "id":618710253,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxODcxMDI1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-23T22:47:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski I think I am mostly done. Not sure why the test is failing, I suspect it has something to do with the Pandas's internal type method with `__eq__`. Although I don't know python so I am unable to figure out. \r\nAlso, do I need to add additional tests ?",
  "created_at":"2020-04-24T21:46:25Z",
  "id":619252399,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYxOTI1MjM5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-25T01:05:34Z",
  "user":"MDQ6VXNlcjc0NjYzNjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Is this still active? I know that you did a lot of work on this, but I think you hit a problem and we weren't certain that it was the right design direction, anyway (implementing it in C++ vs Python).\r\n\r\nPRs never really go away, but can I close this so that I have a better idea of what's in progress?",
  "created_at":"2020-05-14T20:48:21Z",
  "id":628878711,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODg3ODcxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T20:48:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Should this be revived or is it more appropriate to make a fresh start?",
  "created_at":"2020-09-04T18:03:17Z",
  "id":687299852,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NzI5OTg1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-04T18:03:17Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I think a fresh start would be a better choice, least of all because so much has changed in the codebase since this issue (the merge would be big).\r\n\r\nAlso, I'm not sold on the idea that `is_none` needs to be a C++ method, rather than a Python method implemented with `recursively_apply`. (See discussion above.) Most of the difficulty of dealing with many different types of nodes goes away in Python. Some operations really need to be in C++ if they do manipulations that can't easily be expressed in NumPy, but `is_none` is not one of them.\r\n\r\nSpeaking of refactorings, note that you have to use `nplike` now instead of NumPy directly: `nplike.of(some, arrays)` returns either a NumPy proxy or a CuPy proxy, depending on whether those arrays are in main memory or on a GPU, and then dispatches the functions to the right library. This is to manage the fact that NumPy and CuPy are not exactly the same, and even when they are, things like `asarray` are \"antisymmetric\" when they're supposed to be \"symmetric\" (i.e. we'd like it to assume that the array is on the device that the library handles, but `cupy.asarray` naturally assumes that it's in main memory and needs to be moved to the GPU).\r\n\r\n([This](https://github.com/scikit-hep/awkward-1.0/blob/d4cda8fe8885d34a827b74a95aba43f957051809/tests-cuda/test_0388-abstract-all-uses-of-numpy.py) ensures that NumPy and CuPy, with the shim, behave the same.)",
  "created_at":"2020-09-04T20:46:25Z",
  "id":687374491,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NzM3NDQ5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-04T20:46:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That last commit refers to scikit-hep/scikit-hep.github.io#54, which hopefully will be accepted so that I can link to it.",
  "created_at":"2020-04-29T19:53:49Z",
  "id":621427974,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMTQyNzk3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-04-29T19:53:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The Sphinx page is done. I'll open another PR with the next batch of documentation.",
  "created_at":"2020-05-04T20:02:24Z",
  "id":623675895,
  "issue":229,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzY3NTg5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-04T20:02:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The first head-palm issue is that a `SlicedGenerator` (what you get if you single-level slice a VirtualArray) is not a `PyArrayGenerator`. But it's ridiculous to not be able to get a Python reference to it. These sorts of things should bubble up to Python as zero-argument callables.",
  "created_at":"2020-05-03T18:24:14Z",
  "id":623157247,
  "issue":230,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzE1NzI0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-03T18:24:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think it was a mistake making a distinction between regular ArrayGenerators and SlicedGenerators because it also makes it hard to manage caches in the SlicedGenerators (which are opaque to Python). It should be a nested VirtualArray that performs a slice. The new VirtualArray has to not be lazy about slicing, though, or else the whole thing would infinite-loop, so `bool lazyslice` should be a VirtualArray constructor argument.",
  "created_at":"2020-05-03T19:19:31Z",
  "id":623165877,
  "issue":230,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzE2NTg3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-03T19:19:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Are you online? I'm not finding a satisfactory way to deal with lazy slices and I need to be reminded why we cared so much. Skype?",
  "created_at":"2020-05-03T19:40:50Z",
  "id":623169072,
  "issue":230,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzE2OTA3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-03T19:40:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is the ampersand that was causing all the segfaults (00f69138a714733bca7b649be7707ae5b8a139a1).\r\n\r\n```diff\r\n@@ -89,7 +89,7 @@ namespace awkward {\r\n                     const std::string& post) const override;\r\n \r\n   protected:\r\n-    const ArrayGeneratorPtr& generator_;\r\n+    const ArrayGeneratorPtr generator_;\r\n     const Slice slice_;\r\n   };\r\n }\r\n```",
  "created_at":"2020-05-04T00:27:04Z",
  "id":623209621,
  "issue":230,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzIwOTYyMQ==",
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "total_count":1
  },
  "updated_at":"2020-05-04T00:27:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2020-05-03T19:35:44Z",
  "id":623168335,
  "issue":232,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzE2ODMzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-03T19:35:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The errors are in new code: VirtualArray didn't exist when you last worked on this:\r\n\r\n```\r\n  In file included from /Users/runner/runners/2.166.4/work/1/s/src/libawkward/array/VirtualArray.cpp:9:\r\n  In file included from /Users/runner/runners/2.166.4/work/1/s/include/awkward/array/VirtualArray.h:13:\r\n  /Users/runner/runners/2.166.4/work/1/s/include/awkward/virtual/ArrayGenerator.h:20:23: error: 'ArrayGenerator' has no out-of-line virtual method definitions; its vtable will be emitted in every translation unit [-Werror,-Wweak-vtables]\r\n    class EXPORT_SYMBOL ArrayGenerator {\r\n                        ^\r\n  /Applications/Xcode_11.3.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:4331:26: note: in instantiation of function template specialization 'std::__1::__shared_ptr_emplace<awkward::SliceGenerator, std::__1::allocator<awkward::SliceGenerator> >::__shared_ptr_emplace<const std::__1::shared_ptr<awkward::Form>, long long, const std::__1::shared_ptr<awkward::ArrayGenerator> &, awkward::Slice &>' requested here\r\n      ::new(__hold2.get()) _CntrlBlk(__a2, _VSTD::forward<_Args>(__args)...);\r\n                           ^\r\n  /Applications/Xcode_11.3.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:4710:29: note: in instantiation of function template specialization 'std::__1::shared_ptr<awkward::SliceGenerator>::make_shared<const std::__1::shared_ptr<awkward::Form>, long long, const std::__1::shared_ptr<awkward::ArrayGenerator> &, awkward::Slice &>' requested here\r\n      return shared_ptr<_Tp>::make_shared(_VSTD::forward<_Args>(__args)...);\r\n                              ^\r\n  /Users/runner/runners/2.166.4/work/1/s/src/libawkward/array/VirtualArray.cpp:440:40: note: in instantiation of function template specialization 'std::__1::make_shared<awkward::SliceGenerator, const std::__1::shared_ptr<awkward::Form>, long long, const std::__1::shared_ptr<awkward::ArrayGenerator> &, awkward::Slice &>' requested here\r\n      ArrayGeneratorPtr generator = std::make_shared<SliceGenerator>(\r\n                                         ^\r\n  1 error generated.\r\n  make[2]: *** [CMakeFiles/awkward-objects.dir/src/libawkward/array/VirtualArray.cpp.o] Error 1\r\n```\r\n\r\nThe warnings about overriding are also new and easily fixed. I fixed that one in another branch so you can safely ignore it. It's the warning about VirtualArray that matters.\r\n\r\nI don't know what is different about VirtualArray; I think it has the same mix of pure virtual and concrete methods.",
  "created_at":"2020-05-03T22:31:35Z",
  "id":623193265,
  "issue":233,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzE5MzI2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-03T22:31:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> VirtualArray didn't exist when you last worked on this\r\n\r\nThat's not the issue. The issue is that Apple's toolchain identifies as AppleClang, so you were working without tests.",
  "created_at":"2020-05-03T22:34:13Z",
  "id":623193637,
  "issue":233,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzE5MzYzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-03T22:34:13Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"@veprbl I wasn't sure if you were done, so I was waiting to see what happens. Is this PR done? It passed all the tests when you last worked on it, and I can merge with master to see if it's still okay. If the answer to both questions is \"yes,\" then I'll merge it.\r\n\r\nThanks!",
  "created_at":"2020-05-14T20:50:30Z",
  "id":628879737,
  "issue":233,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODg3OTczNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T20:50:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski This is ready, I believe.",
  "created_at":"2020-05-15T18:04:01Z",
  "id":629401477,
  "issue":233,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTQwMTQ3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T18:04:01Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This implies also support for some utility method to take a keyed form and a dictionary of arrays and return an awkward array.",
  "created_at":"2020-05-04T20:45:40Z",
  "id":623696058,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzY5NjA1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-04T20:45:40Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Or alternatively, a dictionary of callables to be used as generators, or a single callable and the form key is passed as an argument, etc.",
  "created_at":"2020-05-04T20:46:51Z",
  "id":623696637,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzY5NjYzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-04T20:46:51Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"There would be an advantage to having all the metadata be serializable as JSON. Data structures of callables are powerful, but not very portable.",
  "created_at":"2020-05-04T22:09:35Z",
  "id":623734400,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzczNDQwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-04T22:09:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Depends where you want to set the interface between awkward and the data delivery library. Perhaps awkward knows the column key and partition index (for chunked arrays) and expects an external call to provide the flat array.",
  "created_at":"2020-05-04T22:18:53Z",
  "id":623737958,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzczNzk1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-04T22:18:53Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"What I had in mind when I wrote the original thing was that each one-dimensional array would have a (partition-id, position-in-tree) coordinate and that the Form would have a reference to the position-in-tree part.\r\n\r\nFor example, in [old awkward serialization](https://github.com/scikit-hep/awkward-array/blob/master/awkward/persist.py), the full array was broken down into a bunch of one-dimensional arrays that the backend knew how to store. HDF5 was one of those backends: anything that can name binary blobs would do. [Zarr's v3 extension](https://zarr-specs.readthedocs.io/en/core-protocol-v3.0-dev/) could be put to the same task and it could resolve the biggest problem that the old serialization had: the HDF5 file didn't know it was an Awkward HDF5 file. Having an explicit extension mechanism would at least let Zarr users know that a particular array is _supposed_ to be read with the Awkward library, so that it can raise an error if the library isn't available. I'm optimistic about this as a way to do it and wrote it up at zarr-developers/zarr-specs#62.\r\n\r\nThe Form would take the role that the old schema.json took, with better separation between the description of where one-dimensional arrays go and those one-dimensional arrays themselves. You raised the point that we'd rather have one JSON that can be reinterpreted for all partitions than having a JSON per partition that's tightly glued to that partition. Whereas the old schema.json included names of actual arrays to find in the blob store (with their lengths), the new Forms can include keys that when joined with a partition-id tell you the names of actual arrays in the store. In Zarr's terminology, each partition can be a group and each group can have arrays with the same names in it.",
  "created_at":"2020-05-05T00:48:41Z",
  "id":623783508,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyMzc4MzUwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-05T00:48:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As soon as #348 can be merged, we'll have an equivalent of Awkward 0's persistence layer. The new functions are `ak.to_arrayset` and `ak.from_arrayset`. The documentation should go online soon after it's merged to master as well, but here are some examples:\r\n\r\n```python\r\n>>> original = ak.Array([[1, 2, 3], [], [4, 5]])\r\n>>> form, container, num_partitions = ak.to_arrayset(original)\r\n>>> form\r\n{\r\n    \"class\": \"ListOffsetArray64\",\r\n    \"offsets\": \"i64\",\r\n    \"content\": {\r\n        \"class\": \"NumpyArray\",\r\n        \"itemsize\": 8,\r\n        \"format\": \"l\",\r\n        \"primitive\": \"int64\",\r\n        \"form_key\": \"node1\"          # <-- the new form_key that makes this possible\r\n    },\r\n    \"form_key\": \"node0\"              # <-- the new form_key that makes this possible\r\n}\r\n>>> container\r\n{'node0-offsets': array([0, 3, 3, 5], dtype=int64),\r\n 'node1': array([1, 2, 3, 4, 5])}\r\n>>> print(num_partitions)\r\nNone\r\n```\r\n\r\nRead it back with:\r\n\r\n```python\r\n>>> ak.from_arrayset(form, container)\r\n<Array [[1, 2, 3], [], [4, 5]] type='3 * var * int64'>\r\n```\r\n\r\nWrite to partitions with:\r\n\r\n```python\r\n>>> container = {}\r\n>>> form, _, _ = ak.to_arrayset(ak.Array([[1, 2, 3], [], [4, 5]]), container, 0)\r\n>>> form, _, _ = ak.to_arrayset(ak.Array([[6, 7, 8, 9]]), container, 1)\r\n>>> form, _, _ = ak.to_arrayset(ak.Array([[], [], []]), container, 2)\r\n>>> form, _, _ = ak.to_arrayset(ak.Array([[10]]), container, 3)\r\n>>> form\r\n{\r\n    \"class\": \"ListOffsetArray64\",\r\n    \"offsets\": \"i64\",\r\n    \"content\": {\r\n        \"class\": \"NumpyArray\",\r\n        \"itemsize\": 8,\r\n        \"format\": \"l\",\r\n        \"primitive\": \"int64\",\r\n        \"form_key\": \"node1\"\r\n    },\r\n    \"form_key\": \"node0\"\r\n}\r\n>>> container\r\n{'node0-offsets-part0': array([0, 3, 3, 5], dtype=int64),\r\n 'node1-part0': array([1, 2, 3, 4, 5]),\r\n 'node0-offsets-part1': array([0, 4], dtype=int64),\r\n 'node1-part1': array([6, 7, 8, 9]),\r\n 'node0-offsets-part2': array([0, 0, 0, 0], dtype=int64),\r\n 'node1-part2': array([], dtype=float64),\r\n 'node0-offsets-part3': array([0, 1], dtype=int64),\r\n 'node1-part3': array([10])}\r\n```\r\n\r\nRead it back with:\r\n\r\n```python\r\n>>> ak.from_arrayset(form, container, 4)\r\n<Array [[1, 2, 3], [], [4, ... [], [], [10]] type='8 * var * int64'>\r\n>>> ak.partitions(ak.from_arrayset(form, container, 4))\r\n[3, 1, 3, 1]\r\n```\r\n\r\nOr read it back lazily:\r\n\r\n```python\r\n>>> lazy = ak.from_arrayset(form, container, 4, lazy=True, lazy_lengths=[3, 1, 3, 1])\r\n>>> lazy.metadata[\"cache\"]\r\n{}\r\n>>> lazy\r\n<Array [[1, 2, 3], [], [4, ... [], [], [10]] type='8 * var * int64'>\r\n>>> len(lazy.metadata[\"cache\"])\r\n3\r\n>>> lazy + 100\r\n<Array [[101, 102, 103], [], ... [], [], [110]] type='8 * var * int64'>\r\n>>> len(lazy.metadata[\"cache\"])\r\n4\r\n```\r\n\r\nThis is the same partitioned/lazy interface as `ak.from_parquet`.\r\n\r\nAs a side-note, Awkward Arrays can now be pickled. (cPickle protocol 2+ only; pybind11 constraints satisfied by Python 3. I could loosen that by serializing the Forms as JSON text and back again, but I don't think that's necessary\u2014I can just say that this feature isn't automatic in Python 2.)",
  "created_at":"2020-07-22T23:53:33Z",
  "id":662754301,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2Mjc1NDMwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-22T23:53:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I haven't explicitly created the HDF5 read/write and the `*.awkd` file read/write (the format would be different in both cases). These \"deconstructed\" Awkward Arrays are not a good storage format, as @wctaylor discovered in scikit-hep/awkward-array#246.",
  "created_at":"2020-07-22T23:53:50Z",
  "id":662754382,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2Mjc1NDM4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-22T23:53:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"No code was changed in this PR. I want to see the new documentation online, so I'm skipping the tests.",
  "created_at":"2020-05-05T19:22:20Z",
  "id":624256123,
  "issue":236,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNDI1NjEyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-05T19:22:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The only changes here are documentation, so I don't need to wait for the tests to pass.\r\n\r\nMerging now.",
  "created_at":"2020-05-08T01:00:05Z",
  "id":625571804,
  "issue":237,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTU3MTgwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T01:00:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @jpivarski for bug, code, content, data, doc, design, example, ideas, infra, maintenance, question, review, test, tutorial, and talk.\r\n\r\nLet's see what happens.",
  "created_at":"2020-05-08T12:53:42Z",
  "id":625801137,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTgwMTEzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T12:53:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/239) to add @jpivarski! :tada:",
  "created_at":"2020-05-08T12:53:53Z",
  "id":625801197,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTgwMTE5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T12:53:53Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @nsmith- for bug, code, example, ideas, question, test, and talk.\r\n\r\nLet's see if it can add to an existing list.",
  "created_at":"2020-05-08T14:39:07Z",
  "id":625847540,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg0NzU0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T14:39:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI could not determine your intention.\n\nBasic usage: @all-contributors please add @someone for code, doc and infra\n\nFor other usages see the [documentation](https://allcontributors.org/docs/en/bot/usage)",
  "created_at":"2020-05-08T14:39:11Z",
  "id":625847585,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg0NzU4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T14:39:11Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @nsmith- for bug, code, example, ideas, question, test and talk",
  "created_at":"2020-05-08T14:42:38Z",
  "id":625849204,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg0OTIwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T14:42:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/240) to add @nsmith! :tada:",
  "created_at":"2020-05-08T14:42:45Z",
  "id":625849266,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg0OTI2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T14:42:45Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @ianna for code, ideas, maintenance, test\r\n",
  "created_at":"2020-05-08T15:10:18Z",
  "id":625862588,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg2MjU4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:10:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/241) to add @ianna! :tada:",
  "created_at":"2020-05-08T15:10:29Z",
  "id":625862661,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg2MjY2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:10:29Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @lgray for bug, code, ideas, test, and talk.",
  "created_at":"2020-05-08T15:50:24Z",
  "id":625881247,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg4MTI0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:50:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/242) to add @lgray! :tada:",
  "created_at":"2020-05-08T15:50:35Z",
  "id":625881318,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg4MTMxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:50:35Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @henryiii for bug, code, ideas, infra, test, and talk.",
  "created_at":"2020-05-08T15:52:07Z",
  "id":625882010,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg4MjAxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:52:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/243) to add @henryiii! :tada:",
  "created_at":"2020-05-08T15:52:15Z",
  "id":625882064,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg4MjA2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:52:15Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"Glad to see the bot is working! I might need to look into my configuration of it.",
  "created_at":"2020-05-08T15:53:25Z",
  "id":625882589,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg4MjU4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:53:25Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @henryiii for bug, code, ideas, infra, test, and talk.\r\n\r\nIt's sort-of working (see, for example, [this](https://github.com/all-contributors/all-contributors-bot/issues/310)). I'm figuring out a proper way to use it, which apparently involves adding one PR _after_ the next.",
  "created_at":"2020-05-08T15:55:38Z",
  "id":625883592,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg4MzU5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:55:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/244) to add @henryiii! :tada:",
  "created_at":"2020-05-08T15:55:46Z",
  "id":625883658,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg4MzY1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:55:46Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @reikdas for code, ideas, talk.",
  "created_at":"2020-05-08T18:04:24Z",
  "id":625943411,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0MzQxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:04:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/245) to add @reikdas! :tada:",
  "created_at":"2020-05-08T18:04:35Z",
  "id":625943483,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0MzQ4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:04:35Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @trickarcher  for code, ideas, test.",
  "created_at":"2020-05-08T18:05:51Z",
  "id":625944063,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0NDA2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:05:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/246) to add @trickarcher! :tada:",
  "created_at":"2020-05-08T18:05:58Z",
  "id":625944116,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0NDExNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:05:58Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @Ellipse0934  for code, ideas, test.",
  "created_at":"2020-05-08T18:10:38Z",
  "id":625946082,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0NjA4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:10:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/247) to add @Ellipse0934! :tada:",
  "created_at":"2020-05-08T18:10:46Z",
  "id":625946150,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0NjE1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:10:46Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @veprbl for bug, code, infra, test.",
  "created_at":"2020-05-08T18:13:01Z",
  "id":625947094,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0NzA5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:13:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/248) to add @veprbl! :tada:",
  "created_at":"2020-05-08T18:13:09Z",
  "id":625947145,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0NzE0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:13:09Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @glass-ships for code, ideas, test, talk.",
  "created_at":"2020-05-08T18:15:04Z",
  "id":625947876,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0Nzg3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:15:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/249) to add @glass-ships! :tada:",
  "created_at":"2020-05-08T18:15:13Z",
  "id":625947937,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0NzkzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:15:13Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @EscottC for code, ideas, test, talk.",
  "created_at":"2020-05-08T18:18:45Z",
  "id":625949434,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0OTQzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:18:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/250) to add @EscottC! :tada:",
  "created_at":"2020-05-08T18:18:53Z",
  "id":625949485,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk0OTQ4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:18:53Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @masonproffitt for bug, code, ideas, test, talk.",
  "created_at":"2020-05-08T18:25:07Z",
  "id":625952186,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk1MjE4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:25:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/251) to add @masonproffitt! :tada:",
  "created_at":"2020-05-08T18:25:15Z",
  "id":625952250,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk1MjI1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:25:15Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @mhedges for code, ideas, question, test, talk.",
  "created_at":"2020-05-08T18:29:27Z",
  "id":625954020,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk1NDAyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:29:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/252) to add @mhedges! :tada:",
  "created_at":"2020-05-08T18:29:39Z",
  "id":625954115,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk1NDExNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:29:39Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @guitargeek code, ideas, test.",
  "created_at":"2020-05-08T18:32:32Z",
  "id":625955333,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk1NTMzMw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-05-08T18:32:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/253) to add @guitargeek! :tada:",
  "created_at":"2020-05-08T18:32:40Z",
  "id":625955395,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk1NTM5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:32:40Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @Jayd-1234 for code.",
  "created_at":"2020-05-08T18:50:50Z",
  "id":625963436,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2MzQzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:50:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/254) to add @Jayd-1234! :tada:",
  "created_at":"2020-05-08T18:51:01Z",
  "id":625963518,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2MzUxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:51:01Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @benkrikler for code.",
  "created_at":"2020-05-08T18:53:17Z",
  "id":625964471,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2NDQ3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:53:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/255) to add @benkrikler! :tada:",
  "created_at":"2020-05-08T18:53:25Z",
  "id":625964541,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2NDU0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:53:25Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @bfis for code.",
  "created_at":"2020-05-08T18:54:30Z",
  "id":625964984,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2NDk4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:54:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/256) to add @bfis! :tada:",
  "created_at":"2020-05-08T18:54:37Z",
  "id":625965034,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2NTAzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:54:37Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @douglasdavis for code.",
  "created_at":"2020-05-08T18:55:49Z",
  "id":625965568,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2NTU2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:55:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/257) to add @douglasdavis! :tada:",
  "created_at":"2020-05-08T18:55:57Z",
  "id":625965619,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2NTYxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:55:57Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @jpata for ideas.",
  "created_at":"2020-05-08T18:59:19Z",
  "id":625967094,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2NzA5NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-05-08T18:59:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/258) to add @jpata! :tada:",
  "created_at":"2020-05-08T18:59:28Z",
  "id":625967158,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2NzE1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T18:59:28Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @martindurant for ideas.",
  "created_at":"2020-05-08T19:01:41Z",
  "id":625968099,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2ODA5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T19:01:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/259) to add @martindurant! :tada:",
  "created_at":"2020-05-08T19:01:49Z",
  "id":625968159,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2ODE1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T19:01:49Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"This PR doesn't actually make any changes. It's just a place to invoke the all-contributors bot.\r\n\r\nClosing it now.",
  "created_at":"2020-05-08T19:14:58Z",
  "id":625973598,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk3MzU5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T19:14:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @gordonwatts for ideas.",
  "created_at":"2020-05-08T20:27:51Z",
  "id":626005928,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNjAwNTkyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T20:27:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/260) to add @gordonwatts! :tada:",
  "created_at":"2020-05-08T20:28:03Z",
  "id":626006023,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNjAwNjAyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T20:28:03Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to merge this before adding any other contributors to be sure that it works in its long-term mode (adding to an existing list, rather than branching from a master that doesn't have the list and then having to merge them).",
  "created_at":"2020-05-08T14:33:36Z",
  "id":625845029,
  "issue":239,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg0NTAyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T14:33:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors generate",
  "created_at":"2020-05-08T15:17:13Z",
  "id":625865672,
  "issue":241,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg2NTY3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:17:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI could not determine your intention.\n\nBasic usage: @all-contributors please add @someone for code, doc and infra\n\nFor other usages see the [documentation](https://allcontributors.org/docs/en/bot/usage)",
  "created_at":"2020-05-08T15:17:15Z",
  "id":625865679,
  "issue":241,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTg2NTY3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T15:17:15Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thank you! Now I just have to produce something :)",
  "created_at":"2020-05-08T19:03:39Z",
  "id":625968988,
  "issue":259,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNTk2ODk4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-08T19:03:39Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - it looks like the Identities get truncated...\r\n```python\r\n        content1 = awkward1.layout.NumpyArray(numpy.array([1, 2, 3, 4, 5]))\r\n        content2 = awkward1.layout.NumpyArray(numpy.array([1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9]))\r\n        offsets = awkward1.layout.Index64(numpy.array([0, 3, 3, 5, 6, 9]))\r\n        listoffsetarray = awkward1.layout.ListOffsetArray64(offsets, content2)\r\n    \r\n        recordarray = awkward1.layout.RecordArray({\"one\": content1, \"two\": listoffsetarray})\r\n        recordarray2 = awkward1.layout.RecordArray({\"outer\": awkward1.layout.RegularArray(recordarray, 1)})\r\n        recordarray2.setidentities()\r\n        assert recordarray2[\"outer\"].identities.fieldloc == [(0, \"outer\")]\r\n>       assert recordarray2[\"outer\", 0, \"one\"].identities.fieldloc == [(0, \"outer\"), (1, \"one\")]\r\nE       AssertionError: assert [(0, 'outer')] == [(0, 'outer'), (1, 'one')]\r\nE         Right contains one more item: (1, 'one')\r\nE         Full diff:\r\nE         - [(0, 'outer')]\r\nE         + [(0, 'outer'), (1, 'one')]\r\n\r\n```\r\nHere carry index is:\r\n```xml\r\n<Index64 i=\"[0]\" offset=\"0\" length=\"1\" at=\"0x7fa648d80fa0\"/>\r\n```\r\n\r\nthat gives the following IndexedArray as an output of the `RecordArray::carry`\r\n```xml\r\n<IndexedArray64>\r\n    <Identities32 ref=\"7\" fieldloc=\"[(0, 'outer')]\" width=\"2\" offset=\"0\" length=\"1\" at=\"0x7f92edc07d70\"/>\r\n    <index><Index64 i=\"[0]\" offset=\"0\" length=\"1\" at=\"0x7f92edc04990\"/></index>\r\n    <content><RecordArray>\r\n        <Identities32 ref=\"7\" fieldloc=\"[(0, 'outer')]\" width=\"2\" offset=\"0\" length=\"5\" at=\"0x7f92edc06af0\"/>\r\n        <field index=\"0\" key=\"one\">\r\n            <NumpyArray format=\"l\" shape=\"5\" data=\"1 2 3 4 5\" at=\"0x7f92eda05b70\">\r\n                <Identities32 ref=\"7\" fieldloc=\"[(0, 'outer') (1, 'one')]\" width=\"2\" offset=\"0\" length=\"5\" at=\"0x7f92edc06af0\"/>\r\n            </NumpyArray>\r\n        </field>\r\n        <field index=\"1\" key=\"two\">\r\n            <ListOffsetArray64>\r\n                <Identities32 ref=\"7\" fieldloc=\"[(0, 'outer') (1, 'two')]\" width=\"2\" offset=\"0\" length=\"5\" at=\"0x7f92edc06af0\"/>\r\n                <offsets><Index64 i=\"[0 3 3 5 6 9]\" offset=\"0\" length=\"6\" at=\"0x7f92eda06bd0\"/></offsets>\r\n                <content><NumpyArray format=\"d\" shape=\"9\" data=\"1.1 2.2 3.3 4.4 5.5 6.6 7.7 8.8 9.9\" at=\"0x7f92eda046e0\">\r\n                    <Identities64 ref=\"8\" fieldloc=\"[(0, 'outer') (1, 'two')]\" width=\"3\" offset=\"0\" length=\"9\" at=\"0x7f92edc073a0\"/>\r\n                </NumpyArray></content>\r\n            </ListOffsetArray64>\r\n        </field>\r\n    </RecordArray></content>\r\n</IndexedArray64>\r\n```\r\nHere is what it should have been before:\r\n```xml\r\n<RecordArray>\r\n    <Identities32 ref=\"7\" fieldloc=\"[(0, 'outer')]\" width=\"2\" offset=\"0\" length=\"1\" at=\"0x7fa648d7fd30\"/>\r\n    <field index=\"0\" key=\"one\">\r\n        <NumpyArray format=\"l\" shape=\"1\" data=\"3\" at=\"0x7fa648d7fa40\">\r\n            <Identities32 ref=\"7\" fieldloc=\"[(0, 'outer') (1, 'one')]\" width=\"2\" offset=\"0\" length=\"1\" at=\"0x7fa648d839f0\"/>\r\n        </NumpyArray>\r\n    </field>\r\n    <field index=\"1\" key=\"two\">\r\n        <ListArray64>\r\n            <Identities32 ref=\"7\" fieldloc=\"[(0, 'outer') (1, 'two')]\" width=\"2\" offset=\"0\" length=\"1\" at=\"0x7fa648d83c20\"/>\r\n            <starts><Index64 i=\"[3]\" offset=\"0\" length=\"1\" at=\"0x7fa648d7fa30\"/></starts>\r\n            <stops><Index64 i=\"[5]\" offset=\"0\" length=\"1\" at=\"0x7fa648d83b40\"/></stops>\r\n            <content><NumpyArray format=\"d\" shape=\"9\" data=\"1.1 2.2 3.3 4.4 5.5 6.6 7.7 8.8 9.9\" at=\"0x7fa648d4e5a0\">\r\n                <Identities64 ref=\"8\" fieldloc=\"[(0, 'outer') (1, 'two')]\" width=\"3\" offset=\"0\" length=\"9\" at=\"0x7fa648d653c0\"/>\r\n            </NumpyArray></content>\r\n        </ListArray64>\r\n    </field>\r\n</RecordArray>\r\n```",
  "created_at":"2020-05-12T14:55:57Z",
  "id":627397425,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNzM5NzQyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-12T15:41:15Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, I'd need to modify the high-level Python functions:\r\n```python\r\n    def test_listarray():\r\n        one = awkward1.Array([[{\"x\": 1}, {\"x\": 2}, {\"x\": 3}], [], [{\"x\": 4}, {\"x\": 5}]], check_valid=True)\r\n        two = awkward1.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]], check_valid=True)\r\n>       assert awkward1.to_list(awkward1.with_field(one, two, \"y\")) == [[{\"x\": 1, \"y\": 1.1}, {\"x\": 2, \"y\": 2.2}, {\"x\": 3, \"y\": 3.3}], [], [{\"x\": 4, \"y\": 4.4}, {\"x\": 5, \"y\": 5.5}]]\r\n\r\ntests/test_0107-assign-fields-to-records.py:80: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nawkward1/operations/structure.py:453: in with_field\r\n    out = awkward1._util.broadcast_and_apply(\r\nawkward1/_util.py:714: in broadcast_and_apply\r\n    out = apply(broadcast_pack(inputs, isscalar), 0)\r\nawkward1/_util.py:570: in apply\r\n    outcontent = apply(nextinputs, depth + 1)\r\nawkward1/_util.py:614: in apply\r\n    outcontent = apply(nextinputs, depth + 1)\r\nawkward1/_util.py:449: in apply\r\n    return apply([x if not isinstance(x, indexedtypes) else x.project()\r\nawkward1/_util.py:449: in apply\r\n```\r\n```python\r\n>   if (any(isinstance(x, listtypes) for x in inputs) and\r\n        not any(isinstance(x, (awkward1.layout.Content,\r\n                               awkward1.layout.Record)) and\r\n                x.has_virtual_form for x in inputs)):\r\nE               RecursionError: maximum recursion depth exceeded while calling a Python object\r\n\r\nawkward1/_util.py:401: RecursionError\r\n\r\n```",
  "created_at":"2020-06-16T15:59:04Z",
  "id":644855350,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDg1NTM1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T15:59:04Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know where the recursion error is, but yes, it should be investigated. Does that test example work in master or is it somehow related to this PR? (If the latter, it's an assumption that broke about what to expect from a given function.)\r\n\r\nOh! It could be `IndexedArray::project`, which should return something with one less level of indexing. On the Python side, `broadcast_and_apply` unwraps arrays to get down to the NumpyArray level (the leaves of the tree) and it unwraps IndexedArrays with `project`. If `project` on the C++ side is calling `carry` on its argument and now `RecordArray::carry` returns a new IndexedArray, then that unwrapping step is oscillating between unwrap and rewrap. `IndexedArray::project`, at least, should eagerly carry, so maybe `carry` needs an eagerness argument to let `IndexedArray::project` force an eager projection in `RecordArray::carry`, whereas it's lazy in most other cases.\r\n\r\nIf that's it, then the bug (bad assumption) manifested itself on the Python side, but can/should be fixed on the C++ side.",
  "created_at":"2020-06-16T16:43:47Z",
  "id":644882058,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDg4MjA1OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-06-16T16:43:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I don't know where the recursion error is, but yes, it should be investigated. Does that test example work in master or is it somehow related to this PR? (If the latter, it's an assumption that broke about what to expect from a given function.)\r\n> \r\n> Oh! It could be `IndexedArray::project`, which should return something with one less level of indexing. On the Python side, `broadcast_and_apply` unwraps arrays to get down to the NumpyArray level (the leaves of the tree) and it unwraps IndexedArrays with `project`. If `project` on the C++ side is calling `carry` on its argument and now `RecordArray::carry` returns a new IndexedArray, then that unwrapping step is oscillating between unwrap and rewrap. `IndexedArray::project`, at least, should eagerly carry, so maybe `carry` needs an eagerness argument to let `IndexedArray::project` force an eager projection in `RecordArray::carry`, whereas it's lazy in most other cases.\r\n> \r\n> If that's it, then the bug (bad assumption) manifested itself on the Python side, but can/should be fixed on the C++ side.\r\n\r\n@jpivarski - thanks! An eager projection has fixed this issue.",
  "created_at":"2020-06-17T08:13:34Z",
  "id":645225778,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NTIyNTc3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-17T08:13:34Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - Here is where we stand so far. The 10 tests that cause a Segmentation fault are commented out, the 3 identity tests give different results - please, see in the commit. One test has been modified to allow a different type:\r\n```python\r\nassert isinstance(a, (awkward1.layout.RecordArray, awkward1.layout.IndexedArray64))\r\n```",
  "created_at":"2020-06-17T09:00:13Z",
  "id":645249218,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NTI0OTIxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-17T09:00:13Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"What specifically do you want me to check?\r\n\r\nThe Python change you made is very likely right: the output that used to always be RecordArray can now be IndexedArray (whenever the carry is lazy, which is almost always: `IndexedArray::project` being an exception). I guess it would always be IndexedArray64 because new (output) arrays are always 64-bit.",
  "created_at":"2020-06-17T10:33:19Z",
  "id":645293932,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NTI5MzkzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-17T10:33:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> What specifically do you want me to check?\r\n> \r\n> The Python change you made is very likely right: the output that used to always be RecordArray can now be IndexedArray (whenever the carry is lazy, which is almost always: `IndexedArray::project` being an exception). I guess it would always be IndexedArray64 because new (output) arrays are always 64-bit.\r\n\r\n@jpivarski - Please, review the PR when you have time. I think, I'm done with it for the moment: all but identity tests run fine. I'm not sure what to do with truncated identities and identity fields. Also, I might have overdone on eager carry in `IndexedArray::getitem_* `",
  "created_at":"2020-06-17T13:00:57Z",
  "id":645358792,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NTM1ODc5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-17T13:00:57Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> FYI, to have a clean build I just picked up a master and tried to build it on my Mac. There is a failure in the test\r\n\r\nThis is an error _in_ Matplotlib, something unrelated to the thing we're testing (colors?), and is probably due to a bad Matplotlib installation. It's an optional test\u2014if you don't have Matplotlib installed, it will be skipped. (I do have Matplotlib installed, and the test runs fine; CI does not and skips it.)\r\n\r\nIf you don't need Matplotlib, you can uninstall your copy and the test will be skipped. Otherwise, you probably want to fix your installation, anyway. If you do have Matplotlib and it works in other contexts, then I don't know what's going on.\r\n\r\nI usually install these things with Conda (exclusively\u2014I switched over about a year ago with a new computer). It manages binary versions of all your packages so that you always have a compatible set (which is why it has to be exclusive\u2014it was hard to switch over from apt-get without starting fresh). The downside of this is that installing one little package sometimes means upgrading 20 others and downgrading 5, but at least they all work together.",
  "created_at":"2020-06-19T11:40:05Z",
  "id":646589885,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjU4OTg4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-19T11:40:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > FYI, to have a clean build I just picked up a master and tried to build it on my Mac. There is a failure in the test\r\n> \r\n> This is an error _in_ Matplotlib, something unrelated to the thing we're testing (colors?), and is probably due to a bad Matplotlib installation. It's an optional test\u2014if you don't have Matplotlib installed, it will be skipped. (I do have Matplotlib installed, and the test runs fine; CI does not and skips it.)\r\n> \r\n> If you don't need Matplotlib, you can uninstall your copy and the test will be skipped. Otherwise, you probably want to fix your installation, anyway. If you do have Matplotlib and it works in other contexts, then I don't know what's going on.\r\n> \r\n> I usually install these things with Conda (exclusively\u2014I switched over about a year ago with a new computer). It manages binary versions of all your packages so that you always have a compatible set (which is why it has to be exclusive\u2014it was hard to switch over from apt-get without starting fresh). The downside of this is that installing one little package sometimes means upgrading 20 others and downgrading 5, but at least they all work together.\r\n\r\nThanks, Jim! Yes, I've done that. It's just I haven't seen this kind of failure before and thought I'd let you know.",
  "created_at":"2020-06-19T12:36:20Z",
  "id":646612251,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjYxMjI1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-19T12:36:20Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> It's just I haven't seen this kind of failure before and thought I'd let you know.\r\n\r\nThanks!",
  "created_at":"2020-06-19T12:41:10Z",
  "id":646614224,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjYxNDIyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-19T12:41:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - Unrelated to this PR - it looks like there is an initialisation problem somewhere. My guess, it's an \"offset\" value causing an out of range access. It happens very rarely and the tests would pass just fine on subsequent runs. \r\n\r\nHere is where I've seen it locally:\r\n```python\r\n       array4 = array3.rpad(3, 1)\r\n        assert awkward1.to_list(array4) == [\r\n            [2.2, 1.1, 3.3],\r\n            [None, None, None],\r\n            [4.4, 5.5, None],\r\n            [5.5, None, None],\r\n            [-4.4, -5.5, -6.6]]\r\n    \r\n        array5 = array4.sort(0, True, False)\r\n>       assert awkward1.to_list(array5.sort(0, True, False)) == [\r\n            [-4.4, -5.5, -6.6],\r\n            [2.2, 1.1, 3.3],\r\n            [4.4, 5.5, None],\r\n            [5.5, None, None],\r\n            [None, None, None]]\r\n\r\ntests/test_0074-argsort-and-sort.py:177: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nawkward1/operations/convert.py:403: in to_list\r\n    return [to_list(x) for x in array]\r\nawkward1/operations/convert.py:403: in <listcomp>\r\n    return [to_list(x) for x in array]\r\nawkward1/operations/convert.py:403: in to_list\r\n    return [to_list(x) for x in array]\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n.0 = <Iterator at=\"3\">\r\n    <IndexedOptionArray64>\r\n        <index><Index64 i=\"[2 6 9]\" offset=\"6\" length=\"3\" at=\"0x7ff894544...\" data=\"-4.4 2.2 4.4 5.5 -5.5 1.1 5.5 -6.6 3.3\" at=\"0x7ff894545040\"/></content>\r\n    </IndexedOptionArray64>\r\n</Iterator>\r\n\r\n>   return [to_list(x) for x in array]\r\nE   ValueError: in IndexedOptionArray64 attempting to get 2, index[i] >= len(content)\r\n\r\nawkward1/operations/convert.py:403: ValueError\r\n\r\n```\r\nand in the build of this PR on MacOS/Python 2.7:\r\n```python\r\n>       assert awkward1.to_list(array.argsort(0, True, False)) == [\r\n            [    3,    1,    2 ],\r\n            [    0,    0,    0 ],\r\n            [    1, None,    1 ],\r\n            [    2, None, None ],\r\n            [ None, None, None ]]\r\n\r\ntests/test_0074-argsort-and-sort.py:122: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/Users/runner/hostedtoolcache/Python/2.7.18/x64/lib/python2.7/site-packages/awkward1/operations/convert.py:403: in to_list\r\n    return [to_list(x) for x in array]\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\narray = <IndexedOptionArray64>\r\n    <index><Index64 i=\"[3 -1 9]\" offset=\"9\" length=\"3\" ...ata=\"3 0 1 2 1 0 2 0 1\" at=\"0x7fa0ecf3f2f0\"/></content>\r\n</IndexedOptionArray64>\r\n\r\n>           return [to_list(x) for x in array]\r\nE           ValueError: in IndexedOptionArray64 attempting to get 2, index[i] >= len(content)\r\n\r\n```",
  "created_at":"2020-06-22T15:48:24Z",
  "id":647608388,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NzYwODM4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-22T15:48:24Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Unrelated to this PR - it looks like there is an initialisation problem somewhere. My guess, it's an \"offset\" value causing an out of range access. It happens very rarely and the tests would pass just fine on subsequent runs.\r\n\r\nRare bugs are the worst. If it is related to an offset, you may be able to coerce it to always happen by doing a test on a slice: `somearray[2:]` introduces an offset `2` into one of the Indexes. You just have to be careful then that you don't do another operation that invokes a copy, effectively eliminating the offset of `2` (new copies have offsets of `0`). Note that slices like `somearray[:, 2:]` do a copy, since the first dimension might be jagged.",
  "created_at":"2020-06-22T16:06:47Z",
  "id":647618764,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NzYxODc2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-22T16:06:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski -\r\n**Edited:** it's ` ...:                 depth_limit=1)  `    not  `...:                 depthlimit=1) `     \r\n<strike> I'm running into some problems (see at the end of the log). Perhaps, you could have a look and suggest what should I do. I'm using the master branch and following the https://github.com/jpivarski/2020-04-08-eic-jlab/blob/master/2020-04-08-eic-jlab-EVALUATED.ipynb workbook.</strike>\r\n```python\r\nIn [61]: print(awkward1.__version__)                                                                                                         \r\n0.2.25\r\n\r\n```\r\n<strike>Most of</strike>All  the instructions work fine (with a correction of the `depth_limit`):\r\n```python\r\nIn [3]: import uproot                                                                                                                        \r\n\r\nIn [4]: file = uproot.open(\"../../PR204/awkward-1.0/open_charm_18x275_10k.root\")                                                             \r\n\r\nIn [5]: tree = file[\"events/tree\"]                                                                                                           \r\n\r\nIn [6]: evt_id, evt_prt_count = tree.arrays([\"evt_id\", \"evt_prt_count\"], outputtype=tuple)                                                   \r\n\r\nIn [7]: tree.array(\"evt_id\", entrystart=500, entrystop=600)                                                                                  \r\nOut[7]: \r\narray([500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\r\n       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\r\n       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\r\n       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\r\n       552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\r\n       565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\r\n       578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\r\n       591, 592, 593, 594, 595, 596, 597, 598, 599], dtype=uint64)\r\n\r\nIn [8]: # Make a cache with an acceptable limit. \r\n   ...: gigabyte_cache = uproot.ArrayCache(\"1 GB\") \r\n   ...:  \r\n   ...: # Read the array from disk: \r\n   ...: tree.array(\"evt_id\", cache=gigabyte_cache) \r\n   ...:  \r\n   ...: # Get the array from the cache: \r\n   ...: tree.array(\"evt_id\", cache=gigabyte_cache)                                                                                           \r\nOut[8]: array([   0,    1,    2, ..., 9997, 9998, 9999], dtype=uint64)\r\n\r\nIn [9]: %%timeit \r\n   ...:  \r\n   ...: tree.arrays(\"*\") \r\n   ...:  \r\n   ...:                                                                                                                                      \r\n534 ms \u00b1 6.81 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [10]: %%timeit \r\n    ...:  \r\n    ...: tree.arrays(\"*\", cache=gigabyte_cache) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n2.53 ms \u00b1 46.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [11]: len(gigabyte_cache)                                                                                                                 \r\nOut[11]: 52\r\n\r\nIn [12]: gigabyte_cache.clear()                                                                                                              \r\n\r\nIn [13]: len(gigabyte_cache)                                                                                                                 \r\nOut[13]: 0\r\n\r\nIn [14]: for arrays in tree.iterate(\"evt_*\", entrysteps=1000): \r\n    ...:     print({name: len(array) for name, array in arrays.items()}) \r\n    ...:                                                                                                                                     \r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n{b'evt_id': 1000, b'evt_true_q2': 1000, b'evt_true_x': 1000, b'evt_true_y': 1000, b'evt_true_w2': 1000, b'evt_true_nu': 1000, b'evt_has_dis_info': 1000, b'evt_prt_count': 1000, b'evt_weight': 1000}\r\n\r\nIn [15]: for arrays in tree.iterate(\"evt_*\", entrysteps=\"100 kB\"): \r\n    ...:     print({name: len(array) for name, array in arrays.items()}) \r\n    ...:                                                                                                                                     \r\n{b'evt_id': 1576, b'evt_true_q2': 1576, b'evt_true_x': 1576, b'evt_true_y': 1576, b'evt_true_w2': 1576, b'evt_true_nu': 1576, b'evt_has_dis_info': 1576, b'evt_prt_count': 1576, b'evt_weight': 1576}\r\n{b'evt_id': 1576, b'evt_true_q2': 1576, b'evt_true_x': 1576, b'evt_true_y': 1576, b'evt_true_w2': 1576, b'evt_true_nu': 1576, b'evt_has_dis_info': 1576, b'evt_prt_count': 1576, b'evt_weight': 1576}\r\n{b'evt_id': 1576, b'evt_true_q2': 1576, b'evt_true_x': 1576, b'evt_true_y': 1576, b'evt_true_w2': 1576, b'evt_true_nu': 1576, b'evt_has_dis_info': 1576, b'evt_prt_count': 1576, b'evt_weight': 1576}\r\n{b'evt_id': 1576, b'evt_true_q2': 1576, b'evt_true_x': 1576, b'evt_true_y': 1576, b'evt_true_w2': 1576, b'evt_true_nu': 1576, b'evt_has_dis_info': 1576, b'evt_prt_count': 1576, b'evt_weight': 1576}\r\n{b'evt_id': 1576, b'evt_true_q2': 1576, b'evt_true_x': 1576, b'evt_true_y': 1576, b'evt_true_w2': 1576, b'evt_true_nu': 1576, b'evt_has_dis_info': 1576, b'evt_prt_count': 1576, b'evt_weight': 1576}\r\n{b'evt_id': 1576, b'evt_true_q2': 1576, b'evt_true_x': 1576, b'evt_true_y': 1576, b'evt_true_w2': 1576, b'evt_true_nu': 1576, b'evt_has_dis_info': 1576, b'evt_prt_count': 1576, b'evt_weight': 1576}\r\n{b'evt_id': 544, b'evt_true_q2': 544, b'evt_true_x': 544, b'evt_true_y': 544, b'evt_true_w2': 544, b'evt_true_nu': 544, b'evt_has_dis_info': 544, b'evt_prt_count': 544, b'evt_weight': 544}\r\n\r\nIn [16]: for arrays in tree.iterate(\"evt_id\", entrysteps=\"100 kB\"): \r\n    ...:     print({name: len(array) for name, array in arrays.items()}) \r\n    ...:                                                                                                                                     \r\n{b'evt_id': 10000}\r\n\r\n```\r\n\r\n```python\r\nIn [29]: import awkward1 as ak                                                                                                               \r\n\r\nIn [30]: ak.from_awkward0(tree.array(\"pdg\"))                                                                                                 \r\nOut[30]: <Array [[11, 211, -211, -321, ... 130, 22, 22]] type='10000 * var * int64'>\r\n\r\nIn [31]: arrays = {name: ak.from_awkward0(array) for name, array in tree.arrays(namedecode=\"utf-8\").items()} \r\n    ...: arrays                                                                                                                              \r\nOut[31]: \r\n{'evt_id': <Array [0, 1, 2, 3, ... 9996, 9997, 9998, 9999] type='10000 * uint64'>,\r\n 'evt_true_q2': <Array [0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0, 0] type='10000 * float64'>,\r\n 'evt_true_x': <Array [0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0, 0] type='10000 * float64'>,\r\n 'evt_true_y': <Array [0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0, 0] type='10000 * float64'>,\r\n 'evt_true_w2': <Array [0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0, 0] type='10000 * float64'>,\r\n 'evt_true_nu': <Array [0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0, 0] type='10000 * float64'>,\r\n 'evt_has_dis_info': <Array [0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0, 0] type='10000 * int8'>,\r\n 'evt_prt_count': <Array [51, 26, 27, 28, 30, ... 37, 42, 25, 11] type='10000 * uint64'>,\r\n 'evt_weight': <Array [0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0, 0] type='10000 * float64'>,\r\n 'id': <Array [[12, 79, 80, 81, ... 54, 60, 61, 62]] type='10000 * var * uint64'>,\r\n 'pdg': <Array [[11, 211, -211, -321, ... 130, 22, 22]] type='10000 * var * int64'>,\r\n 'trk_id': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * float64'>,\r\n 'charge': <Array [[-1, 1, -1, -1, 1, ... -1, 1, 0, 0, 0]] type='10000 * var * float64'>,\r\n 'dir_x': <Array [[0.0276, 0.248, ... -0.215, -0.124]] type='10000 * var * float64'>,\r\n 'dir_y': <Array [[-0.159, -0.112, ... 0.243, 0.0141]] type='10000 * var * float64'>,\r\n 'dir_z': <Array [[0.987, -0.962, ... -0.946, -0.992]] type='10000 * var * float64'>,\r\n 'p': <Array [[3.26, 2.2, 2.29, ... 0.0956, 3.04]] type='10000 * var * float64'>,\r\n 'px': <Array [[-0.516, -0.246, ... -0.0205, -0.377]] type='10000 * var * float64'>,\r\n 'py': <Array [[0.0892, 0.548, ... 0.0232, 0.043]] type='10000 * var * float64'>,\r\n 'pz': <Array [[3.21, -2.11, ... -0.0904, -3.02]] type='10000 * var * float64'>,\r\n 'tot_e': <Array [[3.46, 2.25, 2.25, ... 0.109, 3.03]] type='10000 * var * float64'>,\r\n 'm': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * float64'>,\r\n 'time': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * float64'>,\r\n 'is_beam': <Array [[False, False, ... False, False]] type='10000 * var * bool'>,\r\n 'is_stable': <Array [[False, False, ... False, False]] type='10000 * var * bool'>,\r\n 'gen_code': <Array [[False, False, ... False, False]] type='10000 * var * bool'>,\r\n 'mother_id': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * uint64'>,\r\n 'mother_second_id': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * uint64'>,\r\n 'has_pol_info': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * float64'>,\r\n 'pol_x': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * float64'>,\r\n 'pol_y': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * float64'>,\r\n 'pol_z': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * float64'>,\r\n 'has_vtx_info': <Array [[True, True, True, ... True, True]] type='10000 * var * bool'>,\r\n 'vtx_id': <Array [[0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0]] type='10000 * var * uint64'>,\r\n 'vtx_x': <Array [[-0.000263, -0.0298, ... -0.0445]] type='10000 * var * float64'>,\r\n 'vtx_y': <Array [[0.008, -0.0308, ... -0.0137, -0.0137]] type='10000 * var * float64'>,\r\n 'vtx_z': <Array [[0.00735, 0.0124, ... -0.366, -0.366]] type='10000 * var * float64'>,\r\n 'vtx_t': <Array [[0, 2.48e-10, ... 0.371, 0.371]] type='10000 * var * float64'>,\r\n 'has_smear_info': <Array [[True, True, True, ... True, True]] type='10000 * var * bool'>,\r\n 'smear_has_e': <Array [[True, False, False, ... True, True]] type='10000 * var * bool'>,\r\n 'smear_has_p': <Array [[True, True, True, ... False, False]] type='10000 * var * bool'>,\r\n 'smear_has_pid': <Array [[False, False, ... False, False]] type='10000 * var * bool'>,\r\n 'smear_has_vtx': <Array [[True, True, True, ... False, False]] type='10000 * var * bool'>,\r\n 'smear_has_any_eppid': <Array [[True, True, True, ... True, True]] type='10000 * var * bool'>,\r\n 'smear_orig_tot_e': <Array [[3.24, 2.25, 2.25, ... 0.0956, 3.04]] type='10000 * var * float64'>,\r\n 'smear_orig_p': <Array [[3.24, 2.24, 2.25, ... 0.0956, 3.04]] type='10000 * var * float64'>,\r\n 'smear_orig_px': <Array [[0.0893, 0.557, ... -0.0205, -0.377]] type='10000 * var * float64'>,\r\n 'smear_orig_py': <Array [[-0.517, -0.251, ... 0.0232, 0.043]] type='10000 * var * float64'>,\r\n 'smear_orig_pz': <Array [[3.2, -2.16, ... -0.0904, -3.02]] type='10000 * var * float64'>,\r\n 'smear_orig_vtx_x': <Array [[0, -3.8e-12, ... -0.0445, -0.0445]] type='10000 * var * float64'>,\r\n 'smear_orig_vtx_y': <Array [[0, 7.25e-12, ... -0.0137, -0.0137]] type='10000 * var * float64'>,\r\n 'smear_orig_vtx_z': <Array [[0, -2.34e-10, ... -0.366, -0.366]] type='10000 * var * float64'>}\r\n\r\nIn [32]: %%timeit -n1 -r1 \r\n    ...:  \r\n    ...: vtx_dist = [] \r\n    ...: for xs, xy, xz in zip(arrays[\"vtx_x\"], arrays[\"vtx_y\"], arrays[\"vtx_z\"]): \r\n    ...:     out = [] \r\n    ...:     for x, y, z in zip(xs, xy, xz): \r\n    ...:         out.append(np.sqrt(x**2 + y**2 + z**2)) \r\n    ...:     vtx_dist.append(out) \r\n    ...:                                                                                                                                     \r\n\r\n1min 19s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\r\n```\r\n",
  "created_at":"2020-06-26T09:00:21Z",
  "id":650071886,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDA3MTg4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T10:31:05Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"For the failed unit tests, update Numba. The new minimum version of Numba is 0.50, because I've been in communication with them about fixing their \"entry points\" (which we rely on), and they fixed it on Numba 0.50. If you have any pip-installed versions of Awkward, try uninstalling them (and maybe even installing and uninstalling a new version of Awkward from PyPI using pip, to ensure that the bad entry points are removed: entry points are a package-level thing). Specifically, the tests are failing because we now rely on entry points to declare Awkward-Numba types when Numba is loaded, not when Awkward is loaded, and with the wrong combination, those types never get defined and the tests fail.",
  "created_at":"2020-06-26T10:38:03Z",
  "id":650111694,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDExMTY5NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-06-26T10:38:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"For `depthlimit` disappearing\u2014actually, it's just the spelling that's changed: `depthlimit` \u2192 `depth_limit`. Before the first public demo, I poked the community about names, and there was a strong, widespread opinion that words like this should be separated with underscores. I've found and changed most public-facing ones (it matters less what words we use internally), but there are a few left. There's an open issue to fix `keep_something` (I can't remember the second word). Word pairs that we have for consistency with NumPy are an exception: NumPy had a `keepdims`, for insurance, so we use exactly the same spelling they do.",
  "created_at":"2020-06-26T10:43:02Z",
  "id":650113649,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDExMzY0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T10:43:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> For `depthlimit` disappearing\u2014actually, it's just the spelling that's changed: `depthlimit` \u2192 `depth_limit`. Before the first public demo, I poked the community about names, and there was a strong, widespread opinion that words like this should be separated with underscores. I've found and changed most public-facing ones (it matters less what words we use internally), but there are a few left. There's an open issue to fix `keep_something` (I can't remember the second word). Word pairs that we have for consistency with NumPy are an exception: NumPy had a `keepdims`, for insurance, so we use exactly the same spelling they do.\r\n\r\nThanks @jpivarski!",
  "created_at":"2020-06-26T10:45:13Z",
  "id":650114462,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDExNDQ2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T10:45:13Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> For the failed unit tests, update Numba. The new minimum version of Numba is 0.50, because I've been in communication with them about fixing their \"entry points\" (which we rely on), and they fixed it on Numba 0.50. If you have any pip-installed versions of Awkward, try uninstalling them (and maybe even installing and uninstalling a new version of Awkward from PyPI using pip, to ensure that the bad entry points are removed: entry points are a package-level thing). Specifically, the tests are failing because we now rely on entry points to declare Awkward-Numba types when Numba is loaded, not when Awkward is loaded, and with the wrong combination, those types never get defined and the tests fail.\r\n\r\nHas anyone looking into updating the build to add a warning about it?",
  "created_at":"2020-06-26T10:48:33Z",
  "id":650115655,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDExNTY1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T10:48:33Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Has anyone looking into updating the build to add a warning about it?\r\n\r\nNow that I think about it, you should have been hit by this:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/62d79bc3b8b2f38c472856e7b884f36b1eb95076/src/awkward1/_connect/_numba/__init__.py#L29-L36\r\n\r\nwhich would have given you a bunch of messages like\r\n\r\n```\r\nawkward1 can only work with numba 0.50 or later (you have version 0.49)\r\n```\r\n\r\nI wonder why that didn't happen\u2014you do have the latest Python code because you merged with master.\r\n\r\nBeyond that, I expect that most of the users will be starting in the future (Awkward 0 has a big userbase; Awkward 1 does not, yet), and for them, the minimum Numba will always be 0.50.",
  "created_at":"2020-06-26T11:04:52Z",
  "id":650121496,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDEyMTQ5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T11:04:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > Has anyone looking into updating the build to add a warning about it?\r\n> \r\n> Now that I think about it, you should have been hit by this:\r\n> \r\n> https://github.com/scikit-hep/awkward-1.0/blob/62d79bc3b8b2f38c472856e7b884f36b1eb95076/src/awkward1/_connect/_numba/__init__.py#L29-L36\r\n> \r\n> which would have given you a bunch of messages like\r\n> \r\n> ```\r\n> awkward1 can only work with numba 0.50 or later (you have version 0.49)\r\n> ```\r\n> \r\n> I wonder why that didn't happen\u2014you do have the latest Python code because you merged with master.\r\n> \r\n> Beyond that, I expect that most of the users will be starting in the future (Awkward 0 has a big userbase; Awkward 1 does not, yet), and for them, the minimum Numba will always be 0.50.\r\n\r\nHmm... I'll look into it.",
  "created_at":"2020-06-26T11:07:39Z",
  "id":650122540,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDEyMjU0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T11:07:39Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - please, see the benchmark results on MacOS 10.15.5 (19F101) 2.6 GHz 6-Core Intel Core i7\r\n\r\nAwkward array version 0.2.25:\r\n```python\r\nIn [72]: %%timeit -n1000 -r1 \r\n    ...: pairs = ak.cartesian([events.pions, events.protons], with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n67.4 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1000 loops each)\r\n\r\nIn [80]: %%timeit -n100 -r1 \r\n    ...: pairs = ak.combinations(events.pions, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n356 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 100 loops each)\r\n\r\nIn [83]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid], 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n830 ms \u00b1 14.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [85]: evts = events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid]                                                            \r\n\r\nIn [86]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(evts, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n551 ms \u00b1 2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [94]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid], 2, with_name=\"pair\") \r\n    ...: pizero = pizero_candidates[pizero_candidates.mass(0, 0) - 0.13498 < 0.000001] \r\n    ...: pizero[\"px\"] = pizero.slot0.px + pizero.slot1.px \r\n    ...: pizero[\"py\"] = pizero.slot0.py + pizero.slot1.py \r\n    ...: pizero[\"pz\"] = pizero.slot0.pz + pizero.slot1.pz \r\n    ...: pizero[\"p\"] = np.sqrt(pizero.px**2 + pizero.py**2 + pizero.pz**2) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n2.24 s \u00b1 36.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\nAwkward array version 0.2.25 with this PR:\r\n```python\r\nIn [38]: %%timeit -n1000 -r1  \r\n    ...: pairs = ak.cartesian([events.pions, events.protons], with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n24.5 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1000 loops each)\r\n\r\nIn [43]: %%timeit -n100 -r1 \r\n    ...: pairs = ak.combinations(events.pions, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n44.2 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 100 loops each)\r\n\r\nIn [46]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid], 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n655 ms \u00b1 104 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [47]: evts = events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid]                                                            \r\n\r\nIn [48]: %%timeit  \r\n    ...: pizero_candidates = ak.combinations(evts, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n50.6 ms \u00b1 7.86 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [66]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid], 2, with_name=\"pair\") \r\n    ...: pizero = pizero_candidates[pizero_candidates.mass(0, 0) - 0.13498 < 0.000001] \r\n    ...: pizero[\"px\"] = pizero.slot0.px + pizero.slot1.px \r\n    ...: pizero[\"py\"] = pizero.slot0.py + pizero.slot1.py \r\n    ...: pizero[\"pz\"] = pizero.slot0.pz + pizero.slot1.pz \r\n    ...: pizero[\"p\"] = np.sqrt(pizero.px**2 + pizero.py**2 + pizero.pz**2) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n2.5 s \u00b1 160 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n```\r\n",
  "created_at":"2020-06-26T13:45:50Z",
  "id":650188142,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDE4ODE0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T14:04:53Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - please, see the benchmark results on MacOS 10.15.5 (19F101) 2.6 GHz 6-Core Intel Core i7\r\n\r\nThese look great!\r\n\r\nThe PR accomplished its goal of speeding up operations on big records:\r\n\r\n   * Cartesian product with big records (pions and protons): 2.7\u00d7\r\n   * Combinations with big records (pions): 8\u00d7\r\n   * Combinations with big records (photons): 11\u00d7 (with the much slower photon selection factored out)\r\n\r\nThe reason it was a fix to `RecordArray::carry`, rather than `ak.cartesian` and `ak.combinations`, was so that performance on non-records wouldn't be worsened. But since these weren't changed, they can't be any faster or slower than they were.\r\n\r\nI looked into the one failing test, but I couldn't see what was going on there. I just [triggered a test on master](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2247&view=results) to see if it's some new bug that's crept in due to dependencies changing under us or something.\r\n\r\nI'll review the actual code soon. Thanks!",
  "created_at":"2020-06-26T14:15:38Z",
  "id":650202919,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDIwMjkxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T14:15:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > @jpivarski - please, see the benchmark results on MacOS 10.15.5 (19F101) 2.6 GHz 6-Core Intel Core i7\r\n> \r\n> These look great!\r\n> \r\n> The PR accomplished its goal of speeding up operations on big records:\r\n> \r\n> * Cartesian product with big records (pions and protons): 2.7\u00d7\r\n> * Combinations with big records (pions): 8\u00d7\r\n> * Combinations with big records (photons): 11\u00d7 (with the much slower photon selection factored out)\r\n> \r\n> The reason it was a fix to `RecordArray::carry`, rather than `ak.cartesian` and `ak.combinations`, was so that performance on non-records wouldn't be worsened. But since these weren't changed, they can't be any faster or slower than they were.\r\n> \r\n> I looked into the one failing test, but I couldn't see what was going on there. I just [triggered a test on master](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2247&view=results) to see if it's some new bug that's crept in due to dependencies changing under us or something.\r\n> \r\n> I'll review the actual code soon. Thanks!\r\n\r\nThanks, @jpivarski ! Here is the price we pay (`length = 10000`):\r\n\r\nBefore:\r\n```python\r\nIn [97]: arr = ak.Array([[_ for _ in range(length)]])                                                                                        \r\n\r\nIn [98]: %%timeit         #TEST 3 \r\n    ...: x = ak.combinations(arr, 2) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n828 ms \u00b1 11.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [99]: %%timeit         #TEST 4 \r\n    ...: x = ak.combinations(arr, 2) \r\n    ...: ak.sum(x) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n907 ms \u00b1 19.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n\r\n```\r\nvs with this PR:\r\n```python\r\nIn [73]: arr = ak.Array([[_ for _ in range(length)]])                                                                                        \r\n\r\nIn [74]: %%timeit         #TEST 3 \r\n    ...: x = ak.combinations(arr, 2) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n2.04 s \u00b1 175 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [75]: %%timeit         #TEST 4 \r\n    ...: x = ak.combinations(arr, 2) \r\n    ...: ak.sum(x) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n2.06 s \u00b1 197 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n\r\n```",
  "created_at":"2020-06-26T14:20:31Z",
  "id":650205118,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDIwNTExOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T14:20:31Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like a factor of 2 cost, but I don't know why there would be any cost. Maybe it's something else? Nevertheless, I'll be looking at the code soon.",
  "created_at":"2020-06-26T14:25:11Z",
  "id":650207416,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDIwNzQxNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-06-26T14:25:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I looked into the one failing test, but I couldn't see what was going on there. I just [triggered a test on master](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2247&view=results) to see if it's some new bug that's crept in due to dependencies changing under us or something.\r\n\r\nThe master branch is passing, so I [started another on this branch](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2248&view=results).",
  "created_at":"2020-06-26T14:26:56Z",
  "id":650208239,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDIwODIzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T14:26:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - Moving back to an eager carry for all except for `ak.combinations` improves its performance, however, there is no improvement for `ak.cartesian`\r\n```python\r\nIn [19]: %%timeit \r\n    ...: pairs = ak.cartesian([events.pions, events.protons]) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n69.7 ms \u00b1 2.83 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [20]: %%timeit -n100 -r1  \r\n    ...: pairs = ak.combinations(events.pions, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n12.8 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 100 loops each)\r\n\r\nIn [21]: evts = events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid]                                                            \r\n\r\nIn [22]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(evts, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n12.5 ms \u00b1 213 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\n```\r\non the plus side,  there is no performance degradation here:\r\n```python\r\nIn [25]: arr = ak.Array([[_ for _ in range(length)]])                                                                                        \r\n\r\nIn [26]: %%timeit \r\n    ...: x = ak.combinations(arr, 2) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n812 ms \u00b1 15.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [27]: %%timeit \r\n    ...: x = ak.combinations(arr, 2) \r\n    ...: ak.sum(x) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n879 ms \u00b1 27.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n```",
  "created_at":"2020-06-28T12:37:06Z",
  "id":650747234,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDc0NzIzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-28T12:37:06Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - finally, everything looks reasonable here:\r\n```python\r\nIn [15]: %%timeit  \r\n    ...: pairs = ak.cartesian([events.pions, events.protons])  \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n14.6 ms \u00b1 624 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [16]: %%timeit -n100 -r1  \r\n    ...: pairs = ak.combinations(events.pions, 2, with_name=\"pair\")  \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n9.44 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 100 loops each)\r\n\r\nIn [17]: evts = events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid]                                                            \r\n\r\nIn [18]: %%timeit  \r\n    ...: pizero_candidates = ak.combinations(evts, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n12.4 ms \u00b1 140 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\nand here:\r\n```python\r\nIn [5]: length=10000                                                                                                                         \r\n\r\nIn [6]: arr = ak.Array([[_ for _ in range(length)]])                                                                                         \r\n\r\nIn [7]: %%timeit \r\n   ...: x = ak.combinations(arr, 2) \r\n   ...: ak.sum(x) \r\n   ...:  \r\n   ...:                                                                                                                                      \r\n873 ms \u00b1 8.04 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [8]: %%timeit \r\n   ...: x = ak.combinations(arr, 2) \r\n   ...:  \r\n   ...:                                                                                                                                      \r\n814 ms \u00b1 11.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```",
  "created_at":"2020-06-29T15:50:49Z",
  "id":651206666,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MTIwNjY2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-29T15:50:49Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> It looks like a factor of 2 cost, but I don't know why there would be any cost. Maybe it's something else? Nevertheless, I'll be looking at the code soon.\r\n\r\nIt was a cost of a dynamic cast put in place as a workaround. It\u2019s not needed any more and was removed. ",
  "created_at":"2020-06-30T07:07:48Z",
  "id":651592447,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MTU5MjQ0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-30T07:07:48Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, the PR is ready for a review :-)",
  "created_at":"2020-06-30T07:09:44Z",
  "id":651593483,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MTU5MzQ4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-30T07:09:44Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Now I just want to get a handle on the ways `allow_lazy` are used. The parameter is passed down in only 5 classes:\r\n\r\n   * VirtualArray\r\n   * UnmaskedArray\r\n   * BitMaskedArray\r\n   * ByteMaskedArray\r\n   * RegularArray\r\n\r\nIs there a reason to single out these 5? I just answered my own question: these are the only 5 classes for whom `carry` is not a shallow operation\u2014if any other classes passed `carry` down to their contents, they, too, would pass `allow_lazy` rather than setting it to be a particular value. The one exception to this is that `RecordArray::carry` is not shallow when `allow_lazy = false`, but then forcing `allow_lazy = false` when recursing in `RecordArray::carry` is a tautology.\r\n\r\nFortunately, all the other times that `carry` is called, the `true` or `false` is on the same line (I checked). So all the `trues` are:\r\n\r\n```\r\n% fgrep -rn 'carry(' src | fgrep true\r\nsrc/libawkward/array/ListOffsetArray.cpp:285:    ContentPtr nextcontent = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/ListOffsetArray.cpp:1474:      ContentPtr nextcontent = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/ListOffsetArray.cpp:1664:        contents.push_back(content_.get()->carry(Index64(ptr, 0, totallen), true));\r\nsrc/libawkward/array/ListOffsetArray.cpp:2037:    ContentPtr nextcontent = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/ListOffsetArray.cpp:2086:    ContentPtr nextcontent = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/ListOffsetArray.cpp:2145:      ContentPtr nextcontent = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/ListOffsetArray.cpp:2169:      ContentPtr nextcontent = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/UnionArray.cpp:661:      return contents[0].get()->carry(index, true);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:1039:      ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:363:      return content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:1144:        ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:1162:        ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:1283:      ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:1310:      ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:1668:      ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:1861:    ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:1929:        ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:1965:        ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:2285:      ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/IndexedArray.cpp:2306:      ContentPtr next = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/ListArray.cpp:1360:        contents.push_back(content_.get()->carry(Index64(ptr, 0, totallen), true));\r\nsrc/libawkward/array/RegularArray.cpp:230:      ContentPtr nextcontent = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/RegularArray.cpp:983:        contents.push_back(content_.get()->carry(Index64(ptr, 0, totallen), true));\r\nsrc/libawkward/array/RegularArray.cpp:1203:      ContentPtr nextcontent = content_.get()->carry(nextcarry, true);\r\nsrc/libawkward/array/RegularArray.cpp:1225:      ContentPtr nextcontent = content_.get()->carry(nextcarry, true);\r\n```\r\n\r\nand all the `falses` are\r\n\r\n```\r\n% fgrep -rn 'carry(' src | fgrep false\r\nsrc/libawkward/array/ListOffsetArray.cpp:1790:      ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListOffsetArray.cpp:1804:      outcontent = outcontent.get()->carry(outcarry, false);\r\nsrc/libawkward/array/ListOffsetArray.cpp:1945:      ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListOffsetArray.cpp:1959:      outcontent = outcontent.get()->carry(outcarry, false);\r\nsrc/libawkward/array/UnionArray.cpp:431:    return contents_[(size_t)index].get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:223:    return content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:610:      ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:659:                                             content_.get()->carry(carry, false),\r\nsrc/libawkward/array/ByteMaskedArray.cpp:712:      ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:735:      ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:944:    ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:1009:      ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:1086:    ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:1171:    ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ByteMaskedArray.cpp:1300:      ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/RecordArray.cpp:754:        contents.push_back(content.get()->carry(carry, false));\r\nsrc/libawkward/array/IndexedArray.cpp:375:      return content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/IndexedArray.cpp:2020:    ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/IndexedArray.cpp:2123:    ContentPtr next = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListArray.cpp:266:    ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListArray.cpp:1456:    ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListArray.cpp:1510:    ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListArray.cpp:1574:      ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListArray.cpp:1598:      ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListArray.cpp:1637:    ContentPtr carried = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListArray.cpp:1700:    ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/ListArray.cpp:1758:      ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/RegularArray.cpp:1093:    ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\nsrc/libawkward/array/RegularArray.cpp:1145:    ContentPtr nextcontent = content_.get()->carry(nextcarry, false);\r\n```\r\n\r\nTo be more specific, `carry` is only used with `allow_lazy = true` in the following situations:\r\n\r\n   * **A:** `ByteMaskedArray::combinations`.\r\n   * **A:** `IndexedArray::combinations`.\r\n   * **A:** `ListArray::combinations`.\r\n   * **A:** `ListOffsetArray::combinations`.\r\n   * **A:** `RegularArray::combinations`.\r\n   * **A:** In fact, every class whose `combinations` implementation calls `carry` does so with `allow_lazy = true`. Good.\r\n   * **B:** `IndexedArray::project`, but only when it's option-type. This is the minimum required to break recursion: if `broadcast_and_apply` (on the Python side) is opening up an IndexedOptionArray containing a RecordArray, the first step would turn it into an IndexedArray of RecordArray and the next step would turn it into a RecordArray (because `IndexedArray::project` is eager when not option-type). But if the default policy is to be eager, only lazily carrying in `combinations`, then perhaps `IndexedArray::project` should be simplified to only be eager like everything else.\r\n   * **B:** `IndexedArray::num` (both). Same argument.\r\n   * **B:** `IndexedArray::offsets_and_flattened` (only relevant for option-type). Same argument.\r\n   * **B:** `IndexedArray::asslice` (only relevant for option-type). Same argument, more strongly in this case: arrays used as slices do not allow indirection (for simplicity), so the laziness will be going away anyway. This case should always be eager, to save an unnecessary extra step.\r\n   * **B:** `IndexedArray::reduce_next` (both). Same argument, and also fairly strongly for this method because reducers will completely unravel an array eventually, anyway. Laziness doesn't help.\r\n   * **B:** `IndexedArray::localindex` (only relevant for option-type). Same argument.\r\n   * **B:** `IndexedArray::getitem_next_jagged_generic` (both). Same argument.\r\n   * **B:** `ListOffsetArray::reduce_next`. This one should be eager for the reasons described in `IndexedArray::reduce_next`.\r\n   * **C:** `ListOffsetArray::broadcast_tooffsets64`. This could actually be very good for performance in functions defined in Python involving broadcasting. If broadcasting is lazy on RecordArrays, then broadcasting and only accessing one or a few fields would have better performance than eagerly broadcasting all fields. It keeps the broadcasting operation shallow.\r\n   * **C:** `RegularArray::broadcast_tooffsets64`. Good: this is consistent with the above.\r\n   * **D:** `UnionArray::simplify_uniontype`. This could be good for performance.\r\n   * **E:** `IndexedArray::getitem_next(all)` (both option-type and non-option-type). It could be useful for `getitem_next` methods to be lazy, but that should be consistent for all classes.\r\n   * **E:** `ListOffsetArray::getitem_next(at, range, array)`. Same as above.\r\n   * **E:** `RegularArray::getitem_next(at, range)` are eager and `RegularArray::getitem_next(array)` is lazy. The inconsistency is probably not a good thing.\r\n\r\nIn the next commit, I'm going to make all the `IndexedArray` implementations except `IndexedArray::combinations` and `IndexedArray::getitem_next` be eager, as well as `ListOffsetArray::reduce_next` (all the **B** cases above).",
  "created_at":"2020-06-30T16:18:56Z",
  "id":651899658,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MTg5OTY1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-30T16:18:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Of the `getitem_next` (cases **E** above), only the following are relevant (non-trivial implementation that calls `carry`):\r\n\r\n   * `ByteMaskedArray:getitem_next` (centralized method). Currently, it uses **eager carry**.\r\n   * `RegularArray::getitem_next` (decentralized: looking at `SliceAt`, `SliceRange`, `SliceArray`). Currently, it uses **eager carry**.\r\n   * `IndexedArray::getitem_next` (centralized). Currently, it uses **lazy carry**.\r\n   * `ListOffsetArray::getitem_next` (decentralized). Currently, it uses **lazy carry**.\r\n   * `ListArray::getitem_next` (decentralized). Currently, it uses **eager carry**.\r\n\r\nI'm going to try making all get-items of `SliceAt`, `SliceRange`, and `SliceArray` use a **lazy carry** (`allow_lazy = true`).\r\n",
  "created_at":"2020-06-30T16:36:52Z",
  "id":651909298,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MTkwOTI5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-30T16:36:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna I think it's done. Thanks for all your work on this!\r\n\r\nI may have invalidated your latest timing measurements, but probably not by much because I left the laziness on in `ak.combinations`. With laziness turned on in all cases of `broadcast_tooffsets64`, the `ak.cartesian` function (which takes two arguments, but is otherwise similar to `ak.combinations`) is now fully lazy as well, which should give it a similar performance boost as `ak.combinations`. At least, they have the same form (the `x`/`y` RecordArray below is wrapped in IndexedArray as a result of the operation\u2014no expensive carrying of wide records):\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> records = ak.Array([[{\"x\": 1.1, \"y\": 100}, {\"x\": 2.2, \"y\": 200}, {\"x\": 3.3, \"y\": 300}],\r\n...                    [],\r\n...                    [{\"x\": 4.4, \"y\": 400}, {\"x\": 5.5, \"y\": 500}]])\r\n>>> records.layout.form\r\n{\r\n    \"class\": \"ListOffsetArray64\",\r\n    \"offsets\": \"i64\",\r\n    \"content\": {\r\n        \"class\": \"RecordArray\",\r\n        \"contents\": {\r\n            \"x\": \"float64\",\r\n            \"y\": \"int64\"\r\n        }\r\n    }\r\n}\r\n>>> ak.combinations(records, 2).layout.form\r\n{\r\n    \"class\": \"ListOffsetArray64\",\r\n    \"offsets\": \"i64\",\r\n    \"content\": {\r\n        \"class\": \"RecordArray\",\r\n        \"contents\": [\r\n            {\r\n                \"class\": \"IndexedArray64\",\r\n                \"index\": \"i64\",\r\n                \"content\": {\r\n                    \"class\": \"RecordArray\",\r\n                    \"contents\": {\r\n                        \"x\": \"float64\",\r\n                        \"y\": \"int64\"\r\n                    }\r\n                }\r\n            },\r\n            {\r\n                \"class\": \"IndexedArray64\",\r\n                \"index\": \"i64\",\r\n                \"content\": {\r\n                    \"class\": \"RecordArray\",\r\n                    \"contents\": {\r\n                        \"x\": \"float64\",\r\n                        \"y\": \"int64\"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}\r\n>>> ak.cartesian([records, records]).layout.form\r\n{\r\n    \"class\": \"ListOffsetArray64\",\r\n    \"offsets\": \"i64\",\r\n    \"content\": {\r\n        \"class\": \"RecordArray\",\r\n        \"contents\": [\r\n            {\r\n                \"class\": \"IndexedArray64\",\r\n                \"index\": \"i64\",\r\n                \"content\": {\r\n                    \"class\": \"RecordArray\",\r\n                    \"contents\": {\r\n                        \"x\": \"float64\",\r\n                        \"y\": \"int64\"\r\n                    }\r\n                }\r\n            },\r\n            {\r\n                \"class\": \"IndexedArray64\",\r\n                \"index\": \"i64\",\r\n                \"content\": {\r\n                    \"class\": \"RecordArray\",\r\n                    \"contents\": {\r\n                        \"x\": \"float64\",\r\n                        \"y\": \"int64\"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}\r\n```\r\n\r\nHaving this extra handle will give us the leverage to tune performance wherever users raise the issue. Thanks!",
  "created_at":"2020-06-30T19:46:51Z",
  "id":652005854,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MjAwNTg1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-30T19:46:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thank you @jpivarski !!!",
  "created_at":"2020-06-30T20:40:18Z",
  "id":652030899,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MjAzMDg5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-30T20:40:18Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - the final numbers look great! Thanks!\r\nHere is the master:\r\n```python\r\nIn [33]: %%timeit -n1000 -r1 \r\n    ...: pairs = ak.cartesian([events.pions, events.protons], with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n7.93 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1000 loops each)\r\n\r\nIn [34]: %%timeit -n100 -r1 \r\n    ...: pairs = ak.combinations(events.pions, 2, with_name=\"pair\")  \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n16.4 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 100 loops each)\r\n\r\nIn [35]: %%timeit  \r\n    ...: pizero_candidates = ak.combinations(events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid], 2, with_name=\"pair\")  \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n269 ms \u00b1 3.66 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [36]: evts = events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid]                                                            \r\n\r\nIn [37]: %%timeit  \r\n    ...: pizero_candidates = ak.combinations(evts, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n25.1 ms \u00b1 472 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [38]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid], 2, with_name=\"pair\") \r\n    ...: pizero = pizero_candidates[pizero_candidates.mass(0, 0) - 0.13498 < 0.000001]  \r\n    ...: pizero[\"px\"] = pizero.slot0.px + pizero.slot1.px  \r\n    ...: pizero[\"py\"] = pizero.slot0.py + pizero.slot1.py  \r\n    ...: pizero[\"pz\"] = pizero.slot0.pz + pizero.slot1.pz \r\n    ...: pizero[\"p\"] = np.sqrt(pizero.px**2 + pizero.py**2 + pizero.pz**2)  \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n1.08 s \u00b1 22.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\nand before:\r\n```python\r\nIn [72]: %%timeit -n1000 -r1 \r\n    ...: pairs = ak.cartesian([events.pions, events.protons], with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n67.4 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1000 loops each)\r\n\r\nIn [80]: %%timeit -n100 -r1 \r\n    ...: pairs = ak.combinations(events.pions, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n356 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 100 loops each)\r\n\r\nIn [83]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid], 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n830 ms \u00b1 14.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [85]: evts = events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid]                                                            \r\n\r\nIn [86]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(evts, 2, with_name=\"pair\") \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n551 ms \u00b1 2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [94]: %%timeit \r\n    ...: pizero_candidates = ak.combinations(events.prt[events.prt.pdg == Particle.from_string(\"gamma\").pdgid], 2, with_name=\"pair\") \r\n    ...: pizero = pizero_candidates[pizero_candidates.mass(0, 0) - 0.13498 < 0.000001] \r\n    ...: pizero[\"px\"] = pizero.slot0.px + pizero.slot1.px \r\n    ...: pizero[\"py\"] = pizero.slot0.py + pizero.slot1.py \r\n    ...: pizero[\"pz\"] = pizero.slot0.pz + pizero.slot1.pz \r\n    ...: pizero[\"p\"] = np.sqrt(pizero.px**2 + pizero.py**2 + pizero.pz**2) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n2.24 s \u00b1 36.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\nThere is no change in here:\r\n```python\r\nIn [19]: length=10000                                                                                                                        \r\n\r\nIn [20]: arr = ak.Array([[_ for _ in range(length)]])                                                                                        \r\n\r\nIn [21]: %%timeit  \r\n    ...: x = ak.combinations(arr, 2)  \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n851 ms \u00b1 7.37 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [22]: %%timeit \r\n    ...: x = ak.combinations(arr, 2)  \r\n    ...: ak.sum(x)  \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n910 ms \u00b1 8.88 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nbefore:\r\n```python\r\nIn [97]: arr = ak.Array([[_ for _ in range(length)]])                                                                                        \r\n\r\nIn [98]: %%timeit\r\n    ...: x = ak.combinations(arr, 2) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n828 ms \u00b1 11.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [99]: %%timeit \r\n    ...: x = ak.combinations(arr, 2) \r\n    ...: ak.sum(x) \r\n    ...:  \r\n    ...:                                                                                                                                     \r\n907 ms \u00b1 19.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n",
  "created_at":"2020-07-01T07:17:35Z",
  "id":652239393,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MjIzOTM5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-01T07:17:35Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I chose to open up a PR, since it's easier to point out mistakes in a code review rather than shoving an entire piece of code somewhere. Also, it should ease the work of @reikdas, since it would take him fewer steps to reproduce the same results!",
  "created_at":"2020-05-13T14:17:27Z",
  "id":628019866,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODAxOTg2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T14:17:27Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"A draft PR is good! (And by the way, I do intend to go through your Arrow PR; I know that it's done and waiting for me.)\r\n\r\nI took a quick look at this. The Kernel class shouldn't need to be a long-lived object attached to every array node, I believe: if we just have a lightweight marker of where an array lives (e.g. `-1` for main memory; a non-negative integer for GPU id) then we can make the Kernel object on demand just as we're about to call a kernel. As described in the CONTRIBUTING, the once-per-array (even once-per-operation-on-an-array) is not where we care about performance because that doesn't scale with array size, but even still, creating a Kernel object whose only data in memory is a `typeinfo` (virtual method dispatch between CPUKernel and GPUKernel) and maybe an integer (device id in the case of GPUKernel) is extremely lightweight.\r\n\r\nThat would also free us from a lot of bookkeeping. The Kernel objects should be created on demand from the location flag (which can be wrapped in some abstract type, but for the foreseeable future, it would just be an integer).",
  "created_at":"2020-05-13T14:41:34Z",
  "id":628036450,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODAzNjQ1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T14:41:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hey, thanks for looking into this.\n\nWouldn't forming an instance of the KernelCore Class every time we need to invoke a kernel operation, bring in the problem of multiple if..else{} constructs? Please correct me if I am wrong but in my understanding, I think this was the first problem we were trying to mitigate. Also, the kernel object being long lived shouldn't pose an issue, since we aren't allocating memory anywhere in the class while invocation. So there's no issue of memory leakage, either. Again, I might be wrong here. ",
  "created_at":"2020-05-13T15:05:03Z",
  "id":628052420,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODA1MjQyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T15:05:03Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Regarding, the issue of Content Markers, I looked into it and did some research. There are multiple ways of going about it. However, I do want to discuss with @reikdas and you, about how the entire GPU backend is going to look like to a Python User and more importantly how is he going to interact with it. I think it would bring in some changes like should the Arrays itself have the flag since most of the Arrays have an Index and a Content so it would be weird if only the Content has a flag with the Index being isolated. I'll prepare a document detailing my concerns and we can discuss this on Friday. Does this sound good?\n\nPS I am sending this from my mobile, so please excuse me if there's any typos.",
  "created_at":"2020-05-13T15:11:45Z",
  "id":628056758,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODA1Njc1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T15:11:45Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Holding onto these Kernels long-term doesn't break anything in principle, but I'd like to only make `std::shared_ptr` for things that really need them. These are attached to objects that flow between C++ and Python and have to maintain combined reference counts in each. Again, I think it will work, but I only want to do it for those things that need it (the tree of nested nodes, Identities; even Indexes are copied in, rather than shared pointers, because their type is glued to the container's type).\r\n\r\nShared pointers are not trivial objects, which you can see from the fact that I pass them into argument lists with a `const &`. Every `std::shared_ptr` points to a lookup table with an atomic reference count. Using one of those tables to store one integer or less rubs me the wrong way. It's wasteful; I know that it's probably below the level of what we care about, but it's really not necessary, so I don't think we should design in that direction (and have trouble backing out of it later). Who knows? Maybe it would become a cyclic dependency issue in the future, which reference counts without a garbage collector can't deal with.\r\n\r\nAs for making them on the fly, let's say that the \"array location\" is an integer behind a `using` (i.e. modern `typedef`) or a class containing one integer with copy semantics. One function would turn the integer/class instance into the appropriate Kernel, which we use to immediately call the desired kernel, then it goes away. It could even involve a (short-lived) `std::shared_ptr`.\r\n\r\n```c++\r\nconst std::shared_ptr<Kernel> get_kernel(int memory_location) {\r\n  if (memory_location < 0) {\r\n    return std::make_shared<CPUKernel>();\r\n  }\r\n  else {\r\n    return std::make_shared<GPUKernel>(memory_location);\r\n  }\r\n}\r\n```\r\n\r\nand then at every site that calls a kernel, every `Content` has a `memory_location_` (protected, like `identities_` and `parameters_`):\r\n\r\n```c++\r\nError err = util::get_kernel(memory_location_).get()->awkward_really_long_name<T>(\r\n  lots,\r\n  of,\r\n  arguments);\r\n```\r\n\r\netc. Sure this creates the `std::shared_ptr`, but only because we need the kernel object to be a pointer because we want to use virtual dispatch. The `std::shared_ptr` with its table and atomic reference count get created, but we're not maintaining them for a long time, which is the thing that worries me about possibly unforeseen long-term issues. (Cyclic reference counts is the one that comes to mind.)\r\n\r\n**Edit:** Actually, the above example could just as easily be done with `std::unique_ptr`, which is more lightweight, since it is only being used once in one place. It does the same job of protecting us from raw pointers. The only issue is that creating the `std::unique_ptr` inside the `get_kernel` function can't use `std::make_unique`, which is a C++14 thing. (No biggie: just `std::unique_ptr<Kernel>(new CPUKernel(...))`.)\r\n\r\nIn fact, the dynamic dispatch\u2014a performance issue I said we weren't going to care about at this level\u2014can be removed by making only one Kernel class (no CPUKernel vs GPUKernel) and have it be a bag of static methods that do the choice based on `memory_location` in the static method. At the calling site, the call would be\r\n\r\n```c++\r\nError err = util::Kernel::really_long_name<T>(\r\n  memory_location_,\r\n  lots,\r\n  of,\r\n  arguments);\r\n```\r\n\r\nwith\r\n\r\n```c++\r\nError Kernel::really_long_name<int32_t>(\r\n  int64_t memory_location,\r\n  int32_t* lots,\r\n  const int32_t* of,\r\n  int64_t arguments) {\r\n  if (memory_location < 0) {\r\n    return awkward_really_long_name_32(lots, of, arguments);\r\n  }\r\n  else {\r\n    return awkwardgpu_really_long_name_32(lots, of arguments);\r\n  }\r\n}\r\n```\r\n\r\nThis also provides the possibility of passing `memory_location` into the `awkwardgpu` function, which might be necessary.\r\n\r\nIn the CPUKernel-GPUKernel system, we have to write wrapper code for each method in each of the two classes, create some kind of pointer so that a function call on it gets dynamically dispatched to the right wrapper. In a single Kernel full of static methods, no object ever gets created and we have to write one wrapper with an if statement inside each one of those wrappers.\r\n\r\nI'm not sure whether it's more arduous to write two classes full of wrappers without if statements or one class full of wrappers with if statements. The latter gives us new abilities, though: we can pass the `memory_location` down to the GPU implementation, if need be, because it has already been passed this far.\r\n\r\nBacking up a bit, the main goal here was to get the \"if CPU, else GPU\" logic out of the calling sites (in ListArray, RecordArray, and the rest), and all of these solutions do that.\r\n\r\n[\"I would have written a shorter comment if I'd had more time.\"](https://quoteinvestigator.com/2012/04/28/shorter-letter/)",
  "created_at":"2020-05-13T15:48:50Z",
  "id":628079559,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODA3OTU1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T15:48:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">Holding onto these Kernels long-term doesn't break anything in principle, but I'd like to only make std::shared_ptr for things that really need them. \r\n\r\nYes, of course. This completely slipped my head. I forgot that `std::shared_ptr` keeps a count of references. I think we could do use unique_ptr with much less overhead just as you suggested!\r\n\r\nMaking the `Kernel` Class as a Wrapper to contain `CPUKernels` and `GPUKernels` seems much better through your demonstrations. We do have access to memory location too which is great and doesn't have the runtime `dynamic dispatch` overhead. I remember reading the design document and it mentioned that we prefer fast compile time operations, so dynamic dispatch shouldn't come into picture to respect that. I think we could go with this, I'll just make the modifications and push the code and then it should be better.",
  "created_at":"2020-05-14T13:18:32Z",
  "id":628629168,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODYyOTE2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T13:35:25Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This should be done now. I have followed what @jpivarski suggested. So it's mostly what was discussed in this thread but for NumpyArray. I took `awkward_new_indentities32` and made an identical function for GPU. And since `KernelClass` houses all the `pass-through` functions, everything is `static`. We can finalize this if everything is in place.",
  "created_at":"2020-05-14T15:11:33Z",
  "id":628699624,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODY5OTYyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T15:11:33Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I am done with this PR. @jpivarski Can you verify the changes?",
  "created_at":"2020-05-19T04:12:56Z",
  "id":630567811,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDU2NzgxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-19T04:12:56Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Perhaps the current include/awkward/cpu-kernels/util.h could become include/awkward/common.h with the contents of the current src/cpu-kernels/util.cpp inline so that common.h can be header-only, and then included in all three libraries (libawkward-cpu-kernels.so, libawkward-gpu-kernels.so, and libawkward.so)?\r\n\r\nI really was aiming for the `gpu_kernels/util.h` to be drastically different from `cpu_kernels/util.h`, since that would be a nice place to dump all the `cuda` errors or warnings and generate all the error messages that the `cpu-kernels` already generate. I don't know how this will transfer to the python layer but it should be similar to what happens currently with the `cpu-kernels`. We could rename it to `CUDA_ERROR` for `gpu_kernels`. What do you think of this?\r\n\r\n> We'll also have to make the compilation of the GPU kernels dependent on whether CUDA libraries are present. What libraries, specifically, will it need? Is there a conda package that provides them? I said I'd add the pip-install switch that chooses non-GPU or GPU builds, but supposing that information is available as a CMake switch, like -DAWKWARD_GPU (is a boolean enough?) or -DAWKWARD-CUDA-PATH (a full path to some CUDA base?), how would the CMakeLists.txt take advantage of that?\r\n\r\nYeah, there are CMake Variables that handle this. Do you want this to go into the studies folder, with the other `cuda_experiments` on `memory_tracking` that  I am currently working on? So that we'll later add it to the main repository while solving the `cupy` issue?",
  "created_at":"2020-05-19T12:35:56Z",
  "id":630788764,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDc4ODc2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-19T12:35:56Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"A util.h (or maybe pick a different name because it might be a bad idea to have files with the same name, even if they're in different directories) for CUDA stuff is a good idea. But it's different from the common definitions here. I guess the conclusion is that these definitions should never have been in cpu-kernels anyway; they're more general. The totally general stuff, used by all libraries, should be a header-only thing because that will make it easiest to include in all.\r\n\r\nAny provisional studies can go in the studies directory. We somehow have to tinker with some provisional stuff and also pull it all together into a product; probably more tinkering right now.",
  "created_at":"2020-05-19T13:08:19Z",
  "id":630805227,
  "issue":262,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDgwNTIyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-19T13:08:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thank you so much for this! @jpivarski, I never knew there were so many things hidden away. I'll write better test suites from now on! Thanks again!",
  "created_at":"2020-05-14T03:12:47Z",
  "id":628360312,
  "issue":263,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODM2MDMxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T03:12:47Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Don't worry about it\u2014I don't think these issues could have been solved by \"more diligence.\" There's just a lot of things that need to be considered and it will take time to get them all in the documentation.\r\n\r\nOne thing that none of these tests would have triggered, including the ones I ported over from old Awkward, is that contents can be longer than the enclosing container needs it to be (e.g. a ListOffsetArray might have \"10\" as `offsets[-1]`, but its `content` can have a length that is longer than 10). This is valid in Awkward Array, though rare and not included in any tests, but I don't think it's valid in Arrow. So when `to_arrow` goes down to the next level with `recurse`, I've just added a slice of the content so that it's exactly the right length, all the way down. This is an example of where 100% test coverage fails: even though the tests now have 100% coverage, they would not have caught this.",
  "created_at":"2020-05-14T12:12:39Z",
  "id":628593002,
  "issue":263,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODU5MzAwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T12:12:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Wow yeah, I never tested it out with that possibility. However, strangely enough, I did catch this issue during the `to_arrow` conversion of `ByteMaskedArray`. So beyond the masks, the content array was copied `as-is`, which was not what `awkward-1.0` was doing. `awkward-1.0` stopped when the masks ran out of indices to apply masks on. So I just sliced them after the entire conversion was done.\r\nAlso, are we testing the conversions out with some large dataset?",
  "created_at":"2020-05-14T12:40:54Z",
  "id":628607263,
  "issue":263,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODYwNzI2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T12:40:54Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"> Also, are we testing the conversions out with some large dataset?\r\n\r\nI don't have plans to do so, but it wouldn't be a bad idea. However, @kyungeonchoi might end up doing this as part of ssl-hep/ServiceX#131. That will involve large datasets pretty soon.",
  "created_at":"2020-05-14T12:46:03Z",
  "id":628609975,
  "issue":263,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODYwOTk3NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-05-14T12:46:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yup, makes sense. Thanks!",
  "created_at":"2020-05-14T12:49:13Z",
  "id":628611674,
  "issue":263,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODYxMTY3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T12:49:13Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Aha! It's because some of the lists it's trying to reduce over don't exist at the level that you're looking (you have some empty lists at `axis=1` and are reducing over `axis=2`). This is indeed a bug.\r\n\r\nFurthermore, it only happens if the empty list at `axis=1` is _last_. Here's a simpler example (I use prime numbers and `ak.prod` because it's easier to see what it's grouping; booleans are hard to follow by eye).\r\n\r\n```python\r\n>>> ak.prod(ak.Array([[[2, 3, 5]], [[7], [11]], [[]]]), axis=-1)\r\n<Array [[30], [7, 11], [1]] type='3 * var * int64'>\r\n>>> ak.prod(ak.Array([[[2, 3, 5]], [[7], [11]], []]), axis=-1)\r\nTraceback (most recent call last):\r\n...\r\nValueError: in ListOffsetArray64 attempting to get 2, offsets[i] > offsets[i + 1]\r\n```\r\n\r\nbut\r\n\r\n```python\r\n>>> ak.prod(ak.Array([[[2, 3, 5]], [], [[7], [11]]]), axis=-1)\r\n<Array [[30], [], [7, 11]] type='3 * var * int64'>\r\n```\r\n\r\nis okay and\r\n\r\n```python\r\n>>> ak.prod(ak.Array([[], [[2, 3, 5]], [[7], [11]]]), axis=-1)\r\n<Array [[], [30], [7, 11]] type='3 * var * int64'>\r\n```\r\n\r\nis okay. I'll look into it.",
  "created_at":"2020-05-14T16:07:37Z",
  "id":628734148,
  "issue":264,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODczNDE0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T16:07:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Trimming the offending last element makes it work again, so it's not a problem that the empty list is somewhere in memory; it's a problem that it's in the range that's considered:\r\n\r\n```python\r\n>>> ak.prod(ak.Array([[[2, 3, 5]], [[7], [11]], []])[:-1], axis=-1)\r\n<Array [[30], [7, 11]] type='2 * var * int64'>\r\n```\r\n\r\nIn your example, trimming the last two gets you back to having an empty `axis=1` list last. If, in your example, you only trim off enough to leave the last `axis=1` list nonempty, it works:\r\n\r\n```python\r\n>>> ak.any(a[:-1], axis=2)\r\n<Array [[], [True], [], [False, True]] type='4 * var * bool'>\r\n```\r\nwhich, as I read it, is the right answer.\r\n\r\nIt looks like only one bug: the last-empty bug.",
  "created_at":"2020-05-14T16:13:34Z",
  "id":628737490,
  "issue":264,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODczNzQ5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T16:13:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Better yet, the exception does not occur in the reduction step; it happens in the print-out because the reduction step produced invalid output. We can inspect these by looking at the layouts:\r\n\r\n```python\r\n>>> ak.prod(ak.Array([[[2, 3, 5]], [[7], [11]], [[]]]), axis=-1).layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 1 3 4]\" offset=\"0\" length=\"4\" at=\"0x563bfcc0fd00\"/></offsets>\r\n    <content><NumpyArray format=\"l\" shape=\"4\" data=\"30 7 11 1\" at=\"0x563bfcc058d0\"/></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\nfor the good one and\r\n\r\n```python\r\n>>> ak.prod(ak.Array([[[2, 3, 5]], [[7], [11]], []]), axis=-1).layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 1 0 3]\" offset=\"0\" length=\"4\" at=\"0x563bfcbd8150\"/></offsets>\r\n    <content><NumpyArray format=\"l\" shape=\"3\" data=\"30 7 11\" at=\"0x563bfcb2dce0\"/></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\nfor the bad one. The reduction step made the wrong offsets. It should be `[0 1 3 3]`.",
  "created_at":"2020-05-14T17:22:30Z",
  "id":628776685,
  "issue":264,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODc3NjY4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-14T17:22:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's fixed in PR #265 and I'll merge with master soon. A few more things are going to go into the next version before I deploy, however.\r\n\r\nFor your original example,\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> a = ak.from_iter([[], [[False, False, True]], [], [[False], [True, False]], []])\r\n>>> ak.any(a, axis=2)\r\n<Array [[], [True], [], [False, True], []] type='5 * var * bool'>\r\n>>> ak.any(a[:-1], axis=2)\r\n<Array [[], [True], [], [False, True]] type='4 * var * bool'>\r\n>>> ak.any(a[:-2], axis=2)\r\n<Array [[], [True], []] type='3 * var * bool'>\r\n```",
  "created_at":"2020-05-14T17:43:27Z",
  "id":628787225,
  "issue":264,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODc4NzIyNQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-05-14T17:43:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Great start!",
  "created_at":"2020-05-15T19:07:37Z",
  "id":629429403,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTQyOTQwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T19:07:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Maybe just put the LICENSE comment line at the top of dev/genpython.py.\r\n\r\nDone.\r\n\r\n> What does the generated documentation look like? You could attach some screenshots to the GitHub comments.\r\n\r\n![image](https://user-images.githubusercontent.com/11775615/84102934-51284580-aa2f-11ea-93a5-667c98fd1baf.png)\r\n",
  "created_at":"2020-06-09T03:28:36Z",
  "id":641006102,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MTAwNjEwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-09T03:28:36Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I like the choice of putting it last in the left-bar, since it is the most internal of internal interfaces. It should probably be called \"kernels,\" rather than \"cpukernels,\" and get some text at the beginning explaining what this is all about, but that can come later. There's no bulleted-list-as-table-of-contents, but I had forgotten that the left-bar expands when you open one of these, so everything that we need is there.\r\n\r\nWhen a function has several type variants, like `awkward_ListArray_min_range`, how do you find out that the corresponding real names are\r\n\r\n   * `awkward_ListArray32_min_range`\r\n   * `awkward_ListArrayU32_min_range`\r\n   * `awkward_ListArray64_min_range`\r\n\r\nAre they in a bulleted list below the screenshot you showed? They could be given as three blue signatures below the heading. Then you wouldn't need the `for C in int` which is wrong (can't be evaluated; `int` isn't iterable; you probably intended it for cases like `for C in (bool, int, float)` but it came out wrong in this case).\r\n\r\nIf it makes things any easier, you could just pretend that `int32_t`, `uint32_t`, `int64_t` are Python types, as though someone did this:\r\n\r\n```python\r\n>>> int32_t = int\r\n>>> uint32_t = int\r\n>>> int64_t = int\r\n```\r\n\r\nWhen we evaluate them for tests, we can put that in the preamble.\r\n\r\nThe requirements are (a) we have to be able to look up the real names of the function, such as `awkward_ListArray32_min_range`, because that's what ctypes sees, (b) the Python code needs to be executable, because we'll want to use exactly the same code in the tests, to demonstrate that these really are the definitions, and (c) type-variants of the same function (defined in terms of the same templated code, internally) need to be grouped somehow.\r\n\r\nIt would be enough to make it look like this:\r\n\r\n```\r\nawkward_ListArray_min_range\r\n===========================\r\n\r\n.. py:function:: awkward_ListArray32_min_range(tomin: List[int64_t], fromstarts: List[int32_t], fromstops: List[int32_t], lenstarts: int64_t, startsoffset: int64_t, stopsoffset: int64_t)\r\n\r\n.. py:function:: awkward_ListArrayU32_min_range(tomin: List[int64_t], fromstarts: List[uint32_t], fromstops: List[uint32_t], lenstarts: int64_t, startsoffset: int64_t, stopsoffset: int64_t)\r\n\r\n.. py:function:: awkward_ListArray64_min_range(tomin: List[int64_t], fromstarts: List[int64_t], fromstops: List[int64_t], lenstarts: int64_t, startsoffset: int64_t, stopsoffset: int64_t)\r\n\r\n.. code-block:: python\r\n\r\n    def awkward_ListArray_min_range(\r\n        tomin,\r\n        fromstarts,\r\n        fromstops,\r\n        lenstarts,\r\n        startsoffset,\r\n        stopsoffset\r\n    ):\r\n        shorter = fromstops[stopsoffset + 0] - fromstarts[startsoffset + 0]\r\n        for i in range(lenstarts):\r\n            range = fromstops[stopsoffset + i] - fromstarts[startsoffset + i]\r\n            shorter = (shorter < range) if shorter else range\r\n        tomin[0] = shorter\r\n\r\n    awkward_ListArray32_min_range = awkward_ListArray_min_range\r\n    awkward_ListArrayU32_min_range = awkward_ListArray_min_range\r\n    awkward_ListArray64_min_range = awkward_ListArray_min_range\r\n```\r\n\r\n(Or some other simple way to indicate that the three type variants correspond to the one implementation, without a lot of verbiage.)\r\n\r\nA few things I found in typing this out: (1) there's a terrible bug in the original function: `startsoffset` was used in `fromstops` and `stopsoffset` was used in `fromstarts`. The bug never caused an error because it's likely that the offsets have always been zero, but it's a ticking time bomb. Can you please fix this (in another PR, if you want to keep it separate from this work) and search for cases where offsets are used with the wrong arrays? They are _always_ linked by name; you'll always be able to tell. If you fix that and all tests continue to pass, then we're nevertheless in a much better state because these bugs will reveal themselves as segfaults, and would be hard to debug.\r\n\r\nThe second thing is (2) the `tomin` assignment must be `tomin[0] = shorter`. Otherwise, Python can't export the value of `tomin` from the function. In the signatures, you're saying it's a `List`, let's take that seriously and use it that way because it's the only way to translate the assignment-through-pointer of C into Python. There aren't any `&tomin` instances in the codebase\u2014explicit pointers are used throughout, so that should make it easier.\r\n\r\n(3) This while loop wasn't translated into a for loop. I don't see any reason why it can't be\u2014it has simple structure.\r\n\r\n(4) Python builtin function names like `range` should be given the same treatment as keywords like `from`: we should avoid them in these code blocks because they're needed for things like making for loops. You don't have to look up a whole list of builtins to avoid, fixing `range` should be enough for now.\r\n\r\nSorry I found so many things to fix! I wanted to merge this and move on to testing, but some of these things will bite us in the tests. (It's probably a good hint that the tests should include non-zero offsets, to find any other bugs like the one above.)",
  "created_at":"2020-06-09T13:02:34Z",
  "id":641276123,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MTI3NjEyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-09T13:02:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> It should probably be called \"kernels,\" rather than \"cpukernels,\"\r\n\r\nDone.\r\n\r\nI made the functions having various template type variants look like this - \r\n![image](https://user-images.githubusercontent.com/11775615/84233001-1695da00-ab0f-11ea-8da5-7b0fa8998c73.png)\r\n\r\n![image](https://user-images.githubusercontent.com/11775615/84232795-a9824480-ab0e-11ea-80b1-af7ab0d4f3b2.png)\r\nWhat do you think?\r\n\r\n> The second thing is (2) the tomin assignment must be tomin[0] = shorter\r\n\r\nI made all `*x = y` assignments into `x[0] = y`. \r\n\r\n> (3) This while loop wasn't translated into a for loop. \r\n\r\nDone.\r\n\r\n> (4) Python builtin function names like range should be given the same treatment as keywords like from: we should avoid them in these code blocks because they're needed for things like making for loops. \r\n\r\nDone.",
  "created_at":"2020-06-10T06:16:50Z",
  "id":641749706,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MTc0OTcwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-10T06:33:31Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"I know I said earlier that integer types should all be `int` in Python, but we have to distinguish the integer specializations, so revert to `int32_t`, `uint32_t`, `int64_t`. It's a matter of removing the code that maps all of them to `int`. Same for `float` vs `double`. We can pretend that these symbols have been defined in Python, as that's easy to do in the testing code.\r\n\r\nThe signatures are all on the specialized functions (in blue), so they don't need to be generalized in the implementation, which means that you don't need a `C` or a `C = int`. See my code example.\r\n\r\nAfter that, the documentation is perfect.\r\n\r\nBut then there's still the issue of the wrong offsets being used, discovered by looking at this documentation, but independent of it. That can be another PR, but a sweep through all kernels, looking for this type of mistake, needs to be done.",
  "created_at":"2020-06-10T11:31:19Z",
  "id":641941708,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MTk0MTcwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-10T11:31:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"![image](https://user-images.githubusercontent.com/11775615/84429369-a0e05a00-ac45-11ea-86a7-65ea62da8dfe.png)\r\n@jpivarski What do you think?\r\n\r\nThere is specialized code in genpython.py to deal with functions having dynamic memory arrays like https://github.com/scikit-hep/awkward-1.0/blob/master/src/cpu-kernels/operations.cpp#L3307. I'll remove that code(to make the cpu-kernels check stricter) when I refactor the code to remove the dynamic size allocation.",
  "created_at":"2020-06-11T19:16:30Z",
  "id":642878848,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0Mjg3ODg0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-11T19:19:42Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"This looks great! I'd merge it now if you're ready. Are you?",
  "created_at":"2020-06-11T20:06:00Z",
  "id":642900967,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MjkwMDk2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-11T20:06:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This looks great! I'd merge it now if you're ready. Are you?\r\n\r\nYes :) ",
  "created_at":"2020-06-12T05:16:02Z",
  "id":643069685,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzA2OTY4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-12T05:16:02Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Obviously, the tests fail, but in numba/numba#5717 I describe how this fixes our issue and this PR will definitely go in when Numba 0.50 is available.",
  "created_at":"2020-05-18T13:55:59Z",
  "id":630199518,
  "issue":270,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDE5OTUxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T13:55:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and I'll merge this as soon as the test passes, unless you have an objection (that is, if you're still working on it).",
  "created_at":"2020-05-18T14:20:54Z",
  "id":630214801,
  "issue":271,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDIxNDgwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T14:20:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> It probably doesn't have consequences outside of pycparser.\r\n\r\nIn that case should I close this PR and add this commit to #269 ?\r\n\r\n> Are you able to write to the scikit-hep/awkward-1.0 repo? Making PRs directly on the repo simplifies the process of me committing to your branch, if I ever need to do that (i.e. to make a suggestion). It's worth testing to see that the machinery is in place.\r\n\r\nDid you mean make a branch on the scikit-hep/awkward-1.0 repo? - that is what I have done here.\r\n\r\n> Oh, and I'll merge this as soon as the test passes, unless you have an objection (that is, if you're still working on it).\r\n\r\nI am done with this :) ",
  "created_at":"2020-05-18T14:27:05Z",
  "id":630219002,
  "issue":271,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDIxOTAwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T14:27:05Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> Did you mean make a branch on the scikit-hep/awkward-1.0 repo? - that is what I have done here.\r\n\r\nAh, it is! Okay, I was confused. It's a `/` between your name and the `parse-cast`, not a `:`.\r\n\r\nI'm merging this.",
  "created_at":"2020-05-18T14:30:37Z",
  "id":630221332,
  "issue":271,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDIyMTMzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T14:30:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Alternatively, or in addition, a helper function like `VirtualArray.withlength` could be provided to make a copy of a virtual array node but with the length provided in the case where it was not available previously.",
  "created_at":"2020-05-18T14:47:29Z",
  "id":630232192,
  "issue":272,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDIzMjE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T14:47:29Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"No problem (on both). I should get to that this afternoon.",
  "created_at":"2020-05-18T14:50:14Z",
  "id":630233990,
  "issue":272,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDIzMzk5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T14:50:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've been on the fence about this because all the other attributes that attach to an `ak.Array` have to be attached _carefully_ (by directly accessing the `__dict__` or calling the superclass's `__getattr__` or `__getattribute__` or something; I'd have to look it up and experiment).\r\n\r\nI've asked this question when presenting Awkward, whether the argument for `x.d = d` is strong enough to warrant the potential for issues like, \"Oops, now we can't add new features to `ak.Array`,\" or \"Oops, now exceptions that happen inside the assignment (like `d` doesn't broadcast to `x`) are hidden, or a wrong error message is raised.\" Something very much like that happened with `__getattr__` until we found a way to navigate around all the consequences. In that case, the argument for `__getattr__` access was strong enough to do it.\r\n\r\nAlso something to consider: Pandas _used to_ have this ability and now they've deprecated it and raise warnings if users attempt it.\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [1.1, 2.2, 3.3]})\r\n>>> df\r\n   x    y\r\n0  1  1.1\r\n1  2  2.2\r\n2  3  3.3\r\n>>> df.z = [100, 200, 300]\r\n__main__:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\r\n>>> df\r\n   x    y\r\n0  1  1.1\r\n1  2  2.2\r\n2  3  3.3\r\n>>> df[\"z\"] = [100, 200, 300]\r\n>>> df\r\n   x    y    z\r\n0  1  1.1  100\r\n1  2  2.2  200\r\n2  3  3.3  300\r\n```\r\n\r\n(But they do allow `df.x is df[\"x\"]`.)\r\n\r\nI don't know if they hit some bad design issues or something. Maybe a warning is in order? I normally don't like warnings because they tend to not do what they're supposed to do\u2014they often get buried in a script with no one watching to see them, or when they are observed, they pertain to something so many steps away from what the user is doing that the warning doesn't make sense. But since Pandas does it, maybe we should too...\r\n\r\nI think I'm going to label this as a \"discussion\" because what we have to think about goes well beyond the simple matter of implementing `__setattr__`.",
  "created_at":"2020-05-18T15:57:33Z",
  "id":630276568,
  "issue":273,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDI3NjU2OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-05-18T15:57:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"A similar problem occurs when one tries to use `__setitem__` on nested structures. This is then related to where pandas gives it's `SettingWithCopy` warning which is then confusing when i actually want to use that copy. So I'm also a bit undecided if these things (both the `__setattr__` and the `__setitem__` on a copy) should show warnings, because they can indeed lead to confusion. On the other hand, if there is no perfect solution, maybe confusion is better than making unnoticed mistakes?\r\n",
  "created_at":"2020-05-20T08:29:29Z",
  "id":631324077,
  "issue":273,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMTMyNDA3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-20T08:29:29Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"While working on something else, I was reminded that if you declare that Python class has `__slots__`, then attempts to assign attributes that are not in those slots is an error.\r\n\r\n```python\r\n>>> class Something(object):\r\n...     __slots__ = [\"only\", \"allowed\", \"members\"]\r\n... \r\n>>> something = Something()\r\nAttributeError: only\r\n>>> something.only = 1\r\n>>> something.allowed = 2\r\n>>> something.members = 3\r\n>>> something.whatever = 4\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'Something' object has no attribute 'whatever'\r\n```\r\n\r\nMaybe this is the way forward on attribute-assignment. \"'Something' object has no attribute 'whatever'\" seems like a pretty clear message. I could just assign slots to ak.Array, ak.Record, and ak.ArrayBuilder (whose only real attributes, not properties, start with underscores) and then you'd get the above error message if you mistakenly try to assign to it. It would be nice, however, if the error message included an indication that you can use item-assignment instead.\r\n\r\nThen you brought up another point:\r\n\r\n> A similar problem occurs when one tries to use __setitem__ on nested structures. This is then related to where pandas gives it's SettingWithCopy warning which is then confusing when i actually want to use that copy.\r\n\r\nI hadn't thought of that, but someone could do\r\n\r\n```python\r\n>>> array.inner.structure[\"x\"] = new_field\r\n```\r\n\r\nand the new field has been added to a temporary that immediately goes away, but one might think that it is now a part of `array`. Pandas does have a `SettingWithCopy` warning\u2014which means that they are somehow able to detect this situation, and I don't know how they do it\u2014but that warning is one of the most googled Pandas questions out there. I've run into it a few times and found myself in good company.\r\n\r\nOne of the nice things about Python is that you can always do this\u2014assign a deep member of a structure and have it now be attached to the main structure\u2014and you forget how much that uncertainty plagues languages like C with both pass-by-value and pass-by-reference semantics. In Python, this intuitive situation is possible because _everything_ is pass-by-reference.\r\n\r\nIt won't work for Awkward Array because these slices might be novel objects: `array` might be nested lists of nested lists of records that have a field named `inner`, and `array.inner` projects that field to make nested lists of nested lists of just the `inner` field. NumPy's structured arrays do the same thing\u2014you can slice with integers first to pick rows and then strings to pick columns or vice-versa\u2014Awkward just generalizes that beyond number-labeled rows to jagged arrays and string-labeled columns to nested records. But it does mean that if you ask for `array.inner`, you might have just created a new thing that has never been seen before. Python's pass-by-reference won't work because there's no long-lived, preexisting object to reference.\r\n\r\nSome languages (like Haskell) take the opposite tack and only allow pass-by-value. (I was introduced to it by Scala, which takes it to less of an extreme than Haskell, but it still makes the pattern apparent.) Like being purely pass-by-reference, being purely pass-by-value avoids the confusion of allowing both. In the implementation, I took a relaxed view of being mostly immutable, but discovered along the way that most of my problems were coming from the exceptions, so pretty soon the layout nodes will be purely immutable (#176 and #177). All these confusion exceptions to mutability are therefore concentrated on the top-most ak.Array layer, where I can focus on them because they're all in one place.\r\n\r\nBut maybe ak.Array should follow suit and be immutable as well? Maybe attempts to assign shouldn't just use [ak.with_field](https://awkward-array.readthedocs.io/en/latest/_auto/ak.with_field.html) but point the user to it as what they really mean. The `array.inner.structure` example illustrates an issue: building this up would require three nested `ak.with_field` calls. Perhaps the interface of `ak.with_field` should be expanded to take a `path` that assigns a field deeply and packs everything up, giving you the `array_with_x`?\r\n\r\nBut if `ak.with_field` had a `path`, I guess we could use it in the `__setitem__` interface like this:\r\n\r\n```python\r\narray[\"inner\", \"structure\", \"x\"] = new_field\r\n```\r\n\r\nand include all sorts of warnings in the documentation that\r\n\r\n```python\r\narray[\"inner\"][\"structure\"][\"x\"] = new_field\r\n```\r\n\r\nwon't work?",
  "created_at":"2020-05-20T11:13:10Z",
  "id":631409513,
  "issue":273,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMTQwOTUxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-20T11:13:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Maybe this is the way forward on attribute-assignment. \"'Something' object has no attribute 'whatever'\" seems like a pretty clear message. I could just assign slots to ak.Array, ak.Record, and ak.ArrayBuilder (whose only real attributes, not properties, start with underscores) and then you'd get the above error message if you mistakenly try to assign to it. It would be nice, however, if the error message included an indication that you can use item-assignment instead.\r\n\r\nThis sounds good and clear.\r\n\r\nI also like the idea that `ak.with_field` gets a `path` argument and that assigning to inner elements can work with `__setitem__` with multiple arguments. Also i agree it probably is better to warn that `array[\"inner\"][\"structure\"][\"x\"] = new_field` is not working in the documentation rather than with a confusing warning like in pandas.\r\n\r\n",
  "created_at":"2020-05-20T14:50:15Z",
  "id":631523176,
  "issue":273,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMTUyMzE3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-20T14:50:15Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"I changed the title to be clear about what this feature request is requesting. I'm also going to add \"good first issue\" because this can be implemented with only a little Python meddling, not deep changes to the C++.",
  "created_at":"2020-05-20T15:32:55Z",
  "id":631550977,
  "issue":273,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMTU1MDk3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-20T15:32:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think this fixes #272; I think it's enough to have all of these accessors on the ArrayGenerator and not the VirtualArray:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/33c73c9c4e854b81b760c286431d516237508dfa/tests/test_0274-access-ArrayGenerator-in-Python.py#L13-L48\r\n\r\nThere's a strong argument for putting them on the generator, because there are two generator types accessible in Python (a Pythonized ArrayGenerator and an opaque SliceGenerator, which can't easily provide access to its slice because I have Python \u2192 C++ slice conversions but not in the other direction; that would be a big project). The `with_` methods let you change the Form and length without needing to know which type of generator you have. If it's a Pythonized ArrayGenerator, then you can change the `callable`, `args`, and `kwargs`, too.\r\n\r\nOn VirtualArray, however, there's a `generator` property that gives you the generator (regardless of its type) that you can then call `with_*` on. Having to rebuild a VirtualArray from all of its components to just replace the length might be a chunky bit of code, but it would be much easier to wrap that up in a helper function in Python than to build a full set of `with_` functions at this next level up.\r\n\r\nDoes this work for you? If so, I'll merge it immediately.",
  "created_at":"2020-05-18T17:35:41Z",
  "id":630332601,
  "issue":274,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDMzMjYwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T17:35:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Forgot to add @nsmith- to my previous comment.",
  "created_at":"2020-05-18T17:35:57Z",
  "id":630332749,
  "issue":274,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDMzMjc0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T17:35:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yeah since `length` is a generator property and not a virtual array property, I think it makes sense to have `with_length` on the generator. There is not so much work to rewrap as the virtual array properties all appear accessible already. I suppose the following is sufficient:\r\n```python\r\ndef add_length(array: awkward1.layout.VirtualArray, length: int):\r\n    return awkward1.layout.VirtualArray(\r\n        array.generator.with_length(length),\r\n        array.cache,\r\n        array.cache_key,\r\n        array.identities,\r\n        array.parameters,\r\n    )\r\n```",
  "created_at":"2020-05-18T18:56:55Z",
  "id":630375402,
  "issue":274,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDM3NTQwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T18:56:55Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I checked out this branch, and ran into another issue that is familiar from awkward0: the form check upon materialization in my parent array is not happy that the expected form has `has_length=False` for some of the children nodes where the materialized one has `has_length=True`. If I'm willing to lie to awkward and give a dummy length while making the original form then it \"works\" but I'm not sure of the implications here.",
  "created_at":"2020-05-18T19:46:36Z",
  "id":630398640,
  "issue":274,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDM5ODY0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T19:53:32Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> I checked out this branch, and ran into another issue that is familiar from awkward0: the form check upon materialization in my parent array is not happy that the expected form has `has_length=False` for some of the children nodes where the materialized one has `has_length=True`.\r\n\r\nThat, in itself, is a good argument against making the length mutable.\r\n\r\n> If I'm willing to lie to awkward and give a dummy length while making the original form then it \"works\" but I'm not sure of the implications here.\r\n\r\nThe only implication is that if it is materialized before you expect it to be, there would be an error due to the dummy length being the wrong length.\r\n\r\nI was thinking that I should weaken the constraint for you: to make `has_length` be three-valued: true, false, and \"I don't care.\" But, thinking about it, isn't \"I don't care\" the same as `has_length=False`? If you have nested VirtualArrays, the outer one can have a different `has_length` than the inner one.",
  "created_at":"2020-05-18T20:08:55Z",
  "id":630408921,
  "issue":274,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMDQwODkyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-18T20:08:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"another thing: while trying to get `__setitem__` to work for `Array`  i saw that this checks if the `where` argument is a string (to which i would add list/tuple then)\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/c11597ffaa58a46a05bee2cb36985e6e545188df/src/awkward1/highlevel.py#L882-L883\r\n\r\nWhat is the problem with integer indices? `ak.with_field` does not complain about these and it seems to work fine?",
  "created_at":"2020-05-22T12:14:22Z",
  "id":632661516,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMjY2MTUxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-22T12:14:22Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski, since you labelled my feature request #273 as \"good first issue\" i'm taking this as a broad hint to give it a try.\r\n\r\nThanks! It wasn't meant as a nudge, just an indication that I didn't think the problem runs deep.\r\n\r\n> Why is that?\r\n\r\nI had to break it down. With these small and complex objects, it also helps to slap a `.tolist()` at the end to view them without the ellipsis for big data and the type descriptor, which are just noise here.\r\n\r\nThe difference between your two cases is that the latter replaces the only field that exists. I think that by removing `\"a\"` before putting it back in, the length became zero. It shouldn't need to\u2014RecordArray takes a `length` parameter to allow non-zero length arrays of empty records:\r\n\r\n```python\r\n>>> ak.layout.RecordArray([], [], 12)\r\n<RecordArray length=\"12\">\r\n</RecordArray>\r\n>>> ak.Array(ak.layout.RecordArray([], [], 12))\r\n<Array [{}, {}, {}, {}, ... {}, {}, {}, {}] type='12 * {}'>\r\n```\r\n\r\nbut maybe the specific code in the ak.with_field implementation didn't take advantage of this. (If you see where that happens, fixing this case could be good, too. It should be pretty close to the part you're working on, in the Python implementation.)\r\n\r\nTo show that this is the case, take your example that didn't work and make the new field `\"c\"`, rather than overwriting `\"a\"`:\r\n\r\n```python\r\n>>> base = ak.zip({\"a\" : ak.zip({\"x\" : [1, 2, 3]})}, depth_limit=1)\r\n>>> base.tolist()\r\n[{'a': {'x': 1}}, {'a': {'x': 2}}, {'a': {'x': 3}}]\r\n>>> ak.with_field(base, ak.with_field(base.a, [1.1, 2.2, 3.3], \"y\"), \"c\").tolist()\r\n[{'a': {'x': 1}, 'c': {'x': 1, 'y': 1.1}}, {'a': {'x': 2}, 'c': {'x': 2, 'y': 2.2}}, {'a': {'x': 3}, 'c': {'x': 3, 'y': 3.3}}]\r\n```\r\n\r\nIt wouldn't be wrong to check to see if a field is replacing the only field that exists, and then you can just drop the old RecordArray, rather than having to carefully remove the field while keeping the length the same. There's nothing special about the old RecordArray object\u2014because they're immutable, they're created and destroyed at will. Only the high-level ak.Array wrapper has a well-defined lifespan.\r\n\r\n> What is the problem with integer indices? ak.with_field does not complain about these and it seems to work fine?\r\n\r\nMaybe it's an oversight that ak.with_field doesn't complain about integer arguments, and I don't know how it can work fine.\r\n\r\nThe `__getitem__` takes a wide variety of objects as arguments; it's a little broader than NumPy (allowing jagged and option-type arrays as indexes), and NumPy drove the Python language to include all of these in the syntax. However, that's a much different problem: as long as we're only _looking_ at elements, we can slice \"rows\" and \"columns\" in many ways.\r\n\r\nThe `__setitem__` (and therefore ak.with_field) is limited to \"columns\" because it's much more complex thinking about how to splice in \"rows\" when they're jagged or otherwise structured. Columns are much easier because the columns are not actually stored together anyway\u2014each column is a physically separate array. In the early days of Awkward0, I tried to add `__setitem__` semantics for rows and ran into problems at the level of definition\u2014not just implementation. That might be a long disussion.\r\n\r\nI'll be looking at your PR soon. One comment, though: the `__setitem__` should only accept tuples, not lists, because tuples are a path from outer to inner but lists are a group of fields of the same record. This is how it works in NumPy (for its structured arrays), too. Maybe the ak.with_field function can take any iterable as an argument because it doesn't have a prior tradition the way that `__setitem__` does.",
  "created_at":"2020-05-22T16:08:57Z",
  "id":632770515,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMjc3MDUxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-22T16:08:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> What is the problem with integer indices? `ak.with_field` does not complain about these and it seems to work fine?\r\n\r\nWhat you might have seen is that the low-level `Content::setitem_field` takes an integer when setting a column by position, rather than by name. That's needed for tuples, which are records with unnamed fields. It's not acting on rows\u2014tuple fields are columns without names.",
  "created_at":"2020-05-22T17:40:45Z",
  "id":632824161,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMjgyNDE2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-22T17:40:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I tried to implement the modifications for the type checks you suggested. I also had to add a distinction between `str` and `Iterable` at some places since `isinstance(str, Iterable)` is also `True` but have to be treated differently. Also i changed `ValueError` to `TypeError`.\r\nThe case where i replace the only field should also work now, but i wasn't entirely sure what the `broadcast_and_apply` does and if the add the behavior of `what` is added correctly in the not `highlevel=True` case.\r\n\r\n",
  "created_at":"2020-05-25T15:17:30Z",
  "id":633618501,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMzYxODUwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-25T15:17:30Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks again for the detailed explanation! I moved the `behaviour` assignment to the top of the function and added a short explanation to the `__setitem__` docstring including the note that things like `nested[\"a\", \"y\"] = new_stuff` won't work.",
  "created_at":"2020-05-26T14:45:00Z",
  "id":634070873,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNDA3MDg3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-26T14:45:00Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"This looks great\u2014when the tests pass, I'll merge it.\r\n\r\nThanks again!",
  "created_at":"2020-05-26T14:51:06Z",
  "id":634074555,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNDA3NDU1NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-05-26T14:51:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(I scanned it over carefully at the same time that you were requesting a review. Nothing looks amiss and the tests will prove that it works.)",
  "created_at":"2020-05-26T14:53:14Z",
  "id":634075856,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNDA3NTg1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-26T14:53:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add nikoladze for code.\r\n\r\nThanks!",
  "created_at":"2020-06-03T15:42:18Z",
  "id":638280639,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODI4MDYzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-03T15:42:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/287) to add @nikoladze! :tada:",
  "created_at":"2020-06-03T15:42:29Z",
  "id":638280739,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODI4MDczOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-03T15:42:29Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Not sure what is not working on windows. Google says that -1073741819 is a \"File system error\".",
  "created_at":"2020-05-25T00:23:53Z",
  "id":633322779,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMzMyMjc3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-25T00:23:53Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I checked MacOS and Windows for warnings, but didn't notice that this PR introduced [warnings into the Linux build](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2142&view=logs&j=e9afaa34-1a0f-534e-78fc-fae528ccd915&t=ab3b1ed9-7948-5ab3-1bca-f55a57df2add&l=6676):\r\n\r\n```\r\n/home/pivarski/irishep/awkward-1.0/src/libawkward/Index.cpp:311:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n   template class EXPORT_SYMBOL IndexOf<int8_t>;\r\n                                ^~~~~~~~~~~~~~~\r\n/home/pivarski/irishep/awkward-1.0/src/libawkward/Index.cpp:312:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n   template class EXPORT_SYMBOL IndexOf<uint8_t>;\r\n                                ^~~~~~~~~~~~~~~~\r\n/home/pivarski/irishep/awkward-1.0/src/libawkward/Index.cpp:313:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n   template class EXPORT_SYMBOL IndexOf<int32_t>;\r\n                                ^~~~~~~~~~~~~~~~\r\n/home/pivarski/irishep/awkward-1.0/src/libawkward/Index.cpp:314:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n   template class EXPORT_SYMBOL IndexOf<uint32_t>;\r\n                                ^~~~~~~~~~~~~~~~~\r\n/home/pivarski/irishep/awkward-1.0/src/libawkward/Index.cpp:315:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n   template class EXPORT_SYMBOL IndexOf<int64_t>;\r\n                                ^~~~~~~~~~~~~~~~\r\n```\r\n\r\n(I noticed it when compiling on my Linux machine.) There might be a difference between MacOS (or maybe Clang) and Linux (or maybe gcc) in where the type attributes are applied, which could explain why not adding `EXPORT_SYMBOL` here prevented dependent-project from working for you whereas it did for me, but then also this warning.\r\n\r\nI'm not sure what to do about it... still thinking. Maybe the right thing to do is to make this conditional (\"if Linux\" or \"if gcc\"). I might need to get Clang working on Linux to determine whether it's the compiler or the platform, but I didn't manage to do that while working on an earlier issue.",
  "created_at":"2020-05-27T11:33:30Z",
  "id":634601129,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNDYwMTEyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-27T11:33:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski I have a relevant future PR in mind, so I can take care of this then as well, if you don't mind.",
  "created_at":"2020-05-27T17:29:58Z",
  "id":634820901,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNDgyMDkwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-27T17:29:58Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I definitely don't mind! As you can probably tell, I'm a novice about these linking issues. Your improvements have already made a big difference.",
  "created_at":"2020-05-27T17:39:42Z",
  "id":634826318,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNDgyNjMxOA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-05-27T17:39:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"What if we could isolate our CUDA dependency to only be in libawkward-cuda-kernels.so and make that a completely separate project? That is, libawkward-cuda-kernels.so would be built by a different CMakeLists.txt and the main codebase would only be aware of it as a shared library that could possibly be dynamically loaded? Then it could have a different conda name (and even PyPI name) like \"awkward-cuda-kernels\". If it can be dynamically loaded, then GPU pointers can be dispatched to the appropriate kernel; if not, there's a runtime error.\r\n\r\nIt sounds to me like that would simplify things, if we really can isolate our CUDA dependency to just the libawkward-cuda-kernels.so. It means that we can't use `cudaPointerGetAttributes` to tell us if a pointer is a main memory pointer or a GPU pointer, but [maybe that's not possible anyway](https://stackoverflow.com/a/20592462/1623645). We might have to keep track of that information as an enum.",
  "created_at":"2020-05-25T15:54:47Z",
  "id":633632369,
  "issue":278,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMzYzMjM2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-25T15:54:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've been pondering this for a while and I think a separate package is the best way to go. Especially considering conda-forge will build at least two different `libawkward-cuda-kernels.so` binaries for different CUDA versions.\r\n\r\nI presume `awkward-cuda-kernels` and `awkward` will be versioned together? If so, it would be nice if `libawkward-cuda-kernels.so` can be built without depending on the `awkward` Python package so that it can be completely separate. (If not the first update build of the PR `awkward-cuda-kernels` will always fail due to `awkward-array` not being built yet.)\r\n\r\nAlso are you thinking `libawkward` should be a separately installable package for conda?",
  "created_at":"2020-05-25T17:15:27Z",
  "id":633657626,
  "issue":278,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMzY1NzYyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-25T17:15:27Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"> Also are you thinking `libawkward` should be a separately installable package for conda?\r\n\r\nI hadn't been thinking that, although I suppose it's a possibility. If that would allow projects that depend on C++ libawkward (such as the hypothetical FastJet interface that communicates via jagged arrays of events) to be kept in binary compatibility with the Python interface, it would be a benefit.\r\n\r\n(I'm not sure if there's binary compatibility to maintain; [dependent-project](https://github.com/scikit-hep/awkward-1.0/tree/master/dependent-project) can dynamically link, but some of its interfaces use STL\u2014I hadn't known at the beginning of the project that this is a bad idea.)\r\n\r\n> I presume awkward-cuda-kernels and awkward will be versioned together?\r\n\r\nIt will likely be necessary for `awkward1` version `x.y.z` to depend on `awkward1-cuda-kernels==x.y.z`, so they're going to live in the same GitHub repository. There's no reason for `awkward1-cuda-kernels` to depend on `awkward1` or Python, though. It's a one-way dependency.\r\n\r\nHow much customization can we do in the conda recipe? I'm wary about splitting the CMakeLists.txt that makes libawkward1-cpu-kernels.so, libawkward1.so, and the Python extension module now that it works with pip. Trying to set something up that works with pip _and_ separates libawkward1.so and the Python extension module for conda sounds daunting. Can we have different CMakeLists.txt for conda? Particularly if such a thing wouldn't have to be integrated with setup.py?",
  "created_at":"2020-05-25T17:30:46Z",
  "id":633661991,
  "issue":278,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMzY2MTk5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-25T17:30:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@trickarcher is isolating all of the CUDA-dependent stuff in a pip-installable package named `awkward1-cuda-kernels`, which is a Python package that only announces its full path and contains a `libawkward-cuda-kernels.so`. The main codebase tries to load this Python package dynamically when it's needed. The main package will have an \"extras\" dependency on `awkward1-cuda-kernels` so that one can\r\n\r\n```bash\r\npip install awkward1[cuda]\r\n```\r\n\r\nto get both (and maybe something similar in conda).\r\n\r\nI think this is a pretty good solution.",
  "created_at":"2020-07-17T13:29:28Z",
  "id":660107816,
  "issue":278,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDEwNzgxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-17T13:29:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@veprbl I had to revert the shared link to a static link because it [couldn't build a wheel on Linux](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2150&view=logs&j=18457e4f-4fea-51bc-a68e-6695bc227e9c&t=38cf0338-680f-5ffe-2e82-fd2e293cf965&l=142).\r\n\r\nI think you said that the use of shared linking wasn't strictly necessary for your use-case, but it would give the shared library more exposure because until now it had been largely untested (because we link against the other one). Is that correct?\r\n\r\nAnother possibility is that we could shared-link the MacOS one, since (a) Windows has trouble linking and (b) Linux has trouble deploying with the shared-link. However, [MacOS had no trouble in both cases](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2150&view=results) (maybe it has to do with a different library search used on MacOS? or could it actually be Cling?).",
  "created_at":"2020-05-29T00:49:28Z",
  "id":635693520,
  "issue":281,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNTY5MzUyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-29T00:49:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> @veprbl I had to revert the shared link to a static link because it [couldn't build a wheel on Linux](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2150&view=logs&j=18457e4f-4fea-51bc-a68e-6695bc227e9c&t=38cf0338-680f-5ffe-2e82-fd2e293cf965&l=142).\r\n\r\nIf building wheels is a must, then, perhaps, that should be a part of CI tests.\r\n\r\n> I think you said that the use of shared linking wasn't strictly necessary for your use-case, but it would give the shared library more exposure because until now it had been largely untested (because we link against the other one). Is that correct?\r\n\r\nYes, this is correct.\r\n\r\n> Another possibility is that we could shared-link the MacOS one, since (a) Windows has trouble linking and (b) Linux has trouble deploying with the shared-link. However, [MacOS had no trouble in both cases](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2150&view=results) (maybe it has to do with a different library search used on MacOS? or could it actually be Cling?).\r\n\r\nI feel like diverging those two builds on UNIX-like systems would only create extra confusion for developers. We should be looking to streamline things where possible, not complicate them.\r\n\r\nAlso note that Clang and Cling are different things, one is a widely known C/C++ compiler frontend and latter is a C/C++ REPL done by the ROOT team.",
  "created_at":"2020-05-29T19:41:32Z",
  "id":636152747,
  "issue":281,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNjE1Mjc0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-29T19:41:32Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a good line of thought, and I agree that the best way to think of these things is as one-dimensional arrays, like they way they're represented in Pandas.\r\n\r\nI added some comments to the gists.",
  "created_at":"2020-06-01T18:44:17Z",
  "id":637037502,
  "issue":284,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNzAzNzUwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-01T18:44:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This will still be searchable, but I don't see any action items in it, so I'm going to close it.",
  "created_at":"2020-10-30T22:07:43Z",
  "id":719821642,
  "issue":284,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgyMTY0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:07:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I spent some time thinking about this and playing, and I believe ultimately `awkward` arrays should be a new collection type in Dask. Here is why the existing ones won't work out:\r\n\r\n- *DataFrame*: the conclusion of #350 is that it is inappropriate to make awkward arrays a special Pandas column dtype, and anyway as I understand, there was some issue with conflicts in the awkward extension of pandas and the dask extension of pandas. Certainly the fact that pandas is extensible is enough of a feat to engineer that having two separate packages extend it without conflict is even harder. (c.f. also jax extensions of numpy vs. awkward)\r\n- *Array*: Dask makes great use of the fact that the full shape is known to design partitions and chunk work appropriately. We immediately have to toss all of this out since only the first dimension shape is known. (an exception is when all nodes are Record or RegularArray, so maybe those can be made compatible)\r\n- *Bag*: This is the first collection type that awkward could conceivably extend, because it is conceptually very similar to the awkward *PartitionedArray* albeit with no knowledge of partition sizes. The downside is that all bag operations are geared towards the assumption that both an outer loop over partitions and an inner loop over entries in partitions is carried out in pure python. There is an escape valve via `map_partitions` but clearly we are not able to use most of the *Bag* functions (such as max, mean, repartition, etc.) in an efficient manner out of the box.\r\n\r\nI sketched a simple bag extension:\r\n```python\r\nimport numbers\r\nfrom operator import getitem, add\r\nfrom numpy.lib.mixins import NDArrayOperatorsMixin\r\nfrom dask.bag import Bag, map_partitions, from_delayed\r\n\r\n\r\n\r\nclass AwkwardBag(Bag, NDArrayOperatorsMixin):\r\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\r\n        if \"out\" in kwargs:\r\n            raise ValueError(\"awkward dask ufuncs cannot use 'out'\")\r\n\r\n        if method != \"__call__\":\r\n            raise ValueError(\"awkward1 only supports ufunc call method\")\r\n\r\n        for x in inputs:\r\n            if not isinstance(x, (numbers.Number, AwkwardBag)):\r\n                return NotImplemented\r\n\r\n        return map_partitions(ufunc, *inputs, **kwargs)\r\n\r\n    def __getitem__(self, key):\r\n        if isinstance(key, tuple):\r\n            if isinstance(key[0], numbers.Number):\r\n                raise NotImplementedError(\"We need to know partition sizes to find the right position\")\r\n            elif isinstance(key[0], slice):\r\n                if key[0] == slice(None):\r\n                    pass  # We can do this\r\n                else:\r\n                    raise NotImplementedError(\"We need to know partition sizes to correctly slice\")\r\n            else:\r\n                raise NotImplementedError(\"Maybe its an array? we could handle that if we partition it to match\")\r\n        return map_partitions(getitem, self, key)\r\n\r\n    def __getattr__(self, key):\r\n        if key in dir(type(self)):\r\n            return super(DaskAwkwardArray, self).__getattribute__(key)\r\n        # it would be nice to have the Form to check an item is available\r\n        return map_partitions(getattr, self, key)\r\n\r\n    @classmethod\r\n    def from_bag(cls, bag):\r\n        \"\"\"Convert a bag of json-like python objects to awkward arrays\"\"\"\r\n        return bag.map_partitions(ak.from_iter)\r\n    \r\n    @classmethod\r\n    def from_delayed(cls, items):\r\n        out = from_delayed(items)\r\n        out.__class__ = cls\r\n        return out\r\n```\r\n\r\nwith an example of use:\r\n```python\r\nimport random\r\nimport numpy as np\r\nimport awkward1 as ak\r\nfrom dask import delayed\r\n\r\n@delayed\r\ndef gen_partition(i):\r\n    return ak.zip({\r\n        \"x\": i * np.ones(100),\r\n        \"y\": ak.from_iter([range(random.randint(0, 6)) for _ in range(100)])\r\n    })\r\n\r\nabag = AwkwardBag.from_delayed(map(gen_partition, range(10)))\r\n(abag.y[:, :2] + 3).reduction(lambda x: x, ak.concatenate).compute()\r\n```\r\nwhere once the partitions of the bag are generated, any broadcastable operation (such as the `add` ufunc here) are going to be computed in the fast path for each partition. Then to bring the results back I have to concatenate the arrays, resulting in:\r\n```\r\n<Array [[3, 4], [3, 4], ... [3, 4], [3], []] type='1000 * var * int64'>\r\n```\r\n\r\nSince this seemed promising, I went further and implemented a [custom collection](https://docs.dask.org/en/latest/custom-collections.html) that tracked partition sizes, and am looking into saving the [awkward Form](https://awkward-array.readthedocs.io/en/latest/ak.forms.Form.html) (which is known ahead of time for my use case, but could be inferred when loading a dataset similar to how columns would be inferred when reading many csv files). The Form would allow checking that fields exist during the graph construction, and maybe could validate certain structure operations as well.\r\n\r\nMany of the `ak.*` namespace functions that we would want to work with these arrays (think `ak.mean(abag, axis=None)`, etc.) in principle have already been written for [PartitionedArray](https://github.com/scikit-hep/awkward-1.0/blob/master/src/awkward1/partition.py#L100) (which is, conveniently, pure python) but the assumption there is that all operations are eager (e.g. the general [reduce](https://github.com/scikit-hep/awkward-1.0/blob/master/src/awkward1/partition.py#L262-L272)). Perhaps something as simple as wrapping each such eager op with `dask.delayed()` is all that's needed.",
  "created_at":"2020-11-20T22:14:18Z",
  "id":731434583,
  "issue":284,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTQzNDU4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T22:14:18Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Please see also https://github.com/martindurant/awkward_extras/issues/1 for discussion on the pandas extension array path - I'd say it certainly can be done for dask-dataframe. Whether it is the right or best model is another matter,",
  "created_at":"2020-11-24T14:49:02Z",
  "id":733021798,
  "issue":284,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzAyMTc5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-24T14:49:02Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Oops; wrong project.",
  "created_at":"2020-06-02T11:25:52Z",
  "id":637475776,
  "issue":285,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNzQ3NTc3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-02T11:25:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"There are two safer alternatives:\r\n\r\n   * replace the `as_strided` call with `numpy.broadcast_to([what], shape=(len(base),))`, which ought to be the same thing, but it's not in NumPy's \"danger zone\" (i.e. maybe I'm missing something important about making a zero-stride array);\r\n   * replace the `as_strided` with something that actually creates the array, such as `numpy.repeat(what, len(base))`.\r\n\r\nIf the first one works, then there was some subtlety in making zero-stride arrays that I didn't know about but `broadcast_to` gets right. Anyway, this should be preferred because it's higher-level code.\r\n\r\nIf the second one works, then performance suffers, but probably not enough to notice, compared to other operations, and it's better to be correct than fast.",
  "created_at":"2020-06-03T14:49:28Z",
  "id":638248387,
  "issue":286,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODI0ODM4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-03T14:49:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The first alternative doesn't help, there seems to be a general problem with the stride 0 view. The second alternative seems to help in this case.\r\nShould such memory views work in general? If there are no other places in the code that rely on this then probably here it also should be just an  actual array?",
  "created_at":"2020-06-03T15:06:52Z",
  "id":638258683,
  "issue":286,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODI1ODY4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-03T15:06:52Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"In principle it should work, but the zero-stridedness is not maintained after Awkward broadcasting (unlike NumPy broadcasting, Awkward broadcasting does have to copy elements to fit them in to unequal-length lists). So just calling `numpy.repeat` here is not giving up much.\r\n\r\nWould you be willing to add this as a PR, since it works in your local repo?",
  "created_at":"2020-06-03T15:40:59Z",
  "id":638279791,
  "issue":286,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODI3OTc5MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-06-03T15:40:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is great, thanks! It also looks complete: unless you tell me otherwise, I'll merge it as soon as it passes tests.",
  "created_at":"2020-06-03T16:12:42Z",
  "id":638299761,
  "issue":288,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODI5OTc2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-03T16:12:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I do such things by an incremental search for \"offset\", so that my eyeballs have scanned over all instances.\r\n\r\nThat is exactly what I did :) \r\n\r\n> Just tell me if you've scanned everything.\r\n\r\nI have. These _should_ be the only instances where the wrong offsets are applied. ",
  "created_at":"2020-06-10T13:20:57Z",
  "id":642004216,
  "issue":292,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MjAwNDIxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-10T13:25:45Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Great! I'll be mostly offline today because of [#shutdownstem](https://www.particlesforjustice.org/), but I'll merge your PRs so that I don't leave you in the lurch.",
  "created_at":"2020-06-10T13:29:05Z",
  "id":642009930,
  "issue":292,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MjAwOTkzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-10T13:29:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> You have full paths to the cuda-kernels library on your own hard drive; there's a mechanism for library-finding based on RPATH and LD_LIBRARY_PATH, which should be used. (They also strip the \"lib\" off the beginning and \".so\" at the end, since different platforms have different conventions. But we only need to test Linux\u2014realistically, that's where the Nvidia GPUs are.)\r\n\r\nIt must have slipped my mind, I have made the changes. \r\n\r\n> You might be able to implement the allocators in a templated way, to reduce duplication. If not (for some technical reason I'm not aware of), then it's okay.\r\n\r\nI believe it's templated on the `libawkward` side, however since everything in `libawkward-cuda-kernels` is under `extern \"C\"`, and with the additional complication of handling `function_pointers` and it's `typenames`, I had to resort to duplication.\r\n\r\n> There's still debugging code in this (print-outs with cout and fputs).\r\n\r\nI removed the `cout` in the recent commit, however I still have to figure out an exit strategy in case the library can't be opened, so I have kept the `fputs`. I'll remove it before the final merge, though.\r\n\r\n> It might be necessary to use default arguments for these KernelsLib enums, at least for now, to ease them in, but if it's possible to make them required and get the code to compile again, that explicitness will pay off when we start linking.\r\n\r\nI can do this, but it will require a lot of refactoring of the existing code_base. Plus, since we are on this topic, does every instance of a temp `Index` or `IdentitiesOf` inherit the `ptr_lib_` of the highest level array type? Or should it be based on the reference from where it is being formed, like `util::make_starts(offsets_)` for example. Should I inherit from offsets_ or the highest level array in this case? The former seems more logical to me since, we ensure a `low_granularity` setup there.\r\n\r\nNext, I am planning to implement the `get` and `set` functions, so that we can have a minimum working example and after that I'll work on the `listarray_num` which won't be too much work, I guess after the `get` and `set` functions get implemented. I think this would help us build arrays using `ArrayBuilder`(I think?) directly on the GPU.\r\n",
  "created_at":"2020-06-11T06:44:01Z",
  "id":642446013,
  "issue":293,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MjQ0NjAxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-11T06:44:01Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"> I can do this, but it will require a lot of refactoring of the existing code_base.\r\n\r\nMy point was that we'll see centrally have to do this, and the compiler would help us catch all those instances, but that would be easier to do if it's the only change, maybe in its own PR.\r\n\r\nYou reminded me about `make_starts` and `make_stops`. Neither of these should get a `ptr_lib` argument, much less one and not the other. They're just slicing an existing index, so there's no way they could change where it's located.\r\n\r\nIf by \"inheritance,\" you mean the output gets the same `ptr_lib` as the input, then yes, that's regards would happen in so cases where it's possible.\r\n\r\nI hadn't been considering filling ArrayBuilder on the GPU. That's not a fast algorithm by its nature\u2014it's slow enough that ending it with a wholesale copy of the data from main memory to GPU would not be a major loss. So we don't need to consider ArrayBuilder part of the scope of this project.\r\n\r\n",
  "created_at":"2020-06-11T11:54:36Z",
  "id":642595033,
  "issue":293,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MjU5NTAzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-11T11:54:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I hadn't been considering filling ArrayBuilder on the GPU. That's not a fast algorithm by its nature\u2014it's slow enough that ending it with a wholesale copy of the data from main memory to GPU would not be a major loss. So we don't need to consider ArrayBuilder part of the scope of this project.\r\n\r\nOkay, do you have some particular idea in mind on how to deploy values to `cuda`? If I am not wrong, here has to be an initial `builder` architecture of some sort, right?",
  "created_at":"2020-06-11T12:00:42Z",
  "id":642597997,
  "issue":293,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MjU5Nzk5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-11T12:00:42Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"> Okay, do you have some particular idea in mind on how to deploy values to `cuda`? If I am not wrong, here has to be an initial `builder` architecture of some sort, right?\r\n\r\nThere will be some copy-to-GPU, copy-from-GPU functions, so users can build up an array on the main memory and copy it over. Also, it can come from layouts constructed from CuPy arrays.",
  "created_at":"2020-06-11T12:09:28Z",
  "id":642602924,
  "issue":293,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MjYwMjkyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-11T12:09:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks, I'll work on that then!",
  "created_at":"2020-06-11T12:18:24Z",
  "id":642607255,
  "issue":293,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MjYwNzI1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-11T12:18:24Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"After replacing the `library_path` with my `libawkward-cuda-kernels.so` full path,\r\n\r\n```diff\r\nindex c88d161..93716e8 100644\r\n--- a/tests/test_0293-cuda-allocation-tests.cpp\r\n+++ b/tests/test_0293-cuda-allocation-tests.cpp\r\n@@ -14,7 +14,7 @@ public:\r\n     StartupLibraryPathCallback() = default;\r\n \r\n     const std::string library_path() const override {\r\n-      std::string library_path = (\"/path/to/python-pip/awkward1_cuda_kernels/libawkward-cuda-kernels.so\");\r\n+      std::string library_path = (\"/home/pivarski/miniconda3/lib/python3.7/site-packages/awkward1_cuda_kernels/libawkward-cuda-kernels.so\");\r\n       return library_path;\r\n     };\r\n };\r\ndiff --git a/tests/test_0293-listarray-num-kernel.cpp b/tests/test_0293-listarray-num-kernel.cpp\r\nindex 97f830e..5b97d76 100644\r\n--- a/tests/test_0293-listarray-num-kernel.cpp\r\n+++ b/tests/test_0293-listarray-num-kernel.cpp\r\n@@ -15,7 +15,7 @@ public:\r\n     StartupLibraryPathCallback() = default;\r\n \r\n     const std::string library_path() const override {\r\n-      std::string library_path = (\"/path/to/python-pip/awkward1_cuda_kernels/libawkward-cuda-kernels.so\");\r\n+      std::string library_path = (\"/home/pivarski/miniconda3/lib/python3.7/site-packages/awkward1_cuda_kernels/libawkward-cuda-kernels.so\");\r\n       return library_path;\r\n     };\r\n };\r\n```\r\n\r\nI get the following errors from CTest:\r\n\r\n```\r\ncmake --build localbuild --target test -- CTEST_OUTPUT_ON_FAILURE=1 --no-print-directory\r\nRunning tests...\r\nTest project /home/pivarski/irishep/awkward-1.0/localbuild\r\n    Start 1: test0016\r\n1/6 Test #1: test0016 .........................   Passed    0.00 sec\r\n    Start 2: test0019\r\n2/6 Test #2: test0019 .........................Child aborted***Exception:   0.09 sec\r\nterminate called after throwing an instance of 'std::invalid_argument'\r\n  what():  install the 'awkward1-cuda-kernels' package with:\r\n\r\n                pip install awkward1[cuda] --upgrade\r\n\r\n    Start 3: test0030\r\n3/6 Test #3: test0030 .........................   Passed    0.00 sec\r\n    Start 4: test0293_cuda_alloc\r\n4/6 Test #4: test0293_cuda_alloc ..............Child aborted***Exception:   0.40 sec\r\nmunmap_chunk(): invalid pointer\r\n\r\n    Start 5: test0293_listarray_num\r\n5/6 Test #5: test0293_listarray_num ...........   Passed    0.20 sec\r\n    Start 6: test0074\r\n6/6 Test #6: test0074 .........................***Failed    0.00 sec\r\n\r\n\r\n50% tests passed, 3 tests failed out of 6\r\n\r\nTotal Test time (real) =   0.69 sec\r\n\r\nThe following tests FAILED:\r\n\t  2 - test0019 (Child aborted)\r\n\t  4 - test0293_cuda_alloc (Child aborted)\r\n\t  6 - test0074 (Failed)\r\nErrors while running CTest\r\nMakefile:90: recipe for target 'test' failed\r\nmake: *** [test] Error 8\r\nTraceback (most recent call last):\r\n  File \"localbuild.py\", line 89, in <module>\r\n    check_call([\"cmake\", \"--build\", \"localbuild\", \"--target\", \"test\", \"--\", \"CTEST_OUTPUT_ON_FAILURE=1\", \"--no-print-directory\"])\r\n  File \"localbuild.py\", line 55, in check_call\r\n    return subprocess.check_call(args, env=env)\r\n  File \"/home/pivarski/miniconda3/lib/python3.7/subprocess.py\", line 363, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', 'localbuild', '--target', 'test', '--', 'CTEST_OUTPUT_ON_FAILURE=1', '--no-print-directory']' returned non-zero exit status 2.\r\n```\r\n\r\nwhich means that some of them worked: I don't see any errors from `test_0293-listarray-num-kernel.cpp`, which is a very good sign because that's the full test. The others might have some little errors (I haven't delved into it).\r\n\r\nMeanwhile, you could replace the explicit \"put full path here\" with something that picks up the library path if you can execute\r\n\r\n```bash\r\npython -c 'import awkward1_cuda_kernels; print(awkward1_cuda_kernels.shared_library_path)'\r\n```\r\n\r\nin the C++. (That's how I got my full path, anyway.) [This recipe](https://stackoverflow.com/a/478960/1623645) defines an\r\n\r\n```c++\r\nstd::string exec(const char* cmd)\r\n```\r\n\r\nfunction that can be used to run the Python command above. That would make testing more automatic.",
  "created_at":"2020-06-29T13:17:50Z",
  "id":651114737,
  "issue":293,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MTExNDczNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-29T13:17:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Okay, I know this. So `test0019` is failing because of the now useless cuda transfers I made in there. It was long before I had the callback ready so, I'll  make a commit to remove it.(Again, it's my fault I shouldn't have made changes to that test). `test_0293_cuda_alloc` is failing because I am creating pointers on the stack and wrapping them with `shared_ptr` to feed them into the `awkward ecosystem`, which doesn't make any sense because we can't call `delete[]` on the stack-allocated pointer. And that's why you get the `double free or corruption` error. Honestly, for `test0293` I did it beacuse, I found it easier and cleaner, but the correct way would be to allocate ptr's using `new` and then wrapping them with `shared_ptr`.",
  "created_at":"2020-06-29T13:28:50Z",
  "id":651120925,
  "issue":293,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MTEyMDkyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-29T13:28:50Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"This should fix your documentation error:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/pull/323 (from @reikdas, naturally)\r\n\r\nIt's in master.",
  "created_at":"2020-07-08T15:17:12Z",
  "id":655584094,
  "issue":293,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NTU4NDA5NA==",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2020-07-08T15:17:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The build command should be in a triple-backtick block to make it easier to copy paste.\r\n\r\nIt should also say you start in the top-level directory of the git repo and specify the exact directories (again, for quick copy-paste). There's no reason to suggest flexibility here: we can dictate where you call it from and where the output should go.\r\n\r\nIt should them say where the entry HTML is located, so that this path can be copied to a browser. (Relative to git top-level.)\r\n\r\nIt should also mention that doxygen is required and it will be run\u2014some people don't have doxygen and need to get it (is that pip installable? Can it go in the dev requirements?) and anyway, people might be surprised to see doxygen running. They should at least be warned.\r\n\r\nUsers should also be warned that a lot of custom Python gets run with the Sphinx job, including some that requires a network connection. That would definitely be a surprise.\r\n\r\nFinally, some directories are generated by this process that we sometimes want to delete to start fresh: docs-sphinx/_static, docs-sphinx/_auto, and docs-sphinx/_build or maybe _html. These should be listed, perhaps in an \"rm\" command that can be copy-pasted.",
  "created_at":"2020-06-12T11:38:20Z",
  "id":643225114,
  "issue":295,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzIyNTExNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-12T11:38:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like you also need\r\n\r\n```bash\r\nconda install babel imagesize sphinxcontrib sphinxcontrib-serializinghtml sphinxcontrib-applehelp sphinxcontrib-devhelp sphinxcontrib-htmlhelp sphinxcontrib-qthelp alabaster\r\n```\r\n\r\nMaybe a Sphinx update loosened its strict requirements and these are now \"used, but not labeled as required.\"\r\n\r\nAnd of course, there's\r\n\r\n```bash\r\nconda install pycparser black\r\n```\r\n\r\nfor developers building the documentation. Alternatively, we could ask them to install `docs-sphinx/requirements.txt`, since that's where we tell ReadTheDocs to get it, and then that's all in one place.",
  "created_at":"2020-06-12T14:29:10Z",
  "id":643300960,
  "issue":295,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzMwMDk2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-12T14:29:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> some people don't have doxygen and need to get it (is that pip installable? Can it go in the dev requirements?)\r\n\r\nDoxygen isn't on PyPI :( \r\n\r\n> It looks like you also need\r\n> \r\n> ```shell\r\n> conda install babel imagesize sphinxcontrib sphinxcontrib-serializinghtml sphinxcontrib-applehelp sphinxcontrib-devhelp sphinxcontrib-htmlhelp sphinxcontrib-qthelp alabaster\r\n> ```\r\n> \r\n> Maybe a Sphinx update loosened its strict requirements and these are now \"used, but not labeled as required.\"\r\n\r\nIn that case, should we also include these packages in docs-sphinx/requirements.txt? (Also I noticed that there is no package named sphinxcontrib on PyPI - must be one of those few instances conda and pip names for common packages vary)\r\n",
  "created_at":"2020-06-13T05:56:12Z",
  "id":643575232,
  "issue":295,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzU3NTIzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-13T05:56:12Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"The docs-sphinx/requirements.txt is for ReadTheDocs, which has some things preloaded, and I wouldn't want to meet that up. Setup instructions for developers can be longer than setup instructions for users. We can also expect them to get doxygen \"somehow.\" Since this is an old, stable package, I used apt-get (being an old version didn't matter).\r\n\r\nThe \"sphinxcontrib\" one is the only one that might not be needed. While I was trying to satisfy requirements one at a time, I thought that might solve the \"sphinxcontrib-serializinghtml\" one, but it didn't. It might be some metapackage.",
  "created_at":"2020-06-13T11:26:30Z",
  "id":643610227,
  "issue":295,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzYxMDIyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-13T11:26:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Great, thanks!",
  "created_at":"2020-06-15T11:29:36Z",
  "id":644072268,
  "issue":295,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDA3MjI2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-15T11:29:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Do you need the results of the currently ongoing test, or can I cancel it to run #270? (Numba just released 0.50 yesterday, and this will become the minimum required version because it fixes a long-standing entry-point issue.) I want to see if the #270 test is successful so that I can merge it and make a new version that depends on the new Numba.",
  "created_at":"2020-06-12T12:12:35Z",
  "id":643237907,
  "issue":296,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzIzNzkwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-12T12:12:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Go ahead and cancel it :) \r\nBut we should probably restart the tests later to make sure nothing breaks (I am done with this PR).",
  "created_at":"2020-06-12T12:16:21Z",
  "id":643239700,
  "issue":296,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzIzOTcwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-12T12:16:21Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"In that case, I've waited long enough by round-robining through my other emails that we should let this one finish, since the results of the test are going to be used for something (evaluating this PR that you're done with).",
  "created_at":"2020-06-12T12:22:53Z",
  "id":643242284,
  "issue":296,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzI0MjI4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-12T12:22:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Good catch, and thanks for this. However, I've deleted requirements-dev.txt because that file is actually not used (so this PR needs to be closed). For some reason, I couldn't get this requirements-dev.txt file included in the package so setup.py fails when it needs to compile. It was a battle I didn't want to fight at the time. But it's always better to delete a second copy of data rather than let them get out of sync. (As it turns out, the right place for `pycparser` and `black` was in docs-sphinx/requirements.txt. I admit that it's pretty confusing.)\r\n\r\nI did put this change in the relevant place in setup.py; thanks for alerting me to it. A few commits directly to master have corrected readthedocs and the online website [now has your kernels documentation](https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html). Time to focus on the tests!\r\n\r\nSee you later today.",
  "created_at":"2020-06-12T13:58:20Z",
  "id":643284644,
  "issue":297,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzI4NDY0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-12T13:58:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`:)`",
  "created_at":"2020-06-12T15:56:33Z",
  "id":643349072,
  "issue":298,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzM0OTA3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-12T15:56:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yeah, I missed those completely. Sorry about that. There's a lot of functions, around `300`. I did some refactoring and now there's `247` left to go `:P`. I'll try finishing this up by tomorrow.",
  "created_at":"2020-06-13T17:17:32Z",
  "id":643651948,
  "issue":299,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzY1MTk0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-14T07:28:57Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"So all the kernels have been `re-templatized` in `kernel` namespace and the kernels are in the order of their appearance in the `cpu_kernels` folder. I have removed all the instances of `KernelsLib`, because there wasn't any utility for it and introduced a `default argument` dependence. ",
  "created_at":"2020-06-14T20:12:58Z",
  "id":643816330,
  "issue":299,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0MzgxNjMzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-14T20:12:58Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I honestly don't have an opinion here, either of them is fine by me but I believe you added the `util::` namespace to make it more type safe by adding a ` _64` suffix to the kernels. This would prevent us to not use template types that have not been specified before hand, but again we can catch those pretty easily during compile time. I think @reikdas 's input on this would be much more valuable here, since one of the two ways would make it easier for him to deploy the automated testing.",
  "created_at":"2020-06-15T11:46:20Z",
  "id":644079471,
  "issue":299,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDA3OTQ3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-15T11:46:20Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I think @reikdas 's input on this would be much more valuable here, since one of the two ways would make it easier for him to deploy the automated testing.\r\n\r\nI don't think it matters - if a consistent code style is maintained, any required parsing in the future should not be too difficult. ",
  "created_at":"2020-06-15T12:57:00Z",
  "id":644118198,
  "issue":299,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDExODE5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-15T12:57:00Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks! I'll go with the `non-retemplatizing` `i.e` suffixing with `_64` or `_32`, since it would be more type safe.",
  "created_at":"2020-06-15T13:00:01Z",
  "id":644119655,
  "issue":299,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDExOTY1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-15T13:00:01Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Then let's leave `_64` in the kernel names and not templatize this parameter, which never varies in the current (and foreseen) codebase. Then the PR will be done. (@trickarcher, our comments crossed, but we came to the same conclusion. There aren't any instances of `_32`, though. Types that actually vary, like `listarray32_XYZ`, `listarrayU32_XYZ`, `listarray64_XYZ` \u2192 `listarray_XYZ<T>` should be templatized.)\r\n\r\nI'll also be merging this with @ianna's #168 somehow. Perhaps #299 should get merged into master first and then I'll adjust #168 accordingly? I'll be merging in new kernels in cpu-kernels/sorting.h and sorting.cpp, which will go through the new `kernel::` namespace but might need to be skipped in @reikdas's documentation. (They have regular C-style interfaces, so the interfaces (blue function signatures) could be documented, but their implementations use a lot of C++ standard library utilities that we shouldn't try to pass through the same documenting/testing infrastructure. They'll be special cases for a while.)",
  "created_at":"2020-06-15T13:09:10Z",
  "id":644124504,
  "issue":299,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDEyNDUwNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-06-15T13:09:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"#168 was ready first, so that went into master. You'll at least need to git merge master. If you want, we can take care of the new kernels (in cpu-kernels/sorting.h) after this PR, but if you want to do all kernels in one PR, then the new batch is all in that file. (Most of them are templated variants of one or two main functions.)\r\n\r\nFor @reikdas: your parser does not need to parse the implementations of the functions in cpu-kernels/sorting.h, though it could be good to document the interfaces of the new functions.",
  "created_at":"2020-06-15T18:40:31Z",
  "id":644309442,
  "issue":299,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDMwOTQ0Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2020-06-15T18:40:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"So, currently the sorting kernels reside in the `kernel` namespace, and I made the changes you requested.",
  "created_at":"2020-06-15T20:18:54Z",
  "id":644358581,
  "issue":299,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDM1ODU4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-15T20:18:54Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Don't make any more changes; I'm committing some last tweaks before merging.",
  "created_at":"2020-06-17T15:18:32Z",
  "id":645439855,
  "issue":299,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NTQzOTg1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-17T15:18:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(Yeah, this is really about documentation, because we need an example for that SO question.)\r\n\r\nThe missing piece is Parquet reading and writing. It was done through pyarrow [in the old version](https://github.com/scikit-hep/awkward-array/blob/d88527c69d3070aa49db2aa9e14d9f02adb73e19/awkward/arrow.py#L452-L578) and should be again: [ak.to_arrow](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_arrow.html) and [ak.from_arrow](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_arrow.html) exist, as do [PartitionedArray](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partition.PartitionedArray.html) and [VirtualArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.VirtualArray.html) for lazily loading Parquet columns and row groups. Then #172 would provide some good tests to verify it (as would [the old tests](https://github.com/scikit-hep/awkward-array/blob/d88527c69d3070aa49db2aa9e14d9f02adb73e19/tests/test_arrow.py#L409-L474)).\r\n\r\nThe Numba JIT-compiling is fairly mature; I've demoed it a number of times, and I'm usually fighting Numba type errors, not Awkward errors/missing features. [ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html) provides an easier way to generate a complex schema'ed output than I've seen elsewhere, though understandably it's not as fast as specialized code. (ArrayBuilder internally has dynamic typing, which is something we'd have to warn against for performance-conscious users.) That would probably be the easiest way to _make_ a Parquet file.\r\n\r\nSo maybe the right thing to say is that for this SO question, we're not ready, but Parquet reading/writing is the blocker\u2014maybe in other cases as well.",
  "created_at":"2020-06-16T16:28:25Z",
  "id":644871877,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDg3MTg3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T16:28:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Probably a long-hand demo using current fromarrow and just about any  jitted function would be nice; the obvious benchmark comparison being the same column in JSON form, converted to python dictionaries.",
  "created_at":"2020-06-16T17:32:40Z",
  "id":644906542,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDkwNjU0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T17:32:40Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"There is this: https://awkward-array.org/what-is-awkward.html\r\n\r\nthough it starts from JSON, rather than Parquet.",
  "created_at":"2020-06-16T17:56:28Z",
  "id":644918205,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDkxODIwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T17:56:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"There is a jitted general-purpose function there? I am thinking of something, anything, that doesn't translate nicely into numpy syntax.\r\nFrom a benchmarking standpoint, to show awkward's usefulness for something like the SO question, it would have to start with a large amount of data in parquet.",
  "created_at":"2020-06-16T18:07:43Z",
  "id":644923785,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDkyMzc4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T18:07:43Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Ah, I forgot where I did these things. It was in a talk, which I should feed back into the documentation so that it has a Numba example.\r\n\r\nhttps://github.com/jpivarski/2020-07-06-scipy2020/blob/c951aa50bbfa879f011c9f2c343a3d0168be28e0/main.tex#L486-L503\r\n\r\n```python\r\nimport numba as nb                  #  50\u00d7 faster than ak.sum version\r\n@nb.jit                             # 250\u00d7 faster than the pure Python\r\ndef compute_lengths(bikeroutes):\r\n    route_length = np.zeros(len(bikeroutes.features))\r\n    for i in range(len(bikeroutes.features)):\r\n        for path in bikeroutes.features[i].geometry.coordinates:\r\n            first = True\r\n            last_east, last_north = 0.0, 0.0\r\n            for lng_lat in path:\r\n                km_east = lng_lat[0] * 82.7\r\n                km_north = lng_lat[1] * 111.1\r\n                if not first:\r\n                    dx2 = (km_east - last_east)**2\r\n                    dy2 = (km_north - last_north)**2\r\n                    route_length[i] += np.sqrt(dx2 + dy2)\r\n                first = False\r\n                last_east, last_north = km_east, km_north\r\n    return route_length\r\n```\r\n\r\n**It would be great to start from Parquet, but that Parquet I/O through Arrow is the missing link.**\r\n\r\n(Incidentally, this example, chosen for pedagogy, has unimpressive speedup compared to other examples. The Awkward version is \"only\" 5-8\u00d7 faster than pure Python but the Numba version with its single-pass over data and no intermediate arrays is 250\u00d7 faster than pure Python. It happens to be a case where the performance advantage of Numba + Awkward is much more impressive than Awkward by itself\u2014unless I find a performance bug in ak.sum or something. Given only this example, the story becomes \"Awkward Array for convenience, Numba + Awkward for speed,\" though a different example might tell a different story.\r\n\r\nAnother performance point about this example: the array version of the dataset is 250 MB of RAM, 5.2\u00d7 smaller than the equivalent Python objects. Coming from Parquet would mean no intermediate Python objects in the full workflow from columnar data file through Numba.)",
  "created_at":"2020-06-16T18:32:32Z",
  "id":644940324,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDk0MDMyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T18:32:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"That is *exactly* the sort of thing I had in mind!\r\nfrom_arrow does work now, right? So the only thing missing would be to have a decent dataset in parquet format (perhaps the same bike stuff) and have a few lines to get the arrow record batch and convert it to ak.",
  "created_at":"2020-06-16T18:37:17Z",
  "id":644942787,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDk0Mjc4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T18:37:17Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes. I wonder if there's a JSON \u2192 Parquet converter somewhere. You'd have to assign a schema, but [ak.type](https://awkward-array.readthedocs.io/en/latest/_auto/ak.type.html) should help there. I take it you're thinking of a one-off Parquet \u2192 Arrow \u2192 ak.from_arrow \u2192 Numba without lazy column/row-group reading of the Parquet, right? (The bikeroutes file would be too small to justify row-groups, anyway.)",
  "created_at":"2020-06-16T18:43:22Z",
  "id":644945640,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDk0NTY0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T18:43:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"No need for anything lazy for this, and if there were multiple row-groups, I would read them separately using Dask.",
  "created_at":"2020-06-16T18:59:17Z",
  "id":644952816,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDk1MjgxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T18:59:17Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> I wonder if there's a JSON \u2192 Parquet converter somewhere\r\n\r\nSpark? :) \r\nI don't know how effective https://arrow.apache.org/docs/python/json.html is",
  "created_at":"2020-06-16T19:13:59Z",
  "id":644961112,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NDk2MTExMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-16T19:13:59Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Just to update, pyspark did successfully turn the Bikeroutes into parquet, but arrow can't load the output:\r\n```\r\nArrowInvalid: Mix of struct and list types not yet supported\r\n```\r\nVery disappointed! Arrow also can't read the JSON directly, not sure what it's expecting.\r\n\r\nThe rendering of the schema in spark looks like this:\r\n```\r\nroot\r\n |-- _corrupt_record: string (nullable = true)\r\n |-- geometry: struct (nullable = true)\r\n |    |-- coordinates: array (nullable = true)\r\n |    |    |-- element: array (containsNull = true)\r\n |    |    |    |-- element: array (containsNull = true)\r\n |    |    |    |    |-- element: double (containsNull = true)\r\n |    |-- type: string (nullable = true)\r\n |-- properties: struct (nullable = true)\r\n |    |-- BIKEROUTE: string (nullable = true)\r\n |    |-- F_STREET: string (nullable = true)\r\n |    |-- STREET: string (nullable = true)\r\n |    |-- TYPE: string (nullable = true)\r\n |    |-- T_STREET: string (nullable = true)\r\n |-- type: string (nullable = true)\r\n```\r\nbut the actual parquet schema, rendered by fastparquet, is the following (because spark lists-type elements can be either `[]` or `NULL`):\r\n```\r\n- spark_schema:\r\n| - _corrupt_record: BYTE_ARRAY, UTF8, OPTIONAL\r\n| - geometry: OPTIONAL\r\n| | - coordinates: LIST, OPTIONAL\r\n| |   - list: REPEATED\r\n| |     - element: LIST, OPTIONAL\r\n| |       - list: REPEATED\r\n| |         - element: LIST, OPTIONAL\r\n| |           - list: REPEATED\r\n| |             - element: DOUBLE, OPTIONAL\r\n|   - type: BYTE_ARRAY, UTF8, OPTIONAL\r\n| - properties: OPTIONAL\r\n| | - BIKEROUTE: BYTE_ARRAY, UTF8, OPTIONAL\r\n| | - F_STREET: BYTE_ARRAY, UTF8, OPTIONAL\r\n| | - STREET: BYTE_ARRAY, UTF8, OPTIONAL\r\n| | - TYPE: BYTE_ARRAY, UTF8, OPTIONAL\r\n|   - T_STREET: BYTE_ARRAY, UTF8, OPTIONAL\r\n  - type: BYTE_ARRAY, UTF8, OPTIONAL\r\n```",
  "created_at":"2020-06-18T20:56:36Z",
  "id":646301382,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjMwMTM4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-18T20:56:36Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Oh wait, I think spark is assuming one record par line, rather than a well-formed single JSON object",
  "created_at":"2020-06-18T21:00:18Z",
  "id":646303039,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjMwMzAzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-18T21:00:18Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK, that works fine for spark, arrow still fails with the same error. Schema according to fastparquet is\r\n```\r\n- spark_schema:\r\n| - crs: OPTIONAL\r\n| | - properties: OPTIONAL\r\n| |   - name: BYTE_ARRAY, UTF8, OPTIONAL\r\n|   - type: BYTE_ARRAY, UTF8, OPTIONAL\r\n| - features: LIST, OPTIONAL\r\n|   - list: REPEATED\r\n|     - element: OPTIONAL\r\n|     | - geometry: OPTIONAL\r\n|     | | - coordinates: LIST, OPTIONAL\r\n|     | |   - list: REPEATED\r\n|     | |     - element: LIST, OPTIONAL\r\n|     | |       - list: REPEATED\r\n|     | |         - element: LIST, OPTIONAL\r\n|     | |           - list: REPEATED\r\n|     | |             - element: DOUBLE, OPTIONAL\r\n|     |   - type: BYTE_ARRAY, UTF8, OPTIONAL\r\n|     | - properties: OPTIONAL\r\n|     | | - BIKEROUTE: BYTE_ARRAY, UTF8, OPTIONAL\r\n|     | | - F_STREET: BYTE_ARRAY, UTF8, OPTIONAL\r\n|     | | - STREET: BYTE_ARRAY, UTF8, OPTIONAL\r\n|     | | - TYPE: BYTE_ARRAY, UTF8, OPTIONAL\r\n|     |   - T_STREET: BYTE_ARRAY, UTF8, OPTIONAL\r\n|       - type: BYTE_ARRAY, UTF8, OPTIONAL\r\n  - type: BYTE_ARRAY, UTF8, OPTIONAL\r\n```",
  "created_at":"2020-06-18T21:04:15Z",
  "id":646304822,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjMwNDgyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-18T21:04:15Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Maybe remove the structs, so that it's just the ListArray(ListArray(ListArray(ListArray(double)))) representing the coordinates? In the past, I've found that structs are fine for conversions to and from Arrow, but then Arrow \u2194 Parquet has only implemented nested lists without any structs.\r\n\r\nThe majority of the exercise can be done with only the coordinates (see the Awkward example for quickly projecting them out; probably easier than in any other framework).\r\n\r\nOr maybe it's the Java/Spark converter that's limited: perhaps\r\n\r\n```python\r\nas_arrow = ak.to_arrow(bikeroutes)\r\n```\r\n\r\nand then convert `as_arrow` to Parquet with pyarrow. The C++ implementation used to have this limitation about structs in Parquet, but maybe they've finished that by now.\r\n\r\n(The disappointing thing is that changing the nested struct configuration of columnar data is purely metadata transformations; Arrow should be able to do this!)",
  "created_at":"2020-06-18T22:02:41Z",
  "id":646328008,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjMyODAwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-18T22:02:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Interesting case, I wonder if you can tell me the difference between\r\n```\r\nbikeroutes = ak.from_json(\"Bikeroutes.geojson\")\r\n```\r\nand\r\n```\r\nbikeroutes_json = open(\"Bikeroutes.geojson\").read()\r\nbikeroutes_pyobj = json.loads(bikeroutes_json)\r\nbikeroutes2 = ak.Record(bikeroutes_pyobj)\r\n```\r\nThe former has some outer objects, but `bikeroutes[0][0]` appears identical to `bikeroutes2`; however, when used with the `compute_lengths` function, the former takes 1.95ms versus 170us (on my machine). Obviously, I would prefer the simpler-looking syntax.",
  "created_at":"2020-06-22T20:25:56Z",
  "id":647751681,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0Nzc1MTY4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-22T20:50:59Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I think they ought to be the same, but I didn't manage to fix the issue with `ak.from_json` before I needed to make this demo. In `ak.from_json`, I simply forgot to consider the case in which the JSON is a single JSON object, rather than a JSON array.\r\n\r\nInternally, `ak.from_json` calls a C++ `Content::fromjson` method, which uses RapidJSON in SAX mode to fill an ArrayBuilder. Not going through the Python objects would be a time-saver (and probably memory, too).\r\n\r\nThe `ak.Record` constructor would call `ak.from_json` if the argument had been a string. Since it's a Python object, it uses pybind11 to walk over the Python objects, feeding an ArrayBuilder.\r\n\r\nArrayBuilder assumes that what it's getting is some kind of array (not a single record). In other words, for arrays, you don't have to start with `begin_list` and end with `end_list`, just adding objects fills it with the assumption that they are array elements. This also means there's one fewer error condition to consider: adding a second object is not an error, but it would be if ArrayBuilder didn't assume that what it's getting is an array. Also, ArrayBuilder is user-visible, and forgetting to call `begin_list` and `end_list` outside of the main loop would be a common error, if it had been required.\r\n\r\nThus, if the data are in one big record, that's the special case that has to be handled: first by recognizing that the wrapping structure is a record, then by implicitly calling `begin_record` and `end_record`, then by pulling the finished record out of the length-one array. The `ak.Record` constructor does this because just by using it, the user is communicating the fact that they want a record. `ak.from_json` should go through the same procedure once it recognizes that the outermost JSON structure is a JSON object, not a JSON array.\r\n\r\nI'll make an issue out of this.",
  "created_at":"2020-06-22T20:57:59Z",
  "id":647765943,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0Nzc2NTk0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-22T20:57:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the detail, @jpivarski !",
  "created_at":"2020-06-22T21:01:56Z",
  "id":647767570,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0Nzc2NzU3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-22T21:01:56Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"**C++\u2019s sort()** \u2013 Introsort (Hybrid of Quicksort, Heap Sort and Insertion Sort)\r\n\r\n- Best Case Time Complexity- O(NlogN)\r\n- Average Case Time Complexity- O(NlogN)\r\n- Worse Case Time Complexity- O(NlogN)\r\n- Auxiliary Space- O(logN)\r\n- Stable- No\r\n- Adaptive- No\r\n\r\n**C++\u2019s stable_sort()** \u2013 Mergesort\r\n\r\n- Best Case Time Complexity- O(NlogN)\r\n- Average Case Time Complexity- O(NlogN)\r\n- Worse Case Time Complexity- O(NlogN)\r\n- Auxiliary Space- O(N)\r\n- Stable- Yes\r\n- Adaptive- Yes",
  "created_at":"2020-06-17T15:14:44Z",
  "id":645437539,
  "issue":304,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NTQzNzUzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-17T15:14:44Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Some screenshots - \r\n![image](https://user-images.githubusercontent.com/11775615/85139314-dd254300-b261-11ea-940b-7f34d33d893a.png)\r\n![image](https://user-images.githubusercontent.com/11775615/85139358-ef06e600-b261-11ea-8da6-f4aaa43a3a33.png)\r\n\r\nWhat do you think?\r\n\r\nI wanted to use Black to format the code I wrote by hand for the kernels in sorting.cpp (similar to the generated Python translations of kernels) but there was a weird indentation error I was unable to handle :(",
  "created_at":"2020-06-19T13:52:18Z",
  "id":646648664,
  "issue":306,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjY0ODY2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-19T13:52:18Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"These are looking good. I didn't realize that you'd try manual conversions, but that's good because it will eventually be needed. Eventually, hard-coded Python will be the normative definitions; most of these normative definitions will come from automatic conversions before being hard-coded, but the sorting ones will be hard-coded by hand.\r\n\r\nSince you have manually written Python for the sorting kernels, you'll be able to include them in your tests and work out the bugs that way. That will be a good test, having both automatically converted kernels and manually written kernels\u2014each would catch errors the other doesn't.\r\n\r\nAbout formatting with Black; please do, for consistency and to avoid scrolling in the window. Black formatting has forced me to use fewer list comprehensions because comprehensions that would make the code easier to read and clearer on a long line make the code harder to read and more obscure with Black's indentation policies. As such, I usually rewrite list comprehensions into regular for loops if Black makes a mess of them. That might also make the Python more beginner-friendly because list comprehensions are an intermediate topic. (What I don't like is how everything gets longer, stretched vertically\u2014vertical screen space is a limited quantity, too! But oh well, it's how things are being done these days.)\r\n\r\nLet me know when you're done with the PR and I'll merge it. I like the screenshots. Maybe you want to say that the code samples you've written are \"not normative,\" rather than \"not accurate.\" At the moment, we're doing dry runs of building up a normative specification.",
  "created_at":"2020-06-19T14:05:41Z",
  "id":646655208,
  "issue":306,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjY1NTIwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-19T14:05:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The PR should be done now.",
  "created_at":"2020-06-19T14:33:05Z",
  "id":646668533,
  "issue":306,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NjY2ODUzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-19T14:33:16Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> On the names, why not \"input\" and \"output\"?\r\n\r\nI was thinking we should avoid doing that since `input` is a Python built-in function, but yes it will not throw an error, so I'll just use \"input\" and \"output\" instead of \"inparam\" and \"outparam\". \r\n\r\n> If you were planning on putting only the roles in doxygen, you'd have to have two separate lists of comments above each function, and that would get unwieldy.\r\n\r\nOkay",
  "created_at":"2020-06-20T15:03:07Z",
  "id":647006829,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NzAwNjgyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-20T15:03:07Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> I was thinking we should avoid doing that since input is a Python built-in function, but yes it will not throw an error, so I'll just use \"input\" and \"output\" instead of \"inparam\" and \"outparam\".\r\n\r\nGood point. I was just in another thread about CUDA interfaces, and they use \"inparam\" and \"outparam\" to indicate which arrays need to be copied, in which direction. (Or if it's not a CUDA thing, it's a PyCUDA and/or Numba thing.) So that might make it a good choice of words (because it's similar to something familiar) or bad (because we're not guaranteeing that the arrays get _copied_, just which ones are used as though they were). Probably the good outweighs the bad. It's your call, but worth looking into what words already exist.",
  "created_at":"2020-06-20T15:54:44Z",
  "id":647012808,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NzAxMjgwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-20T15:54:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I decided to go with \"inparam\" and \"outparam\" for now, but it should not be too hard to change it to something else in the future. \r\n\r\nThis is how it looks now - \r\n![image](https://user-images.githubusercontent.com/11775615/85207512-e805d380-b346-11ea-80b0-9da849658ae2.png)\r\n![image](https://user-images.githubusercontent.com/11775615/85207492-d7edf400-b346-11ea-9100-ccf47ca9d0a0.png)\r\n",
  "created_at":"2020-06-20T17:10:11Z",
  "id":647022090,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NzAyMjA5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-20T17:10:11Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"The screenshots of generated documentation are the easiest thing for me to check, so I'll comment on them first. Putting the roles in a string is a good idea, but how about making it a docstring? Just putting it under the `def` line with proper indentation would make it more Pythonic/satisfy the principle of least surprise.\r\n\r\nScalar quantities like `length` are being labeled as `inparam`, which is fine internally, but for the documentation, it makes it a little harder to see at a glance which arguments are arrays and which are scalars. Perhaps just for the rendering in the docs, only show \"`inparam`\" for array/pointer-typed arguments? I think all the information is there to generate the rst that way.\r\n\r\nNow I'm going to look more deeply into the diff.",
  "created_at":"2020-06-30T13:29:57Z",
  "id":651791779,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MTc5MTc3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-30T13:29:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I scanned the code changes and the suite of tests (JSON) look like a good start. You'll probably want a lot more of each, but it's good to start small.\r\n\r\nCan you show here an example of how the tests run and what gets run? This is for helping me understand what it is that you have done. For one thing, where are the tests? I'd like to see what they look like, how they run, what exactly they're testing, etc. At least temporarily (one does not want non-deterministic tests in a suite), we could try passing `ListArray.random(choices=[NumpyArray])` into some function, such as `awkward_ListArrayU32_num_64`, to see how the Python specification and cpu-kernels-through-ctypes deal with a large set of valid trials.\r\n\r\nAfter that, perhaps we could use the randomizers to generate a fixed set of trials (not as many, and put in some invalid examples by hand).",
  "created_at":"2020-07-16T19:50:57Z",
  "id":659633038,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTYzMzAzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-16T19:50:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This is the code block that executes the tests - \r\nhttps://github.com/scikit-hep/awkward-1.0/pull/307/files#diff-54d0b78db06034e98e68cf17edb59c64R505-R612\r\n\r\nTests = passing the same arguments in both the C and Python kernels by reference and comparing the arguments marked `outparam`. \r\n\r\nCurrently, just one set of arguments are passed to the kernels (from testcases.json). Next, I will test for `return failure` and then multiple sets of arguments.\r\n\r\nCan you elaborate a little on what testing example I should show you?",
  "created_at":"2020-07-16T20:14:32Z",
  "id":659646738,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTY0NjczOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-16T20:14:32Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"That link doesn't point me to any code block. It points to all differences in this PR, and I can't find any that are the tests themselves.",
  "created_at":"2020-07-16T20:32:23Z",
  "id":659655852,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTY1NTg1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-16T20:32:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Is this better? - https://github.com/scikit-hep/awkward-1.0/blob/reikdas/label/dev/genpython.py#L597-L612\r\n\r\n(This is not a blocker, so we can discuss this in more detail in tomorrow's meeting instead)",
  "created_at":"2020-07-16T20:36:48Z",
  "id":659658130,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTY1ODEzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-16T20:52:41Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"That helps; I didn't realize that it would be embedded within genpython.py. It would be better pulled out as something that can go in a testing suite. The genpython.py is starting to do too many things: making the Python and testing it. It should output the Python to a directory that will eventually become the specification, and the tests should start from the specification and ctypes.\r\n\r\nIt could also be more clear if it prints out the function it's testing, the values of the arguments, and the return values from the normative Python and cpu-kernels through ctypes. Then we can look at those print-outs as the thing to review. Also, you'll probably want them when tests fail.\r\n\r\nSorry that I've been distracted\u2014there are other things going on.",
  "created_at":"2020-07-16T21:07:02Z",
  "id":659673225,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTY3MzIyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-16T21:07:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"For some reason, I can't write a review.\r\n\r\nAnyway, what I see in kernel-specification/samples.json looks great! I'm going to check it out now to generate a specification and the tests to see what they look like. I see that the tests have passed (and they're in CI).",
  "created_at":"2020-08-03T18:36:15Z",
  "id":668178127,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE3ODEyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:36:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">For some reason, I can't write a review.\r\n\r\nI clicked on \"re-request review\". Maybe that will fix it?",
  "created_at":"2020-08-03T18:37:59Z",
  "id":668178922,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE3ODkyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:37:59Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> I clicked on \"re-request review\". Maybe that will fix it?\r\n\r\nI suspect my network, that it's a page not loading... But it's not a thing I want to delve into at the moment. I'm compiling your branch now.",
  "created_at":"2020-08-03T18:40:43Z",
  "id":668180173,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE4MDE3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:40:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm looking at kernel-specification/operations/awkward_ListArray_num.yml. YAML accepts single-quoted strings? (If so, and if it means that they're easier to make because you can just `repr` the Python strings, then that's a good enough argument for using them.)\r\n\r\n```yaml\r\n  parameters: ['tonum', 'fromstarts', 'startsoffset', 'fromstops', 'stopsoffset', 'length']\r\n  inparams: ['fromstarts', 'startsoffset', 'fromstops', 'stopsoffset', 'length']\r\n  outparams: ['tonum']\r\n```",
  "created_at":"2020-08-03T18:47:34Z",
  "id":668183362,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE4MzM2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:47:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is the \"arguments as an explicit list\" that I was asking about on Slack over the weekend. I could have sworn it was always here, right? Or were the \"` - `\" missing before the `tonum:`, `fromstarts:`, etc?\r\n\r\n```yaml\r\n- name: awkward_ListArray_num\r\n  specializations:\r\n    - name: awkward_ListArray32_num_64\r\n      args:\r\n        - tonum: List[int64_t]\r\n        - fromstarts: List[int32_t]\r\n        - startsoffset: int64_t\r\n        - fromstops: List[int32_t]\r\n        - stopsoffset: int64_t\r\n        - length: int64_t\r\n```",
  "created_at":"2020-08-03T18:49:13Z",
  "id":668184168,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE4NDE2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:49:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, I see: it's also here\r\n\r\n```yaml\r\n  parameters: ['tonum', 'fromstarts', 'startsoffset', 'fromstops', 'stopsoffset', 'length']\r\n```\r\n\r\nIt seems that you could get this information from the first specialization, right? (There's always at least one.)",
  "created_at":"2020-08-03T18:53:01Z",
  "id":668185816,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE4NTgxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:53:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">YAML accepts single-quoted strings?\r\n\r\nYes - https://stackoverflow.com/a/22235064/4647107\r\nBut should I remove them? I think it is easier to write to the file as strings. (I don't explicitly put in the quotes; I just write Python strings to a file)\r\n\r\n>This is the \"arguments as an explicit list\" that I was asking about on Slack over the weekend. I could have sworn it was always here, right? Or were the \"-\" missing before the tonum:, fromstarts:, etc?\r\n\r\nAhhh yes. It was there. I didn't need to reinvent this ordering with `parameters`. (I was viewing it as a dict and missed that it is indeed a list of dicts).\r\nI think we can remove the `parameters` key from the specification. Should I?\r\n\r\n>It seems that you could get this information from the first specialization, right? (There's always at least one.)\r\n\r\n(Ignore last question)\r\nYes! I'll remove this :) ",
  "created_at":"2020-08-03T18:53:52Z",
  "id":668186179,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE4NjE3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:53:52Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"I see that kernel-specification/operations/awkward_ListArray_broadcast_tooffsets.yml is demonstrating the right combinatorics: the `fromoffsets` comes from a different group than this ListArray's `fromstarts` and `fromstops`, so you do make a Cartesian product of a fixed set of `fromstarts`, `fromstops` pairs and `fromoffsets` samples. I don't see where the `fromoffsets` comes from (not in kernel-specification/samples.json), but the combinatorics is right.",
  "created_at":"2020-08-03T18:58:19Z",
  "id":668188115,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE4ODExNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:58:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I think we can remove the `parameters` key from the specification. Should I?\r\n> \r\n> > It seems that you could get this information from the first specialization, right? (There's always at least one.)\r\n> \r\n> (Ignore last question)\r\n> Yes! I'll remove this :)\r\n\r\nThat should be a relatively easy change because whatever variable you store the `parameters` in can be populated from the specialization, and since we haven't committed YAML into the repo yet, that's still very flexible.",
  "created_at":"2020-08-03T19:00:17Z",
  "id":668188940,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE4ODk0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T19:00:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It seems that\r\n\r\n```bash\r\n% git status\r\nOn branch reikdas/label\r\nYour branch is up-to-date with 'origin/reikdas/label'.\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\r\n        dev/kernels.py\r\n\r\nnothing added to commit but untracked files present (use \"git add\" to track)\r\n```\r\n\r\nis generated but not git-ignored.",
  "created_at":"2020-08-03T19:02:02Z",
  "id":668189744,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE4OTc0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T19:02:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">I don't see where the fromoffsets comes from (not in kernel-specification/samples.json), but the combinatorics is right.\r\n\r\nIt is from ListOffsetArray-offsets - https://github.com/scikit-hep/awkward-1.0/blob/reikdas/label/include/awkward/cpu-kernels/operations.h#L641",
  "created_at":"2020-08-03T19:03:21Z",
  "id":668190430,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE5MDQzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T19:03:21Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"That's a lot of tests\u2014but they all pass!\r\n\r\nThe \"not tested\" messages; do they mean that a test hasn't been created for some reason? pytest has its own skipping mechanism that we can use by decorating some of them with pytest marks (and then the `-rs` has it list them after the test-execution run).\r\n\r\nTo get everything in one place, perhaps if you have reason to not make a test (what reasons?), you should create a dummy test, marked with a pytest skip, containing `raise NotImplementedError`?",
  "created_at":"2020-08-03T19:06:39Z",
  "id":668191894,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE5MTg5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T19:06:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">The \"not tested\" messages; do they mean that a test hasn't been created for some reason?\r\n\r\nIt means that we are unable to generate any tests for that kernel (and can be fixed by tuning the `samples.json` file)\r\n\r\n>To get everything in one place, perhaps if you have reason to not make a test (what reasons?), you should create a dummy test, marked with a pytest skip, containing raise NotImplementedError?\r\n\r\nOkay!",
  "created_at":"2020-08-03T19:10:45Z",
  "id":668193658,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE5MzY1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T19:10:45Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Now looking at the tests (which, by the way, are awesome: \"`3206 passed in 12.66s`\"). The output arrays are compared with a tolerance of 0.0001, but nearly all of these are integers.\r\n\r\npytest provides more useful output if you assert that the whole array is equal, like\r\n\r\n```python\r\n    assert list(carrylen) == [15]\r\n```\r\n\r\ninstead of (in tests-kernels/test_cpuawkward_ListArray_getitem_jagged_carrylen_64.py)\r\n\r\n```python\r\n    outcarrylen = [15]\r\n    for i in range(len(outcarrylen)):\r\n        assert abs(carrylen[i] - outcarrylen[i]) <= 0.0001\r\n```\r\n\r\n(The latter also runs the risk of shadowing a Python variable named \"`outcarrylen`\", which is a likely-sounding function parameter name. Another thing you could do to protect your generated code from variable name conflicts is to put the parameter names inside a dict: `p[\"carrylen\"]` or to prepend them with a substring that you promise yourself not to use elsewhere: `p_carrylen`.)\r\n\r\nFor floating point arrays, you can use [pytest.approx](https://docs.pytest.org/en/latest/reference.html#pytest-approx) to compare lists, allowing for round-off errors in the elements of the lists. NumPy also has a [numpy.testing.assert_alllose](https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_allclose.html#numpy.testing.assert_allclose), but the pytest function would probably give better output in the case of errors.\r\n\r\nThe pytest.approx function takes both relative and absolute tolerances, but let's not pick a number like `0.0001` (let pytest decide) until we see what borderline cases look like. In principle, we should only start seeing approximation errors in the CUDA kernels, since they'll be affected by the non-associativity of floating point numbers. Oh! We should also see rather large (`1e-7`) errors from single-precision floating point.",
  "created_at":"2020-08-03T19:25:58Z",
  "id":668199924,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE5OTkyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T19:25:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Everything I had to comment on was cosmetic: mostly names, not structures. I'll leave it to you to determine when you're done with these fixes and then ping me and I'll merge the PR.\r\n\r\nThanks!",
  "created_at":"2020-08-03T19:36:37Z",
  "id":668204498,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODIwNDQ5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T19:36:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> It seems that\r\n> \r\n> ```shell\r\n> % git status\r\n> On branch reikdas/label\r\n> Your branch is up-to-date with 'origin/reikdas/label'.\r\n> Untracked files:\r\n>   (use \"git add <file>...\" to include in what will be committed)\r\n> \r\n>         dev/kernels.py\r\n> \r\n> nothing added to commit but untracked files present (use \"git add\" to track)\r\n> ```\r\n> \r\n> is generated but not git-ignored.\r\n\r\nI am unable to reproduce this.. `dev/kernels.py` should not be generated.",
  "created_at":"2020-08-03T21:09:17Z",
  "id":668243137,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODI0MzEzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T21:09:17Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> I am unable to reproduce this.. `dev/kernels.py` should not be generated.\r\n\r\nThen it's very likely that this was leftover from some other test I ran; not an issue with the PR at all. Go ahead and ignore it.",
  "created_at":"2020-08-03T21:20:10Z",
  "id":668247274,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODI0NzI3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T21:20:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> pytest provides more useful output if you assert that the whole array is equal, like\r\n> \r\n>     assert list(carrylen) == [15]\r\n\r\nDirect conversion of `ctypes` returned array to a Python `list` is a little weird. Iteration seems like the easiest way to do this.\r\n\r\nIf that is okay and CI tests pass, I am done fixing the changes you suggested and you can merge if the changes look okay :D\r\n\r\n",
  "created_at":"2020-08-03T21:54:48Z",
  "id":668260700,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODI2MDcwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T21:54:48Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> Direct conversion of `ctypes` returned array to a Python `list` is a little weird. Iteration seems like the easiest way to do this.\r\n\r\nIf iteration works, you can do\r\n\r\n```python\r\n[int(x) for x in carrylen] == [15]\r\n```\r\n\r\nThe main thing I was trying to get was to have the `==` be on the whole list, rather than an individual element, because then if it's not equal, the error message would display the two lists side by side, highlighting the places where they differ. If the assertion is on an individual element, it will say something like \"2 is not equal to 3,\" but we have no idea if the whole lists are different, if that's a difference in one element, if it's at the beginning or end, etc. It will speed up debugging.\r\n\r\nBut that can be for a later PR if you want. Considering the above, are you ready for me to merge this?",
  "created_at":"2020-08-03T22:06:46Z",
  "id":668264991,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODI2NDk5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T22:06:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, please merge it. I'll make that change in a new PR.",
  "created_at":"2020-08-03T22:16:45Z",
  "id":668268351,
  "issue":307,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODI2ODM1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T22:16:45Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This should be the complete list of kernels up to now which have loop carried dependencies (my apologies if I missed any) - \r\n\r\n- awkward_NumpyArray_getitem_boolean_nonzero - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-numpyarray-getitem-boolean-nonzero\r\n- awkward_ListArray_getitem_next_range_counts - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-getitem-next-range-counts\r\n- awkward_IndexedArray_numnull - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-indexedarray-numnull\r\n- awkward_IndexedArray_getitem_nextcarry_outindex - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-indexedarray-getitem-nextcarry-outindex\r\n- awkward_IndexedArray_getitem_nextcarry_outindex_mask - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-indexedarray-getitem-nextcarry-outindex-mask\r\n- awkward_ListOffsetArray_getitem_adjust_offsets - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listoffsetarray-getitem-adjust-offsets\r\n- awkward_ListOffsetArray_getitem_adjust_offsets_index - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listoffsetarray-getitem-adjust-offsets-index\r\n- awkward_IndexedArray_getitem_adjust_outindex - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-indexedarray-getitem-adjust-outindex\r\n- awkward_UnionArray_project - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-unionarray-project\r\n- awkward_ListArray_getitem_jagged_carrylen - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-getitem-jagged-carrylen\r\n- awkward_ListArray_getitem_jagged_apply - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-getitem-jagged-apply\r\n- awkward_ListArray_getitem_jagged_numvalid - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-getitem-jagged-numvalid\r\n- awkward_ListArray_getitem_jagged_shrink - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-getitem-jagged-shrink\r\n- awkward_ListArray_getitem_jagged_descend - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-getitem-jagged-descend\r\n- awkward_ByteMaskedArray_numnull - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-bytemaskedarray-numnull\r\n- awkward_ByteMaskedArray_getitem_nextcarry - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-bytemaskedarray-getitem-nextcarry\r\n- awkward_ByteMaskedArray_getitem_nextcarry_outindex - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-bytemaskedarray-getitem-nextcarry-outindex\r\n- awkward_MaskedArray_getitem_next_jagged_project - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-maskedarray-getitem-next-jagged-project\r\n- awkward_IndexedArray_flatten_none2empty - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-indexedarray-flatten-none2empty\r\n- awkward_UnionArray_flatten_length - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-unionarray-flatten-length\r\n- awkward_UnionArray_flatten_combine - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-unionarray-flatten-combine\r\n- awkward_IndexedArray_flatten_nextcarry - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-indexedarray-flatten-nextcarry\r\n- awkward_ListArray_compact_offsets - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-compact-offsets\r\n- awkward_IndexedOptionArray_rpad_and_clip_mask_axis1 - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-indexedoptionarray-rpad-and-clip-mask-axis1\r\n- awkward_ListOffsetArray_rpad_length_axis1 - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listoffsetarray-rpad-length-axis1\r\n- awkward_ListOffsetArray_rpad_axis1 - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listoffsetarray-rpad-axis1\r\n- awkward_ListArray_combinations_length - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-combinations-length\r\n- awkward_ListOffsetArray_reduce_nonlocal_maxcount_offsetscopy_64 - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listoffsetarray-reduce-nonlocal-maxcount-offsetscopy-64\r\n- awkward_ListOffsetArray_reduce_nonlocal_preparenext_64 - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listoffsetarray-reduce-nonlocal-preparenext-64\r\n- awkward_ListOffsetArray_reduce_nonlocal_findgaps_64 - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listoffsetarray-reduce-nonlocal-findgaps-64\r\n- awkward_ListOffsetArray_Reduce_nonlocal_outstartsstops_64 -https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listoffsetarray-reduce-nonlocal-outstartsstops-64\r\n- awkward_IndexedArray_reduce_next64 - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-indexedarray-reduce-next-64\r\n- awkward_ByteMaskedArray_reduce_next_64 - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-bytemaskedarray-reduce-next-64\r\n- awkward_sorting_ranges_length - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-sorting-ranges-length\r\n- awkward_ListArray_min_range - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-min-range\r\n- awkward_ListArray_rpad_and_clip_length_axis1 - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward-listarray-rpad-and-clip-length-axis1",
  "created_at":"2020-07-12T11:01:15Z",
  "id":657206480,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NzIwNjQ4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-02T12:00:04Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The new categorization of kernels by parallelizability - \r\n\r\n**Embarrassingly Parallel**\r\n\r\n1. awkward_Index8_to_Index64\r\n2. awkward_IndexU8_to_Index64\r\n3. awkward_Index32_to_Index64\r\n4. awkward_IndexU32_to_Index64\r\n5. awkward_index_carry\r\n6. awkward_index_carry_nocheck\r\n7. awkward_slicemissing_check_same\r\n8. awkward_carry_arange\r\n9. awkward_Identities_getitem_carry\r\n10. awkward_NumpyArray_contiguous_init\r\n11. awkward_NumpyArray_contiguous_next\r\n12. awkward_NumpyArray_getitem_next_at\r\n13. awkward_NumpyArray_getitem_next_range\r\n14. awkward_NumpyArray_getitem_next_range_advanced\r\n15. awkward_NumpyArray_getitem_next_array\r\n16. awkward_NumpyArray_getitem_next_array_advanced\r\n17. awkward_ListArray_getitem_next_at\r\n18. awkward_ListArray_getitem_next_array\r\n19. awkward_ListArray_getitem_next_array_advanced\r\n20. awkward_ListArray_getitem_carry\r\n21. awkward_RegularArray_getitem_next_range\r\n22. awkward_RegularArray_getitem_next_range_spreadadvanced\r\n23. awkward_RegularArray_getitem_next_array_regularize\r\n24. awkward_RegularArray_getitem_next_array\r\n25. awkward_RegularArray_getitem_next_array_advanced\r\n26. awkward_RegularArray_getitem_carry\r\n27. awkward_IndexedArray_getitem_carry\r\n28. awkward_missing_repeat\r\n29. awkward_RegularArray_getitem_jagged_expand\r\n30. awkward_ListArray_getitem_jagged_expand\r\n31. awkward_ByteMaskedArray_getitem_carry\r\n32. awkward_ByteMaskedArray_toIndexedOptionArray\r\n33. awkward_new_Identities\r\n34. awkward_Identities32_to_Identities64\r\n35. awkward_Identities_from_ListOffsetArray\r\n36. awkward_Identities_from_RegularArray\r\n37. awkward_Identities_extend\r\n38. awkward_ListArray_num\r\n39. awkward_RegularArray_num\r\n40. awkward_ListOffsetArray_flatten_offsets\r\n41. awkward_IndexedArray_overlay_mask\r\n42. awkward_IndexedArray_mask\r\n43. awkward_ByteMaskedArray_mask\r\n44. awkward_zero_mask\r\n45. awkward_IndexedArray_simplify\r\n46. awkward_RegularArray_compact_offsets\r\n47. awkward_ListOffsetArray_compact_offsets\r\n48. awkward_RegularArray_broadcast_tooffsets\r\n49. awkward_NumpyArray_fill_frombool\r\n50. awkward_NumpyArray_fill_tobool\r\n51. awkward_NumpyArray_fill\r\n52. awkward_ListArray_fill\r\n53. awkward_IndexedArray_fill\r\n54. awkward_IndexedArray_fill_count\r\n55. awkward_UnionArray_filltags\r\n56. awkward_UnionArray_fillindex\r\n57. awkward_UnionArray_filltags_const\r\n58. awkward_UnionArray_fillindex_count\r\n59. awkward_UnionArray_simplify\r\n60. awkward_UnionArray_simplify_one\r\n61. awkward_ListArray_validity\r\n62. awkward_IndexedArray_validity\r\n63. awkward_UnionArray_validity\r\n64. awkward_UnionArray_fillna\r\n65. awkward_index_rpad_and_clip_axis0\r\n66. awkward_RegularArray_rpad_and_clip_axis1\r\n67. awkward_localindex\r\n68. awkward_RegularArray_localindex\r\n69. awkward_ByteMaskedArray_overlay_mask\r\n70. awkward_BitMaskedArray_to_ByteMaskedArray\r\n71. awkward_BitMaskedArray_to_IndexedOptionArray\r\n72. awkward_content_reduce_zeroparents_64\r\n73. awkward_IndexedArray_reduce_next_fix_offsets_64\r\n74. awkward_NumpyArray_reduce_adjust_starts_64\r\n75. awkward_NumpyArray_reduce_adjust_starts_shifts_64\r\n76. awkward_NumpyArray_reduce_mask_ByteMaskedArray_64\r\n77. awkward_RegularArray_getitem_next_at\r\n78. awkward_regularize_arrayslice\r\n\r\n**Loop carried variable**\r\n1. awkward_NumpyArray_getitem_boolean_nonzero\r\n2. awkward_IndexedArray_getitem_nextcarry_outindex\r\n3. awkward_IndexedArray_getitem_nextcarry_outindex_mask\r\n4. awkward_IndexedArray_getitem_adjust_outindex\r\n5. awkward_IndexedArray_getitem_nextcarry\r\n6. awkward_ByteMaskedArray_getitem_nextcarry\r\n7. awkward_ByteMaskedArray_getitem_nextcarry_outindex\r\n8. awkward_Content_getitem_next_missing_jagged_getmaskstartstop\r\n9. awkward_MaskedArray_getitem_next_jagged_project\r\n10. awkward_IndexedArray_flatten_nextcarry\r\n11. awkward_ListArray_broadcast_tooffsets\r\n12. awkward_RegularArray_broadcast_tooffsets_size1\r\n13. awkward_IndexedOptionArray_rpad_and_clip_mask_axis1\r\n14. awkward_index_rpad_and_clip_axis1\r\n15. awkward_ListOffsetArray_rpad_axis1\r\n16. awkward_ListOffsetArray_reduce_nonlocal_preparenext_64\r\n17. awkward_IndexedArray_reduce_next_64\r\n18. awkward_IndexedArray_reduce_next_nonlocal_nextshifts_64\r\n19. awkward_IndexedArray_reduce_next_nonlocal_nextshifts_fromshifts_64\r\n20. awkward_ByteMaskedArray_reduce_next_64\r\n21. awkward_ByteMaskedArray_reduce_next_nonlocal_nextshifts_64\r\n22. awkward_ByteMaskedArray_reduce_next_nonlocal_nextshifts_fromshifts_64\r\n23. awkward_UnionArray_project\r\n24. awkward_ListOffsetArray_rpad_and_clip_axis1\r\n25. awkward_ListOffsetArray_reduce_local_nextparents_64\r\n\r\n**Loop carried dependency through array element**\r\n\r\n1. awkward_ListArray_getitem_jagged_descend\r\n2. awkward_ListArray_compact_offsets\r\n\r\n**Restricted Loop carried array**\r\n\r\n1. awkward_reduce_argmax_bool_64\r\n2. awkward_reduce_argmax\r\n3. awkward_reduce_argmin\r\n4. awkward_reduce_argmin_bool_64\r\n5. awkward_reduce_count_64\r\n6. awkward_reduce_countnonzero\r\n7. awkward_reduce_max\r\n8. awkward_reduce_min\r\n9. awkward_reduce_prod_bool\r\n10. awkward_reduce_sum\r\n11. awkward_reduce_sum_int64_bool_64\r\n12. awkward_reduce_sum_int32_bool_64\r\n13. awkward_reduce_sum_bool\r\n14. awkward_reduce_prod\r\n15. awkward_reduce_prod_int64_bool_64\r\n16. awkward_reduce_prod_int32_bool_64\r\n\r\n**Simple Reducer**\r\n\r\n1. awkward_ByteMaskedArray_numnull\r\n2. awkward_IndexedArray_numnull\r\n3. awkward_ListArray_getitem_jagged_carrylen\r\n4. awkward_ListArray_getitem_jagged_numvalid\r\n5. awkward_ListArray_getitem_next_range_counts\r\n6. awkward_NumpyArray_Getitem_boolean_numtrue\r\n7. awkward_UnionArray_regular_index_getsize\r\n8. awkward_ListArray_min_range\r\n9. awkward_ListArray_rpad_and_clip_length_axis1\r\n10. awkward_ListOffsetArray_toRegularArray\r\n11. awkward_UnionArray_flatten_length\r\n12. awkward_ListOffsetArray_Reduce_nonlocal_maxcount_offsetscopy_64\r\n\r\n**Others**\r\n\r\n1. awkward_ListArray_getitem_next_range_carrylength\r\n2. awkward_ListArray_getitem_next_range\r\n3. awkward_ListOffsetArray_getitem_adjust_offsets\r\n4. awkward_ListOffsetArray_getitem_adjust_offsets_index\r\n5. awkward_ListArray_getitem_jagged_shrink\r\n6. awkward_IndexedArray_flatten_none2empty\r\n7. awkward_ListOffsetArray_rpad_length_axis1\r\n8. awkward_ListArray_combinations_length\r\n9. awkward_RegularArray_combinations\r\n10. awkward_ListArray_getitem_jagged_apply\r\n11. awkward_ListArray_getitem_next_range_spreadadvanced\r\n12. awkward_UnionArray_regular_index\r\n13. awkward_ListArray_localindex\r\n14. awkward_ListOffsetArray_reduce_local_outoffsets_64\r\n15. awkward_ListArray_rpad_axis1\r\n16. awkward_ListOffsetArray_reduce_nonlocal_nextshifts_64\r\n17. awkward_ListOffsetArray_reduce_nonlocal_outstartsstops_64\r\n18. awkward_ListOffsetArray_reduce_nonlocal_findgaps_64\r\n19. awkward_Identities_from_ListArray\r\n20. awkward_Identities_from_IndexedArray\r\n21. awkward_Identities_from_UnionArray\r\n22. awkward_UnionArray_flatten_combine\r\n23. awkward_ListOffsetArray_reduce_nonlocal_nextstarts_64\r\n\r\n**Missing Python Code**\r\n\r\n1. awkward_NumpyArray_contiguous_copy\r\n2. awkward_NumpyArray_Getitem_next_null\r\n3. awkward_slicearray_ravel\r\n4. awkward_combinations\r\n5. awkward_argsort\r\n6. awkward_sorting_ranges\r\n7. awkward_sorting_ranges_length\r\n8. awkward_sort\r\n9. awkward_ListOffsetArray_local_preparenext_64\r\n10. awkward_NumpyArray_sort_asstrings_uint8\r\n11. awkward_IndexedArray_local_preparenext_64\r\n\r\n**No loop**\r\n\r\n1. awkward_ListOffsetArray_reduce_global_startstop_64\r\n\r\n**Unimplemented**\r\n\r\n1. awkward_ListArray_combinations\r\n",
  "created_at":"2020-09-11T19:54:17Z",
  "id":691284390,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTI4NDM5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-11T19:54:17Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"I think this is done; the card can be moved. (Unless you want to define \"done\" by having an appropriate set of proto-samples\u2014though that could be a new card, too.)",
  "created_at":"2020-08-06T17:14:31Z",
  "id":670061910,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MDA2MTkxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-06T17:14:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I am closing this. I'll make the appropriate set of proto-samples a new card. \r\n(Closed by #307)",
  "created_at":"2020-08-06T17:49:39Z",
  "id":670081782,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MDA4MTc4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-06T17:49:39Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"This would not be too hard to fix, though it would likely involve the C++, not just the Python (or it might be exclusively C++; not sure).",
  "created_at":"2020-06-22T21:02:07Z",
  "id":647767643,
  "issue":311,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0Nzc2NzY0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-22T21:02:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Done, as part of #364.",
  "created_at":"2020-08-06T17:17:43Z",
  "id":670063712,
  "issue":311,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MDA2MzcxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-06T17:17:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"In practice, I believe this would need converting large_binary -> binary and LargeStringArray->StringArray in to_arrow; but I also wonder at large_list and the like.",
  "created_at":"2020-06-23T20:04:22Z",
  "id":648388810,
  "issue":312,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0ODM4ODgxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-23T21:08:54Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a good point and worth fixing, though I know how it happened. Awkward defaults to 64-bit when not constrained by inputs, but Arrow likes 32-bit. The policy should be different: instead of trying the closest type-match, it should use minimum sizes for the scale of the array.",
  "created_at":"2020-06-24T03:46:56Z",
  "id":648565642,
  "issue":312,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0ODU2NTY0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-24T03:46:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's also a good first issue because the translation is all in two functions in awkward1.operatons.convert, on the Python side.",
  "created_at":"2020-06-24T03:50:10Z",
  "id":648566401,
  "issue":312,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0ODU2NjQwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-24T03:50:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> but Arrow likes 32-bit\r\n\r\nIn this case, also a limitation of parquet's string representation.",
  "created_at":"2020-06-24T13:24:39Z",
  "id":648817833,
  "issue":312,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0ODgxNzgzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-24T13:24:39Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, fixing this bug means the following array copy in Uproot won't be necessary:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/43306c11eb750e78247a44fd30a2b0a88b0c4449/uproot4/interpretation/library.py#L140-L141\r\n\r\nThis will happen again when dealing with fixed-width objects (i.e. TLorentzVector), so it's worth fixing and dealing with the Awkward1 \u2194 Uproot4 version synchronization.",
  "created_at":"2020-06-26T15:26:39Z",
  "id":650240640,
  "issue":313,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MDI0MDY0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-26T15:26:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for taking this up! I just finished updating your #261 and will be merging it now.",
  "created_at":"2020-06-30T19:08:02Z",
  "id":651987247,
  "issue":314,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MTk4NzI0Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-06-30T19:08:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to merge with master for you, then give you back the branch.",
  "created_at":"2020-07-09T15:49:54Z",
  "id":656205973,
  "issue":314,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NjIwNTk3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-09T15:49:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It should be up to date now. (I'm not making any more changes.)",
  "created_at":"2020-07-09T15:55:34Z",
  "id":656208977,
  "issue":314,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NjIwODk3Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-07-09T15:55:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - The tests pass as expected. I've also added more tests for the negative axis. Please, review when you have time. Thanks! ",
  "created_at":"2020-07-20T16:24:58Z",
  "id":661153174,
  "issue":314,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTE1MzE3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T16:24:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks, @jpivarski ! ",
  "created_at":"2020-07-21T07:21:44Z",
  "id":661683161,
  "issue":314,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTY4MzE2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-21T07:21:44Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The part about\r\n\r\n> However, awkward does depart from numy as soon as a ListArray is involved.\r\n\r\nis a different topic from the bug reported at\r\n\r\n> However, there are a few cases where this breaks down.\r\n\r\nThe first topic (differing from NumPy) is known and might be called a \"gotcha\" rather than a bug: we want jagged indexing to be recursive, rather than broadcasted, because we want ufunc'ed predicates to work as jagged boolean slices. The simple cases work as people would expect\u2014it takes some careful digging to realize that it's different from what NumPy does. The argument goes, \"If you're paying that close attention, you should also notice the difference between `RegularType` and `ListType`.\"\r\n\r\nThe latter part about `f[g2]` and `f[g3]` is actually a bug: None values in the place of lists should count as part of the length of the array. Here is a better example, from our Slack discussion on June 22:\r\n\r\n```python\r\n>>> a = ak.Array([[0, 1, 2], [3, 4], [5, 6, 7, 8]])\r\n```\r\n\r\nThe following should not work:\r\n\r\n```python\r\n>>> a[[[False, True, True], [True, False], None, [True, False, False, True]]]\r\n<Array [[1, 2], [3], None, [5, 8]] type='4 * option[var * int64]'>\r\n```\r\n\r\nand the following should work:\r\n\r\n```python\r\n>>> a[[[False, True, True], None, [True, False, False, True]]]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/pivarski/miniconda3/lib/python3.7/site-packages/awkward1/highlevel.py\", line 844, in __getitem__\r\n    return awkward1._util.wrap(self._layout[where], self._behavior)\r\nValueError: cannot fit jagged slice with length 2 into RegularArray of size 3\r\n```\r\n\r\nBasically, the constraint _ought_ to be that the number of top-level elements in the predicate should be equal to the number of top-level elements in the array `a`. Instead, the number of non-None top-level elements in the predicate is constrained to be equal to the number of top-level elements in the array `a`. What _ought_ to happen, after removing the None values and moving down to handle the lists inside, is that the missing lists should be replaced with empty lists at the next level down.\r\n\r\n-------\r\n\r\nHere's what I said the next morning about how to fix this:\r\n\r\n> I was thinking about it this morning. What's probably happening is that project is being called on the option-type array (e.g. IndexedOptionArray), in which None values are removed and you get a non-option-type array of non-None values. There's a variant of this used internally in many places that both projects and keeps track of where the None values were, to put them back in after the operation is carried through on the option-type's content. That's probably why the MissingArraySlice(JaggedArraySlice(...)) acts like a JaggedArraySlice(...) of the non-None values and then puts the Nones back in as placeholders.\r\n> \r\n> That project-and-reintroduce Nones technique is nice because it's independent of what's inside the option-type array, avoiding an explosion of special cases. But it does the wrong thing here.\r\n> \r\n> This MissingArraySlice(JaggedArraySlice(...)) is already a special case, and it's good to treat it as such. The other case, MissingArraySlice(ArraySlice()) already works (that's what we determined last night; it's the one Arrow can do) and there aren't any other cases beyond these two: the only Awkward Arrays allowed as slices are option-type (MissingArraySlice(...)), list-type (JaggedArraySlice(...)), and regular, NumPy-like arrays (ArraySlice()). Missing-of-missing shouldn't be allowed\u2014at least, it should be caught and fixed further upstream, not as part of this problem\u2014so the only incorrectly implemented case is MissingArraySlice(JaggedArraySlice(...)). (This includes a possible jagged-of-jagged-of-jagged chain. After the first jagged, the normal machinery can take it the rest of the way down the chain.)\r\n> \r\n> Supposing we have MissingArraySlice(JaggedArraySlice(...)); what we want to do is not project, but to make a new JaggedArraySlice(...) in which the spots occupied by None are replaced with empty arrays, apply the JaggedArraySlice(...), and then cover the empty array output at the None positions with a ByteMaskedArray that hides them. This is not a project and it explicitly makes use of the knowledge that what the MissingArraySlice contains is a JaggedArraySlice (down one level; it doesn't require knowledge of what's inside of that).\r\n> \r\n> The \"replace None with empty list\" strategy only works for integer array slices, but boolean array slices are converted into integer array slices early in the process. Actually passing around boolean array slices is a future optimization that will complicate these implementations. For now, the fact that you'll only ever see integer array slices at this part of the code just means fewer complications to think about.\r\n> \r\n> It is 100% okay if this requires a new kernel (i.e. custom for loops over the option-type index or the jagged offsets). If you write the for loop in place, in the C++, I can help with the boilerplate of moving that into the cpu-kernels library. We're not bound to minimizing the number of distinct kernels.",
  "created_at":"2020-07-01T22:56:24Z",
  "id":652685283,
  "issue":315,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MjY4NTI4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-01T22:56:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I only mention the ListArray behavior to set up and explain why `f[g]` can even select multiple dimensions (a not-obvious feature from a numpy user's point of view) I don't think its a bug.\r\nI'd like your thoughts on `f[g3]` behavior as that example is more subtle.",
  "created_at":"2020-07-02T13:54:58Z",
  "id":653020193,
  "issue":315,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MzAyMDE5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-02T13:54:58Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"The `f[g2]` and `f[g3]` examples have the additional complication that they involve EmptyArrays, which have floating point type in normal usage (for consistency with NumPy's default), but have integer type when used as a slice (I checked yesterday, thinking that might be relevant for this issue). For user sanity, it should be either integer or Boolean type, since these are the only array types allowed in slices, and if assuming it's Boolean doesn't lead to an error (because of mismatched lengths), assuming it's integer leads to the same answer.\r\n\r\nThe fact that `f[g3]` converts missing values into empty lists probably shouldn't be allowed. A definitive answer requires more thought\u2014attempting to implement it often provides such clarity\u2014but at the moment, I'm leaning toward no. `f[g2]` is fine.",
  "created_at":"2020-07-02T14:27:10Z",
  "id":653038483,
  "issue":315,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MzAzODQ4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-02T14:27:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"In the end,\r\n```\r\n>>> g3 = ak.Array([[], [], [], []])\r\n>>> f[g3]\r\n<Array [[], None, [], []] type='4 * option[var * ?int64]'>\r\n```\r\nwas the most straightforward implementation.",
  "created_at":"2020-07-09T17:06:15Z",
  "id":656244737,
  "issue":315,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NjI0NDczNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-09T17:06:15Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Closed in #322 ",
  "created_at":"2020-07-09T22:02:31Z",
  "id":656372632,
  "issue":315,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NjM3MjYzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-09T22:02:31Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi! Thanks for looking into this again. Since I work on Linux, I've been seeing the warnings for a while now, thinking I should look into it and figure out how to fix it. Maybe that's still true because the warnings are still present in the Linux build (below is from Azure, the Linux + Python 3.8 build). I also tried compiling it on my computer, and see the same warnings.\r\n\r\n```\r\n2020-07-03T08:21:31.1652947Z   -- The CXX compiler identification is GNU 5.4.0\r\n2020-07-03T08:21:31.1724229Z   -- Check for working CXX compiler: /usr/bin/g++\r\n2020-07-03T08:21:31.2408078Z   -- Check for working CXX compiler: /usr/bin/g++ - works\r\n2020-07-03T08:21:31.2419326Z   -- Detecting CXX compiler ABI info\r\n2020-07-03T08:21:31.3086722Z   -- Detecting CXX compiler ABI info - done\r\n2020-07-03T08:21:31.3288860Z   -- Detecting CXX compile features\r\n2020-07-03T08:21:31.3299107Z   -- Detecting CXX compile features - done\r\n2020-07-03T08:21:31.3309768Z   -- CMake version 3.17.3\r\n2020-07-03T08:21:31.3310419Z   -- Release\r\n2020-07-03T08:21:31.3544007Z   -- Found PythonInterp: /opt/hostedtoolcache/Python/3.8.3/x64/bin/python (found version \"3.8.3\")\r\n2020-07-03T08:21:31.3915784Z   -- Found PythonLibs: /opt/hostedtoolcache/Python/3.8.3/x64/lib/libpython3.8.so\r\n2020-07-03T08:21:31.3997094Z   -- pybind11 v2.4.3\r\n2020-07-03T08:21:31.4071238Z   -- Performing Test HAS_FLTO\r\n2020-07-03T08:21:33.4117854Z   -- Performing Test HAS_FLTO - Success\r\n2020-07-03T08:21:33.4118841Z   -- LTO enabled\r\n2020-07-03T08:21:33.4136840Z   -- Configuring done\r\n2020-07-03T08:21:33.4460654Z   -- Generating done\r\n2020-07-03T08:21:33.4475968Z   -- Build files have been written to: /tmp/pip-req-build-5pd9zbqc/build/temp.linux-x86_64-3.8\r\n2020-07-03T08:21:33.5158355Z   Scanning dependencies of target awkward-cpu-kernels-objects\r\n2020-07-03T08:21:33.5194153Z   [  1%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/getitem.cpp.o\r\n2020-07-03T08:21:33.5588086Z   Scanning dependencies of target awkward-objects\r\n2020-07-03T08:21:33.5684033Z   [  2%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Content.cpp.o\r\n2020-07-03T08:21:35.1728746Z   [  4%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/identities.cpp.o\r\n2020-07-03T08:21:36.1183319Z   [  5%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/operations.cpp.o\r\n2020-07-03T08:21:38.1010411Z   [  7%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/reducers.cpp.o\r\n2020-07-03T08:21:39.4957864Z   [  8%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Identities.cpp.o\r\n2020-07-03T08:21:39.5703725Z   [  9%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/sorting.cpp.o\r\n2020-07-03T08:21:39.9355493Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/Identities.cpp:334:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:39.9357268Z      template class EXPORT_SYMBOL IdentitiesOf<int64_t>;\r\n2020-07-03T08:21:39.9357994Z                                   ^\r\n2020-07-03T08:21:40.9654520Z   [ 11%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Index.cpp.o\r\n2020-07-03T08:21:41.3524168Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/Index.cpp:310:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:41.3526811Z      template class EXPORT_SYMBOL IndexOf<int8_t>;\r\n2020-07-03T08:21:41.3527716Z                                   ^\r\n2020-07-03T08:21:41.3538329Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/Index.cpp:311:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:41.3540096Z      template class EXPORT_SYMBOL IndexOf<uint8_t>;\r\n2020-07-03T08:21:41.3540874Z                                   ^\r\n2020-07-03T08:21:41.3549097Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/Index.cpp:312:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:41.3550069Z      template class EXPORT_SYMBOL IndexOf<int32_t>;\r\n2020-07-03T08:21:41.3550429Z                                   ^\r\n2020-07-03T08:21:41.3558440Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/Index.cpp:313:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:41.3559305Z      template class EXPORT_SYMBOL IndexOf<uint32_t>;\r\n2020-07-03T08:21:41.3559910Z                                   ^\r\n2020-07-03T08:21:41.3566180Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/Index.cpp:314:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:41.3567098Z      template class EXPORT_SYMBOL IndexOf<int64_t>;\r\n2020-07-03T08:21:41.3567567Z                                   ^\r\n2020-07-03T08:21:42.5986890Z   [ 12%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Iterator.cpp.o\r\n2020-07-03T08:21:43.1330863Z   [ 14%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Reducer.cpp.o\r\n2020-07-03T08:21:45.5040215Z   [ 15%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Slice.cpp.o\r\n2020-07-03T08:21:48.1318014Z   [ 16%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/BitMaskedArray.cpp.o\r\n2020-07-03T08:21:50.0815141Z   [ 18%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/ByteMaskedArray.cpp.o\r\n2020-07-03T08:21:52.1530494Z   [ 18%] Built target awkward-cpu-kernels-objects\r\n2020-07-03T08:21:52.1557889Z   [ 19%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/EmptyArray.cpp.o\r\n2020-07-03T08:21:52.3740618Z   [ 21%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/IndexedArray.cpp.o\r\n2020-07-03T08:21:52.8726228Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/IndexedArray.cpp:2343:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:52.8727771Z      template class EXPORT_SYMBOL IndexedArrayOf<int64_t, true>;\r\n2020-07-03T08:21:52.8728501Z                                   ^\r\n2020-07-03T08:21:52.8733472Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/IndexedArray.cpp:2345:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:52.8735042Z      template class EXPORT_SYMBOL IndexedArrayOf<int32_t, false>;\r\n2020-07-03T08:21:52.8735671Z                                   ^\r\n2020-07-03T08:21:52.8741512Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/IndexedArray.cpp:2346:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:52.8742725Z      template class EXPORT_SYMBOL IndexedArrayOf<uint32_t, false>;\r\n2020-07-03T08:21:52.8743480Z                                   ^\r\n2020-07-03T08:21:52.8752177Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/IndexedArray.cpp:2347:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:52.8753361Z      template class EXPORT_SYMBOL IndexedArrayOf<int64_t, false>;\r\n2020-07-03T08:21:52.8754052Z                                   ^\r\n2020-07-03T08:21:52.8762495Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/IndexedArray.cpp:2348:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:52.8763626Z      template class EXPORT_SYMBOL IndexedArrayOf<int32_t, true>;\r\n2020-07-03T08:21:52.8764337Z                                   ^\r\n2020-07-03T08:21:53.7145523Z   Scanning dependencies of target awkward-cpu-kernels-static\r\n2020-07-03T08:21:53.7215045Z   [ 22%] Linking CXX static library libawkward-cpu-kernels-static.a\r\n2020-07-03T08:21:53.7430320Z   [ 22%] Built target awkward-cpu-kernels-static\r\n2020-07-03T08:21:53.7476553Z   Scanning dependencies of target awkward-cpu-kernels\r\n2020-07-03T08:21:53.7519323Z   [ 23%] Linking CXX shared library libawkward-cpu-kernels.so\r\n2020-07-03T08:21:53.8496698Z   [ 23%] Built target awkward-cpu-kernels\r\n2020-07-03T08:21:53.8521192Z   [ 25%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/ListArray.cpp.o\r\n2020-07-03T08:21:54.3415440Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/ListArray.cpp:1838:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:54.3417082Z      template class EXPORT_SYMBOL ListArrayOf<int32_t>;\r\n2020-07-03T08:21:54.3417823Z                                   ^\r\n2020-07-03T08:21:54.3421383Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/ListArray.cpp:1839:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:54.3422609Z      template class EXPORT_SYMBOL ListArrayOf<uint32_t>;\r\n2020-07-03T08:21:54.3423288Z                                   ^\r\n2020-07-03T08:21:54.3427824Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/ListArray.cpp:1840:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:21:54.3428895Z      template class EXPORT_SYMBOL ListArrayOf<int64_t>;\r\n2020-07-03T08:21:54.3430126Z                                   ^\r\n2020-07-03T08:22:00.2964004Z   [ 26%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/ListOffsetArray.cpp.o\r\n2020-07-03T08:22:01.1959129Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/ListOffsetArray.cpp:2238:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:01.1960776Z      template class EXPORT_SYMBOL ListOffsetArrayOf<int32_t>;\r\n2020-07-03T08:22:01.1961317Z                                   ^\r\n2020-07-03T08:22:01.1970443Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/ListOffsetArray.cpp:2239:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:01.1971111Z      template class EXPORT_SYMBOL ListOffsetArrayOf<uint32_t>;\r\n2020-07-03T08:22:01.1971460Z                                   ^\r\n2020-07-03T08:22:01.1979734Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/ListOffsetArray.cpp:2240:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:01.1980605Z      template class EXPORT_SYMBOL ListOffsetArrayOf<int64_t>;\r\n2020-07-03T08:22:01.1981392Z                                   ^\r\n2020-07-03T08:22:02.0206316Z   [ 28%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/None.cpp.o\r\n2020-07-03T08:22:02.7726291Z   [ 29%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/NumpyArray.cpp.o\r\n2020-07-03T08:22:07.3269262Z   [ 30%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/Record.cpp.o\r\n2020-07-03T08:22:08.8669878Z   [ 32%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/RecordArray.cpp.o\r\n2020-07-03T08:22:10.5547958Z   [ 33%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/RegularArray.cpp.o\r\n2020-07-03T08:22:12.7854226Z   [ 35%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/UnionArray.cpp.o\r\n2020-07-03T08:22:13.0500683Z   [ 36%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/UnmaskedArray.cpp.o\r\n2020-07-03T08:22:13.2559503Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/UnionArray.cpp:1951:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:13.2561126Z      template class EXPORT_SYMBOL UnionArrayOf<int8_t, int32_t>;\r\n2020-07-03T08:22:13.2561790Z                                   ^\r\n2020-07-03T08:22:13.2566778Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/UnionArray.cpp:1952:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:13.2567996Z      template class EXPORT_SYMBOL UnionArrayOf<int8_t, uint32_t>;\r\n2020-07-03T08:22:13.2568588Z                                   ^\r\n2020-07-03T08:22:13.2574003Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/array/UnionArray.cpp:1953:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:13.2574947Z      template class EXPORT_SYMBOL UnionArrayOf<int8_t, int64_t>;\r\n2020-07-03T08:22:13.2575482Z                                   ^\r\n2020-07-03T08:22:15.1107321Z   [ 38%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/VirtualArray.cpp.o\r\n2020-07-03T08:22:17.0628329Z   [ 39%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/ArrayBuilder.cpp.o\r\n2020-07-03T08:22:18.0753468Z   [ 40%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/ArrayBuilderOptions.cpp.o\r\n2020-07-03T08:22:18.3200918Z   [ 42%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/BoolBuilder.cpp.o\r\n2020-07-03T08:22:19.0651391Z   [ 43%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/Builder.cpp.o\r\n2020-07-03T08:22:19.4772321Z   [ 45%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/Float64Builder.cpp.o\r\n2020-07-03T08:22:19.7852527Z   [ 46%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/GrowableBuffer.cpp.o\r\n2020-07-03T08:22:20.2426660Z   [ 47%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/IndexedBuilder.cpp.o\r\n2020-07-03T08:22:20.6340355Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/builder/IndexedBuilder.cpp:151:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:20.6341886Z      template class EXPORT_SYMBOL IndexedBuilder<Content>;\r\n2020-07-03T08:22:20.6342603Z                                   ^\r\n2020-07-03T08:22:20.6639559Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/builder/IndexedBuilder.cpp:257:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:20.6640786Z      template class EXPORT_SYMBOL IndexedBuilder<IndexedArray32>;\r\n2020-07-03T08:22:20.6641421Z                                   ^\r\n2020-07-03T08:22:20.6658377Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/builder/IndexedBuilder.cpp:305:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:20.6664875Z      template class EXPORT_SYMBOL IndexedBuilder<IndexedArrayU32>;\r\n2020-07-03T08:22:20.6665578Z                                   ^\r\n2020-07-03T08:22:20.6683028Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/builder/IndexedBuilder.cpp:353:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:20.6684556Z      template class EXPORT_SYMBOL IndexedBuilder<IndexedArray64>;\r\n2020-07-03T08:22:20.6685198Z                                   ^\r\n2020-07-03T08:22:20.6703463Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/builder/IndexedBuilder.cpp:401:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:20.6704540Z      template class EXPORT_SYMBOL IndexedBuilder<IndexedOptionArray32>;\r\n2020-07-03T08:22:20.6705217Z                                   ^\r\n2020-07-03T08:22:20.6721330Z   /tmp/pip-req-build-5pd9zbqc/src/libawkward/builder/IndexedBuilder.cpp:443:32: warning: type attributes ignored after type is already defined [-Wattributes]\r\n2020-07-03T08:22:20.6722458Z      template class EXPORT_SYMBOL IndexedBuilder<IndexedOptionArray64>;\r\n2020-07-03T08:22:20.6723142Z                                   ^\r\n2020-07-03T08:22:20.7522929Z   [ 49%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/Int64Builder.cpp.o\r\n2020-07-03T08:22:21.4767574Z   [ 50%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/ListBuilder.cpp.o\r\n2020-07-03T08:22:22.1767590Z   [ 52%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/OptionBuilder.cpp.o\r\n2020-07-03T08:22:22.3336520Z   [ 53%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/RecordBuilder.cpp.o\r\n2020-07-03T08:22:23.0794748Z   [ 54%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/StringBuilder.cpp.o\r\n2020-07-03T08:22:23.9560825Z   [ 56%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/TupleBuilder.cpp.o\r\n2020-07-03T08:22:24.1407071Z   [ 57%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/UnionBuilder.cpp.o\r\n2020-07-03T08:22:25.1869663Z   [ 59%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/UnknownBuilder.cpp.o\r\n2020-07-03T08:22:25.6792238Z   [ 60%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/io/json.cpp.o\r\n2020-07-03T08:22:26.0103487Z   [ 61%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/io/root.cpp.o\r\n2020-07-03T08:22:26.8226578Z   [ 63%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/kernel.cpp.o\r\n2020-07-03T08:22:28.1675153Z   [ 64%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/partition/IrregularlyPartitionedArray.cpp.o\r\n2020-07-03T08:22:28.1999245Z   [ 66%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/partition/PartitionedArray.cpp.o\r\n2020-07-03T08:22:29.0644783Z   [ 67%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/ArrayType.cpp.o\r\n2020-07-03T08:22:29.1566643Z   [ 69%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/ListType.cpp.o\r\n2020-07-03T08:22:29.6688337Z   [ 70%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/OptionType.cpp.o\r\n2020-07-03T08:22:29.8892538Z   [ 71%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/PrimitiveType.cpp.o\r\n2020-07-03T08:22:30.4484092Z   [ 73%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/RecordType.cpp.o\r\n2020-07-03T08:22:30.6788134Z   [ 74%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/RegularType.cpp.o\r\n2020-07-03T08:22:31.3991430Z   [ 76%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/Type.cpp.o\r\n2020-07-03T08:22:32.1843294Z   [ 77%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/UnionType.cpp.o\r\n2020-07-03T08:22:32.2190904Z   [ 78%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/UnknownType.cpp.o\r\n2020-07-03T08:22:32.8619623Z   [ 80%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/util.cpp.o\r\n2020-07-03T08:22:33.0534420Z   [ 81%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/virtual/ArrayCache.cpp.o\r\n2020-07-03T08:22:33.5305867Z   [ 83%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/virtual/ArrayGenerator.cpp.o\r\n2020-07-03T08:22:35.2345315Z   [ 83%] Built target awkward-objects\r\n2020-07-03T08:22:35.2391601Z   Scanning dependencies of target awkward\r\n2020-07-03T08:22:35.2400573Z   Scanning dependencies of target awkward-static\r\n2020-07-03T08:22:35.2443795Z   [ 84%] Linking CXX shared library libawkward.so\r\n2020-07-03T08:22:35.2450753Z   [ 85%] Linking CXX static library libawkward-static.a\r\n2020-07-03T08:22:35.4142619Z   [ 85%] Built target awkward-static\r\n2020-07-03T08:22:36.2452515Z   [ 85%] Built target awkward\r\n2020-07-03T08:22:36.2453210Z   Scanning dependencies of target _ext\r\n2020-07-03T08:22:36.2453728Z   [ 87%] Building CXX object CMakeFiles/_ext.dir/src/python/_ext.cpp.o\r\n2020-07-03T08:22:36.2454246Z   [ 88%] Building CXX object CMakeFiles/_ext.dir/src/python/content.cpp.o\r\n2020-07-03T08:22:39.6684871Z   [ 90%] Building CXX object CMakeFiles/_ext.dir/src/python/forms.cpp.o\r\n2020-07-03T08:22:50.1727161Z   [ 91%] Building CXX object CMakeFiles/_ext.dir/src/python/identities.cpp.o\r\n2020-07-03T08:22:54.9663911Z   [ 92%] Building CXX object CMakeFiles/_ext.dir/src/python/index.cpp.o\r\n2020-07-03T08:22:59.9279459Z   [ 94%] Building CXX object CMakeFiles/_ext.dir/src/python/io.cpp.o\r\n2020-07-03T08:23:02.5410636Z   [ 95%] Building CXX object CMakeFiles/_ext.dir/src/python/partition.cpp.o\r\n2020-07-03T08:23:06.7429349Z   [ 97%] Building CXX object CMakeFiles/_ext.dir/src/python/types.cpp.o\r\n2020-07-03T08:23:16.2611171Z   [ 98%] Building CXX object CMakeFiles/_ext.dir/src/python/virtual.cpp.o\r\n2020-07-03T08:23:26.1310310Z   [100%] Linking CXX shared module _ext.cpython-38-x86_64-linux-gnu.so\r\n2020-07-03T08:24:28.2186095Z   [100%] Built target _ext\r\n2020-07-03T08:24:28.2998750Z   [  7%] Built target awkward-cpu-kernels-objects\r\n2020-07-03T08:24:28.3066723Z   [  8%] Built target awkward-cpu-kernels-static\r\n2020-07-03T08:24:28.3134632Z   [  9%] Built target awkward-cpu-kernels\r\n2020-07-03T08:24:28.3340981Z   [ 83%] Built target awkward-objects\r\n2020-07-03T08:24:28.3417603Z   [ 84%] Built target awkward-static\r\n2020-07-03T08:24:28.3590699Z   [ 98%] Built target _ext\r\n2020-07-03T08:24:28.3666552Z   [100%] Built target awkward\r\n```",
  "created_at":"2020-07-03T11:24:16Z",
  "id":653499455,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MzQ5OTQ1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-03T11:24:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski Yes, there is a third change coming to fix those.",
  "created_at":"2020-07-03T17:47:53Z",
  "id":653634752,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MzYzNDc1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-03T17:47:53Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi, @veprbl! I'm checking in on your PR because it has been a long time. Is this something you intend to finish?  If not, then I'll close it.\r\n\r\nMeanwhile, I'm going to have to do something about the warnings when building on Linux. Doing so may affect the downstream linking on MacOS that you got working. Do you have a minimal example that I'd be able to include as a test, so that when I fix the compile-time warnings on Linux I don't break downstream linking on MacOS? Would the [dependent project](https://github.com/scikit-hep/awkward-1.0/tree/master/dependent-project), as it is currently defined, be an appropriate test? If so, I'll include this or whatever test you give me into CI so that I don't break things for you. Since I don't have a Mac, CI is the only way I have to know that I'm not breaking your use-case.\r\n\r\nThanks!",
  "created_at":"2020-08-17T18:08:21Z",
  "id":675031038,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTAzMTAzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T18:08:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski There was a delay, but I'm back at it now. This was already ready, all that was missing was some extra explanations as for what I'm doing and why. I will rebase and update this now.",
  "created_at":"2020-08-19T02:45:46Z",
  "id":675819128,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTgxOTEyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T02:45:57Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski There was a delay, but I'm back at it now. This was already ready, all that was missing was some extra explanations as for what I'm doing and why. I will rebase and update this now.\r\n\r\nIn that case, now is a better time than an hour ago\u2014I just put in a big change. I think that nothing has touched the linking process, and maybe even CMakeLists.txt is unchanged since you were last working on this, but there have been some substantial rearrangements of the code. (Moved header files and several search-and-replacements, but I don't think the lines you changed\u2014the first line of each class declaration\u2014have changed.)",
  "created_at":"2020-08-19T04:13:10Z",
  "id":675841133,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTg0MTEzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T04:13:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski I see. I will look into the next rebase tomorrow.",
  "created_at":"2020-08-19T04:57:54Z",
  "id":675851534,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTg1MTUzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T04:57:54Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"No warnings in Linux; that's great!\r\n\r\n```\r\n2020-08-19T19:14:45.1054802Z   -- The CXX compiler identification is GNU 5.5.0\r\n2020-08-19T19:14:45.1187165Z   -- Detecting CXX compiler ABI info\r\n2020-08-19T19:14:45.1919706Z   -- Detecting CXX compiler ABI info - done\r\n2020-08-19T19:14:45.2074959Z   -- Check for working CXX compiler: /usr/bin/g++ - skipped\r\n2020-08-19T19:14:45.2081196Z   -- Detecting CXX compile features\r\n2020-08-19T19:14:45.2088946Z   -- Detecting CXX compile features - done\r\n2020-08-19T19:14:45.2098324Z   -- CMake version 3.18.0\r\n2020-08-19T19:14:45.2099660Z   -- Release\r\n2020-08-19T19:14:45.2396353Z   -- Found PythonInterp: /opt/hostedtoolcache/Python/3.8.5/x64/bin/python (found version \"3.8.5\")\r\n2020-08-19T19:14:45.2791750Z   -- Found PythonLibs: /opt/hostedtoolcache/Python/3.8.5/x64/lib/libpython3.8.so\r\n2020-08-19T19:14:45.2880831Z   -- pybind11 v2.4.3\r\n2020-08-19T19:14:45.2956758Z   -- Performing Test HAS_FLTO\r\n2020-08-19T19:14:46.8389906Z   -- Performing Test HAS_FLTO - Success\r\n2020-08-19T19:14:46.8391963Z   -- LTO enabled\r\n2020-08-19T19:14:46.8410807Z   -- Configuring done\r\n2020-08-19T19:14:46.8711309Z   -- Generating done\r\n2020-08-19T19:14:46.8723205Z   -- Build files have been written to: /tmp/pip-req-build-aoz21nbp/build/temp.linux-x86_64-3.8\r\n2020-08-19T19:14:46.9380571Z   Scanning dependencies of target awkward-cpu-kernels-objects\r\n2020-08-19T19:14:46.9420221Z   [  1%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/allocators.cpp.o\r\n2020-08-19T19:14:46.9728551Z   Scanning dependencies of target awkward-objects\r\n2020-08-19T19:14:46.9815510Z   [  2%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Content.cpp.o\r\n2020-08-19T19:14:47.6928207Z   [  4%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/getitem.cpp.o\r\n2020-08-19T19:14:49.5412868Z   [  5%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/identities.cpp.o\r\n2020-08-19T19:14:50.8293456Z   [  6%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/operations.cpp.o\r\n2020-08-19T19:14:53.4226719Z   [  8%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Identities.cpp.o\r\n2020-08-19T19:14:54.4722199Z   [  9%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/reducers.cpp.o\r\n2020-08-19T19:14:55.3011364Z   [ 10%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Index.cpp.o\r\n2020-08-19T19:14:56.2080454Z   [ 12%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/sorting.cpp.o\r\n2020-08-19T19:14:57.6801792Z   [ 13%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Iterator.cpp.o\r\n2020-08-19T19:14:58.5246321Z   [ 14%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Reducer.cpp.o\r\n2020-08-19T19:15:00.9736606Z   [ 16%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/Slice.cpp.o\r\n2020-08-19T19:15:03.5345843Z   [ 17%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/BitMaskedArray.cpp.o\r\n2020-08-19T19:15:05.7606761Z   [ 18%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/ByteMaskedArray.cpp.o\r\n2020-08-19T19:15:08.3710223Z   [ 18%] Built target awkward-cpu-kernels-objects\r\n2020-08-19T19:15:08.3712645Z   [ 20%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/EmptyArray.cpp.o\r\n2020-08-19T19:15:08.3754733Z   Scanning dependencies of target awkward-cpu-kernels-static\r\n2020-08-19T19:15:08.3805892Z   [ 21%] Linking CXX static library libawkward-cpu-kernels-static.a\r\n2020-08-19T19:15:08.4235451Z   [ 21%] Built target awkward-cpu-kernels-static\r\n2020-08-19T19:15:08.4278036Z   Scanning dependencies of target awkward-cpu-kernels\r\n2020-08-19T19:15:08.4315904Z   [ 22%] Linking CXX shared library libawkward-cpu-kernels.so\r\n2020-08-19T19:15:08.5192336Z   [ 22%] Built target awkward-cpu-kernels\r\n2020-08-19T19:15:08.5233595Z   [ 24%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/IndexedArray.cpp.o\r\n2020-08-19T19:15:10.3912498Z   [ 25%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/ListArray.cpp.o\r\n2020-08-19T19:15:16.0986720Z   [ 27%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/ListOffsetArray.cpp.o\r\n2020-08-19T19:15:17.1552240Z   [ 28%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/None.cpp.o\r\n2020-08-19T19:15:18.8691461Z   [ 29%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/NumpyArray.cpp.o\r\n2020-08-19T19:15:22.1449772Z   [ 31%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/Record.cpp.o\r\n2020-08-19T19:15:24.2402100Z   [ 32%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/RecordArray.cpp.o\r\n2020-08-19T19:15:27.9121497Z   [ 33%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/RegularArray.cpp.o\r\n2020-08-19T19:15:28.6143662Z   [ 35%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/UnionArray.cpp.o\r\n2020-08-19T19:15:30.5188530Z   [ 36%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/UnmaskedArray.cpp.o\r\n2020-08-19T19:15:32.7344610Z   [ 37%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/array/VirtualArray.cpp.o\r\n2020-08-19T19:15:35.0320944Z   [ 39%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/ArrayBuilder.cpp.o\r\n2020-08-19T19:15:35.3571102Z   [ 40%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/ArrayBuilderOptions.cpp.o\r\n2020-08-19T19:15:35.9810443Z   [ 41%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/BoolBuilder.cpp.o\r\n2020-08-19T19:15:36.3162094Z   [ 43%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/Builder.cpp.o\r\n2020-08-19T19:15:37.0200205Z   [ 44%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/Float64Builder.cpp.o\r\n2020-08-19T19:15:37.0541115Z   [ 45%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/GrowableBuffer.cpp.o\r\n2020-08-19T19:15:38.1035318Z   [ 47%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/IndexedBuilder.cpp.o\r\n2020-08-19T19:15:38.2570744Z   [ 48%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/Int64Builder.cpp.o\r\n2020-08-19T19:15:39.3562197Z   [ 50%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/ListBuilder.cpp.o\r\n2020-08-19T19:15:40.2749146Z   [ 51%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/OptionBuilder.cpp.o\r\n2020-08-19T19:15:40.5009011Z   [ 52%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/RecordBuilder.cpp.o\r\n2020-08-19T19:15:41.4692837Z   [ 54%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/StringBuilder.cpp.o\r\n2020-08-19T19:15:42.4348423Z   [ 55%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/TupleBuilder.cpp.o\r\n2020-08-19T19:15:42.8285905Z   [ 56%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/UnionBuilder.cpp.o\r\n2020-08-19T19:15:43.9593032Z   [ 58%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/builder/UnknownBuilder.cpp.o\r\n2020-08-19T19:15:44.5793545Z   [ 59%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/io/json.cpp.o\r\n2020-08-19T19:15:45.1346585Z   [ 60%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/io/root.cpp.o\r\n2020-08-19T19:15:46.2517426Z   [ 62%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/kernel-dispatch.cpp.o\r\n2020-08-19T19:15:47.1853462Z   [ 63%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/partition/IrregularlyPartitionedArray.cpp.o\r\n2020-08-19T19:15:48.3569100Z   [ 64%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/partition/PartitionedArray.cpp.o\r\n2020-08-19T19:15:49.5885255Z   [ 66%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/ArrayType.cpp.o\r\n2020-08-19T19:15:50.4555943Z   [ 67%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/ListType.cpp.o\r\n2020-08-19T19:15:51.4554676Z   [ 68%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/OptionType.cpp.o\r\n2020-08-19T19:15:52.4846162Z   [ 70%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/PrimitiveType.cpp.o\r\n2020-08-19T19:15:53.5493829Z   [ 71%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/RecordType.cpp.o\r\n2020-08-19T19:15:55.1024851Z   [ 72%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/RegularType.cpp.o\r\n2020-08-19T19:15:56.0873635Z   [ 74%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/Type.cpp.o\r\n2020-08-19T19:15:56.8950184Z   [ 75%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/UnionType.cpp.o\r\n2020-08-19T19:15:57.1878032Z   [ 77%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/type/UnknownType.cpp.o\r\n2020-08-19T19:15:58.1085863Z   [ 78%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/util.cpp.o\r\n2020-08-19T19:15:58.1553223Z   [ 79%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/virtual/ArrayCache.cpp.o\r\n2020-08-19T19:15:58.9023077Z   [ 81%] Building CXX object CMakeFiles/awkward-objects.dir/src/libawkward/virtual/ArrayGenerator.cpp.o\r\n2020-08-19T19:16:00.4990977Z   [ 81%] Built target awkward-objects\r\n2020-08-19T19:16:00.5037571Z   Scanning dependencies of target awkward-static\r\n2020-08-19T19:16:00.5041292Z   Scanning dependencies of target awkward\r\n2020-08-19T19:16:00.5076761Z   [ 82%] Linking CXX shared library libawkward.so\r\n2020-08-19T19:16:00.5084547Z   [ 83%] Linking CXX static library libawkward-static.a\r\n2020-08-19T19:16:00.6843908Z   [ 83%] Built target awkward-static\r\n2020-08-19T19:16:00.8603036Z   [ 83%] Built target awkward\r\n2020-08-19T19:16:01.1198388Z   Scanning dependencies of target _ext\r\n2020-08-19T19:16:01.1300583Z   [ 85%] Building CXX object CMakeFiles/_ext.dir/src/python/content.cpp.o\r\n2020-08-19T19:16:01.1302148Z   [ 86%] Building CXX object CMakeFiles/_ext.dir/src/python/_ext.cpp.o\r\n2020-08-19T19:16:04.6530258Z   [ 87%] Building CXX object CMakeFiles/_ext.dir/src/python/forms.cpp.o\r\n2020-08-19T19:16:14.6254245Z   [ 89%] Building CXX object CMakeFiles/_ext.dir/src/python/identities.cpp.o\r\n2020-08-19T19:16:18.9241076Z   [ 90%] Building CXX object CMakeFiles/_ext.dir/src/python/index.cpp.o\r\n2020-08-19T19:16:23.4868145Z   [ 91%] Building CXX object CMakeFiles/_ext.dir/src/python/io.cpp.o\r\n2020-08-19T19:16:26.1226559Z   [ 93%] Building CXX object CMakeFiles/_ext.dir/src/python/kernel_utils.cpp.o\r\n2020-08-19T19:16:29.1618402Z   [ 94%] Building CXX object CMakeFiles/_ext.dir/src/python/partition.cpp.o\r\n2020-08-19T19:16:32.9059236Z   [ 95%] Building CXX object CMakeFiles/_ext.dir/src/python/startup.cpp.o\r\n2020-08-19T19:16:35.2361023Z   [ 97%] Building CXX object CMakeFiles/_ext.dir/src/python/types.cpp.o\r\n2020-08-19T19:16:42.7497944Z   [ 98%] Building CXX object CMakeFiles/_ext.dir/src/python/virtual.cpp.o\r\n2020-08-19T19:16:47.4964151Z   [100%] Linking CXX shared module _ext.cpython-38-x86_64-linux-gnu.so\r\n2020-08-19T19:17:49.4909551Z   [100%] Built target _ext\r\n2020-08-19T19:17:49.6406137Z   [  8%] Built target awkward-cpu-kernels-objects\r\n2020-08-19T19:17:49.6475348Z   [  9%] Built target awkward-cpu-kernels-static\r\n2020-08-19T19:17:49.6555194Z   [ 10%] Built target awkward-cpu-kernels\r\n2020-08-19T19:17:49.6751747Z   [ 81%] Built target awkward-objects\r\n2020-08-19T19:17:49.6825492Z   [ 82%] Built target awkward-static\r\n2020-08-19T19:17:49.7012804Z   [ 98%] Built target _ext\r\n2020-08-19T19:17:49.7086201Z   [100%] Built target awkward\r\n2020-08-19T19:17:49.7143186Z   Install the project...\r\n2020-08-19T19:17:49.7163861Z   -- Install configuration: \"Release\"\r\n2020-08-19T19:17:49.7167640Z   -- Installing: /tmp/pip-req-build-aoz21nbp/build/lib.linux-x86_64-3.8/awkward1/libawkward-static.a\r\n2020-08-19T19:17:49.7287833Z   -- Installing: /tmp/pip-req-build-aoz21nbp/build/lib.linux-x86_64-3.8/awkward1/libawkward.so\r\n2020-08-19T19:17:49.7352491Z   -- Installing: /tmp/pip-req-build-aoz21nbp/build/lib.linux-x86_64-3.8/awkward1/libawkward-cpu-kernels.so\r\n2020-08-19T19:17:49.7363241Z   -- Installing: /tmp/pip-req-build-aoz21nbp/build/lib.linux-x86_64-3.8/awkward1/libawkward-cpu-kernels-static.a\r\n2020-08-19T19:17:49.7380442Z   -- Installing: /tmp/pip-req-build-aoz21nbp/build/lib.linux-x86_64-3.8/awkward1/_ext.cpython-38-x86_64-linux-gnu.so\r\n2020-08-19T19:17:49.7538727Z   installing to build/bdist.linux-x86_64/wheel\r\n```",
  "created_at":"2020-08-19T19:26:09Z",
  "id":676615829,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjYxNTgyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T19:26:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"MacOS/Cling has found some implicit integer sign changes, but that's my problem. I'll deal with it later.\r\n\r\nEdit: the above was fixed in #407, specifically 941aafc3f611e0f5061bdee2f1b6d191ef7a252d .\r\n\r\nWindows/Visual Studio has no warnings (32-bit and 64-bit).",
  "created_at":"2020-08-19T19:29:46Z",
  "id":676617502,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjYxNzUwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T14:00:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This PR is mostly aimed at silencing warnings in a reasonable way. The only visible change should be that the dependent libraries should not re-export any libawkward symbols, before it was pointless to set the default visibility to hidden.",
  "created_at":"2020-08-19T22:59:47Z",
  "id":676797508,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3Njc5NzUwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T22:59:47Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, thanks for all of your help with this. The differences look good to me.\r\n\r\nI do need to get the dependent project into the unit tests; something I'll tackle soon. The thing that was preventing it was not substantial\u2014it was something like setting the PATH on Azure (I couldn't find where it had installed into).",
  "created_at":"2020-08-19T23:55:52Z",
  "id":676818986,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjgxODk4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T23:55:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski Sorry, I don't quite follow you on the dependent project thread. I haven't looked at this yet.\r\n\r\nOn another note, I must admit I still haven't tried applying awkward1 for my analysis, so I had some questions for you. Is it somewhat ready for use? Is parquet file reading supported?",
  "created_at":"2020-08-20T00:09:55Z",
  "id":676822720,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjgyMjcyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-20T00:09:55Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"https://github.com/scikit-hep/awkward-1.0/tree/master/dependent-project\r\n\r\nis an example of a C++ project that links to (depends on) Awkward. It used to be part of CI, then I had to drop it when something broke and I didn't know exactly what, now I'd like to reinstate it, since I think you've fixed linking issues, at least on MacOS.\r\n\r\nOther thread: it is ready to use, and some analysis groups have been using it. Generally speaking, it's more complete and self-consistent than Awkward 0 was, and is better documented, too. Most of my work recently has been responding to bug reports and feature requests of users applying it to data analyses (not all of which are in HEP).\r\n\r\nParquet reading is supported. There are issues like not supporting partitioned datasets (#368), but I'm working through them. In fact, the thing I'm working on right now (#403) is adding a categorical type because generic IndexedArrays (in which \"categories\" are not necessarily unique) is hitting an upstream bug in Arrow (which assumes that they are unique). Declaring categorical types explicitly will make it clear which things I should translate to and from Parquet dictionaries\u2014so this is second-order stuff. To first-order, it works.",
  "created_at":"2020-08-20T00:22:42Z",
  "id":676826074,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjgyNjA3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-20T00:22:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski This sounds very encouraging. Thank you!",
  "created_at":"2020-08-20T00:24:50Z",
  "id":676826588,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjgyNjU4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-20T00:24:50Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"See the new test here:\r\n\r\nhttps://dev.azure.com/jpivarski/Scikit-HEP/_build?definitionId=5\r\n\r\nThe [YAML for this test](https://github.com/scikit-hep/awkward-1.0/blob/master/.ci/azure-doctest-awkward.yml) doesn't actually run the documentation workflow; you'll have to add that. And you might want to remove the compilation, too (now that I think about it), because the documentation needs to be buildable without compilation (so that it works on ReadTheDocs). That will take the running time from 2 minutes down to almost nothing.",
  "created_at":"2020-07-05T15:22:19Z",
  "id":653901753,
  "issue":318,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MzkwMTc1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-05T15:22:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I am done with this PR :) ",
  "created_at":"2020-07-06T12:30:39Z",
  "id":654205148,
  "issue":319,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NDIwNTE0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-06T12:30:39Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - if this is it, it has happened on MacOS Python 3.8 as well. \r\n```python\r\n>       assert awkward1.to_list(array.argsort(0, True, False)) == [\r\n            [    3,    1,    2 ],\r\n            [    0,    0,    0 ],\r\n            [    1, None,    1 ],\r\n            [    2, None, None ],\r\n            [ None, None, None ]]\r\n\r\ntests/test_0074-argsort-and-sort.py:122: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/Users/runner/hostedtoolcache/Python/2.7.18/x64/lib/python2.7/site-packages/awkward1/operations/convert.py:403: in to_list\r\n    return [to_list(x) for x in array]\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\narray = <IndexedOptionArray64>\r\n    <index><Index64 i=\"[3 -1 9]\" offset=\"9\" length=\"3\" ...ata=\"3 0 1 2 1 0 2 0 1\" at=\"0x7fa0ecf3f2f0\"/></content>\r\n</IndexedOptionArray64>\r\n\r\n>           return [to_list(x) for x in array]\r\nE           ValueError: in IndexedOptionArray64 attempting to get 2, index[i] >= len(content)\r\n\r\n\r\n>       assert awkward1.to_list(array5.sort(0, True, False)) == [\r\n            [-4.4, -5.5, -6.6],\r\n            [2.2, 1.1, 3.3],\r\n            [4.4, 5.5, None],\r\n            [5.5, None, None],\r\n            [None, None, None]]\r\n\r\ntests/test_0074-argsort-and-sort.py:177: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nawkward1/operations/convert.py:403: in to_list\r\n    return [to_list(x) for x in array]\r\nawkward1/operations/convert.py:403: in <listcomp>\r\n    return [to_list(x) for x in array]\r\nawkward1/operations/convert.py:403: in to_list\r\n    return [to_list(x) for x in array]\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n.0 = <Iterator at=\"3\">\r\n    <IndexedOptionArray64>\r\n        <index><Index64 i=\"[2 6 9]\" offset=\"6\" length=\"3\" at=\"0x7f85ea425...\" data=\"-4.4 2.2 4.4 5.5 -5.5 1.1 5.5 -6.6 3.3\" at=\"0x7f85ea426710\"/></content>\r\n    </IndexedOptionArray64>\r\n</Iterator>\r\n\r\n>   return [to_list(x) for x in array]\r\nE   ValueError: in IndexedOptionArray64 attempting to get 2, index[i] >= len(content)\r\n\r\nawkward1/operations/convert.py:403: ValueError\r\n\r\n>       assert awkward1.to_list(array5.sort(0, True, False)) == [\r\n            [-4.4, -5.5, -6.6],\r\n            [2.2, 1.1, 3.3],\r\n            [4.4, 5.5, None],\r\n            [5.5, None, None],\r\n            [None, None, None]]\r\n\r\ntests/test_0074-argsort-and-sort.py:177: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nawkward1/operations/convert.py:403: in to_list\r\n    return [to_list(x) for x in array]\r\nawkward1/operations/convert.py:403: in <listcomp>\r\n    return [to_list(x) for x in array]\r\nawkward1/operations/convert.py:403: in to_list\r\n    return [to_list(x) for x in array]\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\n.0 = <Iterator at=\"3\">\r\n    <IndexedOptionArray64>\r\n        <index><Index64 i=\"[2 6 9]\" offset=\"6\" length=\"3\" at=\"0x7fe866e43...\" data=\"-4.4 2.2 4.4 5.5 -5.5 1.1 5.5 -6.6 3.3\" at=\"0x7fe866e43ae0\"/></content>\r\n    </IndexedOptionArray64>\r\n</Iterator>\r\n\r\n>   return [to_list(x) for x in array]\r\nE   ValueError: in IndexedOptionArray64 attempting to get 2, index[i] >= len(content)\r\n\r\nawkward1/operations/convert.py:403: ValueError\r\n\r\n\r\n        elif isinstance(array, awkward1.highlevel.Array):\r\n            return [to_list(x) for x in array]\r\n    \r\n        elif isinstance(array, awkward1.highlevel.Record):\r\n            return to_list(array.layout)\r\n    \r\n        elif isinstance(array, awkward1.highlevel.ArrayBuilder):\r\n            return to_list(array.snapshot())\r\n    \r\n        elif isinstance(array, awkward1.layout.Record) and array.istuple:\r\n            return tuple(to_list(x) for x in array.fields())\r\n    \r\n        elif isinstance(array, awkward1.layout.Record):\r\n            return {n: to_list(x) for n, x in array.fielditems()}\r\n    \r\n        elif isinstance(array, awkward1.layout.ArrayBuilder):\r\n            return [to_list(x) for x in array.snapshot()]\r\n    \r\n        elif isinstance(array, awkward1.layout.NumpyArray):\r\n            return numpy.asarray(array).tolist()\r\n    \r\n        elif isinstance(\r\n            array, (awkward1.layout.Content, awkward1.partition.PartitionedArray)\r\n        ):\r\n>           return [to_list(x) for x in array]\r\nE           ValueError: in IndexedOptionArray64 attempting to get 2, index[i] >= len(content)\r\n\r\n/Users/runner/hostedtoolcache/Python/2.7.18/x64/lib/python2.7/site-packages/awkward1/operations/convert.py:403: ValueError\r\n\r\n\r\n\r\n>       assert awkward1.to_list(array.argsort(0, True, False)) == [\r\n            [    3,    1,    2 ],\r\n            [    0,    0,    0 ],\r\n            [    1, None,    1 ],\r\n            [    2, None, None ],\r\n            [ None, None, None ]]\r\n\r\ntests/test_0074-argsort-and-sort.py:122: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/Users/runner/hostedtoolcache/Python/2.7.18/x64/lib/python2.7/site-packages/awkward1/operations/convert.py:403: in to_list\r\n    return [to_list(x) for x in array]\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\narray = <IndexedOptionArray64>\r\n    <index><Index64 i=\"[3 -1 9]\" offset=\"9\" length=\"3\" ...ata=\"3 0 1 2 1 0 2 0 1\" at=\"0x7fd285859290\"/></content>\r\n</IndexedOptionArray64>\r\n\r\n    def to_list(array):\r\n        \"\"\"\r\n        Converts `array` (many types supported, including all Awkward Arrays and\r\n        Records) into Python objects.\r\n    \r\n        Awkward Array types have the following Pythonic translations.\r\n    \r\n        \r\n            \r\n                elif isinstance(array, awkward1.behaviors.string.CharBehavior):\r\n                    return array.__str__()\r\n            \r\n                elif awkward1.operations.describe.parameters(array).get(\"__array__\") == \"byte\":\r\n                    return awkward1.behaviors.string.CharBehavior(array).__bytes__()\r\n            \r\n                elif awkward1.operations.describe.parameters(array).get(\"__array__\") == \"char\":\r\n                    return awkward1.behaviors.string.CharBehavior(array).__str__()\r\n            \r\n                elif isinstance(array, awkward1.highlevel.Array):\r\n                    return [to_list(x) for x in array]\r\n            \r\n                elif isinstance(array, awkward1.highlevel.Record):\r\n                    return to_list(array.layout)\r\n            \r\n                elif isinstance(array, awkward1.highlevel.ArrayBuilder):\r\n                    return to_list(array.snapshot())\r\n            \r\n                elif isinstance(array, awkward1.layout.Record) and array.istuple:\r\n                    return tuple(to_list(x) for x in array.fields())\r\n            \r\n                elif isinstance(array, awkward1.layout.Record):\r\n                    return {n: to_list(x) for n, x in array.fielditems()}\r\n            \r\n                elif isinstance(array, awkward1.layout.ArrayBuilder):\r\n                    return [to_list(x) for x in array.snapshot()]\r\n            \r\n                elif isinstance(array, awkward1.layout.NumpyArray):\r\n                    return numpy.asarray(array).tolist()\r\n            \r\n                elif isinstance(\r\n                    array, (awkward1.layout.Content, awkward1.partition.PartitionedArray)\r\n                ):\r\n        >           return [to_list(x) for x in array]\r\n        E           ValueError: in IndexedOptionArray64 attempting to get 2, index[i] >= len(content)\r\n\r\n        /Users/runner/hostedtoolcache/Python/2.7.18/x64/lib/python2.7/site-packages/awkward1/operations/convert.py:403: ValueError\r\n\r\n\r\n        >       assert awkward1.to_list(array.argsort(0, True, False)) == [\r\n                    [    3,    1,    2 ],\r\n                    [    0,    0,    0 ],\r\n                    [    1, None,    1 ],\r\n                    [    2, None, None ],\r\n                    [ None, None, None ]]\r\n\r\n        tests/test_0074-argsort-and-sort.py:122: \r\n        _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n        /Users/runner/hostedtoolcache/Python/2.7.18/x64/lib/python2.7/site-packages/awkward1/operations/convert.py:403: in to_list\r\n            return [to_list(x) for x in array]\r\n        _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n        array = <IndexedOptionArray64>\r\n            <index><Index64 i=\"[3 -1 9]\" offset=\"9\" length=\"3\" ...ata=\"3 0 1 2 1 0 2 0 1\" at=\"0x7fd285859290\"/></content>\r\n        </IndexedOptionArray64>\r\n\r\n            def to_list(array):\r\n\r\n```",
  "created_at":"2020-07-06T12:24:23Z",
  "id":654202190,
  "issue":320,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NDIwMjE5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-06T12:24:23Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"That's it\u2014I recognize this part of it:\r\n\r\n>       assert awkward1.to_list(array.argsort(0, True, False)) == [\r\n>            [    3,    1,    2 ],\r\n>            [    0,    0,    0 ],\r\n>            [    1, None,    1 ],\r\n>            [    2, None, None ],\r\n>            [ None, None, None ]]",
  "created_at":"2020-07-06T13:01:45Z",
  "id":654220646,
  "issue":320,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NDIyMDY0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-06T13:01:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It happened on MacOS Python 3.8 again: https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=2901&view=logs&jobId=beebc4e0-08c3-587a-a9f8-8370dd8c77d0&j=beebc4e0-08c3-587a-a9f8-8370dd8c77d0&t=4e13e18d-382c-5754-9d84-b74ae5d60514\r\n\r\nThis isn't good: it means that the sorting kernels are accessing uninitialized data or some other undefined behavior. It's probably working \"by accident\" on other platforms and for most runs of MacOS.\r\n\r\nWhen I re-ran the test, the original URL now points to the wrong place (a Windows build?), so it's no longer a useful link. The re-run passed, so it really is intermittent.",
  "created_at":"2020-07-31T20:21:33Z",
  "id":667340358,
  "issue":320,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NzM0MDM1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-31T20:21:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna Even though this argsort bug only happens intermittently in MacOS, it's a sign that the other cases (Linux and Windows all the time, MacOS most of the time) are relying on a happy accident of undefined behavior. Something is wrong in argsort that needs to be fixed. Until that happens, I'll need to remove high-level access to argsort to prevent data analysts from using it and possibly getting wrong answers in their analyses.\r\n\r\nDo you have reason to believe that it's confined to argsort or should I temporarily disable sort as well? Do you think you'll get a chance to look closely at it on a shorter timescale that it would take me to disable it (which is about a day or two)?",
  "created_at":"2020-09-03T21:50:48Z",
  "id":686782199,
  "issue":320,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Njc4MjE5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T21:50:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @ianna Even though this argsort bug only happens intermittently in MacOS, it's a sign that the other cases (Linux and Windows all the time, MacOS most of the time) are relying on a happy accident of undefined behavior. Something is wrong in argsort that needs to be fixed. Until that happens, I'll need to remove high-level access to argsort to prevent data analysts from using it and possibly getting wrong answers in their analyses.\r\n> \r\n> Do you have reason to believe that it's confined to argsort or should I temporarily disable sort as well? Do you think you'll get a chance to look closely at it on a shorter timescale that it would take me to disable it (which is about a day or two)?\r\n\r\n@jpivarski - I could not reproduce it :-( \r\nI'm going to reimplement kernel functions for both `sort` and `argsort`.",
  "created_at":"2020-09-05T06:53:34Z",
  "id":687562530,
  "issue":320,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NzU2MjUzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-05T06:53:34Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Not necessary.",
  "created_at":"2020-09-11T17:29:28Z",
  "id":691221537,
  "issue":321,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTIyMTUzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-11T17:29:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think the functionality is there now. I think the boilerplate for kernels is pretty out of hand, would be nice to find a way to reduce it. Remaining items:\r\n\r\n- How do I auto-format to 80 characters?\r\n- Where do I put some documentation of this behavior?\r\n- Why is the doctest failing",
  "created_at":"2020-07-08T22:14:38Z",
  "id":655785566,
  "issue":322,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NTc4NTU2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-08T22:14:38Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> * How do I auto-format to 80 characters?\r\n\r\nWe don't have a tool for that; I did it by hand. If it's roughly or mostly 80 characters, that is fine.\r\n\r\n> * Where do I put some documentation of this behavior?\r\n\r\n`__getitem__` behavior is documented here: https://github.com/scikit-hep/awkward-1.0/blob/cfa3a04d679529175e742fb5015047bd499ea1a6/src/awkward1/highlevel.py#L507-L843\r\n\r\n> * Why is the doctest failing\r\n\r\nMerge upstream master; it was fixed today (PR #323).",
  "created_at":"2020-07-08T22:26:01Z",
  "id":655789487,
  "issue":322,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NTc4OTQ4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-08T22:26:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to merge your branch with master, which should fix the doctest.\r\n\r\nAfter that, it's done, right? You're not planning any more changes?",
  "created_at":"2020-07-09T15:56:59Z",
  "id":656209748,
  "issue":322,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NjIwOTc0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-09T15:56:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We're going to ignore the doctest error; I'll ask Reik to look into that after all the merges are done.\r\n\r\nAssuming all the other tests pass, should I merge this? I'm dealing with the compiler warnings on another PR.",
  "created_at":"2020-07-09T16:47:25Z",
  "id":656235762,
  "issue":322,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NjIzNTc2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-09T16:47:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ok, now that its formatted and docs are added, I think its ready",
  "created_at":"2020-07-09T17:00:41Z",
  "id":656242077,
  "issue":322,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NjI0MjA3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-09T17:00:41Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Closes #315 ",
  "created_at":"2020-07-09T17:03:52Z",
  "id":656243612,
  "issue":322,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NjI0MzYxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-09T17:03:52Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Verified that this is still an issue, though the name `ak.pandas.df` \u2192 `ak.to_pandas`.",
  "created_at":"2020-10-30T22:09:59Z",
  "id":719822290,
  "issue":331,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgyMjI5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:09:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Numba function don't accept layouts. I don't know if that's something that should change. (Layouts also can't do NumPy ufuncs and various other things.)\r\n\r\nIt might seem like allowing layouts in Numba would be easy, since they are the \"lower level\" items, anyway\u2014it would seem like allowing `ak.Array` in Numba functions would be implemented in terms of layouts. Actually, it isn't. Entering a Numba-JITed function means turning an array into an `ArrayView`, which has a `Lookup` consisting of arrays that are pointers into the original arrays, posing as integers. It's these pointers to arrays that are traversed through Numba, not any facsimile of the C++ hierarchy, to avoid performance bottlenecks due to Numba's pass-by-value semantics.\r\n\r\n`ak.Array`'s \"wrapper oriented\" perspective is actually helpful in this; the \"composable nodes\" go against the grain. So it wouldn't be an easy thing to do to make layout objects accessible in Numba, and it's also not strongly motivated, since the `ak.Array` works.\r\n\r\nActually, the cheap solution to this might be to give all of the layout classes Numba box/unbox functions that wrap/unwrap an `ak.Array` and then use `ak.Array`'s solution. It sounds perverse, wrapping with a high-level interface in order to generate a low-level Numba model, but that's the situation. Personally, I think it makes more sense to just document it somewhere: a list of all the things that bare layout objects can't do, but `ak.Array` can.",
  "created_at":"2020-07-13T17:55:42Z",
  "id":657703788,
  "issue":332,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NzcwMzc4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-13T18:05:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ok, that's fine. I brought it up since most `ak.*` methods seem to be happy with either (since they are often _un_-wrapping)",
  "created_at":"2020-07-13T18:07:57Z",
  "id":657709874,
  "issue":332,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NzcwOTg3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-13T18:07:57Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Call it documentation issue then?",
  "created_at":"2020-07-13T18:08:17Z",
  "id":657710026,
  "issue":332,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NzcxMDAyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-13T18:08:17Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes. That makes more sense.",
  "created_at":"2020-07-13T18:09:51Z",
  "id":657710744,
  "issue":332,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NzcxMDc0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-13T18:09:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I had half a thought to make layouts wrap themselves as `ak.Array` before entering a Numbafied function, but that would be confusing because if they are passed _through_, they'll be changed, and their semantics inside the Numbafied function would be as high-level, not low-level arrays.\r\n\r\nInstead, I should identify layouts on their way in and have them raise an error there. The in-situ notification will be a lot more useful than documentation hidden in a corner of the website.",
  "created_at":"2020-11-05T13:07:18Z",
  "id":722366688,
  "issue":332,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMjM2NjY4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-05T13:07:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I was just about to do this when i found that I've already done it:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e023cfc7a95c90ea980b7c348049ceb10fdca807/src/awkward/_connect/_numba/layout.py#L16-L49\r\n\r\nFor example,\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import numba as nb\r\n>>> @nb.njit\r\n... def f(array):\r\n...     return array\r\n... \r\n>>> f(ak.Array([[1, 2, 3], [], [4, 5]]))\r\n<Array [[1, 2, 3], [], [4, 5]] type='3 * var * int64'>\r\n```\r\n\r\nbut\r\n\r\n```python\r\n>>> f(ak.Array([[1, 2, 3], [], [4, 5]]).layout)\r\nTypeError: ListOffsetArray64 objects cannot be passed directly into Numba-compiled functions; construct a high-level ak.Array or ak.Record instead\r\n```",
  "created_at":"2020-12-07T22:27:37Z",
  "id":740218817,
  "issue":332,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDIxODgxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T22:27:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Since this is a known fact about these low-level objects, it might make more sense to call this a feature request. But yeah, it would definitely make life easier for developers who are building things on top of Awkward if they could treat `Index` as a normal array-like object.",
  "created_at":"2020-07-13T18:07:06Z",
  "id":657709461,
  "issue":333,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NzcwOTQ2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-13T18:07:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes:\r\n\r\n```python\r\n>>> import numpy\r\n>>> a = numpy.array([1, 2, 3, 4])\r\n>>> b = numpy.array([-1])\r\n>>> c = numpy.array([True, False, True, True])\r\n>>> numpy.where(c, a, b)\r\narray([ 1, -1,  3,  4])\r\n```",
  "created_at":"2020-07-13T18:08:56Z",
  "id":657710306,
  "issue":334,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NzcxMDMwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-13T18:08:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This is on OS X, I believe it also happens on linux but am not sure.",
  "created_at":"2020-07-13T19:16:56Z",
  "id":657742660,
  "issue":335,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1Nzc0MjY2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-13T19:16:56Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"It's reproducible on Linux.\r\n\r\nAlso, I noticed this:\r\n\r\n```python\r\n>>> b.dtype.type\r\n<class 'numpy.longlong'>\r\n>>> b.astype('i8').dtype.type\r\n<class 'numpy.int64'>\r\n```\r\n\r\nlong-long...",
  "created_at":"2020-07-13T19:34:12Z",
  "id":657750449,
  "issue":335,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1Nzc1MDQ0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-13T19:34:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I believe this is related to pybind/pybind11#1908.",
  "created_at":"2020-07-13T19:35:16Z",
  "id":657750906,
  "issue":335,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1Nzc1MDkwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-13T19:35:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is fixed because `ak.Array` now knows about _all_ the NumPy dtypes, including `longlong`. That came as part of a general clean-up, in which I used \"format in {c, b, h, i, l, q, B, H, I, L, Q\" + itemsize to determine its portable type (e.g. `int32_t` vs `int64_t`), rather than relying on the (platform-dependent) value of the format to determine itemsize. It was a major refactoring (3.5k lines). But it introduces uniformity that was lacking and stubs in all the places we would need it if we're ever going to support `float16` or complex numbers.",
  "created_at":"2020-07-15T23:40:33Z",
  "id":659070446,
  "issue":335,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTA3MDQ0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-15T23:40:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Just to piggyback, I feel like `ak.pad` is a well-deserved function that could combine the arguments of `ak.pad_none` and `ak.fill_none`.",
  "created_at":"2020-07-14T21:28:36Z",
  "id":658423033,
  "issue":336,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1ODQyMzAzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-14T21:28:36Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Isn't the fact that\r\n```python\r\nIn [9]: ak.fill_none(ak.pad_none(array.a, 2, clip=True), 0.)\r\nOut[9]: <Array [[1, 2], [0, 0], [3, 4]] type='3 * 2 * float64'>\r\n```\r\ncasts the integers in `array.a` into floats a bug?",
  "created_at":"2020-07-14T21:38:43Z",
  "id":658427056,
  "issue":336,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1ODQyNzA1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-14T21:38:43Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I just took a look at this and I agree that it could be a better interface. But before developing a new function, perhaps I should throw some more ideas into the mix.\r\n\r\nThe real issue here is that the padding and filling aren't going all the way down to the numeric level: they're applying to the records. That's why we get Nones in the place of the records (and the `?` is on the record type, not the numeric fields within the record):\r\n\r\n```python\r\n>>> ak.pad_none(array, 2, clip=True)\r\n<Array [[{a: 1, b: 1.1}, ... a: 4, b: 4.4}]] type='3 * 2 * ?{\"a\": int64, \"b\": fl...'>\r\n>>> ak.pad_none(array, 2, clip=True).tolist()\r\n[[{'a': 1, 'b': 1.1}, {'a': 2, 'b': 2.2}], [None, None], [{'a': 3, 'b': 3.3}, {'a': 4, 'b': 4.4}]]\r\n```\r\n\r\nThen when these get filled with zeros, they're zeros in the place of records, which has to be a union.\r\n\r\n```python\r\n>>> ak.fill_none(ak.pad_none(array, 2, clip=True), 0)\r\n<Array [[{a: 1, b: 1.1}, ... a: 4, b: 4.4}]] type='3 * 2 * union[{\"a\": int64, \"b...'>\r\n>>> ak.fill_none(ak.pad_none(array, 2, clip=True), 0).tolist()\r\n[[{'a': 1, 'b': 1.1}, {'a': 2, 'b': 2.2}], [0, 0], [{'a': 3, 'b': 3.3}, {'a': 4, 'b': 4.4}]]\r\n```\r\n\r\nWhat you really want are zeros in place of the numeric fields, which do unify the array elements with the fill value. To get at the fields individually, we can [ak.unzip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.unzip.html) the records (remembering that breaking and merging records is an _O(1)_ operation that we can do freely).\r\n\r\n```python\r\n>>> ak.unzip(array)\r\n(<Array [[1, 2], [], [3, 4, 5]] type='3 * var * int64'>,\r\n <Array [[1.1, 2.2], [], [3.3, 4.4, 5.5]] type='3 * var * float64'>)\r\n```\r\n\r\nSo what we really need to do is apply the padding and filling to each of these arrays. We can do it independently of the number of record fields with a list comprehension,\r\n\r\n```python\r\n>>> [ak.fill_none(ak.pad_none(x, 2, clip=True), 0) for x in ak.unzip(array)]\r\n[<Array [[1, 2], [0, 0], [3, 4]] type='3 * 2 * int64'>,\r\n <Array [[1.1, 2.2], [0, 0], [3.3, 4.4]] type='3 * 2 * float64'>]\r\n```\r\n\r\nand then to wrap the whole thing up, we can reverse the unzip with [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html).\r\n\r\n```python\r\n>>> regularized = ak.zip(dict(zip(\r\n...     ak.keys(array),\r\n...     [ak.fill_none(ak.pad_none(x, 2, clip=True), 0) for x in ak.unzip(array)]\r\n... )))\r\n>>> ak.type(regularized)\r\n3 * 2 * {\"a\": int64, \"b\": float64}\r\n>>> ak.to_list(regularized)\r\n[[{'a': 1, 'b': 1.1}, {'a': 2, 'b': 2.2}],\r\n [{'a': 0, 'b': 0.0}, {'a': 0, 'b': 0.0}],\r\n [{'a': 3, 'b': 3.3}, {'a': 4, 'b': 4.4}]]\r\n```\r\n\r\nMaybe this should have a high-level function? `ak.pad_fields`?\r\n\r\nCombining `ak.pad_none` and `ak.fill_none` into a single `ak.pad` makes sense (the implementation would just combine the operations on the Python side), but this `ak.pad_fields` is a different thing: it operates at the field level. Perhaps there needs to be `ak.pad_fields_none` and `ak.fill_fields_none` as well? No, because `ak.fill_fields_none`, at least, isn't any different from the `ak.fill_none` operation (which recursively replaces None values).\r\n\r\n```python\r\n>>> only_padded = ak.zip(dict(zip(\r\n...     ak.keys(array), [ak.pad_none(x, 2, clip=True) for x in ak.unzip(array)]\r\n... )))\r\n>>> ak.type(only_padded)\r\n3 * 2 * {\"a\": ?int64, \"b\": ?float64}\r\n>>> ak.to_list(only_padded)\r\n[[{'a': 1, 'b': 1.1}, {'a': 2, 'b': 2.2}],\r\n [{'a': None, 'b': None}, {'a': None, 'b': None}],\r\n [{'a': 3, 'b': 3.3}, {'a': 4, 'b': 4.4}]]\r\n>>> \r\n>>> regularized = ak.fill_none(only_padded, 0)\r\n>>> ak.type(regularized)\r\n3 * 2 * {\"a\": int64, \"b\": float64}\r\n>>> ak.to_list(regularized)\r\n[[{'a': 1, 'b': 1.1}, {'a': 2, 'b': 2.2}],\r\n [{'a': 0, 'b': 0.0}, {'a': 0, 'b': 0.0}],\r\n [{'a': 3, 'b': 3.3}, {'a': 4, 'b': 4.4}]]\r\n```\r\n\r\nSo the missing functionality is `ak.pad_fields_none` (distinct from [ak.pad_none](https://awkward-array.readthedocs.io/en/latest/_auto/ak.pad_none.html)'s `axis` parameter because `axis` is the number of _nested list_ depths, not record depths) and maybe convenience functions that merge `ak.pad_none`/`ak.pad_fields_none` and `ak.fill_none`.\r\n\r\nActually, the `ak.pad_none`/`ak.pad_fields_none` thing feels like it ought to be a function parameter. Then the convenience function `ak.pad` would have that same parameter.",
  "created_at":"2020-07-20T18:30:45Z",
  "id":661259803,
  "issue":336,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTI1OTgwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T18:30:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Isn't the fact that\r\n> \r\n> ```python\r\n> In [9]: ak.fill_none(ak.pad_none(array.a, 2, clip=True), 0.)\r\n> Out[9]: <Array [[1, 2], [0, 0], [3, 4]] type='3 * 2 * float64'>\r\n> ```\r\n> \r\n> casts the integers in `array.a` into floats a bug?\r\n\r\n@nsmith- No, that's intentional:\r\n\r\n```python\r\n>>> ak.fill_none(ak.Array([1, 2, None, 4]), 3)\r\n<Array [1, 2, 3, 4] type='4 * int64'>\r\n>>> ak.fill_none(ak.Array([1, 2, None, 4]), 3.0)\r\n<Array [1, 2, 3, 4] type='4 * float64'>\r\n```\r\n\r\nWhat's happening here is that Nones are first replaced by a temporary UnionArray that combines whatever is in the array with whatever the replacement value is: `union[int64, int64]` and `union[int64, float64]` in the two cases above. Then we attempt to `simplify` the temporary UnionArray. Unions of two numeric types can be unified to a numeric type, which is the broadest of the numeric choices: `int64` and `float64` in the two cases above. It is equivalent to the type unification that NumPy performs when concatenating:\r\n\r\n```python\r\n>>> np.concatenate([np.array([1, 2, 3]), np.array([4])])\r\narray([1, 2, 3, 4])\r\n>>> np.concatenate([np.array([1, 2, 3]), np.array([4.0])])\r\narray([1., 2., 3., 4.])\r\n```\r\n\r\n(In fact, [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) calls does this through a UnionArray `simplify`, too. The PR #337 that you motivated by finding NumPy dtype bugs ensures that we now use exactly the same unification rules as NumPy.)\r\n\r\nIn @nikoladze's case, the UnionArray of records and numbers (zero) could not be `simplified`.",
  "created_at":"2020-07-20T18:39:26Z",
  "id":661264393,
  "issue":336,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTI2NDM5Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-07-20T18:39:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Initial survey: https://github.com/pybind/pybind11/issues/1908#issuecomment-658358767",
  "created_at":"2020-07-14T19:16:50Z",
  "id":658363799,
  "issue":337,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1ODM2Mzc5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-14T19:16:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thank you! Does this cover largebytes and largestrings too?",
  "created_at":"2020-07-16T19:46:15Z",
  "id":659630812,
  "issue":340,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTYzMDgxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-16T19:46:15Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes; in Awkward, strings are lists with special parameters, so converting all lists that can be 32-bit into 32-bit gets the strings as well. The original unit tests don't seem to check the types of any Arrow strings, though.\r\n\r\nI'm having second thoughts about converting all lists, since we might like to pass lists with 64-bit offsets directly to Arrow without a copy if possible. But since Arrow started with everything being 32-bits, the safest thing to do is to use 32-bit wherever possible and consider the avoidance of a down-conversion a future performance optimization.\r\n\r\nI was about to start the Parquet reader/writer, depending on how much time I have this afternoon. #312 seemed to be a blocker.",
  "created_at":"2020-07-16T19:57:22Z",
  "id":659637977,
  "issue":340,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTYzNzk3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-16T19:57:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":">  we might like to pass lists with 64-bit offsets directly to Arrow\r\n\r\n(but not for parquet, so I would defer this, if possible)",
  "created_at":"2020-07-16T19:59:05Z",
  "id":659638862,
  "issue":340,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTYzODg2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-16T19:59:05Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a duplicate of #291.",
  "created_at":"2020-07-17T13:31:03Z",
  "id":660108579,
  "issue":342,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDEwODU3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-17T13:31:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@martindurant As expected, Parquet support was rather easy\u2014I just had to copy the old code. Awkward PartitionedArrays translate into row groups, top-level RecordArrays translate into record batches, and any other kind of array is presented as a record batch with one field whose name is the empty string. (This convention is also used when reading back: if there's only one field and its name is the empty string, we read back into a non-empty array.)\r\n\r\nI also tested it on some ancient samples I made for OAMap, but even after all these years, most of the data structures are not supported by pyarrow:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/97b165e53666d13a88da247ea18d577bfdf85761/tests/test_0341-parquet-reader-writer.py#L105-L246\r\n\r\nTo mitigate this, I added an `explode_records` option to `ak.to_parquet`. The transformation isn't lossy, but the result would have to be read back into [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html). Not ideal.\r\n\r\nBut hey, we have both eager and lazy reading now!",
  "created_at":"2020-07-17T01:15:43Z",
  "id":659770050,
  "issue":343,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTc3MDA1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-17T01:15:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I have this checked out and I've been tinkering with it: I started `nvidia-smi -l 1` so that I see\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      3004      G   /usr/lib/xorg/Xorg                           174MiB |\r\n|    0      3157      G   /usr/bin/gnome-shell                          84MiB |\r\n|    0      4505      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files   109MiB |\r\n|    0      8330      G   /opt/zoom/zoom                                18MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nThen when I\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> import numpy as np\r\n>>> array = ak.Array(np.arange(1000000).reshape(-1, 1))\r\n>>> array2 = ak.copy_to(array, \"cuda\")\r\n>>> array2.layout\r\n<NumpyArray format=\"l\" shape=\"1000000 1\" data=\"0x ...\" at=\"0x7f72e2000000\">\r\n    <Lib name=\"cuda\" device_number=\"0\" device_name=\"GeForce GTX 1060\"/>\r\n</NumpyArray>\r\n```\r\n\r\nthe monitor bumps up to\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      2935      C   python                                        61MiB |\r\n|    0      3004      G   /usr/lib/xorg/Xorg                           174MiB |\r\n|    0      3157      G   /usr/bin/gnome-shell                          84MiB |\r\n|    0      4505      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files   109MiB |\r\n|    0      8330      G   /opt/zoom/zoom                                18MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nGood! It's allocated 61 MiB on the GPU.\r\n\r\nNext, I expected the `ak.num` operation to entirely take place on the GPU, but it does not:\r\n\r\n```python\r\n>>> three = ak.num(array2, axis=1)\r\n>>> three.layout\r\n<NumpyArray format=\"l\" shape=\"1000000\" data=\"1 1 ... 1 1\" at=\"0x7f730365e010\"/>\r\n```\r\n\r\nI was expecting `three` to be a GPU array as well, which would have bumped the memory usage up to 122 MiB, but it stayed at 61 MiB.\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      2935      C   python                                        61MiB |\r\n|    0      3004      G   /usr/lib/xorg/Xorg                           174MiB |\r\n|    0      3157      G   /usr/bin/gnome-shell                          84MiB |\r\n|    0      4505      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files   109MiB |\r\n|    0      8330      G   /opt/zoom/zoom                                18MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n(so what I'm looking at isn't even a copy of a GPU-resident array; the only copy is in main memory).\r\n\r\nDid you intend for it to work that way, or is it an error in the implementation? When I said that we'd need GPU arrays to stay on the GPU, I meant all of their derivatives as well.\r\n\r\nThat might not need to be in this PR, but we'll need it to work that way eventually.\r\n\r\n(Meanwhile, I'm still working on this Docker image; I don't use Docker much but I'm figuring it out.)",
  "created_at":"2020-07-20T20:11:35Z",
  "id":661308170,
  "issue":345,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTMwODE3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T20:11:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"## What the Docker image should include\r\n\r\nThe _base_ from which to launch a test. This must not include recent code changes or any compiled artifacts, but it can include the git repository as it was at some date so that launching a test starts with `git pull`, rather than `git clone`.\r\n\r\nI think the system Python that comes with Ubuntu 18.04 is too old. I know that Python 3.6 is one of our test versions, but something related to the system Python is sufficiently out of date that pip isn't compiling the cuda-kernels. It's generally a bad idea to use system Pythons anyway\u2014we'll have troubles down the road if we follow that route. Therefore, I think we should use conda to install not just the Python, but the compilers and everything (to be sure we can update them as needed).\r\n\r\n## What the test should execute\r\n\r\nIt should `git pull` to a specified branch (it will have to be possible to pass a script option into the `docker run`), pip-install the cuda-kernels, compile the main Awkward codebase any way we like (therefore, localbuild.py), and then run the `tests_cuda`.\r\n\r\n## Working Dockerfile.build\r\n\r\nIn a completely clean recursive git-clone of awkward-1.0 (master branch), I put the following files:\r\n\r\n```\r\n########################## Dockerfile.build\r\n\r\nFROM nvidia/cuda:10.2-devel-ubuntu18.04\r\n\r\nWORKDIR /awkward-1.0\r\n\r\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\r\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\r\n\r\nRUN apt-get update && apt-get install -y emacs-nox wget\r\n\r\nRUN wget \\\r\n    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\r\n    && mkdir /root/.conda \\\r\n    && bash Miniconda3-latest-Linux-x86_64.sh -b \\\r\n    && rm -f Miniconda3-latest-Linux-x86_64.sh \r\n\r\nRUN conda config --add channels conda-forge\r\n\r\nRUN conda update conda -y\r\n\r\nRUN conda install pip git gcc_linux-64 gxx_linux-64 cmake numpy cupy pytest -y\r\n\r\nRUN git config --global pull.rebase false\r\n\r\nCOPY . .\r\n```\r\n\r\nand\r\n\r\n```bash\r\n########################## do-tests.sh\r\n\r\n#!/bin/bash\r\n\r\nset -e\r\n\r\nconda --version\r\npython --version\r\n\r\ngit checkout $1\r\ngit pull\r\ngit status\r\n\r\ncp -a include src VERSION_INFO cuda-kernels\r\n\r\ncd cuda-kernels\r\npip install .\r\ncd ..\r\n\r\npython -c 'import awkward1_cuda_kernels; print(awkward1_cuda_kernels.shared_library_path)'\r\n\r\npython localbuild.py --pytest tests\r\n\r\npython -m pytest -vvrs tests_cuda\r\n```\r\n\r\nand build it with\r\n\r\n```bash\r\ndocker build -f Dockerfile.build -t awkward1-cuda-tests:1.0-cuda10.2 .\r\n```\r\n\r\nSo the Dockerfile is versioned with `1.0` (our version) and `-cuda10.2` (the CUDA image it's base on), and it contains\r\n\r\n   * a text editor for debugging (I include `emacs-nox`; I don't know if you want vi or something)\r\n   * wget, which is needed to get Miniconda\r\n   * an updated Miniconda distribution based on conda-forge\r\n   * git, compilers (gcc), CMake from conda\r\n   * Python 3.7 (with all dev files for compiling)\r\n   * Python packages we'll need, including NumPy, CuPy, and pytest (pybind11 is included in one of our submodules)\r\n   * a clean git checkout of scikit-hep/awkward-1.0 (master branch)\r\n   * the `do-tests.sh` file\r\n\r\nWhen we run\r\n\r\n```bash\r\ndocker run awkward1-cuda-tests:1.0-cuda10.2 sh do-tests.sh trickarcher/python_cuda_interface\r\n```\r\n\r\nthe `do-tests.sh` file:\r\n\r\n   1. checks out the branch we want to test and does a `git pull`\r\n   2. copies the `include`, `src`, `VERSION_INFO` files into `cuda-kernels` because pip is running in a different directory than we thought it might (IMPORTANT: the copy must happen repeatedly for each test, so that these files do not become stale\u2014it's very important that this is not \"baked into\" the Docker image)\r\n   3. does a pip install of awkward1-cuda-kernels\r\n   4. compiles and tests the main Awkward codebase\r\n   5. tests the `tests_cuda`.\r\n\r\n## How this should be arranged in the git repo\r\n\r\nDockerfile.build for the different versions of CUDA should be in the `dev` directory (possibly a subdirectory) because it's something that a developer would do rarely. Also, it's not going to be done once for some value of\r\n\r\n```bash\r\nmodinfo nvidia --field version\r\n```\r\n\r\nbecause the version that we have on the machine that builds the Docker images is not necessarily the version that we have on the cloud instance. I'll be investigating AWS soon, to see what it has (and how to run Docker images on it).\r\n\r\nThe `do-tests.sh` should probably have a name related to CUDA, such as `run-cuda-tests.sh`, and it may be inside `.ci` because it's something that runs tests. It should probably have executable permissions so that we don't have to run it from `sh`, but I was able to test it above without that.\r\n\r\nThe `tests_cuda` directory should be `tests-cuda` and should include an empty `__init__.py` file (required by old Pythons, now just a convention). The `pytest` command should be run from within `python` and have `-vvrs` options (totally verbose, listing skipped tests). That will make it easier to debug.\r\n\r\n**Now I need to find out how to run Docker images on AWS, so that we'll have a procedure in place to run this.**",
  "created_at":"2020-07-20T23:00:37Z",
  "id":661413319,
  "issue":345,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTQxMzMxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T23:00:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski  Thanks for looking into my commits. That `ak.num` was a serious bug. So first things first, `num` wasn't implemented for `RegularArray`(`NumpyArray` calls the `RegularArray` `num` kernel) so the program didn't crash because we are using Unified Memory. After I wrote the kernel for it, I noticed that there was a constant `bus error` happening which caused the program to crash. I fixed it because someone posted the same issue as a `gist` on github, by adding `cudaDeviceSynchronize()` at the end of the `cuda launch` call. This behaviour is very inconsistent in the sense that it would sometimes occur for the other `num`  kernel I had written(`ListArray`). I would like to investigate this further, but an obvious guess would be that something dangerous was happening on the lower levels of `CUDA` . As a note to self, always add `cudaDeviceSynchronize()` , after a kernel launch.",
  "created_at":"2020-07-21T13:35:19Z",
  "id":661865306,
  "issue":345,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTg2NTMwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-21T13:35:19Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - there is a temporary solution to silence the warnings in `NumpyArray::merge` where a compiler asks to add missing switch cases. ",
  "created_at":"2020-07-21T08:06:36Z",
  "id":661703810,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTcwMzgxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-21T08:06:36Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Unhanded enumerations might have been intentional because they might be guaranteed by previous code. (`merge` makes several passes over the LHS and RHS types). However, I'm surprised that there are ~14 unhanded cases\u2014I think there might be about 14 total cases. Something's odd about that message. (The enumeration is defined in include/awkward/util.h.)\r\n\r\nIf the enumeration is guaranteed by previous code (most likely `dtype::NOT_PRIMITIVE`), then the right exception for something that shouldn't happen is `std::runtime_error`. We use `std::invalid_argument` as feedback to the user that the user did something wrong, but if the default case happens in this switch, then it's our fault.\r\n\r\nIncidentally, this enum is what has become of all those `format` checks. We were getting errors about corner cases, so it's now centralized and carried around as an enum.",
  "created_at":"2020-07-21T11:50:33Z",
  "id":661808722,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTgwODcyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-21T11:50:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - here are some thoughts. Please, comment.\r\n\r\n`awkward1.can_cast(fromtype, totype)` to lookup casting rules:\r\n```python\r\nassert awkward1.can_cast(dtype=numpy.float64, dtype=numpy.float32) == True or False  # allow double rounding?\r\nassert awkward1.can_cast(dtype=numpy.float32, dtype=numpy.float64) == True\r\n```\r\n\r\n`awkward1.astype(array, totype)` is a method of an array. It takes the values of the array and produces a new array with the desire `dtype`:\r\n```python\r\ncontent_float64 = awkward1.layout.NumpyArray(numpy.array([1.1, 2.2, 3.3, 4.4, 5.5], dtype=numpy.float64))\r\narray_float64 = awkward1.layout.UnmaskedArray(content_float64)\r\nassert awkward1.to_list(array_float64) == [1.1, 2.2, 3.3, 4.4, 5.5]\r\nassert str(awkward1.type(content_float64)) == \"float64\"\r\nassert str(awkward1.type(awkward1.Array(content_float64))) == \"5 * float64\"\r\nassert str(awkward1.type(array_float64)) == \"?float64\"\r\nassert str(awkward1.type(awkward1.Array(array_float64))) == \"5 * ?float64\"\r\n\r\ncontent_float32 = awkward1.astype(content_float64, dtype=numpy.float32)\r\narray_float32 = awkward1.layout.UnmaskedArray(content_float32)\r\nassert awkward1.to_list(array_float32) == [1.1, 2.2, 3.3, 4.4, 5.5]\r\nassert str(awkward1.type(content_float32)) == \"float32\"\r\nassert str(awkward1.type(awkward1.Array(content_float32))) == \"5 * float32\"\r\nassert str(awkward1.type(array_float32)) == \"?float32\"\r\nassert str(awkward1.type(awkward1.Array(array_float32))) == \"5 * ?float32\"\r\n\r\ncontent_int8 = awkward1.astype(content_float32, dtype=numpy.int8)\r\narray_int8 = awkward1.layout.UnmaskedArray(content_int8)\r\nassert awkward1.to_list(array_int8) == [1, 2, 3, 4, 6]\r\nassert str(awkward1.type(content_int8)) == \"int8\"\r\nassert str(awkward1.type(awkward1.Array(content_int8))) == \"5 * int8\"\r\nassert str(awkward1.type(array_int8)) == \"?int8\"\r\nassert str(awkward1.type(awkward1.Array(array_int8))) == \"5 * ?int8\"\r\n```",
  "created_at":"2020-07-21T12:11:26Z",
  "id":661817913,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTgxNzkxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-21T12:11:26Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Unhanded enumerations might have been intentional because they might be guaranteed by previous code. (`merge` makes several passes over the LHS and RHS types). However, I'm surprised that there are ~14 unhanded cases\u2014I think there might be about 14 total cases. Something's odd about that message. (The enumeration is defined in include/awkward/util.h.)\r\n> \r\n> If the enumeration is guaranteed by previous code (most likely `dtype::NOT_PRIMITIVE`), then the right exception for something that shouldn't happen is `std::runtime_error`. We use `std::invalid_argument` as feedback to the user that the user did something wrong, but if the default case happens in this switch, then it's our fault.\r\n> \r\n> Incidentally, this enum is what has become of all those `format` checks. We were getting errors about corner cases, so it's now centralized and carried around as an enum.\r\n\r\nThanks, @jpivarski ! I've corrected the error messages.",
  "created_at":"2020-07-21T12:50:12Z",
  "id":661839838,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTgzOTgzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-21T12:50:12Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> `awkward1.can_cast(fromtype, totype)` to lookup casting rules:\r\n> \r\n> ```python\r\n> assert awkward1.can_cast(dtype=numpy.float64, dtype=numpy.float32) == True or False  # allow double rounding?\r\n> assert awkward1.can_cast(dtype=numpy.float32, dtype=numpy.float64) == True\r\n> ```\r\n\r\nI don't think a `can_cast` function will be necessary because the numeric conversions desired here would be truncating conversions. Someone may have float64 that they want to convert to int8. An example of a user needing a feature like this is issue #328: it's easy to make arrays containing int64, but not int8\u2014having an `ak.to_numeric_type(array, dtype)` would allow them to create an array with int64 and convert it to int8 afterward. (The solution I gave them was for building the array with int8 in the first place, which involves more low-level routines.)\r\n\r\nOne common use would be after applying the NumPy ufunc `np.trunc`, you get floating point numbers without any decimals. Users would very likely want to then convert those floating point numbers into actual integers, perhaps to use the array as an integer-array slice.\r\n\r\nSo the function would be used in lossy contexts often.\r\n\r\nI don't think it should take an `axis` parameter: a function like this can only apply to the leaves of a structure. The simplest thing is probably to make it apply to all leaves (NumpyArrays), with the exception of NumpyArrays that have parameter `__array__` == `\"char\"` or `\"byte\"`, since we wouldn't want strings to turn into floating point numbers or anything like that.",
  "created_at":"2020-07-21T15:03:31Z",
  "id":661915955,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTkxNTk1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-21T15:03:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As for a name, I think users wouldn't get anything involving the word \"leaf.\" Maybe `ak.to_numeric_type` (there are a lot of `to_` functions in `operations/convert.py`), but it would help if the name specified that this is only changing the types of the numbers, not all other types. It should pass through records, lists, and option-type nodes without affecting them (i.e. nullable floats can be turned into nullable ints).\r\n\r\nMaybe `ak.numbers_to_type(...)`?\r\n\r\nThis would be a good application of the new enums; the NumPy dtype would be examined in the pybind11 layer, an enum selected, and then that enum can pass down through the tree.\r\n\r\nThis:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/1755384ab381074eaa595e4ef984f651e966e7eb/src/python/forms.cpp#L22-L42\r\n\r\nis an example of converting a dtype into `kind` (a `char`) and `itemsize` (an `int`), which can then be converted into an enum with something like\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/1755384ab381074eaa595e4ef984f651e966e7eb/src/libawkward/Content.cpp#L33-L211\r\n\r\nThis numeric type converter shouldn't do anything with `inner_shape`. NumPy dtypes can have `shape`, but that should be an error if passed to the numeric type converter. (Without handling it explicitly, the `kind` of a dtype with `shape` is `\"V\"`, which is not a numeric `kind`.)",
  "created_at":"2020-07-21T15:12:49Z",
  "id":661921401,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTkyMTQwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-21T15:12:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> As for a name, I think users wouldn't get anything involving the word \"leaf.\" Maybe `ak.to_numeric_type` (there are a lot of `to_` functions in `operations/convert.py`), but it would help if the name specified that this is only changing the types of the numbers, not all other types. It should pass through records, lists, and option-type nodes without affecting them (i.e. nullable floats can be turned into nullable ints).\r\n> \r\n> Maybe `ak.numbers_to_type(...)`?\r\n> \r\n> This would be a good application of the new enums; the NumPy dtype would be examined in the pybind11 layer, an enum selected, and then that enum can pass down through the tree.\r\n> \r\n> This:\r\n> \r\n> https://github.com/scikit-hep/awkward-1.0/blob/1755384ab381074eaa595e4ef984f651e966e7eb/src/python/forms.cpp#L22-L42\r\n> \r\n> is an example of converting a dtype into `kind` (a `char`) and `itemsize` (an `int`), which can then be converted into an enum with something like\r\n> \r\n> https://github.com/scikit-hep/awkward-1.0/blob/1755384ab381074eaa595e4ef984f651e966e7eb/src/libawkward/Content.cpp#L33-L211\r\n> \r\n> This numeric type converter shouldn't do anything with `inner_shape`. NumPy dtypes can have `shape`, but that should be an error if passed to the numeric type converter. (Without handling it explicitly, the `kind` of a dtype with `shape` is `\"V\"`, which is not a numeric `kind`.)\r\n\r\nI like `ak.to_numeric_type`",
  "created_at":"2020-07-21T15:22:09Z",
  "id":661926949,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTkyNjk0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-21T15:22:09Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> I like `ak.to_numeric_type`\r\n\r\nI'm having second thoughts, wanting to avoid the impression that this changes the array's type, whatever it happens to be, into something numeric, when it actually changes the numeric types to other numeric types. Maybe only the word order needs to change:\r\n\r\n```\r\nak.numeric_to_type(array, dtype)\r\n```\r\n\r\nor\r\n\r\n```\r\nak.numbers_to_type(array, dtype)\r\n```\r\n\r\n? (\"Numbers\" would probably be easier to remember than \"numeric\".)",
  "created_at":"2020-07-21T15:25:35Z",
  "id":661928918,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTkyODkxOA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-07-21T15:25:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - please, let me know if this is `awkward1.to_list` expected behaviour? Thanks\r\n\r\nThe array of `float32`:\r\n```python\r\n<NumpyArray format=\"f\" shape=\"5\" data=\"1.1 2.2 3.3 4.4 5.5\" at=\"0x7f9d9446c6d0\"/>\r\n\r\n```\r\nand its `awkward1.to_list`:\r\n```python\r\n>       assert awkward1.to_list(array_float32) == [1.1, 2.2, 3.3, 4.4, 5.5]\r\nE       assert [1.100000023841858,\\n 2.200000047683716,\\n 3.299999952316284,\\n 4.400000095367432,\\n 5.5] == [1.1, 2.2, 3.3, 4.4, 5.5]\r\nE         At index 0 diff: 1.100000023841858 != 1.1\r\nE         Full diff:\r\nE           [\r\nE         -  1.100000023841858,\r\nE         -  2.200000047683716,\r\nE         -  3.299999952316284,\r\nE         -  4.400000095367432,\r\nE         +  1.1,\r\nE         +  2.2,\r\nE         +  3.3,\r\nE         +  4.4,\r\nE            5.5,\r\nE           ]\r\n\r\n```",
  "created_at":"2020-07-24T16:33:39Z",
  "id":663625197,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzYyNTE5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-24T16:33:39Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"It's because the float32 is losing precision. The Python `float` type is the IEEE 64-bit binary floating-point: i.e. Python's `float` is C's `double`. When numbers are written in the source code, they're parsed as doubles and compared as doubles, but if the values went through a 32-bit floating point format (in NumPy or Awkward), then there will be round-off error.\r\n\r\npytest has a \"nearly equal to\" comparison, but I think it makes more sense here to just  put a literal `1.100000023841858, 2.200000047683716, 3.299999952316284, 4.400000095367432` in the test code (or make the test points factors of 2, like `0.5`, `0.25`).\r\n\r\n\r\n",
  "created_at":"2020-07-24T18:13:40Z",
  "id":663667850,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY2Nzg1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-24T18:13:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I'm not sure what should I do about these failing doctests. This PR introduces macros. Could this be a problem?\r\n```bash\r\nwriting _auto/ak._connect._numba.builder.lower_integer.rstTraceback (most recent call last):\r\n  File \"/home/vsts/work/1/s/docs-sphinx/../dev/genpython.py\", line 710, in <module>\r\n    ast = pycparser.c_parser.CParser().parse(pfile)\r\n  File \"/opt/hostedtoolcache/Python/3.8.5/x64/lib/python3.8/site-packages/pycparser/c_parser.py\", line 149, in parse\r\n    return self.cparser.parse(\r\n  File \"/opt/hostedtoolcache/Python/3.8.5/x64/lib/python3.8/site-packages/pycparser/ply/yacc.py\", line 331, in parse\r\n    return self.parseopt_notrack(input, lexer, debug, tracking, tokenfunc)\r\n  File \"/opt/hostedtoolcache/Python/3.8.5/x64/lib/python3.8/site-packages/pycparser/ply/yacc.py\", line 1199, in parseopt_notrack\r\n    tok = call_errorfunc(self.errorfunc, errtoken, self)\r\n  File \"/opt/hostedtoolcache/Python/3.8.5/x64/lib/python3.8/site-packages/pycparser/ply/yacc.py\", line 193, in call_errorfunc\r\n    r = errorfunc(token)\r\n  File \"/opt/hostedtoolcache/Python/3.8.5/x64/lib/python3.8/site-packages/pycparser/c_parser.py\", line 1858, in p_error\r\n    self._parse_error(\r\n  File \"/opt/hostedtoolcache/Python/3.8.5/x64/lib/python3.8/site-packages/pycparser/plyparser.py\", line 67, in _parse_error\r\n    raise ParseError(\"%s: %s\" % (coord, msg))\r\npycparser.plyparser.ParseError: :1168:28: before: #\r\n\r\nConfiguration error:\r\nThere is a programmable error in your configuration file:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/hostedtoolcache/Python/3.8.5/x64/lib/python3.8/site-packages/sphinx/config.py\", line 319, in eval_config_file\r\n    execfile_(filename, namespace)\r\n  File \"/opt/hostedtoolcache/Python/3.8.5/x64/lib/python3.8/site-packages/sphinx/util/pycompat.py\", line 88, in execfile_\r\n    exec(code, _globals)\r\n  File \"/home/vsts/work/1/s/docs-sphinx/conf.py\", line 71, in <module>\r\n    subprocess.check_call(\r\n  File \"/opt/hostedtoolcache/Python/3.8.5/x64/lib/python3.8/subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/opt/hostedtoolcache/Python/3.8.5/x64/bin/python', '/home/vsts/work/1/s/docs-sphinx/../dev/genpython.py', '/home/vsts/work/1/s/docs-sphinx/../src/cpu-kernels/identities.cpp', '/home/vsts/work/1/s/docs-sphinx/../src/cpu-kernels/operations.cpp', '/home/vsts/work/1/s/docs-sphinx/../src/cpu-kernels/reducers.cpp', '/home/vsts/work/1/s/docs-sphinx/../src/cpu-kernels/getitem.cpp', '/home/vsts/work/1/s/docs-sphinx/../src/cpu-kernels/sorting.cpp']' returned non-zero exit status 1.\r\n```",
  "created_at":"2020-08-03T12:21:16Z",
  "id":667992413,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2Nzk5MjQxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T12:21:16Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"@reikdas Do you know that's happening here?",
  "created_at":"2020-08-03T14:27:27Z",
  "id":668053613,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODA1MzYxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T14:27:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The C++ -> Python transpiler cannot handle macros yet. I'll try and fix it. ",
  "created_at":"2020-08-03T16:14:16Z",
  "id":668110498,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODExMDQ5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T16:14:16Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna and @reikdas I see what's happening here: the NumpyArray fill functions are very regular\u2014I wrote them with an Emacs macro and Yana is writing them with a C macro, which is more DRY from a source code point of view, but Reik's parser needs to be able to parse this source code.\r\n\r\nExtending Reik's parser to handle macros is likely too difficult, at least too difficult on the timescale we have for the rest of the summer. On the other hand, if we can invoke C's preparser manually to expand those macros into source code that Reik's parser can handle, that could be a good solution.\r\n\r\nIt's a little ironic that we have templated the function, then need to make a concrete instance of each specialization, so we use a C macro, but a C macro is a bit like a templated function anyway. In principle, we could have implemented the whole thing in a macro, rather than using the macro to instantiate the template! But we shouldn't do that because then Reik's parser wouldn't be able to tell that these are all specializations of a single function (that's important information, which he gets from the fact that they come from the same template).\r\n\r\nThe two courses of action that I see as possibilities are\r\n\r\n   1. find a way to invoke the C preparser manually, to expand the macro into named functions that call template instantiations, for Reik's parser to pick up;\r\n   2. expand out the WET code in the source file, as it had been before.\r\n\r\nOption (2) requires almost no work; I'm sure Yana made the macro because, in usual circumstances, WET code is bad. In this circumstance, we have specific reasons why it's easier for the code to be redundantly written out in the source file like this.\r\n\r\nIf option (1) is easy\u2014Yana, if you know what command needs to be invoked to run the C preparser with the output being a temporary source file for Reik's parser\u2014then that could be a solution to both issues. On the other-other hand, having C macros expand template instantiations might be adding complexity just because it's more indirection\u2014sometimes a flat, repetitive file is the most maintainable.",
  "created_at":"2020-08-03T17:34:05Z",
  "id":668148727,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE0ODcyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T17:34:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Extending Reik's parser to handle macros is likely too difficult, at least too difficult on the timescale we have for the rest of the summer.\r\n\r\nI was actually wondering how to handle this and asked you on Slack :p \r\n\r\n> find a way to invoke the C preparser manually, to expand the macro into named functions that call template instantiations, for Reik's parser to pick up;\r\n\r\nI think this is possible - [pycparser even mentions it](https://github.com/eliben/pycparser#interaction-with-the-c-preprocessor). But since we are actually handling C++ code and not C, I am not sure if this will be difficult.",
  "created_at":"2020-08-03T17:41:21Z",
  "id":668152021,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE1MjAyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T17:41:21Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> I think this is possible - [pycparser even mentions it](https://github.com/eliben/pycparser#interaction-with-the-c-preprocessor). But since we are actually handling C++ code and not C, I am not sure if this will be difficult.\r\n\r\nThat's right: you need to parse the templates before handing it to pycparser (which would then run the macros). Because of the existence of non-C constructs, we can't have the macros handled before Reik's parser.\r\n\r\nIn that case, @ianna, it would be better to remove this Don't-Repeat-Yourself macro in favor of Write-Every-Time. The fact that these functions are more like data than code is intentional: it's to help automatic conversion to CUDA. These NumpyArray-filling functions, in particular, are embarrassingly parallel and we especially want Reik's parser to pick them up and convert them. The fact that we're seeing the error in documentation-building is just because that's the _first_ thing Reik's parser is used for.\r\n\r\nIf you know how to use the C++ preparser to expand the macro instances, then it should be easy to copy-paste its output into the source code. I generated them with Emacs macros, which were a little more in the workflow of editing...",
  "created_at":"2020-08-03T18:06:28Z",
  "id":668164705,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE2NDcwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:06:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> If you know how to use the C++ preparser to expand the macro instances, then it should be easy to copy-paste its output into the source code. I generated them with Emacs macros, which were a little more in the workflow of editing...\r\n\r\n```bash\r\nclang file.cc -E\r\n``` \r\nThe same for any other compiler: `gcc` or `g++`\r\n\r\nIn fact, there is no problem to expand them. I\u2019ll do it tomorrow morning.\r\n",
  "created_at":"2020-08-03T20:56:10Z",
  "id":668237336,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODIzNzMzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T20:59:24Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski and @reikdas - C++ preprocessor expands a macro in one line: https://github.com/scikit-hep/awkward-1.0/pull/346/commits/4ae8e44ca87c8262dd013c959c0d76b739a9d649\r\nSo, I've added a clang-format configuration and reformatted the code close to an original style. Now the kernel script fails:\r\n```bash\r\nStarting: Generate Kernel specification\r\n==============================================================================\r\nTask         : Command line\r\nDescription  : Run a command line script using Bash on Linux and macOS and cmd.exe on Windows\r\nVersion      : 2.164.2\r\nAuthor       : Microsoft Corporation\r\nHelp         : https://docs.microsoft.com/azure/devops/pipelines/tasks/utility/command-line\r\n==============================================================================\r\nGenerating script.\r\nScript contents:\r\npython dev/generate-kernelspec.py\r\n========================== Starting Command Output ===========================\r\n/bin/bash --noprofile --norc /Users/runner/work/_temp/31384ca2-88ca-4b7b-b51c-8dbeff0cad96.sh\r\nTraceback (most recent call last):\r\n  File \"dev/generate-kernelspec.py\", line 1073, in <module>\r\n    pfile, tokens = preprocess(filename)\r\n  File \"dev/generate-kernelspec.py\", line 331, in preprocess\r\n    parans.pop()\r\nIndexError: pop from empty list\r\n\r\n##[error]Bash exited with code '1'.\r\nFinishing: Generate Kernel specification\r\n```",
  "created_at":"2020-08-04T09:30:01Z",
  "id":668490361,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODQ5MDM2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-04T09:32:42Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I am not exactly sure about the reason for the issue yet, but it is possibly because I made some assumption about the file structure in my C++ -> C translator in `dev/generate-kernelspec.py` which is not always true in C/C++. I'll take a look and try to fix it. Sorry for the inconvenience. \r\n\r\nOne other thing I should point out is that in the latest commit to `master`, we introduced a new coding style requirement for the cpu-kernel include files - https://github.com/scikit-hep/awkward-1.0/blob/master/CONTRIBUTING.md#contributing-to-files-in-includeawkwardcpu-kernels. It would mostly involve copy-pasting some doxygen style comment strings above each function definition (here is the complete set of roles - https://github.com/scikit-hep/awkward-1.0/blob/master/kernel-specification/samples.json). Not having this _will_ cause the specification generator to break, after the issue you currently encountered is fixed. I can add those in while fixing the C++ -> C translator if you want. \r\n\r\nAn unrelated comment - I was looking at the file changes in this PR, and noticed that clang-format causes a huge diff. Maybe we should use something like [git clang-format](https://github.com/llvm/llvm-project/blob/master/clang/tools/clang-format/git-clang-format) so that only the changes in the PR have formatting applied, to preserve git blame (and have a cleaner PR diff). The disadvantage of doing that would be that the file in question would not be uniformly formatted, so maybe there could be a different PR where we clang-format the entire codebase and require that all new code must be formatted according to our `clang-format` config file? What do you and @jpivarski think?\r\n\r\n ",
  "created_at":"2020-08-04T10:30:48Z",
  "id":668517539,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODUxNzUzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-04T10:31:49Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"At the very least, the clang-format change should be in its own PR. git blame is already shadowed by the PR I did that bound all lines by 80 characters\u2014the lesson to learn from that is that formatting should be applied from day 1.\r\n\r\nIf the formatting is an improvement, then let's do it and be consistent from here on out. It might shadow real changes in git blame, which will be unfortunate, but if that has to happen anyway, it's better for it to happen earlier than later. (git blame is especially useful when a codebase has stabilized to mostly small, focused changes, which Awkward is getting pretty close to, with some exceptions.)\r\n\r\nWhat formatting options are being requested in clang-format? This question should probably be answered in a PR dedicated to formatting, not here, but we might make the impact of the formatting on history lighter by picking only the most useful formatting options.",
  "created_at":"2020-08-04T11:49:59Z",
  "id":668549231,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODU0OTIzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-04T11:49:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - the clang-format rules are in `.clang-format` YAML file as a part of this PR. I added the following options:\r\n\r\n* Should be used for C, C++.\r\n* Based on Google style: https://google.github.io/styleguide/cppguide.html\r\n* The column limit is 80\r\n* The indentation used for all namespaces:\r\n```c++\r\nnamespace out {\r\n  int i;\r\n  namespace in {\r\n    int i;\r\n  }\r\n}\r\n```\r\n* Do not sort `#includes`\r\n* The number of columns to use for indentation is 2\r\n* The extra indent or outdent of access modifiers is -2, e.g. `public:`\r\n* The penalty for each line break introduced inside a comment is 30\r\n* The penalty for each character outside of the column limit: 100\r\n* Horizontally align arguments after an open bracket\r\n* Aligns trailing comments\r\n```c++\r\nint a;     // My comment a      vs.     int a; // My comment a\r\nint b = 2; // comment  b                int b = 2; // comment about b\r\n```\r\n* AllowAllArgumentsOnNextLine: true\r\nIf a function call or braced initializer list doesn\u2019t fit on a line, allow putting all arguments onto the next line:\r\n```c++\r\ntrue:\r\ncallFunction(\r\n    a, b, c, d);\r\n\r\nfalse:\r\ncallFunction(a,\r\n             b,\r\n             c,\r\n             d);\r\n```\r\n* AllowAllParametersOfDeclarationOnNextLine: true\r\nIf the function declaration doesn\u2019t fit on a line, allow putting all parameters of a function declaration onto the next line\r\n```c++\r\ntrue:\r\nvoid myFunction(\r\n    int a, int b, int c, int d, int e);\r\n\r\nfalse:\r\nvoid myFunction(int a,\r\n                int b,\r\n                int c,\r\n                int d,\r\n                int e);\r\n```\r\n* AllowShortBlocksOnASingleLine: false\r\nNever merge blocks into a single line\r\n* AllowShortIfStatementsOnASingleLine: Never\r\nNever put short ifs on the same line\r\n* AllowShortLambdasOnASingleLine: false\r\nNever merge lambdas into a single line\r\n* AllowShortLoopsOnASingleLine: false\r\nNever put short loops on a single line\r\n* AlwaysBreakAfterReturnType: All\r\nAlways break after the return type\r\n* BinPackParameters: false\r\na function declaration\u2019s or function definition\u2019s parameters will either all be on the same line or will have one line each\r\n* AlwaysBreakTemplateDeclarations: Yes\r\nAlways break after template declaration\r\n* ReflowComments: false\r\nDo not re-flow comments\r\n* BinPackArguments: false\r\na function call\u2019s arguments will either be all on the same line or will have one line each\r\n\r\nHere is a link to full documentation:\r\nhttps://clang.llvm.org/docs/ClangFormatStyleOptions.html",
  "created_at":"2020-08-04T13:32:36Z",
  "id":668599155,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODU5OTE1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-04T13:32:36Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I am not exactly sure about the reason for the issue yet, but it is possibly because I made some assumption about the file structure in my C++ -> C translator in `dev/generate-kernelspec.py` which is not always true in C/C++. I'll take a look and try to fix it. Sorry for the inconvenience.\r\n> \r\nNo problem :-)\r\nThe work you've done is impressive! If it's formatting, I can always revert the last two commits and re-format only the auto-generated code. \r\n> One other thing I should point out is that in the latest commit to `master`, we introduced a new coding style requirement for the cpu-kernel include files - https://github.com/scikit-hep/awkward-1.0/blob/master/CONTRIBUTING.md#contributing-to-files-in-includeawkwardcpu-kernels. It would mostly involve copy-pasting some doxygen style comment strings above each function definition (here is the complete set of roles - https://github.com/scikit-hep/awkward-1.0/blob/master/kernel-specification/samples.json). Not having this _will_ cause the specification generator to break, after the issue you currently encountered is fixed. I can add those in while fixing the C++ -> C translator if you want.\r\n> \r\nThanks, I'll add these.\r\n> An unrelated comment - I was looking at the file changes in this PR, and noticed that clang-format causes a huge diff. Maybe we should use something like [git clang-format](https://github.com/llvm/llvm-project/blob/master/clang/tools/clang-format/git-clang-format) so that only the changes in the PR have formatting applied, to preserve git blame (and have a cleaner PR diff). The disadvantage of doing that would be that the file in question would not be uniformly formatted, so maybe there could be a different PR where we clang-format the entire codebase and require that all new code must be formatted according to our `clang-format` config file? What do you and @jpivarski think?\r\n\r\nEither is fine with me as long as I do not have to do it by hand :-)",
  "created_at":"2020-08-04T13:39:54Z",
  "id":668602801,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODYwMjgwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-04T13:39:54Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"#369 should fix the broken CI :) ",
  "created_at":"2020-08-04T19:27:46Z",
  "id":668781942,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODc4MTk0Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "hooray":1,
   "total_count":2
  },
  "updated_at":"2020-08-04T19:27:46Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, I'm done with this PR. Please, review it when you have time. Thanks. ",
  "created_at":"2020-08-05T14:46:16Z",
  "id":669236180,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTIzNjE4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-05T14:46:16Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks, @jpivarski ! I'll leave those things for a later PR and work on `concatenate`. Thanks.",
  "created_at":"2020-08-05T15:49:37Z",
  "id":669271798,
  "issue":346,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTI3MTc5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-05T15:49:37Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks @reikdas !",
  "created_at":"2020-07-23T11:48:24Z",
  "id":662961942,
  "issue":349,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2Mjk2MTk0Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2020-07-23T11:48:24Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think we may have an interesting use case with [sktime](https://github.com/alan-turing-institute/sktime), but not sure if that justifies the extra maintenance burden. \r\n\r\nWe want to represent a variety of time series formats, including univariate, multivariate, panel data, unequal length and unequally sampled time series data. We currently (ab)use pandas by storing entire time series as a `pd.Series` or `np.array` in the cells of a `pd.DataFrame`, but that makes it very awkward to use for most people, which brings me here! Awkward array basically meets all our requirements except two:\r\n\r\n1) Handling of time indices: we'd like to support not just sequences, but series consisting of (index, value) pairs where the value represents the observed value and the index the time points at which we observed the value;\r\n2) Handling of metadata (column names, etc).\r\n\r\nFor time series analysis, 1) seems important. 2) would be nice to have, but not essential. \r\n\r\n* Does awkward array support data/time indexing?\r\n* Are you aware of any other libraries similar to awkward array? We only know of [xarray](http://xarray.pydata.org/en/stable/) (but they don't support ragged arrays) apart from a few other smaller libraries. \r\n\r\nThis has gone slightly off-topic, please let me know if there's a better to place to discuss this! \r\n\r\nFor more info, see our condensed data container discussion [here](https://github.com/alan-turing-institute/sktime/wiki/Time-series-data-container).\r\n\r\ncc @fkiraly @prockenschaub @matteogales",
  "created_at":"2020-07-27T17:39:02Z",
  "id":664538065,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDUzODA2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T19:17:30Z",
  "user":"MDQ6VXNlcjIxMDIwNDgy"
 },
 {
  "author_association":"NONE",
  "body":"The above argument by @mloning is related to an earlier enquiry #289 . Unfortunately I hadn't had the time yet to make a deep dive into the options that @jpivarski laid out in #289 to represent time indices. However, even without time indices [this gist](https://gist.github.com/prockenschaub/4fe4cd0ac995fef42cdacbb342d9ca77) should illustrate what sktime is hoping to achieve by using awkwardarrays as pandas columns (for now assuming time is simply represented by position in the array). \r\n\r\nAs @mloning mentioned, if this is the only usecase it might not justify the extra maintenance burden on you. In this case, maybe there is an option to factor the awkardarray-as-extensionarry into a separate package?",
  "created_at":"2020-07-27T18:13:04Z",
  "id":664555906,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDU1NTkwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T18:13:04Z",
  "user":"MDQ6VXNlcjE1MzgxNzMy"
 },
 {
  "author_association":"NONE",
  "body":">As @mloning mentioned, if this is the only usecase it might not justify the extra maintenance burden on you. \r\n\r\nI slightly disagree with @prockenschaub and @mloning here, since I think that time series* are a pretty important use case, that to my knowledge none of the existing data container solutions is solving particularly well.\r\n\r\nWhile indeed it would put the maintenance burden on you (and not on us :smiley:), I\u00b4d see it as a potential solution to a long-standing annoyance - the eternal search for a great family of data containers for time series* - and therefore with potential to become a \"pillar of data science\"... \r\n\r\n*univariate, multivariate, panel data, unequal length and unequally sampled time series data, as @mloning says.",
  "created_at":"2020-07-27T18:19:14Z",
  "id":664559042,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDU1OTA0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T18:21:49Z",
  "user":"MDQ6VXNlcjc5ODU1MDI="
 },
 {
  "author_association":"NONE",
  "body":"so, where do I vote",
  "created_at":"2020-07-27T18:21:09Z",
  "id":664560082,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDU2MDA4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T18:21:09Z",
  "user":"MDQ6VXNlcjc5ODU1MDI="
 },
 {
  "author_association":"MEMBER",
  "body":"(This is the vote. It's informal. )\r\n\r\nI'm reading what you've written above and also logged into https://gitter.im/Scikit-HEP/awkward-array so we can chat in real time.",
  "created_at":"2020-07-27T18:34:09Z",
  "id":664566994,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDU2Njk5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T18:34:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"So far, I see three things that you need: (1) time-valued data, (2) data-valued index, and (3) complex data structures.\r\n\r\n(1) Pandas has always had good handling of time-valued data (from my perspective as someone who doesn't use time-valued data much).\r\n\r\n(2) The data-valued index is, I think, the thing that sets Pandas (1d) and xarray (nd) apart from NumPy (nd). This isn't a failure of NumPy, either: it's a lower level component that handles the data in the arrays, whereas Pandas and xarray are higher level components that manage what the data means through indexing. Awkward Array has been targeting that lower-level slot, too: it's designed to handle x-y data as two arrays, rather than a unified object like a Pandas Series.\r\n\r\nI've left a placeholder in the implementation called an array's `Identities`, based on some examples ([AwkwardQL](https://github.com/lgray/AwkwardQL)) that it would be useful to keep track of where each item came from for the sake of future joins. The `Identities` is underdeveloped, but with the understanding that it's a stub for future growth. What the `Identities` currently do is associate each quantity of an array with a unique integer\u2014if that unique integer is then associated with elements of a data array, that's a Pandas/xarray style index.\r\n\r\n(3) Awkward Array handles complex data structures in a unique way, which can't be done in a relational-like structure such as Pandas without multiple tables (DataFrames).\r\n\r\nSo the real issue here is that you want all three, you can get (1) and (2) from Pandas/xarray and (3) from Awkward, but not all together. Coming back to the original point of this thread, I'm not 100% sure that putting Awkward Arrays in Pandas DataFrames and Series will make that happen, since the data will be physically contained in those containers, but without operations that know how to use it, it's not much use.\r\n\r\nMoving conversation over to Gitter now...",
  "created_at":"2020-07-27T18:49:00Z",
  "id":664574209,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDU3NDIwOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-07-27T18:49:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"A key function in that example is `summarise_over_time`, which is more efficiently computed over a ragged array of many sublists than a NumPy reducer would be over _separate_ NumPy arrays (assuming that you have many arrays, as discussed on Gitter).",
  "created_at":"2020-07-27T20:20:39Z",
  "id":664617382,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDYxNzM4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T20:20:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"# Awkward arrays as Pandas columns will be deprecated.\r\n\r\nThe next release will present a deprecation warning when you try to use an Awkward array in Pandas (as a Series or a DataFrame column) and it will be removed in 0.3.0.\r\n\r\nThe [ak.pandas.df](https://awkward-array.readthedocs.io/en/latest/ak.pandas.df.html) and [ak.pandas.dfs](https://awkward-array.readthedocs.io/en/latest/ak.pandas.dfs.html) functions will be combined and renamed as [ak.to_pandas](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_pandas.html) for consistency. The new function name already exists and the old ones will be removed in 0.3.0.",
  "created_at":"2020-08-06T15:19:22Z",
  "id":669994088,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTk5NDA4OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-06T15:19:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The next release I deploy will be 0.3.0 and will not have the Awkward-as-Pandas-column feature.",
  "created_at":"2020-09-18T20:29:46Z",
  "id":695071223,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTA3MTIyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T20:29:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> But for the Awkward-in-Pandas, the only things I know of that can be used directly are ufuncs: [...] but not all ufuncs, for some Pandas reason:\r\n\r\nThe specific issue of `np.sqrt(dataframe)` failing for DataFrames with extension arrays comes down to DataFrame not defining `__array_ufunc__` yet. That's a known issue: https://github.com/pandas-dev/pandas/issues/23743 (I don't think anyone is working on it at the moment). But to your next point;\r\n\r\n> The fundamental problem is that Awkward objects are \"black boxes\" to Pandas. Sure, we can put them in a DataFrame, but what's Pandas going to do with them once they're there?\r\n\r\nThat's the essential motivation for ExtensionArrays: a way for pandas and these black boxes of arrays to interact through a well-defined interface. For example, [cyberpandas](https://cyberpandas.readthedocs.io/en/latest/) provides vectorized implementations of [ipaddress](https://docs.python.org/3/library/ipaddress.html) operations to pandas. pandas doesn't need to know about the memory layout of cyberpandas (a 2D int64 ndarray) or any IP operations for this to work.\r\n\r\nNow, the interface is relatively young. Some things work and some things (as you've discovered) don't. But it is improving with each release.\r\n\r\n> There are other downsides to making Awkward Arrays subclasses of `pandas.core.arrays.base.ExtensionArray`\r\n\r\nI personally wouldn't recommend making general-purpose objects like AwkwardArray try to implement pandas' Extension Array interface. As you note, there are some public methods that might clash with implementations in AwkwardArray. And I've never had good experiences making base classes dynamic. I'd instead recommend a dedicated object that implements the interface.\r\n\r\nThis raises some issues around putting `AwkwardArray` objects into a pandas DataFrame, if `AwkwardArray` doesn't implement the interface.\r\nI'm sure the pandas maintainers would be happy to discuss options there (like a `__pandas_extension_array__` interface that objects can\r\nimplement to return a pandas' extension array-compatible object. That would ensure that `pd.DataFrame({\"A\": my_awkward_array})` keeps the data as an awkward array, rather than copying to an object-dtype ndarray.\r\n\r\nAs general point though, the extension array interface is still evolving. If you run into issues please do speak up, either here or on the pandas issue tracker!",
  "created_at":"2020-10-05T18:40:36Z",
  "id":703815846,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMzgxNTg0Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":3,
   "total_count":3
  },
  "updated_at":"2020-10-05T18:40:36Z",
  "user":"MDQ6VXNlcjEzMTI1NDY="
 },
 {
  "author_association":"MEMBER",
  "body":"This issue was describing the problems involved in making Awkward arrays subclasses of the Pandas ExtensionArray, particularly as dynamic subclasses, and justifying the decision to drop this original design requirement. The difficulties encountered and gaps in usefulness once implemented are surmountable technical problems, but for the stability of the Awkward Array library, I had to remove the dynamic subclassing. In the future, it would be great if we could make Awkward arrays into Pandas columns through a loose coupling like `__pandas_extension_array__`, but it shouldn't be done the way it was before this issue was opened.\r\n\r\nIn particular, I'm interested in getting this to work on cuDF, which is introducing ListDtype and StructDtype into its data model and is backed by Apache Arrow. (See #359.) If these column types are not black boxes but something that is understood by the dataframe class, then it could make sense to introduce non-NumPy, non-Pandas functions like [ak.cartesian](https://awkward-array.readthedocs.io/en/latest/_auto/ak.cartesian.html) to the dataframe, implemented by Awkward. Showing that this is a usable interface, sensible for analysis, on a specialized dataframe like cuDF (which is only implemented for GPUs) would make a good argument for bringing it to Pandas and Dask DataFrame, showing that the changes required to make that work are justified. Or maybe by implementing it in cuDF, we might find that the first draft of an interface is wrong and needs to be tweaked.\r\n\r\nBy the way, the above is completely my own aspirations, not a formal plan. I've been talking with the cuDF developers on [their Slack](https://rapids-goai.slack.com/), but only in the sense of floating this idea.",
  "created_at":"2020-10-05T19:05:21Z",
  "id":703829402,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMzgyOTQwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-05T19:05:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, there should probably also be tests for something like this. You have great examples in the documentation\u2014just make them tests. Thanks!",
  "created_at":"2020-07-23T21:58:54Z",
  "id":663252919,
  "issue":354,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI1MjkxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T21:58:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If this is related to a corresponding change in Coffea (you mentioned \"migrating\"), then I could deploy it with a new minor version number, to make the dependency easier to specify.",
  "created_at":"2020-07-23T23:06:24Z",
  "id":663274193,
  "issue":354,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI3NDE5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T23:06:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I wonder if we can make these decorators attempt automatic lowering into numba?\r\nFor most properties and things like `abs` it should be straightforward, although I am not sure what it would look like for functions where you have to zip up into a new record. Is there a way to lower ak.zip so that it forms a record with the correct type when called from a numba-ized function?",
  "created_at":"2020-07-26T17:09:29Z",
  "id":664014839,
  "issue":355,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDAxNDgzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-26T17:09:29Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I've never attempted it, and it would take some thought. The Numba implementation works with scalars, which might be feasible for making new types of records (since they're metadata-only: no new indexes like `starts` and `stops`). Before solving the problem at the decorator level, it would have to be solved at the bare function level, where it's never been done before.\r\n\r\nIt runs counter to the general stance I've taken toward Numba: not to try to reimplement the whole environment you have outside of Numba, but for iterating with for loops as you shouldn't outside of Numba (so that inside and outside of Numba each have their strengths). There is an open issue to add `enumerate` to Awkward-Numba, and `zip` (the ordinary Python version) would also be important. These would iterate over Python-tuples-in-Numba, rather than Awkward records. That's probably closer to what a user would want, if they're using Numba as an imperative alternative to array-at-a-time interfaces.",
  "created_at":"2020-07-27T00:08:05Z",
  "id":664059025,
  "issue":355,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDA1OTAyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T00:08:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Arrow 1.0 does not implement any more of the Parquet types in [tests/test_0341-parquet-reader-writer.py::test_oamap_samples](https://github.com/scikit-hep/awkward-1.0/blob/816d203495c90db5a25cf7bbb64da72e2eb7b14c/tests/test_0341-parquet-reader-writer.py#L236-L246).",
  "created_at":"2020-07-24T23:01:57Z",
  "id":663766474,
  "issue":356,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzc2NjQ3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-24T23:03:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I have cleaned all the `none` tags from the AWS instance and I built the latest `awkward-cuda-test` images from this particular branch, which has all the residues from the other PR cleaned up. I'll build them once again(from master this time), once this PR gets merged.",
  "created_at":"2020-07-25T05:30:22Z",
  "id":663812662,
  "issue":357,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzgxMjY2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-25T12:01:01Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2020-07-27T00:08:47Z",
  "id":664059112,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDA1OTExMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T00:08:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That's awesome! Composable list and struct types within a DataFrame-like context would really help out physicists (among other data analysts, I'm sure), especially if cuDF can also run without GPUs. (The ability to access data types in this way is useful in itself, even if users don't have access to compatible GPUs on their Macs or in CERN's computing farm.)\r\n\r\nThis could also help the deprecation I'm considering in #350. Just getting nested data types into Pandas hasn't been useful in itself, since the Pandas API doesn't have operations that know how to make use of them. Presumably, if you're adding these types to the DataFrame in a non-opaque way, then you'll also be adding operations that use them\u2014for example, performing Cartesian products of nested lists or turning struct fields into columns and vice-versa.\r\n\r\nWill the buffer backing these new data types simply be an Arrow view? If so, then we can share more than Numba code; it would be possible to apply Awkward's array-at-a-time functions to columns of a cuDF in a similar way that NumPy's can be applied to a Pandas DataFrame. Awkward's own internal representation is more general than Arrow (e.g. [ListArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListArray.html) vs [ListOffsetArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html), a useful distinction when manipulating list structures), and it is zero-copy convertible when equivalent constructs are available ([ak.to_arrow](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_arrow.html) and [ak.from_arrow](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_arrow.html)).\r\n\r\nAs for the Numba code, here is where it is located: [src/awkward1/_connect/_numba](https://github.com/scikit-hep/awkward-1.0/tree/master/src/awkward1/_connect/_numba). The data model is based on Awkward's node types, not Arrow, and its primary focus is on lightweight iteration. For that reason, we don't have Numba models for each node type (e.g. ListArray, ListOffsetArray) because these things may be created and destroyed frequently during an iterative loop and Numba's models are by default pass-by-value (and it's not easy to make something pass-by-reference). Copying deep tree structures in every step of iteration would scale poorly.\r\n\r\nInstead, our Numba model is an ArrayView that walks over a Lookup data structure. The Lookup is a set of pointers to all the buffers in the original Awkward Array and the ArrayView represents a slice at some level of depth. The way to properly walk over the Lookup is enforced at compile-time, since each node type generates the appropriate code for `__getitem__`, but that type gets erased at runtime: there's only one type of ArrayView model.\r\n\r\nMy goal for Awkward-Numba-CUDA would be to reuse most of the infrastructure for Awkward-Numba (because it works) and replace the first level of iteration over a large array with the ability of users to write kernels on a single element of that large array. Walking over lists and structs deeper than the first level would be the same, even though it encourages users to write imperative code that might not be optimal on GPUs (e.g. users might write code with a lot of if-branches, but that would be their mistake to make).\r\n\r\nSeparating the Awkward-Numba part into a library of its own (whether CUDA-enabled or not) would be a little tricky, given how the ArrayView model was custom-written for Awkward Array types and not Arrow Arrays.\r\n\r\n   * One possibility would be to take the Awkward-Numba implementation as \"inspiration\" for cuDF.\r\n   * Another would be to view cuDF's data as Awkward Arrays so that it can use the same Numba models (I think Arrow-to-Awkward is _always_ zero-copy, but I'd have to check).\r\n   * Yet another would be to refactor the ArrayViews out into a library that only walks over Arrow types, though I wouldn't prefer this option because some of the Awkward types that are not Arrow types, such as our [ListArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListArray.html), would have to be non-zero-copy converted before being passed as an argument to a Numba-compiled function, _adding_ a performance penalty where there currently isn't one.\r\n\r\n",
  "created_at":"2020-07-27T22:31:01Z",
  "id":664671165,
  "issue":359,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NDY3MTE2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-27T22:31:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> That's awesome! Composable list and struct types within a DataFrame-like context would really help out physicists (among other data analysts, I'm sure), especially if cuDF can also run without GPUs. (The ability to access data types in this way is useful in itself, even if users don't have access to compatible GPUs on their Macs or in CERN's computing farm.)\r\n\r\ncuDF only runs on GPUs as of now and there's no plan / roadmap for running on CPUs at this time, but what @shwina proposed here is to make the Numba pieces GPU/CPU agnostic so everyone benefits. Ideally we could live in a world where Arrow, Awkward, cuDF, etc. can all reuse the same Numba extensions.\r\n\r\n> Will the buffer backing these new data types simply be an Arrow view? If so, then we can share more than Numba code; it would be possible to apply Awkward's array-at-a-time functions to columns of a cuDF in a similar way that NumPy's can be applied to a Pandas DataFrame. Awkward's own internal representation is more general than Arrow (e.g. [ListArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListArray.html) vs [ListOffsetArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html), a useful distinction when manipulating list structures), and it is zero-copy convertible when equivalent constructs are available ([ak.to_arrow](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_arrow.html) and [ak.from_arrow](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_arrow.html)).\r\n\r\nThe buffer backing cuDF columns are not Arrow views, but are our own `rmm::device_buffer` object (http://github.com/rapidsai/rmm). From my perspective though, we'd need to build some type of either abstract base class and/or data / API protocols that someone must implement / conform to in order to be able to utilize the Numba extension. I.E. something similar to Numpy's `__array_interface__` (https://numpy.org/doc/stable/reference/arrays.interface.html) or `__array_function__` (https://numpy.org/neps/nep-0018-array-function-protocol.html). Then anyone who exposes the necessary interface / layout can take advantage of the extension.\r\n\r\n> As for the Numba code, here is where it is located: [src/awkward1/_connect/_numba](https://github.com/scikit-hep/awkward-1.0/tree/master/src/awkward1/_connect/_numba). The data model is based on Awkward's node types, not Arrow, and its primary focus is on lightweight iteration. For that reason, we don't have Numba models for each node type (e.g. ListArray, ListOffsetArray) because these things may be created and destroyed frequently during an iterative loop and Numba's models are by default pass-by-value (and it's not easy to make something pass-by-reference). Copying deep tree structures in every step of iteration would scale poorly.\r\n> \r\n> Instead, our Numba model is an ArrayView that walks over a Lookup data structure. The Lookup is a set of pointers to all the buffers in the original Awkward Array and the ArrayView represents a slice at some level of depth. The way to properly walk over the Lookup is enforced at compile-time, since each node type generates the appropriate code for `__getitem__`, but that type gets erased at runtime: there's only one type of ArrayView model.\r\n> \r\n> My goal for Awkward-Numba-CUDA would be to reuse most of the infrastructure for Awkward-Numba (because it works) and replace the first level of iteration over a large array with the ability of users to write kernels on a single element of that large array. Walking over lists and structs deeper than the first level would be the same, even though it encourages users to write imperative code that might not be optimal on GPUs (e.g. users might write code with a lot of if-branches, but that would be their mistake to make).\r\n\r\nThe goal of reusing the Numba extension for GPU/CPU is shared among us. I think the new goal we're proposing here is figuring out how to reuse the Numba extension across different projects so we can all contribute to a single place and benefit from each others work.\r\n\r\n> Separating the Awkward-Numba part into a library of its own (whether CUDA-enabled or not) would be a little tricky, given how the ArrayView model was custom-written for Awkward Array types and not Arrow Arrays.\r\n> \r\n> * One possibility would be to take the Awkward-Numba implementation as \"inspiration\" for cuDF.\r\n> * Another would be to view cuDF's data as Awkward Arrays so that it can use the same Numba models (I think Arrow-to-Awkward is _always_ zero-copy, but I'd have to check).\r\n\r\nRolling our own implementation for cuDF is the fallback plan, but we have a vested interested in improving the GPU ecosystem \ud83d\ude04. Unfortunately UDFs are pretty important for us to support in cuDF and Awkward is a bit too heavy of a dependency for us to depend on for UDFs.\r\n\r\n> * Yet another would be to refactor the ArrayViews out into a library that only walks over Arrow types, though I wouldn't prefer this option because some of the Awkward types that are not Arrow types, such as our [ListArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListArray.html), would have to be non-zero-copy converted before being passed as an argument to a Numba-compiled function, _adding_ a performance penalty where there currently isn't one.\r\n\r\nI think this goes back into not locking us into specific containers to allow for ease of adoption, and instead using protocols and/or abstract classes to handle the Numba extensions.",
  "created_at":"2020-07-28T18:57:47Z",
  "id":665218115,
  "issue":359,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NTIxODExNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-28T18:57:47Z",
  "user":"MDQ6VXNlcjM2NjUxNjc="
 },
 {
  "author_association":"MEMBER",
  "body":"As I understand it, cuDF is getting a Numba extension now, but I won't be ready to do this for Awkward Array for months. I'll use a lot of the extensions @gmarkall is adding to Numba-CUDA to implement the Awkward one. The overlap is smaller than we had thought because cuDF's internal data model is strictly Arrow; Awkward Array's is not. As a generalization, there are additional features I have to implement for the Awkward one.\r\n\r\nOn the other hand, I'm still very interested in interoperability projects in the future!",
  "created_at":"2020-12-11T22:46:14Z",
  "id":743466129,
  "issue":359,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ2NjEyOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "heart":2,
   "total_count":4
  },
  "updated_at":"2020-12-11T22:46:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @ianna, any progress on this? (I'm asking because it's been 20 days; maybe you haven't pushed local changes.) Thanks!",
  "created_at":"2020-08-17T17:41:54Z",
  "id":675018333,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTAxODMzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T17:41:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"It would be cool to see this implemented, an axis=1 concatenate would be super slick for some analysis work",
  "created_at":"2020-10-23T20:05:49Z",
  "id":715565230,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNTU2NTIzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-23T20:05:49Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - yes, with `broadcast_and_apply` the arrays must be equal length. What's the best approach here? Padding them with `None`s and then dropping the `None`s after the `concatenate` operation works.\r\n```\r\n[[0.0, 1.1, 2.2], [], [3.3, 4.4], [5.5], [6.6, 7.7, 8.8, 9.9], None, None]\r\n[[1.1, 2.2], [3.3], [], [4.4, 5.5], [6.6, 7.7, 8.8], [], [9.9]]\r\n```\r\n",
  "created_at":"2020-11-10T16:43:30Z",
  "id":724823693,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNDgyMzY5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-10T16:43:30Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, users should not be allowed to concatenate arrays with `axis > 0` if their lengths differ. After all, if we want to concatenate\r\n\r\n```\r\n[[1, 2, 3], [], [4, 5]]   and   [[\"a\", \"b\"], [\"c\"], [\"d\", \"e\", \"f\"]]\r\n```\r\n\r\nat `axis=1`, that means we want\r\n\r\n```\r\n[[1, 2, 3, \"a\", \"b\"], [\"c\"], [4, 5, \"d\", \"e\", \"f\"]]\r\n```\r\n\r\nThis operation wouldn't make any sense if the two arrays we want to concatenate have different lengths.\r\n\r\nConcatenation at `axis=0` is qualitatively different: the input arrays can have any lengths at all. But for all other `axis` values, the lengths have to be the same. For `axis > 1`, the lengths have to be the same at several depths, but `broadcast_and_apply` ensures that as well, either through error messages or broadcasting, if possible.",
  "created_at":"2020-11-10T16:54:48Z",
  "id":724830455,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNDgzMDQ1NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-10T16:54:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```\r\ntests/test_0397-arrays-as-constants-in-numba.py::test_refcount FAILED\r\n```\r\nhttps://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=4560&view=logs&j=81547bd0-3aa9-547f-25e1-12509e21f92a&t=6f2c7081-a276-5c13-1bbf-f0639c0526de&l=509",
  "created_at":"2020-11-11T11:58:55Z",
  "id":725383097,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTM4MzA5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T11:58:55Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - please, have a look when you can. Thanks!",
  "created_at":"2020-11-11T14:16:12Z",
  "id":725447512,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTQ0NzUxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T14:16:12Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> ```\r\n> tests/test_0397-arrays-as-constants-in-numba.py::test_refcount FAILED\r\n> ```\r\n> \r\n> https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=4560&view=logs&j=81547bd0-3aa9-547f-25e1-12509e21f92a&t=6f2c7081-a276-5c13-1bbf-f0639c0526de&l=509\r\n\r\nThat one failure is disturbing, since it seems to have nothing to do with your changes. I'm rerunning it and running another test on master, though it's not necessarily a good sign if those pass. (Even worse than an inexplicable error is an intermittent inexplicable error.)",
  "created_at":"2020-11-11T15:07:42Z",
  "id":725476091,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTQ3NjA5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T15:07:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> > ```\r\n> > tests/test_0397-arrays-as-constants-in-numba.py::test_refcount FAILED\r\n> > ```\r\n> > \r\n> > \r\n> > https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=4560&view=logs&j=81547bd0-3aa9-547f-25e1-12509e21f92a&t=6f2c7081-a276-5c13-1bbf-f0639c0526de&l=509\r\n> \r\n> That one failure is disturbing, since it seems to have nothing to do with your changes. I'm rerunning it and running another test on master, though it's not necessarily a good sign if those pass. (Even worse than an inexplicable error is an intermittent inexplicable error.)\r\n\r\nA run on the master branch produced no errors: https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=4573&view=results while retrying this branch reproduced the error: https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=4571&view=logs&j=f72a8481-bd91-5d06-6025-0e453ddcac08&t=0b1b15ad-cf24-5193-0114-2bb1bf897da3, which makes me feel a little better because it's not intermittent. But it also means that somewhere in this PR, there's something that affects reference counting in Numba on Windows. (!?!) I'll pore through the diffs to see if anything looks suspicious.\r\n\r\nEdit: I didn't see anything in your branch that could have caused that. The specific error is that Numba takes references to Python objects when they're used in a closure. Normal Python functions like\r\n\r\n```python\r\ndef f2():\r\n    array, array\r\n    return 3.14\r\n```\r\n\r\ndon't take a reference to `array` because the closure does a fresh lookup in the surrounding scope (i.e. if you were to change `array` after having defined this function, the effect of the function would change). Numba functions, like\r\n\r\n```python\r\n@numba.jit\r\ndef f2():\r\n    array, array\r\n    return 3.14\r\n```\r\n\r\ntake references to `array` (2 of them) for each compilation of the function (i.e. whenever you call the function with different arguments, triggering a compilation). In this case, `array` is truly a constant: changing it in the surrounding scope doesn't change the function.\r\n\r\nThe test verifies that Numba is taking these references, increasing the Python reference count for `array`, but not dropping them when `f2` is removed. Strangely, the test shows that dropping `f1` removes the reference but dropping `f2` does not remove its two references. Even more strangely, this is only happening in Numba in Python 3.8 in Windows 32-bit.",
  "created_at":"2020-11-11T15:59:52Z",
  "id":725505925,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTUwNTkyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T16:19:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Is this branch up-to-date with master? Maybe something old is causing the Windows 32-bit Numba error.\r\n\r\nI've run tests on master twice with no such error, but it seems like it appears in every test on this branch. Even if its probability is 1/2, the always-on-this-branch and never-on-master is somewhat improbable.",
  "created_at":"2020-11-11T17:40:09Z",
  "id":725562383,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTU2MjM4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T17:40:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"#495 does some cross-checks on your kernel-specifications.yml, and that has been merged to master. If you update again, you'll get this cross-checker to see if your newly added kernels match specifications.",
  "created_at":"2020-11-11T18:43:41Z",
  "id":725594340,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTU5NDM0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T18:43:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - the branch is up-to-date. Judging by the tests - it looks like the error was introduced in the https://github.com/scikit-hep/awkward-1.0/pull/361/commits/74fb429799996cbe123b9715aec1550ce892af13 commit. I've reverted the changes to both `src/libawkward/Content.cpp` and `src/python/content.cpp`. It's unlikely that changes in the `tests/test_0184-concatenate-operation.py` caused it, so it must be due to `src/awkward1/operations/structure.py`.",
  "created_at":"2020-11-12T12:47:38Z",
  "id":726057007,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjA1NzAwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T12:47:38Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - The `tests/test_0397-arrays-as-constants-in-numba.py` is SKIPPED on the following architectures due to it's minimum requirement of Python version, I guess:\r\n\r\n- Windows py27-32bit\r\n- Windows py27-64bit\r\n- Windows py35-64bit\r\n- Windows py39-32bit\r\n- Windows py39-64bit\r\n- MacOS py27\r\n- MacOS py35\r\n- MacOS py39\r\n- Linux py27-np13\r\n- Linux py27-np16\r\n- Linux py35-np13\r\n- Linux py39-np*\r\n\r\nI have installed numba locally and when I run on a recent clean master, all the tests including the above one run fine:\r\n```\r\n% python localbuild.py --pytest tests \r\n...\r\n```\r\n\r\n```python\r\ntests/test_0397-arrays-as-constants-in-numba.py::test_refcount PASSED                                                                                                                                                                                [ 90%]\r\ntests/test_0397-arrays-as-constants-in-numba.py::test_Array PASSED                                                                                                                                                                                   [ 91%]\r\ntests/test_0397-arrays-as-constants-in-numba.py::test_Record PASSED                                                                                                                                                                                  [ 91%]\r\ntests/test_0397-arrays-as-constants-in-numba.py::test_ArrayBuilder PASSED                                                                                                                                                                            [ 91%]\r\n\r\n```\r\n\r\nHowever, if I try to rerun the test, all four fail. Am I missing something?\r\n\r\n```python\r\n% python localbuild.py --pytest tests/test_0397-arrays-as-constants-in-numba.py\r\ncmake --build localbuild -- -j12\r\n[ 21%] Built target awkward-objects\r\n[ 90%] Built target awkward-cpu-kernels-objects\r\n[ 91%] Built target awkward-cpu-kernels-static\r\n[ 91%] Built target awkward-cpu-kernels\r\n[ 91%] Built target awkward-static\r\n[ 92%] Built target awkward\r\n[ 93%] Built target test0030\r\n[ 94%] Built target test0019\r\n[ 95%] Built target test0016\r\n[ 96%] Built target test0074\r\n[100%] Built target _ext\r\npython -m pytest -vv -rs tests/test_0397-arrays-as-constants-in-numba.py\r\n=================================================================================================================== test session starts ====================================================================================================================\r\nplatform darwin -- Python 3.8.1, pytest-5.3.5, py-1.8.1, pluggy-0.13.1 -- /Users/yana/.pyenv/versions/3.8.1/bin/python\r\ncachedir: .pytest_cache\r\nrootdir: /Users/yana/Projects/PR184-rebase-with-numba/awkward-1.0, inifile: setup.cfg\r\ncollected 4 items                                                                                                                                                                                                                                          \r\n\r\ntests/test_0397-arrays-as-constants-in-numba.py::test_refcount FAILED                                                                                                                                                                                [ 25%]\r\ntests/test_0397-arrays-as-constants-in-numba.py::test_Array FAILED                                                                                                                                                                                   [ 50%]\r\ntests/test_0397-arrays-as-constants-in-numba.py::test_Record FAILED                                                                                                                                                                                  [ 75%]\r\ntests/test_0397-arrays-as-constants-in-numba.py::test_ArrayBuilder FAILED                                                                                                                                                                            [100%]\r\n\r\n========================================================================================================================= FAILURES =========================================================================================================================\r\n______________________________________________________________________________________________________________________ test_refcount _______________________________________________________________________________________________________________________\r\n\r\n    def test_refcount():\r\n        array = awkward1.Array([1, 2, 3])\r\n    \r\n        @numba.njit\r\n        def f1():\r\n            array\r\n            return 3.14\r\n    \r\n        @numba.njit\r\n        def f2():\r\n            array, array\r\n            return 3.14\r\n    \r\n        assert sys.getrefcount(array) == 2\r\n>       f1()\r\n\r\ntests/test_0397-arrays-as-constants-in-numba.py:29: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/dispatcher.py:415: in _compile_for_args\r\n    error_rewrite(e, 'typing')\r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/dispatcher.py:358: in error_rewrite\r\n    reraise(type(e), e, None)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ntp = <class 'numba.core.errors.TypingError'>\r\nvalue = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\\nUntyped global name \\'array\\': cannot determi...ray\\'>\\n\\nFile \"tests/test_0397-arrays-as-constants-in-numba.py\", line 20:\\n    def f1():\\n        array\\n        ^\\n')\r\ntb = None\r\n\r\n    def reraise(tp, value, tb=None):\r\n        if value is None:\r\n            value = tp()\r\n        if value.__traceback__ is not tb:\r\n>           raise value.with_traceback(tb)\r\nE           numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)\r\nE           Untyped global name 'array': cannot determine Numba type of <class 'awkward1.highlevel.Array'>\r\nE           \r\nE           File \"tests/test_0397-arrays-as-constants-in-numba.py\", line 20:\r\nE               def f1():\r\nE                   array\r\nE                   ^\r\n\r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/utils.py:80: TypingError\r\n________________________________________________________________________________________________________________________ test_Array ________________________________________________________________________________________________________________________\r\n\r\n    def test_Array():\r\n        array = awkward1.Array([1, 2, 3])\r\n    \r\n        @numba.njit\r\n        def f1():\r\n            array\r\n            return 3.14\r\n    \r\n>       f1()\r\n\r\ntests/test_0397-arrays-as-constants-in-numba.py:48: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/dispatcher.py:415: in _compile_for_args\r\n    error_rewrite(e, 'typing')\r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/dispatcher.py:358: in error_rewrite\r\n    reraise(type(e), e, None)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ntp = <class 'numba.core.errors.TypingError'>\r\nvalue = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\\nUntyped global name \\'array\\': cannot determi...ray\\'>\\n\\nFile \"tests/test_0397-arrays-as-constants-in-numba.py\", line 45:\\n    def f1():\\n        array\\n        ^\\n')\r\ntb = None\r\n\r\n    def reraise(tp, value, tb=None):\r\n        if value is None:\r\n            value = tp()\r\n        if value.__traceback__ is not tb:\r\n>           raise value.with_traceback(tb)\r\nE           numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)\r\nE           Untyped global name 'array': cannot determine Numba type of <class 'awkward1.highlevel.Array'>\r\nE           \r\nE           File \"tests/test_0397-arrays-as-constants-in-numba.py\", line 45:\r\nE               def f1():\r\nE                   array\r\nE                   ^\r\n\r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/utils.py:80: TypingError\r\n_______________________________________________________________________________________________________________________ test_Record ________________________________________________________________________________________________________________________\r\n\r\n    def test_Record():\r\n        record = awkward1.Record({\"x\": 1, \"y\": [1, 2, 3]})\r\n    \r\n        @numba.njit\r\n        def f1():\r\n            return record.y[1]\r\n    \r\n>       assert f1() == 2\r\n\r\ntests/test_0397-arrays-as-constants-in-numba.py:92: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/dispatcher.py:415: in _compile_for_args\r\n    error_rewrite(e, 'typing')\r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/dispatcher.py:358: in error_rewrite\r\n    reraise(type(e), e, None)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ntp = <class 'numba.core.errors.TypingError'>\r\nvalue = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\\nUntyped global name \\'record\\': cannot determ...e \"tests/test_0397-arrays-as-constants-in-numba.py\", line 90:\\n    def f1():\\n        return record.y[1]\\n        ^\\n')\r\ntb = None\r\n\r\n    def reraise(tp, value, tb=None):\r\n        if value is None:\r\n            value = tp()\r\n        if value.__traceback__ is not tb:\r\n>           raise value.with_traceback(tb)\r\nE           numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)\r\nE           Untyped global name 'record': cannot determine Numba type of <class 'awkward1.highlevel.Record'>\r\nE           \r\nE           File \"tests/test_0397-arrays-as-constants-in-numba.py\", line 90:\r\nE               def f1():\r\nE                   return record.y[1]\r\nE                   ^\r\n\r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/utils.py:80: TypingError\r\n____________________________________________________________________________________________________________________ test_ArrayBuilder _____________________________________________________________________________________________________________________\r\n\r\n    def test_ArrayBuilder():\r\n        builder = awkward1.ArrayBuilder()\r\n        assert sys.getrefcount(builder._layout) == 3\r\n    \r\n        @numba.njit\r\n        def f():\r\n            builder.append(1)\r\n            builder.append(2)\r\n            builder.append(3)\r\n            return builder, builder\r\n    \r\n        @numba.njit\r\n        def g():\r\n            builder.append(1)\r\n            builder.append(2)\r\n            builder.append(3)\r\n    \r\n>       b, c = f()\r\n\r\ntests/test_0397-arrays-as-constants-in-numba.py:112: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/dispatcher.py:415: in _compile_for_args\r\n    error_rewrite(e, 'typing')\r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/dispatcher.py:358: in error_rewrite\r\n    reraise(type(e), e, None)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ntp = <class 'numba.core.errors.TypingError'>\r\nvalue = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\\nUntyped global name \\'builder\\': cannot deter...le \"tests/test_0397-arrays-as-constants-in-numba.py\", line 101:\\n    def f():\\n        builder.append(1)\\n        ^\\n')\r\ntb = None\r\n\r\n    def reraise(tp, value, tb=None):\r\n        if value is None:\r\n            value = tp()\r\n        if value.__traceback__ is not tb:\r\n>           raise value.with_traceback(tb)\r\nE           numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)\r\nE           Untyped global name 'builder': cannot determine Numba type of <class 'awkward1.highlevel.ArrayBuilder'>\r\nE           \r\nE           File \"tests/test_0397-arrays-as-constants-in-numba.py\", line 101:\r\nE               def f():\r\nE                   builder.append(1)\r\nE                   ^\r\n\r\n../../../.pyenv/versions/3.8.1/lib/python3.8/site-packages/numba/core/utils.py:80: TypingError\r\n==================================================================================================================== 4 failed in 1.17s =====================================================================================================================\r\nTraceback (most recent call last):\r\n  File \"localbuild.py\", line 131, in <module>\r\n    check_call([\"python\", \"-m\", \"pytest\", \"-vv\", \"-rs\", args.pytest], env=env)\r\n  File \"localbuild.py\", line 56, in check_call\r\n    return subprocess.check_call(args, env=env)\r\n  File \"/Users/yana/.pyenv/versions/3.8.1/lib/python3.8/subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['python', '-m', 'pytest', '-vv', '-rs', 'tests/test_0397-arrays-as-constants-in-numba.py']' returned non-zero exit status 1.\r\n\r\n```",
  "created_at":"2020-11-12T17:04:10Z",
  "id":726208719,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjIwODcxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T17:05:55Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right that the restrictions on testing with Numba are based on Python version and platform constraints. Numba doesn't support Python 2 anymore (our minimal Numba version doesn't), and it doesn't support Python 3.9 _yet_. Supporting a new version of Python is difficult for Numba because it directly uses the Python bytecode, which changes from one minor version to the next.\r\n\r\nFor these particular errors, it looks like Awkward's Numba extensions have not been registered. This happens through Python's \"entry points\" mechanism, which might only be enabled with a full package installation. That is, you'd need to install Awkward with pip or conda (it can be `pip install .` in your `awkward-1.0` repo directory) just to get the entry point registered, then the version in the `localbuild` directory would override it if you invoke Python from `awkward-1.0`, as you have been.\r\n\r\nIf that makes the difference, I'll make a note of it in the CONTRIBUTING.md.",
  "created_at":"2020-11-12T17:25:02Z",
  "id":726221238,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjIyMTIzOA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-12T17:25:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ha. All checks passed ;-)",
  "created_at":"2020-11-14T11:05:02Z",
  "id":727189117,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzE4OTExNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-14T11:05:02Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The warning is a good thing\u2014I've deprecated a feature and wanted to make sure that users will see that message, so I added it to the tests. It's also a reminder to remove the feature in version 1.0.\r\n\r\nI was going to suggest bisecting to find what's causing the Windows bug, but you beat me to it.\r\n\r\nIt would be bad if the Windows bug only happens because the garbage collector runs\u2014delaying the garbage collector is not a fix, it only indicates that the bug is hidden in other systems. But I've already checked for this bug by explicitly invoking the garbage collector (on Linux). I don't think we need to worry about that possibility.",
  "created_at":"2020-11-14T13:30:32Z",
  "id":727207627,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzIwNzYyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-14T13:30:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> The warning is a good thing\u2014I've deprecated a feature and wanted to make sure that users will see that message, so I added it to the tests. It's also a reminder to remove the feature in version 1.0.\r\n> \r\n> I was going to suggest bisecting to find what's causing the Windows bug, but you beat me to it.\r\n> \r\n> It would be bad if the Windows bug only happens because the garbage collector runs\u2014delaying the garbage collector is not a fix, it only indicates that the bug is hidden in other systems. But I've already checked for this bug by explicitly invoking the garbage collector (on Linux). I don't think we need to worry about that possibility.\r\n\r\nThe `numbers` and `records` are not `mergeable`. They can be merged as a union in `axis=0` whilst in `axis>0` `broadcast_and_apply` will not accept this union due to its different length from the other arrays.",
  "created_at":"2020-11-14T13:46:34Z",
  "id":727209551,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzIwOTU1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-14T13:46:34Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The `numbers` and `records` are intentionally not mergeable (except \"as union\"). But the second set, which has the same lengths, should be concatenateable with `axis > 0` (\"as union\").",
  "created_at":"2020-11-14T15:07:10Z",
  "id":727220236,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzIyMDIzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-14T15:07:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@reikdas - could you, please, have a look with a fresh eye? Why and where does the PR defines `tests-cpu-kernels/test_cpuawkward_RegularArray_merge_offsets64.py`? I think, I've deleted it a while back. Thanks!",
  "created_at":"2020-11-15T12:39:52Z",
  "id":727563608,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzU2MzYwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-15T12:39:52Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @reikdas - could you, please, have a look with a fresh eye? Why and where does the PR defines `tests-cpu-kernels/test_cpuawkward_RegularArray_merge_offsets64.py`? I think, I've deleted it a while back. Thanks!\r\n\r\nI see that there are some commits after this comment and that tests are passing. Is the issue fixed? ",
  "created_at":"2020-11-15T16:02:13Z",
  "id":727593170,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzU5MzE3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-15T16:02:13Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > @reikdas - could you, please, have a look with a fresh eye? Why and where does the PR defines `tests-cpu-kernels/test_cpuawkward_RegularArray_merge_offsets64.py`? I think, I've deleted it a while back. Thanks!\r\n> \r\n> I see that there are some commits after this comment and that tests are passing. Is the issue fixed?\r\n\r\nNo, I just squashed the last three commits. I've no idea why it all works now. It was failing at a test that did not have any kernel code defined and it still fails in my local area:\r\n```python\r\n========================================================================================================================= FAILURES =========================================================================================================================\r\n______________________________________________________________________________________________________ test_cpuawkward_RegularArray_merge_offsets64_1 ______________________________________________________________________________________________________\r\n\r\n    def test_cpuawkward_RegularArray_merge_offsets64_1():\r\n        tooffsets = [123, 123, 123]\r\n        tooffsets = (ctypes.c_int64*len(tooffsets))(*tooffsets)\r\n        tolength = 3\r\n        length = 3\r\n        size = 3\r\n        otherlength = 3\r\n        othersize = 3\r\n>       funcC = getattr(lib, 'awkward_RegularArray_merge_offsets64')\r\n\r\ntests-cpu-kernels/test_cpuawkward_RegularArray_merge_offsets64.py:13: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../../.pyenv/versions/3.8.1/lib/python3.8/ctypes/__init__.py:386: in __getattr__\r\n    func = self.__getitem__(name)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <CDLL '/Users/yana/Projects/PR184-rebase/awkward-1.0/awkward1/libawkward-cpu-kernels.dylib', handle 7f93a8021720 at 0x1110188b0>, name_or_ordinal = 'awkward_RegularArray_merge_offsets64'\r\n\r\n    def __getitem__(self, name_or_ordinal):\r\n>       func = self._FuncPtr((name_or_ordinal, self))\r\nE       AttributeError: dlsym(0x7f93a8021720, awkward_RegularArray_merge_offsets64): symbol not found\r\n\r\n../../../.pyenv/versions/3.8.1/lib/python3.8/ctypes/__init__.py:391: AttributeError\r\n```\r\nI think, I will start a new branch.",
  "created_at":"2020-11-15T16:07:00Z",
  "id":727593885,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzU5Mzg4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-15T16:07:00Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"move to https://github.com/scikit-hep/awkward-1.0/pull/539",
  "created_at":"2020-11-15T17:47:49Z",
  "id":727608729,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzYwODcyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-15T17:47:49Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I checked out your branch locally, but I I wasn't able to reproduce the error. Maybe there were some auto generated files that were not removed? \r\n(Even if that is not the culprit, @jpivarski maybe we should include a script in `dev/` to clean up all auto generated files, apart from the ones that are checked into git like the kernel headers, so that one can start from a clean slate?)",
  "created_at":"2020-11-15T18:58:47Z",
  "id":727618950,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzYxODk1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-15T19:05:52Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> (Even if that is not the culprit, @jpivarski maybe we should include a script in `dev/` to clean up all auto generated files, apart from the ones that are checked into git like the kernel headers, so that one can start from a clean slate?)\r\n\r\nThat's a good idea, though these generated files are not in the `tests` directory, which is where the failing test was.",
  "created_at":"2020-11-16T14:18:19Z",
  "id":728090573,
  "issue":361,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODA5MDU3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T14:18:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"So I called the function `test_cupy_refcount` and used the linux command `time python tests-cuda/test_cupy_refcount.py` and here's the result. `python tests-cuda/test_cupy_refcount.py  0.53s user 0.24s system 98% cpu 0.776 total`. That 30s seems a bit too much, I think it might be because the CUDA syncs with the GPU when we first run any cuda program but still 30s is a very big number.\r\nAs far as testing the `Index.from_cupy()` is concerned. I forgot to add the `len` and `getitem` tests but you can test it out on the Python prompt and do stuff like `array[3]` or `len(array)` and it works.\r\n",
  "created_at":"2020-08-01T00:48:53Z",
  "id":667440900,
  "issue":362,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NzQ0MDkwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-01T00:48:53Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Also, since we are doing some interpreter level things with `pybind11`, I think it has to do that Initialization call 9 times in a row instead of just one time and that might(I am not sure at all about this) be the reason why that 30s is even happening. But as I am wrting this I realized that there's only one place where `cupy` really allocates an array and all the other awkward arrays are just derived from it. So as a hypothesis maybe that time is `Time taken in the CuPy import` which include the `CudaDeviceInitialization` that might happen when we import `CuPy`. Anyways, I'll have to research both on `pybind11` and `CuPy` to see if this is the actual issue. I hope I can resume work from Tuesday and I'll start looking into this issue.",
  "created_at":"2020-08-01T01:06:50Z",
  "id":667443780,
  "issue":362,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NzQ0Mzc4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-01T01:06:50Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"If you mean the `py::module::import(\"cupy\")`, Python modules can be imported arbitrarily many times without a penalty. The \"initialize once; all subsequent times are a pass-through\" is built into Python's `import` (which can make it difficult if you actually _do_ want to re-initialize, but that's not our case here).\r\n\r\nYou can diagnose it by putting print statements between every step\u2014one of them will be a big one.",
  "created_at":"2020-08-01T03:40:29Z",
  "id":667462570,
  "issue":362,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NzQ2MjU3MA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-01T03:40:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Okay, maybe the bottleneck is somewhere else. I'll try figuring it out.",
  "created_at":"2020-08-01T03:46:51Z",
  "id":667463140,
  "issue":362,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NzQ2MzE0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-01T03:46:51Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"I'm confused about which PR is which. If this is the one that was fixed earlier today with a clean environment, then let's get it up to date (5 files conflict) and I'll merge it. If this is not supposed to be merged, then let's close it. Thanks!",
  "created_at":"2020-08-17T17:40:48Z",
  "id":675017818,
  "issue":362,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTAxNzgxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T17:40:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've lost track of what changes I requested, so I've marked them as approved. But what's the status of this PR, and how does it fit in with #372?",
  "created_at":"2020-08-17T18:43:43Z",
  "id":675046897,
  "issue":362,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTA0Njg5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T18:43:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Can I close this PR, this was intended to be merged a lot earlier, and since then I have moved on to the `Numpy` and `Identites` integration, and that has a lot of different changes within the C++ layer. I'll try to resolve that `to_list` segmentation fault and get the PR to a completion state as soon as possible.",
  "created_at":"2020-08-18T04:12:43Z",
  "id":675238228,
  "issue":362,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTIzODIyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T04:12:43Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"You can close this.\r\n\r\nDoes its content exist elsewhere? The title of this PR says that it's introducing the `from_cupy` method to Index\u2014does that happen in a later PR? (I want to make sure each feature gets implemented exactly once!)",
  "created_at":"2020-08-18T13:37:42Z",
  "id":675483709,
  "issue":362,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTQ4MzcwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T13:37:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, the Index integration with CuPy exists in #372 PR. So that PR can be treated as one CuPy Integration PR, except the `high level` `to_cupy` and `from_cupy`, because that has some issues for which you might want to look at Slack, where I detailed them out.",
  "created_at":"2020-08-18T13:46:51Z",
  "id":675488538,
  "issue":362,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTQ4ODUzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T13:46:51Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"I just test-ran the Sphinx and Jupyter-Book documentation locally; it all looks good.",
  "created_at":"2020-07-31T14:59:22Z",
  "id":667164712,
  "issue":363,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NzE2NDcxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-31T14:59:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This looks like it does the right thing and the tests pass. I'll take your word for it that the it's modifying the tests-kernels that get generated and they are running in the tests that pass. Give me a thumbs-up on that and I'll merge it.",
  "created_at":"2020-08-04T11:39:11Z",
  "id":668545087,
  "issue":365,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODU0NTA4Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-04T11:39:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is actually waiting on a pybind11 feature: it currently can't ingest datetime64 (`'M'`) or timedelta64 (`'m'`) arrays as `py::buffer_info`. I think that's being handled in pybind/pybind11#2078 or pybind/pybind11#2085 or both, I'm not sure. (Issue pybind/pybind11#1970 might also be related.)\r\n\r\nEither way, we have two dtype enums ready and waiting:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/46767d776dcdc825621f8151dbd4289225b09659/include/awkward/util.h#L26-L47\r\n\r\nThe bigger question, though, is what reducers would mean for these types. [ak.sum](https://awkward-array.readthedocs.io/en/latest/_auto/ak.sum.html) could make sense for timedelta64, but not datetime64, [ak.prod](https://awkward-array.readthedocs.io/en/latest/_auto/ak.prod.html) wouldn't make sense for either, though [ak.min](https://awkward-array.readthedocs.io/en/latest/_auto/ak.min.html) and [ak.max](https://awkward-array.readthedocs.io/en/latest/_auto/ak.max.html) could be extended for either. (Maybe the datetime64 could reuse the uint64 min/max and the timedelta64 could reuse the int64 min/max...)\r\n\r\nAll the type coersions when concatenating non-temporal and temporal numbers would also have to be added, which is a lot of boilerplate, but straightforward.",
  "created_at":"2020-08-04T17:08:54Z",
  "id":668717241,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODcxNzI0MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2020-08-04T17:08:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's actually not a pybind11 error, it's a NumPy error:\r\n\r\n```python\r\n>>> memoryview(np.array([\"2018-01-01\", \"2019-01-01\", \"2020-01-01\"], \"datetime64[s]\"))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nValueError: cannot include dtype 'M' in a buffer\r\n```\r\n\r\nand that's why we get an error here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/c41360532ed0e36660fcc918965f49451d1d3677/src/python/content.cpp#L1674\r\n\r\nTo work around this, we'd have to detect it with `array.dtype().kind() == 'M'` and then get the data pointer, shape, strides, itemsize, and mock up a format manually using the `py::array` only, with no help from `py::buffer_info`.\r\n\r\nIn the `py::array` class,\r\n\r\nhttps://github.com/pybind/pybind11/blob/6f3470f757cc5162d5f9115ea9e280e071c212fa/include/pybind11/numpy.h#L543-L834\r\n\r\nI see everything that we need except the pointer (have to `.attr(\"ctypes\").attr(\"data\")` to get a Python integer of the pointer, then unbox that as a `size_t`, then cast that as a `void*`) and the format string. There is no equivalent of the format string, but I could do `py::str(array.dtype())` and cast that as a `std::string` to get something like `\"datetime64[s]\"`, which is not a format, but combined with the `util::dtype` enum telling me it's a `datetime64`, put in special logic to construct the appropriate Python dtype when turning it back into a NumPy array on the other side. It can't go through the buffer interface, but I should be changing the Python to call `to_numpy()` as opposed to `to_cupy()` to accommodate GPU arrays, anyway.",
  "created_at":"2020-08-11T00:49:57Z",
  "id":671661174,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTY2MTE3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T00:49:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'd like to thank you, @YannickJadoul, for your help in the above. Feel free to \"unwatch\" this thread, since giving you credit also pulled you into the GitHub issue as a side-effect.",
  "created_at":"2020-08-11T00:53:35Z",
  "id":671662116,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTY2MjExNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-11T00:53:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hello, is there an ETA on fixing this? I was hoping it would land in 1.0.0.\r\n\r\nThe underlying numpy issue is 6 years old: <https://github.com/numpy/numpy/issues/4983>.\r\n\r\nI'd have a go, except this is probably not the best \"first issue\" for someone new to Awkward!",
  "created_at":"2020-12-08T01:34:42Z",
  "id":740302209,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDMwMjIwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T01:34:42Z",
  "user":"MDQ6VXNlcjI4MzA0NTk="
 },
 {
  "author_association":"MEMBER",
  "body":"There isn't an ETA, though I've been eyeing it as a medium-priority item. It's on my increasingly-misnamed [November bug-fixes](https://github.com/scikit-hep/awkward-1.0/projects/2) project, one of the (currently 3) C++ tasks; the majority are Python tasks and much lower-hanging fruit.\r\n\r\nIf you're interested in working on this, I can help. In particular, I can give background and status of the problem, answer questions on a PR (open a draft PR early so we can talk), and even on Zoom/other chat for \"broader bandwidth\" clarifications and context.\r\n\r\nHere's some initial background to the status:\r\n\r\n   * Since there are many NumPy versions out there already whose datetime dtypes don't support the buffer protocol, an Awkward implementation of this will have to bypass the buffer protocol. This won't be entirely novel, since CuPy arrays are already handled as a special case in [the constructor](https://github.com/scikit-hep/awkward-1.0/blob/2f36142f0373184040e48c4cad03a09a28f56b56/src/python/content.cpp#L1890-L1927) and a [method for output](https://github.com/scikit-hep/awkward-1.0/blob/2f36142f0373184040e48c4cad03a09a28f56b56/src/python/content.cpp#L1965-L1973), and NumPy arrays with datetime type would have to be handled something like that. It means that `np.asarray(numpyarray)` won't work on these [NumpyArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.NumpyArray.html) instances, but this isn't an issue for the high-level user interface. The high-level interface is an [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html) class that wraps these NumpyArrays (and other [layouts](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#ak-array-layout)).\r\n   * Datetime types are already stubbed out everywhere as [enum values](https://github.com/scikit-hep/awkward-1.0/blob/2f36142f0373184040e48c4cad03a09a28f56b56/include/awkward/util.h#L44-L45) that have been commented out. So for instance, [searching the repo for \"datetime64\"](https://github.com/scikit-hep/awkward-1.0/search?q=datetime64) reveals all the places where a change is needed.\r\n   * Most of the operations you might do on specialty types are not actually performed by Awkward Array. All ufuncs, for instance, which includes implementations of operator overloads like `__add__`, `__mul__`, etc., are passed to NumPy (between unwrapping and rewrapping the one-dimensional NumPy array buried inside the Awkward layout). So most of the things that would be different for date-time types, such as the fact that the difference of two datetime64 arrays is a timedelta64 array, do not need any explicit code in Awkward Array.\r\n   * The exception to the above is reducers: (see the base case that all reducers eventually reach, [NumpyArray::reduce_next](https://github.com/scikit-hep/awkward-1.0/blob/2f36142f0373184040e48c4cad03a09a28f56b56/src/libawkward/array/NumpyArray.cpp#L2647-L2819)). The [ReducerSum](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1ReducerSum.html) class, which implements [ak.sum](https://awkward-array.readthedocs.io/en/latest/_auto/ak.sum.html), for instance, needs to know how to add elements of an array and hence it needs to know what types they are. But among [all the reducers](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1Reducer.html), only [ak.count](https://awkward-array.readthedocs.io/en/latest/_auto/ak.count.html), [ak.count_nonzero](https://awkward-array.readthedocs.io/en/latest/_auto/ak.count_nonzero.html), [ak.min](https://awkward-array.readthedocs.io/en/latest/_auto/ak.min.html), [ak.max](https://awkward-array.readthedocs.io/en/latest/_auto/ak.max.html), [ak.argmin](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argmin.html), and [ak.argmax](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argmax.html) could be used on data of date-time type, and possibly [ak.any](https://awkward-array.readthedocs.io/en/latest/_auto/ak.any.html) and [ak.all](https://awkward-array.readthedocs.io/en/latest/_auto/ak.all.html) if we consider `0` to be `false` and any other value to be `true`. Maybe you can [ak.sum](https://awkward-array.readthedocs.io/en/latest/_auto/ak.sum.html) `timedelta64`, but not `datetime64`. ALL of these would be implemented by view-casting the data as `int64_t` anyway, so there isn't any _computation_ to do, just a proper accounting of types before and after the reduction.\r\n   * The NumpyArray class has [somewhat redundant](https://github.com/scikit-hep/awkward-1.0/blob/2f36142f0373184040e48c4cad03a09a28f56b56/include/awkward/array/NumpyArray.h#L798-L803) dtype information: `itemsize` is the number of bytes in each item, `format` is the string returned by Python's buffer protocol, and `dtype` is [our own enum](https://github.com/scikit-hep/awkward-1.0/blob/2f36142f0373184040e48c4cad03a09a28f56b56/include/awkward/util.h#L26-L47), added later to reign in the madness. Part of this \"madness\" is the [platform-dependence of `format`](https://github.com/scikit-hep/awkward-1.0/blob/2f36142f0373184040e48c4cad03a09a28f56b56/src/libawkward/util.cpp#L224-L282). On the bright side, that means there's plenty of room to add all the metadata needed to track date-time types through the system. I know that date-times have units, and perhaps these can be put in the `format` string, since `format` won't be useful when the NumPy array doesn't pass through the buffer protocol. (I wish I hadn't ever used the buffer protocol\u2014pybind11 made it look like the right way to do this, but there are exceptions like CuPy and date-times that have to work around it.) Since NumPy+Python might start representing date-time types in the `format` string someday and we don't know what convention they'll choose, perhaps this `format` string could be prepended by `\"ak\"` or something? (It will always be in conjunction with our `dtype` enum being equal to `datetime64` or `timedelta64`, so maybe that kind of future-proofing is unnecessary.)\r\n\r\nThe problem of adding date-time dtypes is similar to the problem of adding complex dtypes, which is PR #421, temporarily stopped, but @sjperkins intends to pick it up again. The difference with that case is that Python's buffer protocol does support complex numbers (`format` strings that start with `\"Z\"`) and reducers like [ak.sum](https://awkward-array.readthedocs.io/en/latest/_auto/ak.sum.html) and [ak.prod](https://awkward-array.readthedocs.io/en/latest/_auto/ak.prod.html) need special _calculations_ for complex types, but date-time types can just be passed through these calculations as though they were `int64_t`. So they're similar problems, but each is easier/harder in different ways.",
  "created_at":"2020-12-08T15:51:31Z",
  "id":740704342,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDcwNDM0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T15:51:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the extended write up, @jpivarski. I'll dive into the code this weekend and see if I can make some progress. It feels a lot to assimilate for the time I have, so I won't promise a solution. \r\n\r\nOn the positive side, I am highly motivated to try Awkward for a large scale problem (processing 3m Arrow tables of 1k-1m rows, each with time histories of some related business entities). Time is an intrinsic part of the processing I need to do on this dataset.",
  "created_at":"2020-12-08T21:52:55Z",
  "id":741092610,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MTA5MjYxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T21:52:55Z",
  "user":"MDQ6VXNlcjI4MzA0NTk="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @stevesimmons! Let me know if you're willing and able to do this (date-time types in Awkward Array).\r\n\r\nIf not, I'll move it out of \"in progress,\" but it will be a high priority item for me. There's evidently a lot of interest.\r\n\r\nThanks!",
  "created_at":"2020-12-22T19:55:43Z",
  "id":749746818,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0OTc0NjgxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-22T19:55:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski \r\nFirst off all, I'd like to thank you and your co-creators for Awkward Array \u2013 it's so nice to have a tool like this to handle... well, awkward and ragged data structures!\r\n\r\nSecondly, while I don't have the skills required to help with the development of the datetime functionality, it's a feature I'd love to see included in Awkward, as I work with at lot of time series data... so please increment the \"interest counter\" by one ;)\r\n\r\n",
  "created_at":"2020-12-23T11:00:49Z",
  "id":750143503,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MDE0MzUwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-23T11:01:43Z",
  "user":"MDQ6VXNlcjY0MTMzMzg2"
 },
 {
  "author_association":"MEMBER",
  "body":"**I think this is the top-requested feature right now.** I'll keep that in mind and prioritize it accordingly.\r\n\r\nI just went searching for a formal upvote tool, but I couldn't find one that\r\n\r\n   * stays open indefinitely (Slideo, which we used to good effect in the PyHEP workshop, only supports week-long events)\r\n   * lets me pick a list of things to provide upvotes for\r\n   * ideally, would be something I could insert in the GitHub readme, right next to the [Roadmap](https://github.com/scikit-hep/awkward-1.0#roadmap).\r\n\r\n[GitHub suggests \"thumbs up\" on issues](https://github.com/isaacs/github/issues/1533), but I don't see a way to make a leaderboard of most thumbs-up issues. No wait, yes I do.\r\n\r\nI added [instructions for upvoting issues](https://github.com/scikit-hep/awkward-1.0#voting-for-fixesfeatures), though it will take some time before enough people do this that the vote is very meaningful.",
  "created_at":"2020-12-23T16:49:01Z",
  "id":750383574,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MDM4MzU3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-23T16:49:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Currently, \"row groups\" in a Parquet file are mapped to Awkward [PartitionedArrays](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partition.PartitionedArray.html). Awkward PartitionedArrays can't be nested (the only type that can't), so I suppose that any Parquet partitions (multiple files) should be flattened at the same level as row groups (for instance, 3 partitions with 2 row groups each becomes 6 Awkward \"partitions\").\r\n\r\nAlso, I think one would always want `lazy=True` when reading from a multi-file partitioned dataset. Perhaps, then, it should be a different function, like `ak.from_partitioned_parquet`? After all, it's now expecting a directory for its first argument, rather than a file.",
  "created_at":"2020-08-04T19:25:58Z",
  "id":668781110,
  "issue":368,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODc4MTExMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-04T19:25:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I found a rather weird behaviour that seems to be linked to partitioned parquets and my suggested \"workaround\".\r\n> ```python\r\n> # current workaround with pq directly\r\n> table = pq.read_table(parquet_dir)\r\n> ak.from_arrow(table)\r\n> ```\r\n\r\nAs this is not a supported feature, I did not want to open a bug report but figured it's worth sharing as it took me a while to figure out that the above seems to cause the following issue.\r\nPreviously loaded partitioned data and slicing it (on a low level) will result in a `ValueError: in IndexedArray32 attempting to get xxx, index[i] >= len(content)` when saving to parquet and reloading.\r\n\r\nWorking Example:\r\nA bit more extensive that explains the principles of what I am trying to achieve, as I realized the combination of certain aspects matter esp. details about the containing dtypes and conversions.\r\nI couldn't figure out what is happening. I first suspected wrong advanced slicing (#370) or the type conversion awk-parquet-arrow as described [here](https://awkward-array.org/how-to-convert-arrow.html) or (#393).\r\nIt was difficult to reproduce the results - so it might not be really a minimal example. Sorry for that!\r\n\r\n```python\r\n# let's create some fake data that will be saved as parquet\r\ndf = pd.DataFrame([{\"id\": n, \"label\": np.random.choice([\"OK\", \"NOK\"], 1)[0], \"year\": np.random.choice([2019, 2020], 1), \"arr\": np.random.rand(np.random.randint(0,100))} for n in range(10000)])\r\n# cast label\r\ndf.label = df.label.astype(\"category\")\r\ndf.year = df.year.astype(\"uint32\")\r\n# index\r\ndf.set_index(['id'], inplace=True)\r\n# save as partioned parquet\r\ndf.to_parquet('test.parquet',\r\n              partition_cols=['year'], # this seems to be crucial\r\n              version='2.0',\r\n              data_page_version='2.0')\r\n# let's get started. reload\r\ntable = pq.read_table('t.parquet')\r\n# load as awkward array\r\noriginal = ak.from_arrow(table)\r\n```\r\n```python\r\n>>> original.layout\r\n<RecordArray>\r\n    <field index=\"0\" key=\"label\">\r\n        <IndexedArray64>\r\n            <index><Index64 i=\"[0 0 0 1 0 ... 19 19 19 18 18]\" offset=\"0\" length=\"20026\" at=\"0x55d32435ff10\"/></index>\r\n            <content><ListArray64>\r\n                <parameters>\r\n                    <param key=\"__array__\">\"string\"</param>\r\n                </parameters>\r\n                <starts><Index64 i=\"[0 3 5 8 10 ... 38 40 43 45 48]\" offset=\"0\" length=\"20\" at=\"0x55d31bdf77d0\"/></starts>\r\n                <stops><Index64 i=\"[3 5 8 10 13 ... 40 43 45 48 50]\" offset=\"0\" length=\"20\" at=\"0x55d31be3ff70\"/></stops>\r\n                <content><NumpyArray format=\"B\" shape=\"50\" data=\"78 79 75 79 75 ... 78 79 75 79 75\" at=\"0x55d30a4851b0\">\r\n                    <parameters>\r\n                        <param key=\"__array__\">\"char\"</param>\r\n                    </parameters>\r\n                </NumpyArray></content>\r\n            </ListArray64></content>\r\n        </IndexedArray64>\r\n    </field>\r\n    <field index=\"1\" key=\"arr\">\r\n        <ListArray64>\r\n            <starts><Index64 i=\"[0 26 46 54 81 ... 995057 995093 995152 995214 995284]\" offset=\"0\" length=\"20026\" at=\"0x55d3243870f0\"/></starts>\r\n            <stops><Index64 i=\"[26 46 54 81 150 ... 995093 995152 995214 995284 995357]\" offset=\"0\" length=\"20026\" at=\"0x55d3243ae2d0\"/></stops>\r\n            <content><IndexedOptionArray64>\r\n                <index><Index64 i=\"[0 1 2 3 4 ... 995352 995353 995354 995355 995356]\" offset=\"0\" length=\"995357\" at=\"0x55d328748590\"/></index>\r\n                <content><NumpyArray format=\"f\" shape=\"995357\" data=\"0.726096 0.590173 0.699854 0.116417 0.743831 ... 0.0755084 0.756987 0.240388 0.158446 0.722705\" at=\"0x55d312824ab0\"/></content>\r\n            </IndexedOptionArray64></content>\r\n        </ListArray64>\r\n    </field>\r\n    <field index=\"2\" key=\"id\">\r\n        <IndexedOptionArray64>\r\n            <index><Index64 i=\"[0 1 2 3 4 ... 20021 20022 20023 20024 20025]\" offset=\"0\" length=\"20026\" at=\"0x55d3243d54b0\"/></index>\r\n            <content><NumpyArray format=\"l\" shape=\"20026\" data=\"2 3 4 8 9 ... 9984 9987 9992 9996 9998\" at=\"0x55d312bf0b30\"/></content>\r\n        </IndexedOptionArray64>\r\n    </field>\r\n    <field index=\"3\" key=\"year\">\r\n        <NumpyArray format=\"i\" shape=\"20026\" data=\"2019 2019 2019 2019 2019 ... 2020 2020 2020 2020 2020\" at=\"0x55d3233b27b0\"/>\r\n    </field>\r\n</RecordArray>\r\n```\r\nMain focus is on the field `label` with a `ListArray64` that is nested in `IndexedArray64`.\r\n\r\nLet's now mask the array and slice based on indices within each variable length array.\r\n```python\r\n# might be irrelevant\r\nremove = np.random.choice([False, True], size=len(original))\r\nmasked = original[~remove].copy()\r\n# \"materialize\"\r\nmasked = ak.flatten(masked, axis=0)\r\n# slice nested arrays based on #370 (unofficial slicing)\r\nstarts = np.asarray(masked.arr.layout.starts).astype(\"uint64\")\r\nstops = np.asarray(masked.arr.layout.stops).astype(\"uint64\")\r\n# gather plausible indices with intrinsic information (here randomized)\r\ndef _get_each_idx(starts, stops):\r\n    diffs = stops-starts\r\n    nzd = diffs[np.where(diffs > 0)[0]]\r\n    rs = np.random.randint(np.zeros(len(nzd)).astype(int), nzd)\r\n    diffs[np.where(diffs != 0)[0]] = rs\r\n    return diffs.astype(\"uint64\")\r\n# set  new stops and starts\r\nstart_idx = _get_each_idx(starts, stops)\r\nnew_starts = start_idx + starts\r\nend_idx = _get_each_idx(new_starts, stops)\r\nnew_stops = end_idx + new_starts\r\n# change nested arrays (based on unofficial lowlevel layout)\r\nlayout = masked.layout\r\nfields = []\r\nfor k in layout.keys():\r\n    if not k in ['arr']:\r\n        fields.append(layout[k])\r\n    else:\r\n        fields.append(ak.layout.ListArray64(\r\n            ak.layout.Index64(starts_n),\r\n            ak.layout.Index64(stops_n),\r\n            layout[k].content\r\n            )\r\n        )\r\n\r\n# main result. reconstruct back to a RecordArray.   \r\nsliced = ak.layout.RecordArray(\r\n        fields,\r\n        layout.keys()\r\n    )\r\n```\r\nWhile this is a low level composing of arrays it provides consistent arrays and works until the result is exported as parquet.\r\n```python\r\n>>> ak.is_valid(original), ak.validity_error(original), ak.is_valid(masked), ak.validity_error(masked), ak.is_valid(sliced), ak.validity_error(sliced)\r\n(True, None, True, None, True, None)\r\n```\r\n\r\nThe problem starts when reloading data.\r\n```python\r\n>>> sliced_arr = ak.Array(sliced)\r\n>>> sliced_arr = ak.flatten(sliced_arr, axis=0)\r\n>>> ak.to_parquet(sliced_arr, \"tmp.parquet\")\r\n\r\n>>> loaded = ak.from_parquet(\"tmp.parquet\")\r\n>>> loaded\r\n<Array [{label: 'NOK', arr: [, ... year: 2020}] type='10054 * {\"label\": string, ...'>\r\n>>> ak.is_valid(loaded), ak.validity_error(loaded)\r\n(False,\r\n 'at layout.field(0) (IndexedArray32): index[i] >= len(content) at i=4936')\r\n```\r\n\r\nJagged arrays can be accessed along the entire axis.\r\n```python\r\n>>> sliced_arr.arr[10000]\r\n<Array [0.982, 0.695, 0.111, ... 0.637, 0.984] type='77 * ?float32'>\r\n>>> sliced_arr.label[10000]\r\n'NOK'\r\n>>> loaded.arr[10000]\r\n<Array [0.982, 0.695, 0.111, ... 0.637, 0.984] type='77 * ?float32'>\r\n```\r\n\r\n... `label` field will throw a ValueError: in IndexedArray32 attempting to get 4936, index[i] >= len(content)\r\n```python\r\nfor i in range(len(loaded)):\r\n    try:\r\n        loaded[i, 'label'].tolist()\r\n    except ValueError:        \r\n        print(i)\r\n        pass\r\n```\r\n\r\n\r\n@jpivarski ",
  "created_at":"2020-08-19T12:40:18Z",
  "id":676286664,
  "issue":368,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjI4NjY2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T12:40:18Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Indexing can be legitimately confusing (also true of NumPy). I'll break this down to what I think you're asking.\r\n\r\nFirst, the array of records can always be projected onto `\"y\"`. I tried a number of combinations and didn't see any trouble with that. For simplicity of discussion here, instead of talking about\r\n\r\n```python\r\narray = ak.Array([\r\n    [{\"x\": 1.1, \"y\": [1]}],\r\n    [{\"x\": 2.2, \"y\": [11, 12]}],\r\n    [{\"x\": 3.3, \"y\": [21, 22, 23]}],\r\n    #[], # cannot slice this by index   (if empty, you'll just have to pass an empty list in the slice)\r\n    [{\"x\": 3.3, \"y\": [31, 32, 33]}],\r\n    [{\"x\": 4.4, \"y\": [41, 42, 43, 44]}],\r\n    [{\"x\": 5.5, \"y\": [51, 52, 53, 54, 55]}]\r\n])\r\n```\r\n\r\nwhich has type\r\n\r\n```javascript\r\n6 * var * {\"x\": float64, \"y\": var * int64}\r\n```\r\n\r\nwe could talk about\r\n\r\n```python\r\narray[\"y\"]   # or array.y\r\n```\r\n\r\nwhich is\r\n\r\n```python\r\n[[[1]], [[11, 12]], [[21, 22, 23]], [[31, 32, 33]], [[41, 42, 43, 44]], [[51, 52, 53, 54, 55]]]\r\n```\r\n\r\nwith type\r\n\r\n```javascript\r\n6 * var * var * int64\r\n```\r\n\r\nYou can certainly do\r\n\r\n```python\r\n>>> array[\"y\", [[0], [0], [1], [1], [2], [2]]]\r\n<Array [[[[1]]], [[[1, ... [[[21, 22, 23]]]] type='6 * 1 * var * var * int64'>\r\n```\r\n\r\nbecause each of the elements of the slice has length 1, just like `array` (and `array.y`) and the integer values in each is less than the length of each nested list:\r\n\r\n```python\r\n>>> array.y[0, 0]\r\n<Array [1] type='1 * int64'>                    # has an element 0\r\n>>> array.y[1, 0]\r\n<Array [11, 12] type='2 * int64'>               # has an element 0\r\n>>> array.y[2, 0]\r\n<Array [21, 22, 23] type='3 * int64'>           # has an element 1\r\n>>> array.y[3, 0]\r\n<Array [31, 32, 33] type='3 * int64'>           # has an element 1\r\n>>> array.y[4, 0]\r\n<Array [41, 42, 43, 44] type='4 * int64'>       # has an element 2\r\n>>> array.y[5, 0]\r\n<Array [51, 52, 53, 54, 55] type='5 * int64'>   # has an element 2\r\n>>> array.y[6, 0]\r\n```\r\n\r\nand so that's why it works. [ak.singletons](https://awkward-array.readthedocs.io/en/latest/_auto/ak.singletons.html) has nothing to do with it: it's used to convert `None` values into empty lists and everything else into length-1 lists, which you already have.\r\n\r\nRecent versions of NumPy provide a hint about why the `mask` didn't work:\r\n\r\n```python\r\n>>> mask = np.array([\r\n...     [True],\r\n...     [True, True],\r\n...     [False, True, True],\r\n...     [False, True, False],\r\n...     [False, False, True, True],\r\n...     [False, False, True, False]])\r\n```\r\n\r\nraises the warning\r\n\r\n```\r\n<stdin>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a\r\nlist-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to\r\ndo this, you must specify 'dtype=object' when creating the ndarray\r\n```\r\n\r\nThis is a NumPy array of `dtype=object`, which soon won't be created automatically. Constructing the mask as an Awkward array is the first step:\r\n\r\n```python\r\n>>> mask = ak.Array([\r\n...     [True],\r\n...     [True, True],\r\n...     [False, True, True],\r\n...     [False, True, False],\r\n...     [False, False, True, True],\r\n...     [False, False, True, False]])\r\n>>> mask\r\n<Array [[True], [True, ... False, True, False]] type='6 * var * bool'>\r\n```\r\n\r\nbut it also needs the length-1 structure of `startIndices` to fit into the second axis:\r\n\r\n```python\r\n>>> mask = ak.Array([\r\n...     [[True]],\r\n...     [[True, True]],\r\n...     [[False, True, True]],\r\n...     [[False, True, False]],\r\n...     [[False, False, True, True]],\r\n...     [[False, False, True, False]]])\r\n>>> mask\r\n<Array [[[True]], ... False, True, False]]] type='6 * var * var * bool'>\r\n>>> array.y\r\n<Array [[[1]], [[11, ... [51, 52, 53, 54, 55]]] type='6 * var * var * int64'>\r\n>>> ak.num(mask, axis=2)\r\n<Array [[1], [2], [3], [3], [4], [4]] type='6 * var * int64'>\r\n>>> ak.num(array.y, axis=2)\r\n<Array [[1], [2], [3], [3], [4], [5]] type='6 * var * int64'>\r\n```\r\n\r\nOkay; they line up: now we're ready to go!\r\n\r\n```python\r\n>>> array.y[mask]\r\n<Array [[[1]], [[11, 12], ... 43, 44]], [[53]]] type='6 * var * var * int64'>\r\n```\r\n\r\n---------------------\r\n\r\nAbout making a slice option that can be different at each level (e.g. slice list 1 with 0:0, list 2 with 1:2, list 3 with 0:2), that's an interesting idea, something that becomes useful in the context of ragged arrays that you wouldn't have with rectilinear arrays.\r\n\r\nRight now, that sort of thing can be done by opening up the `ak.Array` structure and manipulating its memory `layout`:\r\n\r\n```python\r\n>>> original = array.y.layout\r\n>>> original\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 1 2 3 4 5 6]\" offset=\"0\" length=\"7\" at=\"0x55f65db71150\"/></offsets>\r\n    <content><ListOffsetArray64>\r\n        <offsets><Index64 i=\"[0 1 3 6 9 13 18]\" offset=\"0\" length=\"7\" at=\"0x55f65db75170\"/></offsets>\r\n        <content><NumpyArray format=\"l\" shape=\"18\" data=\"1 11 12 21 22 ... 51 52 53 54 55\" at=\"0x55f65d654e60\"/></content>\r\n    </ListOffsetArray64></content>\r\n</ListOffsetArray64>\r\n>>> starts = np.asarray(original.content.starts)\r\n>>> stops  = np.asarray(original.content.stops)\r\n>>> starts, stops\r\n(array([ 0,  1,  3,  6,  9, 13], dtype=int64),\r\n array([ 1,  3,  6,  9, 13, 18], dtype=int64))\r\n```\r\n\r\nSlicing with a different `start[i]` and `stop[i]` at each `i` is a matter of adding and subtracting the right number from these `starts` and `stops`. Be careful if you modify these NumPy arrays in place: they are views of the Awkward layout and will change the Awkward array in-place (one of the few ways Awkward arrays are mutable).\r\n\r\n```python\r\n>>> starts = starts + [0, 0, 1, 1, 2, 2]\r\n>>> stops  = stops  - [0, 0, 1, 1, 2, 2]\r\n>>> starts, stops\r\n(array([ 0,  1,  4,  7, 11, 15], dtype=int64), array([ 1,  3,  5,  8, 11, 16], dtype=int64))\r\n>>> modified = ak.layout.ListOffsetArray64(\r\n...     original.offsets,\r\n...     ak.layout.ListArray64(\r\n...         ak.layout.Index64(starts),\r\n...         ak.layout.Index64(stops),\r\n...         original.content.content))\r\n>>> modified\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 1 2 3 4 5 6]\" offset=\"0\" length=\"7\" at=\"0x55f65db71150\"/></offsets>\r\n    <content><ListArray64>\r\n        <starts><Index64 i=\"[0 1 4 7 11 15]\" offset=\"0\" length=\"6\" at=\"0x55f65db704d0\"/></starts>\r\n        <stops><Index64 i=\"[1 3 5 8 11 16]\" offset=\"0\" length=\"6\" at=\"0x55f65db5b0b0\"/></stops>\r\n        <content><NumpyArray format=\"l\" shape=\"18\" data=\"1 11 12 21 22 ... 51 52 53 54 55\" at=\"0x55f65d654e60\"/></content>\r\n    </ListArray64></content>\r\n</ListOffsetArray64>\r\n>>> ak.Array(modified)\r\n<Array [[[1]], [[11, 12]], ... [[]], [[53]]] type='6 * var * var * int64'>\r\n>>> ak.Array(modified).tolist()\r\n[[[1]], [[11, 12]], [[22]], [[32]], [[]], [[53]]]\r\n```\r\n\r\nAnd that's probably how a variable starts:stops would be implemented. But if the indexing is tricky, this is tricky-squared. It's pretty easy to make an array that's internally inconsistent (check with [ak.is_valid](https://awkward-array.readthedocs.io/en/latest/_auto/ak.is_valid.html) and [ak.validity_error](https://awkward-array.readthedocs.io/en/latest/_auto/ak.validity_error.html)).",
  "created_at":"2020-08-05T16:44:33Z",
  "id":669304029,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTMwNDAyOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2020-08-05T16:44:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Great, thank you very much for that swift clarification.\r\n\r\nI was particularly looking for the second explanations (overall my motivation for awkward as I am dealing with that sort of tasks a lot in the context of jagged arrays).\r\n\r\nYou can close this issue. Maybe the first part could be part of the [quickstart](https://awkward-array.org/how-to-examine-simple-slicing.html). Let me know if I can help fill the doc stubs with content.",
  "created_at":"2020-08-05T18:06:50Z",
  "id":669348039,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTM0ODAzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-05T18:06:50Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"```python\r\nstarts = np.asarray(original.content.starts)\r\n```\r\nThrows an error with the same `array`:\r\n`AttributeError: 'awkward1._ext.NumpyArray' object has no attribute 'starts'`",
  "created_at":"2020-08-05T18:17:39Z",
  "id":669356136,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTM1NjEzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-05T18:17:39Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the offer! The stubs are there because I have to finish other projects (Uproot 4), which also need documentation\u2014Awkward is half-there in that it has all the reference docs, and the ones in the Python API include examples.\r\n\r\nIf you submit a documentation issue with the examples you'd like to see fill the stub, I'll enter them into the stub. I don't have it set up as a wiki (whenever I do make a wiki, no one edits it!), in part because evaluating the JupyterBook is part of the build (to ensure that tutorial examples are not broken), which gives me a chance to edit. But suggested text definitely bumps it up in priority: if you write it, I'll post it.",
  "created_at":"2020-08-05T18:19:58Z",
  "id":669357379,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTM1NzM3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-05T18:19:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> ```python\r\n> starts = np.asarray(original.content.starts)\r\n> ```\r\n> \r\n> Throws an error with the same `array`:\r\n> `AttributeError: 'awkward1._ext.NumpyArray' object has no attribute 'starts'`\r\n\r\nThat's where you're getting into trickiness-squared. The different node types in a layout have different properties: [NumpyArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.NumpyArray.html) represents rectilinear data, like NumPy, which has no need of `starts` and `stops`. [ListArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListArray.html) is a fully general jaggedness implementation and [ListOffsetArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html) is the common case in which `starts = offsets[:-1]` and `stops = offsets[1:]` for some monotonically increasing `offsets` array with length N+1 (N is the length of the logical array). These links provide more information about the layout classes, but keep in mind that everything under `layout` is semi-internal. (It doesn't start with an underscore because it's public API, but it's for framework developers, not data analysts.)\r\n\r\n",
  "created_at":"2020-08-05T18:24:42Z",
  "id":669364672,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTM2NDY3Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-05T18:24:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Just realized that your suggested array slicing won't result in what I meant by index slicing.\r\n> You can certainly do\r\n> \r\n> ```python\r\n> >>> array[\"y\", [[0], [0], [1], [1], [2], [2]]]\r\n> <Array [[[[1]]], [[[1, ... [[[21, 22, 23]]]] type='6 * 1 * var * var * int64'>\r\n> ```\r\nThis will result in a select of entire nested lists by a collection of indexes (`np.take`). But I meant to get items of each nested list by its respective index (similar to the second part of your clarification - just without a range).\r\n\r\nFor simplicity, let's reconsider:\r\nInstead of accomplishing this / or the cited above:\r\n```python\r\n>>> a = ak.from_iter([[1.1, 2.2, 3.3], [], [4.4, 5.5], [6.6, 7.7, 8.8], [9.9]])\r\n>>> a[[0, 3], [True, False, True]]\r\n<Array [1.1, 8.8] type='2 * float64'>\r\n```\r\nwhere we rearrange and select within nested same-size list (rather uncommon to assume rectangular set of non-jagged arrays, which would fail over `a[[0, 1], [True, False, True]]`)\r\n\r\nI intended to do something like this (multidimensional index slicing):\r\n```python\r\n>>> idx = [0,1,2]\r\n>>> a[[0,1,3], idx]\r\n<Array [[1.1], [], [8.8]] type='3 * var * float64'>\r\n# or generally over entire array\r\nidxs = [0,1,0,2,1]\r\n# assert a.shape[0] == len(idxs)\r\n>>> a[list(range(a.shape[0])), idxs]\r\n<Array [[1.1], [], [4.4], [8.8], []] type='5 * var * float64'>\r\n```\r\nor perhaps like so (treat slice array of type `awkward1._ext.Index64` differently and try to slice nested lists; return `[]` if `ak.num(.) < idx`):\r\n```python\r\na[ak.layout.Index64(idxs)]\r\n<Array [[1.1], [], [4.4], [8.8], []] type='5 * var * float64'>\r\n```\r\n\r\nDo only way I see to do this is by your suggested second approach where I set all `stops = start+1` with `start = idxs`. Maybe there is a more elegant way?",
  "created_at":"2020-08-06T00:13:25Z",
  "id":669609300,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTYwOTMwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-06T13:53:23Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"You could use [ak.pad_none](https://awkward-array.readthedocs.io/en/latest/_auto/ak.pad_none.html) to _make_ each inner list have at least the right number of elements:\r\n\r\n```python\r\n>>> ak.pad_none(a, 3)\r\n<Array [[1.1, 2.2, 3.3], ... [9.9, None, None]] type='5 * var * ?float64'>\r\n```\r\n\r\nThen it would be legal to ask for `[0, 1, 2]` in the second dimension, because its maximum index, `2`, exists:\r\n\r\n```python\r\n>>> ak.pad_none(a, 3)[[0, 1, 3], [0, 1, 2]]\r\n<Array [1.1, None, 8.8] type='3 * ?float64'>\r\n```\r\n\r\nThe Numpyian thing to do when given advanced arrays in two dimensions is to \"iterate over them as one\" and return the elements that match\u2014a single-depth list, as above. In your examples, it looks like you want nested lists, and you want the empty list in `a` to become an empty list in the output. [ak.singletons](https://awkward-array.readthedocs.io/en/latest/_auto/ak.singletons.html), which you're already familiar with, is for that:\r\n\r\n```python\r\n>>> ak.singletons(ak.pad_none(a, 3)[[0, 1, 3], [0, 1, 2]])\r\n<Array [[1.1], [], [8.8]] type='3 * var * float64'>\r\n```\r\n\r\nIn your example, you have `[[1.1], [], [7.7]]`, but I assume that's a mistake because `idx[2]` is `2`, which picks out the last element of `[6.6, 7.7, 8.8]`.\r\n\r\nYour second example would look like this then:\r\n\r\n```python\r\n>>> ak.singletons(ak.pad_none(a, 3)[range(len(a)), [0, 1, 0, 2, 1]])\r\n<Array [[1.1], [], [4.4], [8.8], []] type='5 * var * float64'>\r\n```\r\n\r\nthough if it was big, you wouldn't want to do a Python `range`, you'd do `np.arange`:\r\n\r\n```python\r\n>>> ak.singletons(ak.pad_none(a, 3)[np.arange(len(a)), [0, 1, 0, 2, 1]])\r\n<Array [[1.1], [], [4.4], [8.8], []] type='5 * var * float64'>\r\n```\r\n\r\nI should warn you to stay away from `a.shape`. It exists as a side-effect of supporting Pandas views, but I'll be removing them, in a large part because of the badly named methods and properties it forces me to have (#350). A real \"shape\" property would somehow characterize the nested lists, though that can't be done as a tuple of numbers. (That's why Awkward has [ak.type](https://awkward-array.readthedocs.io/en/latest/_auto/ak.type.html).) Pandas requires it to be `(len(a),)`. I'm 99% sure I'll be deprecating Pandas views, so the `shape` property will be removed.",
  "created_at":"2020-08-06T00:41:34Z",
  "id":669616996,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2OTYxNjk5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-06T00:41:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I cancelled your build so that a deployment will go out. If you need the results of this test, make another commit or ask me to start another for you (scheduled after the deployment). Thanks!",
  "created_at":"2020-08-19T18:49:49Z",
  "id":676598862,
  "issue":373,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjU5ODg2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T18:49:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This looks good. Have you tested it on the AWS instance? That will determine if your generated \"num\" function is equivalent to the one Anish wrote.",
  "created_at":"2020-08-30T13:36:21Z",
  "id":683421648,
  "issue":373,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzQyMTY0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-30T13:36:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The tests should be fixed now :) ",
  "created_at":"2020-08-31T06:55:54Z",
  "id":683599119,
  "issue":373,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzU5OTExOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-31T06:55:54Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"There are some compiler warnings; the generated code has some rough edges, but that's to be expected.\r\n\r\n```\r\nsrc/cuda-kernels/awkward_IndexedArray_mask.cu(15): warning: pointless comparison of unsigned integer with zero\r\n          detected during instantiation of \"void cuda_IndexedArray_mask(int8_t *, const B *, int64_t, Error *) [with B=uint32_t]\" \r\n(81): here\r\n\r\nsrc/cuda-kernels/awkward_ListArray_validity.cu(24): warning: pointless comparison of unsigned integer with zero\r\n          detected during instantiation of \"void cuda_ListArray_validity(const C *, const D *, int64_t, int64_t, Error *) [with C=uint32_t, D=uint32_t]\" \r\n(80): here\r\n\r\nsrc/cuda-kernels/awkward_ListOffsetArray_reduce_global_startstop_64.cu(12): warning: variable \"thread_id\" was declared but never referenced\r\n\r\nsrc/cuda-kernels/awkward_ListOffsetArray_reduce_global_startstop_64.cu(12): warning: variable \"thread_id\" was declared but never referenced\r\n\r\nsrc/cuda-kernels/awkward_UnionArray_fillna.cu(15): warning: pointless comparison of unsigned integer with zero\r\n          detected during instantiation of \"void cuda_UnionArray_fillna(int64_t *, const B *, int64_t, Error *) [with B=uint32_t]\" \r\n(62): here\r\n\r\nsrc/cuda-kernels/awkward_UnionArray_validity.cu(22): warning: pointless comparison of unsigned integer with zero\r\n          detected during instantiation of \"void cuda_UnionArray_validity(const int8_t *, const B *, int64_t, int64_t, const int64_t *, Error *) [with B=uint32_t]\" \r\n(103): here\r\n\r\nsrc/cuda-kernels/awkward_combinations.cu(12): warning: variable \"thread_id\" was declared but never referenced\r\n\r\nsrc/cuda-kernels/awkward_combinations.cu(12): warning: variable \"thread_id\" was declared but never referenced\r\n```\r\n\r\nIt also takes quite a bit longer to compile\u2014an indication that there's a lot more code now.\r\n\r\nBut it all works! (On my computer and the AWS instance; I deleted everything and compiled from scratch.)\r\n\r\n<img src=\"https://user-images.githubusercontent.com/1852447/91727707-ade75800-eb67-11ea-939d-de40b2d6bc30.png\" width=\"600\">\r\n\r\n[cuda-kernels-written.pdf](https://github.com/scikit-hep/awkward-1.0/files/5150703/cuda-kernels-written.pdf)\r\n",
  "created_at":"2020-08-31T13:57:17Z",
  "id":683795400,
  "issue":373,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Mzc5NTQwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-31T13:57:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I guess, it would replace `ak.astype` https://github.com/scikit-hep/awkward-1.0/blob/master/src/awkward1/operations/structure.py#L2696",
  "created_at":"2020-08-07T13:23:15Z",
  "id":670514304,
  "issue":376,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MDUxNDMwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-07T13:23:15Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I see: it would just be renamed. That looks pretty easy.",
  "created_at":"2020-08-07T13:36:59Z",
  "id":670520521,
  "issue":376,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MDUyMDUyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-07T13:36:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The function was named `ak.astype` in #346, but now it's `ak.numbers_to_type` and it's documented.",
  "created_at":"2020-08-07T14:05:42Z",
  "id":670534408,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MDUzNDQwOA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-07T14:05:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Only the doctest needs to pass here (and I'm not sure it's testing the JupyterBook part). The real test will be to see if Netlify can run it.",
  "created_at":"2020-08-07T21:21:24Z",
  "id":670718132,
  "issue":378,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MDcxODEzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-07T21:21:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Is this PR intended as an example (from @trickarcher to @reikdas) or as something to be merged in? It's getting a little stale as src/libawkward/kernel.cpp has changed. If it's an example, let's convert it into a draft and eventually close it after Reik is done, so that I'm not asking myself whether I need to merge it over and over. If it's meant to be merged in, then it will need to be brought up to date and I'll review it.",
  "created_at":"2020-08-17T17:38:25Z",
  "id":675016596,
  "issue":380,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTAxNjU5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T17:38:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"(I hope I am not stepping on any toes) I guess we could either merge it in after @trickarcher finishes making the possible changes discussed inline, or we could convert this PR to a draft since I will be attempting to generate this in my PR anyway and then close it when my PR has been merged, like @jpivarski suggested.",
  "created_at":"2020-08-18T18:20:31Z",
  "id":675637967,
  "issue":380,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTYzNzk2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T18:20:31Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"This is still a good example for @reikdas to follow, but I'm closing it because we have no intention of merging it.",
  "created_at":"2020-08-21T16:06:00Z",
  "id":678369127,
  "issue":380,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODM2OTEyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T16:06:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">I didn't know that I broke the documentation with this update. (I didn't see doctest failing.)\r\n\r\ndoctest passes because the documentation is still being created. It just doesn't look right (also I have no idea why it changed with the recent commits to `master`). This is what I am trying to fix - \r\n\r\n![image](https://user-images.githubusercontent.com/11775615/89821626-873b9000-db6c-11ea-8b35-1c2600ba4945.png)\r\n\r\n>But just removing the section on Pandas compatibility isn't the right way to go: what's broken about this section? The :doc:`ak.to_pandas` should be okay because ak.to_pandas exists in the documentation. The external link looks okay to me, too.\r\n\r\nI get this warning when building documentation locally - \r\n\r\n![image](https://user-images.githubusercontent.com/11775615/89821290-fb296880-db6b-11ea-8589-cb054882ba93.png)\r\n\r\nThis warning doesn't have anything to do with fixing the awkward kernel documentation and I just added this in the same PR because it seemed like something that might have been removed elsewhere but not here. I'll remove the changes in `index.rst` and try and fix all sphinx-build warnings in another PR (there are some others apart from the one about `ak.to_pandas`).",
  "created_at":"2020-08-10T19:24:40Z",
  "id":671542149,
  "issue":382,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTU0MjE0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T19:33:30Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"ak.pandas.rst is new, but it does exist. (That's the link I sent.)\r\n\r\nThat part that you circled is wrong and should be fixed. I think your first correction (adding a carriage return) would do that.\r\n\r\nIt looks like this PR now includes only the carriage return\u2014so I'm happy with it. (I just didn't want my text removed.)\r\n\r\nDo you want me to merge it?",
  "created_at":"2020-08-10T19:55:26Z",
  "id":671556233,
  "issue":382,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTU1NjIzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T19:55:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">Do you want me to merge it?\r\n\r\nYes, please.",
  "created_at":"2020-08-10T20:01:49Z",
  "id":671559406,
  "issue":382,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTU1OTQwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T20:01:49Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"This also happens if you change type from `float64` to `float64`. It doesn't have to be different.",
  "created_at":"2020-08-10T22:31:12Z",
  "id":671622175,
  "issue":383,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTYyMjE3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T22:31:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nevermind; fixed in #385.",
  "created_at":"2020-08-10T23:02:09Z",
  "id":671631592,
  "issue":383,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTYzMTU5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T23:02:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This is ready",
  "created_at":"2020-08-11T13:49:14Z",
  "id":671958298,
  "issue":384,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTk1ODI5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T13:49:14Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I looked this over carefully, and I think it's good. The Form \u2192 JSON \u2192 Python \u2192 modify in place \u2192 JSON \u2192 Form sequence wouldn't be necessary if we had a good traverser for Forms, as we do Content layouts (`_ak._util.recursively_apply`), but it's fine.\r\n\r\nSo, all looks good, the tests would catch errors if there were any, and they pass, so I'm merging it. Thanks!",
  "created_at":"2020-08-11T14:54:28Z",
  "id":671995581,
  "issue":384,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTk5NTU4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T14:54:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We don't have a \"group by\" operation (Awkward is NumPy-like, rather than Pandas-like), though it wouldn't be a bad idea to add one. First, though, we'd have to have a concept of equality for arbitrary structures: if you want to group by some part of a data structure whose type is records or lists, rather than numbers or strings, we'd need something to tell us whether they're the same or not the same. I think such a definition would be unambiguous, though: records have to have the same fields and the same values in all fields (recursively), and lists have to have the same numbers of elements and the same values in each element (recursively). Any missing values must match and indirection has to pass through, so that an IndexedArray on one side can be equal to a non-IndexedArray on the other side as long as the IndexedArray rearranges elements to match those on the other side. It could be hard to turn an equality definition in Python into an efficient search for unique elements (keys of the group-by), so it might need to be defined in C++. I don't see a good way to vectorize it. If a first pass labeled all fields of record-type fields as unique integers, we'd them have to find unique sets of these fields: I just see a lot of intermediate arrays if this is vectorised, so much so that it could reduce performance, rather than improve it.\r\n\r\nThat's not a small project and wouldn't be available right away, so if you're looking for a solution to group by of JSON right now, maybe try jq, JSONiq, jmespath, JaQL, JsonPath, json-path, ... There are a lot of languages that seek to be the SQL of JSON. Awkward Array seeks to be the NumPy of JSON.\r\n\r\nOut of curiosity, what problem are you trying to solve?",
  "created_at":"2020-08-11T11:56:30Z",
  "id":671901259,
  "issue":386,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTkwMTI1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T11:56:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Since this is a bit outside Awkward Array's scope and I haven't heard any follow-up, I'm going to close this issue.",
  "created_at":"2020-10-30T22:28:06Z",
  "id":719827733,
  "issue":386,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgyNzczMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:28:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Tests on the GPU instance have passed:\r\n\r\n```\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.7.7, pytest-6.0.1, py-1.9.0, pluggy-0.13.1 -- /home/ubuntu/anaconda3/bin/python\r\ncachedir: .pytest_cache\r\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/ubuntu/awkward-1.0/.hypothesis/examples')\r\nrootdir: /home/ubuntu/awkward-1.0, configfile: setup.cfg\r\nplugins: remotedata-0.3.2, hypothesis-5.24.0, doctestplus-0.7.0, astropy-header-0.1.2, openfiles-0.5.0, arraydiff-0.3\r\ncollecting ... collected 41 items\r\n\r\ntests-cuda/test_0345-cuda-num.py::test_num PASSED                        [  2%]\r\ntests-cuda/test_0345-cuda-transfers.py::test_tocuda PASSED               [  4%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_array PASSED    [  7%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_asarray PASSED  [  9%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_frombuffer PASSED [ 12%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_zeros PASSED    [ 14%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_ones PASSED     [ 17%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_empty PASSED    [ 19%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_full PASSED     [ 21%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_arange PASSED   [ 24%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_meshgrid PASSED [ 26%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_array_equal PASSED [ 29%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_size PASSED     [ 31%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_searchsorted PASSED [ 34%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_cumsum PASSED   [ 36%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_nonzero PASSED  [ 39%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_unique PASSED   [ 41%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_concatenate PASSED [ 43%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_repeat PASSED   [ 46%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_stack PASSED    [ 48%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_vstack PASSED   [ 51%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_packbits PASSED [ 53%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_unpackbits PASSED [ 56%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_atleast_1d PASSED [ 58%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_broadcast_to PASSED [ 60%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_sqrt PASSED     [ 63%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_exp PASSED      [ 65%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_true_divide PASSED [ 68%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_bitwise_or PASSED [ 70%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_logical_and PASSED [ 73%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_equal PASSED    [ 75%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_ceil PASSED     [ 78%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_all PASSED      [ 80%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_any PASSED      [ 82%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_count_nonzero PASSED [ 85%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_sum PASSED      [ 87%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_prod PASSED     [ 90%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_min PASSED      [ 92%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_max PASSED      [ 95%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_argmin PASSED   [ 97%]\r\ntests-cuda/test_0388-abstract-all-uses-of-numpy.py::test_argmax PASSED   [100%]\r\n\r\n============================== 41 passed in 1.09s ==============================\r\n```",
  "created_at":"2020-08-18T00:17:21Z",
  "id":675178310,
  "issue":388,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTE3ODMxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T00:17:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I don't understand the windows errors, is it using a different c++ standard?",
  "created_at":"2020-08-15T16:34:55Z",
  "id":674420491,
  "issue":390,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDQyMDQ5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-15T16:34:55Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"The first error is\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e4186d058e55f0b2bd5747eb1546e694e4258d21/src/libawkward/array/VirtualArray.cpp#L545\r\n\r\nI would say that a `not` operator doesn't exist in C++, but maybe it does in some recent standard. (I don't know how the non-Windows builds passed.) Anyway, the standard is C++11. Negation is `!`.",
  "created_at":"2020-08-15T17:13:20Z",
  "id":674424890,
  "issue":390,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDQyNDg5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-15T17:13:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"It did, but what do you know, I found another case where the lack of a proper form for purely field slices in SliceGenerator is causing usability issues: passing a subarray to a numba function.\r\nGiven that this implementation is most of the way to a proper implementation of virtual `getitem_field`, maybe we should just finish it.",
  "created_at":"2020-08-17T20:16:43Z",
  "id":675091215,
  "issue":390,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTA5MTIxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T20:16:43Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I'll leave this open. It won't be hard to merge with any upcoming changes.",
  "created_at":"2020-08-17T20:29:26Z",
  "id":675097297,
  "issue":390,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTA5NzI5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T20:29:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think this is done",
  "created_at":"2020-08-19T17:21:51Z",
  "id":676556671,
  "issue":390,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjU1NjY3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T17:21:51Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Awkward 0's  `ones_like` was an internal method that was technically needed for some things. The `ones_like`, `zeros_like`, etc. are useful in NumPy because NumPy's arrays are mutable in-place, but Awkward arrays are immutable\u2014you can only derive new values from old values.\r\n\r\nWith that in mind, if you want to create a new array with the same structure as the old but all the values have been replaced by something like `5`, you can multiply the old array by `0` and add `5`. This works because of broadcasting, but broadcasting might also be the reason that you don't actually need an array with all the same values (depends on your problem).\r\n\r\nAs for documentation, there's already a lot of it in the Python and C++ API references, and the tutorials on awkward-array.org are growing. Where would this go? Maybe a tutorial on broadcasting? The problem might not be \"enough documentation\" but \"finding the one that's needed,\" since it's hard to know these things have names.\r\n\r\nAvoid the `shape` and `dtype` properties. They only exist so that Awkward arrays can be Pandas DataFrame columns, and that feature is being removed: see #350. The structure of an Awkward array is characterized by its `ak.type`.",
  "created_at":"2020-08-14T12:01:34Z",
  "id":674040808,
  "issue":391,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDA0MDgwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-14T12:01:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for your detailed response Jim!\r\n\r\nAh, yes, of course - this is the super obvious approach now that you suggest it. I dunno why I didn't use this in awkward0 - I must have had some issues in discovering this, and `ones_like()` solved the issue, so I just moved on (I also didn't realize that ones_like() was internal). My apologies for the noise on something that is so obvious.\r\n\r\n> This works because of broadcasting, but broadcasting might also be the reason that you don't actually need an array with all the same values (depends on your problem).\r\n\r\nAdmittedly I may not have found the most elegant or best solutions, but there were at least a few times with ObjectArrays in awkward0 that I found duplicating values was the only way I could get things working. Perhaps this won't be an issue in awkward1.\r\n\r\n> The problem might not be \"enough documentation\" but \"finding the one that's needed,\" since it's hard to know these things have names.\r\n\r\nYes, I agree - I think it's more about discovery. I've read a good deal of the Python API reference and much of it is excellent, but I've struggled at times to find the right terms. I'm certainly empathetic that it's hard to find the right term - I couldn't find the right term here. I think a tutorial on broadcasting would be helpful to address these sort of points.\r\n\r\nMore generally, even as an awkward0 user who implemented some `ObjectArrays` for my analysis, etc, there are some changes that are continuing to trip me up (for example, looking for `ones_like()`). Some docs on what changed from awkward0 to awkward1 could be also be helpful. I think there are bits and pieces of this around either in the docs or in notebooks (so at least I've gathered some heuristics in my mind), but I don't recall it being consolidated anywhere (apologies if I've overlooked it). It could be helpful as a reference for those who are updating. It doesn't need to be encyclopedic, but it feels to me that it's shifted a bit conceptually. And pointing out if common functions changed could be nice (`.counts` -> `ak.num`, for example).\r\n\r\nI'm still getting up to speed on this, but would be happy to help in any way I can - for example, with feedback (I don't think I understand awkward1 well enough yet to write this myself)",
  "created_at":"2020-08-14T13:00:37Z",
  "id":674062639,
  "issue":391,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDA2MjYzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-14T13:01:02Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"I took the liberty of interpreting this as a request for an Awkward 0 \u2192 Awkward 1 cheat-sheet. For those users who are coming from Awkward 0, that would maximize discovery.",
  "created_at":"2020-08-14T13:19:18Z",
  "id":674070849,
  "issue":391,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDA3MDg0OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-14T13:19:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sounds great! Thanks Jim!",
  "created_at":"2020-08-14T13:23:15Z",
  "id":674072523,
  "issue":391,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDA3MjUyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-14T13:23:15Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"The unhandled dtypes are\r\n\r\n   * `float16`: need to find a C++ library for it\r\n   * `float128`: same problem\r\n   * `complex64`, `complex128`: C++'s built-in `std::complex` can do this; it's just a matter of filling it in everywhere\r\n   * `complex256`: complex _and_ non-standard floating-point type\r\n   * `datetime64`, `timedelta64`: these require a little more work than the above, since we need to work around the buffer protocol, which doesn't recognize these, and they also have special rules in reducers (can't add datetimes to each other, but can add timedeltas; multiplication doesn't make sense for either). Nevertheless, this is a somewhat higher priority, since dates and times are ubiquitous in Pandas data.\r\n   * fixed-width bytestring and string types: variable-length string objects are more common in Pandas data, and they're handled as non-primitive lists with `parameter[\"__array__\"] == \"bytestring\"` or `\"string\"`. The fixed-width bytestrings would come up if someone were analyzing 6-byte MAC addresses or something.\r\n\r\nOther than these dtypes, NumPy arrays can contain Python objects (which we won't ever do), [record stuctures](https://numpy.org/doc/stable/user/basics.rec.html), which we convert to [Awkward records](https://awkward-array.org/how-to-convert-numpy.html#numpy-s-structured-arrays), [masked-out data](https://numpy.org/doc/stable/reference/maskedarray.generic.html), which we convert to [option-type](https://awkward-array.org/how-to-convert-numpy.html#numpy-s-masked-arrays), and non-trivial shapes, which we convert to [regular-length lists](https://awkward-array.org/how-to-convert-numpy.html#numpyarray-shapes-vs-regulararrays).\r\n\r\nYou say that complex numbers are a low priority. But if there's ever a strong need for them, they can be added by following the pattern set by the other types (i.e. search for all \"float32\" and add cases for the complex numbers). It might even be a \"good first issue.\"",
  "created_at":"2020-08-14T13:12:42Z",
  "id":674067943,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDA2Nzk0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-14T13:12:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I totally missed the fact that this had been on the Awkward 0 repository. New development is in Awkward 1 only.\r\n\r\n(All the things I said about handling NumPy dtypes only applies to Awkward 1; Awkward 0 isn't anywhere near as advanced.)",
  "created_at":"2020-08-14T13:14:52Z",
  "id":674068940,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDA2ODk0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-14T13:14:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for the detailed response @jpivarski and all the work that you've put in!\r\n\r\n> You say that complex numbers are a low priority. But if there's ever a strong need for them, they can be added by following the pattern set by the other types (i.e. search for all \"float32\" and add cases for the complex numbers). It might even be a \"good first issue.\"\r\n\r\nI initially regarded it as low priority because in the example I posted it seemed possible to construct an awkward array from a numpy array of complex numbers. Thus, a workaround seemed possible:\r\n\r\n```python\r\n>>> A = ak.from_numpy(np.asarray([0j, 1j]))\r\n<Array [0j, 1j] type='2 * complex128'>\r\n>>> ak.to_arrayset(A)\r\n({\r\n     \"class\": \"NumpyArray\",\r\n     \"itemsize\": 16,\r\n     \"format\": \"Zd\",\r\n     \"primitive\": \"complex128\",\r\n     \"form_key\": \"node0\"\r\n },\r\n {'node0': array([0.+0.j, 0.+1.j])},\r\n None)\r\n\r\n```\r\n\r\nHowever, from your response I understand that you're saying that the C++ layer needs to change to handle complex dtype's? If so, I'd probably be keen to make that change. I'll take a look at the code.\r\n\r\nI'm coming from a Radio Astronomy context where the observational data is complex. It's ingested as dense arrays, but more recent averaging algorithms can make it ... awkward ;-).\r\n\r\n",
  "created_at":"2020-08-14T13:37:38Z",
  "id":674078946,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDA3ODk0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-14T13:37:38Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"In that case, I can give you some pointers, so that you can evaluate how big of a project it will be.\r\n\r\nFor the complex types, there are fortunately stubs everywhere: they throw `std::runtime_errors`, but that makes it convenient to find all the places where something needs to be inserted.\r\n\r\n```\r\n% fgrep -r complex128 include src tests\r\ninclude/awkward/util.h:        complex128,\r\nsrc/python/forms.cpp:        case ak::util::dtype::complex128:\r\nsrc/awkward1/operations/structure.py:    numpy.dtype(numpy.complex128): \"complex128\",\r\nsrc/libawkward/Content.cpp:                 util::dtype_to_format(util::dtype::complex128),\r\nsrc/libawkward/Content.cpp:                 util::dtype::complex128);\r\nsrc/libawkward/util.cpp:      else if (name == \"complex128\") {\r\nsrc/libawkward/util.cpp:        return util::dtype::complex128;\r\nsrc/libawkward/util.cpp:      case util::dtype::complex128:\r\nsrc/libawkward/util.cpp:        return \"complex128\";\r\nsrc/libawkward/util.cpp:        return dtype::complex128;\r\nsrc/libawkward/util.cpp:      case dtype::complex128:\r\nsrc/libawkward/util.cpp:      case dtype::complex128:\r\nsrc/libawkward/util.cpp:      case dtype::complex128:\r\nsrc/libawkward/array/NumpyArray.cpp:        case util::dtype::complex128:\r\nsrc/libawkward/array/NumpyArray.cpp:          throw std::runtime_error(\"FIXME: complex128 to JSON\");\r\nsrc/libawkward/array/NumpyArray.cpp:            dtype_ == util::dtype::complex128  ||\r\nsrc/libawkward/array/NumpyArray.cpp:            rawother->dtype() == util::dtype::complex128  ||\r\nsrc/libawkward/array/NumpyArray.cpp:      else if (dtype_ == util::dtype::complex128  ||\r\nsrc/libawkward/array/NumpyArray.cpp:               rawother->dtype() == util::dtype::complex128) {\r\nsrc/libawkward/array/NumpyArray.cpp:        dtype = util::dtype::complex128;\r\nsrc/libawkward/array/NumpyArray.cpp:        dtype = util::dtype::complex128;\r\nsrc/libawkward/array/NumpyArray.cpp:      // to complex128\r\nsrc/libawkward/array/NumpyArray.cpp:      case util::dtype::complex128:\r\nsrc/libawkward/array/NumpyArray.cpp:        throw std::runtime_error(\"FIXME: merge to complex128 not implemented\");\r\nsrc/libawkward/array/NumpyArray.cpp:      case util::dtype::complex128:\r\nsrc/libawkward/array/NumpyArray.cpp:        throw std::runtime_error(\"FIXME: reducers on complex128\");\r\nsrc/libawkward/array/NumpyArray.cpp:      case util::dtype::complex128:\r\nsrc/libawkward/array/NumpyArray.cpp:        throw std::runtime_error(\"FIXME: sort for complex128 not implemented\");\r\nsrc/libawkward/array/NumpyArray.cpp:      case util::dtype::complex128:\r\nsrc/libawkward/array/NumpyArray.cpp:        throw std::runtime_error(\"FIXME: argsort for complex128 not implemented\");\r\nsrc/libawkward/array/NumpyArray.cpp:      case util::dtype::complex128:\r\nsrc/libawkward/array/NumpyArray.cpp:        throw std::runtime_error(\"FIXME: numbers_to_type for complex128 not implemented\");\r\nsrc/libawkward/array/NumpyArray.cpp:    case util::dtype::complex128:\r\nsrc/libawkward/array/NumpyArray.cpp:      throw std::runtime_error(\"FIXME: as_type for complex128 not implemented\");\r\n```\r\n\r\nI just looked at these search results in context and none of them need any work except the ones in NumpyArray.cpp. NumpyArray.cpp needs:\r\n\r\n   * a conversion of complex numbers to and from JSON: what would that be? Is there a standard, like `{\"real\": #, \"imag\": #}` or maybe `{\"r\": #, \"i\": #}`? Do other libraries converge on some convention?\r\n   * both `NumpyArray::merge` and `NumpyArray::numbers_to_type` call `kernel::NumpyArray_fill<X, Y>` (the latter does so indirectly), and this needs to be specialized to include complex cases in the `X` and `Y`. That goes into kernel functions named `awkward_NumpyArray_fill_toY_fromX`, which are described in more detail below.\r\n   * the reducers need complex cases, which is a straightforward extrapolation of the floating point ones: `count`, `count_nonzero` (implementations don't depend on the type; it's just a function signature), `sum`, `prod` (add the function signatures and the compiler does the work), `any`, `all` (have to determine whether a complex number is zero or not, but otherwise formulaic), `min`, `max`, `argmin`, `argmax` (can raise `std::invalid_argument(\"don't do that!\")` because complex numbers are unordered).\r\n   * there's a stub for sorting and arg-sorting complex numbers, but this can raise `std::invalid_argument(\"don't do that!\")` because complex numbers are unordered.\r\n\r\nKernel functions separate code that actually touches the arrays from all the rest of the codebase. They're a separate layer in the architecture:\r\n\r\n<img src=\"https://github.com/scikit-hep/awkward-1.0/raw/master/docs-img/diagrams/awkward-1-0-layers.png\" width=\"500\">\r\n\r\nand are, in fact, a separate shared library that can be dynamically loaded (`libawkward-cpu-kernels.so` and `libawkward-cuda-kernels.so`). The actual implementations of these functions (filling arrays and counting, summing, multiplying) are very simple and are templated by numerical type:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/029ae3991849d391733043db5e87389e333638fc/src/cpu-kernels/operations.cpp#L1136-L1146\r\n\r\nHowever, that `extern \"C\"` interface between the kernels layer and the C++ layer means that all of the kernel cases have to be given explicit names that dynamic library loaders can use to identify them. That means there's a lot of boilerplate, linking named functions to template specializations:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/029ae3991849d391733043db5e87389e333638fc/src/cpu-kernels/operations.cpp#L1147-L1162\r\n\r\nOn the other side of the `extern \"C\"` interface, we curb this insanity by gathering them back up again into template instantiations that are easier to use in the C++ codebase:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/029ae3991849d391733043db5e87389e333638fc/src/libawkward/kernel.cpp#L7073-L7095\r\n\r\n(so the number of differently named functions explodes only at the interface). Why do we need an `extern \"C\"` interface if it's causing all this boilerplate? First of all, because of that `ptr_lib == kernel::lib::cuda` case in the above code; it lets us swap CPU-bound implementations for CUDA implementations at runtime, based on whether we get the array from NumPy or from CuPy. (The project we're looking at here is just the CPU-bound implementation\u2014we're trying to auto-generate CUDA kernels from the CPU ones, and whatever you write would be part of that automatic translation.) Beyond swapping out implementations below the `extern \"C\"` interface, it also lets us swap out implementations above it, if someone's interested in building a Julia/Rust/Swift/whatever implementation of Awkward Array.\r\n\r\nThe only _extra_ issue that complex numbers introduces is that `std::complex` is not `extern \"C\"` compatible, so you'd have to `reinterpret_cast` a `std::complex<double>` array of length `n` as a `double` array of length `n * 2` before passing it through the interface. C++ types like `std::complex` can be used on both sides of the `extern \"C\"` interface, just not through it. Thus, you don't need to define type conversion or complex addition/multiplication inside the cpu-kernels, you can `reinterpret_cast` that `double` array of length `n * 2` as a `std::complex<double>` array of length `n` and then the compiler takes over.\r\n\r\nBut I apologize in advance about the boilerplate. I've been generating code with Emacs keyboard macros...\r\n",
  "created_at":"2020-08-14T14:48:33Z",
  "id":674110892,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDExMDg5Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-14T14:48:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"xref complex64 + complex128 support in Arrow: https://issues.apache.org/jira/browse/ARROW-638",
  "created_at":"2020-08-25T14:46:21Z",
  "id":680070470,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDA3MDQ3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T14:46:21Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"My reading of that is that it's legal in the format, but hasn't necessarily been implemented yet. The discussion in the comments are just ideas about how to do it in C++; maybe this is followed up on in another issue.\r\n\r\nNevertheless, I need to get complex numbers and datetimes into Awkward anyway. When C++ Arrow-Parquet is ready (maybe already), then we can take advantage of it, but these types would be useful in Awkward just for computation, regardless of whether they can be saved into Parquet files.",
  "created_at":"2020-08-25T14:53:49Z",
  "id":680075014,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDA3NTAxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T14:53:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @sjperkins! Let me know if you're still working on this (complex numbers in Awkward Array).\r\n\r\nIf not, I'll move it out of \"in progress\" and see when I can find some time to work on it.\r\n\r\nThanks!",
  "created_at":"2020-12-22T19:56:47Z",
  "id":749747173,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0OTc0NzE3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-22T19:56:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi @jpivarski.\n\nBriefly, I'm not working on this currently (I'm away from my laptop), but would like to reconnect and discuss what I'll be working in in the new year and it's relation to awkward.\n\nI should be back from 4th.\n\n",
  "created_at":"2020-12-27T19:15:53Z",
  "id":751505593,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MTUwNTU5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-27T19:15:53Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Here's the python file I was using to stamp out the headers, implementations and kernels. It may be useful.\r\n\r\n```python\r\nimport argparse\r\nfrom itertools import product\r\nfrom string import Template\r\n\r\nnp_to_cpp_map = {\r\n    'bool': 'bool',\r\n    'int8': 'int8_t',\r\n    'int16': 'int16_t',\r\n    'int32': 'int32_t',\r\n    'int64': 'int64_t',\r\n\r\n    'uint8': 'uint8_t',\r\n    'uint16': 'uint16_t',\r\n    'uint32': 'uint32_t',\r\n    'uint64': 'uint64_t',\r\n\r\n    'float32': 'float',\r\n    'float64': 'double',\r\n    'complex64': 'std::complex<float>',\r\n    'complex128': 'std::complex<double>'\r\n}\r\n\r\n\r\nkernel_fill_template = Template('''template <>\r\n    ERROR NumpyArray_fill<${FROM}, ${TO}>(\r\n        kernel::lib ptr_lib,\r\n        ${TO} *toptr,\r\n        int64_t tooffset,\r\n        const ${FROM} *fromptr,\r\n        int64_t length) {\r\n        if (ptr_lib == kernel::lib::cpu) {\r\n            return awkward_NumpyArray_fill_to${NPTO}_from${NPFROM}(\r\n                toptr,\r\n                tooffset,\r\n                fromptr,\r\n                length);\r\n        }\r\n        else if (ptr_lib == kernel::lib::cuda) {\r\n            throw std::runtime_error(\r\n                std::string(\"not implemented: ptr_lib == cuda_kernels \"\r\n                            \"for NumpyArray_fill<${TO}, ${FROM}>\"\r\n                            + FILENAME(__LINE__)));\r\n        }\r\n        else {\r\n            throw std::runtime_error(\r\n                std::string(\"unrecognized ptr_lib \"\r\n                            \"for NumpyArray_fill<${TO}, ${FROM}>\"\r\n                            + FILENAME(__LINE__)));\r\n        }\r\n    }''')\r\n\r\n\r\noph_fill_template = Template('''/// @param toptr outparam\r\n  /// @param tooffset inparam role: IndexedArray-index-offset\r\n  /// @param fromptr inparam role: NumpyArray-ptr\r\n  /// @param length inparam\r\n  EXPORT_SYMBOL struct Error\r\n  awkward_NumpyArray_fill_to${NPTO}_from${NPFROM}(\r\n    ${TO} * toptr,\r\n    int64_t tooffset,\r\n    const ${FROM} * fromptr,\r\n    int64_t length);''')\r\n\r\nopcpp_file_template = Template('''    ERROR\r\n    awkward_NumpyArray_fill_to${NPTO}_from${NPFROM}(${TO}* toptr,\r\n                                            int64_t tooffset,\r\n                                            const ${FROM}* fromptr,\r\n                                            int64_t length) {\r\n    return awkward_NumpyArray_fill(toptr, tooffset, fromptr, length);\r\n    }''')\r\n\r\n\r\ndef create_parser():\r\n    p = argparse.ArgumentParser()\r\n    p.add_argument(\"file\", choices=[\"oph\", \"opcpp\", \"kernel\"])\r\n    return p\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    args = create_parser().parse_args()\r\n\r\n    from_to = list(product(np_to_cpp_map.keys(), np_to_cpp_map.keys()))\r\n\r\n    for np_from_type, np_to_type in from_to:\r\n        cpp_from_type = np_to_cpp_map[np_from_type]\r\n        cpp_to_type = np_to_cpp_map[np_to_type]\r\n\r\n        if args.file == \"oph\":\r\n            template = oph_fill_template\r\n        elif args.file == \"opcpp\":\r\n            template = opcpp_file_template\r\n        elif args.file == \"kernel\":\r\n            template = kernel_fill_template\r\n        else:\r\n            raise ValueError(f\"Invalid output file {args.file}\")\r\n\r\n        fn = template.substitute(TO=cpp_to_type, FROM=cpp_from_type,\r\n                                 NPTO=np_to_type, NPFROM=np_from_type)\r\n\r\n\r\n\r\n        print(fn)\r\n```",
  "created_at":"2020-12-30T06:45:48Z",
  "id":752348789,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjM0ODc4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-30T06:45:48Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah, I would call this a bug. The Parquet file does change the type because everything in Parquet (and Arrow) is nullable\u2014absolutely every level of every structure. Ideally, the Parquet writer shouldn't be creating useless bitmaps, because the format is capable of saying, \"all values are valid\" without creating a bitmap of \"on\" bits (255), but that's an efficiency issue in pyarrow.\r\n\r\nSo you do get back arrays that in principle can contain None, but in practice don't. That's why `Out[4]` is wrong. If none of the `fJetPt` values were None, then none of their comparisons with 0 would be None, either. The rules of slicing and other mathematical operations with None values are defined such that Nones pass through\u2014if you don't have any Nones in, you shouldn't have any Nones out.\r\n\r\nI'll take a look at this. I might need to see how you write the Parquet file to reproduce it.",
  "created_at":"2020-08-17T11:50:50Z",
  "id":674835126,
  "issue":393,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDgzNTEyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T11:50:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Step 1: I've reproduced it:\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> original = ak.Array([0, 1, 2, 3, 4])\r\n>>> original > 1\r\n<Array [False, False, True, True, True] type='5 * bool'>\r\n>>> ak.to_parquet(original, \"tmp.parquet\")\r\n>>> fromparquet = ak.from_parquet(\"tmp.parquet\")\r\n>>> fromparquet\r\n<Array [0, 1, 2, 3, 4] type='5 * ?int64'>\r\n>>> fromparquet > 1\r\n<Array [None, None, None, None, None] type='5 * ?bool'>\r\n>>> fromparquet.layout\r\n<BitMaskedArray valid_when=\"true\" length=\"5\" lsb_order=\"true\">\r\n    <mask><IndexU8 i=\"[31]\" offset=\"0\" length=\"1\" at=\"0x7fef1cd93080\"/></mask>\r\n    <content><NumpyArray format=\"l\" shape=\"5\" data=\"0 1 2 3 4\" at=\"0x7fef1cd93040\"/></content>\r\n</BitMaskedArray>\r\n```\r\n\r\nso I won't need an example of how you wrote it to Parquet\u2014the above works. (Also, it's entirely possible it's not the Parquet-writing that's at fault, but something about the BitMaskedArray. I'll try making one directly.",
  "created_at":"2020-08-17T12:21:18Z",
  "id":674848957,
  "issue":393,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDg0ODk1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T12:21:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Step 2: yes, it's a problem with [BitMaskedArrays](https://awkward-array.readthedocs.io/en/latest/ak.layout.BitMaskedArray.html), which are usually only created in conversions from Arrow or Parquet (most Awkward operations use [IndexedOptionArrays](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedOptionArray.html), or [ByteMaskedArrays](https://awkward-array.readthedocs.io/en/latest/ak.layout.ByteMaskedArray.html) if a mask is appropriate at all).\r\n\r\n```python\r\n>>> content = ak.layout.NumpyArray(np.array([0, 1, 2, 3, 4]))\r\n>>> np.packbits([1, 1, 1, 1, 1, 0, 0, 0], bitorder=\"little\")\r\narray([31], dtype=uint8)\r\n>>> mask = ak.layout.IndexU8(np.packbits([1, 1, 1, 1, 1, 0, 0, 0], bitorder=\"little\"))\r\n>>> bitmasked = ak.layout.BitMaskedArray(mask, content, valid_when=True, length=5, lsb_order=True)\r\n>>> ak.Array(bitmasked)\r\n<Array [0, 1, 2, 3, 4] type='5 * ?int64'>\r\n>>> ak.Array(bitmasked) > 1\r\n<Array [None, None, None, None, None] type='5 * ?bool'>\r\n```\r\n\r\nBut as a ByteMaskedArray,\r\n\r\n```python\r\n>>> bitmasked.toByteMaskedArray()    # get exactly the same thing as a ByteMaskedArray\r\n<ByteMaskedArray valid_when=\"true\">\r\n    <mask><Index8 i=\"[1 1 1 1 1]\" offset=\"0\" length=\"5\" at=\"0x5631bbcf3c40\"/></mask>\r\n    <content><NumpyArray format=\"l\" shape=\"5\" data=\"0 1 2 3 4\" at=\"0x5631bbd00a40\"/></content>\r\n</ByteMaskedArray>\r\n>>> ak.Array(bitmasked.toByteMaskedArray())\r\n<Array [0, 1, 2, 3, 4] type='5 * ?int64'>\r\n>>> ak.Array(bitmasked.toByteMaskedArray()) > 1\r\n<Array [False, False, True, True, True] type='5 * ?bool'>\r\n```\r\n\r\nSo I'll look into what's happening with BitMaskedArrays in ufuncs (like `>`). It could be that `valid_when` is not checked or is reversed.",
  "created_at":"2020-08-17T12:29:30Z",
  "id":674852365,
  "issue":393,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDg1MjM2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T12:29:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks Jim - your quick responses are always appreciated! By the time I saw the notification and got around to starting on a reproducer, you already managed it :-) ",
  "created_at":"2020-08-17T12:33:05Z",
  "id":674853852,
  "issue":393,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDg1Mzg1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T12:33:05Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"(I find that if I don't deal with these things right away, other things intervene and I end up never getting back to them. If it's a bug, I don't want it to go unfixed.)",
  "created_at":"2020-08-17T12:35:04Z",
  "id":674854706,
  "issue":393,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDg1NDcwNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-17T12:35:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The PR above fixes it:\r\n\r\n```diff\r\nindex e3631dd..bd38f1b 100644\r\n--- a/src/libawkward/array/BitMaskedArray.cpp\r\n+++ b/src/libawkward/array/BitMaskedArray.cpp\r\n@@ -248,7 +248,7 @@ namespace awkward {\r\n       bytemask.data(),\r\n       mask_.data(),\r\n       mask_.length(),\r\n-      false,\r\n+      valid_when_,\r\n       lsb_order_);\r\n     util::handle_error(err, classname(), identities_.get());\r\n     return bytemask.getitem_range_nowrap(0, length_);\r\n```\r\n\r\nWhen all tests pass, it will be merged into master, but the next release could be days from now (trying to include a lot of GPU developments). In the meantime, conversions of BitMaskedArray into other types of option-type arrays (ByteMaskedArray, IndexedOptionArray, UnmaskedArray) do work, and those other array types correctly handle ufuncs like `>`. One easy way to force conversion is to multiply by 1:\r\n\r\n```python\r\n>>> a = awkward1.layout.BitMaskedArray(awkward1.layout.IndexU8(numpy.array([31], numpy.uint8)), awkward1.layout.NumpyArray(numpy.array([0, 1, 2, 3, 4])), valid_when=True, length=5, lsb_order=True)\r\n>>> a = awkward1.Array(awkward1.layout.BitMaskedArray(awkward1.layout.IndexU8(numpy.array([31], numpy.uint8)), awkward1.layout.NumpyArray(numpy.array([0, 1, 2, 3, 4])), valid_when=True, length=5, lsb_order=True))\r\n>>> a\r\n<Array [0, 1, 2, 3, 4] type='5 * ?int64'>\r\n>>> a.layout\r\n<BitMaskedArray valid_when=\"true\" length=\"5\" lsb_order=\"true\">\r\n    <mask><IndexU8 i=\"[31]\" offset=\"0\" length=\"1\" at=\"0x5605b5f724b0\"/></mask>\r\n    <content><NumpyArray format=\"l\" shape=\"5\" data=\"0 1 2 3 4\" at=\"0x5605b5f57470\"/></content>\r\n</BitMaskedArray>\r\n>>> a * 1\r\n<Array [0, 1, 2, 3, 4] type='5 * ?int64'>\r\n>>> (a * 1).layout\r\n<IndexedOptionArray64>\r\n    <index><Index64 i=\"[0 1 2 3 4]\" offset=\"0\" length=\"5\" at=\"0x5605b5fd7e10\"/></index>\r\n    <content><NumpyArray format=\"l\" shape=\"5\" data=\"0 1 2 3 4\" at=\"0x5605b5f9d7d0\"/></content>\r\n</IndexedOptionArray64>\r\n>>> (a * 1) > 1\r\n<Array [False, False, True, True, True] type='5 * ?bool'>\r\n```\r\n\r\nOr you could get the latest version from master. (The tests will be done in about 15 minutes.)",
  "created_at":"2020-08-17T13:13:23Z",
  "id":674873620,
  "issue":393,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDg3MzYyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T13:13:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Outstanding, thank you Jim! I'll check out the master after the PR merges",
  "created_at":"2020-08-17T13:34:45Z",
  "id":674885023,
  "issue":393,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDg4NTAyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-17T13:34:45Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's merged. Enjoy!",
  "created_at":"2020-08-17T13:36:10Z",
  "id":674885807,
  "issue":393,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NDg4NTgwNw==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2020-08-17T13:36:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's a bug.\r\n\r\nHere's a simpler reproducer:\r\n\r\n```python\r\nimport awkward1 as ak\r\nimport numpy as np\r\nimport numba as nb\r\n\r\nnumpyarray = ak.layout.NumpyArray(np.arange(100, 200, 10))\r\nindexedarray = ak.layout.IndexedArray64(ak.layout.Index64(np.array([0, 1, 2, 3, 8, 9])), numpyarray)\r\nlistoffsetarray = ak.layout.ListOffsetArray64(ak.layout.Index64(np.array([0, 0, 1, 2, 4, 4, 6])), indexedarray)\r\narray = ak.Array(listoffsetarray)\r\n\r\ndef reproduce(arrays):\r\n    i = 0\r\n    for values in arrays:\r\n        print(\"======== i =\", i)\r\n        parent_indices = values\r\n        print(\"parent_indices\", parent_indices)\r\n        j = 0\r\n        for p in parent_indices:\r\n            print(j, \":\", p)\r\n            assert p == parent_indices[j]  # How??\r\n            j += 1\r\n        i += 1\r\n\r\nprint(array.tolist())\r\nreproduce(array)\r\nnb.njit(reproduce)(array)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n[[], [100], [110], [120, 130], [], [180, 190]]\r\n======== i = 0\r\nparent_indices []\r\n======== i = 1\r\nparent_indices [100]\r\n0 : 100\r\n======== i = 2\r\nparent_indices [110]\r\n0 : 110\r\n======== i = 3\r\nparent_indices [120, 130]\r\n0 : 120\r\n1 : 130\r\n======== i = 4\r\nparent_indices []\r\n======== i = 5\r\nparent_indices [180, 190]\r\n0 : 180\r\n1 : 190\r\n======== i = 0\r\nparent_indices []\r\n======== i = 1\r\nparent_indices [100]\r\n0 : 100\r\n======== i = 2\r\nparent_indices [110]\r\n0 : 120\r\n======== i = 3\r\nparent_indices [120, 130]\r\n0 : 140\r\n1 : 150\r\n======== i = 4\r\nparent_indices []\r\n======== i = 5\r\nparent_indices [180, 190]\r\n0 : 93967057837984\r\n1 : 0\r\n```\r\n\r\nThe IndexedArray is essential\u2014without that, there's no bug. There's also no error if the IndexedArray is used alone, not nested within a ListOffsetArray. Apparently, the Numba implementation of the IndexedArray iterator (`for p in parent_indices`) is not correctly managing the indexing between ListOffsetArray and IndexedArray. I'll start there.\r\n\r\n(The `print` statement in a Numba-compiled function acquires the GIL and creates a Python context, so that's why it looked okay in `print`: the bug is in the Numba implementation, which isn't used in `print`.)",
  "created_at":"2020-08-18T14:48:15Z",
  "id":675524459,
  "issue":395,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTUyNDQ1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T14:48:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"A ListOffsetArray is being added twice. These even-simpler reproducers show it:\r\n\r\n```python\r\ndef reproduce(arrays):\r\n    for values in arrays:\r\n        print(\"values\", values)\r\n        j = 0\r\n        for p in values:\r\n            print(j, \":\", p)\r\n            assert p == values[j]  # How??\r\n            j += 1\r\n```\r\n\r\nwith\r\n\r\n```python\r\nnumpyarray = ak.layout.NumpyArray(np.arange(100, 200, 10))\r\nindexedarray = ak.layout.IndexedArray64(ak.layout.Index64(np.array([0, 1, 2, 3, 4, 5])), numpyarray)\r\nlistoffsetarray = ak.layout.ListOffsetArray64(ak.layout.Index64(np.array([0, 1, 5])), indexedarray)\r\narray = ak.Array(listoffsetarray)\r\n\r\nprint(array.tolist())\r\nprint(\"\\nNo Numba\")\r\nreproduce(array)\r\nprint(\"\\nYes Numba\")\r\nnb.njit(reproduce)(array)\r\n```\r\n\r\nprints\r\n\r\n```python\r\n[[100], [110, 120, 130, 140]]\r\n\r\nNo Numba\r\nvalues [100]\r\n0 : 100\r\nvalues [110, 120, 130, 140]\r\n0 : 110\r\n1 : 120\r\n2 : 130\r\n3 : 140\r\n\r\nYes Numba\r\nvalues [100]\r\n0 : 100\r\nvalues [110, 120, 130, 140]\r\n0 : 120\r\n1 : 130\r\n2 : 140\r\n3 : 150\r\n```\r\n\r\nbut\r\n\r\n```python\r\nnumpyarray = ak.layout.NumpyArray(np.arange(100, 200, 10))\r\nindexedarray = ak.layout.IndexedArray64(ak.layout.Index64(np.array([0, 1, 2, 3, 4, 5])), numpyarray)\r\nlistoffsetarray = ak.layout.ListOffsetArray64(ak.layout.Index64(np.array([0, 2, 5])), indexedarray)\r\narray = ak.Array(listoffsetarray)\r\n\r\nprint(array.tolist())\r\nprint(\"\\nNo Numba\")\r\nreproduce(array)\r\nprint(\"\\nYes Numba\")\r\nnb.njit(reproduce)(array)\r\n```\r\n\r\nprints\r\n\r\n```\r\n[[100, 110], [120, 130, 140]]\r\n\r\nNo Numba\r\nvalues [100, 110]\r\n0 : 100\r\n1 : 110\r\nvalues [120, 130, 140]\r\n0 : 120\r\n1 : 130\r\n2 : 140\r\n\r\nYes Numba\r\nvalues [100, 110]\r\n0 : 100\r\n1 : 110\r\nvalues [120, 130, 140]\r\n0 : 140\r\n1 : 150\r\n2 : 160\r\n```\r\n\r\nThe only difference between these two is that the first has ListOffsetArray offsets of `[0, 1, 5]` and the second has `[0, 2, 5]` (second list starts one later). In the first case, the second list starts `1` too late; in the second case, the second list starts `2` too late.",
  "created_at":"2020-08-18T14:59:41Z",
  "id":675531455,
  "issue":395,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTUzMTQ1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T14:59:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks Jim!",
  "created_at":"2020-08-20T08:54:59Z",
  "id":677469925,
  "issue":395,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NzQ2OTkyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-20T08:54:59Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Not \"as closures,\" but \"in closures.\"",
  "created_at":"2020-08-18T19:10:29Z",
  "id":675661582,
  "issue":397,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTY2MTU4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T19:10:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The `Sequence` base class was removed from `ak.Array` and `ak.ArrayBuilder` in cae8c26e50c252c48bc5342fa0e7365ab0e994af because we don't want `__contains__`, `index`, and `count` (see [abstract containers documentation](https://docs.python.org/3/library/collections.abc.html#collections-abstract-base-classes)). But (consulting the same documentation), we do want `Iterable` and `Sized`. The next commit adds them to the high-level `ak.Array`, `ak.ArrayBuilder` and to Numba-lowered `ak.Array`. (`ak.ArrayBuilder` can't be read in a Numba-lowered function.)",
  "created_at":"2020-08-18T20:51:14Z",
  "id":675713751,
  "issue":397,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NTcxMzc1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-18T20:51:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for reporting this issue, though I have to complain a bit about the reproducer: the relevant part is between the `sliced_arr`, which is valid, and the `loaded`, which is not. Building arrays from handmade layouts is supported (it's part of the public API), but it's exactly the sort of thing that leads to validity errors.\r\n\r\nHowever, in this case it really is a bug: there's an error in the writing of non-minimal categorical/dictionary data (what Awkward calls [IndexedArrays](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedArray.html)) to Parquet. Actually, I think it's a pyarrow bug and I'll be reporting it on their JIRA right after this.\r\n\r\nYour `label` is a categorical variable. Perhaps because it was loaded from partitioned data, the labels are not minimal: `\"OK\"` and `\"NOK\"` are the unique values, so only two are strictly needed, but the array has about ten of them. That's not wrong, just less efficient, and not much more inefficient (though if you're using the integers for equality checks, which one typically does with categorical data, but Awkward doesn't, then it would be wrong).\r\n\r\nHere's a small example:\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> import numpy as np\r\n>>> categories = ak.Array([\"one\", \"two\", \"three\", \"one\", \"two\", \"three\"])\r\n>>> index = ak.layout.Index32(np.array([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5], np.int32))\r\n>>> indexedarray = ak.layout.IndexedArray32(index, categories.layout)\r\n>>> original = ak.Array(indexedarray)\r\n>>> original\r\n<Array ['one', 'two', ... 'two', 'three'] type='12 * string'>\r\n>>> original.tolist()\r\n['one', 'two', 'three', 'one', 'two', 'three', 'one', 'two', 'three', 'one', 'two', 'three']\r\n```\r\n\r\nIt's non-minimal because we could have represented this with just three categories and an `index` of values in `[0, 3)`. This converts to and from Arrow without any problems:\r\n\r\n```python\r\n>>> roundtrip = ak.from_arrow(ak.to_arrow(original))\r\n>>> roundtrip.tolist()\r\n['one', 'two', 'three', 'one', 'two', 'three', 'one', 'two', 'three', 'one', 'two', 'three']\r\n\r\n>>> original.layout\r\n<IndexedArray32>\r\n    <index><Index32 i=\"[0 1 2 3 4 ... 1 2 3 4 5]\" offset=\"0\" length=\"12\" at=\"0x55edccd57930\"/></index>\r\n    <content><ListOffsetArray64>\r\n        <parameters>\r\n            <param key=\"__array__\">\"string\"</param>\r\n        </parameters>\r\n        <offsets><Index64 i=\"[0 3 6 11 14 17 22]\" offset=\"0\" length=\"7\" at=\"0x55edccdae190\"/></offsets>\r\n        <content><NumpyArray format=\"B\" shape=\"22\" data=\"111 110 101 116 119 ... 116 104 114 101 101\" at=\"0x55edccd3ded0\">\r\n            <parameters>\r\n                <param key=\"__array__\">\"char\"</param>\r\n            </parameters>\r\n        </NumpyArray></content>\r\n    </ListOffsetArray64></content>\r\n</IndexedArray32>\r\n\r\n>>> roundtrip.layout\r\n<IndexedArray32>\r\n    <index><Index32 i=\"[0 1 2 3 4 ... 1 2 3 4 5]\" offset=\"0\" length=\"12\" at=\"0x55edccd57930\"/></index>\r\n    <content><ListOffsetArray32>\r\n        <parameters>\r\n            <param key=\"__array__\">\"string\"</param>\r\n        </parameters>\r\n        <offsets><Index32 i=\"[0 3 6 11 14 17 22]\" offset=\"0\" length=\"7\" at=\"0x55edccdc8e60\"/></offsets>\r\n        <content><NumpyArray format=\"B\" shape=\"22\" data=\"111 110 101 116 119 ... 116 104 114 101 101\" at=\"0x55edccd3ded0\">\r\n            <parameters>\r\n                <param key=\"__array__\">\"char\"</param>\r\n            </parameters>\r\n        </NumpyArray></content>\r\n    </ListOffsetArray32></content>\r\n</IndexedArray32>\r\n>>> np.asarray(original.layout.index)\r\n\r\narray([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5], dtype=int32)\r\n>>> ak.to_list(original.layout.content)\r\n['one', 'two', 'three', 'one', 'two', 'three']\r\n\r\n>>> np.asarray(roundtrip.layout.index)\r\narray([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5], dtype=int32)\r\n>>> ak.to_list(roundtrip.layout.content)\r\n['one', 'two', 'three', 'one', 'two', 'three']\r\n```\r\n\r\nHowever, when pyarrow writes it to Parquet (using the output of `ak.to_arrow`), it correctly minimizes the set of categories but garbles the index (probably some interaction with Parquet's very weird \"definition levels\" and \"repetition levels\"):\r\n\r\n```python\r\n>>> ak.to_parquet(original, \"tmp.parquet\")\r\n>>> loaded = ak.from_parquet(\"tmp.parquet\")\r\n>>> loaded.layout\r\n<IndexedArray32>\r\n    <index><Index32 i=\"[0 1 2 3 0 ... 1 2 3 0 1]\" offset=\"0\" length=\"12\" at=\"0x7f25a7e00280\"/></index>\r\n    <content><ListOffsetArray32>\r\n        <parameters>\r\n            <param key=\"__array__\">\"string\"</param>\r\n        </parameters>\r\n        <offsets><Index32 i=\"[0 3 6 11]\" offset=\"0\" length=\"4\" at=\"0x7f25a7e00240\"/></offsets>\r\n        <content><NumpyArray format=\"B\" shape=\"11\" data=\"111 110 101 116 119 ... 116 104 114 101 101\" at=\"0x7f25a7e002c0\">\r\n            <parameters>\r\n                <param key=\"__array__\">\"char\"</param>\r\n            </parameters>\r\n        </NumpyArray></content>\r\n    </ListOffsetArray32></content>\r\n</IndexedArray32>\r\n>>> np.asarray(loaded.layout.index)\r\narray([0, 1, 2, 3, 0, 1, 1, 1, 2, 3, 0, 1], dtype=int32)    # <---- BAD!\r\n>>> ak.to_list(loaded.layout.content)\r\n['one', 'two', 'three']\r\n```\r\n\r\nThe `index` ought to be `[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]`. Even pyarrow, by itself, doesn't like it:\r\n\r\n```python\r\n>>> import pyarrow.parquet\r\n>>> table = pyarrow.parquet.read_table(\"tmp.parquet\")\r\n>>> table\r\npyarrow.Table\r\n: dictionary<values=string, indices=int32, ordered=0>\r\n>>> table.to_pydict()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pyarrow/table.pxi\", line 1587, in pyarrow.lib.Table.to_pydict\r\n  File \"pyarrow/table.pxi\", line 405, in pyarrow.lib.ChunkedArray.to_pylist\r\n  File \"pyarrow/array.pxi\", line 1144, in pyarrow.lib.Array.to_pylist\r\n  File \"pyarrow/scalar.pxi\", line 712, in pyarrow.lib.DictionaryScalar.as_py\r\n  File \"pyarrow/scalar.pxi\", line 701, in pyarrow.lib.DictionaryScalar.value.__get__\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 111, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowIndexError: tried to refer to element 3 but array is only 3 long\r\n```\r\n\r\nIn fact, it's possible to cause this error without using Awkward, just pyarrow (something I'll have to do to report it on the Apache Arrow JIRA).\r\n\r\n```python\r\n>>> import pyarrow as pa\r\n>>> import pyarrow.parquet\r\n>>> pa_array = pa.DictionaryArray.from_arrays(\r\n...     pa.array([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5]),\r\n...     pa.array([\"one\", \"two\", \"three\", \"one\", \"two\", \"three\"])\r\n... )\r\n>>> pa_array\r\n<pyarrow.lib.DictionaryArray object at 0x7f271befa4a0>\r\n\r\n-- dictionary:\r\n  [\r\n    \"one\",\r\n    \"two\",\r\n    \"three\",\r\n    \"one\",\r\n    \"two\",\r\n    \"three\"\r\n  ]\r\n-- indices:\r\n  [\r\n    0,\r\n    1,\r\n    2,\r\n    3,\r\n    4,\r\n    5,\r\n    0,\r\n    1,\r\n    2,\r\n    3,\r\n    4,\r\n    5\r\n  ]\r\n>>> pa_table = pa.Table.from_batches(\r\n...     [pa.RecordBatch.from_arrays([pa_array], [\"column\"])]\r\n... )\r\n>>> pa_table\r\npyarrow.Table\r\ncolumn: dictionary<values=string, indices=int64, ordered=0>\r\n>>> pyarrow.parquet.write_table(pa_table, \"tmp2.parquet\")\r\n>>> pa_loaded = pyarrow.parquet.read_table(\"tmp2.parquet\")\r\n>>> pa_loaded\r\npyarrow.Table\r\ncolumn: dictionary<values=string, indices=int32, ordered=0>\r\n>>> pa_loaded.to_pydict()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pyarrow/table.pxi\", line 1587, in pyarrow.lib.Table.to_pydict\r\n  File \"pyarrow/table.pxi\", line 405, in pyarrow.lib.ChunkedArray.to_pylist\r\n  File \"pyarrow/array.pxi\", line 1144, in pyarrow.lib.Array.to_pylist\r\n  File \"pyarrow/scalar.pxi\", line 712, in pyarrow.lib.DictionaryScalar.as_py\r\n  File \"pyarrow/scalar.pxi\", line 701, in pyarrow.lib.DictionaryScalar.value.__get__\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 111, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowIndexError: tried to refer to element 3 but array is only 3 long\r\n>>> pa.__version__\r\n'1.0.0'\r\n```\r\n\r\nSo it's a pyarrow bug. (Even if they say that non-minimal dictionaries are not valid, it shouldn't silently write the wrong thing into a Parquet file.)\r\n\r\nHowever, I think I'm going to do something about it on the Awkward side, anyway: we ought to have an explicit categorical type, just as we have a string type (as a high-level behavior on IndexedArray). Putting an IndexedArray into categorical form would mean minimizing it, and when it's minimized, we don't encounter the pyarrow bug. Conversion to Arrow and hence Parquet would turn minimized categoricals into Arrow/Parquet dictionaries and would flatten any other IndexedArray.\r\n\r\nI'll report updates on the pyarrow bug and Awkward categoricals here.",
  "created_at":"2020-08-19T15:35:51Z",
  "id":676500573,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjUwMDU3Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-19T15:35:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for moving it Jim.\r\n\r\nI think it is writing _and reading_ as changing the partitioning (?) will reverse the effect.\r\n\r\nRemove the partition here to:\r\n```python\r\ndf.to_parquet('nonpartitioned.parquet',\r\n              version='2.0',\r\n              data_page_version='2.0')\r\noriginal = ak.from_parquet('nonpartitioned.parquet')\r\n# same for pyarrow table = pq.read_table('nonpartitioned.parquet')\r\n...\r\n```\r\nwill invalidate the manipulations for sliced:\r\n```python\r\n...\r\n>>> ak.is_valid(sliced), ak.validity_error(sliced)\r\n(False,\r\n 'at layout.field(2) (ListArray64): stop[i] > len(content) at i=5040')\r\n```\r\n\r\nwhile \r\n```python\r\n>>> ak.is_valid(loaded), ak.validity_error(loaded)\r\n(True, None)\r\n```\r\n\r\nSo maybe also low level slicing or subsequently `is_valid()` does not invalidate something in the first place?",
  "created_at":"2020-08-19T15:36:28Z",
  "id":676500962,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjUwMDk2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T15:36:28Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"I've reported this as [ARROW-9801](https://issues.apache.org/jira/browse/ARROW-9801). Meanwhile, I'll look into fixing things on our side so that we don't hit this, and gain \"categorical\" as a distinct thing from IndexedArray.",
  "created_at":"2020-08-19T15:57:53Z",
  "id":676512835,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjUxMjgzNQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-19T15:57:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Labeled a \"feature request\" because this is not our bug. Specialized categorical data would be a new feature.\r\n\r\nNah, that's weird. The system (Awkward + Arrow) as it stands corrupts data. I should call it a bug. The categorical data type is motivated by making Parquet output not wrong. I'll label it as a bug.",
  "created_at":"2020-08-19T17:41:52Z",
  "id":676566627,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NjU2NjYyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-19T17:41:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This was accidentally closed before because I repurposed a PR to get some updates into a release. But it's really done now, in #403.\r\n\r\nThe fix involved creating a new high-level type: categorical. I had been saying that [IndexedArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedArray.html) nodes are equivalent to categorical data, but they don't guarantee that the their `content` (the categories) are unique, and uniqueness is important for some operations, as we've just seen. The categorical type is explicitly opt-in, since the uniqueness-construction can be expensive. (It scales as _n log(n)_, rather than _n_, and I implemented it with a tight Python loop that fills a dict, which is unusual\u2014nearly everything else is in compiled loops. If this type is useful, it can be lowered into the compiled layer, but doing so for the general case would be a big project.)\r\n\r\nYou probably want your `\"OK\"`/`\"NOK\"` data to be categorical. The function for that is `ak.to_categorical` (check its help string; its documentation will be on the website after I push to master). If the speed of this function is an issue, you can build it manually, since you're comfortable using layouts:\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> import numpy as np\r\n>>> original = ak.Array([\"OK\", \"OK\", \"NOK\", \"OK\", \"NOK\", \"OK\", \"NOK\"])\r\n>>> ak.is_categorical(original)\r\nFalse\r\n>>> original.layout\r\n<ListOffsetArray64>\r\n    <parameters>\r\n        <param key=\"__array__\">\"string\"</param>\r\n    </parameters>\r\n    <offsets><Index64 i=\"[0 2 4 7 9 12 14 17]\" offset=\"0\" length=\"8\" at=\"0x557bbe332760\"/></offsets>\r\n    <content><NumpyArray format=\"B\" shape=\"17\" data=\"79 75 79 75 78 ... 79 75 78 79 75\" at=\"0x557bbe2be9e0\">\r\n        <parameters>\r\n            <param key=\"__array__\">\"char\"</param>\r\n        </parameters>\r\n    </NumpyArray></content>\r\n</ListOffsetArray64>\r\n>>> categorical = ak.to_categorical(original)\r\n>>> ak.is_categorical(categorical)\r\nTrue\r\n>>> ak.categories(categorical)\r\n<Array ['OK', 'NOK'] type='2 * string'>\r\n>>> categorical.layout\r\n<IndexedArray64>\r\n    <parameters>\r\n        <param key=\"__array__\">\"categorical\"</param>\r\n    </parameters>\r\n    <index><Index64 i=\"[0 0 1 0 1 0 1]\" offset=\"0\" length=\"7\" at=\"0x557bbe2b0d80\"/></index>\r\n    <content><ListArray64>\r\n        <parameters>\r\n            <param key=\"__array__\">\"string\"</param>\r\n        </parameters>\r\n        <starts><Index64 i=\"[0 4]\" offset=\"0\" length=\"2\" at=\"0x557bbe2f59d0\"/></starts>\r\n        <stops><Index64 i=\"[2 7]\" offset=\"0\" length=\"2\" at=\"0x557bbe2f5a10\"/></stops>\r\n        <content><NumpyArray format=\"B\" shape=\"17\" data=\"79 75 79 75 78 ... 79 75 78 79 75\" at=\"0x557bbe2be9e0\">\r\n            <parameters>\r\n                <param key=\"__array__\">\"char\"</param>\r\n            </parameters>\r\n        </NumpyArray></content>\r\n    </ListArray64></content>\r\n</IndexedArray64>\r\n>>> manual_categories = ak.Array([\"OK\", \"NOK\"]).layout\r\n>>> manual_index = ak.layout.Index64(np.array([0, 0, 1, 0, 1, 0, 1], np.int64))\r\n>>> manual_categorical = ak.layout.IndexedArray64(manual_index, manual_categories, parameters={\"__array__\": \"categorical\"})\r\n>>> categorical2 = ak.Array(manual_categorical)\r\n>>> categorical2\r\n<Array ['OK', 'OK', 'NOK', ... 'OK', 'NOK'] type='7 * categorical[type=string]'>\r\n>>> ak.is_categorical(categorical2)\r\nTrue\r\n>>> ak.categories(categorical2)\r\n<Array ['OK', 'NOK'] type='2 * string'>\r\n>>> ak.to_list(categorical2)\r\n['OK', 'OK', 'NOK', 'OK', 'NOK', 'OK', 'NOK']\r\n```\r\n\r\nAnd if you know the categories explicitly, you can build the `manual_index` in a way that scales with _n_ and involves only compiled code:\r\n\r\n```python\r\n>>> manual_index_array = np.empty(len(original), dtype=np.int64)\r\n>>> manual_index_array[original == \"OK\"] = 0\r\n>>> manual_index_array[original == \"NOK\"] = 1\r\n>>> manual_index_array\r\narray([0, 0, 1, 0, 1, 0, 1])\r\n```\r\n\r\nThe thing that this explicitness buys us is that Awkward \u2194 Arrow conversion now has the following rule:\r\n\r\n   * If the Awkward IndexedArray is _labeled as categorical_, then it maps to and from Arrow dictionary encoding\u2014we have reason to believe that the categories are unique (we assume that they are constructed that way, which `ak.to_categorical` guarantees).\r\n   * If the Awkward IndexedArray is not labeled as categorical, then it gets \"evaluated\" (`array \u2192 array.content[array.index]`) before conversion to Arrow.\r\n\r\nAs long as the Awkward categorical arrays really do have unique categories, we are insensitive to ARROW-9801, and you can use Parquet files freely.\r\n\r\nIncidentally, I noticed that your data started as Pandas: if you're interested in writing `ak.from_pandas` (does not exist: [Pandas conversion is one-sided](https://awkward-array.org/how-to-convert-pandas.html)), then I could help you out with that. Now that we have a categorical type, we have something to convert categorical Pandas columns into. On my side, I need to enable datetime types and partitioned Parquet datasets, which would then cover a lot of ground.",
  "created_at":"2020-08-20T15:16:08Z",
  "id":677728199,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NzcyODE5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-20T15:16:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This looks great! I will try it with a dataset shortly.\r\n\r\nI'm interested in writing `ak.from_pandas`, kindly guide me through.",
  "created_at":"2020-08-20T17:57:32Z",
  "id":677812432,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NzgxMjQzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-20T17:57:32Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski Jim, I opened a draft pull request according to contribution guideline. Would appreciate your advises, you should be able to edit the PR comment.",
  "created_at":"2020-08-20T18:33:44Z",
  "id":677829432,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NzgyOTQzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-20T18:33:44Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"> I'm interested in writing `ak.from_pandas`, kindly guide me through.\r\n\r\nCool! The `ak.to_pandas` function is here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/83232704d7cd5770804f3ece75559cd3de59eaf3/src/awkward1/operations/convert.py#L3467-L3685\r\n\r\nthough some of the issues are different: Awkward \u2192 Pandas requires, in general, multiple DataFrames to be lossless, and so there's a lot of logic there to determine which DataFrame each column should go in to make a minimal set of DataFrames. For Pandas \u2192 Awkward, only one DataFrame is ever involved. Converting non-trivial indexes to Awkward structures is more straightforward than the reverse.\r\n\r\nThe best way to start would be to open a feature-request issue and propose some translation rules with examples: \"If given a DataFrame like X, we should make an Awkward Array like Y,\" which we can talk over before implementation. I have some ideas to start:\r\n\r\n   * Converting Pandas column types to Awkward types is a completely independent problem from handling indexes.\r\n   * If the DataFrame has a RangeIndex, the columns can simply be wrapped by a [RecordArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.RecordArray.html).\r\n   * If the DataFrame has an Int64Index, the above should be wrapped in an [IndexedOptionArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedOptionArray.html) to arrange the order such that `ak_array[i] == df.loc[i]`, and any missing values can be None (`-1` in the `index`).\r\n   * If the DataFrame has a MultiIndex, the above can be wrapped in layers of [ListOffsetArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html), one for each MultiIndex level.\r\n   * String-valued and datetime-valued indexes would simply have to become columns, since Awkward arrays can't be indexed by anything other than integers.\r\n\r\nThe complexity of this project is progressive; if you find you need to stop after RangeIndex or something, that would already be valuable and completing the index types would be a matter of layering on top of what you've done.\r\n\r\nThanks!",
  "created_at":"2020-08-20T18:34:47Z",
  "id":677829921,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NzgyOTkyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-20T18:34:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I just tried this with a dateset and `0.2.33` and I am still getting a `ValueError: in IndexedArray32 attempting to get 100, index[i] >= len(content)` on a categorical field when doing the following:\r\n```python\r\narray = ak.from_arrow(table)\r\nak.to_parquet(array, 'tmp.parquet')\r\nrec = ak.from_parquet('tmp.parquet')\r\n```\r\nwhere `array` is valid and the arrow `table` consists of:\r\n```python\r\n>>> table\r\npyarrow.Table\r\nlabel: dictionary<values=string, indices=int32, ordered=0>\r\n```\r\nI think I need to rebuild the categorical as you described as the data seems to be corrupt already. Rebuilding it manually will fix any side effects/corruptions of the data introduced by ARROW-9801.\r\n\r\nI will have a look into it later again \r\n\r\nEventhough `array.label` seems to be consistent data and doesn't seem to be corrupted as it's nested. So at least not before masking/slicing. The indexes do not make sense (at least to me; `len(array) == 1023`): \r\n```python\r\n>>> array.label.layout\r\n<IndexedArray64>\r\n    <parameters>\r\n        <param key=\"__array__\">\"categorical\"</param>\r\n    </parameters>\r\n    <index><Index64 i=\"[0 0 1 0 0 ... 24 24 24 24 24]\" offset=\"0\" length=\"1023\" at=\"0x55a45da19470\"/></index>\r\n    <content><IndexedOptionArray64>\r\n        <index><Index64 i=\"[0 1 2 3 4 ... 21 22 23 24 25]\" offset=\"0\" length=\"26\" at=\"0x55a45da18270\"/></index>\r\n        <content><ListArray64>\r\n            <parameters>\r\n                <param key=\"__array__\">\"string\"</param>\r\n            </parameters>\r\n            <starts><Index64 i=\"[0 2 5 7 10 ... 52 55 57 60 62]\" offset=\"0\" length=\"26\" at=\"0x55a451197eb0\"/></starts>\r\n            <stops><Index64 i=\"[2 5 7 10 12 ... 55 57 60 62 65]\" offset=\"0\" length=\"26\" at=\"0x55a459bdfb70\"/></stops>\r\n            <content><NumpyArray format=\"B\" shape=\"65\" data=\"79 75 78 79 75 ... 79 75 78 79 75\" at=\"0x55a45dc4bd80\">\r\n                <parameters>\r\n                    <param key=\"__array__\">\"char\"</param>\r\n                </parameters>\r\n            </NumpyArray></content>\r\n        </ListArray64></content>\r\n    </IndexedOptionArray64></content>\r\n</IndexedArray64>\r\n```\r\nI guess #404 would catch that.",
  "created_at":"2020-08-22T13:57:21Z",
  "id":678644417,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODY0NDQxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-22T13:57:21Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ok, a manual categorical fixed it.\r\nSo this can serve as a workaround (_before_ any data manipulation) for corrupted arrow/parquet data, until [ARROW-9801](https://issues.apache.org/jira/browse/ARROW-9801) is fixed:\r\n```python\r\nfixed_array = ak.with_field(array, manual_categorical, where='label')\r\n```\r\n\r\n(_as long as the content is consistent and categorical indexes are just incomplete but not completely corrupted_)",
  "created_at":"2020-08-22T14:11:06Z",
  "id":678645835,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODY0NTgzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-22T14:20:04Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, this doesn't fix any files created before the bug fix because it was a writing bug, but a reading bug.\r\n\r\nI don't know why the above workaround works\u2014I'd have to think more about it. It would be better to generate new files if possible\u2014that works make it easier to know what's going on.",
  "created_at":"2020-08-22T14:49:17Z",
  "id":678650041,
  "issue":400,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODY1MDA0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-22T14:49:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna One more thing that I hadn't previously considered: `\"char\"` and `\"byte\"` can be applied to NumpyArray or EmptyArray. Just as I don't think `\"string\"` and `\"bytestring\"` operations have been tested on RegularArray, I don't think `\"char\"` and `\"byte\"` have been tested on EmptyArray, but it should be allowed. Any operations that fail with an EmptyArray should just call its `toNumpyArray` method with `uint8` as the `dtype`. (If `EmptyArray::toNumpyArray` doesn't take a `dtype` argument\u2014I don't remember\u2014then it should choose `uint8` _if labeled_ as `\"char\"` or `\"byte\"`. The default for unlabeled arrays is `float64`, to agree with NumPy, but if it's a string, it should have a different default, just as its default in `EmptyArray::asslice` is `int64`, I believe.)\r\n\r\n... Which reminds me of another rule that the validity-checker should check: a NumpyArray labeled with `\"char\"` or `\"byte\"` must have `uint8` as the `dtype`, with `\"B\"` as the `format`. Functions that use strings make this assumption, so the validity-checker should ensure it.\r\n\r\n(No changes on the `\"categorical\"` type.)",
  "created_at":"2020-11-25T16:47:20Z",
  "id":733823324,
  "issue":404,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzgyMzMyNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-25T16:47:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"... And another one: if an EmptyArray is labeled `\"string\"` or `\"bytestring\"`, it should be allowed, and then its concrete realization would be an empty ListOffsetArray of NumpyArray of `uint8`. However, `EmptyArray::toNumpyArray` shouldn't return this because the method name (and return type) would be wrong. I guess this case doesn't have a way to generate a concrete realization.\r\n\r\nBut the important thing for this issue is that EmptyArrays are allowed to be `\"char\"`, `\"byte\"`, `\"string\"`, or `\"bytestring\"`, and `EmptyArray::toNumpyArray` generates the right thing if it's `\"char\"` or `\"byte\"` (raising an exception in the other cases).",
  "created_at":"2020-11-25T16:55:19Z",
  "id":733827929,
  "issue":404,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzgyNzkyOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-25T16:55:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You're quicker than me: instead of opening an issue, this PR can serve as the place to discuss. (Better to have only one place, anyway.)\r\n\r\n> I'm interested in writing `ak.from_pandas`, kindly guide me through.\r\n\r\nCool! The `ak.to_pandas` function is here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/83232704d7cd5770804f3ece75559cd3de59eaf3/src/awkward1/operations/convert.py#L3467-L3685\r\n\r\nthough some of the issues are different: Awkward \u2192 Pandas requires, in general, multiple DataFrames to be lossless, and so there's a lot of logic there to determine which DataFrame each column should go in to make a minimal set of DataFrames. For Pandas \u2192 Awkward, only one DataFrame is ever involved. Converting non-trivial indexes to Awkward structures is more straightforward than the reverse.\r\n\r\nThe best way to start would be to open a feature-request issue and propose some translation rules with examples: \"If given a DataFrame like X, we should make an Awkward Array like Y,\" which we can talk over before implementation. I have some ideas to start:\r\n\r\n   * Converting Pandas column types to Awkward types is a completely independent problem from handling indexes.\r\n   * If the DataFrame has a RangeIndex, the columns can simply be wrapped by a [RecordArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.RecordArray.html).\r\n   * If the DataFrame has an Int64Index, the above should be wrapped in an [IndexedOptionArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedOptionArray.html) to arrange the order such that `ak_array[i] == df.loc[i]`, and any missing values can be None (`-1` in the `index`).\r\n   * If the DataFrame has a MultiIndex, the above can be wrapped in layers of [ListOffsetArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html), one for each MultiIndex level.\r\n   * String-valued and datetime-valued indexes would simply have to become columns, since Awkward arrays can't be indexed by anything other than integers.\r\n\r\nThe complexity of this project is progressive; if you find you need to stop after RangeIndex or something, that would already be valuable and completing the index types would be a matter of layering on top of what you've done.\r\n\r\nThanks!\r\n\r\n_Originally posted by @jpivarski in https://github.com/scikit-hep/awkward-1.0/issues/400#issuecomment-677829921_",
  "created_at":"2020-08-20T18:36:22Z",
  "id":677830627,
  "issue":405,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3NzgzMDYyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-20T18:36:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know when you get a chance to think about this. It's okay if it has to wait.\r\n\r\nThanks!",
  "created_at":"2020-08-26T18:55:11Z",
  "id":681061735,
  "issue":405,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTA2MTczNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T18:55:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sorry, I haven\u2018t gotten to it yet. I only looked into your ideas and compiled a few rough examples.\r\nI\u2018ll update here when I have a more complete list of proposals as suggested. I hope within the next few days.",
  "created_at":"2020-08-26T21:20:39Z",
  "id":681130414,
  "issue":405,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTEzMDQxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T21:20:39Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @drahnreb! It has been a while since you started this PR, and pretty soon, I'll be applying a name change to the whole Awkward Array codebase: all instances of \"`awkward1`\" will become \"`awkward`\" ([see details](https://github.com/scikit-hep/awkward-1.0/wiki/Name-change-procedure)). Since there are no changes here, perhaps you want to close this now and reopen a new PR after the name change? That name change is scheduled for Nov 30/Dec 1.",
  "created_at":"2020-11-23T21:36:50Z",
  "id":732440909,
  "issue":405,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMjQ0MDkwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-23T21:36:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@drahnreb, feel free to reopen this, but merge to master as a first step because a lot has changed! Or equivalently, open a new PR.",
  "created_at":"2020-11-30T14:20:13Z",
  "id":735814111,
  "issue":405,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNTgxNDExMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-30T14:20:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> @drahnreb, feel free to reopen this, but merge to master as a first step because a lot has changed! Or equivalently, open a new PR.\r\n\r\nSorry for the late reply. Will re-/open a new PR after your transition and with concrete translation rules and examples.",
  "created_at":"2020-11-30T14:28:11Z",
  "id":735818745,
  "issue":405,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNTgxODc0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-30T14:28:11Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"This seems like a safe change and I'd accept it, but all the MacOS builds are failing? That doesn't sound like it could be related. I'm going to launch a new run of tests on master to see if this is a general problem.",
  "created_at":"2020-08-21T12:17:52Z",
  "id":678260858,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI2MDg1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T12:17:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"MacOS failed because skbiuld (Scikit-Build) is not installed. We don't require it/don't use it. It was a thought while we were setting things up, but it didn't work for us, so we didn't use it. Does this seem familiar to you? (Is it in this branch and I just didn't see it or something?)",
  "created_at":"2020-08-21T12:25:12Z",
  "id":678263583,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI2MzU4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T12:25:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I didn't do anything with `skbuild` in this branch. If tests on master are passing, but not here, it is weird..",
  "created_at":"2020-08-21T12:26:20Z",
  "id":678263993,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI2Mzk5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T12:26:20Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"No, the tests on master didn't pass. In fact, I manually ran two tests on master of exactly the same commit (the one that says, \"Fix typo identified...\"): yesterday at 10:22am and today at 7:19am (my time). The second one failed in MacOS. Something external has changed.",
  "created_at":"2020-08-21T12:51:39Z",
  "id":678274047,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI3NDA0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T12:51:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It fails when installing the pip package for [cmake](https://pypi.org/project/cmake/), which expects [scikit-build](https://pypi.org/project/scikit-build/) to be present, and it isn't (on MacOS, at least). I think the cmake pip maintainers may be missing a dependency in their requirements list.\r\n\r\nI'll try adding it explicitly and then post a bug report if that turns out to be it.",
  "created_at":"2020-08-21T12:58:22Z",
  "id":678276852,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI3Njg1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T12:58:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It might be related to this:\r\n\r\nhttps://github.com/scikit-build/cmake-python-distributions/issues/103\r\n\r\nbut that was months ago. Whatever happened to us happened in the last 21 hours.",
  "created_at":"2020-08-21T12:59:59Z",
  "id":678277539,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI3NzUzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T12:59:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nope. That didn't do it.",
  "created_at":"2020-08-21T13:06:41Z",
  "id":678280549,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI4MDU0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T13:06:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@henryiii helped me find it on Slack: the latest cmake package is broken (no wheel for MacOS Python 3). For now, at least, I put in a requirement excluding cmake 3.18.2 (we get 3.18.0 instead) and the builds are passing. When this branch passes, I'll merge it. Meanwhile, I'll keep an eye on cmake and remove our kludge when they're ready.",
  "created_at":"2020-08-21T13:26:55Z",
  "id":678289969,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI4OTk2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T13:26:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Bah! I forgot to pull the change that fixed this in from master. One moment.",
  "created_at":"2020-08-21T13:32:46Z",
  "id":678292704,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI5MjcwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T13:32:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`PIP_ONLY_BINARY=\"cmake\"` will fix this properly.",
  "created_at":"2020-08-21T13:41:03Z",
  "id":678296585,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI5NjU4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T13:41:03Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"(Thanks to @YannickJadoul who originally pointed me at that tip)",
  "created_at":"2020-08-21T13:44:56Z",
  "id":678298309,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODI5ODMwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T13:44:56Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"We should move any further discussion of the cmake issues to #407, where I'm trying the `PIP_ONLY_BINARY` method in c21c55a6701c9c04c0936d72da18acfe5a8533ff .",
  "created_at":"2020-08-21T13:54:30Z",
  "id":678302786,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODMwMjc4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T13:54:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This PR (about documentation) is going to pass all of its tests soon and I'll be merging it.",
  "created_at":"2020-08-21T13:55:03Z",
  "id":678303071,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODMwMzA3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T13:55:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(Incidentally, the environment variable method in #407 worked. I'll be merging that, too, so we'll have a proper way of dealing with the incomplete cmake deployment.)",
  "created_at":"2020-08-21T14:02:19Z",
  "id":678306588,
  "issue":406,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODMwNjU4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T14:02:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, it's not a full block of Nones, it's _any_ Nones!\r\n\r\n```python\r\n>>> ak.argmin(ak.Array([[2.2, 1.1], [3.3], [2.2, 1.1]]), axis=-1)\r\n<Array [1, 0, 1] type='3 * ?int64'>\r\n>>> ak.argmin(ak.Array([[2.2, 1.1], [None, 3.3], [2.2, 1.1]]), axis=-1)\r\n<Array [1, 0, 0] type='3 * ?int64'>\r\n>>> ak.argmin(ak.Array([[2.2, 1.1], [None, None, 3.3], [2.2, 1.1]]), axis=-1)\r\n<Array [1, 0, -1] type='3 * ?int64'>\r\n>>> ak.argmin(ak.Array([[2.2, 1.1], [None, None, None, 3.3], [2.2, 1.1]]), axis=-1)\r\n<Array [1, 0, -2] type='3 * ?int64'>\r\n```\r\n\r\nI'm going to take a look now. I have an idea of what's going on\u2014the Nones have to be removed; it looks like the index shift represented by removing them has to be put back in.\r\n\r\nThanks for reporting!",
  "created_at":"2020-08-21T14:40:27Z",
  "id":678326054,
  "issue":408,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODMyNjA1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T14:40:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In fact, it can be very simple:\r\n\r\n```python\r\n>>> ak.argmin(ak.Array([3.3, 2.2, 1.1]), axis=0)\r\n2\r\n>>> ak.argmin(ak.Array([3.3, 2.2, None, 1.1]), axis=0)\r\n2\r\n>>> ak.argmin(ak.Array([3.3, 2.2, None, None, 1.1]), axis=0)\r\n2\r\n>>> ak.argmin(ak.Array([3.3, 2.2, None, None, None, 1.1]), axis=0)\r\n2\r\n```",
  "created_at":"2020-08-21T15:15:06Z",
  "id":678344778,
  "issue":408,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODM0NDc3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T15:15:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's because the following case wasn't tested and needs a correction for the index offset caused by removing None values:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e4932ec4c5c57b5731e609f1d6fbfdc4a439e7ea/src/libawkward/array/IndexedArray.cpp#L1926-L1928\r\n\r\n(A code coverage tool wouldn't have caught it because it _was_ tested for another reducer, `ak.prod`, but this bug is particular to `ak.argmin` and `ak.argmax` because these are the only two that return index positions.)\r\n\r\nThe other branch (the `else` just below the `if` quoted above) is tested and correct. That other branch applies to Nones from missing lists, rather than missing values in lists, as your case illustrates.",
  "created_at":"2020-08-21T15:50:38Z",
  "id":678361836,
  "issue":408,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODM2MTgzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T15:50:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, it's more than just that. Here's an example without any Nones:\r\n\r\n```python\r\n>>> ak.min(ak.Array([[1.1, 5.5], [4.4], [2.2, 3.3, 0.0, -10]]), axis=0)\r\n<Array [1.1, 3.3, 0, -10] type='4 * ?float64'>\r\n>>> ak.argmin(ak.Array([[1.1, 5.5], [4.4], [2.2, 3.3, 0.0, -10]]), axis=0)\r\n<Array [0, 1, 0, 0] type='4 * ?int64'>\r\n```\r\n\r\n`ak.min` does the right thing for `axis=0`: it takes `1.1` from the first list (it's smaller than `4.4` and `2.2`), it takes `3.3` from the second list (it's smaller than `5.5`), and it takes `0.0` and `-10` from the second list (nothing to compare).\r\n\r\nBy the same logic, `ak.argmin` should be `[0, 2, 2, 2]`. It's not getting the message that non-existent values in the earlier lists shift the baseline: the result for the first slot, `0`, is okay but the result for the second slot, `1`, is wrong because it's not counting the `[4.4]` list, which doesn't have a second slot, and the results for the third and forth slots, `0, 0`, are wrong because it's not counting the `[1.1, 5.5]` and `[4.4]` lists, which both don't have third and fourth slots.\r\n\r\nSo this is a more fundamental thing about adjusting index positions for missing values (due to lists being too short) than Nones specifically. I'm going to fix the more fundamental bug first because the one you noticed with Nones might just be a byproduct of that. Even if not, it's better to start adjusting from a correct baseline.",
  "created_at":"2020-08-21T17:27:38Z",
  "id":678403354,
  "issue":408,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODQwMzM1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T17:27:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Although this was a bug\u2014wrong behavior\u2014fixing it was something like adding a new feature: both the error you identified (with Nones) and the error I identified (with variable-length lists) were due to it missing a correction for index positions. All reducers have None and variable-length list aware placement of output, but argmin/argmax additionally needs to have its values updated because they refer to positions, which are affected by Nones/jaggedness.\r\n\r\nThat's done in #410 (I'll be merging soon), and your example works now (among other things).",
  "created_at":"2020-08-22T08:58:55Z",
  "id":678615913,
  "issue":408,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODYxNTkxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-22T08:58:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Oh wow, I didn't know the error went so deep! Thanks for the super speedy reply and fix, especially since it was nastier of a bug than expected!",
  "created_at":"2020-08-22T09:10:33Z",
  "id":678617039,
  "issue":408,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODYxNzAzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-22T09:10:33Z",
  "user":"MDQ6VXNlcjEyNDU1NDc5"
 },
 {
  "author_association":"MEMBER",
  "body":"It was more along the lines of an incomplete implementation. I had tests with jagged arrays and missing values, but this error snaked through the cracks between them. I didn't realize how much there was left to do.\r\n\r\nBut argmin and max are important\u2014I couldn't leave it wrong. This is being deployed as version 0.2.33 right now.",
  "created_at":"2020-08-22T09:14:03Z",
  "id":678617345,
  "issue":408,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODYxNzM0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-22T09:14:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"There's already a copy of all the libraries inside the Python module's directory, which no one can complain about\u2014I can put any data files in there. Maybe the header files should go in there, too, and then maybe\r\n\r\n```bash\r\npython -m awkward1 --cflags --libs\r\n```\r\n\r\ncould output\r\n\r\n```bash\r\n-std=c++11 -I/path/to/awkward1/include -L/path/to/awkward1/lib -lawkward -lawkward-cpu-kernels\r\n```\r\n\r\nlike a `XYZ-config` executable?\r\n",
  "created_at":"2020-08-21T21:18:48Z",
  "id":678515439,
  "issue":411,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODUxNTQzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T21:18:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This might be the best solution. There is a dedicated \"headers\" slot in wheels (though it tends to go to `<>/include/site/python3.6`), I'm not sure if that's really used much; unless you hack it, it can't do nested folders \ud83e\udd26 ).\r\n\r\nAwkward1's CMake doesn't seem to support exporting targets for `find_package` `CONFIG` or use as a submodule, is that intended eventually? That might affect where we want to put libraries.\r\n\r\n(I should become more knowledgeable about all this within a week or so)",
  "created_at":"2020-08-21T21:24:54Z",
  "id":678523811,
  "issue":411,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODUyMzgxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T21:25:14Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Don't know if this is still valid, but: https://blog.ionelmc.ro/presentations/packaging/#slide:16\r\n",
  "created_at":"2020-08-21T21:25:54Z",
  "id":678524137,
  "issue":411,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODUyNDEzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-21T21:25:54Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"On all three systems (Linux, MacOS, Windows), the header files, static libraries, and shared libraries are now strictly contained within the Python package. I assume I can put anything that I want in there, and uninstalling is a one-directory cleanup. That complicates dynamically linking against `libawkward.so` because now a path has to be established, but that's the price.\r\n\r\nAwkward now has a config tool:\r\n\r\n```bash\r\n$ python -m awkward1.config --cflags --libs\r\n-std=c++11 -I/path/to/awkward1/include -L/path/to/awkward1 -lawkward -lawkward-cpu-kernels\r\n```\r\n\r\nand a description of how to use it in context is in the [dependent-project documentation](https://github.com/scikit-hep/awkward-1.0/tree/master/dependent-project).\r\n\r\nFurthermore, the dependent-project is now a formal part of the tests for Linux and MacOS. We'll know if downstream linking ever gets broken. Also, those [three tests](https://github.com/scikit-hep/awkward-1.0/blob/18921bba890c69414fadf0507736ea2a3fb48d38/.ci/azure-buildtest-awkward.yml#L321-L355) act as a demonstration of how to use it.",
  "created_at":"2020-09-08T02:10:45Z",
  "id":688577382,
  "issue":411,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4ODU3NzM4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-08T02:10:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I had to work with generate-kernels.py and generate-tests.py last night when I added some kernels (missing cases in argmin/argmax). I hadn't noticed before how deeply nested the code is. The rule of breaking things up into smaller functions is especially true in Python because of the intention-is-nesting. It can be very hard to know what depth you're in if it gets to be more than 5.",
  "created_at":"2020-08-22T15:56:39Z",
  "id":678657485,
  "issue":412,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODY1NzQ4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-22T15:56:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Actually breaking up some of these into smaller functions shouldn't be too hard. I'll add that to my TODO list (for a later PR). ",
  "created_at":"2020-08-22T15:58:38Z",
  "id":678657730,
  "issue":412,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODY1NzczMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-22T17:00:24Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I had edited the CI for just macOS and not Linux and Windows, causing the tests to fail. All the tests pass now.",
  "created_at":"2020-08-24T13:33:36Z",
  "id":679128923,
  "issue":414,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3OTEyODkyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-24T13:33:36Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"I've run into a few more cases of structures that can be converted to Arrow but not from Arrow to Parquet (https://github.com/scikit-hep/awkward-1.0/blob/59499202e539b283d29419965efbe6f78676371a/tests/test_0341-parquet-reader-writer.py#L237-L247). The way to fix them is to fix the pyarrow/Arrow code, so I'd think these things should be forwarded to the Apache Arrow JIRA.\r\n\r\nBut since they're not bugs\u2014the error messages admit they're missing features\u2014maybe they should be posted elsewhere? I wonder if Arrow or Apache has a missing feature significance site, where we can upvote the priority of features that are important to us? The bugs I've reported so far have been true bugs: segfaults, mostly.\r\n\r\nI'm not sure how to label this issue here, since it is a problem, but not one that can be dealt with in this codebase. Maybe \"not a bug.\"",
  "created_at":"2020-08-25T13:39:36Z",
  "id":680031384,
  "issue":415,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDAzMTM4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T14:50:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> But since they're not bugs\u2014the error messages admit they're missing features\u2014maybe they should be posted elsewhere?\r\n\r\nIndeed, I can drive the complex number issue with the Arrow/Parquet projects.\r\n\r\nHowever, I played with the reproducer some more, and directly placing float32 numpy arrays (as opposed to wrapping them with Awkward Arrays) in the builder works:\r\n\r\n```python\r\n@pytest.mark.parametrize(\"dtype\", [np.float32)\r\ndef test_awkward_arrays_pandas(tmp_path, dtype):\r\n    ak = pytest.importorskip(\"awkward1\")\r\n    pa = pytest.importorskip(\"pyarrow\")\r\n    fastparquet = pytest.importorskip(\"fastparquet\")\r\n\r\n\r\n    builder = ak.ArrayBuilder()\r\n    A = np.array([0, 1, 2], dtype=dtype)\r\n    B = np.array([0, 1], dtype=dtype)\r\n\r\n    with builder.list():\r\n        builder.append(A)\r\n\r\n    with builder.list():\r\n        builder.append(A)\r\n\r\n    with builder.list():\r\n        pass\r\n\r\n    with builder.list():\r\n        builder.append(B)\r\n\r\n    ak.to_parquet(builder.snapshot(), tmp_path / \"data.parquet\")\r\n```\r\n\r\nI don't clearly understand the difference between the wrapped and plain numpy case.",
  "created_at":"2020-08-25T14:35:49Z",
  "id":680064150,
  "issue":415,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDA2NDE1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T14:41:44Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Aha! what's different here is that the first case produced a union (of floats and floats):\r\n\r\n```python\r\n>>> builder.snapshot()\r\n<Array [[0, 1, 2], [0, 1, 2], [], [0, 1]] type='4 * var * union[float32, float32]'>\r\n```\r\n\r\nand the second case did not:\r\n\r\n```python\r\n>>> builder.snapshot()\r\n<Array [[[0, 1, 2]], [[0, 1, ... [], [[0, 1]]] type='4 * var * var * float64'>\r\n```\r\n\r\nThat's because when you `ArrayBuilder.append` an Awkward Array, it uses that to make references into the existing array so that complex structures can be added quickly (as a view). Applying `ArrayBuilder.append` to a non-Awkward sequence, such as a NumPy array, makes it fall back into iterating over the data (as a copy). Since `A` and `B` are different structures, they can't be referenced in the same array without being a union, though a \"union of type T and type T\" looks a little silly, the work this is doing is to allow discontiguous arrays to be interleaved in the same logical array without copying their contents.\r\n\r\nWe can flatten a \"union of type T and type T\" into just \"type T\" by invoking a `simplify` operation on the union (an internal step applied to some operations; there isn't an `ak.simplify`). However, that undermines the goal of having `ArrayBuilder.snapshot` be an _O(1)_ operation. There's a trade-off between wanting the structure of the snapshot be usable in more ways\u2014some Arrow conversions can't deal with unions and Awkward arrays in Numba can't deal with unions\u2014and having the operation be fast. I'm not sure whether I should make the `simplify` happen automatically in an `ArrayBuilder.snapshot` involving unions or to make that a user choice. (It would be hard to discover, since the symptom would be something like \"Arrow can't convert\" or \"Numba can't compile,\" which is difficult to trace back unless you see that it's a union and that raises the appropriate red flags.)\r\n\r\nIn addition, the structure is wrong. I think they should both be `4 * var * var *`. In the second case, it's `float64` because the iteration went through Python, which doesn't have `float32` as a type.",
  "created_at":"2020-08-25T15:16:49Z",
  "id":680089540,
  "issue":415,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDA4OTU0MA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-25T15:16:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The most flagrant error is now fixed: `ArrayBuilder.append(some sequence)` appends the whole sequence, including its begin-list and end-list. Somehow, this has been correct in Numba but not in Python, which is not the way it usually goes.\r\n\r\nOn the original issue about the \"union of T and T\", I'm going to add an internal `simplify` operation, so that any mergable unions get merged. This adds a performance penalty to `snapshot` in cases where it needs to be done (like yours) but not in other cases. Also, I found one other case of a `snapshot` that isn't _O(1)_, so I can't say it's a rule.",
  "created_at":"2020-08-25T16:14:13Z",
  "id":680124316,
  "issue":415,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDEyNDMxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T16:14:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"When #417 is done, it will make your case writable to Parquet. Appending from two different arrays will produce a simple type, like `int64` below, rather than `union[int64, int64]`. This is writable to Parquet. (The reason the type has options in it when read back is because Arrow and Parquet data are nullable at every level.)\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> A = ak.Array([0, 1, 2])\r\n>>> B = ak.Array([0, 1])\r\n>>> builder = ak.ArrayBuilder()\r\n>>> builder.append(A)\r\n>>> builder.append(B)\r\n>>> result = builder.snapshot()\r\n>>> result\r\n<Array [[0, 1, 2], [0, 1]] type='2 * var * int64'>\r\n>>> ak.to_parquet(result, \"tmp.parquet\")\r\n>>> ak.from_parquet(\"tmp.parquet\")\r\n<Array [[0, 1, 2], [0, 1]] type='2 * option[var * ?int64]'>\r\n```\r\n\r\nReferencing preexising data was a late addition to ArrayBuilder, and while the ArrayBuilder model without referencing should never need to `simplify`, you've found a case where referencing allows `union[T, T]`, and that's not good. Another case that you didn't hit, but could have, is `option[option[T]]`, which should also be `simplified`.\r\n\r\nBy the way, \"fastparquet\" is not needed/not used in the serialization to Parquet. That's a Parquet reader/writer implemented in Numba; pyarrow uses one implemented in C++.",
  "created_at":"2020-08-25T16:34:24Z",
  "id":680135673,
  "issue":415,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDEzNTY3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T16:34:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for addressing this and for the clear explanations!\r\n\r\n> By the way, \"fastparquet\" is not needed/not used in the serialization to Parquet. That's a Parquet reader/writer implemented in Numba; pyarrow uses one implemented in C++.\r\n\r\nThis is useful to know. I'm still getting to grips with the Awkward/Arrow/Parquet ecosystem so these pointers are helpful.",
  "created_at":"2020-08-26T07:29:59Z",
  "id":680710661,
  "issue":415,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDcxMDY2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T07:29:59Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for catching this! Fortran-order NumpyArrays are supported, since they come for free with NumPy's `shape`, `strides` mechanism, which is needed for any sliced NumPy arrays. (This `shape`, `strides` thing is very clever! Worth reading about if you don't already know about it.)\r\n\r\nAs seen in PR #420, there were two errors: the calculation of `NumpyArray::bytelength`\r\n\r\n```diff\r\n@@ -422,7 +422,11 @@ namespace awkward {\r\n       return itemsize_;\r\n     }\r\n     else {\r\n-      return shape_[0]*strides_[0];\r\n+      ssize_t out = itemsize_;\r\n+      for (size_t i = 0;  i < shape_.size();  i++) {\r\n+        out += (shape_[i] - 1)*strides_[i];\r\n+      }\r\n+      return out;\r\n     }\r\n   }\r\n```\r\n\r\nwhich showed up in print-outs of the NumpyArray layout, and `NumpyArray::carry`, which is used to propagate slices down in `__getitem__`. Handling the full non-contiguous case in `carry` would be complicated; I decided to punt for now and just ensure that the array is C-contiguous.\r\n\r\nThere are quite a few spots like that, in which non-contiguous arrays are allowed as inputs, but they aren't handled efficiently: they're first converted to contiguous and then the operation proceeds. The plan is to get everything working first and optimize hotspots later (i.e. not this year, unless they're particularly egregious). Therefore, if you're using Fortran-order arrays for efficiency, that's not a good strategy with Awkward Array.\r\n\r\n---------------\r\n\r\n**Before:**\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> import numpy as np\r\n>>> np_content = np.asfortranarray(np.arange(15).reshape(3, 5))\r\n>>> np_content\r\narray([[ 0,  1,  2,  3,  4],\r\n       [ 5,  6,  7,  8,  9],\r\n       [10, 11, 12, 13, 14]])\r\n>>> ak_content = ak.layout.NumpyArray(np_content)\r\n>>> ak_content\r\n<NumpyArray format=\"l\" shape=\"3 5\" strides=\"8 24\" data=\"0x 00000000 00000000 05000000 00000000 0a000000 00000000\" at=\"0x55ff9abd35b0\"/>\r\n>>> offsets = ak.layout.Index64(np.array([0, 0, 1, 1, 2, 2, 3, 3]))\r\n>>> listoffsetarray = ak.layout.ListOffsetArray64(offsets, ak_content)\r\n>>> listoffsetarray[1, 0]\r\n<NumpyArray format=\"l\" shape=\"5\" strides=\"24\" data=\"0 33 94556304121488 94556298227728 3\" at=\"0x55ff9aeb1e90\"/>\r\n>>> ak.to_list(listoffsetarray[1, 0])\r\n[0, 33, 94556304513392, 94556298227728, 94556301333088]\r\n```\r\n\r\n**After:**\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> import numpy as np\r\n>>> np_content = np.asfortranarray(np.arange(15).reshape(3, 5))\r\n>>> np_content\r\narray([[ 0,  1,  2,  3,  4],\r\n       [ 5,  6,  7,  8,  9],\r\n       [10, 11, 12, 13, 14]])\r\n>>> ak_content = ak.layout.NumpyArray(np_content)\r\n>>> ak_content\r\n<NumpyArray format=\"l\" shape=\"3 5\" strides=\"8 24\" data=\"0x 00000000 00000000 05000000 00000000 ... 09000000 00000000 0e000000 00000000\" at=\"0x55570d3eab20\"/>\r\n>>> offsets = ak.layout.Index64(np.array([0, 0, 1, 1, 2, 2, 3, 3]))\r\n>>> listoffsetarray = ak.layout.ListOffsetArray64(offsets, ak_content)\r\n>>> listoffsetarray[1, 0]\r\n<NumpyArray format=\"l\" shape=\"5\" data=\"0 1 2 3 4\" at=\"0x55570d401e50\"/>\r\n>>> ak.to_list(listoffsetarray[1, 0])\r\n[0, 1, 2, 3, 4]\r\n>>> listoffsetarray[3, 0]\r\n<NumpyArray format=\"l\" shape=\"5\" data=\"5 6 7 8 9\" at=\"0x55570d3fe2f0\"/>\r\n>>> ak.to_list(listoffsetarray[3, 0])\r\n[5, 6, 7, 8, 9]\r\n```",
  "created_at":"2020-08-27T17:06:01Z",
  "id":682076171,
  "issue":418,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjA3NjE3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T17:06:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Interestingly, I noticed along the way that NumPy's `nbytes` doesn't really describe how many bytes a sliced array covers, either:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import ctypes\r\n>>> a = np.arange(15, dtype=\"i2\").reshape(3, 5)\r\n>>> a\r\narray([[ 0,  1,  2,  3,  4],\r\n       [ 5,  6,  7,  8,  9],\r\n       [10, 11, 12, 13, 14]], dtype=int16)\r\n>>> b = a[:, :-1]\r\n>>> b\r\narray([[ 0,  1,  2,  3],\r\n       [ 5,  6,  7,  8],\r\n       [10, 11, 12, 13]], dtype=int16)\r\n>>> b.shape\r\n(3, 4)\r\n>>> b.strides\r\n(10, 2)\r\n>>> b.itemsize\r\n2\r\n```\r\n\r\nUsing the algorithm I implemented for `NumpyArray::bytelength`, the length of the buffer is `(shape[0] - 1)*strides[0] + (shape[1] - 1)*strides[1] + itemsize`:\r\n\r\n```python\r\n>>> (3 - 1)*10 + (4 - 1)*2 + 2\r\n28\r\n```\r\n\r\nbut `ndarray.nbytes` says\r\n\r\n```python\r\n>>> b.nbytes\r\n24\r\n```\r\n\r\n(which is `shape[0]*shape[1]*itemsize`). If you look at 28 bytes in memory, starting at the data pointer,\r\n\r\n```python\r\n>>> np.ctypeslib.as_array((ctypes.c_int16 * int(28/2)).from_address(b.ctypes.data))\r\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\r\n      dtype=int16)\r\n```\r\n\r\nyou do need to go 28 bytes out to see everything in `b`. Internally, NumPy must be calculating the right length in order to copy array slices, but the `nbytes` they provide is rather naive. (It's the size the array would be if you copied it into a contiguous buffer, not the size that it is currently occupying in memory.)",
  "created_at":"2020-08-27T17:27:32Z",
  "id":682087037,
  "issue":418,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjA4NzAzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T17:27:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks a lot for the quick fix! I didn't have a use case in mind where fortran ordering would be used for efficiency - i just didn't notice that i was using fortran ordered arrays as they came out of a numpy operation (`np.argwhere`). So returning a C-contiguous copy after `__getitem__`  is perfectly fine.\r\n\r\nIndeed the `strides`, `shape` logic is quite interesting!",
  "created_at":"2020-08-28T07:38:09Z",
  "id":682378292,
  "issue":418,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjM3ODI5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T07:38:09Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"I had intended to try this out, but I'm not going to have a Raspberry Pi set up to try to do this anytime soon. Since we don't actually support this platform (that is, we don't provide a binary wheel), this would make a better StackOverflow question.",
  "created_at":"2020-10-30T22:33:14Z",
  "id":719829215,
  "issue":419,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgyOTIxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:33:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Wow, this is great! I just went over all the files, and everything is in the right place. (It's a lot of boilerplate, I know, but you're not missing anything.) I think you didn't mean to check in the \"Pipfile\" and its lock, though.\r\n\r\nJust to keep track, the two main features that are missing are reducers (some don't make sense for complex, such as min and max) and a Builder. I think when we talked earlier, I didn't mention that the ArrayBuilder should become aware of complex numbers\u2014I was reminded of it by your test. The append method is capable of adding types to an ArrayBuilder that the ArrayBuilder doesn't know about (float32 in the previous example), but something as distinct as complex numbers should have its own method, so you can pass in Python complex objects, rather than having to make Awkward arrays and pass them in. (That would enable complex numbers anywhere in a list of lists passed in through `ak.from_iter`.) Unless you want to try it, I can add a Complex128Builder after this PR.",
  "created_at":"2020-08-28T12:05:36Z",
  "id":682488570,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjQ4ODU3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T12:05:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I was just looking for a standard way to encode complex numbers in JSON, and there don't seem to be any protocols out there. This might be the closest: math.js makes objects with two keys: `\"re\"` and `\"im\"`.\r\n\r\nhttps://mathjs.org/docs/datatypes/complex_numbers.html",
  "created_at":"2020-08-28T12:18:16Z",
  "id":682493778,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjQ5Mzc3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T12:18:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for taking a look. Just note that this isn't ready (I've marked the PR as a draft).\r\n\r\nThe Pipfile should be removed from the final version, I actually committed it because I wanted to do a quick `git clean -fxd`. Guess I could add it to .gitignore.\r\n\r\n> (It's a lot of boilerplate, I know, but you're not missing anything).\r\n\r\nI was wondering if much of this could be stamped out with macros? I didn't bite the bullet.\r\n\r\nAlso I need to cast to double and float in the C layer as you mentioned at the end of your comment in https://github.com/scikit-hep/awkward-1.0/issues/392#issuecomment-674110892.\r\n\r\n\r\n> Just to keep track, the two main features that are missing are reducers (some don't make sense for complex, such as min and max) and a Builder. \r\n\r\nOK will have a look. \r\n\r\n> I think when we talked earlier, I didn't mention that the ArrayBuilder should become aware of complex numbers\u2014I was reminded of it by your test. The append method is capable of adding types to an ArrayBuilder that the ArrayBuilder doesn't know about (float32 in the previous example), but something as distinct as complex numbers should have its own method, so you can pass in Python complex objects, rather than having to make Awkward arrays and pass them in. (That would enable complex numbers anywhere in a list of lists passed in through ak.from_iter.) Unless you want to try it, I can add a Complex128Builder after this PR.\r\n\r\nThanks for the explanation. As I understand it, this would be something like the existing Float64Builder class that the ArrayBuilder would then use when calling it's snapshot method?\r\n\r\nLet me see how i go and if it's too much I'll ask for your assistance.\r\n",
  "created_at":"2020-08-28T12:37:24Z",
  "id":682501669,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjUwMTY2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T12:37:24Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> > (It's a lot of boilerplate, I know, but you're not missing anything).\r\n> \r\n> I was wondering if much of this could be stamped out with macros?\r\n\r\nSome of it probably will, either C preprocessor macros or the code-generation (Python scripts generating kernels from YAML) that @reikdas is developing.\r\n\r\n> Thanks for the explanation. As I understand it, this would be something like the existing Float64Builder class that the ArrayBuilder would then use when calling it's snapshot method?\r\n> \r\n> Let me see how i go and if it's too much I'll ask for your assistance.\r\n\r\nIt would be like Float64Builder. The tricky thing about adding a new Builder class and fill method\u2014`Builder::complex`, in analogy with `Builder::real`, etc.\u2014is that an appropriate action has to be assigned to every Builder class for the new method. Each Builder class has a \"natural\" method: for example, `Float64Builder::real` simply appends to its internal list, but the rest require structures to change. Most \"non-natural\" methods replace the current node with a UnionBuilder with itself nested in the UnionBuilder, and then the UnionBuilder adds new types to the union if they don't match any existing ones. However, Float64Builder has a special rule for generalizing integers unlike any of the others: it replaces the integers with floating-point numbers, then fills the next real number. The `Builder::complex` methods should have a similar behavior: they should convert integers and floating-point numbers into complex numbers, then continue filling, instead of replacing the current node with a union for these types.\r\n\r\nHandling Complex128Builder should probably be a separate pull request, whether you do it or I do it. Actually, it doesn't even need for this PR to be finished\u2014Builders are sufficiently independent of Contents that a Complex128Builder could be fully implemented and tested without NumpyArrays being complex-aware. (`ak.to_list` wouldn't work in the tests, but the correctness of the output could be tested by deconstructing the layout.) The offer stands: if you want to concentrate on the NumpyArray part, I can add a Complex128Builder in a separate PR, which you could use for tests when you get to that point. But if you want to try it, go ahead, but put it in a separate PR to avoid scope creep.\r\n\r\n(Also, do you want to be added to the developers list so that you can work on this PR as a scikit-hep/awkward-1.0 branch, rather than in your own fork of the repo?)",
  "created_at":"2020-08-28T13:54:01Z",
  "id":682587912,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjU4NzkxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T13:54:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks, I'll take you up on the Complex128Builder!\r\n\r\nI tried to add a complex number overload for `awkward_NumpyArray_fill` but it's not picking it up. \r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/pull/421/commits/a5483153dd7ed3e3ae94e3d05a2677e554d3cbf8#diff-b9071bed05afe0f918e34768fe605eefR1165-R1173\r\n\r\n```bash\r\n$ python localbuild.py \r\ncmake --build localbuild -- -j8\r\n[  1%] Building CXX object CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/operations.cpp.o\r\n[ 64%] Built target awkward-objects\r\n/home/sperkins/work/ska/code/awkward-1.0/src/cpu-kernels/operations.cpp: In instantiation of \u2018Error awkward_NumpyArray_fill(TO*, int64_t, const FROM*, int64_t) [with FROM = std::complex<double>; TO = double; int64_t = long int]\u2019:\r\n/home/sperkins/work/ska/code/awkward-1.0/src/cpu-kernels/operations.cpp:2082:40:   required from here\r\n/home/sperkins/work/ska/code/awkward-1.0/src/cpu-kernels/operations.cpp:1157:27: error: invalid cast from type \u2018const std::complex<double>\u2019 to type \u2018double\u2019\r\n     toptr[tooffset + i] = (TO)fromptr[i];\r\n                           ^~~~~~~~~~~~\r\nCMakeFiles/awkward-cpu-kernels-objects.dir/build.make:134: recipe for target 'CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/operations.cpp.o' failed\r\nmake[2]: *** [CMakeFiles/awkward-cpu-kernels-objects.dir/src/cpu-kernels/operations.cpp.o] Error 1\r\nCMakeFiles/Makefile2:864: recipe for target 'CMakeFiles/awkward-cpu-kernels-objects.dir/all' failed\r\nmake[1]: *** [CMakeFiles/awkward-cpu-kernels-objects.dir/all] Error 2\r\nMakefile:140: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\nTraceback (most recent call last):\r\n  File \"localbuild.py\", line 86, in <module>\r\n    check_call([\"cmake\", \"--build\", \"localbuild\", \"--\", \"-j\" + args.j])\r\n  File \"localbuild.py\", line 55, in check_call\r\n    return subprocess.check_call(args, env=env)\r\n  File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', 'localbuild', '--', '-j8']' returned non-zero exit status 2.\r\n```\r\n\r\nThis is to support the NumPy complex -> real casting where the imaginary part is discarded.\r\n\r\nAny ideas? The following seems to work as I expect:\r\n\r\n```cpp\r\n#include <complex>\r\n#include <iostream>\r\n\r\n\r\ntemplate <typename FROM, typename TO>\r\nvoid convert(TO * to, const FROM * from)\r\n{\r\n    std::cout << \"General Case \" << *from << std::endl;\r\n}\r\n\r\ntemplate <typename FROM, typename TO>\r\nvoid convert(TO * to, const std::complex<FROM> * from)\r\n{\r\n    std::cout << \"COMPLEX \" << from[0].real() << \" \" << from[0].imag() << std::endl;\r\n}\r\n\r\n\r\nint main()\r\n{\r\n    float a = 2.0;\r\n    float b = 3.0;\r\n    std::complex<double> c = {1, 2};\r\n    std::string bob = \"bob\";\r\n\r\n    convert<float, float>(&a, &b);\r\n    convert<std::complex<double>, float>(&a, &c);\r\n    convert<std::string, float>(&a, &bob);\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n\r\n",
  "created_at":"2020-08-28T15:07:58Z",
  "id":682680895,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjY4MDg5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T15:07:58Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> (Also, do you want to be added to the developers list so that you can work on this PR as a scikit-hep/awkward-1.0 branch, rather than in your own fork of the repo?)\r\n\r\nYes, this would be helpful.",
  "created_at":"2020-08-28T15:12:02Z",
  "id":682687104,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjY4NzEwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T15:12:02Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I tried to add a complex number overload for awkward_NumpyArray_fill but it's not picking it up.\r\n\r\nI think the technical term here is [partial ordering of overloaded function templates](https://en.cppreference.com/w/cpp/language/function_template). ",
  "created_at":"2020-08-28T15:31:34Z",
  "id":682717352,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjcxNzM1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T15:31:34Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"The cast to a template parameter that works for all other types doesn't seem to be working for `std::complex<double>`.\r\n\r\n```\r\n/home/sperkins/work/ska/code/awkward-1.0/src/cpu-kernels/operations.cpp:1157:27: error: invalid cast from type \u2018const std::complex<double>\u2019 to type \u2018double\u2019\r\n     toptr[tooffset + i] = (TO)fromptr[i];\r\n```\r\n\r\nIt could be that `std::complex<double>`, being an explicitly C++ type, requires [static_cast](https://stackoverflow.com/a/37497093/1623645). I've been regretting my use of C type casts\u2014I use C++ `dynamic_cast` and `reinterpret_cast` when I want to downcast a virtual class or reinterpret the bits of an array, respectively, but I used C type casts everywhere that I wanted to convert a numeric type. I should have been using `static_cast` (and have no C type casts). I plan to sweep through the codebase at some point, replacing C type casts with `static_cast` (since I know that I _never_ used C type casts to mean anything other than `static_cast`), but it will be difficult because the C cast syntax is not searchable.\r\n\r\nSo this `(TO)fromptr[i]` really ought to be\r\n\r\n```c++\r\nstatic_cast<TO>(fromptr[i])\r\n```\r\n\r\nanyway. Try that.\r\n\r\nThis brings up another point I need to make now because the compiler isn't enforcing it: the [kernel functions need to have a pure C interface](https://github.com/scikit-hep/awkward-1.0/blob/master/CONTRIBUTING.md#project-organization)\u2014C++ is not allowed. Specifically, that means that `std::complex<double>` can't be in the argument list. The argument lists could either call these types `void*` or `double*` and then do a `reinterpret_cast` on both sides of the C interface. That's how we ended up with all of this boilerplate: we want to use some C++ features, such as templating many versions of a function for different numeric types, but the interface has to be pure C so that it can be swapped for cuda-kernels and so that non-C++ libraries can be built on the same suite of kernels. So we ended up implementing the kernel functions with templates and adding the template abstraction back in with kernel-dispatch.h and .cpp. In the case of complex numbers, we'll also need to reinterpret-cast the `std::complex<double>` type on the buffer on both sides of the C interface.\r\n\r\nI hope that all C++ compilers use a binary representation of a `std::complex<double>` array of length `N` that is the same as a `double` array of length `2*N` with the real and imaginary parts of each number interleaved. Not only is that important for passing arrays between kernels and the C++ codebase (which can be compiled by different compilers; awkward1-cuda-kernels is a separately distributed library), but it's necessary for interpreting complex NumPy buffers:\r\n\r\n```python\r\n>>> array = numpy.array([3.14 + 1.1j, 2.71 + 2.2j])\r\n>>> array.view(numpy.float64)   # reinterpret_cast\r\narray([3.14, 1.1 , 2.71, 2.2 ])\r\n```",
  "created_at":"2020-08-28T15:46:16Z",
  "id":682742923,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Mjc0MjkyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T15:46:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and @all-contributors please add @sjperkins  for code",
  "created_at":"2020-08-28T15:48:34Z",
  "id":682747852,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Mjc0Nzg1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T15:48:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/423) to add @sjperkins! :tada:",
  "created_at":"2020-08-28T15:48:43Z",
  "id":682748425,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Mjc0ODQyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T15:48:43Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Note that I deleted the scikit-hep/awkward-1.0:support-complex-numbers branch -- I mistakenly created it instead of pushing up to my fork.\r\n\r\nI think it makes more sense to keep this PR on my fork as we have the conversation here, but I'll push branches to the main repo in future PRs.",
  "created_at":"2020-08-29T09:26:05Z",
  "id":683264190,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzI2NDE5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-29T09:26:05Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay\u2014makes sense!",
  "created_at":"2020-08-29T12:57:53Z",
  "id":683286938,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzI4NjkzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-29T12:57:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I can see CI is failing because the kernel specification generator is unable to handle some of the kernels you added. @sjperkins This is not an issue on your end and I'll try to fix this. ",
  "created_at":"2020-08-31T17:52:43Z",
  "id":683932279,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzkzMjI3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-31T17:52:43Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for the headsup @reikdas. I haven't yet merged master into this branch, so I wouldn't suspect breakage changes from #373.\r\n\r\nI haven't had a good look at the internals of #373 but it looks like it auto-generates CUDA functions. Is there some overlap with generating the boilerplate for this PR? If I could leverage #373 I'd prefer to manually adding functions is error-prone and cases are likely to be missed.",
  "created_at":"2020-09-01T16:40:10Z",
  "id":684987982,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NDk4Nzk4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-01T16:40:10Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"There is no overlap of this and #373. The problem is that the C++ parser in `dev/generate-kernelspec.py` needs to be extended to handle `std::complex<double>`. \r\nIf the failing CI is a blocker for you to complete this PR, you could temporarily comment out these blocks - https://github.com/scikit-hep/awkward-1.0/blob/master/.ci/azure-buildtest-awkward.yml#L92-L103 (for all 3 of Windows, MacOS and Linux CI) and remove the [azure-doctest-awkward.yml](https://github.com/scikit-hep/awkward-1.0/blob/master/.ci/azure-doctest-awkward.yml) file. Once you are done with the PR, please re-enable them so we can fix the C++ parser and make sure CI passes before merging.",
  "created_at":"2020-09-01T18:21:58Z",
  "id":685050671,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NTA1MDY3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-01T18:21:58Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ah thanks for explaining this part of the build process @reikdas. I had to search a bit for link to the azure pipeline logs. I've been testing with `python localbuild.py`.\r\n\r\nNote that I'm working on this when I have the time so this may not all happen in a quick burst.",
  "created_at":"2020-09-01T19:51:52Z",
  "id":685095796,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NTA5NTc5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-01T19:51:52Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @sjperkins! It has been a while since you started this PR, and pretty soon, I'll be applying a name change to the whole Awkward Array codebase: all instances of \"`awkward1`\" will become \"`awkward`\" ([see details](https://github.com/scikit-hep/awkward-1.0/wiki/Name-change-procedure)). It might be easier to close this now and reopen a new PR to reintroduce this work after the name change? That name change is scheduled for Nov 30/Dec 1.",
  "created_at":"2020-11-23T21:38:08Z",
  "id":732441460,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMjQ0MTQ2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-23T21:38:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Hi @sjperkins! It has been a while since you started this PR, and pretty soon, I'll be applying a name change to the whole Awkward Array codebase: all instances of \"`awkward1`\" will become \"`awkward`\" ([see details](https://github.com/scikit-hep/awkward-1.0/wiki/Name-change-procedure)). It might be easier to close this now and reopen a new PR to reintroduce this work after the name change? That name change is scheduled for Nov 30/Dec 1.\r\n\r\nHi @jpivarski. Thanks for the headsup. I think that's probably a good idea, since I'd started on some Python code to autogenerate the kernels in any case.",
  "created_at":"2020-11-26T14:19:41Z",
  "id":734323825,
  "issue":421,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNDMyMzgyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-26T14:19:41Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Of course in the long run it will be better to have #402 but for now it is \ud83d\udc4d ",
  "created_at":"2020-08-28T16:11:51Z",
  "id":682808856,
  "issue":422,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjgwODg1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T16:11:51Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski The single failing test seems unrelated. Can you restart the tests please?",
  "created_at":"2020-08-29T11:34:28Z",
  "id":683278259,
  "issue":424,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzI3ODI1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-29T11:34:47Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"This was #320 again. There's definitely something wrong with `argsort`.",
  "created_at":"2020-08-29T13:08:24Z",
  "id":683288063,
  "issue":424,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzI4ODA2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-29T13:08:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Is this done? I haven't looked carefully into `awkward1.highlevel.Array.Mask._repr`, but if you need me to, I can.\r\n\r\nThere are three similar cases:\r\n\r\n   * `awkward1.highlevel.ArrayBuilder.List._repr`\r\n   * `awkward1.highlevel.ArrayBuilder.Record._repr`\r\n   * `awkward1.highlevel.ArrayBuilder.Tuple._repr`\r\n\r\nThese four nested classes are all intermediate objects (`Mask` for supplying `array.mask[whatever]` and the `ArrayBuilder` ones for `with builder.list():`, etc.).",
  "created_at":"2020-09-03T13:55:39Z",
  "id":686509302,
  "issue":428,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NjUwOTMwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T13:55:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ok its done now. I patched up Mask because I think I understand what it should display, e.g.:\r\n```\r\nIn [1]: events.mask\r\nOut[1]: <NanoEventsArray.mask [<event 1:3:2021>, ... ] type='100000 * event'>\r\n```",
  "created_at":"2020-09-03T20:37:02Z",
  "id":686752132,
  "issue":428,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Njc1MjEzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T20:37:02Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! That looks great.\r\n\r\nNow I remember why `__repr__` wanted to chain caches: so that data read by printing it to the screen doesn't have to be reread when using it in calculations. The double-read seemed wasteful. But if the `ak.Array` has a persistent cache and the `ak.Array` continues to exist, then the problem goes away. Your solution of ignoring chaining is a good one.",
  "created_at":"2020-09-03T21:41:03Z",
  "id":686778716,
  "issue":428,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Njc3ODcxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T21:41:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski We are able to generate CUDA for a lot more kernels. The ones that are left need to be handled in a different way. Maybe we should merge this?",
  "created_at":"2020-09-03T18:40:30Z",
  "id":686687741,
  "issue":429,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NjY4Nzc0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T18:40:30Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":">Also, I didn't actually test this in the AWS instance; I'd like positive confirmation that you did that because GitHub doesn't tell me (and we want to be as formal with that as we are with the CPU-bound CI).\r\n\r\nThanks for reminding me. I thought that kernel having trouble with different Python versions was fixed, but apparently not. I'll move it out of the list of kernels that can be generated for now. ",
  "created_at":"2020-09-03T20:06:58Z",
  "id":686727511,
  "issue":429,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NjcyNzUxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T20:06:58Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":">I thought that kernel having trouble with different Python versions was fixed, but apparently not. I'll move it out of the list of kernels that can be generated for now.\r\n\r\nThere was a more fundamental issue - it is fixed now.\r\n\r\nEverything works on the AWS instance.",
  "created_at":"2020-09-04T10:30:23Z",
  "id":687064568,
  "issue":429,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NzA2NDU2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-04T10:30:23Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Something went wrong in the installation procedure: the `awkward1._ext` (Python extension module providing compiled Awkward routines) was not compiled or not moved into its final installation position.\r\n\r\nFrom the error message (\"DLL load failed\"), it looks like you're using Windows. If you're using pip to install and you have one of these combinations,\r\n\r\n   * 32-bit Windows, Python 2.7\r\n   * 64-bit Windows, Python 2.7\r\n   * 32-bit Windows, Python 3.5\r\n   * 64-bit Windows, Python 3.5\r\n   * 32-bit Windows, Python 3.6\r\n   * 64-bit Windows, Python 3.6\r\n   * 32-bit Windows, Python 3.7\r\n   * 64-bit Windows, Python 3.7\r\n   * 32-bit Windows, Python 3.8\r\n   * 64-bit Windows, Python 3.8\r\n\r\n(see [wheels in PyPI](https://pypi.org/project/awkward1/0.2.35/#files)), then it should be installing a precompiled library. If not, pip would try to compile the library using CMake and Visual Studio, which is unlikely to work unless you have a development environment all set up. (Or perhaps I should say, \"all set up the way that Azure has it,\" since the Azure system we use to build and deploy these wheels is the only Windows system on which I know that it works!)\r\n\r\nDo you see what might have gone wrong in installation? Sometimes it's necessary to pass `--no-cache-dir` to pip if installation failed the first time but you've fixed your procedure and don't want it to draw from the broken installation.\r\n\r\n(The fact that you're using Jupyter _might_ be relevant, since Jupyter can sometimes run with a different search path to Python libraries than Python on the commandline. To diagnose this error, you might want to try both.)",
  "created_at":"2020-09-02T20:39:36Z",
  "id":685985508,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NTk4NTUwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-02T20:39:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Interesting. I am running Python 3.8.3 on 64-bit Windows, installing other libraries recently has been fine.\r\n\r\nI will readily admit that I need to do more reading up on how wheels and other structures in python actually work, and how they are build so that I can have more useful conversation when I encounter issues. \r\n\r\nI can't tell in your post whether you are asking me to further look into the installation process, or whether that was a rhetorical and there was something else for me to pick up which I missed. Would you mind reiterating?\r\n\r\nVery possible I misinterpreted you, but I also tried passing `--no-cache-dir` to pip and retrying: `pip install awkward1 --no-cache-dir`. This returned (1). \r\n\r\nIf it is helpful, `help(\"modules\")` from command line finds awkward1 pictured in (2), but `import awkward1` from the commandline raises the error pictured in (3).\r\n\r\n(1)\r\n![image](https://user-images.githubusercontent.com/64329370/92162074-691d2480-ede6-11ea-8945-b3d88886ac65.png)\r\n(2)\r\n![image](https://user-images.githubusercontent.com/64329370/92160971-b6989200-ede4-11ea-88dc-5922c72958d2.png)\r\n(3)\r\n![image](https://user-images.githubusercontent.com/64329370/92161385-6e2da400-ede5-11ea-9142-ea48c7eaf181.png)\r\n\r\nThoughts?",
  "created_at":"2020-09-03T20:10:51Z",
  "id":686729120,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NjcyOTEyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T20:10:51Z",
  "user":"MDQ6VXNlcjY0MzI5Mzcw"
 },
 {
  "author_association":"MEMBER",
  "body":"> I can't tell in your post whether you are asking me to further look into the installation process, or whether that was a rhetorical and there was something else for me to pick up which I missed. Would you mind reiterating?\r\n\r\nI can't diagnose an installation error\u2014it has something to do with particular files in particular places on your computer, which I don't have access to.\r\n\r\nPython installation procedures have too much freedom:\r\n\r\n![](https://imgs.xkcd.com/comics/python_environment.png)\r\n\r\nso it often helps to pick only one way of installing things and stick to it. Personally, I use Miniconda\u2014packages that can only be installed with pip (like awkward1) are installed in the same environment as long as Miniconda is in the path.\r\n\r\nWhen I've presented tutorials, it was always the case that some 10% of the users couldn't install the software because of variance in laptop configurations. For the Windows users, the typical difficulty was getting to a terminal that has Miniconda/Anaconda in the path: Anaconda has its own terminal-like thing that has to be used instead of a normal terminal. It was hard following all the variations in laptop configurations when I could look over peoples' shoulders to see what was happening. Debugging it remotely is a non-starter.\r\n\r\nSo my comment above was a list of things you can try to debug it, since you're the only one in a position to do that.\r\n\r\nAnother suggestion: try\r\n\r\n```bash\r\npip uninstall awkward1\r\n```\r\n\r\nenough times to be sure there are no versions of it on the computer. If you have a full disk search, try searching for files with \"awkward1\" in their names to be sure it's gone. Then start from the beginning.\r\n\r\nIf you have `awkward1_cuda_kernels` on your computer, remove it. It only works in Linux (and the wheel is labeled \"Linux\", so pip shouldn't have allowed it to be downloaded to your computer).\r\n\r\nAs a last-minute thought, it occurred to me that I should verify that the wheels I'm publishing have the `awkward1._ext` module in them. You're using Python 3.8 on 64-bit Windows, so that's [awkward1-0.2.35-cp38-cp38-win_amd64.whl](https://files.pythonhosted.org/packages/a8/a3/692dacf01b1af73777e58dde35569d7245e2e0f9a8e81b54048d05fd8272/awkward1-0.2.35-cp38-cp38-win_amd64.whl). I downloaded it and unzipped it (Python wheels are just ZIP files) and indeed it contains\r\n\r\n```\r\nawkward1/_ext.cp38-win_amd64.pyd\r\n```\r\n\r\nwhich is the DLL that your Python needs to be finding.\r\n\r\nOne last-last thought: you're running standard Python, not PyPy or anything, right? The DLL is labeled for CPython (standard Python), which alternatives like PyPy can't run. (Alternative Python implementations can't run extension modules in general.)",
  "created_at":"2020-09-03T21:37:59Z",
  "id":686777611,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Njc3NzYxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T21:37:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hello,\r\n\r\nI just tried to run uproot4 for the first time and I think to have run into the same error:\r\n\r\n![grafik](https://user-images.githubusercontent.com/11306732/93667906-9bd34800-fa89-11ea-8ead-ab4f8c7e7881.png)\r\n![grafik](https://user-images.githubusercontent.com/11306732/93667952-be656100-fa89-11ea-8482-7978354c49be.png)\r\n![grafik](https://user-images.githubusercontent.com/11306732/93668041-506d6980-fa8a-11ea-8a71-ea7f5434ddb1.png)\r\n\r\nThe first image is uproot4 (v. 0.0.27) and awkward1 (v. 0.2.38) being installed by pip. The second image is the same being installed by conda from the conda-forge rep. The last image is the setup from the second image but executed directly from the command line.\r\n\r\nI'm using numpy 1.19.1 and python 3.8.5 both from conda-forge.\r\n",
  "created_at":"2020-09-19T13:16:34Z",
  "id":695212203,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTIxMjIwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T13:16:34Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"You're using Python 3.8 and (I assume) Windows 64-bit, so I just manually downloaded the appropriate wheel from pip:\r\n\r\nhttps://files.pythonhosted.org/packages/21/15/8b710df9cdd88be9fbace2a1b85c6453da60d6574d352a295353c2614c12/awkward1-0.2.38-cp38-cp38-win_amd64.whl\r\n\r\nand checked inside: it has a `_ext.cp38-win_amd64.pyd` file.\r\n\r\nCould you list `C:\\ProgramData\\Anaconda3_8\\envs\\work\\lib\\site-packages\\awkward1\\_ext.*`? I think you should find\r\n\r\n   * `_ext.cp38-win_amd64.pyd`\r\n   * `_ext.lib`\r\n   * `_ext.exp`\r\n\r\nTo be honest, I don't understand dynamically loadable libraries on Windows well enough to know if or why three files are needed, rather than the single `_ext.cpython-38-x86_64-linux-gnu.so` that is used by Linux (or `.dylib` on MacOS).\r\n\r\nActually, I don't have any positive confirmation that anyone has been able to install awkward1 on Windows. The wheels contain all the right files, but that doesn't mean it will work. @gordonwatts and @kpedro88, have you ever tried `pip install awkward1` on Windows?\r\n\r\nAs of the last few days, awkward1 and uproot4 are available on conda-forge:\r\n\r\n   * https://anaconda.org/conda-forge/awkward1\r\n   * https://anaconda.org/conda-forge/uproot4\r\n\r\nThis is another avenue to try, though you should `pip uninstall awkward1 uproot4` before attempting it. (Cleaning previous installation attempts is good practice in general.)",
  "created_at":"2020-09-19T13:57:10Z",
  "id":695216671,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTIxNjY3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T13:57:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes. I'm using Windows 10 v. 1909 64-bit.\r\n\r\nThe files in `C:\\ProgramData\\Anaconda3_8\\envs\\work\\lib\\site-packages\\awkward1\\_ext.*` are identically to the three you named:\r\n\r\n![grafik](https://user-images.githubusercontent.com/11306732/93669669-0ee2bb80-fa96-11ea-82f2-a22405a43611.png)\r\n\r\nI already tried the conda-forge version and the error stays the same (it's the second and third picture from my first comment).\r\n",
  "created_at":"2020-09-19T14:36:02Z",
  "id":695220888,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTIyMDg4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T14:36:02Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for checking. I pinged @gordonwatts and @kpedro88 because they are two Windows power-users that I personally know, but now I also posted a more general call for Windows testers on https://gitter.im/HSF/PyHEP .\r\n\r\nWithout knowing of any successful installations, I don't know if the Windows wheels are broken for all Windows users or if there's something particular about your two computers. (I started with that assumption when addressing @zoyt's problem, but hadn't considered that maybe it's more general.)",
  "created_at":"2020-09-19T14:51:11Z",
  "id":695227032,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTIyNzAzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T14:51:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I tried to build awkward1 locally. I don't know if I did it correctly as I'm not used to building Python Packages from source.\r\nAnyway, it results in the attached error:\r\n\r\n[myoutput.txt](https://github.com/scikit-hep/awkward-1.0/files/5250105/myoutput.txt)\r\n",
  "created_at":"2020-09-19T15:31:51Z",
  "id":695268485,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTI2ODQ4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T15:31:51Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"NONE",
  "body":"The error stays the same on a Python 3.7 enviroment.",
  "created_at":"2020-09-20T01:40:00Z",
  "id":695459398,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTQ1OTM5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-20T01:40:00Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"I've been asking around about this. Other Windows users have hit other errors (couldn't compile with Cygwin, but we don't want it to compile at all, we want it to pick up the precompiled wheels) and still others have had no problems installing at all. I've been wondering if the Windows version matters (which version do you have?), but instructions on Python packaging for Windows only describe dependencies on Python version and 32 vs 64-bit.\r\n\r\nI'm still searching, but we'll likely have more luck during the work week.\r\n\r\nBoth of your problems (@superharz and @voyt) have the strange combination of the library file existing, but it doesn't import (\"could not be found\"?!?).",
  "created_at":"2020-09-20T03:48:07Z",
  "id":695642109,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTY0MjEwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-20T03:48:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In case it suggests anything, the libraries you have but can't import were compiled with \"Visual Studio 15 2017\".",
  "created_at":"2020-09-20T03:51:01Z",
  "id":695646327,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTY0NjMyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-20T03:51:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I noticed the following:\r\n\r\n* Renaming `_ext.cp38-win_amd64.pyd` to something else results in `ModuleNotFoundError: No module named 'awkward1._ext'`\r\n* Changing the import order in `layout.py` makes no difference. The error occurs by whatever is imported first from `_ext`\r\n\r\nI tried the following:\r\n\r\n* Installed the latest updates for W10 1909 -> No difference\r\n* Added `Anaconda3\\Library\\bin` to my system environment as proposed by user1024 in [stackoverflow](https://stackoverflow.com/a/57441828) -> No difference\r\n* Rebooted my PC -> No difference",
  "created_at":"2020-09-20T15:00:17Z",
  "id":695796871,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTc5Njg3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-20T15:00:17Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"([copying over from gittter](https://gitter.im/HSF/PyHEP?at=5f6841a7d993b837e078addc))\r\n\r\nIt looks like the wheels are linking to the debug runtime instead of the release one:\r\n\r\n![image](https://user-images.githubusercontent.com/5220533/93760294-001e1500-fc0c-11ea-88ab-834cf0911703.png)\r\n\r\nDownloading these DLLs fixes the problem and if I check the build logs I see they contain `-- Install configuration: \"Debug\"` so this probably just needs to be modified to be a release build.",
  "created_at":"2020-09-21T11:14:34Z",
  "id":696050165,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjA1MDE2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T11:14:34Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"On Gitter, I was suspicious that it's a \"compiling in Debug vs Runtime\" issue, but this suggests that it is:\r\n\r\nhttps://social.msdn.microsoft.com/Forums/vstudio/en-US/06b66cae-d010-441d-bc72-64a8c7cd65bc/missing-vcruntime140ddll-and-ucrtbaseddll?forum=visualstudiogeneral\r\n\r\nI'll look into that.",
  "created_at":"2020-09-21T13:46:27Z",
  "id":696125308,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjEyNTMwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T13:46:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Just realizing I was tagged on this - sounds like from @chrisburr a possible root cause has been found. Let me know if you want me to do test anything, @jpivarski - in general, I've not moved any of the code I used over to awkward1.\r\n\r\nIn response to an earlier question about file types:\r\n\r\n- .dll - contains the actual code that is run.\r\n- .lib - Exactly what is in here depends on your build process. It could be a static copy of all the awkward1 routines, for example. Or it could be stubs that call into the DLL. Regardless, they would only be used if someone was linking against the awkward libraries.\r\n- .exp - Lists of symbols that the DLL exports. Again, used by build tools when you want to load and use the .so file.\r\n\r\nI do not know if you intend the python distribution to have, as a use case, others linking directly against it. If that is the case... And I'm pretty sure the python shared library loader does not use the exp file. I would guess this would work fine if you got rid of the .exp file. Getting rid of the .lib file might be possible too - but I can't a-priori tell the role it plays here.",
  "created_at":"2020-09-21T13:53:48Z",
  "id":696129566,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjEyOTU2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T13:53:48Z",
  "user":"MDQ6VXNlcjE3NzgzNjY="
 },
 {
  "author_association":"MEMBER",
  "body":"@gordonwatts Thanks for the explanation!\r\n\r\nIn Linux and MacOS, we include both static and dynamic link libraries with the intention of letting [C++ projects link against the pure C++ parts](https://github.com/scikit-hep/awkward-1.0/tree/master/dependent-project), so this is a good reason to keep the .lib and .exp for awkward.dll and awkward-cpu-kernels.dll, but perhaps not _ext.dll (the Python interface to the C++ layer). We could perhaps drop _ext.lib and _ext.exp, but maybe not just yet.\r\n\r\n\r\n",
  "created_at":"2020-09-21T17:42:48Z",
  "id":696266322,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjI2NjMyMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-21T17:42:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@Superharz and @zoyt, I managed to get the build process to say\r\n\r\n```\r\n2020-09-21T17:34:55.6723805Z     -- Install configuration: \"Release\"\r\n```\r\n\r\ninstead of \"Debug\", so maybe it will look for non-debug versions of `MSVCP140D.dll`, `VCRUNTIME140D.dll`, and `ucrtbased.dll`. The files are in\r\n\r\nhttps://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=4279&view=artifacts&type=publishedArtifacts\r\n\r\n(NOT on PyPI; this is a testing area before deploying.) Could you uninstall any copies of awkward1 you have (`pip uninstall awkward1` and `conda remove awkward1` if you used conda at any point, possibly multiple times for pip if several versions were involved), download the appropriate wheel for your Python version and 32-bit vs 64-bit, and install that wheel with\r\n\r\n```bash\r\npip install path\\to\\wheel\\file.whl\r\n```\r\n\r\n([installing from a wheel file](https://stackoverflow.com/a/27909082/1623645)).\r\n\r\n@Superharz, since you installed the debug libraries, could you try it without them? Thanks!",
  "created_at":"2020-09-21T17:52:12Z",
  "id":696271113,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjI3MTExMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T17:52:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I downloaded the wheel from Azure:\r\n\r\nThe `.dll` files look good:\r\n\r\n![grafik](https://user-images.githubusercontent.com/11306732/93804572-cd454280-fc46-11ea-8a61-ce6b44d7ae76.png)\r\n\r\nThe import throws no error:\r\n\r\n![grafik](https://user-images.githubusercontent.com/11306732/93804616-dfbf7c00-fc46-11ea-8461-1427d366fe06.png)\r\n",
  "created_at":"2020-09-21T18:14:08Z",
  "id":696283207,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjI4MzIwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T18:14:08Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"That's a very good sign! Here's a minimal but non-trivial test to be sure that we're using the C++ part:\r\n\r\n```python\r\n>>> array = ak.Array([[1, 2, 3], [], [4, 5]])\r\n>>> array\r\n<Array [[1, 2, 3], [], [4, 5]] type='3 * var * int64'>\r\n>>> ak.sum(array, axis=1)\r\n<Array [6, 0, 9] type='3 * int64'>\r\n```\r\n\r\nDoes that work? If so, then I'll deploy this release and it will be available as a regular thing.",
  "created_at":"2020-09-21T18:22:28Z",
  "id":696287479,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjI4NzQ3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T18:22:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"It seems to work:\r\n\r\n![grafik](https://user-images.githubusercontent.com/11306732/93806491-8147cd00-fc49-11ea-894b-4c86166308c5.png)\r\n",
  "created_at":"2020-09-21T18:32:05Z",
  "id":696292653,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjI5MjY1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T18:32:05Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"Then I think it's in good shape. I'll keep this issue open until the new release is deployed (0.3.0), which should be within an hour or so.\r\n\r\nThanks for bearing with us while we struggled to figure this out.\r\n\r\n(And thanks @chrisburr: you figured out the key piece, the fact that the library was being released in debug mode. I think it compiled both debug and release, because the final fix didn't change the build step, it was to explicitly pass the `--config Release` to the installation step.)",
  "created_at":"2020-09-21T18:38:30Z",
  "id":696296177,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjI5NjE3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T18:38:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The Windows wheels for this release are out. I had trouble with the PyPI limit and had to remove old releases (and [ask for an increase](https://github.com/pypa/pypi-support/issues/630)), so the release still isn't finished\u2014it will be another half hour at least.\r\n\r\nIt should be possible to do a regular `pip install` now, though you don't strictly need to because the official wheel is no different from the one you already have.",
  "created_at":"2020-09-21T20:55:50Z",
  "id":696371194,
  "issue":430,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjM3MTE5NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2020-09-21T20:55:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"All but one test passed, and it seems to be in an unrelated section of code.",
  "created_at":"2020-09-03T21:28:30Z",
  "id":686773926,
  "issue":431,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Njc3MzkyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T21:28:30Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Issue #320 has happened again!",
  "created_at":"2020-09-03T21:43:01Z",
  "id":686779483,
  "issue":431,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4Njc3OTQ4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-03T21:43:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In the time since this was first filed, the behavior has changed. Are you still looking for a fix?\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> \r\n>>> class Canary(dict):\r\n...     def __getitem__(self, key):\r\n...         print(f\"getting {key}\")\r\n...         return super().__getitem__(key)\r\n... \r\n>>> a = ak.zip({\"x\": [[0, 1, 2], [3, 4], [5]], \"y\": [[0, -1, -2], [-3, -4], [-5]]})\r\n>>> form, container, _ = ak.to_arrayset(a)\r\n>>> lazy = ak.from_arrayset(form, Canary(container), lazy=True, lazy_lengths=3)\r\n>>> ak.firsts(lazy)\r\ngetting node0-offsets\r\ngetting node2\r\ngetting node3\r\n<Array [{x: 0, y: 0}, ... {x: 5, y: -5}] type='3 * ?{\"x\": int64, \"y\": int64}'>\r\n```",
  "created_at":"2020-10-30T22:35:35Z",
  "id":719829870,
  "issue":432,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgyOTg3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:35:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"What you printed shows materialization, but presumably because of the repr of the return value. Indeed:\r\n```python\r\nIn [3]: lazy = ak.from_arrayset(form, Canary(container), lazy=True, lazy_lengths=3)\r\n\r\nIn [4]: lazy[\"a\"] = lazy.x * 1\r\n   ...: lazy[\"b\"] = lazy.x * 2\r\n   ...: lazy[\"c\"] = lazy.x * 3\r\ngetting node0-offsets\r\ngetting node2\r\n\r\nIn [5]: x = ak.firsts(lazy)\r\n\r\nIn [6]: x\r\nOut[6]: getting node3\r\n<Array [{x: 0, y: 0, a: 0, ... b: 10, c: 15}] type='3 * ?{\"x\": int64, \"y\": int64...'>\r\n\r\n```\r\nSo it appears this is fixed, somehow.\r\n",
  "created_at":"2020-10-30T22:51:28Z",
  "id":719833861,
  "issue":432,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgzMzg2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:51:28Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, \"singletons\" was created (along with \"firsts\") because I hadn't thought of using `keepdims=True`!\r\n\r\nThe original rationale for having `keepdims=True` return regular-typed lists was because I knew the length was always equal to 1. However, it makes more sense to keep it the same as whatever the dimension used to be.\r\n\r\nAdditionally, regular-typed lists should remain regular-typed when reducing (with or without `keepdims=True`) unless there are any None values (because they can change the lengths of the outputs). That all has to be decided based on the types, not whether the lists happen to have the same number of items in each. For example, UnmaskedArray counts the same as ByteMaskedArray or any other option-type, even though it doesn't have any Nones.\r\n\r\nEverything has been implemented and tested in PR #447 (exhaustively tested: all combinations of variable and regular-typed lists three levels deep, with `axis=-1`, `-2`, `-3`...)\r\n\r\nSo this should be ready for use as maxby/minby. I guess you don't need to use [ak.firsts](https://awkward-array.readthedocs.io/en/latest/_auto/ak.firsts.html) if the lists were created with `keepdims=True`, since there won't ever be any empty lists. I'm going to start recommending `keepdims=True` for maxby/minby type problems.\r\n\r\n(#203 was an idea about adding specialized functions for maxby/minby because the [ak.singletons](https://awkward-array.readthedocs.io/en/latest/_auto/ak.singletons.html)/[ak.firsts](https://awkward-array.readthedocs.io/en/latest/_auto/ak.firsts.html) method is so cumbersome. But maybe that's not needed?)",
  "created_at":"2020-09-10T05:01:45Z",
  "id":689982812,
  "issue":434,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTk4MjgxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T05:01:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The `42adfe5` build fail with:\r\n```log\r\nerror C3861: 'assert': identifier not found\r\n```\r\nthat is fixed in the next commit.",
  "created_at":"2020-09-08T13:58:16Z",
  "id":688893961,
  "issue":435,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4ODg5Mzk2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-08T13:58:16Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - Unfortunately, I could not reproduce the error, not even once. So, trying to guess from the previous reports.\r\n \r\na) it is cured by re-running and b) the error message shows a non zero `offset` in an `IndexedArray` that is most likely the case of the error\r\n\r\nThe changes in this PR make sure that the `offset` is always initialised. The constructors are explicit, the explicit copy constructors are used between C++ and Python (by pybind11). Well, it could be an overkill and I should trust the compiler to do implicit copies correctly...\r\n\r\nPlease, have a look when you have time. Thanks!\r\n",
  "created_at":"2020-09-08T16:01:31Z",
  "id":688976534,
  "issue":435,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4ODk3NjUzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-08T16:01:31Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - Thanks! Using your branch I managed to reproduce and fix the bug.\r\n\r\nLast thing to do: a newly introduced parameter in https://github.com/scikit-hep/awkward-1.0/pull/435/commits/0619e197cf10b7912ac9d740f349ff9f576b4e81 needs to be documented to fix the error:\r\n```shell\r\nStarting: Generate Kernel specification\r\n==============================================================================\r\nTask         : Command line\r\nDescription  : Run a command line script using Bash on Linux and macOS and cmd.exe on Windows\r\nVersion      : 2.164.2\r\nAuthor       : Microsoft Corporation\r\nHelp         : https://docs.microsoft.com/azure/devops/pipelines/tasks/utility/command-line\r\n==============================================================================\r\nGenerating script.\r\nScript contents:\r\npython dev/generate-kernelspec.py\r\n========================== Starting Command Output ===========================\r\n/bin/bash --noprofile --norc /Users/runner/work/_temp/9db7d3b6-b487-4f50-97f2-37657c0ebe8f.sh\r\nTraceback (most recent call last):\r\n  File \"dev/generate-kernelspec.py\", line 855, in <module>\r\n    paramchecks = get_paramcheck(funcroles, tokens, funcargs)\r\n  File \"dev/generate-kernelspec.py\", line 821, in get_paramcheck\r\n    funcs[funcname][arg] = funcroles[keyfunc][arg][\"check\"]\r\nKeyError: Token(PARAMNAME, 'nextparentslength')\r\n\r\n##[error]Bash exited with code '1'.\r\nFinishing: Generate Kernel specification\r\n```",
  "created_at":"2020-09-09T11:28:37Z",
  "id":689500351,
  "issue":435,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTUwMDM1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T11:28:37Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - Thanks! Using your branch I managed to reproduce and fix the bug.\r\n\r\nGreat! I'm glad that helped.\r\n\r\n> Last thing to do: a newly introduced parameter in [0619e19](https://github.com/scikit-hep/awkward-1.0/commit/0619e197cf10b7912ac9d740f349ff9f576b4e81) needs to be documented to fix the error:\r\n\r\nI think you did the right thing in 3d6ee018d120fd69ca9dde41d4c276b10009a23f, namely adding `@param` in the header file for the new parameter, but I'll ping @reikdas in case the generated kernels fail again. So far, they're looking good.\r\n\r\nNote to self: I should add these extra steps to the `localbuild.py` script so that it's easier to do them locally. I usually\r\n\r\n```bash\r\ncat .ci/azure-buildtest-awkward.yml\r\n```\r\n\r\nand find the command lines to copy-paste to test them locally, but this is not sustainable.\r\n\r\n--------------------\r\n\r\n@ianna If the assignment of class members to zero is no longer necessary, I'd like to remove those changes. If there's a way to make the compiler raise a warning or an error for any constructor that doesn't set all of the class members explicitly, then let's turn on that compiler check instead. As I said in my review comments above, leaving these members uninitialized is bad, but arbitrarily setting them to zero is almost as bad.\r\n\r\nIf you think our diligence in manually checking that all constructors set all class members is not good enough and there's no automated way to check for it, then I suppose the next best thing would be to make the default values `-999` to try to ensure a runtime error. The problem with that is that the runtime error would likely be a segfault, and segfaults are hard to debug.",
  "created_at":"2020-09-09T12:37:05Z",
  "id":689534519,
  "issue":435,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTUzNDUxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T12:37:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - hopefully it fixes the bug without introducing unrelated changes :-)\r\n\r\nPerhaps, it would be useful if the script `dev/generate-kernelspec.py` produces an error such as:\r\n\r\n\"Failed to generate documentation for parameter` 'nextparentslength'.` Have you forgotten to document it in the file `'include/awkward/kernels/sorting.h'`?\"\r\n\r\nI think, there is enough information in the script to print these details.",
  "created_at":"2020-09-09T14:05:54Z",
  "id":689585832,
  "issue":435,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTU4NTgzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T14:05:54Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski Now, that I have the automated test generation framework running for the hand written kernels, I think it is a good checkpoint to see if this is going fine, since after this a lot of those `loop-dependent-variable-kernels` and some others follow exactly the same pattern. To run the tests, you'll have to `cd studies/cuda-kernels` and then do a `./test-cuda-kernels.sh`. A review on the three kernels I have written would be nice!. And you can merge this if you want, everything is in the studies directory.",
  "created_at":"2020-09-15T11:25:30Z",
  "id":692653464,
  "issue":436,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjY1MzQ2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-15T11:25:30Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"This looks good! I paused on the `cudaMalloc` for a moment because the kernels are not supposed to allocate memory, but then I realized that these have to because the intermediate scan has to go somewhere. In C++, this would be dangerous because an exception between the allocation and the deallocation would be a memory leak, but the C interface forbids us from raising exceptions from these functions, anyway, so it's okay. If there's ever an error in one of the first steps, there should be a way to skip the later steps, deallocate, and return it, but these kernels might not have errors to raise in the early steps. (How could a prefix scan ever fail?)\r\n\r\nI'd be happy to accept this as-is. We'll integrate it into the main line of code eventually (though the basic plumbing should be established with the auto-generated kernels, first).",
  "created_at":"2020-09-15T13:56:12Z",
  "id":692731633,
  "issue":436,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjczMTYzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-15T13:56:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The problem is that this file technically isn't JSON: it's a stream of many JSON objects. Applied to the same data (taking care because the file is huge), Python's `json.loads` says,\r\n\r\n```\r\njson.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 2453)\r\n```\r\n\r\nPython gives a little more information: the beginning of the second object is on the beginning of the second line of text. Though not satisfying the JSON specification, this file fits a widely used format that I like to call \"JSONS\": a complete JSON object on each line of text. The best way to parse such files is to split them at the line breaks (which are faster to find than balancing JSON curly brackets) and then running a JSON parser on each string.\r\n\r\nFor your case, this would be\r\n\r\n```python\r\n>>> import json\r\n>>> import time\r\n>>> import awkward1 as ak\r\n>>> import numpy as np\r\n>>> \r\n>>> builder = ak.ArrayBuilder()\r\n>>> for lineno, line in enumerate(open(\"dataset.json\")):\r\n...     if lineno % 114919 == 0:\r\n...         print(time.strftime(\"%H:%M:%S\"), \":\", lineno/114919, \"percent complete\")\r\n...     builder.append(json.loads(line))\r\n... \r\n08:47:16 : 0.0 percent complete\r\n08:47:23 : 1.0 percent complete\r\n08:47:29 : 2.0 percent complete\r\n...\r\n>>> array = builder.snapshot()\r\n```\r\n\r\nI got the total number of lines in this file, 11491971, by running `wc -l dataset.json` on the terminal. It took `wc` 6 seconds to find all the line breaks and count the number of lines; Python's JSON parsing + Awkward's rowwise-to-columnar translation would take 648 seconds if I had the memory for it.\r\n\r\nI ran out of memory trying to read this 16 GB JSON file; my computer has 15.5 GB of real memory and 4 GB of swap space. That's close. The text representation of each floating point number uses 9\u201213 characters (9\u201213 bytes), each of which becomes 8 bytes as a double precision floating point number. Add to that the indexing of the variable-length lists and the tags to build a union-type from the fact that this file mixes lists of different depths (i.e. different data types) in the same array, and the in-memory representation will be larger than this particular text representation. (If it had more or fewer digits, it would be a different story.)\r\n\r\nI changed the title of your issue into a feature request because my first thought was that you'd want the handling of JSONS format to be automatic. For future reference, that involves turning on RapidJSON's `kParseStopWhenDoneFlag` (see [Parsing documentation](http://miloyip.github.io/rapidjson/md_doc_dom.html#Parsing)) and setting up tests to make sure that Awkward's JSON SAX \u2192 ArrayBuilder does the right thing with the structure that RapidJSON serves in such a case.\r\n\r\nHaving that capability built in would mean that you wouldn't have to write a for loop to handle each line of text separately. (And it would be more capable: if the JSON objects didn't have line breaks between them, RapidJSON's SAX parser would separate them correctly.) However, if you have a file as large as this one, you'd still have to write a for loop or split it up in appropriate places if you don't have enough memory to hold the whole thing as an array.\r\n\r\nIn addition, this file has strings mixed in with the numbers: `\"NaN\"`, `\"inf\"`, and `\"-inf\"`. There is no JSON standard for the three \"non-finite\" floating point values, so the author of this file decided to make them strings. Awkward Array will read those in as strings; dealing with them after the fact means some manipulations. Maybe `ak.from_json` should have some options for interpreting these strings (or different spellings of them, like `\"nan\"` and `\"Infinity\"`, since there isn't a standard) as the appropriate floating point values instead of strings.\r\n\r\nNaively reading them in and doing\r\n\r\n```python\r\n>>> tonan = (array == \"NaN\")\r\n```\r\n\r\non a 10% sample used up a lot of memory\u2014the string-equality has been implemented in NumPy, rather than low-level C++, so it has some inefficiencies due to storing intermediate arrays. Also, I don't know if [ak.where](https://awkward-array.readthedocs.io/en/latest/_auto/ak.where.html) is currnetly up to the task of deeply replacing data with union-type (unions are the most complex type; that part might not have been implemented _yet_).\r\n\r\nReplacing these strings while reading makes the reading step take about twice as long:\r\n\r\n```python\r\n>>> def fix(x):\r\n...     if isinstance(x, list):\r\n...         return [fix(y) for y in x]\r\n...     elif x == \"NaN\" or x == \"-NaN\":\r\n...         return np.nan\r\n...     elif x == \"inf\":\r\n...         return np.inf\r\n...     elif x == \"-inf\":\r\n...         return -np.inf\r\n...     elif isinstance(x, str):\r\n...         raise Exception(\"unhandled string: \" + repr(x))\r\n...     else:\r\n...         return x\r\n... \r\n>>> builder = ak.ArrayBuilder()\r\n>>> for lineno, line in enumerate(open(\"dataset.json\")):\r\n...     if lineno % 114919 == 0:\r\n...         print(time.strftime(\"%H:%M:%S\"), \":\", lineno/114919, \"percent complete\")\r\n...     builder.append(fix(json.loads(line)))\r\n... \r\n09:29:12 : 0.0 percent complete\r\n09:29:22 : 1.0 percent complete\r\n09:29:32 : 2.0 percent complete\r\n09:29:42 : 3.0 percent complete\r\n09:29:52 : 4.0 percent complete\r\n09:30:02 : 5.0 percent complete\r\n09:30:12 : 6.0 percent complete\r\n09:30:21 : 7.0 percent complete\r\n09:30:31 : 8.0 percent complete\r\n09:30:41 : 9.0 percent complete\r\n09:30:51 : 10.0 percent complete\r\n^CTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"<stdin>\", line 3, in fix\r\n  File \"<stdin>\", line 3, in <listcomp>\r\n  File \"<stdin>\", line 3, in fix\r\n  File \"<stdin>\", line 3, in <listcomp>\r\n  File \"<stdin>\", line 3, in fix\r\n  File \"<stdin>\", line 3, in <listcomp>\r\n  File \"<stdin>\", line 3, in fix\r\n  File \"<stdin>\", line 3, in <listcomp>\r\n  File \"<stdin>\", line 4, in fix\r\nKeyboardInterrupt\r\n>>> array = builder.snapshot()\r\n>>> ak.type(array)\r\n1156435 * var * union[float64, var * union[float64, var * var * float64]]\r\n```\r\n\r\nBut at least now you're only dealing with\r\n\r\n   * list of numbers (`var * float64`)\r\n   * lists of lists of numbers (`var * var * float64`)\r\n   * lists of lists of lists of lists of numbers (`var * var * var * var * float64`).\r\n\r\nAwkward Array can load union-type data, but this type is more limited in operations. In your dataset, it looks like each outer list has exactly 6 elements and each of these has a different depth of structure. Putting even more intelligence into the reading process,\r\n\r\n```python\r\n>>> b1 = ak.ArrayBuilder()\r\n>>> b2 = ak.ArrayBuilder()\r\n>>> b3 = ak.ArrayBuilder()\r\n>>> b4 = ak.ArrayBuilder()\r\n>>> b5 = ak.ArrayBuilder()\r\n>>> b6 = ak.ArrayBuilder()\r\n>>> for lineno, line in enumerate(open(\"dataset.json\")):\r\n...     if lineno % 114919 == 0:\r\n...         print(time.strftime(\"%H:%M:%S\"), \":\", lineno/114919, \"percent complete\")\r\n...     l1, l2, l3, l4, l5, l6 = fix(json.loads(line))\r\n...     b1.append(l1)\r\n...     b2.append(l2)\r\n...     b3.append(l3)\r\n...     b4.append(l4)\r\n...     b5.append(l5)\r\n...     b6.append(l6)\r\n... \r\n09:41:48 : 0.0 percent complete\r\n09:41:58 : 1.0 percent complete\r\n09:42:07 : 2.0 percent complete\r\n09:42:16 : 3.0 percent complete\r\n09:42:25 : 4.0 percent complete\r\n09:42:34 : 5.0 percent complete\r\n09:42:44 : 6.0 percent complete\r\n09:42:53 : 7.0 percent complete\r\n09:43:02 : 8.0 percent complete\r\n09:43:11 : 9.0 percent complete\r\n09:43:20 : 10.0 percent complete\r\n^CTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"/home/jpivarski/miniconda3/lib/python3.8/json/__init__.py\", line 357, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/home/jpivarski/miniconda3/lib/python3.8/json/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"/home/jpivarski/miniconda3/lib/python3.8/json/decoder.py\", line 353, in raw_decode\r\n    obj, end = self.scan_once(s, idx)\r\nKeyboardInterrupt\r\n>>> a1 = b1.snapshot()\r\n>>> a2 = b2.snapshot()\r\n>>> a3 = b3.snapshot()\r\n>>> a4 = b4.snapshot()\r\n>>> a5 = b5.snapshot()\r\n>>> a6 = b6.snapshot()\r\n>>> ak.type(a1)\r\n1155960 * float64\r\n>>> ak.type(a2)\r\n1155960 * float64\r\n>>> ak.type(a3)\r\n1155960 * int64\r\n>>> ak.type(a4)\r\n1155960 * var * float64\r\n>>> ak.type(a5)\r\n1155960 * var * float64\r\n>>> ak.type(a6)\r\n1155960 * var * var * var * float64\r\n```\r\n\r\nwhich would be a lot easier to deal with if each of these 6 arrays have different meanings. In fact, it's a little faster to read than the union case (fewer cases to handle in ArrayBuilder) and might even fit into memory now that we don't need to store a bunch of tags to represent the union structure.\r\n\r\nSo to summarize this long-winded answer (which I spent too much time on because it piqued my interest), automatically recognizing JSONS format would be a nice feature, as would automatically substituting strings for the non-finite floating point values. However, in your case, you'd _still_ want to iterate over these data manually to make their structure easier to use later in the analysis. Perhaps, ultimately, the ideal is to include yet more features: better memory handling in expressions like `array == \"NaN\"` and better ways to deal with union-typed data. But on the timescale of this year, you're better off writing a Python loop to ingest the data, \"fixing\" it as you go.\r\n\r\nOne last comment: I tried loading the whole thing as 6 separate arrays and ran out of memory at 74%. This technique uses _less_ memory, but the file still needs to be split into at least two parts for a computer with 15.5 GB of RAM. Be sure to split it on a line boundary. Or if you don't need all 6 of those differently typed arrays, dropping the ones you won't use before loading them into ArrayBuilders will use a lot less memory. (Watch it in `htop`: the memory use jumps up and down because everything that's not being accumulated into arrays gets garbage collected by Python.)\r\n\r\nAnother \"last\" comment: once you've read these arrays into columnar form, save them as Parquet files: [ak.to_parquet](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_parquet.html). Parquet is a numerical format that stores lists of lists of lists in a _columnar_ way, like Awkward and unlike JSON/JSONS. With an exact 10% sample (1149197 lines from the original JSONS file):\r\n\r\n```python\r\n>>> array = ak.Array({\"a1\": a1, \"a2\": a2, \"a3\": a3, \"a4\": a4, \"a5\": a5, \"a6\": a6})\r\n>>> ak.type(array)\r\n1149197 * {\"a1\": float64, \"a2\": float64, \"a3\": int64, \"a4\": var * float64, \"a5\": var * float64, \"a6\": var * var * var * float64}\r\n>>> array.a1\r\n<Array [47.9, 35, 26.6, ... 67.3, 23.9, 32.3] type='1149197 * float64'>\r\n>>> array.a2\r\n<Array [1.89, 0.61, -0.53, ... 0.569, 0.655] type='1149197 * float64'>\r\n>>> array.a3\r\n<Array [5, 5, 5, 5, 5, 5, ... 0, 5, 5, 5, 5, 0] type='1149197 * int64'>\r\n>>> array.a4\r\n<Array [[21.2, 8.37, 29, ... -1, -inf, -inf]] type='1149197 * var * float64'>\r\n>>> array.a5\r\n<Array [[32.9, 3, 7, 0.162, ... inf, -1, -inf]] type='1149197 * var * float64'>\r\n>>> array.a6\r\n<Array [[[[0.0312, 0.082, ... 1, 0.644]]]] type='1149197 * var * var * var * flo...'>\r\n>>> ak.to_parquet(array, \"array.parquet\")\r\n```\r\n\r\ncreated a file that is 30\u00d7 smaller, so 3\u00d7 smaller per datum (remember that this is a 10% sample) and took 3.3 seconds to load:\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> starttime = time.time(); array = ak.from_parquet(\"array.parquet\"); print(time.time() - starttime)\r\n3.3123741149902344\r\n>>> array.a1\r\n<Array [47.9, 35, 26.6, ... 67.3, 23.9, 32.3] type='1149197 * ?float64'>\r\n>>> array.a2\r\n<Array [1.89, 0.61, -0.53, ... 0.569, 0.655] type='1149197 * ?float64'>\r\n>>> array.a3\r\n<Array [5, 5, 5, 5, 5, 5, ... 0, 5, 5, 5, 5, 0] type='1149197 * ?int64'>\r\n>>> array.a4\r\n<Array [[21.2, 8.37, 29, ... -1, -inf, -inf]] type='1149197 * option[var * ?floa...'>\r\n>>> array.a5\r\n<Array [[32.9, 3, 7, 0.162, ... inf, -1, -inf]] type='1149197 * option[var * ?fl...'>\r\n>>> array.a6\r\n<Array [[[[0.0312, 0.082, ... 1, 0.644]]]] type='1149197 * option[var * option[v...'>\r\n```\r\n\r\nwhich is 28\u00d7 faster than parsing the JSONS (93 seconds vs 3.3 seconds). It still doesn't solve the problem of memory overhead\u2014I can't, for instance, load 10 of these into the same Python process.\r\n\r\nNow I'm going to delete this 16 GB file and I'm not going to download it again. Good luck!",
  "created_at":"2020-09-07T15:21:00Z",
  "id":688389890,
  "issue":437,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4ODM4OTg5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-07T15:21:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks Jim for your time and effort. I just skimmed through your reply, lemme work through it.",
  "created_at":"2020-09-07T17:01:41Z",
  "id":688438784,
  "issue":437,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4ODQzODc4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-07T17:01:41Z",
  "user":"MDQ6VXNlcjMwNDI2NDM2"
 },
 {
  "author_association":"MEMBER",
  "body":"Done (in #568): both multi-JSON and nan/inf/-inf strings.",
  "created_at":"2020-12-03T23:44:16Z",
  "id":738455245,
  "issue":437,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczODQ1NTI0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-03T23:44:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I approved this PR. If all the tests pass after this correction for Python 2.7 and you don't have any more plans for it, you're free to squash-and-merge the PR into master.",
  "created_at":"2020-09-08T16:59:15Z",
  "id":689011129,
  "issue":438,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTAxMTEyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-08T16:59:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for fixing the Python 2.7 issue! ",
  "created_at":"2020-09-08T17:24:34Z",
  "id":689024804,
  "issue":438,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTAyNDgwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-08T17:24:34Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"NONE",
  "body":"You were faster a little bit! Here is a bit more detailed issue: https://github.com/scikit-hep/awkward-1.0/issues/443",
  "created_at":"2020-09-09T14:14:41Z",
  "id":689591338,
  "issue":442,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTU5MTMzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T14:14:41Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Looking at your issue, I don't see how they're related. They might be related. I need to do a quick profile of this because there's a lot of linear time here that isn't due to NumPy, and I can't imagine what it might be.",
  "created_at":"2020-09-09T14:21:37Z",
  "id":689595732,
  "issue":442,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTU5NTczMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-09T14:21:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes you are right, my issue is more related to the overhead when dealing with tinier bits of data.",
  "created_at":"2020-09-09T14:59:55Z",
  "id":689620067,
  "issue":442,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTYyMDA2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T14:59:55Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #444 fixes the linear scaling issue:\r\n\r\n![quick-plot-2](https://user-images.githubusercontent.com/1852447/92623284-a290dc80-f28b-11ea-9a33-144257b0b5ee.png)\r\n\r\nThe initial implementation conservatively assumed that ListArray/ListOffsetArray's starts/stops/offsets are always incompatible and have to be normalized to the same structure before applying the ufunc. That's why the discrepancy with NumPy scaled linearly: that normalization adjusts all of the starts/stops/offsets as well as the content to fit. In this case, there was _only one list_ in the calculation (`array < 3`), so of course it wouldn't need to be normalized to itself!\r\n\r\nNow we check for consistency and only do the normalization if necessary, so simple cases are brought exactly in line with NumPy because only NumPy is doing anything that scales with the size of the array. I had thought about adding a check for this before, but functionality comes before optimization and I wasn't sure if the check for consistency might be as expensive as the normalization. Your example demonstrates that it's not\u2014not by a long shot!\r\n\r\nIt is still the case that if you have any ListArrays with gaps (e.g. `stops[i] == N` and `starts[i + 1] != N`), the full normalization applies. This is because the ufunc is not supposed to be executed on unreachable data (e.g. `content[N]`). This is partly for performance, so that you're not running the ufunc on unreachable data that won't matter to the output anyway, but also for correctness: if the ufunc would raise an error for unreachable data but not for reachable data, then the ufunc should not raise an error. That will save person-years of head-scratching!\r\n\r\nThe scaling as presented above is what we expect and strive for: the arrays have to be relatively large (10000 entries in this example) before they perform as well as NumPy. Optimizing operations on tiny arrays is out of scope for the Awkward array-at-a-time functions. The whole idea is that you have big data to work on; if it's small and there's only one of them, you don't notice the fraction of a second anyway, but if you have a lot of small arrays, that doesn't fit our vectorized model well. For that, you'd want just-in-time compilations: Numba or Julia!",
  "created_at":"2020-09-09T16:18:54Z",
  "id":689669226,
  "issue":442,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTY2OTIyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T16:18:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Awesome, thanks for both the implementation and the fix!\r\n\r\n...and of course for the detailed description of the whole problem",
  "created_at":"2020-09-09T17:36:23Z",
  "id":689712929,
  "issue":442,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTcxMjkyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T17:36:23Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #444 fixed a linearly scaling performance bug in handling jaggedness. All of your examples here are non-jagged, so that doesn't apply.\r\n\r\nIn issue #442, the overhead of doing a binary ufunc between a doubly jagged array and a scalar is about 500 \u00b5s. In these examples of a binary ufunc between a non-jagged array and a scalar is 144 \u00b5s. That's not too different, considering that the metadata-handling function does three recursive steps for a doubly jagged array and one for a non-jagged array.\r\n\r\nIf we check the scaling, we see that this is indeed a constant overhead, which is expected.\r\n\r\n```python\r\nIn [10]: for n in (100, 1000, 10000, 100000, 1000000, 10000000):\r\n    ...:     print(n)\r\n    ...:     arr_np = np.random.rand(n)\r\n    ...:     arr_ak = ak.Array(arr_np)\r\n    ...:     %timeit arr_np / 10\r\n    ...:     %timeit arr_ak / 10\r\n    ...: \r\n100\r\n809 ns \u00b1 19.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n157 \u00b5s \u00b1 1.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n1000\r\n1.43 \u00b5s \u00b1 24.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n160 \u00b5s \u00b1 2.89 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n10000\r\n6.19 \u00b5s \u00b1 65.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n165 \u00b5s \u00b1 2.29 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n100000\r\n51.5 \u00b5s \u00b1 693 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n212 \u00b5s \u00b1 7.84 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n1000000\r\n637 \u00b5s \u00b1 11.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n808 \u00b5s \u00b1 13 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n10000000\r\n12.3 ms \u00b1 58.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n12.8 ms \u00b1 237 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nThe whole idea of Awkward Array is that you can't eliminate the overhead of dealing with complex structures whose type is known after compile-time (i.e. not like Julia), but you can move all of it into constant-time metadata-twiddling, leaving the linear-time part for precompiled, specialized loops, most often in NumPy itself. That's the thing that's new, as far as I can tell: isolating structure manipulations to a constant-time part.",
  "created_at":"2020-09-09T16:49:52Z",
  "id":689686643,
  "issue":443,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTY4NjY0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T16:49:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> ```python\r\n> %timeit arr_ak.__array__() / 10\r\n> \r\n> 9.5 ms \u00b1 138 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n> ```\r\n\r\nThe reason that the conversion to a NumPy array is not instantaneous is that it has to check to see if it's legal\u2014i.e. if the data are rectilinear. `ak.Array.__array__` calls [ak.to_numpy](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_numpy.html), which does not return NumPy `dtype=\"O\"` type arrays: it raises an error if the data aren't shaped right.\r\n\r\nPerhaps it's worth investigating if this time is expected for a NumpyArray, which is just one layer to unwrap.\r\n\r\nThe fastest thing you can do if you know you have a plain NumpyArray is to pull it out as a `layout` and cast it as NumPy. The NumpyArray object is a Python buffer.\r\n\r\n```python\r\nIn [15]: arr_ak\r\nOut[15]: <Array [0.454, 0.245, 0.282, ... 0.362, 0.809] type='10000000 * float64'>\r\n\r\nIn [16]: arr_ak.layout\r\nOut[16]: <NumpyArray format=\"d\" shape=\"10000000\" data=\"0.454025 0.245414 0.28192 0.23692 0.577678 ... 0.867228 0.0388754 0.484831 0.362226 0.809129\" at=\"0x7f40c2dcb010\"/>\r\n\r\nIn [17]: %timeit arr_ak.__array__()\r\n5.43 ms \u00b1 94.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [18]: %timeit np.asarray(arr_ak.layout)\r\n954 ns \u00b1 36.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n```\r\n\r\n(These timings are unaffected by whether the array has 10000000 entries or 100 entries. It's all constant-time unwrapping.)\r\n\r\nThe `layout` is the \"low-level but public\" interface, intended for framework developers like you. When you access this layer, however, you have to be aware of low-level issues like the \"unreachable data\" I talked about on issue #340 \r\n\r\nhttps://awkward-array.readthedocs.io/en/latest/ak.layout.Content.html",
  "created_at":"2020-09-09T17:00:18Z",
  "id":689693926,
  "issue":443,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTY5MzkyNg==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2020-09-09T17:02:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Since this issue is about small arrays (100 entries is very small!), I'm going to close it. I intend to focus more on performance issues next year, after Awkward 0 and Uproot 3 are fully and finally retired, but even then the performance issues will be the linearly scaling ones. Only overwhelmingly bad constant-time scaling issues will be addressed (which the 5 ms for NumpyArray \u2192 ndarray might be).\r\n\r\nWhat I had intended to focus on today is a C++ implementation of doubly jagged numbers \u2192 Awkward, as discussed in scikit-hep/uproot4#90. The full generality has to wait until next year, but I was thinking about it and this special case is both easy and important. That might fix your problem by allowing you to work with larger arrays.\r\n\r\nBut the issue treadmill keeps going! More have come in while answering this...",
  "created_at":"2020-09-09T17:16:26Z",
  "id":689703063,
  "issue":443,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTcwMzA2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T17:16:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks Jim, that `np.asarray(Awkward.Array().layout)` is very helpful and the way to go. This issue is anyways very minor and specific to this particular analysis implementation I am about to review, so no worries!\r\n\r\n> But the issue treadmill keeps going! More have come in while answering this...\r\n\r\n\ud83d\ude09 \r\n\r\nI wish I had a bit more time to also invest in understanding the internals of awkward1 or even to contribute; I'll try harder in future.\r\n",
  "created_at":"2020-09-09T17:30:04Z",
  "id":689709816,
  "issue":443,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTcwOTgxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T17:30:04Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"> We should add a flag to allow `cuda-build.sh` to use the system nvcc to compile the CUDA source files, instead of always using docker to do so.\r\n\r\nIs the download or storage of the Docker image a problem? Using the system nvcc could be a non-default option.\r\n\r\nI had considerable trouble installing an nvcc that is consistent with my compiler (gcc) because I use conda to install a compiler. There are two ways to install nvcc with conda; the one that worked involved downloading a 6 GB install script. (The conda package was 4 kB, but the \"installing\" phase took over an hour because of that internal install script.) The Docker image is itself 6 GB, but at least I know where it is and why it's there.\r\n\r\nAlso, it must be in a Docker image for the Azure build test, since that's by far the easiest way to guarantee that the Azure node is configured properly. Fortunately, Azure is well-connected and a 6 GB download in every CI run isn't a big deal!\r\n\r\n> Also, it would be good to allow the user to set their own options to nvcc's `--gencode` flag - https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-steering-gpu-code-generation-generate-code\r\n\r\n@trickarcher suggested that we compile in CUDA 9.0 because it works equally well on later systems. As I understand it, the PTX (virtual architecture) produced by a given version of CUDA gets translated into device-specific bytecode (real architecture) anyway, so what benefit would it be to generate PTX for different CUDA versions?",
  "created_at":"2020-09-09T17:34:15Z",
  "id":689711844,
  "issue":445,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTcxMTg0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T17:34:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"By the way, if you want this (item 1, the flag for native nvcc), go ahead and do it.\r\n\r\nThe cuda-build.sh script is pretty simple, so adding a second option in an order-independent way would mean having to check both orders\u2014beyond that, we should think about real solutions. But while we're still talking about just two flags, we can do the hacky thing and check all combinations of order (both of them).",
  "created_at":"2020-09-09T19:29:17Z",
  "id":689770126,
  "issue":445,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTc3MDEyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T19:29:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Is the download or storage of the Docker image a problem? Using the system nvcc could be a non-default option.\r\n\r\nIt just seems weird that we need docker to compile the CUDA code. I understand that we need it for AWS, but I feel that we should still provide a way to use the system nvcc to compile the code.\r\n\r\n> By the way, if you want this (item 1, the flag for native nvcc), go ahead and do it.\r\n\r\nThanks!\r\n\r\n> @trickarcher suggested that we compile in CUDA 9.0 because it works equally well on later systems. As I understand it, the PTX (virtual architecture) produced by a given version of CUDA gets translated into device-specific bytecode (real architecture) anyway, so what benefit would it be to generate PTX for different CUDA versions?\r\n\r\nThe bytecode generated by nvcc if no flags are specified is the lowest supported architecture and compute values for the installed version of CUDA. Architecture is backwards compatible - code compiled for `compute_35` would also work on `compute_60`, so we don't _need_ users to pass the appropriate arch and compute flags for their hardware to nvcc, but if you have more recent hardware and the bytecode generated is for the lowest supported architecture, then you might be losing out on some optimizations for newer hardware or utilizing newer instructions.\r\nAnd when you say \"generate PTX for different CUDA versions\" do you mean generating bytecode for architecture not supported by the installed CUDA version? I don't mean that. If one tried to compile to a lower compute/sm than supported by the installed version of CUDA, they will get a compilation error I think. ",
  "created_at":"2020-09-09T20:34:51Z",
  "id":689806909,
  "issue":445,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTgwNjkwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T20:37:33Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> The bytecode generated by nvcc if no flags are specified is the lowest supported architecture and compute values for the installed version of CUDA. Architecture is backwards compatible - code compiled for `compute_35` would also work on `compute_60`, so we don't _need_ users to pass the appropriate arch and compute flags for their hardware to nvcc, but if you have more recent hardware and the bytecode generated is for the lowest supported architecture, then you might be losing out on some optimizations for newer hardware or utilizing newer instructions.\r\n\r\nOkay, but we have to compile for the least common denominator because the wheel we deploy could be used anywhere. (Users with hardware that's too old for CUDA 9.0 are out of luck.) We don't have any plans for users to be able to compile their own copy of awkward1-cuda-kernels: if that's what you have in mind, that would be a new path, and a _very_ tricky one to get right.",
  "created_at":"2020-09-09T21:17:47Z",
  "id":689827160,
  "issue":445,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTgyNzE2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T21:17:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> We don't have any plans for users to be able to compile their own copy of awkward1-cuda-kernels: if that's what you have in mind, that would be a new path, and a very tricky one to get right.\r\n\r\nI just realised that users will be getting the compiled CUDA kernels from PyPI and only developers will need to compile the CUDA source code using `cuda-build.sh`. This definitely removes any requirement for the second feature request (user defined `gencode` values) and reduces how strongly I felt about adding a choice for using the system nvcc - if I decide to add it, it will definetely be after everything else is done. \r\nSorry about that :( \r\nShould we close this issue and perhaps create a new one later if I decide to add a flag to `cuda-build.sh` to use system nvcc to compile the CUDA source?",
  "created_at":"2020-09-10T06:56:37Z",
  "id":690030978,
  "issue":445,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDAzMDk3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T06:56:37Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"If you're not planning to do it right away, let's close the issue. If at any point you think it would help you to add it, add it in a pull request and I'll check it there. That helps to keep the issues board clear, which I need to prioritize work.\r\n\r\n(Actually, I'll close it now so we don't forget.)",
  "created_at":"2020-09-10T12:10:52Z",
  "id":690233108,
  "issue":445,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDIzMzEwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T12:10:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Many of these commits, starting with \"Differences in parameters are checked at the 'mergeable' level..\", are actually from PR #449, which I merged into this for testing. It all goes to the same master branch, anyway.",
  "created_at":"2020-09-12T13:25:11Z",
  "id":691487439,
  "issue":448,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTQ4NzQzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T13:25:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - thanks for heads-up! It makes total sense.",
  "created_at":"2020-09-11T10:14:42Z",
  "id":691009817,
  "issue":449,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTAwOTgxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-11T10:14:42Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"One last thing to do: the Python ak.concatenate function should call `mergemany`, not `merge`.",
  "created_at":"2020-09-12T00:46:20Z",
  "id":691368059,
  "issue":449,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTM2ODA1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T00:46:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@trickarcher Maybe this PR also solves the issue about the missing tests for the kernels you mentioned earlier on slack.",
  "created_at":"2020-09-17T08:47:32Z",
  "id":694091611,
  "issue":453,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDA5MTYxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T08:47:32Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, this fixes that issue. Thanks!",
  "created_at":"2020-09-17T11:17:12Z",
  "id":694165051,
  "issue":453,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDE2NTA1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T11:17:12Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @trickarcher! Since all of your work is in the \"studies\" directory, I don't think you'll be affected by the name change that's coming early next week: all instances of \"`awkward1`\" will become \"`awkward`\" ([see details](https://github.com/scikit-hep/awkward-1.0/wiki/Name-change-procedure)). I've been warning all the other PR developers about it, but I don't think your PR will have any issues.",
  "created_at":"2020-11-23T21:40:05Z",
  "id":732442289,
  "issue":454,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMjQ0MjI4OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-23T21:40:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I wanted to point you to this, even while it is still a draft, in case there were any changes to be made in spec.yml.",
  "created_at":"2020-09-17T12:37:27Z",
  "id":694201803,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDIwMTgwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T12:37:27Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":">What had originally been a separate samples.json should be part of this monolithic YAML. The top-level of this document should be a mapping with a place for front-matter (description: \"Specification for all Awkward Array kernel functions.\"). The list that this spec.yml currently represents should be under a key named kernels, and there should be another key named tests that has samples.json converted into YAML. With the files combined like that, there would be no reason to have \"kernel-specification\" as a directory. Instead, it could be a single file at the top level of the repo named \"kernel-specification.yml\".\r\n\r\nCan I do this in a different PR? ",
  "created_at":"2020-09-17T17:38:32Z",
  "id":694390498,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDM5MDQ5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T17:38:32Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> > What had originally been a separate samples.json should be part of this monolithic YAML. The top-level of this document should be a mapping with a place for front-matter (description: \"Specification for all Awkward Array kernel functions.\"). The list that this spec.yml currently represents should be under a key named kernels, and there should be another key named tests that has samples.json converted into YAML. With the files combined like that, there would be no reason to have \"kernel-specification\" as a directory. Instead, it could be a single file at the top level of the repo named \"kernel-specification.yml\".\r\n> \r\n> Can I do this in a different PR?\r\n\r\nWhen this list is put inside of a structure with front-matter, it will add two spaces to every line.\r\n\r\nMaybe do this for now:\r\n\r\n```yaml\r\nkernels:\r\n  - name: first_kernel\r\n    ...\r\n  - name: second_kernel\r\n    ...\r\n```\r\n\r\nso that the indentation is right, and we can add front matter before the `kernels:` line later. At least this way, most of the text will be final.",
  "created_at":"2020-09-17T17:48:40Z",
  "id":694396203,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDM5NjIwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T17:48:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Or we could have thousands of line changes. It's not the first time it's happened and it doesn't break git. We want to minimize it, but if it's hard to do so, it's okay.",
  "created_at":"2020-09-17T17:54:18Z",
  "id":694399362,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDM5OTM2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T17:54:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">The kernels with a specialization for every numeric type, such as awkwrad_NumpyArray_fill, are not sorted by type, though they'll be much easier to maintain if they are.\r\n\r\nFor specializations that are named something like `from<some_type>to<some_type>` (eg - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward_NumpyArray_fill_toint16_fromuint32), we order by the first type in the name? ",
  "created_at":"2020-09-17T17:58:15Z",
  "id":694401728,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQwMTcyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T18:00:38Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":">Or we could have thousands of line changes. It's not the first time it's happened and it doesn't break git. We want to minimize it, but if it's hard to do so, it's okay.\r\n\r\nI didn't realise it would involve a lot of changes. I'll just implement all of the fixes you suggested and once those are in place, I'll work on this one :D So we won't need different PRs.",
  "created_at":"2020-09-17T17:59:41Z",
  "id":694402588,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQwMjU4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T17:59:41Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> > The kernels with a specialization for every numeric type, such as awkwrad_NumpyArray_fill, are not sorted by type, though they'll be much easier to maintain if they are.\r\n> \r\n> For specializations that are named something like `from<some_type>to<some_type>` (eg - https://awkward-array.readthedocs.io/en/latest/_auto/kernels.html#awkward_NumpyArray_fill_toint16_fromuint32), we order by the first type in the name?\r\n\r\nLexicographically (as it is now in the header file, by the way). First sort by first template parameter, then sort by second. You can define a `sorted` key function by returning the two as a tuple and Python knows how to lexicographically sort tuples.",
  "created_at":"2020-09-17T18:07:14Z",
  "id":694406746,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQwNjc0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T18:07:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski Can you take a look? If this is okay, I'll merge the samples.json into this (and delete dev/generate-kernelspec.py)",
  "created_at":"2020-09-18T08:32:33Z",
  "id":694735071,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDczNTA3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T08:32:55Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":">The only thing that I see is that awkward_NumpyArray_fill is not sorted. I checked a few others with specializations and this wasn't true for them, either.\r\n\r\nThe sorting should be fixed now.\r\n\r\n>The documentation, tests, and CUDA generation can all start from the specification in this form?\r\n\r\nYes.\r\n\r\nIf you give me the go ahead, I'll add in the samples.json to the yml file ;) ",
  "created_at":"2020-09-18T14:05:16Z",
  "id":694888567,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDg4ODU2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T14:05:16Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> The sorting should be fixed now.\r\n\r\nIt is!\r\n\r\n> If you give me the go ahead, I'll add in the samples.json to the yml file ;)\r\n\r\nYes, please!",
  "created_at":"2020-09-18T14:09:04Z",
  "id":694890720,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDg5MDcyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T14:09:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski The PR should be complete. ",
  "created_at":"2020-09-18T15:58:33Z",
  "id":694949715,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDk0OTcxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T15:58:33Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"I don't remember what we said: is this done now? Can I merge it?",
  "created_at":"2020-09-18T18:18:28Z",
  "id":695015763,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTAxNTc2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T18:18:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I don't think we reached a definite conclusion about this. Maybe I should change extra-tests to manual-tests and add the automatic-tests field before we merge this in? ",
  "created_at":"2020-09-18T18:28:29Z",
  "id":695020331,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTAyMDMzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T18:28:29Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, that's right. Okay, I'll wait for you to do that, then give me an explicit go-ahead.",
  "created_at":"2020-09-18T18:50:09Z",
  "id":695030161,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTAzMDE2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T18:50:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Should be done now.",
  "created_at":"2020-09-19T09:36:22Z",
  "id":695191096,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTE5MTA5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T09:36:22Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Great! This will make it into 0.3.0.",
  "created_at":"2020-09-19T11:21:37Z",
  "id":695201061,
  "issue":455,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTIwMTA2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T11:21:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"For the _record_ I did propose also to only disable ufuncs on arrays where `layout.purelist_parameter(\"__record__\") == None` but I learned that this name is used for other purposes than just defining the desired behavioral mixin (e.g. if multiple differently-named record is encountered in ArrayBuilder, then the resulting array will be a union of each record type rather than a single record type with optional subfields).",
  "created_at":"2020-09-18T14:40:16Z",
  "id":694908704,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDkwODcwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T14:40:16Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Also, I want to keep these rules of \"what applies when\" as simple as possible, to minimize surprising behavior.",
  "created_at":"2020-09-18T15:17:20Z",
  "id":694928666,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDkyODY2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T15:17:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm not going to change the policy on this: the behavior will stay the same. (Although NumPy ufuncs won't be allowed on strings, as in the above example: #504.)",
  "created_at":"2020-10-30T22:42:01Z",
  "id":719831543,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgzMTU0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:42:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@lgray FYI\r\nIsn't the risk of user error extremely large here?",
  "created_at":"2020-10-30T22:47:27Z",
  "id":719832912,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgzMjkxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:47:27Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"So yeah, my 2 cents here is that the probability of the user doing something they don't intend is incredibly large.\r\n\r\nNick ran into an interesting case where NanoEvents jets (which are four vectors beneath) can be element-wise multiplied by each other. This doesn't have a defined behavior from a physics standpoint and would also be very easy to accidentally do or do intentionally without knowing what it meant. In either case it can result in technically valid but wrong behavior when writing analysis - which is a much less preferred outcome than throwing an error.\r\n\r\n",
  "created_at":"2020-10-30T22:57:10Z",
  "id":719835238,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgzNTIzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:57:10Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"The way it works now is more prone to error for the interpretation of records as objects. \"Extremely\" is subjective.\r\n\r\nHeck, in crafting any kind of response to this, I'm only convincing myself that it needs to be done. It is the case that examples of this antifeature are actually in some of my slides, so \"fixing\" it will make my slides wrong.\r\n\r\nBut it is an important point. I think this issue could be implemented as an inhibitor that prevents broadcasting through any records unless a customization is defined. Note that that even means this:\r\n\r\n```python\r\n>>> array = ak.Array([{\"only\": 1}, {\"only\": 2}, {\"only\": 3}])\r\n>>> array + 10\r\n<Array [{only: 11}, {only: 12}, {only: 13}] type='3 * {\"only\": int64}'>\r\n```\r\n\r\nIf/when people complain that what used to work no longer works, I'll just have to explain that. (In the above example, they'd have to do `array.only + 10`.)",
  "created_at":"2020-10-30T23:27:18Z",
  "id":719843034,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg0MzAzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T23:27:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm calling it a bug because I've just defined the current behavior as wrong. (Even though I've presented it in talks.)\r\n\r\n(A motivator for being short with these things is that the list of issues is a lot longer than I thought, and I have only until December 1 to make this `awkward==1.0.0`.)",
  "created_at":"2020-10-30T23:29:43Z",
  "id":719843600,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg0MzYwMA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-10-30T23:29:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yeah - I agree that a good policy is to not allow broadcasting ufuncs through records unless that ufunc has been defined for that record type. This has the added benefit that people have to agree on the ufuncs/interface for whatever domain-specific set of records they want to use.",
  "created_at":"2020-10-31T17:01:55Z",
  "id":719960466,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTk2MDQ2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-31T17:01:55Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"@henryiii I'm working on this now: broadcasting will be prevented from passing through records unless an overload has been implemented. For example, `array_of_records * 10` won't multiply all fields of the records by `10` unless those records have explicitly defined a `behavior` for `np.multiply`. Thus, a Lorentz vector object in cylindrical won't accidentally get its `phi` member multiplied by `10`, but the cartesian object would have to explicitly define this.\r\n\r\nIt is a change in interface, and I've even presented examples of this interface in public, so I have to Do The Right Thing and give it a deprecation cycle. It's _removing_ a feature, which is good because _changing_ a feature doesn't have a way to provide both old and new behaviors for an overlapping time period. What will happen is this: if any function attempts to broadcast through a record, it will raise a warning now, saying that this will become an error in version 1.0.0 on Dec 1, 2020, unless that broadcast is an overloaded ufunc.\r\n\r\nThen I'm going to push this change into a released version of Awkward Array right away, so that about a month of releases will get the message. I'm sure there are users who update less frequently than that, and unfortunately, they won't get the message. Post-1.0.0, I'll have to start publishing regular release schedules, so that deprecation warnings can refer to both a version number and a date.",
  "created_at":"2020-11-03T18:39:28Z",
  "id":721307248,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTMwNzI0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T18:39:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In Awkward Array 0.4.1, which I want to release later today, the warning that you get from\r\n\r\n```python\r\none = awkward1.Array([{\"x\": 1}, {\"x\": 2}, {\"x\": 3}])\r\ntwo = awkward1.Array([{\"x\": 1.1}, {\"x\": 2.2}, {\"x\": 3.3}])\r\none + two\r\n```\r\n\r\nis\r\n\r\n```\r\nDeprecationWarning: In version 1.0.0 (target date: 2020-12-01), this will be an error.\r\n(Set ak.deprecations_as_errors = True to get a stack trace now.)\r\n\r\nValueError: cannot broadcast: <class 'awkward1._ext.RecordArray'>, <class 'awkward1._ext.RecordArray'>\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/0.4.1/src/awkward1/_util.py#L886)\r\n```\r\n\r\nBut\r\n\r\n```python\r\ndef overload_add(left, right):\r\n    return awkward1.Array({\"x\": left.x + right.x})\r\n\r\nbehavior = {}\r\nbehavior[numpy.add, \"Overload\", \"Overload\"] = overload_add\r\n\r\none = awkward1.Array([{\"x\": 1}, {\"x\": 2}, {\"x\": 3}], with_name=\"Overload\", behavior=behavior)\r\ntwo = awkward1.Array([{\"x\": 1.1}, {\"x\": 2.2}, {\"x\": 3.3}], with_name=\"Overload\", behavior=behavior)\r\n\r\nassert (one + two).tolist() == [{\"x\": 2.1}, {\"x\": 4.2}, {\"x\": 6.3}]\r\n```\r\n\r\nraises no warnings or errors.",
  "created_at":"2020-11-03T20:19:54Z",
  "id":721354241,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTM1NDI0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T20:19:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Looks like the correct behavior to me. :-)",
  "created_at":"2020-11-03T20:23:53Z",
  "id":721356006,
  "issue":457,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTM1NjAwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T20:23:53Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"I had the same thought\u2014I'm also dropping commented out old code for the scikit-build based build, which is now old enough that swapping in these commented out lines probably wouldn't work.\r\n\r\nI'm glad this works for the Conda build now!",
  "created_at":"2020-09-18T14:38:00Z",
  "id":694907507,
  "issue":458,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDkwNzUwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T14:38:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Types are different if any of their parameters are different\u2014this is what makes a list of numbers different from a string.\r\n\r\nBut perhaps that definition could be reduced to \"types are different if their `__array__` or `__record__` parameter is different.\"\r\n\r\nI know that you put documentation into the parameters; that's something that shouldn't change a type.\r\n\r\nBroadening the definition of type-equality in this way and doing nothing else would give the output of an operation like this the parameters of the _first_ object. Maybe some merging might make more sense, but I don't want to overly complicate the rule.",
  "created_at":"2020-09-18T15:20:42Z",
  "id":694930337,
  "issue":459,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDkzMDMzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T15:23:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Good point about `__array__` or `__record__`. Since those are the two (well, also `__doc__` but we know that is ignorable) parameters that awkward1 itself readds to decide how to treat the array, they should not disappear.\r\nI think probably the most sensible output in the case where the union doesn't have any of the above is to have no parameters. Users can always add their favorite parameters back.",
  "created_at":"2020-09-18T15:27:38Z",
  "id":694934127,
  "issue":459,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDkzNDEyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T15:27:38Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"So the rules would be,\r\n\r\n   * if arrays have different `__array__` or `__record__` parameters, they are not equal;\r\n   * if they otherwise have different parameters, the types can be equal, but merging (concatenation, option-simplify, or union-simplify) removes parameters other than `__array__` and `__record__`.\r\n",
  "created_at":"2020-09-18T15:41:23Z",
  "id":694941328,
  "issue":459,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDk0MTMyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T15:41:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think that makes sense, would welcome other opinions though",
  "created_at":"2020-09-18T15:59:38Z",
  "id":694950294,
  "issue":459,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDk1MDI5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T15:59:38Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll do this one. I'm scheduling it for November, but it might slip.",
  "created_at":"2020-10-30T22:44:53Z",
  "id":719832294,
  "issue":459,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgzMjI5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:44:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This PR removed `Array.columns`, is this intended? How do we enumerate the items of a highlevel record array in its absence?",
  "created_at":"2020-09-18T21:19:21Z",
  "id":695091347,
  "issue":460,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTA5MTM0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T21:19:21Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"That was intended. Wasn't there a deprecation message? The proper way is\r\n\r\n```python\r\nak.keys(array)\r\n```",
  "created_at":"2020-09-18T21:25:45Z",
  "id":695093737,
  "issue":460,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTA5MzczNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T21:25:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I also forgot that `array.tojson` should be dropped in favor of `ak.to_json(array)`. The only method that stays is `array.tolist` (no space) because it's so useful, NumPy has it, and it's not going to conflict with a domain-specific method.\r\n\r\nIs a non-method for `keys` too cumbersome? Would `array.keys()` conflict with any physics cases? `ak.Array` isn't going to be a `Mapping`, and I don't want to give that impression. `array.columns` was one of the unintended side-effects of supporting Pandas, which confused at least one person in a tutorial.",
  "created_at":"2020-09-18T21:31:47Z",
  "id":695095968,
  "issue":460,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTA5NTk2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T21:31:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"On master, \r\n```python\r\n>>> import awkward1\r\n>>> a = awkward1.Array([{\"x\": 1, \"y\": 2}])\r\n>>> a.columns\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward1/highlevel.py\", line 1058, in __getattr__\r\n    raise AttributeError(\r\nAttributeError: no field named 'columns'\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/0.3.0/src/awkward1/highlevel.py#L1060)\r\n```\r\nWe've been using `a.columns` in coffea documentation for a while, and it was available in ak0..\r\nWhy drop this and not `['Mask', 'behavior', 'cache', 'layout', 'mask', 'nbytes', 'numba_type', 'slot0', 'slot1', 'slot2', 'slot3', 'slot4', 'slot5', 'slot6', 'slot7', 'slot8', 'slot9', 'tojson', 'tolist']` ? It seems to me the well-intentioned separation between awkward operations and behaviors is becoming overzealous.\r\n",
  "created_at":"2020-09-18T21:35:20Z",
  "id":695097205,
  "issue":460,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTA5NzIwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T21:35:20Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"`columns` was never intended or desired. The high-level name for them everywhere is `keys`. (Although internally, I do slip into `fields`.)\r\n\r\n`tojson` was a generalization of `tolist`, but having `tolist` as a method is far more useful.\r\n\r\n`nbytes` is because caches use this to determine how much of the cache an object is using (not just Uproot's).\r\n\r\nThe `slot*` form a pattern. It's not really taking up 10 names from the namespace, but 1-ish.\r\n\r\n`layout` and `behavior` were intended from the beginning, as intentional gaps in the namespace.\r\n\r\n`numba_type` was a later addition, but unlikely to hit anyone's domain-specific data because it has a product name in it.\r\n\r\n`mask` is specifically for the `array[cut]` \u2192 `array.mask[cut]` generalization, so it has to be a property. `Mask` is the same thing, capitalized, so the two of these are 1-ish.\r\n\r\n`cache` was originally `metadata` so that any more such things would be hidden inside that one spot in the namespace. But you know how that turned out.\r\n\r\nThat's all of them. They all had reasons. If it's convenient to want to add `.keys()` or `.fields` as an afterthought, after typing the `array`, then I'm willing to add it in the same spirit as `array.tolist()`. You've pointed out how I'm not 100% purist about using this namespace; I'm willing to make this a method/property, but not named \"columns.\" We don't use the word \"columns\" anywhere else to refer to this concept, and it is good to reign in the number of different names for the same thing.",
  "created_at":"2020-09-18T21:45:12Z",
  "id":695100239,
  "issue":460,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTEwMDIzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T21:45:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I guess now or never to change. I scanned code I have and it looks like switching `x.columns` to `ak.keys(x)` is palatable. The fact that awkward0 arrays were near enough to pandas (or Spark) dataframes with respect to this property was, I guess, an unintended feature that we've gotten used to.",
  "created_at":"2020-09-18T22:09:24Z",
  "id":695108253,
  "issue":460,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTEwODI1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T22:09:24Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- I'm going to keep this open until you have a chance to test it (as discussed on Slack).\r\n\r\nWe want to make this generalization, but I want you to test it on your case to be sure we're not generalizing wrong.",
  "created_at":"2020-09-18T23:30:47Z",
  "id":695127528,
  "issue":463,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTEyNzUyOA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-18T23:30:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I can confirm this works for my use case.",
  "created_at":"2020-09-19T00:59:20Z",
  "id":695141682,
  "issue":463,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTE0MTY4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T00:59:20Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"This certainly works in the three cases we know about: our CI and the two cases encountered in the conda-forge deployment. Since they of the strings differed in only having \"Win64\" at the end, perhaps this is a suffix. But \"NMake Makefiles\" is completely different, and might be hiding whether the bit size is fixed or not.",
  "created_at":"2020-09-19T11:17:10Z",
  "id":695200675,
  "issue":465,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTIwMDY3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T11:17:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is the case in the latest release (though I think the source tarball hasn't been deployed yet).",
  "created_at":"2020-09-21T21:06:25Z",
  "id":696376856,
  "issue":466,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjM3Njg1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T21:06:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I mentioned this on slack because I have not had full internet until today. If a user can't get the Windows wheel, they can manually set up a build environment (including putting CMake in the path), then they can build with `--no-build-isolation`. Or they can manually install Scikit-Build, which will enable them to install CMake from the CMake SDist, as long as they have the requirements to build CMake.",
  "created_at":"2020-09-21T21:14:58Z",
  "id":696381192,
  "issue":466,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjM4MTE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T21:14:58Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm putting that back in #470. I remembered at the last minute that this was an issue, but I didn't remember how it wasn't resolved. There are too many things in-flight, I think I opened 6 one-line PRs yesterday.",
  "created_at":"2020-09-21T21:41:00Z",
  "id":696393234,
  "issue":466,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NjM5MzIzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-21T21:41:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> But maybe we should go further and adopt cibuildwheel (not sure what that involves).\r\n\r\n(I can help with that, probably next week)",
  "created_at":"2020-09-23T21:11:28Z",
  "id":697974961,
  "issue":472,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5Nzk3NDk2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-23T21:11:28Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"It looks like for ListOffsetArray, the canonical way to describe an empty array is `offsets = [0]` ? In which case we don't need to change anything for that.",
  "created_at":"2020-09-25T14:57:41Z",
  "id":698978373,
  "issue":473,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5ODk3ODM3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-25T14:57:41Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> It looks like for ListOffsetArray, the canonical way to describe an empty array is `offsets = [0]` ? In which case we don't need to change anything for that.\r\n\r\nFor ListOffsetArray, `[n]` for any number `n` represents an empty array. [Documentation](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html):\r\n\r\n> The offsets must have at least length 1 (corresponding to an empty array), **but it need not start with 0** or include all of the content. Just as ak.layout.RegularArray can have unreachable content if it is not an integer multiple of size, a ListOffsetArray can have unreachable content before the first list and after the last list.\r\n\r\nAn `offsets` of, say, `[33]`, would be turned into a `starts` of `[]` and a `stops` of `[]` because they drop the last and first elements, respectively.",
  "created_at":"2020-09-25T15:26:24Z",
  "id":698993910,
  "issue":473,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5ODk5MzkxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-25T15:26:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Is this good/finished? I wouldn't want this to sit unmerged because I forgot about it.\r\n\r\nI see comments from me (answering a question), but not requests for changes.",
  "created_at":"2020-09-30T17:00:39Z",
  "id":701518579,
  "issue":473,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTUxODU3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T17:00:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think its done yes",
  "created_at":"2020-09-30T18:15:20Z",
  "id":701557998,
  "issue":473,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTU1Nzk5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T18:15:20Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Sure, I'm interested, though I don't understand how that would be a good thing. What's the use-case?\r\n\r\nThe reason I'm asking so bluntly is because I've found that if two Python extension modules are compiled with very different versions of pybind11, they can't share objects in each others' function arguments. I can see this becoming a problem for C++ projects that produce or consume Awkward Arrays through a Python interface (like [dependent-project](https://github.com/scikit-hep/awkward-1.0/tree/master/dependent-project)). I think then we'd have to be very clear about which version of pybind11 a given version of Awkward Array is compiled with so that dependent projects can use this interface. (Like, a table mapping major/minor Awkward version numbers, X.Y for all .z, to specific pybind11 version numbers.)\r\n\r\nIf it's easy to change the pybind11 version, won't there be a \"version hell\" problem trying to get the right one? Wouldn't making the compilation more flexible make interoperability more difficult?\r\n\r\nAlso, I'm trying to make sure that the vast majority of users can get a wheel for their system. What's the specific reason you're compiling?",
  "created_at":"2020-09-28T22:19:39Z",
  "id":700314069,
  "issue":476,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMDMxNDA2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-28T22:19:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Of course, the use-case is to include awkward in the software stack of an experiment design study. Not as big as the LHC experiments, but still, it's not desirable to have three separate packages ship their own pybind11. At this scale, \"version hell\" is a known problem anyway, and software librarians have tooling to get consistent installs. But packages that treat dependencies in a clear and uniform way do help a lot here!\r\n\r\nIn particular, I'm evaluating the spack build manager, which is very effective at dealing with problem like the one you describe. For now I'm fine with installing a patched version of uproot4\r\nhttps://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/py-awkward1/pybind11.patch\r\nBut I do think that treating dependencies as such instead of hiding them to reduce the complexity for users is a good idea (especially since this can be optional) -- just mention the required version(s) of pybind11 in the CMakeList.\r\n\r\nAlso, it seems to me that for the problem you describe, it would be worse anyway to have different packages set up their own pybind11 instead of using one external one?",
  "created_at":"2020-09-29T08:41:59Z",
  "id":700544894,
  "issue":476,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMDU0NDg5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-29T08:43:22Z",
  "user":"MDQ6VXNlcjUwNTc4ODQ="
 },
 {
  "author_association":"MEMBER",
  "body":"If it were a shared library with externally visible symbols, then definitely it would be worse for packages to bring their own. In fact, I've seen this become an issue between ROOT and Julia: the LLVM symbols clashed.\r\n\r\nBut I think a header-only library is different. At least, as an extreme, the embedding of RapidJSON is 100% safe because this dependency isn't even visible\u2014it's only used in Awkward implementations, never any header files, and only objects defined in Awkward's header files are exposed (visibility=false with explicit exceptions). If two libraries had different versions of RapidJSON, they wouldn't even find out. You might say that one is a waste of disk space, but that's not what we usually care about.\r\n\r\nBut pybind11 is a different case. It's visible\u2014it literally is the interface\u2014but it's not presenting a suite of pybind11 symbols to the dynamic loader, it's making PyCapsules for the Python C API out of Awkward objects. Mixing pybind11 versions would prevent packages that depend on Awkward for being able to pass their data into each other's functions in Python, but it wouldn't affect unrelated libraries using pybind11 for types that are incompatible anyway. In fact, it was to prevent accidental collisions of intermediate symbols that pybind11 requires projects to be compiled with visibility=false.\r\n\r\nIf your software stack has Awkward compiled with a different version of pybind11 than a matrix claims, then dependent packages that are supposed to work with that version won't work with that version.\r\n\r\nI understand the point that sharing libraries usually makes a collection of packages more maintainable because you can avoid symbol clashes, but here there are no symbols to clash and introducing uncertainty about which version of Awkward uses which version of pybind11 will make it impossible to know if a dependent library is going to work. (Unless, of course, the dependent library is compiled in the same package library you're developing\u2014but do you want to say that users can't bring in their own packages from elsewhere? That's a valid option\u2014what conda does\u2014but then you have to be prepared to include everything every user wants in the distribution. Maybe this isn't for users? Is it a reconstruction workflow?)",
  "created_at":"2020-09-29T12:46:15Z",
  "id":700678590,
  "issue":476,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMDY3ODU5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-29T12:46:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, what happens when you try compiling the normal way? I assume that everything builds as expected, the tests run, and if there are any issues, they're possible/future issues.\r\n\r\nIt also wouldn't be a difficult thing, technically, to add a CMake build variable to opt into a different pybind11 directory location, which changes this line:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/820a2b0ba655fa17e19042f5b32b6c5996347d3c/CMakeLists.txt#L110\r\n\r\nThe \"version hell\" I was talking about was one that would fall on the shoulders of scientists attempting to use the library, which is a much bigger issue than a software librarian dealing with them. (\"At this scale, \"version hell\" is a known problem anyway, and software librarians have tooling to get consistent installs.\") But, as is evident in the parenthetical sentence at the end of my last message, I'm just now realizing that the software stack you're building might not be intended for individual scientists to use. It might be supporting a reconstruction pipeline whose application is known ahead of time. The example I'm familiar with, CMSSW, is used both ways: to support the large-scale reconstruction and to set up an environment in which individual scientists work on projects whose dependencies aren't known in advance.\r\n\r\nSo if you're building a pipeline that will run one application and 100% of the dependencies will always be supplied through the software stack, then let's add that line to the Awkward CMakeLists so that you can use whichever pybind11 you prefer. (It's still not clear to me that this is necessary, but in this case, it doesn't hurt.)\r\n\r\nBut if you're building an environment for individual scientists to use ad-hoc, individuals who might bring in Python packages from other sources like pip, conda, homebrew, ... etc., then I'm going to want to force particular versions of Awkward to use particular versions of pybind11 so that this will be deterministic. Otherwise, _they'll_ be the ones trapped in version hell, and they won't have the expertise or the tooling to fix it.",
  "created_at":"2020-09-29T13:57:08Z",
  "id":700722315,
  "issue":476,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMDcyMjMxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-29T13:57:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Actually, what happens when you try compiling the normal way? I assume that everything builds as expected, the tests run, and if  there are any issues, they're possible/future issues.\r\n\r\nAh, right my bad, I was compiling from the github release tarball, not the pypi downloads. This doesn't include the submodules, and the error prompted me to open the issue. \r\n\r\nI'm not that familiar with pybind11, and from your comment it seems indeed to be less clear cut. For individual ad-hoc use, I fully subscribe to bundlining it. Our usecase is similar to CMSSW - awkward is not directly used in reconstruction, but we want to be able to run also common analysis script with the central software installation (for reproducibility etc.). In this case there is much less potentials for errors due to version mismatches. And while it may not \"cost\" much to bundle pybind11, it does remind me of this comment I read somewhere:\r\n\r\n>By my count, the unreal engine source tree only has 3 full copies of boost inside it. In my opinion, any real engine should easily have 4 or 5 copies of boost to really deliver on quality.\r\n\r\nBut this verges on the philosophical, so I'm fine with closing this issue.\r\n",
  "created_at":"2020-09-29T18:00:07Z",
  "id":700883274,
  "issue":476,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMDg4MzI3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-29T18:00:07Z",
  "user":"MDQ6VXNlcjUwNTc4ODQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, thanks for clarifying! Since this is a case in which a downstream library depending on Awkward might come from a different compilation, let's stick with the embedded pybind11.\r\n\r\nSince the GitHub release tarball doesn't include submodules, perhaps I should look into a way of removing that link. When cloning the repository, there's the `--recursive` flag, but that doesn't apply to the tarball.\r\n\r\nPyPI is a better source anyway; it's the distribution you'd get with\r\n\r\n```bash\r\npip install awkward1 --no-binary\r\n```\r\n\r\nwhich ought to compile Awkward using only the files shipped as part of that version. (I hope I didn't get the `--no-binary` flag wrong; you'd want it to take the NumPy wheel but compile Awkward. NumPy is Awkward's only pip dependency.)",
  "created_at":"2020-09-29T18:43:19Z",
  "id":700907872,
  "issue":476,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMDkwNzg3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-29T18:43:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> How does it feel to change 10,000 lines in one commit?\r\n\r\nHaha, I actually had several arbitrary commits before squashing them into a single one. ",
  "created_at":"2020-09-30T15:07:28Z",
  "id":701451888,
  "issue":477,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQ1MTg4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T15:07:28Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"While we're moving everything around, the allocators.h should probably go into kernel-utils.h, too, because these are two files of \"not regular kernels\" that can get lost in the hundreds-long list. Putting all the weirdos in one place makes them easier to find.",
  "created_at":"2020-09-30T15:09:44Z",
  "id":701453327,
  "issue":477,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQ1MzMyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T15:09:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski The changes you suggested should be fixed :D ",
  "created_at":"2020-09-30T16:04:04Z",
  "id":701487164,
  "issue":477,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQ4NzE2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T16:04:04Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I am pushing another commit.. I think I didn't move the allocators into kernel-utils in the proper way. Sorry!",
  "created_at":"2020-09-30T16:14:54Z",
  "id":701493540,
  "issue":477,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQ5MzU0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T16:15:04Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"This looks good! Let me know when you're ready to merge.\r\n\r\nI noticed that\r\n\r\n```bash\r\npython -m pytest -vv -rs tests-cuda\r\n```\r\n\r\nis now broken, but I think that can/should be a separate PR. (It's not part of the standard CI tests and nobody is using it yet. However, let's get it fixed soon!)",
  "created_at":"2020-09-30T16:30:21Z",
  "id":701502220,
  "issue":477,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTUwMjIyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T16:30:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This looks good! Let me know when you're ready to merge.\r\n\r\nI think it can be merged now.\r\n\r\n> I noticed that\r\n> \r\n> ```shell\r\n> python -m pytest -vv -rs tests-cuda\r\n> ```\r\n> \r\n> is now broken, but I think that can/should be a separate PR. (It's not part of the standard CI tests and nobody is using it yet. However, let's get it fixed soon!)\r\n\r\nYikes, I missed that. Thanks for pointing it out. I'll create a new PR to fix it.\r\n",
  "created_at":"2020-09-30T16:36:07Z",
  "id":701505306,
  "issue":477,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTUwNTMwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T16:36:07Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Just checked - works on the AWS instance.",
  "created_at":"2020-09-30T19:28:19Z",
  "id":701596645,
  "issue":478,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTU5NjY0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T19:28:19Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- Just thinking... below are notes.\r\n\r\nSomething to think about: the cache was attached to the `ak.Array` with the understanding that any transformations of that array would lose the original `ak.Array` and therefore also lose the cache, but since the array has been transformed, the original isn't needed, anyway.\r\n\r\nBut here's a case where the cache should be retained but the `ak.Array` goes away. `ak.zip` combines many arrays, so the output `ak.Array` has to have a cache that holds all of the old ones, but doesn't necessarily add to them, like this:\r\n\r\n```python\r\nclass MergedCaches(MutableMapping):\r\n    def __init__(self, *arrays):\r\n        self._old_caches = [x.cache for x in arrays\r\n                            if isinstance(x, awkward1.highlevel.Array) and x.cache is not None]\r\n    def __getitem__(self, where):\r\n        for cache in self._old_caches:\r\n            try:\r\n                return cache[where]\r\n            except KeyError:\r\n                pass\r\n        else:\r\n            raise KeyError(where)\r\n    def __setitem__(self, where, what):\r\n        pass\r\n```\r\n\r\nAnd just as every operation calls `awkward1._util.behavior_of(*input_arrays)` to get a merged behavior for the output, they'd have to create a `MergedCaches(*input_arrays)` to pass to the `cache` argument of\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/c314dfe6c907609114a6d53a7ccee2fbaf10174f/src/awkward1/_util.py#L375-L391\r\n\r\nso that old caches are not forgotten, but the new cache object is not usable as a place to put things. (Perhaps the caches attached to `ak.Array` need to be Mappings, not MutableMappings. The cache passed to `ArrayCache` must be a MutableMapping, but for the sake of attaching it to an `ak.Array`, it doesn't need to be mutable.\r\n\r\nAlso, the `ak.Array` cache would have to be updated to take on any new caches attached through `ak.Array.__setitem__`.",
  "created_at":"2020-09-30T19:42:25Z",
  "id":701603905,
  "issue":479,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTYwMzkwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T19:42:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Shoot. My first reaction was that I thought `ak.zip` would materialize the arrays. I think merging caches is reasonable, and indeed ~all of these are `Mapping` instances at this interface~ literally the only purpose of tracking `Array.cache` is to hold a reference, we don't need any of the functionality in theory. One tweak I would suggest is to make a set of the array caches, since often (and indeed in this example) the actual cache object will be the same for both high-level arrays being passed to `ak.zip`.",
  "created_at":"2020-09-30T19:52:12Z",
  "id":701609107,
  "issue":479,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTYwOTEwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T19:54:06Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I was thinking that, too, while writing the code. Maybe the list comprehension in the `MergedCaches.__init__` should be\r\n\r\n```python\r\nlist({\r\n    id(x.cache): x.cache\r\n    for x in arrays\r\n    if isinstance(x, awkward1.highlevel.Array) and x.cache is not None\r\n}.values())\r\n```\r\n\r\n(Caches won't, in general, be hashable.)",
  "created_at":"2020-09-30T20:04:03Z",
  "id":701615034,
  "issue":479,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTYxNTAzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T20:04:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think, similar to behavior, there should be a function that returns the original object if it is useable, and an instance of this merged cache object if otherwise",
  "created_at":"2020-09-30T20:17:38Z",
  "id":701621576,
  "issue":479,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTYyMTU3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T20:17:38Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"And also keeps it flat, rather than making MergedCaches of MergedCaches (which could be viewed as part of what you described above: 0 caches \u2192 None, 1 cache \u2192 that cache, 2 or more \u2192 a MergedCache of the caches, but no nesting).",
  "created_at":"2020-09-30T20:22:51Z",
  "id":701624089,
  "issue":479,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTYyNDA4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T20:22:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"What I did in #512 is a more complete solution than what is described above. Instead of `ak.Array.cache` being a user-settable property of an `ak.Array`, a read-only property `ak.Array.caches` is a tuple of all distinct caches used in the layout. When an `ak.Array` is first built, it collects references to the caches the layout contains and puts them in this tuple to maintain their references.\r\n\r\nThere's still a potential for gaps, in which old references might be lost before the new reference is made, but these necessarily happen in low-level functions; it's never a high-level user's responsibility to keep track of them. For instance, the following implementation of `do_something` can lose cache references, leading to the RuntimeError:\r\n\r\n```python\r\ndef do_something(array):\r\n    layout = array.layout\r\n    del array\r\n    return ak.Array(layout)\r\n```\r\n\r\nbut the following will not lose cache references/no RuntimeError:\r\n\r\n```python\r\ndef do_something(array):\r\n    layout = array.layout\r\n    return ak.Array(layout)\r\n```\r\n\r\nReal bugs are unlikely to be that simple, but this is the essence: a low-level function that extracts a layout from an `ak.Array` and does something with it must keep the original `ak.Array` around long enough for the new, output `ak.Array` to be returned.\r\n\r\nAnother way it could fail is if a new cache is not held before making the output:\r\n\r\n```python\r\ndef do_something(func):\r\n    layout = ak.layout.VirtualArray(func, ak.layout.ArrayCache(ak._util.MappingProxy({})))\r\n    return ak.Array(layout)\r\n```\r\n\r\nwould fail because the reference to `ak._util.MappingProxy({})` is lost but\r\n\r\n```python\r\ndef do_something(func):\r\n    hold_cache = ak._util.MappingProxy({})\r\n    layout = ak.layout.VirtualArray(func, ak.layout.ArrayCache(hold_cache))\r\n    return ak.Array(layout)\r\n```\r\n\r\nwould succeed because `hold_cache` stays in scope until the `ak.Array` can be made.\r\n\r\nSo it's not foolproof, but it is out of most users' way\u2014something only framework developers need to think about\u2014and this solution doesn't depend on every operation passing cache references on to the next operation: all caches are collected from the entire layout whenever an `ak.Array` is made; none would be missed.\r\n\r\nThe changes in interface are now marked with DeprecationWarnings. @nsmith-, you should test this with NanoEvents. (I'll be pushing a new release because the interface changes also need to be coordinated with Uproot.)",
  "created_at":"2020-11-04T18:05:29Z",
  "id":721888045,
  "issue":479,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTg4ODA0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-04T18:05:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Noted: this will be the next section I write.\r\n\r\nIn particular, I'll update #409 (where I was _starting_ to write docs again, but it fell by the wayside because of other work), write the page you requested, and then finish that PR so that it updates on the web.\r\n\r\nHowever, I think I can give you some quick pointers to help you now.\r\n\r\n   * If your arrays all have fixed-size dimensions (as in your example), you can use NumPy directly. Awkward Array's syntax is the same as NumPy's, and Awkward Array can be as fast as but no faster than NumPy, so the reason you'd use Awkward is to get generality. If your problem is not more general than NumPy, I don't see a reason to use it.\r\n   * I'm not sure what you're intending to do with `sub = arr[:, :ind]` because `ind` is a list of numbers. Python only allows scalar numbers as the start, stop, or step of a slice (the `:` syntax inside square brackets).\r\n\r\nFor example, if you have\r\n\r\n```python\r\n>>> arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\r\n>>> arr\r\narray([[ 1,  2,  3],\r\n       [ 4,  5,  6],\r\n       [ 7,  8,  9],\r\n       [10, 11, 12]])\r\n```\r\n\r\n(It's always good to use small, non-random examples, so that you can see what's going on.)\r\n\r\nGetting the first `k` (e.g. `2`) of each row is\r\n\r\n```python\r\n>>> arr[:, :2]\r\narray([[ 1,  2],\r\n       [ 4,  5],\r\n       [ 7,  8],\r\n       [10, 11]])\r\n```\r\n\r\n[NumPy's advanced indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html) lets you rearrange, duplicate, and/or drop elements from each row:\r\n\r\n```python\r\n>>> arr[:, [2, 0, 0, 1]]\r\narray([[ 3,  1,  1,  2],\r\n       [ 6,  4,  4,  5],\r\n       [ 9,  7,  7,  8],\r\n       [12, 10, 10, 11]])\r\n```\r\n\r\nAs a special case, boolean indexing lets you drop elements only:\r\n\r\n```python\r\n>>> arr[:, [True, False, True]]\r\narray([[ 1,  3],\r\n       [ 4,  6],\r\n       [ 7,  9],\r\n       [10, 12]])\r\n```\r\n\r\nAll of the above is equally true of an Awkward Array:\r\n\r\n```python\r\n>>> arr = ak.from_numpy(arr)\r\n>>> arr\r\n<Array [[1, 2, 3], [4, 5, ... 9], [10, 11, 12]] type='4 * 3 * int64'>\r\n>>> arr[:, :2]\r\n<Array [[1, 2], [4, 5], [7, 8], [10, 11]] type='4 * 2 * int64'>\r\n>>> arr[:, [2, 0, 0, 1]]\r\n<Array [[3, 1, 1, 2], ... [12, 10, 10, 11]] type='4 * 4 * int64'>\r\n>>> arr[:, [True, False, True]]\r\n<Array [[1, 3], [4, 6], [7, 9], [10, 12]] type='4 * 2 * int64'>\r\n```\r\n\r\nBut Awkward Arrays also work on irregular shapes:\r\n\r\n```python\r\n>>> arr = ak.Array([[1, 2, 3], [4], [5, 6]])   # these lists have different lengths; not allowed in NumPy\r\n>>> arr\r\n<Array [[1, 2, 3], [4], [5, 6]] type='3 * var * int64'>\r\n>>> arr[:, :2]\r\n<Array [[1, 2], [4], [5, 6]] type='3 * var * int64'>\r\n>>> arr[:, :1]\r\n<Array [[1], [4], [5]] type='3 * var * int64'>\r\n>>> arr[:, [0, -1]]\r\n<Array [[1, 3], [4, 4], [5, 6]] type='3 * 2 * int64'>\r\n```\r\n\r\nand using an Awkward Array _as a slice_ has no analogue in NumPy, but it lets you pick out different numbers of elements from each row.\r\n\r\n```python\r\n>>> arr[ak.Array([[1, 1, 0], [0, -1], []])]\r\n<Array [[2, 2, 1], [4, 4], []] type='3 * var * int64'>\r\n>>> arr[ak.Array([[True, False, True], [False], [True, True]])]\r\n<Array [[1, 3], [], [5, 6]] type='3 * var * int64'>\r\n```\r\n\r\nI hope this helps you get started!\r\n\r\n",
  "created_at":"2020-10-05T14:18:33Z",
  "id":703662963,
  "issue":480,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMzY2Mjk2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-05T14:18:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hello Jim, thank you for your reply. It looks like the last two examples are related to the scenario I'm dealing with. What I am looking for is that given the original array has regular shape, and I have a list which contains varying number denoted as ls[i] = k, denoted as take the first k element in row i of the regular array, i.e.\r\n```\r\n>>> arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\r\n>>> arr\r\narray([[ 1,  2,  3],\r\n       [ 4,  5,  6],\r\n       [ 7,  8,  9],\r\n       [10, 11, 12]])\r\n>>> ls = [0, 3, 1 ,2]\r\n>>> arr[:, :ls]\r\narray([[],\r\n       [4, 5, 6],\r\n       [7],\r\n       [10, 11]])\r\n```\r\nNumpy supports it well if you slice each row with the same number of elements but not if you want to slice each row with different number of elements. I see awkward array can do something similar to what I described above shown in the last example, but more a np.take style rather than slicing. \r\n\r\nMaybe I'm asking too much for this and need to rethink the strategy of my code. All I need to do is taking the argmin or min of the first k element in each row of a np.ndarray, with k varying. I have fair experience vectorizing my code but in this work I don't see any strategies to optimize it other than parallelizing the for-loop or use numba.jit to automate the optimization so far.\r\n",
  "created_at":"2020-10-05T14:49:01Z",
  "id":703681981,
  "issue":480,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMzY4MTk4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-05T14:49:01Z",
  "user":"MDQ6VXNlcjIzNDg3MjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, so it is an Awkward problem, after all.\r\n\r\nSince you want to select everything up to N, you need to make a list of integers from 0 to N, like what `range` does. The slice syntax (`:` character) is not involved.\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import awkward1 as ak\r\n>>> arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\r\n>>> lengths = [0, 3, 1, 2]\r\n>>> ak.from_numpy(arr)[[range(length) for length in lengths]]\r\n<Array [[], [4, 5, 6], [7], [10, 11]] type='4 * var * int64'>\r\n```\r\n\r\nThe slowest part of that is creating the slice: `[range(length) for length in lengths]` is a collection of Python objects that have to be implicitly converted into an Awkward array to use it as a slice. The structure has to be like the following, and I don't know of any functions that will make it for you:\r\n\r\n```python\r\n>>> ak.Array([[], [0, 1, 2], [0], [0, 1]])\r\n```\r\n\r\nso it has to be created by hand. Creating it by hand in Numba is faster; you could use\r\n\r\n```python\r\n>>> import numba as nb\r\n>>> @nb.jit\r\n... def make_index(lengths, builder):\r\n...     for length in lengths:\r\n...         builder.begin_list()\r\n...         for x in range(length):\r\n...             builder.append(x)\r\n...         builder.end_list()\r\n...     return builder\r\n... \r\n>>> lengths = np.array([0, 3, 1, 2])\r\n>>> make_index(lengths, ak.ArrayBuilder()).snapshot()\r\n<Array [[], [0, 1, 2], [0], [0, 1]] type='4 * var * int64'>\r\n```\r\n\r\nKnowing that this is an interesting case, I can include it in the documentation on slicing. Mixing Numba's ability to speed up for-loops with Awkward's for-loopless interface can be powerful.\r\n\r\n-------------------------\r\n\r\nAlthough... I guess you could also take a more direct approach using Numba only:\r\n\r\n```python\r\n>>> arr = ak.from_numpy(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]))\r\n>>> @nb.jit\r\n... def make_direct(input, lengths, builder):\r\n...     for row, length in zip(input, lengths):\r\n...         builder.append(row[:length])\r\n...     return builder\r\n... \r\n>>> make_direct(arr, lengths, ak.ArrayBuilder()).snapshot()\r\n<Array [[], [4, 5, 6], [7], [10, 11]] type='4 * var * int64'>\r\n```\r\n\r\nWhatever seems more natural to you. (There might also be a performance difference for large arrays, since the two techniques are doing rather different things with the data you have in memory. The resulting arrays would also have different performance characteristics, but which is better depends on how it's used _after_ creation. But for performance questions, try one and if it's fine for your needs, no reason to change it!)",
  "created_at":"2020-10-05T16:25:05Z",
  "id":703740986,
  "issue":480,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMzc0MDk4Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2020-10-05T16:25:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, Azure's not ready yet. I'll try again later.",
  "created_at":"2020-10-06T14:36:17Z",
  "id":704313043,
  "issue":482,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDMxMzA0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-06T14:36:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"These failures are due to Azure, and it's veeery slow to restart. Everything works, so I'm going to merge it.",
  "created_at":"2020-10-29T19:37:19Z",
  "id":718976353,
  "issue":482,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODk3NjM1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T19:37:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@veprbl Is this similar or different from what you were working on? (Just asking, because I remember you were linking into Awkward with MacOS.)",
  "created_at":"2020-10-06T23:08:45Z",
  "id":704599740,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDU5OTc0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-06T23:08:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@raymondEhlers Does it work if your simple function accepts an `ak::NumpyArray`?",
  "created_at":"2020-10-07T04:06:36Z",
  "id":704678436,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDY3ODQzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T04:06:36Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi @veprbl , I tried with (I dropped the original awkwardTest function here for clarity):\r\n\r\n```c++\r\nstd::shared_ptr<ak::NumpyArray> awkwardTestNumpyArray(const std::shared_ptr<ak::NumpyArray> & arr)\r\n{\r\n    std::cout << \"In function for NumpyArray\\n\";\r\n    return arr;\r\n}\r\n\r\n// Bindings\r\nPYBIND11_MODULE(_src, m) {\r\n  // Awkward array\r\n  // Ensure dependencies are loaded.\r\n  py::module::import(\"awkward1\");\r\n\r\n  m.def(\"awkward_test_numpy_array\", &awkwardTestNumpyArray, \"arr\"_a, \"...\");\r\n}\r\n```\r\n\r\nand called it with:\r\n\r\n```\r\npba.awkward_test_numpy_array(ak.Array([1,2,3]).layout)\r\n```\r\n\r\n it works on macOS, but unfortunately it still fails on linux",
  "created_at":"2020-10-07T07:58:16Z",
  "id":704764828,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDc2NDgyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T07:58:16Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Seems like this could be a variant https://github.com/pybind/pybind11/issues/912, which should not be an issue on gcc/stdlibc++. Do you have an ability to recompile with a custom pybind? I would try removing this clause https://github.com/pybind/pybind11/blob/00edc3001bb10d6e6d4669e385ee43996e674b08/include/pybind11/detail/internals.h#L56-L60 and keeping the `#else` one instead.",
  "created_at":"2020-10-07T08:21:15Z",
  "id":704776176,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDc3NjE3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T08:21:15Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"One more follow up: I was concerned that my reproducer repository was somehow the source of the issue, so I tried running the [dependent-project](https://github.com/scikit-hep/awkward-1.0/tree/master/dependent-project) from this repository. (I built my package based on it as an example, so they're not entirely independent). It took a little hacking to get running (kernels moved, some json issues), but in any case, I get the same issue",
  "created_at":"2020-10-07T08:23:22Z",
  "id":704777264,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDc3NzI2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T08:23:22Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the suggestion! Yeah, I can recompile with a custom pybind11. To be clear, I need the same version of pybind11 for awkward1 and my package, right? So I need to modify it for both and recompile them? Or just for my package?",
  "created_at":"2020-10-07T08:24:56Z",
  "id":704778101,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDc3ODEwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T08:24:56Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"After modifying pybind11 for both awkward1 and my package, and then recompiling everything, it works! (I first tried the lazy approach of just modifying pybind11 for my package, but it didn't work - in retrospect, not super surprising). Thanks for your help! How do we proceed from here?",
  "created_at":"2020-10-07T08:51:11Z",
  "id":704791958,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDc5MTk1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T08:51:11Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@raymondEhlers I would report to the pybind11 upstream. I suppose this involves preparing an MRE that doesn't involve awkward1.",
  "created_at":"2020-10-07T09:35:43Z",
  "id":704817025,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDgxNzAyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T09:35:43Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Okay, I see. I'm not sure I quite understand it enough to reproduce without awkward yet, but I'll read the code more closely and try the linked pybind11 issue and see what I can come up with. Thanks!",
  "created_at":"2020-10-07T09:57:44Z",
  "id":704828766,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDgyODc2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T09:57:44Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I looked into this a bit, but following the example in that pybind11 issue, I wasn't able to reproduce it without awkward1. Getting into controlling the exporting of symbols, etc, is a bit out of my depths, so I'm sure I'm overlooking something. However, as a simpler solution, I tried updating to pybind 2.5.0, and it seems to work correctly.\r\n\r\nI'd like to run a few more tests to be certain that I've 100% cleaned up the old artifacts and that it's truly working, but so far so good. @jpivarski would updating pybind11 to 2.5.0 be possible?",
  "created_at":"2020-10-07T11:29:50Z",
  "id":704873154,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDg3MzE1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T11:29:50Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski would updating pybind11 to 2.5.0 be possible?\r\n\r\nAbsolutely. Is that the latest version? I can update it to whatever is necessary, as long as I don't run into errors in CI (which would have to be addressed anyway\u2014newer versions were supposed to be better, so if Awkward can't be compiled against it, that's something wrong with Awkward...)\r\n\r\nDoes updating pybind11 completely solve the problem? Everything's fine after that?\r\n\r\nThe issue you faced, is that something that could be added to the dependent project test? The dependent project runs in CI, so if this is an issue that wasn't fully treated before, I'd like to add it.\r\n\r\nThanks for looking into this, @veprbl!",
  "created_at":"2020-10-07T11:38:38Z",
  "id":704877080,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDg3NzA4MA==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2020-10-07T11:38:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Excellent! I just cloned everything cleanly, with the only changes being updating to pybind11 2.5.0, and it seem to work with my minimal repo linked in my initial report. So I'm fairly convinced that this resolves the issue. 2.5.0 is the latest pybind11 stable release (there's a 2.6.0 beta 1).\r\n\r\nI'm not sure what to make of the dependent project passing in the CI, but failing for me. With pybind11 2.5, it seems to build for me now. Most of the initial build problems were due to rapidJSON (other than the main issue that I report here), so those were perhaps due to a mistake with cloning rapidJSON. I just built it now in my clean directory and virtualenv with pybind11 2.5, and the only issue I see now is that when using the `CMakeLists.txt`, I needed to add an __init__.py to the build directory to get it to be recognized as a package. So in terms of adding a test to reproduce my issue, unfortunately I'm not sure what to suggest. Passing the ak.Array back and forth as is done in the dependent project was enough to see this issue. I imagine it's some combination of compilers, libraries, and options, but I'm not sure how to isolate them without a ton of work, sorry. I'm of course open to suggestions for how to isolate it though",
  "created_at":"2020-10-07T12:19:23Z",
  "id":704896778,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDg5Njc3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T12:20:53Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"It's a trivial PR, but I opened #485 to update to pybind11 2.5.0",
  "created_at":"2020-10-07T12:42:52Z",
  "id":704909144,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDkwOTE0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T12:42:52Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I'm not sure what to make of the dependent project passing in the CI, but failing for me.\r\n\r\nI don't know if you were doing deeper tests. The dependent project only compiles against Awkward and passes some data to and from its own functions. I might not have passed an interesting enough data type for it to have triggered the bug you saw.\r\n\r\nIf [dependent.cpp](https://github.com/scikit-hep/awkward-1.0/blob/master/dependent-project/dependent.cpp) is doing something that's obviously less than what you had in your project, we could add what you have in your project to make it a better test, for the future.\r\n\r\nThe next release of Awkward will need a new minor version number: 0.3.1 \u2192 0.4.0 because of the dependency version change. I'll need to start keeping an Awkward version \u2192 pybind11 version map somewhere, so that dependent projects can use compatible ones.",
  "created_at":"2020-10-07T14:13:23Z",
  "id":704964829,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDk2NDgyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T14:13:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sorry, I think I wasn't super clear on this point: this issue was triggered just by passing arrays from python -> cpp. It also occurred trying to compile and run `dependent.cpp` on our cluster. So I don't see any obvious additional functionality to add to `dependent.cpp` that will trigger this issue. Which makes this especially puzzling to me. In any case, if I think of a test as I keep developing my code, I'll definitely open a PR.\r\n\r\nSounds good - I look forward to the next release! \r\n\r\nThanks again to you and @veprbl for all of the help!",
  "created_at":"2020-10-07T14:42:20Z",
  "id":704984002,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDk4NDAwMg==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2020-10-07T14:42:20Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, okay\u2014a platform-dependent error (worked in CI, failed on your system). I'm glad the new pybind11 fixes that because it would be tough to debug.\r\n\r\nThe next version will likely come out when Python 3.9 can be tested, which will be as soon as it's added to Azure's pipelines. (c.f. microsoft/azure-pipelines-tool-lib#77).",
  "created_at":"2020-10-07T14:50:49Z",
  "id":704989516,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDk4OTUxNg==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2020-10-07T14:50:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"It's a bit strange that updating to 2.5.0 resolved the issue. At first glance I don't see any relevant change: https://github.com/pybind/pybind11/compare/80d452484c5409444b0ec19383faa84bb7a4d351...3b1dbebabc801c9cf6f0953a4c20b904d444f879",
  "created_at":"2020-10-09T01:16:29Z",
  "id":705909346,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNTkwOTM0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-09T01:16:29Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"It has been a long _time_ since it was updated, I assumed there might have been something. @henryiii is recommending another upgrade to 2.6.0, for better Python 3.9 handling, so a version of Awkward based on 2.5.0 might never get released.\r\n\r\n(If we have to track Awkward \u2190\u2192 pybind11 version members, that would be easier if it's not changed often.",
  "created_at":"2020-10-09T01:53:19Z",
  "id":705920589,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNTkyMDU4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-09T01:53:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That does it! This commit hash corresponds to an official release of pybind11, which is the only constraint I'd apply, other than the tests passing. So I'll take it. `:)`",
  "created_at":"2020-10-07T14:07:30Z",
  "id":704961034,
  "issue":485,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDk2MTAzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T14:07:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"There's a whole discussion of this issue, starting at https://github.com/scikit-hep/awkward-1.0/issues/490#issuecomment-712250246\r\n\r\nWhen I wrote it there, I thought I was narrowly saving it from oblivion because I only remembered hearing it on Slack. I could have just pointed to this issue. Anyway, the cross-reference will be useful in solving it.",
  "created_at":"2020-10-20T03:27:34Z",
  "id":712564861,
  "issue":487,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjU2NDg2MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-10-20T03:27:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Good point, though the command won't work without `-c conda-forge` or\r\n\r\n```bash\r\nconda config --add channels conda-forge\r\n```\r\n\r\nfor users who haven't already set up conda-forge as one of their channels. (It's not one of the default ones.)",
  "created_at":"2020-10-16T17:01:12Z",
  "id":710227174,
  "issue":488,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMDIyNzE3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-16T17:01:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah, duh, let me fix that. :-)",
  "created_at":"2020-10-16T17:02:53Z",
  "id":710230024,
  "issue":488,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMDIzMDAyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-16T17:02:53Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"The reason it's not working is because they're isn't an Awkward function overriding this NumPy function. That would be a good feature to add (hadn't thought of it), and it would be a whole new category of overload (it doesn't quite belong in ak.operations.structure, though it would be implemented in a similar way).\r\n\r\nFor the time being, I guess you'd have to unwrap the `a` object manually with `a.layout.content` until you get the underlying NumpyArray, cast this as a `np.asarray`, compute the random numbers, and then wrap it up as a ListOffsetArray64 using the same `offsets` as the original, and then put that in a new `ak.Array`. That's essentially what a built-in function would do, but for general structures, and it's also why such a function would be a nice addition.",
  "created_at":"2020-10-17T20:56:00Z",
  "id":711079353,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTA3OTM1Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-10-17T20:56:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you! There is something I want to clarify:\r\n\r\n>  and then wrap it up as a ListOffsetArray64 using the same offsets as the original, and then put that in a new ak.Array\r\n\r\nHow exactly can I do this? (Sorry I'm just starting to learn awkward). So suppose I did the following:\r\n\r\n```\r\nb = np.asarray(a.layout.content)\r\nb_random = np.random.normal(b, b*0.15)\r\n```\r\n\r\nWhat should I do to convert `b_random` back to an awkward array with the same offsets as `a.layout.offsets`?\r\n\r\n",
  "created_at":"2020-10-17T21:22:48Z",
  "id":711082109,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTA4MjEwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-17T21:43:11Z",
  "user":"MDQ6VXNlcjM1MTcxNDE3"
 },
 {
  "author_association":"MEMBER",
  "body":"(I'm waiting from a phone, so it's hard to give examples.)\r\n\r\nThe `a.layout.offsets` is one of the two arguments to the `ak.layout.ListOffsetArray64` constructor, which puts the same list structure around the random numbers that you've made as the original list had, and then passing this to `ak.Array` gives it the high level interface of `a` (same not `a.layout`). Try this on a terminal to see what I mean. The `repr` view of these objects should help show what's going on\u2014it's particularly instructive to practice on small objects and then scale up when you understand the structure.",
  "created_at":"2020-10-17T22:12:09Z",
  "id":711086618,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTA4NjYxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-17T22:12:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Well then I guess I would do this to make a new awkward array with the same offset?\r\n\r\n```\r\nak.Array(ak.layout.ListOffsetArray64(a.layout.offsets, ak.Array(b_random).layout))\r\n```",
  "created_at":"2020-10-17T22:44:25Z",
  "id":711089752,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTA4OTc1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-17T22:44:25Z",
  "user":"MDQ6VXNlcjM1MTcxNDE3"
 },
 {
  "author_association":"NONE",
  "body":"This is my whole function, would it be similar to what you have in mind for future implementation?\r\n\r\n\r\n```\r\ndef smear(arr):\r\n    \r\n    from awkward1 import Array\r\n    from awkward1.layout import ListOffsetArray64\r\n    \r\n    #Convert it to a 1D numpy array and perform smearing\r\n    numpy_arr = np.asarray(arr.layout.content)\r\n    smeared_arr = np.random.normal(numpy_arr, numpy_arr*0.15)\r\n    \r\n    #Convert it back to awkward form\r\n    return Array(ListOffsetArray64(arr.layout.offsets, Array(smeared_arr).layout))\r\n```",
  "created_at":"2020-10-17T22:58:37Z",
  "id":711090923,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTA5MDkyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-17T22:58:37Z",
  "user":"MDQ6VXNlcjM1MTcxNDE3"
 },
 {
  "author_association":"MEMBER",
  "body":"Your NumpyArray is unnecessarily wrapped (`Array`) and unwrapped (`.layout`), but otherwise, yes. Also, the general function would work for all data structures. (There's an internal `ak._util.broadcast_and_apply` that generalizes the process of unwrapping and re-wrapping. That's how all of the ufuncs work. But since it's an internal function and not a part of the stable API, you should use the technique you use here.)\r\n\r\nOh! I guess the reason you wrapped and unwrapped the NumpyArray is because you didn't know it was called that. `:)` Your can use `ak.layout.NumpyArray` instead of `Array` and then `.layout`. The effect is the same, but it's more direct.",
  "created_at":"2020-10-18T00:39:14Z",
  "id":711099210,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTA5OTIxMA==",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2020-10-18T00:39:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for your help!",
  "created_at":"2020-10-18T00:45:05Z",
  "id":711099715,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTA5OTcxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-18T00:45:05Z",
  "user":"MDQ6VXNlcjM1MTcxNDE3"
 },
 {
  "author_association":"MEMBER",
  "body":"I'm reopening this as a reminder to add the feature.",
  "created_at":"2020-10-18T13:48:40Z",
  "id":711170565,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTE3MDU2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-18T13:48:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ok, also if you can point me to where to look at I'll be willing to make the PR for the feature too! ",
  "created_at":"2020-10-18T14:26:23Z",
  "id":711175734,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTE3NTczNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-18T14:26:23Z",
  "user":"MDQ6VXNlcjM1MTcxNDE3"
 },
 {
  "author_association":"MEMBER",
  "body":"I'll want to start a new submodule for this, so it might be done before there's enough of a pattern to build on. However, I could start with the randomization functions and if there are any others you need, it should be clear how to build on that pattern.",
  "created_at":"2020-10-18T15:09:26Z",
  "id":711208226,
  "issue":489,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTIwODIyNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-10-18T15:09:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I just realised that is_none exist and work like charm, \r\n\r\na[~ak.is_none(a)]\r\n",
  "created_at":"2020-10-19T00:06:13Z",
  "id":711444717,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMTQ0NDcxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T00:06:13Z",
  "user":"MDQ6VXNlcjQ5OTY2MDk="
 },
 {
  "author_association":"NONE",
  "body":"> \r\n> \r\n> I just realised that is_none exist and work like charm,\r\n> \r\n> a[~ak.is_none(a)]\r\n\r\nI agree with your solution. \r\nHowever, it doesn't seem to work for nested arrays, like:\r\n```\r\na = [[1, None], [3]   , [] , [4, None, 6]]\r\na = ak.from_iter(a)\r\nak.is_none(a)\r\n#<Array [False, False, False, False] type='4 * bool'>\r\n``` \r\nIt would be nice if an ```axis``` parameter could be added to ```ak.is_none()```",
  "created_at":"2020-10-19T14:34:00Z",
  "id":712206175,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjIwNjE3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T14:34:30Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"I agree. That's #193.",
  "created_at":"2020-10-19T14:48:47Z",
  "id":712218324,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjIxODMyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T14:48:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I agree, i end up in a situation where I have the nested arrays. I would wait for the feature to be in the awkward array in future. \r\n",
  "created_at":"2020-10-19T14:51:31Z",
  "id":712220182,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjIyMDE4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T14:51:31Z",
  "user":"MDQ6VXNlcjQ5OTY2MDk="
 },
 {
  "author_association":"NONE",
  "body":"I haven't been aware of #193. Thanks for bringing it up.",
  "created_at":"2020-10-19T15:05:16Z",
  "id":712229894,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjIyOTg5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T15:05:16Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"Also, someone contacted me on Slack about this\u2014the natural-looking syntax\r\n\r\n```python\r\na[a != None]\r\n```\r\n\r\nwon't work because `np.equal` and `np.not_equal` have not been overridden to interpret missing value `==` missing value as True and missing value `==` anything else as False. (More likely, they'd be overridden to call `ak.is_none`.)\r\n\r\n(If \"None\" were \"NaN,\" then things would be even more complicated, semantically, because `NaN == NaN` is always _supposed to be_ False. By spelling it \"None,\" we've committed to a user expectation that `None == None` is True, which is more palatable.)\r\n\r\nAnd another thing, which has bothered me but apparently nobody else yet, is that using a slice to remove None values keeps the type of the array as option-type, potentially missing values, even though all of the actually missing values have been removed. Since the result of `~ak.is_none(a)` has lost its connection to `a`, passing it to `a[\u00b7]` can't tell it, \"Yes, this eliminates every last None; you can simplify your type.\" If it's desirable to have such a feature, we'd probably want to create `ak.drop_none`. Doing it in one step allows us to know enough to change the type. Also, an operation like `ak.drop_none` on missing values would have parallels with an operation to project a union onto only one of its possibilities and simplify _its_ type, which is important because unions can't be used everywhere (for example, Numba).",
  "created_at":"2020-10-19T15:35:51Z",
  "id":712250246,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjI1MDI0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T15:35:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"```ak.drop_none``` sounds like a nice thing to have.\r\n\r\nAbout the Slack: is there a way to join it myself?",
  "created_at":"2020-10-19T15:54:17Z",
  "id":712262157,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjI2MjE1Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-10-19T15:54:17Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"> About the Slack: is there a way to join it myself?\r\n\r\nThis is the [IRIS-HEP](https://iris-hep.org/) Slack, though there are also non-IRIS-HEP members of the particle physics community on it. (It's not specifically an Awkward Array Slack or anything; for that, I have a [Gitter channel](https://gitter.im/Scikit-HEP/awkward-array)). It's not restricted, and if you think it makes sense to join, send me your email address by email (I'm pivarski at princeton.edu) and I'll send you an invite.\r\n\r\nActually, I'd prefer these sorts of requests to be on GitHub Issues, anyway. Getting them on a different platform makes it less likely that I'll remember to do them and a private message on Slack has zero chance of anyone else stepping in to do it, since they don't even know about it. In fact, it's lucky that the above discussion reminded me about the `a[a != None]` thing long enough to write it down here. Without that reminder, it probably would have just died in my memory.",
  "created_at":"2020-10-19T16:10:06Z",
  "id":712272186,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjI3MjE4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T16:10:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I also tried a[a!=None] but did not work, but could not understand why? \r\nI also asked the same question on stackoverflow a week ago and due to no response I posted directly here :) \r\n\r\nhttps://stackoverflow.com/questions/64308530/removing-none-from-an-awkward-array/64419535#64419535\r\n\r\nwe can update it, one a[a!=None] is working \r\n",
  "created_at":"2020-10-19T16:15:47Z",
  "id":712275607,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjI3NTYwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T16:15:47Z",
  "user":"MDQ6VXNlcjQ5OTY2MDk="
 },
 {
  "author_association":"MEMBER",
  "body":"I have no idea why I didn't get an alert about the StackOverflow question. I'm signed up for email alerts for everything tagged `[awkward-array]`. There's something about StackOverflow's \"watching\" system that I don't understand...\r\n\r\n-----\r\n\r\nAs it stands, `a != None` is evaluated just like any other `np.not_equal`: any None values are set aside, everything else is compared using NumPy (for which all the non-None values in `a` all return False\u2014they're not equal to None by definition), and then the True/False values (all False) are mixed in with the None values that have been set aside. For\r\n\r\n```python\r\n>>> a = ak.Array([96, 99, 67, None, 1, None])\r\n>>> a\r\n<Array [96, 99, 67, None, 1, None] type='6 * ?int64'>\r\n```\r\n\r\nthe result of `a != None` is the same as the result of `a != 5`:\r\n\r\n```python\r\n>>> a != None\r\n<Array [True, True, True, None, True, None] type='6 * ?bool'>\r\n>>> a != 5      # notice that the outputs are True, False, or None; the type is not bool but ?bool\r\n<Array [True, True, True, None, True, None] type='6 * ?bool'>\r\n```\r\n\r\nbecause \"missing\" is a special value that is not checked for equality like everything else. As I said in a parenthetical statement above, defining the equality of missing values has had a long, strange history. At least if we're spelling it \"None\" and not \"NaN\", Python has a tradition of interpreting None as an ordinary object that can be equal or not equal to things\u2014specifically, it's not equal to anything except another None. To make Awkward Array more familiar to newcomers from Python, we should make `!= None` mean False for missing values, True otherwise.\r\n\r\nGetting `a == None` and `a != None` to return non-option-type booleans (i.e. type `bool`, not `?bool`) from `ak.is_none` would require modification to this function\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e1fe6f2497ae007bdddcdbd60a08f7bbec4e1029/src/awkward1/_connect/_numpy.py#L71-L105\r\n\r\nto recognize that (a) the `ufunc` is `numpy.equal` or `numpy.not_equal`, (b) one of the `inputs` is `None`, and (c) that the other of the `inputs` (there can only be two) has option-type:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e1fe6f2497ae007bdddcdbd60a08f7bbec4e1029/src/awkward1/_util.py#L70-L76\r\n\r\nand then return a lambda that calls `awkward1.operations.structure.is_none` with `highlevel=False`, rather than the default of returning no lambda (None), letting the recursion continue to the next level of depth (after which, the missing values have already been set aside and it's too late).\r\n\r\nBut doing this, formally recognizing that missing values are equal to None (instead of just presenting them on the screen that way and converting them to None in `ak.to_list`), we'd be obliged to also say that\r\n\r\n```python\r\nak.Array([1, None, 2]) == ak.Array([3, None, None])\r\n```\r\n\r\nshould return\r\n\r\n```python\r\nak.Array([False, True, False])\r\n```\r\n\r\nwhich is to say, the intersection of `ak.is_none` (which checks to see where the two arrays both have missing values) and the normal definition of `np.equal` (one step further in the recursion, after the None values have already been set aside). This is totally doable, but it's more complicated than the above, and both must be implemented together for consistency. That's why it's in the \"should be done, hasn't been done\" category.",
  "created_at":"2020-10-19T16:57:02Z",
  "id":712299099,
  "issue":490,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjI5OTA5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T16:57:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've answered it on StackOverflow, and I'm looking into why I didn't get an email about the use of the `[awkward-array]` tag. According to StackOverflow, it's one of my watched tags.",
  "created_at":"2020-10-20T00:21:14Z",
  "id":712512603,
  "issue":491,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjUxMjYwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-20T00:21:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It turned out that StackOverflow had confirmed my email address for watching the `[uproot]` tag, but it hadn't confirmed my email address for watching the `[awkward-array]` tag. Since most of the questions had previously been tagged with both `[uproot]` and `[awkward-array]` (because Awkward is still growing into recognition as a library on its own), I hadn't realized that I was missing questions marked with `[awkward-array]` only.\r\n\r\nThat's fixed now. Sorry I missed your question!",
  "created_at":"2020-10-20T03:24:11Z",
  "id":712563895,
  "issue":491,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjU2Mzg5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-20T03:24:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"If I understand right, you need to resolve this cross-reference `sim_id_gem_cluster` into the appropriate entry in the `gem_cluster` array? This is a pretty common pattern in e.g. CMS NanoAOD (see `Electron_JetIdx` etc.). The challenge is that the cluster index is currently a _local_ index with respect to the start of each event. What I have done for coffea NanoEvents is to convert the index to a global one by adding the event offsets (of the gem_cluster array) and then using a `IndexedArray`.  For example,\r\n```python\r\nimport numpy as np\r\nimport awkward1 as ak\r\n\r\nsim_muons = ak.zip(\r\n    {\r\n        \"pt\": ak.Array([[], [], [20.0, 30.0]]),\r\n        \"cluster_idx\": ak.Array([[], [], [[2, 5], [6, 2]]]),\r\n    }\r\n)\r\n\r\ngem_clusters = ak.zip(\r\n    {\r\n        \"value\": ak.Array(\r\n            [[10.0, 20.0], [30.0, 40.0], [50.0, 60.0, 70.0, 80.0, 90.0, 100.0, 110.0]]\r\n        ),\r\n        \"bx\": ak.Array([[-1, 0], [1, 0], [0, 1, 2, -2, 1, 1, 1]]),\r\n    }\r\n)  # type \"nevents * nclusters * whatever\"\r\nid_global = ak.flatten(\r\n    sim_muons.cluster_idx + np.asarray(gem_clusters.layout.starts), axis=None\r\n)\r\nsim_muons[\"clusters\"] = ak.Array(\r\n    ak.layout.ListOffsetArray64(\r\n        sim_muons.layout.offsets,\r\n        ak.layout.ListOffsetArray64(\r\n            sim_muons.layout.content.offsets,\r\n            ak.flatten(gem_clusters)[id_global].layout,\r\n        ),\r\n    )\r\n)\r\n```\r\n\r\nThen you could compute a mask that selects, e.g. muons that have at least one cluster with bx 0:\r\n```\r\nIn [2]: ak.any(sim_muons.clusters.bx == 0, axis=1)\r\nOut[2]: <Array [[], [], [False, False]] type='3 * var * bool'>\r\n```\r\nor select using whatever else is available in the cluster record.",
  "created_at":"2020-10-21T14:28:26Z",
  "id":713618066,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMzYxODA2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-21T14:28:26Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"In general, these sort of \"build-up structure\" operations are part of what the NanoEvents schema classes are trying to abstract, but it would be nice if awkward-array made a few convenience functions to simplify this. For example, if we were in numpy we could use the multi-axis fancy-indexing:\r\n```\r\na = np.random.normal(size=12).reshape(3,4)\r\nidx = np.random.randint(0, 3, size=24).reshape(3,4,2)\r\nidx0 = np.arange(3).repeat(8).reshape(3,4,2)  # make a first-axis indexer\r\na[idx0, idx]\r\n```\r\nOne of the requirements there is that each item in the getitem tuple is the same shape. Perhaps a new type of multi-axis indexing with N ListArrays could be allowed, where as long as they are all broadcast-compatible, they do the same indexing operation as in numpy but generalized to jagged axes.",
  "created_at":"2020-10-21T14:44:05Z",
  "id":713628976,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMzYyODk3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-21T14:48:41Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks, @nsmith-! Yesterday, @dildick and I were talking about this on Gitter (https://gitter.im/Scikit-HEP/awkward-array). Since this is a common problem, we need a way to do it with high-level functions (i.e. without explicitly building layouts). I've come to the conclusion that an appropriate high-level function does not exist, and your solution involves layouts, too (although quite compact).\r\n\r\nThe big question for such a function is, \"What part of the problem should it handle?\" If it's hyper-specific, then it could only be used for this case. If it's totally general, it might not be obvious that it's a solution to this problem. Naming that function, determining what its arguments and return value should be is the subject of this issue.\r\n\r\nFor the record, I'm going to copy-paste my Gitter solution here, just so that the information isn't lost. (I don't think there's a way to cross-reference a GitHub issue with a time-slice in a Gitter stream.)\r\n\r\n------------\r\n\r\nMy other meeting just finished and I have a low-level solution for you. As a reminder, my version of your arrays are\r\n\r\n```python\r\n>>> sim_id_gem_cluster\r\n<Array [[], [], [[2, 5], [6, 2]]] type='3 * var * var * int64'>\r\n>>> gem_cluster\r\n<Array [[], ... {id: 4}, {id: 5}, {id: 6}]] type='3 * var * {\"id\": int64}'>\r\n>>> gem_cluster.tolist()\r\n[[], [], [{'id': 0}, {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}, {'id': 5}, {'id': 6}]]\r\n```\r\n\r\nThe problem is that each muon in each event (2 levels of jaggedness) indexes a cluster in each event (1 level of jaggedness). To be able to slice gem_cluster with sim_id_gem_cluster, you need a full copy of each event's clusters for every muon. In other words, we need to make gem_cluster look like this:\r\n\r\n```python\r\n[[], [], [[{'id': 0}, {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}, {'id': 5}, {'id': 6}],\r\n          [{'id': 0}, {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}, {'id': 5}, {'id': 6}]]]\r\n```\r\n\r\nThat's the part that unfortunately doesn't have a high-level function. It can be made by creating an index that repeats as many times as the muons:\r\n\r\n```python\r\n>>> index = np.repeat(np.arange(len(sim_id_gem_cluster)), ak.num(sim_id_gem_cluster))\r\n>>> index\r\narray([2, 2])\r\n```\r\n\r\nand then using that index to build a (low-level \"layout\") [IndexedArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedArray.html):\r\n\r\n```python\r\n>>> indexedarray = ak.layout.IndexedArray64(ak.layout.Index64(index), gem_cluster.layout)\r\n>>> ak.to_list(indexedarray)\r\n[[{'id': 0}, {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}, {'id': 5}, {'id': 6}],\r\n [{'id': 0}, {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}, {'id': 5}, {'id': 6}]]\r\n```\r\n\r\n(If you look at that indexedarray, you'll see its internal structure; that's why I pass it through ak.to_list.)\r\n\r\nThen we can build a doubly jagged array using parts of the sim_id_gem_cluster:\r\n\r\n```python\r\n>>> new_gem_cluster_layout = ak.layout.ListOffsetArray64(sim_id_gem_cluster.layout.offsets, indexedarray)\r\n>>> ak.to_list(new_gem_cluster_layout)\r\n[[], [], [[{'id': 0}, {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}, {'id': 5}, {'id': 6}],\r\n          [{'id': 0}, {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}, {'id': 5}, {'id': 6}]]]\r\n```\r\n\r\nAnd that's what we wanted to make, so wrap it up as a new (high-level) array:\r\n\r\n```python\r\n>>> new_gem_cluster = ak.Array(new_gem_cluster_layout)\r\n>>> new_gem_cluster\r\n<Array [[], ... {id: 4}, {id: 5}, {id: 6}]]] type='3 * var * var * {\"id\": int64}'>\r\n```\r\n\r\nNow we can simply slice it.\r\n\r\n```python\r\n>>> new_gem_cluster[sim_id_gem_cluster]\r\n<Array [[], ... {id: 5}], [{id: 6}, {id: 2}]]] type='3 * var * var * {\"id\": int64}'>\r\n>>> new_gem_cluster[sim_id_gem_cluster].tolist()\r\n[[], [], [[{'id': 2}, {'id': 5}], [{'id': 6}, {'id': 2}]]]\r\n```\r\n\r\nThis act of \"copying\" the whole set of clusters for each muon to pick from is not expensive: the IndexedArray is functioning as a set of pointers. We've actually just built a set of pointers with the same multiplicity as the muons that point to the set of all clusters in the same event.\r\n\r\nActually, instead of a StackOverflow post, could you make the above a feature request for the missing high-level function? You shouldn't have to build low-level layouts by hand to solve a problem like this. There ought to be some high-level function for making indexes in sim_id_gem_cluster match up with items in gem_cluster to make new_gem_cluster in one line. The hard part will be determining what a natural interface should be\u2014how you would expect such a function to be named and parameterized. That's where I need to get input from you, because I don't know what would be the most \"natural\" or \"guessable\" for you.\r\n\r\nFor something completely different, here's how you could do it with Numba:\r\n\r\n```python\r\n>>> import numba as nb\r\n>>> @nb.jit\r\n... def select(builder, gem_cluster, sim_id_gem_cluster):\r\n...     for clusters, muons in zip(gem_cluster, sim_id_gem_cluster):\r\n...         builder.begin_list()\r\n...         for muon in muons:\r\n...             builder.begin_list()\r\n...             for i in muon:\r\n...                 builder.append(clusters[i])\r\n...             builder.end_list()\r\n...         builder.end_list()\r\n...     return builder\r\n... \r\n>>> select(ak.ArrayBuilder(), gem_cluster, sim_id_gem_cluster).snapshot()\r\n<Array [[], ... {id: 5}], [{id: 6}, {id: 2}]]] type='3 * var * var * {\"id\": int64}'>\r\n```\r\n\r\nYou can drop the `@nb.jit` to make this run slowly, but more easily debuggable. With `@nb.jit`, it tries to compile the contents, which means that any type errors yield lots of output.\r\n\r\nThe ak.ArrayBuilder is a way to make structured arrays gradually, which is a better fit to for-loop style code. Every begin_list must be accompanied by an end_list, and it does the append at the right level of depth. An ak.ArrayBuilder is not an ak.Array; that's what the .snapshot() is for.\r\n\r\n(For all but the most complex problems, I want it to be possible to do it with slices and also to do it with Numba, so that whichever is more natural for a given problem may be used. There are some limitations in Numba, such as \"all indexing must be simple indexing: i must be an int.\" Thus, you're forced to have three nested for loops in this example. They're compiled, so they're fast, but verbose.)",
  "created_at":"2020-10-21T14:44:47Z",
  "id":713629487,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMzYyOTQ4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-21T14:44:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> it would be nice if awkward-array made a few convenience functions to simplify this.\r\n> \r\n> ```python\r\n> a = np.random.normal(size=12).reshape(3,4)\r\n> ```\r\n\r\nIs this one #489?",
  "created_at":"2020-10-21T14:46:59Z",
  "id":713630971,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMzYzMDk3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-21T14:46:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@nsmith- Thanks. I might be able to use that as a temporary solution until a suitable function becomes available.",
  "created_at":"2020-10-22T18:16:27Z",
  "id":714671615,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDY3MTYxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T18:16:27Z",
  "user":"MDQ6VXNlcjQxMzQ3ODY="
 },
 {
  "author_association":"NONE",
  "body":"@nsmith- So I tried hooking up your solution in my script. I think something is wrong.\r\n\r\nIf I use this sim_muons array\r\n```\r\nsim_muons = ak.zip(\r\n    {\r\n        \"pt\": ak.Array([[], [], [20.0, 30.0, 40]]),\r\n        \"cluster_idx\": ak.Array([[], [], [[2, 5], [6, 2], [0]]]),\r\n    }\r\n)\r\n```\r\nI get as output \r\n```\r\nak.any(sim_muons.clusters.bx == 0, axis=1)\r\n[[], [], [True, False]]\r\n```\r\nI was expecting \r\n```\r\n[[], [], [False, False, True]]\r\n```\r\n\r\n",
  "created_at":"2020-10-22T19:51:47Z",
  "id":714724196,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDcyNDE5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T19:51:47Z",
  "user":"MDQ6VXNlcjQxMzQ3ODY="
 },
 {
  "author_association":"NONE",
  "body":"And the output remains the same when I add more muons which match to the same cluster\r\n```\r\nsim_muons = ak.zip(\r\n    {\r\n        \"pt\": ak.Array([[], [], [20.0, 30.0, 40, 50, 60]]),\r\n        \"cluster_idx\": ak.Array([[], [], [[2, 5], [6, 2], [0], [0], [0]]]),\r\n    }\r\n)\r\n```",
  "created_at":"2020-10-22T19:54:27Z",
  "id":714725492,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDcyNTQ5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T19:54:27Z",
  "user":"MDQ6VXNlcjQxMzQ3ODY="
 },
 {
  "author_association":"NONE",
  "body":"Changing to ```ak.any(sim_muons.clusters.bx == 0, axis=2)``` seems to do the trick. It now prints\r\n```\r\n[[], [], [False, False, True, True, True]]\r\n```\r\n\r\n@nsmith- Another question I'm struggling with: how would this work each muon has K index arrays, with each index array matching for a different object? It seems I can only add a single index array. When I add a second one I get an error which originates from the line with the new index array.\r\n```\r\n  File \"/cvmfs/cms.cern.ch/slc7_amd64_gcc820/external/py2-awkward1/0.3.1/lib/python2.7/site-packages/awkward1/operations/structure.py\", line 337, in zip\r\n    out = awkward1._util.broadcast_and_apply(layouts, getfunction, behavior)\r\n  File \"/cvmfs/cms.cern.ch/slc7_amd64_gcc820/external/py2-awkward1/0.3.1/lib/python2.7/site-packages/awkward1/_util.py\", line 820, in broadcast_and_apply\r\n    out = apply(broadcast_pack(inputs, isscalar), 0)\r\n  File \"/cvmfs/cms.cern.ch/slc7_amd64_gcc820/external/py2-awkward1/0.3.1/lib/python2.7/site-packages/awkward1/_util.py\", line 659, in apply\r\n    outcontent = apply(nextinputs, depth + 1)\r\n  File \"/cvmfs/cms.cern.ch/slc7_amd64_gcc820/external/py2-awkward1/0.3.1/lib/python2.7/site-packages/awkward1/_util.py\", line 708, in apply\r\n    outcontent = apply(nextinputs, depth + 1)\r\n  File \"/cvmfs/cms.cern.ch/slc7_amd64_gcc820/external/py2-awkward1/0.3.1/lib/python2.7/site-packages/awkward1/_util.py\", line 697, in apply\r\n    nextinputs.append(x.broadcast_tooffsets64(offsets).content)\r\n```\r\n",
  "created_at":"2020-10-22T20:06:56Z",
  "id":714731723,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDczMTcyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T20:06:56Z",
  "user":"MDQ6VXNlcjQxMzQ3ODY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi,\r\nYeah it should have been a reduction along `axis=2`, that was my mistake. For a new index array, you have to re-compute the global index with respect to the destination collection. I would suggest making a helper function like:\r\n```python\r\ndef embed_crossref(source, idx_name, dest):\r\n    \"\"\"Embed a cross-reference\r\n\r\n    Parameters\r\n    ----------\r\n        source : ak.Array\r\n            any array with shape N * var * {record}\r\n        idx_name : str\r\n            A field in the source record\r\n        dest : ak.Array\r\n            any array with shape N * var * {record}, where:\r\n            ``ak.max(source[idx_name], axis=1) < ak.num(dest)`` and\r\n            ``ak.min(source[idx_name], axis=1) >= 0``\r\n    \"\"\"\r\n    assert ak.all(ak.max(source[idx_name], axis=1) < ak.num(dest))\r\n    assert ak.all(ak.min(source[idx_name], axis=1) >= 0)\r\n\r\n    id_global = ak.flatten(\r\n        source[idx_name] + np.asarray(dest.layout.starts), axis=None\r\n    )\r\n    return ak.Array(\r\n        ak.layout.ListOffsetArray64(\r\n            source.layout.offsets,\r\n            ak.layout.ListOffsetArray64(\r\n                source.layout.content.offsets,\r\n                ak.flatten(dest)[id_global].layout,\r\n            ),\r\n        )\r\n    )\r\n\r\nsim_muons[\"clusters\"] = embed_crossref(sim_muons, \"cluster_idx\", gem_clusters)\r\n```\r\n\r\nA small caveat, its not really _any_ array with the given shape, but restricted to `ListOffsetArray` or `ListArray` types, which is usually the case when extracting fresh arrays from a ROOT file.",
  "created_at":"2020-10-22T20:18:00Z",
  "id":714737209,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDczNzIwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T20:28:23Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"If you're interested, coffea NanoEvents allows you to do things like the above, with the addition of _lazy access_, which means the destination collections of such cross references are not read until you need them. You can create a custom `Schema` class deriving from https://github.com/CoffeaTeam/coffea/blob/master/coffea/nanoevents/schemas.py#L76 for your nTuple format once and then skip the boilerplate like this for subsequent work.",
  "created_at":"2020-10-22T20:26:21Z",
  "id":714741557,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDc0MTU1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T20:26:21Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith-, getting back to the original request of slicing a jagged array using a doubly jagged array, is this something that NanoEvents can do? I'm not asking about completely general types, but for the case @dildick originally wanted. If so, I'd rather close this and leave this to NanoEvents.\r\n\r\n(My list of tasks is a mile long!)",
  "created_at":"2020-10-30T22:54:25Z",
  "id":719834601,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgzNDYwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:54:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Well I provided a reasonably general solution in the self-contained `embed_crossref` function above, and there is a lazy equivalent in NanoEvents, I'm not sure if that qualifies? I would leave it to @dildick to clarify if these solutions are sufficient.\r\nPerhaps I could offer one takeaway: given layout is semi-private, maybe there should be a public method to create new jagged arrays from counts or offsets, a la ak0's `JaggedArray.from_counts` or `JaggedArray.from_offsets`.",
  "created_at":"2020-11-02T17:43:46Z",
  "id":720623193,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDYyMzE5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T17:43:46Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"NONE",
  "body":"@nsmith, so I tried your solution. Strangely enough, the procedure works outside an encapsulating function, but once in a separate function, it does not... I have been trying to figure out what happens. Also, when I include extra index arrays, e.g. for other matching objects, the broadcasting fails",
  "created_at":"2020-11-02T18:10:34Z",
  "id":720637978,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDYzNzk3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T18:10:34Z",
  "user":"MDQ6VXNlcjQxMzQ3ODY="
 },
 {
  "author_association":"NONE",
  "body":"I made a slight change to the function, added \"dest_name\" so that\r\n```\r\ndef embed_crossref(source, idx_name, dest, dest_name):\r\n    \"\"\"Embed a cross-reference\r\n\r\n    Parameters\r\n    ----------\r\n        source : ak.Array\r\n            any array with shape N * var * {record}\r\n        idx_name : str\r\n            A field in the source record\r\n        dest : ak.Array\r\n            any array with shape N * var * {record}, where:\r\n            ``ak.max(source[idx_name], axis=1) < ak.num(dest)`` and\r\n            ``ak.min(source[idx_name], axis=1) >= 0``\r\n    \"\"\"\r\n\r\n    assert ak.all(ak.max(source[idx_name], axis=1) < ak.num(dest))\r\n    assert ak.all(ak.min(source[idx_name], axis=1) >= 0)\r\n\r\n    id_global = ak.flatten(\r\n        source[idx_name] + np.asarray(dest.layout.starts), axis=None\r\n    )\r\n    source[dest_name] = ak.Array(\r\n        ak.layout.ListOffsetArray64(\r\n            source.layout.offsets,\r\n            ak.layout.ListOffsetArray64(\r\n                source.layout.content.offsets,\r\n                ak.flatten(dest)[id_global].layout,\r\n            ),\r\n        )\r\n    )\r\n```\r\nso I call it with\r\n```\r\nembed_crossref(sim_muon, \"sim_id_gem_cluster\", gem_cluster, \"clusters\")\r\n```\r\nand that crashes with \r\n```\r\nTraceback (most recent call last):\r\n  File \"flatPlots.py\", line 200, in <module>\r\n    embed_crossref(sim_muon, \"sim_id_gem_cluster\", gem_cluster, \"clusters\")\r\n  File \"/uscms_data/d3/dildick/work/NewCSCTriggerPatterns/CMSSW_11_2_0_pre7/src/GEMCode/GEMValidation/scripts/objects.py\", line 35, in embed_crossref\r\n    assert ak.all(ak.max(source[idx_name], axis=1) < ak.num(dest))\r\nAssertionError\r\n```\r\n",
  "created_at":"2020-11-02T18:31:51Z",
  "id":720649144,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDY0OTE0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T18:31:51Z",
  "user":"MDQ6VXNlcjQxMzQ3ODY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hm for me it was ok, the assertions just guard against indexes out of bounds (so maybe your nTuples have some bug?)\r\nI guess they could be simplified though, to\r\n```\r\n    assert ak.all(source[idx_name] < ak.num(dest))\r\n    assert ak.all(source[idx_name] >= 0)\r\n```\r\nor they could be removed if you want to live dangerously (and possibly have incorrect results)",
  "created_at":"2020-11-03T16:57:40Z",
  "id":721253578,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTI1MzU3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T16:58:41Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I want to hold off on merging this until after #482. That involved updating the pybind11 dependency, and I got a little confused with updating submodules.\r\n\r\n(Nice to see that this is moving forward, though!)",
  "created_at":"2020-10-21T19:41:12Z",
  "id":713830780,
  "issue":495,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMzgzMDc4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-21T19:41:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski Ping",
  "created_at":"2020-11-11T16:52:23Z",
  "id":725535462,
  "issue":495,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTUzNTQ2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T16:52:23Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the reminder. I'll start by merging with master.",
  "created_at":"2020-11-11T16:59:26Z",
  "id":725539316,
  "issue":495,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTUzOTMxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T16:59:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"First failure: no module \"yaml\" in Python 3.9. If installing a package like yaml is a problem because Python 3.9 is so new, we could restrict this test to only a few or only one task. After all, if the kernels aren't sorted, that can be checked by any one task: it's not a version-dependent statement.\r\n\r\nBy the way, are all of these still requirements?\r\n\r\n```bash\r\n% head -n 1000 requirements*.txt\r\n==> requirements-dev.txt <==\r\nPyYAML\r\npycparser\r\nlark-parser\r\nnumba>=0.50.0;python_version>=\"3.6\"\r\npandas>=0.24.0;python_version>=\"3.6\"\r\nnumexpr;python_version>=\"3.6\"\r\nautograd;python_version>=\"3.6\"\r\npyarrow>=1.0;python_version>=\"3.6\" and sys_platform != \"win32\"\r\n\r\n==> requirements-test.txt <==\r\npytest>=3.9\r\n\r\n==> requirements.txt <==\r\nnumpy>=1.13.1\r\n```\r\n\r\nMaybe pycparser and lark-parser are not needed if we're not routinely parsing C code anymore? That is, everything comes from the kernel-specification now, right? That only needs a Python parser, which is built in.\r\n\r\nThe yaml module you're looking for might be PyYAML\u2014the Python 3.9 task avoids installing requirements-dev.txt (and does fewer tests as a result). Some of those things aren't available for Python 3.9 yet. If all of the failures are 3.9, that's an indication that the new test should be conditioned on \"not Python 3.9,\" which is the case for most tests right now.",
  "created_at":"2020-11-11T17:14:08Z",
  "id":725547765,
  "issue":495,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTU0Nzc2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T17:14:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, this needs to become a high-level function in src/awkward1/operations/structure.py.\r\n\r\nThe implementation exists and is even accessible to Python at the \"layout\" level, but it isn't a high-level function recommended for users with documentation and everything. That is, you can do this:\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> array = ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]])\r\n>>> ak.Array(array.layout.localindex())\r\n<Array [[0, 1, 2], [], [0, 1]] type='3 * var * int64'>\r\n```\r\n\r\nBut you'd really want to be able to do that without having to manually unwrap with `.layout` and re-wrap with `ak.Array(\u00b7)`.\r\n\r\nI think the main thing waiting on this is, what's a good name? We don't use words joined without an underscore at high-level (non-layout) anymore, so at least it would have to be `ak.local_index(\u00b7)`. Is that a guessable name? If someone wanted this feature and didn't know what it was called, would seeing that name in the list make them think, \"Yes, that's it!\" Are there any NumPy parallels that would make sense here, such as a variation on `arange`?\r\n\r\nShould this function have an `axis` parameter? I think that could be implemented through `awkward1._util.recursively_apply` (i.e. descend to the right depth, then call `localindex` on the layout at that depth). Clearly the default should be like the code snippet above, but is that default `axis=1` or `axis=0`? To me, it looks like `axis=1`, where `axis=0` might be `np.arange(len(array))`.",
  "created_at":"2020-10-22T14:24:42Z",
  "id":714531073,
  "issue":496,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDUzMTA3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T14:24:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- might want to weigh in. I know you use this function, too.",
  "created_at":"2020-10-22T14:24:58Z",
  "id":714531245,
  "issue":496,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDUzMTI0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T14:24:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Personally I'd say default `axis=-1` but its not so critical. I wonder if there is any relationship between localindex and the eventual Identities feature that is worth thinking about at this point.",
  "created_at":"2020-10-22T14:50:06Z",
  "id":714548268,
  "issue":496,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDU0ODI2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T14:50:06Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"The default could be `axis=-1`, which would also make the above example work by default. Mainly, I'm trying to work out what `axis=0` and `axis=1` should mean, so that it doesn't get defined off-by-one from what it should naturally be.\r\n\r\nYou're right that localindex and an initial set of `Identities` are numerically equal. They're \"colored differently\" (think \"red functions and green functions\") because `Identities` are carried around as part of an array, manipulated by every operation on that array, whereas the output of localindex simply is an array.",
  "created_at":"2020-10-22T15:02:15Z",
  "id":714556347,
  "issue":496,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDU1NjM0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T15:02:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Right, I think `axis=-1` is the correct default. The closest simple numpy parallel that I can think of is `np.indices(array.shape)[axis]`, although that seems less like a \"localindex\" for `axis != -1` than `np.indices(array.shape[:(len(array.shape) + axis if axis < 0 else axis) + 1])[-1]` does (which would preserve `np.arange(len(array))` for `axis=0`).",
  "created_at":"2020-10-23T13:47:40Z",
  "id":715352300,
  "issue":496,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNTM1MjMwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-23T13:47:40Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"Since NumPy's `np.indices` is close to this functionality, using a word like `ak.local_index` or `ak.local_indices` isn't too far, mentally. (It can't be exactly `ak.indices` because it does a different thing.)\r\n\r\nIt seems that `np.indices` does both of what JaggedArray's `parents` and `localindex` did. There's value to a `parents` equivalent; perhaps `ak.global_index`/`ak.global_indices`? There's definitely a symmetry between `parents` and `localindex`.\r\n\r\n(I've never liked pluralizing a word like \"indices\" because sometimes it's legitimately \"indexes,\" and I can never guess which it's going to be. I also don't like \"axes,\" even though \"axises\" is not a legal spelling. As I type this, there's red under \"axises\" but not the other three.)",
  "created_at":"2020-10-23T14:03:30Z",
  "id":715362497,
  "issue":496,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNTM2MjQ5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-23T14:03:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"From the documentation,\r\n\r\n```python\r\n>>> array = ak.Array([\r\n    [[0.0, 1.1, 2.2], []],\r\n    [[3.3, 4.4]],\r\n    [],\r\n    [[5.5], [], [6.6, 7.7, 8.8, 9.9]]])\r\n>>> ak.local_index(array, axis=0)\r\n<Array [0, 1, 2, 3] type='4 * int64'>\r\n>>> ak.local_index(array, axis=1)\r\n<Array [[0, 1], [0], [], [0, 1, 2]] type='4 * var * int64'>\r\n>>> ak.local_index(array, axis=2)\r\n<Array [[[0, 1, 2], []], ... [], [0, 1, 2, 3]]] type='4 * var * var * int64'>\r\n```\r\n\r\nNote that you can make a Pandas-style MultiIndex by calling this function on every axis.\r\n\r\n```python\r\n>>> multiindex = ak.zip([ak.local_index(array, i) for i in range(array.ndim)])\r\n>>> multiindex\r\n<Array [[[(0, 0, 0), (0, 0, ... ), (3, 2, 3)]]] type='4 * var * var * (int64, in...'>\r\n>>> ak.to_list(multiindex)\r\n[[[(0, 0, 0), (0, 0, 1), (0, 0, 2)], []],\r\n [[(1, 0, 0), (1, 0, 1)]],\r\n [],\r\n [[(3, 0, 0)], [], [(3, 2, 0), (3, 2, 1), (3, 2, 2), (3, 2, 3)]]]\r\n>>> ak.to_list(ak.flatten(ak.flatten(multiindex)))\r\n[(0, 0, 0),\r\n (0, 0, 1),\r\n (0, 0, 2),\r\n (1, 0, 0),\r\n (1, 0, 1),\r\n (3, 0, 0),\r\n (3, 2, 0),\r\n (3, 2, 1),\r\n (3, 2, 2),\r\n (3, 2, 3)]\r\n```",
  "created_at":"2020-11-05T23:10:56Z",
  "id":722697900,
  "issue":496,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMjY5NzkwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-05T23:10:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Copying more things here (because GitHub Issues are a more permanent, well cross-referenced record than Slack), here's a Numba implementation of the integer \u2192 boolean slice:\r\n\r\n```python\r\n>>> @nb.njit\r\n... def make_mask(builder, a, indices):\r\n...     for ai, index in zip(a, indices):\r\n...         builder.begin_list()\r\n...         for j in range(len(ai)):\r\n...             contained = False     # this whole stanza, defining \"contained\", should be \"j in index\"\r\n...             for k in index:\r\n...                 if k == j:\r\n...                     contained = True\r\n...                     break\r\n...             builder.append(contained)\r\n...         builder.end_list()\r\n...     return builder\r\n... \r\n>>> make_mask(ak.ArrayBuilder(), a, indices).snapshot()\r\n<Array [[], [True], [False, True]] type='3 * var * bool'>\r\n```\r\n\r\nThough we don't implement features in Awkward using Numba. (That would be a bad surprise to users without Numba. Besides, the above only works at one level of depth; I think we'd want something more general.) This implementation doesn't use `j in index` because of #494.",
  "created_at":"2020-10-22T14:57:52Z",
  "id":714553345,
  "issue":497,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNDU1MzM0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-22T14:57:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a duplicate of #203.\r\n\r\nTo avoid complicating the interface of `ak.min` and `ak.max` (which are supposed to follow NumPy's, anyway), I think a new name is better: `ak.minby` and `ak.maxby`. (Also avoids the complication that all reducers go through the same code path and none of the other reducers would have an equivalent of this `condition`.)\r\n\r\nThe good news is that I think #203 can be implemented entirely in Python. The `ak.singletons` and `ak.firsts` were created to do minby/maxby\u2014the only problem is that it's cumbersome. I encountered this difficulty while writing\r\n\r\nhttps://github.com/jpivarski-talks/2020-04-08-eic-jlab/blob/master/2020-04-08-eic-jlab-EVALUATED.ipynb\r\n",
  "created_at":"2020-10-23T20:18:09Z",
  "id":715570352,
  "issue":498,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNTU3MDM1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-23T20:18:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I just verified that I can reproduce the bug with the pickle file, thanks.",
  "created_at":"2020-10-23T22:10:30Z",
  "id":715612135,
  "issue":499,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNTYxMjEzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-23T22:10:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"More information: this has something to do with the IndexedArray:\r\n\r\n```python\r\n>>> a.layout\r\n<IndexedArray64>\r\n    <index><Index64 i=\"[8 19 26 30 33 ... 82 86 93 94 97]\" offset=\"0\" length=\"15\" at=\"0x557360eaf610\"/></index>\r\n    <content><ListOffsetArray64>\r\n        <offsets><Index64 i=\"[0 9 25 38 51 ... 1150 1162 1177 1190 1209]\" offset=\"0\" length=\"101\" at=\"0x557360f28dd0\"/></offsets>\r\n        <content><ListOffsetArray64>\r\n            <offsets><Index64 i=\"[0 24 26 42 53 ... 7755 7757 7759 7766 7769]\" offset=\"0\" length=\"1210\" at=\"0x557360a246d0\"/></offsets>\r\n            <content><IndexedArray64>\r\n                <index><Index64 i=\"[0 1 3 5 6 ... 17461 17493 17449 17468 17491]\" offset=\"0\" length=\"7769\" at=\"0x557360f52fc0\"/></index>\r\n                <content><NumpyArray format=\"d\" shape=\"17522\" data=\"-139.329 -53.6797 -53.2201 -46.8106 -20.6568 ... 0.74419 -0.702457 0.92883 -0.971263 -0.697438\" at=\"0x557360f622c0\">\r\n                    <parameters>\r\n                        <param key=\"__doc__\">\"fX[Tracks_]\"</param>\r\n                    </parameters>\r\n                </NumpyArray></content>\r\n            </IndexedArray64></content>\r\n        </ListOffsetArray64></content>\r\n    </ListOffsetArray64></content>\r\n</IndexedArray64>\r\n>>> b.layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 9 24 40 48 ... 143 151 165 175 190]\" offset=\"0\" length=\"16\" at=\"0x557360ea1920\"/></offsets>\r\n    <content><ListOffsetArray64>\r\n        <offsets><Index64 i=\"[0 19 40 69 71 ... 1446 1450 1453 1457 1464]\" offset=\"0\" length=\"191\" at=\"0x557360f2eb80\"/></offsets>\r\n        <content><NumpyArray format=\"?\" shape=\"1464\" data=\"false true false true true ... false false false false false\" at=\"0x557360f2f9c0\"/></content>\r\n    </ListOffsetArray64></content>\r\n</ListOffsetArray64>\r\n```",
  "created_at":"2020-10-30T22:59:00Z",
  "id":719835688,
  "issue":499,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgzNTY4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:59:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Got it: it actually wasn't related to IndexedArray; it was because a ListArray can have gaps, which is a similar kind of issue. It is the kind of issue I was very careful about in the first set of `getitems`, but apparently missed a case when I (much later) implemented jagged `getitems`. I don't see anything similar.",
  "created_at":"2020-11-02T20:45:28Z",
  "id":720714474,
  "issue":499,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDcxNDQ3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T20:45:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Generally, you should use high-level functions, like [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) to concatenate:\r\n\r\n```python\r\n>>> merged = ak.concatenate([one, two, three])\r\n```\r\n\r\nbut I reproduced the bug you reported. The difference between `{one, two, three}` and `merged` is that `merged` is a ListArray, rather than a ListOffsetArray, and it's likely that has never been tested.\r\n\r\nIt's somewhere in here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/40fdf1e6bf427dab7923934d6180e1621ea59e73/src/awkward1/behaviors/string.py#L100-L125\r\n\r\nNote to self: also test it in Numba.",
  "created_at":"2020-10-30T23:08:06Z",
  "id":719838149,
  "issue":501,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgzODE0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T23:08:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ok, thanks for the quick reply.\nI recently updated from pre-0.2.32 and experienced that bug with an array constructed via `from_arrow()` (same code and data that worked before).\n\nThe last working version was 0.2.36. Which is why I debugged 0.2.37 and tried to reproduce the bug with `mergemany()` directly (the main change in v0.2.37, and function that is called by `from_arrow`).\n\nInterestingly, `merged` is a `NumpyArray` of `format='c'`. Not sure if [that](https://github.com/scikit-hep/awkward-1.0/blob/40fdf1e6bf427dab7923934d6180e1621ea59e73/src/libawkward/array/NumpyArray.cpp#L1593) string type is correct?",
  "created_at":"2020-10-31T00:58:19Z",
  "id":719860628,
  "issue":501,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg2MDYyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-31T00:58:19Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"There are two format strings that correspond to raw bytes, and `\"c\"` is one of them. Within the C++ part of Awkward, I ignore the difference between them, but I think `\"c\"` is more appropriate for character data.\r\n\r\nThis format is probably the reason why NumPy raised the error it did, instead of a different one, but I think there's something structural going on...\r\n\r\nYou know, maybe not... I'm slow on the uptake sometimes\u2014maybe this is exactly it. It will be the first thing I try, anyway. Thanks for the tip!",
  "created_at":"2020-10-31T01:47:04Z",
  "id":719866809,
  "issue":501,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg2NjgwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-31T01:47:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Already had one of these: #391.",
  "created_at":"2020-10-30T22:29:09Z",
  "id":719828034,
  "issue":502,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgyODAzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T22:29:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Let's do this together, and we can use it as the point where we migrate to cibuildwheel. I expect together we'll be faster than if we did it separately.",
  "created_at":"2020-11-13T19:25:11Z",
  "id":726987959,
  "issue":503,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjk4Nzk1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-13T19:25:11Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"We can share screen in Zoom, but we might want to also use Atom/TeleType or the VSCode equivalent you use. (I haven't used VSCode before.) Otherwise, we could pass suggestions back and forth on a Google Doc, while only one of us is entering changes.",
  "created_at":"2020-11-13T19:28:07Z",
  "id":726989176,
  "issue":503,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjk4OTE3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-13T19:28:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, this will be more complicated than it was at the beginning of the year...",
  "created_at":"2020-11-13T19:32:19Z",
  "id":726991016,
  "issue":503,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjk5MTAxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-13T19:32:19Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@yimuchen - I think, it is expected since the `numpy` would give the same results:\r\n```\r\n>>> import numpy as np\r\n>>> a = np.array([[[0.01539855, 0.45536142, 0.9510973 , 0.17593855, 0.30485241],\r\n...                      [0.57157337, 0.2281758 , 0.1437675 , 0.80287155, 0.11580015],],\r\n...                     [[0.77897828, 0.75505657, 0.61302232, 0.56694878, 0.99761142],\r\n...                      [0.38639971, 0.69037858, 0.61298759, 0.6602239 , 0.93297311],],])\r\n>>> \r\n>>> a.argsort(-1)\r\narray([[[0, 3, 4, 1, 2],\r\n        [4, 2, 1, 0, 3]],\r\n\r\n       [[3, 2, 1, 0, 4],\r\n        [0, 2, 3, 1, 4]]])\r\n```\r\n",
  "created_at":"2020-11-02T12:37:58Z",
  "id":720446435,
  "issue":506,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDQ0NjQzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T12:37:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"Hm... I hadn't tested that. Thanks for pointing this out. So is this an upstream bug in numpy?",
  "created_at":"2020-11-02T13:49:00Z",
  "id":720483192,
  "issue":506,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDQ4MzE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T13:49:00Z",
  "user":"MDQ6VXNlcjExNzAzNjQ0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Hm... I hadn't tested that. Thanks for pointing this out. So is this an upstream bug in numpy?\r\n\r\nI think, it's how the axis is interpreted. I'd say it's a feature ;p ",
  "created_at":"2020-11-02T13:54:59Z",
  "id":720486354,
  "issue":506,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDQ4NjM1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T13:54:59Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Focusing on the second innermost list,\r\n\r\n```\r\n[0.57157337, 0.2281758 , 0.1437675 , 0.80287155, 0.11580015]\r\n```\r\n\r\nthe actual output of\r\n\r\n```\r\n[4 2 1 0 3]\r\n```\r\n\r\nwould put them in the correct order:\r\n\r\n```python\r\n>>> np.array([0.57157337, 0.2281758 , 0.1437675 , 0.80287155, 0.11580015])[[4, 2, 1, 0, 3]]\r\narray([0.11580015, 0.1437675 , 0.2281758 , 0.57157337, 0.80287155])\r\n>>> ak.Array([0.57157337, 0.2281758 , 0.1437675 , 0.80287155, 0.11580015])[[4, 2, 1, 0, 3]]\r\n<Array [0.116, 0.144, 0.228, 0.572, 0.803] type='5 * float64'>\r\n```\r\n\r\nbut the order you say you're expecting,\r\n\r\n```\r\n[3 2 1 4 0]\r\n```\r\n\r\nwould put them in the wrong order:\r\n\r\n```python\r\n>>> np.array([0.57157337, 0.2281758 , 0.1437675 , 0.80287155, 0.11580015])[[3, 2, 1, 4, 0]]\r\narray([0.80287155, 0.1437675 , 0.2281758 , 0.11580015, 0.57157337])\r\n>>> ak.Array([0.57157337, 0.2281758 , 0.1437675 , 0.80287155, 0.11580015])[[3, 2, 1, 4, 0]]\r\n<Array [0.803, 0.144, 0.228, 0.116, 0.572] type='5 * float64'>\r\n```\r\n\r\nI'm not sure what you intended, but `axis=-1` means that the deepest level of nesting gets sorted, individually for each list object. If you want to sort across inner lists, use lower values of `axis` (`axis=-1` is `axis=num_dimensions-1`, which is `2` for this 3-dimensional array).",
  "created_at":"2020-11-02T14:18:11Z",
  "id":720499243,
  "issue":506,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDQ5OTI0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T14:18:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know if this is still an issue and I'll reopen it. I'm closing it now because I'm trying to get a handle on what work needs to be done\u2014there's a lot!",
  "created_at":"2020-11-02T15:02:48Z",
  "id":720525937,
  "issue":506,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDUyNTkzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T15:02:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski Ah, thanks. It looks like I have misunderstood the meaning of the output: \r\nSo basically, the `argsort` function outputs the indices to retrieve the objects to place the numbers in sorted order. While the output I was expecting was the position at which the numbers will end up. Thanks for the clarification! (I guess it was unfortunate that the first row of the example outputs has two answers coincided, so I as confused as to why the output is 'sometimes right' when it as I that was wrong all along)\r\n\r\nSo yes this issue is closed. ",
  "created_at":"2020-11-02T15:49:31Z",
  "id":720554909,
  "issue":506,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDU1NDkwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T15:49:31Z",
  "user":"MDQ6VXNlcjExNzAzNjQ0"
 },
 {
  "author_association":"MEMBER",
  "body":"Aha! You thought it was the inverse, a natural assumption to make.\r\n\r\nThat is, if selecting an item from an array is a function, `f: int \u2192 dtype`, then slicing by an array, `g: int \u2192 int`, is composition: `f \u2297 g: int \u2192 int \u2297 int \u2192 dtype` or `f \u2297 g: int \u2192 dtype`. That `g: int \u2192 int` function can have an inverse, `g\u207b\u00b9`, (depending on how general we take it to be, but from argsort, it's always a permutation), and it's natural to guess that it's supposed to be `g\u207b\u00b9` when it's supposed to be `g`. They have the same `int \u2192 int` signature, after all.\r\n\r\nThat's why we check to see what NumPy does, to be sure we pick the same conventions.",
  "created_at":"2020-11-02T16:34:39Z",
  "id":720582710,
  "issue":506,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDU4MjcxMA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-02T16:34:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Note to self: It's a coincidence that the two MacOS failures are the oldest Python versions; it's just the Azure/NodeJS bug again. When everything else finishes, restart failed jobs to get these two done.",
  "created_at":"2020-11-02T21:28:56Z",
  "id":720735008,
  "issue":508,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDczNTAwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T21:28:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This could go either way, \"bug\" or \"feature,\" because it's highly expected, but I'm not certain how or how generally it can be implemented.",
  "created_at":"2020-11-03T17:29:55Z",
  "id":721272178,
  "issue":509,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTI3MjE3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T17:29:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"[Questions about how to implement it on Numba's Discourse](https://numba.discourse.group/t/extending-numba-with-a-convertible-to-type/338).",
  "created_at":"2020-11-17T15:48:54Z",
  "id":729018349,
  "issue":509,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTAxODM0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T15:48:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"A few notes:\r\n\r\nFirst, there's a `@lower_cast` decorator that might be what I need: https://numba.pydata.org/numba-doc/dev/extending/low-level.html#lower_cast\r\n\r\nSecondly, there's an example of an ArrayCompatible in Numba's unit tests. The lowering of the `__array__` method is here:\r\n\r\nhttps://github.com/numba/numba/blob/13ece9b97e6f01f750e870347f231282325f60c3/numba/tests/pdlike_usecase.py#L180-L188",
  "created_at":"2020-12-11T22:57:58Z",
  "id":743469589,
  "issue":509,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ2OTU4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T22:57:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"A low level function exists for this, so it will be a matter of connecting it to the high level. Overriding `__copy__` and `__deepcopy__` are good ideas, too.",
  "created_at":"2020-11-04T03:15:11Z",
  "id":721488335,
  "issue":511,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTQ4ODMzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-04T03:15:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Not so much out of scope as not considered yet. I don't think I've overloaded whichever `__method__` corresponds to the `@`, so I don't know which method is getting called here.\r\n\r\nMaybe there's some opportunity to do more than NumPy here: I've seen some cases where it would be handy to do matrix multiplication on many different-sized matrices in a vectorized way, using a jagged array to represent the collection of matricies. The Awkward behavior can't differ from the NumPy behavior on the same inputs, but NumPy can't take jagged arrays. Or maybe it should be a named type, like `\"string\"` and `\"categorical\"`.",
  "created_at":"2020-11-07T20:38:12Z",
  "id":723492718,
  "issue":521,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzQ5MjcxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-07T20:38:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Not so much out of scope as not considered yet. I don't think I've overloaded whichever `__method__` corresponds to the `@`, so I don't know which method is getting called here.\r\n> \r\n> Maybe there's some opportunity to do more than NumPy here: I've seen some cases where it would be handy to do matrix multiplication on many different-sized matrices in a vectorized way, using a jagged array to represent the collection of matricies. The Awkward behavior can't differ from the NumPy behavior on the same inputs, but NumPy can't take jagged arrays. Or maybe it should be a named type, like `\"string\"` and `\"categorical\"`.\r\n\r\nVery good ideas. I think the proper generalization of `__matmul__`, at least when the two lowest dimensions are square, is definitely to consider the awkward arrays as collection of matrices. \r\n\r\nThe reference docs for `ak.Array.__array_ufunc__` say \"When any ufunc is applied to an Awkward Array, it applies to the innermost level of structure\". I think a quick win could be to make ufuncs operate on the innermost square tensors, basically viewing all Awkward arrays as collections of tensors when the lower dimensions are square. This generalization is hopefully meaningful for other ufuncs that operate on tensors as well.",
  "created_at":"2020-11-08T08:10:38Z",
  "id":723544263,
  "issue":521,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzU0NDI2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-08T08:10:38Z",
  "user":"MDQ6VXNlcjgxNzAwMQ=="
 },
 {
  "author_association":"MEMBER",
  "body":"The reason this failed is because `np.matmul` is an unusual ufunc in that it operates on the last two dimensions of an array, rather than the last zero dimensions (i.e. on the scalars at the lowest level). I guess that's what makes it a gufunc. I fixed it by ensuring that any multi-node structures like `RegularArray(RegularArray( ... NumpyArray ... ))` get collapsed into a single NumpyArray node with the same shape. (\"RegularArray\" is an array node that makes nested equal-length lists. The only reason it exists, when NumPy's `shape` does the same job, is because it might contain something _other_ than a NumPy array.)\r\n\r\nThat solves the problem if the lists have \"regular list\" type. However, they might be \"variable-length lists\" that happen to all have the same length (i.e. ListArray or ListOffsetArray nodes). Your example doesn't touch those cases, but it's a separate case. Furthermore, they might have irregular lengths.\r\n\r\nSo I implemented the many-matrix multiplication of different sizes, as discussed above. It's the first Awkward Array feature that has been implemented using Numba, but I think that will be temporary. It can be lowered entirely into awkward-cpu-kernels.so, and therefore be part of the CPU \u2194 GPU interoperability, but I don't have time for that now. In fact, writing a quick implementation is a good way to find out if anybody really needs it, to see if people use it.\r\n\r\nThe interface is:\r\n\r\n```python\r\n>>> import awkward1 as ak\r\n>>> lefts = ak.Array([\r\n...     [[1, 2],\r\n...      [3, 4],\r\n...      [5, 6]],\r\n... \r\n...     [[1, 2, 3, 4],\r\n...      [5, 6, 7, 8]],\r\n... \r\n...     [[1],\r\n...      [2],\r\n...      [3],\r\n...      [4]],\r\n... ])\r\n>>> rights = ak.Array([\r\n...     [[7, 8, 9],\r\n...      [10, 11, 12]],\r\n... \r\n...     [[8, 10],\r\n...      [11, 12],\r\n...      [13, 14],\r\n...      [15, 16]],\r\n... \r\n...     [[5, 6, 7]],\r\n... ])\r\n>>> (lefts @ rights).tolist()\r\n[\r\n    [[ 27,  30,  33],\r\n     [ 61,  68,  75],\r\n     [ 95, 106, 117]],\r\n\r\n    [[129, 140],\r\n     [317, 348]],\r\n\r\n    [[ 5,  6,  7],\r\n     [10, 12, 14],\r\n     [15, 18, 21],\r\n     [20, 24, 28]]\r\n]\r\n```\r\n\r\nbut if you don't have Numba, you'll get an ImportError. Regular-length lists don't need Numba.\r\n\r\n",
  "created_at":"2020-11-13T23:07:58Z",
  "id":727078131,
  "issue":521,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzA3ODEzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-13T23:07:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the quick turn around on this.\r\nKeep up the good work!",
  "created_at":"2020-11-14T09:25:45Z",
  "id":727173773,
  "issue":521,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzE3Mzc3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-14T09:25:45Z",
  "user":"MDQ6VXNlcjgxNzAwMQ=="
 },
 {
  "author_association":"MEMBER",
  "body":"Minimal reproducer:\r\n\r\n```python\r\n>>> content1 = ak.Array([0.0, 1.1, 2.2, 3.3, 4.4]).layout\r\n>>> content2 = ak.Array([  0, 100, 200, 300, 400]).layout\r\n>>> tags = ak.layout.Index8(np.array([0, 0, 0, 1, 1, 0, 0, 1, 1, 1], np.int8))\r\n>>> index = ak.layout.Index64(np.array([0, 1, 2, 0, 1, 3, 4, 2, 3, 4], np.int64))\r\n>>> unionarray = ak.Array(ak.layout.UnionArray8_64(tags, index, [content1, content2]))\r\n>>> unionarray\r\n<Array [0, 1.1, 2.2, 0, ... 4.4, 200, 300, 400] type='10 * union[float64, int64]'>\r\n>>> unionarray + 10\r\n<Array [10, 11.1, 12.2, 10, ... 210, 310, 410] type='10 * float64'>\r\n>>> 10 + unionarray\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/miniconda3/lib/python3.8/site-packages/numpy/lib/mixins.py\", line 31, in func\r\n    return ufunc(other, self)\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward1/highlevel.py\", line 1369, in __array_ufunc__\r\n    return awkward1._connect._numpy.array_ufunc(ufunc, method, inputs, kwargs)\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward1/_connect/_numpy.py\", line 173, in array_ufunc\r\n    out = awkward1._util.broadcast_and_apply(\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward1/_util.py\", line 993, in broadcast_and_apply\r\n    out = apply(broadcast_pack(inputs, isscalar), 0)\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward1/_util.py\", line 756, in apply\r\n    outcontent = apply(nextinputs, depth + 1)\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward1/_util.py\", line 652, in apply\r\n    nextinputs.append(x[mask].project(combo[str(i)]))\r\nValueError: no field of name 1\r\n```\r\n\r\nAnd you only get into this situation (of having a UnionArray) because of #459: floating point numbers with documentation are not being merged with floating point numbers without documentation. But anyway, I'm glad that the other bug exposed this one and will be fixing them both in one PR.",
  "created_at":"2020-11-11T20:53:35Z",
  "id":725655745,
  "issue":522,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTY1NTc0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T20:53:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"As a workaround first do `ak.fill_none(data, [0])` and `argsort` will work properly. That may also tell you what is going wrong above.",
  "created_at":"2020-11-09T05:48:18Z",
  "id":723772622,
  "issue":523,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzc3MjYyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T05:48:18Z",
  "user":"MDQ6VXNlcjE3NzgzNjY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"thanks, @gordonwatts ! I managed to reproduce the error:\r\n```python\r\n    def test_gordons_test():\r\n        array = awkward1.Array([[149, 115, 73.5, 49.1, 44.1, 40.8], [151, 94.8]]).layout\r\n        assert awkward1.to_list(array.argsort(1, True, False)) == [[5, 4, 3, 2, 1, 0], [1, 0]]\r\n        with open('arg_sort_boom.pcl', 'rb') as f:\r\n            data = pickle.load(f)\r\n    \r\n>           assert awkward1.to_list(awkward1.argsort(data, axis=1)) == [[5, 4, 3, 2, 1, 0], [1, 0]]\r\n\r\ntests/test_0074-argsort-and-sort.py:19: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\narray = <Array [[149, 115, 73.5, 49.1, ... [151, 94.8]] type='2 * option[var * ?float64]'>, axis = 1, ascending = True, stable = True, highlevel = True\r\n\r\n    @awkward1._connect._numpy.implements(\"argsort\")\r\n    def argsort(array, axis=-1, ascending=True, stable=True, highlevel=True):\r\n        \"\"\"\r\n        Args:\r\n            array: Data for which to get a sorting index, possibly within nested\r\n                lists.\r\n            axis (int): The dimension at which this operation is applied. The\r\n                outermost dimension is `0`, followed by `1`, etc., and negative\r\n                values count backward from the innermost: `-1` is the innermost\r\n                dimension, `-2` is the next level up, etc.\r\n            ascending (bool): If True, the first value in each sorted group\r\n                will be smallest, the last value largest; if False, the order\r\n                is from largest to smallest.\r\n            stable (bool): If True, use a stable sorting algorithm (introsort:\r\n                a hybrid of quicksort, heapsort, and insertion sort); if False,\r\n                use a sorting algorithm that is not guaranteed to be stable\r\n                (heapsort).\r\n            highlevel (bool): If True, return an #ak.Array; otherwise, return\r\n                a low-level #ak.layout.Content subclass.\r\n    \r\n        For example,\r\n    \r\n            >>> ak.argsort(ak.Array([[7.7, 5.5, 7.7], [], [2.2], [8.8, 2.2]]))\r\n            <Array [[1, 0, 2], [], [0], [1, 0]] type='4 * var * int64'>\r\n    \r\n        The result of this function can be used to index other arrays with the\r\n        same shape:\r\n    \r\n            >>> data = ak.Array([[7, 5, 7], [], [2], [8, 2]])\r\n            >>> index = ak.argsort(index)\r\n            >>> index\r\n            <Array [[1, 0, 2], [], [0], [1, 0]] type='4 * var * int64'>\r\n            >>> data[index]\r\n            <Array [[5, 7, 7], [], [2], [2, 8]] type='4 * var * int64'>\r\n        \"\"\"\r\n        layout = awkward1.operations.convert.to_layout(\r\n            array, allow_record=False, allow_other=False\r\n        )\r\n>       out = layout.argsort(axis, ascending, stable)\r\nE       RuntimeError: argsort_next with unbranching depth > negaxis is only expected to return RegularArray or ListOffsetArray64; instead, it returned IndexedOptionArray64\r\nE       \r\nE       (https://github.com/scikit-hep/awkward-1.0/blob/0.4.4/src/libawkward/array/IndexedArray.cpp#L2311)\r\n\r\nawkward1/operations/structure.py:1247: RuntimeError\r\n\r\n```",
  "created_at":"2020-11-09T09:12:40Z",
  "id":723878092,
  "issue":523,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzg3ODA5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T09:12:40Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - it looks like the `argsort_next` produces a correct output here:\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/c3b9442644004784d81fbdd3cde577a528ada93e/src/libawkward/array/IndexedArray.cpp#L2264\r\nthen it should have bailed out here, but didn't due to a `branchdepth.second` :\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/c3b9442644004784d81fbdd3cde577a528ada93e/src/libawkward/array/IndexedArray.cpp#L2271\r\n ",
  "created_at":"2020-11-09T10:29:35Z",
  "id":723922780,
  "issue":523,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzkyMjc4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T10:29:35Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The key thing is that `ak.argsort` isn't handling IndexedOptionArray (which allows for the _possibility_ of None; that's why `ak.fill_none` is a work-around: you didn't have any None values, but you had the structure that would allows for it).\r\n\r\nFor example, this works:\r\n\r\n```python\r\n>>> ak.argsort(ak.Array([[3, 2, 1], [], [4, 5]]), axis=1)\r\n<Array [[2, 1, 0], [], [0, 1]] type='3 * var * int64'>\r\n```\r\n\r\nbut this does not work:\r\n\r\n```python\r\n>>> ak.argsort(ak.Array([[3, 2, 1], [], None, [4, 5]]), axis=1)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward1/operations/structure.py\", line 1247, in argsort\r\n    out = layout.argsort(axis, ascending, stable)\r\nRuntimeError: argsort_next with unbranching depth > negaxis is only expected to return RegularArray or ListOffsetArray64; instead, it returned IndexedOptionArray64\r\n```\r\n\r\nIt doesn't matter that there's a None in the data; we could put it at the end and slice it off, yet have the same problem:\r\n\r\n```python\r\n>>> ak.argsort(ak.Array([[3, 2, 1], [], [4, 5], None])[:-1], axis=1)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward1/operations/structure.py\", line 1247, in argsort\r\n    out = layout.argsort(axis, ascending, stable)\r\nRuntimeError: argsort_next with unbranching depth > negaxis is only expected to return RegularArray or ListOffsetArray64; instead, it returned IndexedOptionArray64\r\n```\r\n\r\nAnd that's because an array that could have a None in it has a different layout:\r\n\r\n```python\r\n>>> ak.Array([[3, 2, 1], [], [4, 5], None])[:-1].layout\r\n<IndexedOptionArray64>\r\n    <index><Index64 i=\"[0 1 2]\" offset=\"0\" length=\"3\" at=\"0x55badd956480\"/></index>\r\n    <content><ListOffsetArray64>\r\n        <offsets><Index64 i=\"[0 3 3 5]\" offset=\"0\" length=\"4\" at=\"0x55badd952460\"/></offsets>\r\n        <content><NumpyArray format=\"l\" shape=\"5\" data=\"3 2 1 4 5\" at=\"0x55badd954470\"/></content>\r\n    </ListOffsetArray64></content>\r\n</IndexedOptionArray64>\r\n```\r\n\r\nthan an array that cannot have a None in it:\r\n\r\n```python\r\n>>> ak.Array([[3, 2, 1], [], [4, 5]]).layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 3 3 5]\" offset=\"0\" length=\"4\" at=\"0x55badd958490\"/></offsets>\r\n    <content><NumpyArray format=\"l\" shape=\"5\" data=\"3 2 1 4 5\" at=\"0x55badd95a4a0\"/></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\n(IndexedOptionArray64 vs ListOffsetArray64). The `ak.argsort` operation is missing a handler for the IndexedOptionArray64 case, and the internal RuntimeError is saying that this assumption is not satisfied.\r\n\r\nI'm going to try PR #524 now.",
  "created_at":"2020-11-09T14:58:06Z",
  "id":724065792,
  "issue":523,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNDA2NTc5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T14:58:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think, this can be closed now.",
  "created_at":"2020-11-17T23:12:36Z",
  "id":729270303,
  "issue":523,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTI3MDMwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T23:12:36Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"That's right. (I thought it was linked to the PR.)",
  "created_at":"2020-11-17T23:32:28Z",
  "id":729277121,
  "issue":523,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTI3NzEyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T23:32:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - exactly the same issue with `sort_next `",
  "created_at":"2020-11-09T13:12:18Z",
  "id":724003416,
  "issue":524,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNDAwMzQxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T13:12:18Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, I'm done with it. Please, review it when you have time. Thanks!",
  "created_at":"2020-11-17T19:22:49Z",
  "id":729147920,
  "issue":524,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTE0NzkyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T19:22:49Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> I think, I'm done with it.\r\n\r\nI agree! I'll just wait for the tests before merging.",
  "created_at":"2020-11-17T19:32:31Z",
  "id":729153162,
  "issue":524,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTE1MzE2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T19:32:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- Make sure this doesn't break NanoEvents: it affects VirtualArrays.\r\n\r\n(Remember how `Form::getitem_field` was returning the content of the field, rather than the Form of what `Content::getitem_field` would return? This fixes that. It also means that SliceGenerators from field-slices can have Forms now, because now they're correct. But if you have any downstream code that depends on the old kluge, that will need to get updated!)",
  "created_at":"2020-11-09T17:27:01Z",
  "id":724158117,
  "issue":525,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNDE1ODExNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T17:27:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"One concern would be that if SliceGenerator now has a form, slices may act slightly _more_ lazy than before, which may have undesired implications if they get nested a bunch and the form metadata builds up to be a performance/memory hog.",
  "created_at":"2020-11-09T17:35:16Z",
  "id":724162825,
  "issue":525,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNDE2MjgyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T17:35:16Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Field-slices probably want to be lazy, though, right? Because a field-slice on an array is always an _O(1)_ operation, but getting the array might be expensive.",
  "created_at":"2020-11-09T17:39:35Z",
  "id":724165241,
  "issue":525,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNDE2NTI0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T17:39:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Will do.\r\n\r\nFor `zeros_like` and `ones_like`, #493 will hopefully be done soon. (It's probably faster to write that function than the documentation on Numba, though the latter is also needed.)\r\n\r\nBy \"total size of a ListOffsetArray,\" the `len` can be used in a Numbafied function, but maybe you mean the sum of lengths of inner arrays. Outside of JIT, you can do `np.sum(ak.num(array))`, but inside, you'd have to:\r\n\r\n```python\r\n@nb.njit\r\ndef example(array):\r\n    total = 0\r\n    for subarray in array:\r\n        total += len(subarray)\r\n```\r\n\r\nI tried various combinations with list comprehensions, to make it shorter and more idiomatic, but Numba complained about unsupported features.\r\n\r\nI was going to profile the plain for loop against a loop comprehension, if it were even possible, so I'll at least share the results of the plain for loop:\r\n\r\n```python\r\n>>> content = np.random.normal(size=1000000)\r\n>>> offsets = np.arange(0, 1000001, 50)\r\n>>> offsets[1::2] += 47\r\n>>> ak_content = ak.layout.NumpyArray(content)\r\n>>> ak_offsets = ak.layout.Index64(offsets)\r\n>>> ak_listoffsetarray = ak.layout.ListOffsetArray64(ak_offsets, ak_content)\r\n>>> array = ak.Array(ak_listoffsetarray)\r\n\r\n>>> array     # an array that alternates between length-97 and length-3 lists\r\n<Array [[-0.356, -0.825, ... 0.132, -0.651]] type='20000 * var * float64'>\r\n>>> array[0]\r\n<Array [-0.356, -0.825, ... 1.16, -1.93] type='97 * float64'>\r\n>>> array[1]\r\n<Array [0.74, 0.789, 0.483] type='3 * float64'>\r\n>>> array[2]\r\n<Array [-2.85, 0.835, 1.13, ... -1.94, -0.379] type='97 * float64'>\r\n```\r\n\r\n```python\r\nIn [2]: @nb.njit\r\n   ...: def example1(array):\r\n   ...:     total = 0\r\n   ...:     for subarray in array:\r\n   ...:         total += len(subarray)\r\n   ...:     return total\r\n   ...: \r\nIn [12]: %%timeit\r\n    ...: example1(array)\r\n    ...: \r\n    ...: \r\n57.7 \u00b5s \u00b1 982 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n```",
  "created_at":"2020-11-12T14:42:58Z",
  "id":726120568,
  "issue":528,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjEyMDU2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T14:42:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"One would have to look at the assembly for this. If the loop really just adds the sizes of the arrays, then it is fine. If the loop over the subarrays constructs intermediate objects just to get at the size and then destroys them, this is seems wasteful, especially since I can get the size in Python without doing a loop, by stops[-1] - starts[0] or something similar.",
  "created_at":"2020-11-12T18:15:16Z",
  "id":726250448,
  "issue":528,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjI1MDQ0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T18:15:16Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"Having written the assembly-generator for Awkward's Numba extension, it does not create intermediate lists, at least not any that copy array data (final `contents` or indexes like `starts` and `stops`). It does create stack-allocated structs of a fixed size (maybe a few dozen bytes) for each `__getitem__`; those structs contain pointers into the array buffers. Early on, I had to redesign to ensure that the size of this struct does not scale with the complexity of the type, let alone the size of the data it contains. Now the structs are completely fixed size:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ddab4313c30db001f0175406cfbe437855264292/src/awkward1/_connect/_numba/arrayview.py#L410-L421\r\n\r\nIt looks like 48 bytes. 48 bytes get stack-allocated with each `__getitem__`.\r\n\r\nThere's no reference counting: a single reference to the Python view of the array is kept throughout the JITed function execution, so everything within is a safe borrow. Iteration creates a single Iterator whose index updates in place, but then data are extracted at each step of iteration with a `__getitem__` from that index, so `for x in array` would have the same performance as `for i in range(len(array))` and `array[i]`.",
  "created_at":"2020-11-12T18:43:42Z",
  "id":726266191,
  "issue":528,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjI2NjE5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T18:43:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That seems ok then. As a designer I would personally be bothered by this, but as a user of the library, this is completely fine.",
  "created_at":"2020-11-12T18:49:17Z",
  "id":726269122,
  "issue":528,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjI2OTEyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T18:49:17Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a duplicate of #493, which is on the project board for November. (Since it came up multiple times, I'll mentally bump it up in priority.)\r\n\r\nJust a note: since Awkward Arrays are immutable, there would be no advantage to having an `empty_like`, but `zeros_like` and `ones_like` are well-motivated.",
  "created_at":"2020-11-12T14:22:20Z",
  "id":726108313,
  "issue":529,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjEwODMxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T14:22:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't know they are immutable. So I always have to build them incrementally, even if I know the total size in advance?",
  "created_at":"2020-11-12T18:18:15Z",
  "id":726252035,
  "issue":529,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjI1MjAzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T18:18:15Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"I was assuming mutable arrays. I don't understand why the values should be immutable. I suppose it is part of the abstraction, but it seems like a very harsh requirement for a ListOffsetArray",
  "created_at":"2020-11-12T18:19:42Z",
  "id":726252808,
  "issue":529,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjI1MjgwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T18:19:42Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"I mention this at every opportunity\u2014I don't remember who I've told, but I've had this conversation many times.  `:)`  Awkward Arrays have been immutable since an early prototype in summer 2018 when I found that managing mutability\u2014just from the perspective of interface\u2014is too complex. View-vs-copy is a significant issue when using NumPy, but at least in NumPy, an array is either entirely a view or entirely a copy. Most Awkward Arrays are part-view, part-new. It quickly gets impossible to keep track of which arrays are linked by shared memory, so changes are non-local in surprising ways.\r\n\r\nIt _is_ possible to make an Awkward Array a view of a NumPy array and then mutate that NumPy array. Any mutation would require deep knowledge of an Awkward Array's `layout`, so the fact that this method requires such knowledge isn't an additional burden. Such a technique can be used in high-performance situations: see [this part of the docs](https://awkward-array.org/how-to-convert-numpy.html#mutability-of-awkward-arrays-from-numpy).\r\n\r\nAt this moment, I was just working on `ak.ones_like` and `ak.full_like`, which create arrays like `[\"1\", \"1\", \"1\", ..., \"1\"]` when the original array has string type. I was building it like\r\n\r\n```python\r\n>>> ak.Array(ak.layout.ListArray64(\r\n...     ak.layout.Index64(np.full(1000, 0)),\r\n...     ak.layout.Index64(np.full(1000, 5)),\r\n...     ak.layout.NumpyArray(np.frombuffer(b\"hello\", np.uint8), parameters={\"__array__\": \"char\"}),\r\n...     parameters={\"__array__\": \"bytestring\"}))\r\n<Array ['hello', 'hello', ... 'hello', 'hello'] type='1000 * bytes'>\r\n```\r\n\r\nwhich actually has only one `'hello'`, but repeating starts of `[0, 0, 0, ..., 0]` and stops of `[5, 5, 5, ..., 5]` made a logical array with a thousand strings in it. Some steps of processing would leave it as one internal `'hello'`, but other steps would force the `'hello'` to get duplicated. If the array you have is of the former type, then assigning one character in one of these strings would make all of these strings change. If it's the latter type, then it would only change one. This is what it means to need deep knowledge of the \"layout.\"\r\n\r\nFor building up arrays, there's the [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html), though this isn't the fastest possible way to build a structure, even in Numba. There's a solution to the general problem, with layout-dependent hacks to speed up special cases, but there's an intermediate case that needs to be better developed: fast-building of an array when you fully know the type. That could be improved.",
  "created_at":"2020-11-12T19:07:40Z",
  "id":726278967,
  "issue":529,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjI3ODk2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T19:07:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"*Note: I will use `Array` when I talk about an awkward array and `ndarray` when I talk about numpy*\r\n\r\nOk, thank you for the explanation. I didn't know that `.to_numpy` produces a mutable view even if the Array itself is immutable. So in theory, I could create an Array with `empty_like` (which then actually has a use!) then generate a numpy view of it and manipulate the Array with the ndarray.\r\n\r\nI haven't tried what happens when I try to do `to_numpy` on a ListOffsetArray.",
  "created_at":"2020-11-17T16:11:27Z",
  "id":729032961,
  "issue":529,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTAzMjk2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T16:11:27Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"The two patterns described [here](https://awkward-array.org/how-to-convert-numpy.html#mutability-of-awkward-arrays-from-numpy) and [here](https://awkward-array.org/how-to-convert-numpy.html#mutability-of-awkward-arrays-converted-to-numpy) are: (1) create an `ndarray` and wrap it as `ak.Array`, then mutate the `ndarray`, and (2) create an `ak.Array` and extract the underlying `ndarray`, which you can mutate.\r\n\r\nFollowing either of those patterns necessarily gets you involved in the details of the layout, and the fact that some operations view, others copy, and most partly-view, partly-copy. So if you use these patterns, you'll need to be aware of when you transform an array with a slice or other operation because the output of that operation might have copied the data you expect to be able to mutate. You might want to do all the mutation in an early \"initialization\" stage and use the arrays as immutable from that point onward (\"Clojure-style\").\r\n\r\nSince what you want to do depends on layout, you should extract the `ndarray` from the layout like this, not with a high-level function like `ak.to_numpy` (or equivalently, `np.asarray` applied to the high-level `ak.Array`):\r\n\r\n```python\r\n>>> array = probably_from_uproot()     # returning an ak.Array\r\n>>> content = np.asarray(array.layout.content)\r\n```\r\n\r\nThis guarantees that the `ndarray` in `content` is a view of the underlying data. If you do a high-level operation, there is no such guarantee. The symptom of the latter would be that changing the `ndarray` doesn't change the Awkward Array.",
  "created_at":"2020-11-17T17:25:16Z",
  "id":729080039,
  "issue":529,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTA4MDAzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T17:25:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm calling this a \"bug\" because it's needing a better error message. Bad error messages are bugs.",
  "created_at":"2020-11-12T17:16:13Z",
  "id":726216015,
  "issue":531,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjIxNjAxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T17:16:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It works for iminuit, for instance.",
  "created_at":"2020-11-12T19:41:33Z",
  "id":726299065,
  "issue":533,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjI5OTA2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T19:41:33Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"sorry, wrong repo",
  "created_at":"2020-11-12T19:46:21Z",
  "id":726301563,
  "issue":533,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjMwMTU2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T19:46:21Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"It seems like writing is missing completely from uproot4 yet.",
  "created_at":"2020-11-12T19:45:30Z",
  "id":726301152,
  "issue":534,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjMwMTE1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T19:45:30Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"sorry, wrong repo",
  "created_at":"2020-11-12T19:45:54Z",
  "id":726301338,
  "issue":534,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjMwMTMzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-12T19:45:54Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"https://issues.apache.org/jira/browse/ARROW-9556 has been fixed since August 20, 2020, so it should be in 2.0.0. Therefore, I need to get those two tests up to date now.",
  "created_at":"2020-11-13T01:12:09Z",
  "id":726441968,
  "issue":536,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjQ0MTk2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-13T01:12:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Here's a profile for the `__setitem__` version:\r\n```\r\n\r\n  _     ._   __/__   _ _  _  _ _/_   Recorded: 15:45:07  Samples:  10278\r\n /_//_/// /_\\ / //_// / //_'/ //     Duration: 10.409    CPU time: 10.359\r\n/   _/                      v3.2.0\r\n\r\nProgram: example_for_jim.py\r\n\r\n10.409 <module>  example_for_jim.py:1\r\n\u2514\u2500 10.409 __setitem__  awkward1/highlevel.py:966\r\n   \u2514\u2500 10.346 with_field  awkward1/operations/structure.py:479\r\n      \u251c\u2500 8.330 broadcast_and_apply  awkward1/_util.py:492\r\n      \u2502  \u251c\u2500 7.106 apply  awkward1/_util.py:549\r\n      \u2502  \u2502  \u251c\u2500 2.904 apply  awkward1/_util.py:549\r\n      \u2502  \u2502  \u2502  \u251c\u2500 1.463 of  awkward1/nplike.py:10\r\n      \u2502  \u2502  \u2502  \u2502  \u2514\u2500 1.463 kernels  awkward1/operations/convert.py:533\r\n      \u2502  \u2502  \u2502  \u2502     \u2514\u2500 1.461 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502  \u2502        \u2514\u2500 1.453 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502  \u2502           \u2514\u2500 1.414 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502  \u2502              \u251c\u2500 1.228 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502  \u2502              \u2502  \u251c\u2500 0.631 apply  awkward1/operations/convert.py:564\r\n      \u2502  \u2502  \u2502  \u2502              \u2502  \u2502  \u251c\u2500 0.366 [self]  \r\n      \u2502  \u2502  \u2502  \u2502              \u2502  \u2502  \u2514\u2500 0.261 isinstance  <built-in>:0\r\n      \u2502  \u2502  \u2502  \u2502              \u2502  \u251c\u2500 0.317 isinstance  <built-in>:0\r\n      \u2502  \u2502  \u2502  \u2502              \u2502  \u2514\u2500 0.280 [self]  \r\n      \u2502  \u2502  \u2502  \u2502              \u2514\u2500 0.119 [self]  \r\n      \u2502  \u2502  \u2502  \u2514\u2500 1.438 getfunction  awkward1/operations/structure.py:541\r\n      \u2502  \u2502  \u2502     \u2514\u2500 1.438 of  awkward1/nplike.py:10\r\n      \u2502  \u2502  \u2502        \u2514\u2500 1.438 kernels  awkward1/operations/convert.py:533\r\n      \u2502  \u2502  \u2502           \u2514\u2500 1.437 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502              \u2514\u2500 1.430 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502                 \u2514\u2500 1.387 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502                    \u2514\u2500 1.223 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502                       \u251c\u2500 0.630 apply  awkward1/operations/convert.py:564\r\n      \u2502  \u2502  \u2502                       \u2502  \u251c\u2500 0.338 [self]  \r\n      \u2502  \u2502  \u2502                       \u2502  \u2514\u2500 0.279 isinstance  <built-in>:0\r\n      \u2502  \u2502  \u2502                       \u251c\u2500 0.312 isinstance  <built-in>:0\r\n      \u2502  \u2502  \u2502                       \u2514\u2500 0.281 [self]  \r\n      \u2502  \u2502  \u251c\u2500 1.488 getfunction  awkward1/operations/structure.py:541\r\n      \u2502  \u2502  \u2502  \u2514\u2500 1.488 of  awkward1/nplike.py:10\r\n      \u2502  \u2502  \u2502     \u2514\u2500 1.487 kernels  awkward1/operations/convert.py:533\r\n      \u2502  \u2502  \u2502        \u2514\u2500 1.483 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502           \u2514\u2500 1.483 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502              \u2514\u2500 1.479 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502                 \u2514\u2500 1.436 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502                    \u251c\u2500 1.269 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502                    \u2502  \u251c\u2500 0.619 apply  awkward1/operations/convert.py:564\r\n      \u2502  \u2502  \u2502                    \u2502  \u2502  \u251c\u2500 0.352 [self]  \r\n      \u2502  \u2502  \u2502                    \u2502  \u2502  \u2514\u2500 0.255 isinstance  <built-in>:0\r\n      \u2502  \u2502  \u2502                    \u2502  \u251c\u2500 0.345 [self]  \r\n      \u2502  \u2502  \u2502                    \u2502  \u2514\u2500 0.305 isinstance  <built-in>:0\r\n      \u2502  \u2502  \u2502                    \u2514\u2500 0.115 [self]  \r\n      \u2502  \u2502  \u251c\u2500 1.432 of  awkward1/nplike.py:10\r\n      \u2502  \u2502  \u2502  \u2514\u2500 1.432 kernels  awkward1/operations/convert.py:533\r\n      \u2502  \u2502  \u2502     \u2514\u2500 1.430 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502        \u2514\u2500 1.427 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502           \u2514\u2500 1.422 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502              \u2514\u2500 1.371 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502                 \u251c\u2500 1.191 recursive_walk  awkward1/_util.py:1287\r\n      \u2502  \u2502  \u2502                 \u2502  \u251c\u2500 0.600 apply  awkward1/operations/convert.py:564\r\n      \u2502  \u2502  \u2502                 \u2502  \u2502  \u251c\u2500 0.338 [self]  \r\n      \u2502  \u2502  \u2502                 \u2502  \u2502  \u2514\u2500 0.245 isinstance  <built-in>:0\r\n      \u2502  \u2502  \u2502                 \u2502  \u251c\u2500 0.317 isinstance  <built-in>:0\r\n      \u2502  \u2502  \u2502                 \u2502  \u2514\u2500 0.274 [self]  \r\n      \u2502  \u2502  \u2502                 \u2514\u2500 0.132 [self]  \r\n      \u2502  \u2502  \u2514\u2500 1.167 [self]  \r\n      \u2502  \u2514\u2500 1.216 <genexpr>  awkward1/_util.py:998\r\n      \u2502     \u2514\u2500 1.216 broadcast_unpack  awkward1/_util.py:1025\r\n      \u2502        \u2514\u2500 1.175 [self]  \r\n      \u251c\u2500 1.018 wrap  awkward1/_util.py:393\r\n      \u2502  \u2514\u2500 1.017 __init__  awkward1/highlevel.py:209\r\n      \u2502     \u2514\u2500 0.957 find_caches  awkward1/_util.py:1346\r\n      \u2502        \u2514\u2500 0.957 recursive_walk  awkward1/_util.py:1287\r\n      \u2502           \u2514\u2500 0.949 recursive_walk  awkward1/_util.py:1287\r\n      \u2502              \u2514\u2500 0.921 recursive_walk  awkward1/_util.py:1287\r\n      \u2502                 \u2514\u2500 0.816 recursive_walk  awkward1/_util.py:1287\r\n      \u2502                    \u251c\u2500 0.303 isinstance  <built-in>:0\r\n      \u2502                    \u251c\u2500 0.294 [self]  \r\n      \u2502                    \u2514\u2500 0.219 apply  awkward1/_util.py:1349\r\n      \u2502                       \u2514\u2500 0.204 [self]  \r\n      \u2514\u2500 0.963 [self]  \r\n```\r\n\r\nas well as the awkward.zip version:\r\n```\r\n\r\n  _     ._   __/__   _ _  _  _ _/_   Recorded: 15:45:18  Samples:  348\r\n /_//_/// /_\\ / //_// / //_'/ //     Duration: 0.356     CPU time: 0.354\r\n/   _/                      v3.2.0\r\n\r\nProgram: example_for_jim.py\r\n\r\n0.356 <module>  example_for_jim.py:1\r\n\u251c\u2500 0.267 zip  awkward1/operations/structure.py:263\r\n\u2502  \u251c\u2500 0.226 broadcast_and_apply  awkward1/_util.py:492\r\n\u2502  \u2502  \u251c\u2500 0.175 apply  awkward1/_util.py:549\r\n\u2502  \u2502  \u2502  \u251c\u2500 0.064 apply  awkward1/_util.py:549\r\n\u2502  \u2502  \u2502  \u2502  \u251c\u2500 0.057 of  awkward1/nplike.py:10\r\n\u2502  \u2502  \u2502  \u2502  \u2502  \u2514\u2500 0.057 kernels  awkward1/operations/convert.py:533\r\n\u2502  \u2502  \u2502  \u2502  \u2502     \u2514\u2500 0.057 recursive_walk  awkward1/_util.py:1287\r\n\u2502  \u2502  \u2502  \u2502  \u2502        \u2514\u2500 0.055 recursive_walk  awkward1/_util.py:1287\r\n\u2502  \u2502  \u2502  \u2502  \u2502           \u251c\u2500 0.046 recursive_walk  awkward1/_util.py:1287\r\n\u2502  \u2502  \u2502  \u2502  \u2502           \u2502  \u251c\u2500 0.025 apply  awkward1/operations/convert.py:564\r\n\u2502  \u2502  \u2502  \u2502  \u2502           \u2502  \u2502  \u251c\u2500 0.013 isinstance  <built-in>:0\r\n\u2502  \u2502  \u2502  \u2502  \u2502           \u2502  \u2502  \u2514\u2500 0.012 [self]  \r\n\u2502  \u2502  \u2502  \u2502  \u2502           \u2502  \u251c\u2500 0.012 isinstance  <built-in>:0\r\n\u2502  \u2502  \u2502  \u2502  \u2502           \u2502  \u2514\u2500 0.009 [self]  \r\n\u2502  \u2502  \u2502  \u2502  \u2502           \u2514\u2500 0.007 [self]  \r\n\u2502  \u2502  \u2502  \u2502  \u2514\u2500 0.006 <lambda>  awkward1/operations/structure.py:392\r\n\u2502  \u2502  \u2502  \u251c\u2500 0.062 of  awkward1/nplike.py:10\r\n\u2502  \u2502  \u2502  \u2502  \u2514\u2500 0.062 kernels  awkward1/operations/convert.py:533\r\n\u2502  \u2502  \u2502  \u2502     \u2514\u2500 0.061 recursive_walk  awkward1/_util.py:1287\r\n\u2502  \u2502  \u2502  \u2502        \u2514\u2500 0.059 recursive_walk  awkward1/_util.py:1287\r\n\u2502  \u2502  \u2502  \u2502           \u2514\u2500 0.058 recursive_walk  awkward1/_util.py:1287\r\n\u2502  \u2502  \u2502  \u2502              \u2514\u2500 0.053 recursive_walk  awkward1/_util.py:1287\r\n\u2502  \u2502  \u2502  \u2502                 \u251c\u2500 0.025 apply  awkward1/operations/convert.py:564\r\n\u2502  \u2502  \u2502  \u2502                 \u2502  \u251c\u2500 0.014 [self]  \r\n\u2502  \u2502  \u2502  \u2502                 \u2502  \u2514\u2500 0.011 isinstance  <built-in>:0\r\n\u2502  \u2502  \u2502  \u2502                 \u251c\u2500 0.017 isinstance  <built-in>:0\r\n\u2502  \u2502  \u2502  \u2502                 \u2514\u2500 0.011 [self]  \r\n\u2502  \u2502  \u2502  \u2514\u2500 0.044 [self]  \r\n\u2502  \u2502  \u251c\u2500 0.045 <genexpr>  awkward1/_util.py:998\r\n\u2502  \u2502  \u2502  \u2514\u2500 0.045 broadcast_unpack  awkward1/_util.py:1025\r\n\u2502  \u2502  \u2502     \u2514\u2500 0.044 [self]  \r\n\u2502  \u2502  \u2514\u2500 0.006 broadcast_pack  awkward1/_util.py:1001\r\n\u2502  \u2514\u2500 0.039 wrap  awkward1/_util.py:393\r\n\u2502     \u2514\u2500 0.039 __init__  awkward1/highlevel.py:209\r\n\u2502        \u2514\u2500 0.037 find_caches  awkward1/_util.py:1346\r\n\u2502           \u2514\u2500 0.037 recursive_walk  awkward1/_util.py:1287\r\n\u2502              \u2514\u2500 0.037 recursive_walk  awkward1/_util.py:1287\r\n\u2502                 \u2514\u2500 0.037 recursive_walk  awkward1/_util.py:1287\r\n\u2502                    \u2514\u2500 0.035 recursive_walk  awkward1/_util.py:1287\r\n\u2502                       \u251c\u2500 0.017 [self]  \r\n\u2502                       \u251c\u2500 0.009 isinstance  <built-in>:0\r\n\u2502                       \u2514\u2500 0.009 apply  awkward1/_util.py:1349\r\n\u2502                          \u2514\u2500 0.008 [self]  \r\n\u2514\u2500 0.085 <dictcomp>  example_for_jim.py:48\r\n   \u2514\u2500 0.085 __getitem__  awkward1/highlevel.py:614\r\n      \u251c\u2500 0.046 wrap  awkward1/_util.py:393\r\n      \u2502  \u2514\u2500 0.046 __init__  awkward1/highlevel.py:209\r\n      \u2502     \u251c\u2500 0.036 find_caches  awkward1/_util.py:1346\r\n      \u2502     \u2502  \u2514\u2500 0.035 recursive_walk  awkward1/_util.py:1287\r\n      \u2502     \u2502     \u2514\u2500 0.035 recursive_walk  awkward1/_util.py:1287\r\n      \u2502     \u2502        \u2514\u2500 0.033 recursive_walk  awkward1/_util.py:1287\r\n      \u2502     \u2502           \u251c\u2500 0.014 [self]  \r\n      \u2502     \u2502           \u251c\u2500 0.010 isinstance  <built-in>:0\r\n      \u2502     \u2502           \u2514\u2500 0.009 apply  awkward1/_util.py:1349\r\n      \u2502     \u2514\u2500 0.007 arrayclass  awkward1/_util.py:141\r\n      \u2502        \u2514\u2500 0.005 [self]  \r\n      \u2514\u2500 0.039 [self]  \r\n```",
  "created_at":"2020-11-13T21:47:05Z",
  "id":727051178,
  "issue":538,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzA1MTE3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-13T21:47:05Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"First observation: 78% of the `__setitem__` time is spent in `broadcast_and_apply` (Python) and 0.1% of the time is spent in `setitem_field` (C++, copying RecordArrays). So the immutability of RecordArrays, requiring them to be copied every time, is not at fault. The fact that it's in Python will make it easier to narrow in on.",
  "created_at":"2020-11-16T16:09:06Z",
  "id":728160015,
  "issue":538,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODE2MDAxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T16:09:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like about 1/4 of the time is identifying whether this is a CPU array or a GPU array and 3/4 is in what ought to be trivial `getitem_at` and `getitem_slice` operations on RecordArrays. Both of these are disturbing, but the CPU vs GPU determination most of all. I'll start by looking into that.\r\n\r\n(CPU vs GPU is to determine whether we can use NumPy or CuPy.)",
  "created_at":"2020-11-16T16:49:26Z",
  "id":728185161,
  "issue":538,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODE4NTE2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T16:49:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Step 1, which affects 1/4 of the time but shouldn't be allowed to be a bottleneck (deciding between NumPy and CuPy _ought_ to be free): I moved it from a Python `recursively_apply` into C++ and it's 300\u00d7 faster. (\"Ought to be free\" because it's _O(1)_.)\r\n\r\nOn to step 2, the remaining 3/4 of the bottleneck...",
  "created_at":"2020-11-16T19:05:46Z",
  "id":728263324,
  "issue":538,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODI2MzMyNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-16T19:06:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, it was actually two more steps (three optimizations total), but now the original code in https://github.com/scikit-hep/awkward-1.0/issues/538#issue-742805318 goes from (without fixes):\r\n\r\n```\r\n% python testy3.py \r\nnumber of variants 50\r\ntime to make variants: 0.3175015449523926\r\ntime to assign with __setitem__: 5.913200616836548\r\ntime to assign with rezip: 0.21291756629943848\r\n% python testy3.py \r\nnumber of variants 50\r\ntime to make variants: 0.32436084747314453\r\ntime to assign with __setitem__: 6.110222101211548\r\ntime to assign with rezip: 0.2174851894378662\r\n% python testy3.py \r\nnumber of variants 50\r\ntime to make variants: 0.31608152389526367\r\ntime to assign with __setitem__: 5.901622295379639\r\ntime to assign with rezip: 0.21786880493164062\r\n```\r\n\r\nto (with fixes):\r\n\r\n```\r\n% python testy3.py \r\nnumber of variants 50\r\ntime to make variants: 0.08354496955871582\r\ntime to assign with __setitem__: 0.11821508407592773\r\ntime to assign with rezip: 0.028712034225463867\r\n% python testy3.py \r\nnumber of variants 50\r\ntime to make variants: 0.08423042297363281\r\ntime to assign with __setitem__: 0.11730170249938965\r\ntime to assign with rezip: 0.02914595603942871\r\n% python testy3.py \r\nnumber of variants 50\r\ntime to make variants: 0.08287811279296875\r\ntime to assign with __setitem__: 0.11925029754638672\r\ntime to assign with rezip: 0.028862953186035156\r\n```\r\n\r\nAs a bottom line, the `__setitem__` method (which was the main target) is now **50\u00d7 faster**. (Speedups for the individual parts were much larger, but that made them not bottlenecks anymore. Beyond the three fixes in #540, they're not single-item hotspots.) ",
  "created_at":"2020-11-16T23:56:18Z",
  "id":728446316,
  "issue":538,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODQ0NjMxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T23:56:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I need some guidance on negative axis. So far if the `axis` is negative, the first array is used to convert it to positive axis. That assumes that all arrays have the same depth. This will not work if we want to concatenate arrays of different depths at axis -1. I guess, I'd need to do the axis wrap and check for each array in a `getfunction`.\r\n\r\nThe deprecated warning/future error comes from the following:\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/4ecae71066c10d8a3ea5a0efc80bade0394f360e/src/awkward1/_connect/_numpy.py#L214\r\nwhere `broadcast_and_apply` does not allow records:\r\n```python\r\n    out = awkward1._util.broadcast_and_apply(\r\n        inputs, getfunction, behavior, allow_records=False\r\n    )\r\n```\r\non the other hand, merging records as union should be allowed. I wonder if the `allow_records` flag could be percolated from here:\r\n```python\r\nout = awkward1._util.broadcast_and_apply(contents, getfunction, behavior=awkward1._util.behaviorof(*arrays),allow_records=True)[0]\r\n```\r\n",
  "created_at":"2020-11-16T10:50:26Z",
  "id":727900439,
  "issue":539,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNzkwMDQzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T10:50:26Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> So far if the `axis` is negative, the first array is used to convert it to positive axis. That assumes that all arrays have the same depth. This will not work if we want to concatenate arrays of different depths at axis -1.\r\n\r\nThat's an important point that I hadn't thought of. Because of that, it could be acceptable to require `axis` to be nonnegative for all functions that take more than one array, but your solution of just refusing them if they have different depths shrink-wraps around the problem more naturally.",
  "created_at":"2020-11-16T14:26:33Z",
  "id":728095816,
  "issue":539,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODA5NTgxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T14:26:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> where `broadcast_and_apply` does not allow records:\r\n\r\nThe new `allow_records` parameter of `broadcast_and_apply` determines whether the recursion will go through records. Should `concatenate` recurse _through_ records? That would mean\r\n\r\n```python\r\nak.concatenate([ak.Array([{\"x\": [1], \"y\": [1.1]}]), ak.Array([{\"x\": [2, 3], \"y\": [2.2]}])], axis=-1)\r\n```\r\n\r\nwould return\r\n\r\n```python\r\nak.Array([{\"x\": [1, 2, 3], \"y\": [1.1, 2.2]}])\r\n```\r\n\r\nbecause `broadcast_and_apply` can match up field names. The new parameter was added to prevent this for NumPy ufuncs (e.g. `np.sqrt`, `np.cos`, `np.exp`, ...) because records with special meanings like Lorentz vectors overload these ufuncs (especially operations like addition and subtraction), and the default implementation is often wrong (such as elementwise addition when the Lorentz vector is polar). Automatically broadcasting _ufuncs_ through records is error-prone because someone might think that the ufunc has been overloaded when it's actually the (wrong) default behavior.\r\n\r\nThat doesn't apply to concatenation. Since concatenation can't be overloaded, I think it would be fine to allow concatenation to recurse through records. Negative `axis` is defined through `wrap_if_negative`, which descends through records\u2014the reason it needs to be called at each depth is because it's inconclusive until it passes through any records whose fields have different depths. By using `wrap_if_negative`, you're already hinting that it's possible to recurse through records.\r\n\r\nSo I think you can set `allow_records=True`. It shouldn't require any work other than setting that value.\r\n",
  "created_at":"2020-11-16T14:44:01Z",
  "id":728106565,
  "issue":539,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODEwNjU2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T14:44:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`allow_records=True` may be all you need to fix the deprecation warning. When `allow_records=False`, such as in ufuncs, it doesn't raise an error but a warning because it's a new behavior.\r\n\r\nAre you still getting errors when you run `localbuild.py --pytest tests` with Numba? When you last posted them, they (all three of them) were not the same kind of error as was in the Windows job (which was one error with a different message). I think your errors can be fixed by installing Awkward Array with pip in addition to running `localbuild.py`.\r\n",
  "created_at":"2020-11-16T14:56:28Z",
  "id":728114264,
  "issue":539,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODExNDI2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T14:56:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> `allow_records=True` may be all you need to fix the deprecation warning. When `allow_records=False`, such as in ufuncs, it doesn't raise an error but a warning because it's a new behavior.\r\n> \r\n> Are you still getting errors when you run `localbuild.py --pytest tests` with Numba? When you last posted them, they (all three of them) were not the same kind of error as was in the Windows job (which was one error with a different message). I think your errors can be fixed by installing Awkward Array with pip in addition to running `localbuild.py`.\r\n\r\nThanks, @jpivarski ! No, I don't have any errors except for the future one I've just pushed. I cannot figure out how to pass `allow_records=True` to `apply`...",
  "created_at":"2020-11-16T16:43:37Z",
  "id":728181592,
  "issue":539,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODE4MTU5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T16:43:37Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I fixed the deprecation warning; it was because\r\n\r\n```python\r\nak.Array([1, 2, 3]) == ak.Array([1, 2, 3])\r\n```\r\n\r\nevaluates to\r\n\r\n```python\r\nak.Array([True, True, True])\r\n```\r\n\r\nnot\r\n\r\n```python\r\nTrue\r\n```\r\n\r\nand `==` is the NumPy ufunc we will not be allowing to descend through records in version 1.0.0. The deprecation warning was telling us something real.\r\n\r\nThis has another bad consequence:\r\n\r\n```python\r\nassert ak.Array([1, 2, 3]) == ak.Array([3, 2, 1])\r\n```\r\n\r\n_would not_ raise an assertion. The immediate fix was to add \"`awkward1.to_list(\u00b7)`\" around your arrays, since Python lists do evaluate to True or False with the `==` operator. This business of returning an array of True/False is (very convenient!) NumPy behavior that Awkward Array adheres to. Since assertions (or any checks) with `==` are now dangerous, NumPy raises an assertion whenever an array is used where a language feature (`if`, `while`, ...) wants to determine the scalar truth of an expression. Therefore, Awkward Array should, too, so I added that.\r\n\r\nAnd then\u2014wouldn't you know it?\u2014this uncovered a few more bugs.\r\n\r\nI put in fixes for all of these things. I think this PR is now finished, but I'll let you be the judge of that. I'd like to include it in version 0.4.4 tomorrow morning, but we can talk about that on our call.",
  "created_at":"2020-11-17T01:53:29Z",
  "id":728634330,
  "issue":539,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODYzNDMzMA==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2020-11-17T01:53:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Wow! Thanks, @jpivarski ! Yes, I think I'm done with this PR.",
  "created_at":"2020-11-17T08:40:18Z",
  "id":728776895,
  "issue":539,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODc3Njg5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T08:40:18Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a \"gotcha,\" rather than a \"bug,\" in that it's the intended behavior but may be surprising.\r\n\r\nAssignment uses broadcasting, and there's a subtlety about left-broadcasting versus right-broadcasting that is [documented here](https://awkward-array.readthedocs.io/en/latest/_auto/ak.broadcast_arrays.html) (though it could get a more prominent tutorial on [awkward-array.org](https://awkward-array.org/)).\r\n\r\nIn short, NumPy does right-broadcasting, but left-broadcasting is much more natural for general data structures. (I don't know whether NumPy had a reason to choose right-broadcasting, but the choice is more arbitrary for rectilinear data.)\r\n\r\nHere's the essence of what you want to do as an assignment, written as an addition (since NumPy can't add fields to an existing structured array for an exact analogy). You _want_\r\n\r\n```\r\n[1, 2, 3, 4, 5] +\r\n        [[10, 100], [20, 200], [30, 300], [40, 400], [50, 500]] \u2192\r\n                                                      [[11, 101], [22, 202], [33, 303], [44, 404], [55, 505]]\r\n```\r\n\r\nbut this is what NumPy says:\r\n\r\n```python\r\n>>> (\r\n...     np.array([1, 2, 3, 4, 5]) +\r\n...     np.array([[10, 100], [20, 200], [30, 300], [40, 400], [50, 500]]\r\n... )\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nValueError: operands could not be broadcast together with shapes (5,) (5,2) \r\n```\r\n\r\nFor consistency with NumPy, Awkward Array does the same thing if the list types have _regular lengths_ (i.e. if they're numbers in the type string, not \"`var`\").\r\n\r\n```python\r\n>>> (\r\n...     ak.Array(np.array([1, 2, 3, 4, 5])) +\r\n...     ak.Array(np.array([[10, 100], [20, 200], [30, 300], [40, 400], [50, 500]])\r\n... )\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n...\r\nValueError: operands could not be broadcast together with shapes (1,1,5) (1,5,2) \r\n```\r\n\r\nIf they are _irregular lengths_, Awkward Array left-broadcasts, which is what you want to do. (We're allowed to deviate from NumPy's behavior for irregular lengths because there are no irregular lengths in NumPy.)\r\n\r\n```python\r\n>>> (\r\n...     ak.Array([1, 2, 3, 4, 5]) +\r\n...     ak.Array([[10, 100], [20, 200], [30, 300], [40, 400], [50, 500]]\r\n... )\r\n<Array [[11, 101], [22, ... 404], [55, 505]] type='5 * var * int64'>\r\n```\r\n\r\nApplying this to assignment, it means that you can do this if the thing you're assigning has irregular length.\r\n\r\n```python\r\n>>> rcd_array = ak.Array([{\"x\": i} for i in range(10)])\r\n>>> x1 = ak.Array([[i*10, i*100] for i in range(10)])\r\n>>> x2 = ak.Array([[i*0.1, i*0.01] for i in range(10)])\r\n>>> rcd_array[\"x1\"] = x1\r\n>>> rcd_array[\"x2\"] = x2\r\n>>> rcd_array\r\n<Array [{x: 0, x1: [0, 0, ... x2: [0.9, 0.09]}] type='10 * {\"x\": int64, \"x1\": va...'>\r\n```\r\n\r\nIt does _not_ work if the thing you're assigning has regular length, just as with addition.\r\n\r\n```python\r\n>>> rcd_array = ak.Array([{\"x\": i} for i in range(10)])\r\n>>> x1 = ak.Array(np.array([[i*10, i*100] for i in range(10)]))\r\n>>> x2 = ak.Array(np.array([[i*0.1, i*0.01] for i in range(10)]))\r\n>>> rcd_array[\"x1\"] = x1\r\nTraceback (most recent call last):\r\n...\r\nValueError: cannot broadcast RegularArray of size 2 with RegularArray of size 10\r\n```\r\n\r\nThis actually doesn't have anything to do with virtualness.\r\n\r\n\r\n\r\n",
  "created_at":"2020-11-17T15:18:37Z",
  "id":728997409,
  "issue":541,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODk5NzQwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T15:20:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yeah I just noticed this wasn't working in 0.4.3 either. The much more complex example it's derived from was working in 0.4.3 but now no longer does either... Though the progression of versions has been non-linear recently, maybe my history is a bit corrupted.",
  "created_at":"2020-11-17T15:21:52Z",
  "id":728999605,
  "issue":541,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODk5OTYwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T15:21:52Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"If a Form wasn't specified, it might have allowed you to attach the VirtualArray, though it would have been wrong as soon as it was evaluated. Maybe that's the difference?",
  "created_at":"2020-11-17T15:24:43Z",
  "id":729001381,
  "issue":541,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTAwMTM4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T15:24:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"No, it was evaluating all the way through just fine, there was no error thrown (the CI for it in the coffea PR completed).\r\nI'll keep hacking at it.\r\n\r\nJust really odd that it completely stopped working. Maybe a type was getting coerced before that's not now? If possible.",
  "created_at":"2020-11-17T15:32:26Z",
  "id":729006353,
  "issue":541,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTAwNjM1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T15:32:26Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"As for broadcasting, turning the regular dimension into a `var` dimension is sufficient:\r\n\r\n```python\r\n>>> data = ak.Array([{\"x\": 1}, {\"x\": 2}, {\"x\": 3}])\r\n>>> y = ak.Array(np.array([[1, 2], [3, 4], [5, 6]]))\r\n>>> data[\"y\"] = y\r\nTraceback (most recent call last):\r\n...\r\nValueError: cannot broadcast RegularArray of size 2 with RegularArray of size 3\r\n>>> data[\"y\"] = ak.Array(y.tolist())\r\n>>> print(data)\r\n[{x: 1, y: [1, 2]}, {x: 2, y: [3, 4]}, {x: 3, y: [5, 6]}]\r\n```\r\n\r\nThe thing is, we need a fast alternative for `ak.Array(y.tolist())`. There should be a function that turns a RegularArray into a ListOffsetArray at a given `axis` by inserting an `np.cumsum`. Actually, there are already low-level functions for this; it's just a matter of making the high-level function and documenting it: issue #551.",
  "created_at":"2020-11-18T16:24:36Z",
  "id":729792122,
  "issue":541,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTc5MjEyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-18T16:24:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That looks like a good idea, and I don't see how it could hurt. Is this complete? (Can I merge?)",
  "created_at":"2020-11-17T13:53:06Z",
  "id":728942019,
  "issue":542,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODk0MjAxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T13:53:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'm done with it yes",
  "created_at":"2020-11-17T15:59:30Z",
  "id":729025072,
  "issue":542,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTAyNTA3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T15:59:30Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"As a note, an unexpected but easily remedied consequence of this was that sphinx crashed on trying to load the documentation for `XArray` or `XRecord`. We solved this by simply adding an `__all__ = [\"X\", ...]` declaration in our modules where the mixin decorator was used.",
  "created_at":"2020-12-01T01:57:26Z",
  "id":736165874,
  "issue":542,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNjE2NTg3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-01T01:57:26Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right: we shouldn't support single quotes. That's a Python print-out, not JSON.\r\n\r\nThis PR isn't so much about supporting @aswin1447's specific files as the general features of data like this:\r\n\r\n```\r\n{\"x\": 1.1, \"y\": []}\r\n{\"x\": 2.2, \"y\": [1]}\r\n{\"x\": 3.3, \"y\": [1, 2]}\r\n{\"x\": 4.4, \"y\": [1, 2, 3]}\r\n{\"x\": 5.5, \"y\": [1, 2, 3, 4]}\r\n{\"x\": 6.6, \"y\": [1, 2, 3, 4, 5]}\r\n```\r\n\r\nas though it were in a single list with commas in between. This would not be by literally transforming the text: see\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/05ca0f3202e3aa2886bce1b7e811ebde52289a40/src/python/io.cpp#L20-L79\r\n\r\nand\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/05ca0f3202e3aa2886bce1b7e811ebde52289a40/src/awkward1/operations/convert.py#L827-L833\r\n\r\nIt's already doing some funny business with ArrayBuilder's `beginlist` and `endlist` to support records; just a little more tweaking would be required to tell RapidJSON to keep parsing after a JSON document is done and handling the `beginlist`/`endlist` to return an array, rather than a record. It might not be necessary to rely on carriage returns (`\\n` or `\\r\\n`) to separate JSON items in a stream, though this is a very common case. (In principle, it _should_ be enough to see that the RapidJSON parser has come to the end of a valid JSON object but there's more text left; JSON provides its own delimiters.)\r\n\r\nThe other part of this is to _optionally_ read and write specified strings as floating point `nan`, `inf`, and `-inf`. The nicest interface would let users specify strings that can be as these values, with no such interpretation being the default (the option can be None or a string). The place where this would be applied is:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/master/src/libawkward/io/json.cpp\r\n\r\n(This is also the only place where we can use RapidJSON directly\u2014it's hidden from the interface/header files so that downstream users of the library won't also depend on RapidJSON. That's why some of the calling conventions are convoluted.)\r\n\r\nThis PR can also include issue #192, passing a Python file pointer down to the C++ (which also mentions this issue of taking a stream of JSON objects, by the way).",
  "created_at":"2020-11-18T15:31:19Z",
  "id":729757262,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTc1NzI2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-18T15:31:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Sure, the issues are related. I was thinking of using a generator to stream the JSON fragments:\r\n```python\r\n    def stream_read_json(fn):\r\n        start_pos = 0\r\n        with open(fn, 'r') as f:\r\n            while True:\r\n                try:\r\n                    obj = json.load(f)\r\n                    yield obj\r\n                    return\r\n                except json.JSONDecodeError as e:\r\n                    f.seek(start_pos)\r\n                    json_str = f.read(e.pos)\r\n                    obj = json.loads(json_str)\r\n                    start_pos += e.pos\r\n                    yield obj\r\n``` \r\nbut I'm not sure about performance - it's way too slow or the test file is way too big :-)",
  "created_at":"2020-11-18T15:57:38Z",
  "id":729774503,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTc3NDUwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-18T15:57:38Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> but I'm not sure about performance - it's way too slow or the test file is way too big :-)\r\n\r\nIt would be much better to tweak the rules under which `ArrayBuilder::beginlist()` and `ArrayBuilder::endlist()` are called. The difference would be two function calls (in C++) versus modifying the text stream. Generally, these data files _are_ big. (That's why we use RapidJSON's SAX parser, rather than creating a DOM\u2014it can make the difference between having or not having enough memory is realistic use-cases.)",
  "created_at":"2020-11-18T16:28:19Z",
  "id":729794420,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTc5NDQyMA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-18T16:28:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"(*) failing integration tests are due to a C++17 `string_vew`. I'll deal with it alternatively :-)\r\n\r\n@jpivarski - I run into a curious comparison error. The reported diff seems to be identical...\r\nAre the `numpy.inf` and `numpy.nan` handled as I expect? What shall I use instead? Thanks\r\n```python\r\n>       assert awkward1.to_list(array) == [1.1, 2.2, 3.3, numpy.inf, -numpy.inf,\r\n            [4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1, 11.11],\r\n            [12.12, 13.13, 14.14, 15.15, 16.16, 17.17],\r\n            [[[18.18,   19.19,   20.2,    21.21,   22.22],\r\n              [23.23,   24.24,   25.25,   26.26,   27.27,\r\n               28.28,   29.29,   30.3,    31.31,   32.32,\r\n               33.33,   34.34,   35.35,   36.36,   37.37],\r\n              [38.38],\r\n              [39.39, 40.4, numpy.nan, numpy.nan, 41.41, 42.42, 43.43]],\r\n             [[44.44,   45.45,   46.46,   47.47,   48.48],\r\n              [49.49,   50.5,    51.51,   52.52,   53.53,\r\n               54.54,   55.55,   56.56,   57.57,   58.58,\r\n               59.59,   60.6,    61.61,   62.62,   63.63],\r\n              [64.64],\r\n              [65.65, 66.66, numpy.nan, numpy.nan, 67.67, 68.68, 69.69]]]]\r\nE       assert [1.1,\\n 2.2,\\n 3.3,\\n inf,\\n -inf,\\n [4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1, 11.11],\\n [12.12, 13.13, 14.14, 15.15, 16.16, 17.17],\\n [[[18.18, 19.19, 20.2, 21.21, 22.22],\\n   [23.23,\\n    24.24,\\n    25.25,\\n    26.26,\\n    27.27,\\n    28.28,\\n    29.29,\\n    30.3,\\n    31.31,\\n    32.32,\\n    33.33,\\n    34.34,\\n    35.35,\\n    36.36,\\n    37.37],\\n   [38.38],\\n   [39.39, 40.4, nan, nan, 41.41, 42.42, 43.43]],\\n  [[44.44, 45.45, 46.46, 47.47, 48.48],\\n   [49.49,\\n    50.5,\\n    51.51,\\n    52.52,\\n    53.53,\\n    54.54,\\n    55.55,\\n    56.56,\\n    57.57,\\n    58.58,\\n    59.59,\\n    60.6,\\n    61.61,\\n    62.62,\\n    63.63],\\n   [64.64],\\n   [65.65, 66.66, nan, nan, 67.67, 68.68, 69.69]]]] == [1.1,\\n 2.2,\\n 3.3,\\n inf,\\n -inf,\\n [4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1, 11.11],\\n [12.12, 13.13, 14.14, 15.15, 16.16, 17.17],\\n [[[18.18, 19.19, 20.2, 21.21, 22.22],\\n   [23.23,\\n    24.24,\\n    25.25,\\n    26.26,\\n    27.27,\\n    28.28,\\n    29.29,\\n    30.3,\\n    31.31,\\n    32.32,\\n    33.33,\\n    34.34,\\n    35.35,\\n    36.36,\\n    37.37],\\n   [38.38],\\n   [39.39, 40.4, nan, nan, 41.41, 42.42, 43.43]],\\n  [[44.44, 45.45, 46.46, 47.47, 48.48],\\n   [49.49,\\n    50.5,\\n    51.51,\\n    52.52,\\n    53.53,\\n    54.54,\\n    55.55,\\n    56.56,\\n    57.57,\\n    58.58,\\n    59.59,\\n    60.6,\\n    61.61,\\n    62.62,\\n    63.63],\\n   [64.64],\\n   [65.65, 66.66, nan, nan, 67.67, 68.68, 69.69]]]]\r\nE         At index 7 diff: [[[18.18, 19.19, 20.2, 21.21, 22.22], [23.23, 24.24, 25.25, 26.26, 27.27, 28.28, 29.29, 30.3, 31.31, 32.32, 33.33, 34.34, 35.35, 36.36, 37.37], [38.38], [39.39, 40.4, nan, nan, 41.41, 42.42, 43.43]], [[44.44, 45.45, 46.46, 47.47, 48.48], [49.49, 50.5, 51.51, 52.52, 53.53, 54.54, 55.55, 56.56, 57.57, 58.58, 59.59, 60.6, 61.61, 62.62, 63.63], [64.64], [65.65, 66.66, nan, nan, 67.67, 68.68, 69.69]]] != [[[18.18, 19.19, 20.2, 21.21, 22.22], [23.23, 24.24, 25.25, 26.26, 27.27, 28.28, 29.29, 30.3, 31.31, 32.32, 33.33, 34.34, 35.35, 36.36, 37.37], [38.38], [39.39, 40.4, nan, nan, 41.41, 42.42, 43.43]], [[44.44, 45.45, 46.46, 47.47, 48.48], [49.49, 50.5, 51.51, 52.52, 53.53, 54.54, 55.55, 56.56, 57.57, 58.58, 59.59, 60.6, 61.61, 62.62, 63.63], [64.64], [65.65, 66.66, nan, nan, 67.67, 68.68, 69.69]]]\r\nE         Full diff:\r\nE           [\r\nE            1.1,\r\nE            2.2,\r\nE            3.3,\r\nE            inf,\r\nE            -inf,\r\nE            [4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1, 11.11],\r\nE            [12.12, 13.13, 14.14, 15.15, 16.16, 17.17],\r\nE            [[[18.18, 19.19, 20.2, 21.21, 22.22],\r\nE              [23.23,\r\nE               24.24,\r\nE               25.25,\r\nE               26.26,\r\nE               27.27,\r\nE               28.28,\r\nE               29.29,\r\nE               30.3,\r\nE               31.31,\r\nE               32.32,\r\nE               33.33,\r\nE               34.34,\r\nE               35.35,\r\nE               36.36,\r\nE               37.37],\r\nE              [38.38],\r\nE              [39.39, 40.4, nan, nan, 41.41, 42.42, 43.43]],\r\nE             [[44.44, 45.45, 46.46, 47.47, 48.48],\r\nE              [49.49,\r\nE               50.5,\r\nE               51.51,\r\nE               52.52,\r\nE               53.53,\r\nE               54.54,\r\nE               55.55,\r\nE               56.56,\r\nE               57.57,\r\nE               58.58,\r\nE               59.59,\r\nE               60.6,\r\nE               61.61,\r\nE               62.62,\r\nE               63.63],\r\nE              [64.64],\r\nE              [65.65, 66.66, nan, nan, 67.67, 68.68, 69.69]]],\r\nE           ]\r\n\r\ntests/test_0437-stream-of-many-json-files.py:26: AssertionError\r\n\r\n```",
  "created_at":"2020-11-19T12:41:05Z",
  "id":730350199,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMDM1MDE5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-19T12:41:05Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Are the `numpy.inf` and `numpy.nan` handled as I expect? What shall I use instead?\r\n\r\nMaybe it's because NaN is not equal to NaN (according to IEEE rules):\r\n\r\n```python\r\n>>> float(\"nan\") == float(\"nan\")\r\nFalse\r\n>>> np.nan == np.nan\r\nFalse\r\n```\r\n\r\nA [suggestion on StackOverflow](https://stackoverflow.com/questions/39166292/comparing-lists-containing-nans) is to replace the NaN values with strings, which can be done after the conversion to Python lists.\r\n\r\n```python\r\n>>> data1 = [[44.44,   45.45,   46.46,   47.47,   48.48],\r\n...          [49.49,   50.5,    51.51,   52.52,   53.53,\r\n...           54.54,   55.55,   56.56,   57.57,   58.58,\r\n...           59.59,   60.6,    61.61,   62.62,   63.63],\r\n...          [64.64],\r\n...          [65.65, 66.66, float(\"nan\"), float(\"nan\"), 67.67, 68.68, 69.69]]\r\n>>> data2 = [[44.44,   45.45,   46.46,   47.47,   48.48],\r\n...          [49.49,   50.5,    51.51,   52.52,   53.53,\r\n...           54.54,   55.55,   56.56,   57.57,   58.58,\r\n...           59.59,   60.6,    61.61,   62.62,   63.63],\r\n...          [64.64],\r\n...          [65.65, 66.66, float(\"nan\"), float(\"nan\"), 67.67, 68.68, 69.69]]\r\n>>> data1 == data2\r\nFalse\r\n>>> \r\n>>> def fix(x):\r\n...     if isinstance(x, dict):\r\n...         return dict((k, fix(v)) for k, v in x.items())\r\n...     elif isinstance(x, list):\r\n...         return [fix(v) for v in x]\r\n...     elif isinstance(x, tuple):\r\n...         return tuple(fix(v) for v in x)\r\n...     elif np.isnan(x):\r\n...         return \"nan\"\r\n...     else:\r\n...         return x\r\n... \r\n>>> fix(data1) == fix(data2)\r\nTrue\r\n```\r\n\r\nThere's no such problem with infinity:\r\n\r\n```python\r\n>>> float(\"inf\") == float(\"inf\")\r\nTrue\r\n>>> float(\"inf\") == float(\"-inf\")\r\nFalse\r\n>>> float(\"-inf\") == float(\"-inf\")\r\nTrue\r\n>>> np.inf == np.inf\r\nTrue\r\n>>> np.inf == -np.inf\r\nFalse\r\n>>> -np.inf == -np.inf\r\nTrue\r\n```\r\n\r\nIn the IEEE floating point definition, only one bit pattern corresponds to infinity, another bit pattern corresponds to minus infinity, but there's a large space of bit patterns that all correspond to not-a-number. Here are two of them:\r\n\r\n```python\r\n>>> np.array([0, 0, 0, 0, 0, 0, 248, 127], dtype=np.uint8).view(np.float64)\r\narray([nan])\r\n>>> np.array([0, 0, 0, 0, 0, 0, 249, 127], dtype=np.uint8).view(np.float64)\r\narray([nan])\r\n```\r\n\r\nRather than requiring implementations of IEEE floating point arithmetic to complicate equality checks by checking to see if both sides are in or not in the \"not-a-number\" class, they're just defined as being not equal to each other. (After all, most implementations these days are in hardware, and you want to keep that as simple as possible.)",
  "created_at":"2020-11-19T16:34:14Z",
  "id":730492293,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMDQ5MjI5Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-19T16:34:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":" \r\n\r\n> Oh, I'd rather these options not be a part of ArrayBuilderOptions, since they only apply to JSON (as they work around a JSON limitation). ArrayBuilder is used in a lot of contexts other than JSON. If there's some reason why it would be very difficult to do this, then we can consider this an internal detail, since it's not exposed to the Python interface to ArrayBuilder, but it's looking to me like a bad direction to grow.\r\n> \r\n> The reason we have ArrayBuilderOptions is because ArrayBuilder makes a tree of indeterminate depth. At any moment, it could encounter new data that broadens the type of its array, so each node needs to have access to the configuration options, which includes such things as \"how big should the initial allocation be?\"\r\n> \r\n> These JSON options don't need to be pushed down this way. The JSON-handling code is all in a SAX parser class that doesn't create nodes of a tree (it calls methods on ArrayBuilder, which internally builds those nodes). So the JSON options don't have to be \"pushed down\" in the same way. That's why they don't really belong in ArrayBuilderOptions.\r\n> \r\n\r\ndone. It's the `*Options` that confused me :-)\r\n\r\n> Also, about the options themselves: users need to be able to specify the spelling of `\"nan\"`, `\"inf\"`, and `\"-inf\"`, since there's no standard\u2014different JSON writers generate different strings: `\"NaN\"`, `\"Inf\"`, `\"-Inf\"`, `\"Infinity\"`, `\"-Infinity\"`, and `\"minusInfinity\"` are all things I've seen. A single dataset won't have multiple spellings, so these are three new options:\r\n> \r\n> * the string that gets mapped to `std::numeric_limits<double>::quiet_NaN()`\r\n> * the string that gets mapped to `std::numeric_limits<double>::infinity()`\r\n> * the string that gets mapped to `-std::numeric_limits<double>::infinity()`\r\n> \r\n\r\nreimplemented. There are a few parameters with default definitions that can be overwritten by a user. It's a long-ish list.  Please, comment.\r\n```python\r\nconvertNanAndInf=False # to float\r\nreplaceNanAndInf=False # with another string\r\nfromNan='NaN'\r\nfromInf='inf'\r\nfromMinusInf='-inf'\r\ntoNan='NaN'\r\ntoInf='inf'\r\ntoMinusInf='-inf'\r\n```\r\nfor example:\r\n```python\r\n    array = awkward1.from_json('tests/samples/test-nan-inf.json', convertNanAndInf=True,\r\n        fromInf='Infinity', fromNan='None')\r\n```\r\n\r\n> any of which can be None (i.e. _no_ string that gets mapped to a numeric limit).\r\n\r\nlooking into it :-) Thanks!",
  "created_at":"2020-11-20T18:10:30Z",
  "id":731329199,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTMyOTE5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T18:10:30Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The `to_json` and `from_json` functions are separate and can get separate sets of options; no one needs all 6, they each need only 3. They can take the same names, too:\r\n\r\n```python\r\ndef to_json(\r\n    array,\r\n    destination=None,\r\n    pretty=False,\r\n    maxdecimals=None,\r\n    nan_string=None,\r\n    infinity_string=None,\r\n    minus_infinity_string=None,\r\n    buffersize=65536,\r\n):\r\n    ...\r\n\r\ndef from_json(\r\n    source,\r\n    nan_string=None,\r\n    infinity_string=None,\r\n    minus_infinity_string=None,\r\n    highlevel=True,\r\n    behavior=None,\r\n    initial=1024,\r\n    resize=1.5,\r\n    buffersize=65536,\r\n):\r\n    ...\r\n```\r\n\r\nGiving them the same names highlights the symmetry. I've grouped them here with other JSON parameters (`pretty` and `maxdecimals` in `to_json`, before `highlevel` and `behavior`, which are usually last, but ArrayBuilder parameters, `initial`, `resize`, and `buffersize` are usually of even less interest).\r\n\r\nThere needs to be a distinction between \"no conversion string\" and \"the conversion string that is '`None`'\", since the latter is a possible encoding of NaN.",
  "created_at":"2020-11-20T18:24:34Z",
  "id":731335632,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTMzNTYzMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-20T18:24:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks, @jpivarski ! I think, I'm done with this PR.",
  "created_at":"2020-11-24T08:41:54Z",
  "id":732745575,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMjc0NTU3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-24T08:41:54Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - the tests pass. There is a workaround for a string that is not a file name.\r\n\r\nThe following change would help to eliminate the need to distinguish between a file name and a string. Would that be satisfactory?\r\n```diff\r\ndiff --git a/src/awkward1/operations/convert.py b/src/awkward1/operations/convert.py\r\nindex 6ffb080..11c135b 100644\r\n--- a/src/awkward1/operations/convert.py\r\n+++ b/src/awkward1/operations/convert.py\r\n@@ -6,6 +6,8 @@ import numbers\r\n import json\r\n import collections\r\n import math\r\n+import io\r\n+import os\r\n import threading\r\n import distutils.version\r\n \r\n@@ -842,7 +844,7 @@ def from_json(source,\r\n     See also #ak.to_json.\r\n     \"\"\"\r\n     layout = awkward1._ext.fromjson(\r\n-        source,\r\n+        source if not os.path.isfile(source) else open(source, \"r\").read(),\r\n         nan_string=nan_string,\r\n         infinity_string=infinity_string,\r\n         minus_infinity_string=minus_infinity_string,\r\n```\r\nthen there would be only need for `ak::FromJsonString` and `FromJsonFile` could be deleted.\r\nPlease, let me know if that's the direction you'd like me to take and if you'd like this be part of the PR. I've got it locally and all tests pass. Thanks!",
  "created_at":"2020-11-25T13:25:34Z",
  "id":733704773,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzcwNDc3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-25T13:25:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Currently, have you put in any changes to let `to_json` and `from_json` accept a Python file object? I didn't see any such change in the GitHub diff, just the diff you quoted. (I'm only asking because I want to know if there's some code I'm supposed to look at.)\r\n\r\nAs for this proposed solution: it reads the whole JSON file as a string before doing any conversion. If a user is passing a filename or Python file object to the function, they're not expecting that, and very large files would then be a problem. (Note: a filled Awkward Array uses much less memory than the text of a JSON file.)\r\n\r\nWe do need `FromJsonFile` to do this incremental parsing. That's why we're using RapidJSON's SAX parser. If given a Python file object, you can get its file-handle as an integer:\r\n\r\n```python\r\n>>> file = open(\"setup.py\", \"r\")\r\n>>> file.fileno()\r\n3\r\n```\r\n\r\n(Don't check `isinstance` to see if it's a plain file\u2014a lot of interfaces in Python have this `fileno()` method so they can be used interchangeably. Do `hasattr(file, \"fileno\")` to check for it. This also doesn't cover a common Python case of file-like objects that don't have a `fileno` but do have a `read` method. We can put that off for later, as it will require a `FromJsonStringIncremental`, which simply hasn't been written yet.)\r\n\r\nThe `source` of `from_json` has a weakness that if a string is both a filename and valid JSON, it's ambiguous how to parse it. I think we pick \"filename\" in these cases. Having a filename that is valid JSON is weird.",
  "created_at":"2020-11-25T17:35:17Z",
  "id":733850133,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzg1MDEzMw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-25T17:35:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Currently, have you put in any changes to let `to_json` and `from_json` accept a Python file object? I didn't see any such change in the GitHub diff, just the diff you quoted. (I'm only asking because I want to know if there's some code I'm supposed to look at.)\r\n> \r\nNo, I did not push any changes - all this is in my private area :-)\r\n\r\n> As for this proposed solution: it reads the whole JSON file as a string before doing any conversion. If a user is passing a filename or Python file object to the function, they're not expecting that, and very large files would then be a problem. (Note: a filled Awkward Array uses much less memory than the text of a JSON file.)\r\n> \r\nYes, you are right.\r\n\r\n> We do need `FromJsonFile` to do this incremental parsing. That's why we're using RapidJSON's SAX parser. If given a Python file object, you can get its file-handle as an integer:\r\n> \r\n> ```python\r\n> >>> file = open(\"setup.py\", \"r\")\r\n> >>> file.fileno()\r\n> 3\r\n> ```\r\n> \r\n> (Don't check `isinstance` to see if it's a plain file\u2014a lot of interfaces in Python have this `fileno()` method so they can be used interchangeably. Do `hasattr(file, \"fileno\")` to check for it. This also doesn't cover a common Python case of file-like objects that don't have a `fileno` but do have a `read` method. We can put that off for later, as it will require a `FromJsonStringIncremental`, which simply hasn't been written yet.)\r\n> \r\n\r\nOk, thanks.\r\n\r\n> The `source` of `from_json` has a weakness that if a string is both a filename and valid JSON, it's ambiguous how to parse it. I think we pick \"filename\" in these cases. Having a filename that is valid JSON is weird.\r\n\r\nPlease, let me know if I should continue working on this PR or is it ok asis to be merged? Thanks!",
  "created_at":"2020-11-25T18:25:44Z",
  "id":733877171,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzg3NzE3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-25T18:25:44Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I just put in one correction, hiding `nan_and_inf_as_float` from users. (That's an optimization that should be implicit from the `nan_string`, `infinity_string`, or `minus_infinity_string` being None.)\r\n\r\nI also noticed this:\r\n\r\n```python\r\n>>> ak.from_json('[\"one\", \\n\"two\"] \\n [\"three\"]')\r\n<Array ['one', 'two', 'three'] type='3 * string'>\r\n```\r\n\r\nThe output should be\r\n\r\n```\r\n[['one', 'two'], ['three']]\r\n```\r\n\r\nbecause each separate JSON document should be separate. Sorry that I wasn't clear when listing test cases.\r\n\r\nMeanwhile,\r\n\r\n```python\r\n>>> ak.from_json('{\"x\": 1, \"y\": 2} \\n {\"x\": 10, \"y\": 20}')\r\n<Array [[{x: 1, y: 2}], [{x: 10, y: 20}]] type='2 * var * {\"x\": int64, \"y\": int64}'>\r\n```\r\n\r\nis too deep: it should be\r\n\r\n```\r\n[{'x': 1, 'y': 2}, {'x': 10, 'y': 20}]\r\n```\r\n\r\nI'll be in a meeting now, so I'm not working on that now. I'll check in again tomorrow. Thanks!\r\n\r\nAs for passing in the `FILE` handle with `fileno`, that doesn't need to be in this PR.",
  "created_at":"2020-11-25T19:15:07Z",
  "id":733901392,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzkwMTM5Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-25T19:15:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I just put in one correction, hiding `nan_and_inf_as_float` from users. (That's an optimization that should be implicit from the `nan_string`, `infinity_string`, or `minus_infinity_string` being None.)\r\n> \r\n> I also noticed this:\r\n> \r\n> ```python\r\n> >>> ak.from_json('[\"one\", \\n\"two\"] \\n [\"three\"]')\r\n> <Array ['one', 'two', 'three'] type='3 * string'>\r\n> ```\r\n> \r\n> The output should be\r\n> \r\n> ```\r\n> [['one', 'two'], ['three']]\r\n> ```\r\n> \r\n> because each separate JSON document should be separate. Sorry that I wasn't clear when listing test cases.\r\n>\r\n\r\ndone.\r\n \r\n> Meanwhile,\r\n> \r\n> ```python\r\n> >>> ak.from_json('{\"x\": 1, \"y\": 2} \\n {\"x\": 10, \"y\": 20}')\r\n> <Array [[{x: 1, y: 2}], [{x: 10, y: 20}]] type='2 * var * {\"x\": int64, \"y\": int64}'>\r\n> ```\r\n> \r\n> is too deep: it should be\r\n> \r\n> ```\r\n> [{'x': 1, 'y': 2}, {'x': 10, 'y': 20}]\r\n> ```\r\n>\r\n\r\ndone.\r\n \r\n> I'll be in a meeting now, so I'm not working on that now. I'll check in again tomorrow. Thanks!\r\n> \r\n> As for passing in the `FILE` handle with `fileno`, that doesn't need to be in this PR.\r\n\r\nThe last thing on my list is parsing a string that starts with a number:\r\n```python\r\n        str = \"\"\" \"1\r\n        2\r\n    \r\n        3 \" \"\"\"\r\n        array = awkward1.from_json(str)\r\n>       assert awkward1.to_list(array) == [1, 2, 3]\r\nE       assert [2, 3] == [1, 2, 3]\r\nE         At index 0 diff: 2 != 1\r\nE         Right contains one more item: 3\r\nE         Full diff:\r\nE         - [2, 3]\r\nE         + [1, 2, 3]\r\nE         ?  +++\r\n\r\n```",
  "created_at":"2020-11-27T17:43:55Z",
  "id":734935060,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNDkzNTA2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-27T17:43:55Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, I'll close it and make a new PR after the name migration.",
  "created_at":"2020-11-30T12:29:56Z",
  "id":735757654,
  "issue":543,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNTc1NzY1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-30T12:29:56Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ha it didn't take long :-)",
  "created_at":"2020-11-17T20:43:39Z",
  "id":729202213,
  "issue":545,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTIwMjIxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T20:43:39Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Really, these are things I should have checked when reviewing your code, but I didn't think of them. They came up in use-cases (questions on Slack).",
  "created_at":"2020-11-17T20:47:34Z",
  "id":729204665,
  "issue":545,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTIwNDY2NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-17T20:47:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - `concatenate` currently expects a `Content` subtype as an argument (in any axis). Shall it be extended to accept other types?\r\n\r\nfor example, this fails with a `ValueError: content argument must be a Content subtype`\r\n```python\r\n>>> ak.concatenate([ak.Array([[1, 2, 3], [], [4, 5]]), 999], axis=0)\r\n```",
  "created_at":"2020-11-17T21:07:50Z",
  "id":729214539,
  "issue":545,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTIxNDUzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T21:07:50Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Just before entering the C++, `999` should be promoted to a NumpyArray:\r\n\r\n```python\r\n>>> length = 10\r\n>>> awkward1.layout.NumpyArray(numpy.repeat(999, length))\r\n<NumpyArray format=\"l\" shape=\"10\" data=\"999 999 999 999 999 999 999 999 999 999\" at=\"0x5598403ae950\"/>\r\n```\r\n\r\nAt that point in the process, you'll know what `length` is supposed to be.",
  "created_at":"2020-11-17T21:13:19Z",
  "id":729217132,
  "issue":545,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTIxNzEzMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-17T21:13:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"should this operation be commutative?\r\n```python\r\nak.concatenate([ak.Array([[1, 2, 3], [], [4, 5]]), 999], axis=1) == ak.concatenate([999, ak.Array([[1, 2, 3], [], [4, 5]])], axis=1)\r\n```",
  "created_at":"2020-11-17T21:25:48Z",
  "id":729223593,
  "issue":545,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTIyMzU5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T21:25:48Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"No, because in the first, the `999` goes at the end of each list, and in the second, the `999` goes at the beginning.\r\n\r\nBut this is already the case:\r\n\r\n```python\r\n>>> ak.concatenate([ak.Array([[1, 2, 3], [], [4, 5]]), ak.Array([[999], [999], [999]])], axis=1)\r\n<Array [[1, 2, 3, 999], [999], [4, 5, 999]] type='3 * var * union[int64, int64]'>\r\n\r\n>>> ak.concatenate([ak.Array([[999], [999], [999]]), ak.Array([[1, 2, 3], [], [4, 5]])], axis=1)\r\n<Array [[999, 1, 2, 3], [999], [999, 4, 5]] type='3 * var * union[int64, int64]'>\r\n```",
  "created_at":"2020-11-17T21:38:26Z",
  "id":729229488,
  "issue":545,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTIyOTQ4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T21:38:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Should it be allowed to concatenate in different axis? The second array has only `axis=0`\r\n```python\r\n>>> ak.concatenate([ak.Array([[1, 2, 3], [], [4, 5]]), ak.Array([123, 223, 323])], axis=1)\r\n```\r\n",
  "created_at":"2020-11-17T22:55:10Z",
  "id":729263227,
  "issue":545,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTI2MzIyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T22:55:10Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"`broadcast_and_apply` adds a dimension as it descends so that when you get to your `getfunction`, they have the same dimension.",
  "created_at":"2020-11-17T22:56:51Z",
  "id":729263905,
  "issue":545,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTI2MzkwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-17T22:56:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Fixed by #548.",
  "created_at":"2020-11-18T15:33:12Z",
  "id":729758542,
  "issue":545,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTc1ODU0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-18T15:33:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't realize they were failing; thanks! I asked about it here: https://github.com/conda-forge/awkward1-feedstock/pull/10#issuecomment-729300402",
  "created_at":"2020-11-18T00:38:24Z",
  "id":729300657,
  "issue":549,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTMwMDY1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-18T00:38:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"That's good to hear.\r\nCan you estimate when we can expect the next Conda-Froge release?",
  "created_at":"2020-11-20T14:38:13Z",
  "id":731207661,
  "issue":549,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTIwNzY2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T14:38:13Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"As described here, https://github.com/conda-forge/awkward1-feedstock/pull/10#issuecomment-731221098, this is a strange time. By around Dec 1, Uproot 4 and Awkward 1 will be pushed to packages named `uproot` and `awkward`, first as release candidates, then as real releases.",
  "created_at":"2020-11-20T15:04:16Z",
  "id":731222310,
  "issue":549,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTIyMjMxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T15:04:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"The latest version of awkward1 has been uploaded to Conda-Forge.\r\nThanks for your work!",
  "created_at":"2020-11-25T10:35:00Z",
  "id":733621782,
  "issue":549,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzYyMTc4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-25T10:35:00Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"I labeled this \"performance\" because it's equivalent to sort-and-slice.",
  "created_at":"2020-12-15T13:44:34Z",
  "id":745294973,
  "issue":554,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTI5NDk3Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-12-15T13:44:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Great, thanks! I don't see any issues, and I'll want to keep the PRs clear during the \"`awkward1`\" \u2192 \"`awkward`\" name change:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/wiki/Name-change-procedure",
  "created_at":"2020-11-23T21:59:37Z",
  "id":732450580,
  "issue":556,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMjQ1MDU4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-23T21:59:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We haven't implemented all of NumPy's functions or all of NumPy's function arguments, and this is one of them. (Actually, it was added in NumPy 1.15, so it's relatively new.)\r\n\r\nMy first thought was that implementing this would be rather involved\u2014passing down a new argument through every `Content::reduce_next` (which is used for _all_ reducers, including sum, count, argmin, etc.) and that everything but min/max would have to pass down a dummy value. But as it turns out, we're luckier than that: the `Content::reducer_next` methods already pass down a `Reducer` object, which is used to say whether this reduction is \"sum,\" \"count,\" \"argmin,\" etc., since this information is only needed at the end of a recursive descent. Since this is an object, we could extend the ReducerMin and ReducerMax objects to take an optional `initial` argument, and the object would carry that down to the kernel function where it's actually used. Then when \"min\" or \"max\" is called in Python,\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/af774dee5efa3ced4c478e2172f4cb71057ace33/src/python/content.cpp#L1341-L1348\r\n\r\npassing an optional `initial` argument would set that argument on the ReducerMin/ReducerMax and then set `mask` to `false`.\r\n\r\nThe `mask` argument (only accessible in the low-level layout nodes, not the high-level `ak.min` or `np.min`/`np.amin`) is what directs min/max to cover `initial` values with None:\r\n\r\n```python\r\n>>> np.amin(ak.Array([[3.3, 2.2, 1.1], [], [4.4, 5.5]]), axis=1)\r\n<Array [1.1, None, 4.4] type='3 * ?float64'>\r\n>>> ak.Array(ak.Array([[3.3, 2.2, 1.1], [], [4.4, 5.5]]).layout.min(axis=1, mask=False))\r\n<Array [1.1, inf, 4.4] type='3 * float64'>\r\n```\r\n\r\nTo get what you want now (before `initial` is implemented), you can do a `fill_none` to replace those None values with the `initial` that you want:\r\n\r\n```python\r\n>>> ak.fill_none(np.amin(ak.Array([[3.3, 2.2, 1.1], [], [4.4, 5.5]]), axis=1), 0)\r\n<Array [1.1, 0, 4.4] type='3 * float64'>\r\n```\r\n\r\nbut that's not quite the same thing because `initial` will replace any values larger than it (for `np.amin`), not just any empty lists:\r\n\r\n```python\r\n>>> np.amin([[3.3, 2.2, 1.1], [4.4, 5.5, 6.6]], axis=1, initial=3)\r\narray([1.1, 3. ])\r\n```\r\n\r\nSo actually implementing this `initial` argument does something that `fill_none` doesn't do. (...Unless you first cut with that value:\r\n\r\n```python\r\n>>> array = ak.Array([[3.3, 2.2, 1.1], [], [4.4, 5.5]])\r\n>>> array[array < 3]\r\n<Array [[2.2, 1.1], [], []] type='3 * var * float64'>\r\n>>> np.amin(array[array < 3], axis=1)\r\n<Array [1.1, None, None] type='3 * ?float64'>\r\n>>> ak.fill_none(np.amin(array[array < 3], axis=1), 3)\r\n<Array [1.1, 3, 3] type='3 * float64'>\r\n```\r\n\r\nwhich uses the `initial` value in two places: in the cut and in the `fill_none`, to get the same effect.)",
  "created_at":"2020-11-23T14:46:44Z",
  "id":732207661,
  "issue":557,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMjIwNzY2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-23T14:46:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"```python\r\n    data = ak.Array([[1, 3, 5, 4, 2], [], [2, 3, 1], [5]])\r\n    assert ak.min(data, axis=1, initial=4).tolist() == [1, None, 1, 4]\r\n    assert ak.max(data, axis=1, initial=4).tolist() == [5, None, 4, 5]\r\n\r\n    data = ak.Array([[1.1, 3.3, 5.5, 4.4, 2.2], [], [2.2, 3.3, 1.1], [5.5]])\r\n    assert ak.min(data, axis=1, initial=4).tolist() == [1.1, None, 1.1, 4]\r\n    assert ak.max(data, axis=1, initial=4).tolist() == [5.5, None, 4, 5.5]\r\n```\r\n\r\nin PR #566.",
  "created_at":"2020-12-01T20:41:06Z",
  "id":736806990,
  "issue":557,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNjgwNjk5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-01T20:41:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Awesome! Thank you!",
  "created_at":"2020-12-03T08:50:00Z",
  "id":737757831,
  "issue":557,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNzc1NzgzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-03T08:50:00Z",
  "user":"MDQ6VXNlcjEyMTMyNzY="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! The way we've kept track of these caches has changed a few times to avoid circular references and lost references\u2014this fix was a lost reference because we remembered to add them in `__init__` but not `__setitem__`.",
  "created_at":"2020-11-25T18:31:22Z",
  "id":733880324,
  "issue":560,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzg4MDMyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-25T18:31:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I couldn't reproduce this on my APT-based system, and I don't have any familiarity with RPM-based systems.\r\n\r\nThings might be different now that the Python package name is \"awkward\", rather than \"awkward1\" (as of [1.0.0rc1](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.0.0rc1)).\r\n\r\nThe CMakeExtension is our own class, a subclass of `setuptools.Extension`, and the name given here is used to help it find its way around the directories\u2014which are different on Windows vs everything else. If `bdist_rpm` needs this name to be one thing and we need the name to be another thing for directory navigation, it would be possible to modify setup.py to do so. But someone with a lot more knowledge about RPMs would have to describe the constraints.",
  "created_at":"2020-12-01T19:07:52Z",
  "id":736757534,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNjc1NzUzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-01T19:07:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim,\n\nI'm thinking that this has nothing to do with RPM in particular, but\nthat python setup.py install --record lists the wrong files for some\nreason. You can test this by e.g.\n\nmkdir testprefix\npython setup.py install --prefix=`pwd`/testprefix --record=recorded_files.txt\n\nand then inspecting recorded_files.txt, which doesn't list all of them,\ndespite the fact that the correct files are actually installed to the\nprefix. It looks like there's only a custom handler for copying include\nfiles, which are not the issue.  I checked that this happens now too\nafter the package was renamed.\n\nThis could be a bug in setuptools, although the argument passed to\nCMakeExtension seems to affect what is recorded, so I think it probably\nneeds slightly more information somehow. A very cursory look into\nsetuptools/disttools suggests that each \"step\" needs to report its\noutputs somehow, but I have to look more into it to figure out what's\ngoing on. \n\n\nThanks,\n-Cosmin\n\nOn Tue, 2020-12-01 at 11:08 -0800, Jim Pivarski wrote:\n> I couldn't reproduce this on my APT-based system, and I don't have\n> any familiarity with RPM-based systems.\n> \n> Things might be different now that the Python package name is\n> \"awkward\", rather than \"awkward1\" (as of 1.0.0rc1).\n> \n> The CMakeExtension is our own class, a subclass of\n> setuptools.Extension, and the name given here is used to help it find\n> its way around the directories\u2014which are different on Windows vs\n> everything else. If bdist_rpm needs this name to be one thing and we\n> need the name to be another thing for directory navigation, it would\n> be possible to modify setup.py to do so. But someone with a lot more\n> knowledge about RPMs would have to describe the constraints.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n\n",
  "created_at":"2020-12-02T16:29:22Z",
  "id":737343096,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNzM0MzA5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-02T16:29:22Z",
  "user":"MDQ6VXNlcjkyMDY1Njk="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't know about the `--record` argument, but I can try to make that log correctly list all the files that would be included in the package.",
  "created_at":"2020-12-02T16:32:06Z",
  "id":737345599,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNzM0NTU5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-02T16:32:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yeah, it's what various pieces (including RPM building and I assume\nuninstalling) rely on.\n\nOn Wed, 2020-12-02 at 08:32 -0800, Jim Pivarski wrote:\n> I didn't know about the --record argument, but I can try to make that\n> log correctly list all the files that would be included in the\n> package.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n\n",
  "created_at":"2020-12-02T16:34:58Z",
  "id":737347350,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNzM0NzM1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-02T16:34:58Z",
  "user":"MDQ6VXNlcjkyMDY1Njk="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #562 fixes the `setup.py --record` argument so that its list of files matches the files that are actually installed. This involved adding includes, shared and static libraries, and correcting the name of the extension module. As far as I can tell, only RPM distribution uses this `--record` parameter (comments in setuptools explicitly reference the RPM case, which is suggestive, and there's very little documentation on it).\r\n\r\nPR #562 will close this issue, but that doesn't necessarily mean that `bdist_rpm` will work, only that `--record` will be correct. Depending on the nature of the remaining issues (if any), I might not be able to help because I don't actually know about/can't run RPM-building.",
  "created_at":"2020-12-07T20:29:55Z",
  "id":740162301,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDE2MjMwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T20:29:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim,\n\nThanks, this helps a lot!\n\nI can now generate a usable rpm, with the small caveat that out of the\nbox, debuginfo generation doesn't work. This is probably not a big deal\nto most people, other than that it might lead to a build error with\nsome versions of rpm (that can be worked around easily). \u00a0\n\nThe problem of course arises because the default build type is Release.\nIf I just change the CMake build type to RelWithDebInfo (by modifying\nsetup.py), building debuginfo works fine as expected. Debug would\nprobably also work, and I think has a command-line flag, but I imagine\nthat would be slower. \n\nIn case anybody other than me is interested, here are some basic\ninstructions:\n\n#If you don't care about debuginfo, and you are using a recent enough\u00a0\n# version of rpm that it complains about no debuginfo, this is the\u00a0\n# simplest workaroud. You can reenable afterwards if you care.\n# Necessary on Fedora 33, probably not on Fedora 32 or EL 7/8. \n\n   echo '%debug_package %{nil}' >> ~/.rpmmacros\n\n# Otherwise, you can just change Release to RelWithDebInfo on line 62 of setup.py\n\n# Then, build the RPM:\n   python setup.py bdist_rpm --requires='python3-numpy >= 1.13' \n\n# and install it. This glob also installs debuginfo if you enabled it\n   sudo dnf localinstall dist/awkward*.`arch`.rpm  # or yum if EL 7/8\n\n\nThanks,\n-Cosmin\n\n \nOn Mon, 2020-12-07 at 14:15 -0800, Jim Pivarski wrote:\n> Closed #562 via #577.\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n\n\n",
  "created_at":"2020-12-07T23:46:32Z",
  "id":740250637,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDI1MDYzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T23:46:32Z",
  "user":"MDQ6VXNlcjkyMDY1Njk="
 },
 {
  "author_association":"MEMBER",
  "body":"The CMake type shouldn't be Debug, but I don't know the consequences of Release versus RelWithDebInfo.\r\n\r\nAccording to https://stackoverflow.com/a/48755129/1623645, RelWithDebInfo sounds like a good idea. Even if we're not making RPMs, having human-readable debugging symbols around is likely to be useful. It doesn't slow down the runtime execution.\r\n\r\nI'm willing to accept the changes you've described above as a pull request (adding the `.rpmmacros` file, for instance).",
  "created_at":"2020-12-08T00:07:38Z",
  "id":740257929,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDI1NzkyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T00:07:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim,\n\nThe ~/.rpmmacros is per-user (NOT .rpmmacros), so it does not make\nsense to package that (silly, I know...). At least I don't think RPM\nsupports reading from the cwd to do that.\n\nDefaulting or adding the option to build RelWithDebInfo would be a\nbetter solution I think. This (to my understanding) is the equivalent\nof just adding -g to the Release build, so that the debugging symbols\nget generated (although, since it's optimized, sometimes the debug\nsymbols are confusing because the compiler has rearranged or inlined\nsome things). Checking the compiler command line with make VERBOSE=1\nconfirms that that's true in this case at least (in particular, \n-DNDEBUG is present in both Release and RelWithDebInfo, so things like\nasserts etc. are the same)\n\nRelWithDebInfo's only disadvantage (if you're not trying to hide debug\nsymbols like proprietary vendors do) is that the generated files are\nbigger, but it makes debugging/bug reports/profiling a lot easier.  \nSome packaging systems (.deb and .rpm) support separating the debuginfo\ninto a separate package for size reasons, but I don't know if pip or\nconda can do that. Hopefully they at least compress the binary files\nfor distribution. \n\nDoing the cmake build in Release vs. RelWithDebInfo options (gcc\n10.2.1):\n\nRelease:\n\nls -alh libawkward*\n-rwxrwxr-x. 1 cozzy cozzy 643K Dec  7 18:53 libawkward-cpu-kernels.so\n-rw-rw-r--. 1 cozzy cozzy 1.4M Dec  7 18:53 libawkward-cpu-kernels-\nstatic.a\n-rwxrwxr-x. 1 cozzy cozzy 5.4M Dec  7 18:53 libawkward.so\n-rw-rw-r--. 1 cozzy cozzy  11M Dec  7 18:53 libawkward-static.a\n\nRelWithDebInfo:\n\nls -alh libawkward*\n-rwxrwxr-x. 1 cozzy cozzy 8.9M Dec  7 18:51 libawkward-cpu-kernels.so\n-rw-rw-r--. 1 cozzy cozzy  22M Dec  7 18:51 libawkward-cpu-kernels-\nstatic.a\n-rwxrwxr-x. 1 cozzy cozzy  97M Dec  7 18:51 libawkward.so\n-rw-rw-r--. 1 cozzy cozzy 229M Dec  7 18:51 libawkward-static.a\n\n\nCompressed sizes are obviously smaller (the debuginfo RPM generated on\nmy system is 43 M, not sure what compression RPM uses nowadays), but\nit's possible that the installed size is too large to make DebugInfo\nthe default if the debuginfo can't be easily split from the rest of the\nwith pip/conda? Especially for users who might be using this on a\ncluster with a small disk quota. That said, I imagine the static\nlibraries don't have a huge number of users?\n\nThanks,\n-Cosmin\n\nOn Mon, 2020-12-07 at 16:07 -0800, Jim Pivarski wrote:\nThe CMake type shouldn't be Debug, but I don't know the consequences of\nRelease versus RelWithDebInfo.\nAccording to https://stackoverflow.com/a/48755129/1623645,\nRelWithDebInfo sounds like a good idea. Even if we're not making RPMs,\nhaving human-readable debugging symbols around is likely to be useful.\nIt doesn't slow down the runtime execution.\nI'm willing to accept the changes you've described above as a pull\nrequest (adding the .rpmmacros file, for instance).\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or unsubscribe.\n\n\n",
  "created_at":"2020-12-08T01:12:31Z",
  "id":740288162,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDI4ODE2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T01:12:31Z",
  "user":"MDQ6VXNlcjkyMDY1Njk="
 },
 {
  "author_association":"MEMBER",
  "body":"I was going to say that compiled binary size doesn't matter, but checked myself because that's a matter of degree. 11 MB \u2192 229 MB is actually too much because PyPI has a quota that I've already run into with the smaller releases. Part of the problem is that I have so many wheels. We've been talking about splitting the compiled C++ and Python parts into separate packages because the C++ part only needs one copy per platform, not one copy per platform + Python version, but that still doesn't save a lot of space.\r\n\r\nThe main thing I'm planning to do for the PyPI-space problem is to upload non-release candidates infrequently (fortnightly or monthly?) and _delete_ release candidates. That is, introduce a hierarchy in which users are only allowed to pin to these somewhat-more-major monthly releases, but can test bug-fixes on the temporary release candidates. Since there could be ~10 release candidates in each cycle, that practice would save a factor of ~10 of PyPI space.\r\n\r\nBut this RelWithDebInfo would add a factor of 20. I can't afford that.",
  "created_at":"2020-12-08T16:19:13Z",
  "id":740729502,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDcyOTUwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T16:19:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim,\n\nYeah, I see the factor of 20 in the wheel size with RelWithDebInfo\n(107MB vs 5.7 MB). It's only a factor of ~12 with RPM but that's\nprobably because RPM uses xz compression nowadays which might be\nsmarter about compressing the debuginfo. \n\nOne thing that might help a little bit might be to remove the static\nlibraries from the distribution (just removing them from the\nCMakeLists.txt INSTALL lines is fine, I think), since I suspect they're\nnot really used by anyone (Python should use the dynamic library, and I\nsuspect most if not all t C++ users also will use the dynamic\nlibraries). \n\nWithout the static library, wheel sizes are:\nRelWithDebInfo: 71 MB \u00a0\nRelease: 3.9 MB\n\nWhich is still more than a factor of 10. So probably best is to just to\nmake it slightly easier to enable/disable debug info. Since I don't\nknow much about setuptools, a hacky solution might be something like\nthis in setup.py: \n\n   cfg = os.environ['CMAKE_BUILD_TYPE'] if 'CMAKE_BUILD_TYPE' in os.environ else \"Debug\" if self.debug else \"Release\"   \n\nwhich would let users do something like:\u00a0\n\n  CMAKE_BUILD_TYPE=RelWithDebInfo python setup.py CMD\n\nWith some more sophistication, it might make sense for debuginfo to be\nthe default for source builds but not for binary wheels?\n\n\nThanks,\n-Cosmin\n\nOn Tue, 2020-12-08 at 08:19 -0800, Jim Pivarski wrote:\n> I was going to say that compiled binary size doesn't matter, but\n> checked myself because that's a matter of degree. 11 MB \u2192 229 MB is\n> actually too much because PyPI has a quota that I've already run into\n> with the smaller releases. Part of the problem is that I have so many\n> wheels. We've been talking about splitting the compiled C++ and\n> Python parts into separate packages because the C++ part only needs\n> one copy per platform, not one copy per platform + Python version,\n> but that still doesn't save a lot of space.\n> The main thing I'm planning to do for the PyPI-space problem is to\n> upload non-release candidates infrequently (fortnightly or monthly?)\n> and delete release candidates. That is, introduce a hierarchy in\n> which users are only allowed to pin to these somewhat-more-major\n> monthly releases, but can test bug-fixes on the temporary release\n> candidates. Since there could be ~10 release candidates in each\n> cycle, that practice would save a factor of ~10 of PyPI space.\n> But this RelWithDebInfo would add a factor of 20. I can't afford\n> that.\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n\n\n",
  "created_at":"2020-12-08T18:30:52Z",
  "id":740824145,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDgyNDE0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T18:30:52Z",
  "user":"MDQ6VXNlcjkyMDY1Njk="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, the libraries are deliberately included in the Python packages so that people can compile against exactly the version of Awkward Array that is used by Python. It's not a _common_ use, but it is a design requirement and we have some specific use-cases lined up. This seemed to be the most robust way to do the version bookkeeping, which becomes a nightmare if they come from different sources.\r\n\r\nWhy do we need both static and dynamic libraries? Some systems, such as Windows, can't seem to mix them. But that's just a factor of 2.",
  "created_at":"2020-12-08T20:06:25Z",
  "id":740953319,
  "issue":562,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDk1MzMxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T20:06:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Success!\r\n\r\nManually uploaded: https://pypi.org/project/awkward1/1.0.0rc1/",
  "created_at":"2020-12-01T00:57:20Z",
  "id":736147101,
  "issue":564,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNjE0NzEwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-01T00:57:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> As I understand it, this gives a warning (not an error) in case a kernel is not implemented, right?\r\n\r\nIt prints out the names of the kernels that have not been implemented (either in the specification, as a cpu kernel, or a gpu kernel) instead of raising an exception. We consider a kernel to \"exist\" if it's name is present in the specification. When this script says that a kernel is not implemented in the specification, it means that the Python implementation is missing. \r\n\r\n\r\nI havn't added this to CI. Should I? (The other part of this script which checks if the specification is sorted is a part of the CI, since it raises an exception if the specification file isn't sorted)\r\n",
  "created_at":"2020-12-01T13:51:14Z",
  "id":736564239,
  "issue":565,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNjU2NDIzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-01T13:51:14Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, I see. Yes: it makes sense to include the sorted-check in CI, but not the list of missing kernels, since we want to ensure that the kernels are sorted, but we can't ensure that there are no missing kernels until they've all been written. (So that's a check that should someday be in CI, but not yet.)",
  "created_at":"2020-12-01T14:50:24Z",
  "id":736600323,
  "issue":565,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNjYwMDMyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-01T14:50:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"By the way, is it done?",
  "created_at":"2020-12-01T22:24:14Z",
  "id":736858654,
  "issue":565,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNjg1ODY1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-01T22:24:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yup :) ",
  "created_at":"2020-12-02T05:51:39Z",
  "id":737008185,
  "issue":565,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNzAwODE4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-02T05:51:39Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"PR #570 fixes it:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import gc\r\n>>> builder = ak.ArrayBuilder()\r\n>>> builder.integer(123)\r\nCPU  malloc at 0x56286a4b4a50 (8192 bytes)\r\n>>> builder.integer(456)\r\n>>> del builder\r\nCPU  free   at 0x56286a4b4a50\r\n>>> builder = ak.ArrayBuilder()\r\n>>> builder.integer(123)\r\nCPU  malloc at 0x56286a4b4a50 (8192 bytes)\r\n>>> builder.integer(456)\r\n>>> array = builder.snapshot()\r\n>>> del builder\r\n>>> array\r\n<Array [123, 456] type='2 * int64'>\r\n>>> del array\r\n>>> gc.collect()\r\nCPU  free   at 0x56286a4b4a50\r\n7\r\n```\r\n\r\nThe ArrayBuilder nodes weren't freeing their memory before because each node had a circular reference. They had to return a `std::shared_ptr` to themselves, so they kept a \"that\" pointer, which was my `std::shared_ptr`-wrapped version of \"this\". What I really needed was the C++11 feature `shared_from_this()`. Switching to `shared_from_this()`, the reference counts apparently drop to zero at the appropriate times: when an unsnapshotted `builder` gets deleted or when all references to the data get deleted (snapshot shares the underlying buffer as a `std::shared_ptr`).\r\n\r\nI also have these snazzy new malloc/free messages that I can turn on when debugging.",
  "created_at":"2020-12-04T16:56:07Z",
  "id":738894020,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczODg5NDAyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-04T16:56:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski , it is amazing to see how fast you react to problems and start fixing things. Last Friday i asked myself how long it will take until this issue  will be fixed and now on Monday morning there are already two new versions! So i'm sad to report that even with the version (awkward 1.0.0) the memory still grows when i overwrite awkward arrays in a loop.  Did you test the loops that you posted in Gitter with the new version?",
  "created_at":"2020-12-07T09:09:27Z",
  "id":739782929,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTc4MjkyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T09:09:27Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"MEMBER",
  "body":"I wonder if you're using the version you think you're using. I just ran\r\n\r\n```python\r\n% python -i -c 'import awkward as ak; import numpy as np; import gc'\r\n>>> for i in range(1000000):\r\n...   a = ak.Array([i])\r\n...   del a\r\n...   tmp = gc.collect()\r\n... \r\n```\r\n\r\nand your original\r\n\r\n```python\r\n% python -i -c 'import awkward as ak; import numpy as np; import gc'\r\n>>> for i in range(1000000):\r\n...   a = ak.Array([i])\r\n...   del a\r\n... \r\n```\r\n\r\n(garbage collector outside of the loop, and therefore not relevant for scaling during the loop), and I don't see any significant increases in memory in two minutes. \"Significant\" for me means 100 MB, since this is the level of background noise on htop.\r\n\r\nhtop isn't targeted, so I turned on my debugging statements and ran the loop without garbage collector:\r\n\r\n```python\r\n% python -i -c 'import awkward as ak; import numpy as np; import gc'\r\n>>> for i in range(10):\r\n...   a = ak.Array([i])\r\n...   del a\r\n... \r\nCPU  malloc at 0x55615983ce70 (8192 bytes)\r\nCPU  free   at 0x55615983ce70\r\nCPU  malloc at 0x55615983ce70 (8192 bytes)\r\nCPU  free   at 0x55615983ce70\r\nCPU  malloc at 0x55615983ce70 (8192 bytes)\r\nCPU  free   at 0x55615983ce70\r\nCPU  malloc at 0x55615983ce70 (8192 bytes)\r\nCPU  free   at 0x55615983ce70\r\nCPU  malloc at 0x55615983ce50 (8192 bytes)\r\nCPU  free   at 0x55615983ce50\r\nCPU  malloc at 0x55615983ce50 (8192 bytes)\r\nCPU  free   at 0x55615983ce50\r\nCPU  malloc at 0x55615983ce50 (8192 bytes)\r\nCPU  free   at 0x55615983ce50\r\nCPU  malloc at 0x55615983ce50 (8192 bytes)\r\nCPU  free   at 0x55615983ce50\r\nCPU  malloc at 0x55615983ce50 (8192 bytes)\r\nCPU  free   at 0x55615983ce50\r\nCPU  malloc at 0x55615983ce50 (8192 bytes)\r\nCPU  free   at 0x55615983ce50\r\n```\r\n\r\nEach `del` is freeing memory. Doing it again without `del`:\r\n\r\n```python\r\n% python -i -c 'import awkward as ak; import numpy as np; import gc'\r\n>>> for i in range(10):\r\n...   a = ak.Array([i])\r\n... \r\nCPU  malloc at 0x5620d2865bc0 (8192 bytes)\r\nCPU  malloc at 0x5620d2868440 (8192 bytes)\r\nCPU  free   at 0x5620d2865bc0\r\nCPU  malloc at 0x5620d2865bc0 (8192 bytes)\r\nCPU  free   at 0x5620d2868440\r\nCPU  malloc at 0x5620d2868440 (8192 bytes)\r\nCPU  free   at 0x5620d2865bc0\r\nCPU  malloc at 0x5620d2865bc0 (8192 bytes)\r\nCPU  free   at 0x5620d2868440\r\nCPU  malloc at 0x5620d2868440 (8192 bytes)\r\nCPU  free   at 0x5620d2865bc0\r\nCPU  malloc at 0x5620d2865bc0 (8192 bytes)\r\nCPU  free   at 0x5620d2868440\r\nCPU  malloc at 0x5620d2868440 (8192 bytes)\r\nCPU  free   at 0x5620d2865bc0\r\nCPU  malloc at 0x5620d2865bc0 (8192 bytes)\r\nCPU  free   at 0x5620d2868440\r\nCPU  malloc at 0x5620d2868440 (8192 bytes)\r\nCPU  free   at 0x5620d2865bc0\r\n>>> del a\r\nCPU  free   at 0x5620d2868440\r\n```\r\n\r\nJust replacing `a` deletes the previous one. Before the explicit `del a`, there are 10 allocations and 9 deallocations.",
  "created_at":"2020-12-07T15:24:33Z",
  "id":739987089,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTk4NzA4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T15:24:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"If i run the following, i reach your limit of 100 MB in one and a half minutes. To be honest i don't know if tracemalloc also fills the memory with something but the tree largest positions in the print are due to the awkward package. \r\n\r\n```\r\n%%time\r\nimport awkward as ak\r\nimport tracemalloc\r\nimport gc\r\n\r\nprint(\"Awkward version:\"+str(ak.__version__))\r\n\r\ntracemalloc.start()\r\n\r\nfor i in range(700000):\r\n    b = ak.Array([i])\r\n    del b\r\n    \r\ngc.collect()\r\n    \r\nsnapshot = tracemalloc.take_snapshot()\r\nfor stat in snapshot.statistics(\"lineno\")[:3]:\r\n    #print the 3 largest memory allocations \r\n    print(stat)\r\n\r\ncurrent, peak = tracemalloc.get_traced_memory()\r\nprint(\"currently using {} MB; peak was {} MB\".format(current/10**6,peak/10**6))\r\ntracemalloc.stop()\r\n```\r\n\r\n```\r\nAwkward version:1.0.0\r\n/.local/lib/python3.6/site-packages/awkward/highlevel.py:268: size=35.4 MiB, count=700000, average=53 B\r\n/.local/lib/python3.6/site-packages/awkward/_util.py:151: size=35.4 MiB, count=700000, average=53 B\r\n/.local/lib/python3.6/site-packages/awkward/_util.py:146: size=35.4 MiB, count=700000, average=53 B\r\ncurrently using 148.420465 MB; peak was 148.420945 MB\r\nCPU times: user 1min 24s, sys: 288 ms, total: 1min 24s\r\nWall time: 1min 24s\r\n```",
  "created_at":"2020-12-07T16:56:46Z",
  "id":740045369,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDA0NTM2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T16:58:58Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"MEMBER",
  "body":"100 MB wasn't my limit\u2014it's the level of noise: things jump up and down by 100 MB. Focusing in on the Python process could help, but still, memory accounting is tricky because OS-level memory can be shared among processes. That's why I was looking for a much stronger signal than 100 MB, to be sure it was a reproducible thing.\r\n\r\nOn the opposite end of the granularity scale, I do see that your example of repeatedly creating `ak.Array([i])` with the same Python name does not leak memory from Awkward's C++ layer. 100% of the array allocations go through the function that produced the above print-outs\u2014I banished free `new` operators in PR #570 (and found one additional memory leak in the process, in `ak.combinations` for ListArrays). Whatever tracemalloc is seeing is not Awkward-C++ related, though it could be Python-related and happening in the `awkward` library. Remember that you're not garbage-collecting in the loop.",
  "created_at":"2020-12-07T17:05:21Z",
  "id":740050624,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDA1MDYyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T17:05:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Doing the garbage-collecting in the loop unfortunately increases the runtime so much that it becomes absolutely impossible to run the code. The event loop in my project is way to large and for each event i have to compute quite a few variables. ",
  "created_at":"2020-12-07T17:20:08Z",
  "id":740059142,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDA1OTE0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T17:20:08Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"MEMBER",
  "body":"I wasn't recommending running `gc.collect()` in a loop\u2014or at all, in production code. I meant it as a debugging technique.",
  "created_at":"2020-12-07T17:59:12Z",
  "id":740081288,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDA4MTI4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T17:59:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski , i'm wondering if you still consider this memory leakage an open issue or not ? I'm sorry that i keep annoying you about it but i have to know if i should start looking for a possible work around.  ",
  "created_at":"2020-12-10T09:35:00Z",
  "id":742401448,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MjQwMTQ0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-10T09:35:00Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"MEMBER",
  "body":"I don't see a memory leak anymore\u2014at least, I can't reproduce it.",
  "created_at":"2020-12-10T13:33:13Z",
  "id":742523596,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MjUyMzU5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-10T13:33:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I have to agree that the growth of the memory is no longer as easy to observe as it was before the version 1.0.0 but it i think it is still there. Try to run something like this inside the loop. \r\n\r\n```\r\n for i in range(500000): \r\n    a = ak.Array([[i],[i],[],[i],[i],[i],[]])\r\n    b = ak.Array([[i],[i],[],[i],[i],[i],[]])\r\n    c = ak.Array([[i],[i],[],[i],[i],[i],[]])\r\n    d = ak.Array([[i],[i],[],[i],[i],[i],[]])\r\n    \r\n    del a,b,c,d \r\n```\r\n\r\nI just put in a couple of variables in the loop with a structure that is not completely trivial. Running this code requires 425 MB and takes 0:02:26. As structures gets more complicated by adding a loop inside the loop and computing more variables this will reach the memory limit rather quickly.  ",
  "created_at":"2020-12-10T15:22:24Z",
  "id":742588176,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MjU4ODE3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-10T15:23:05Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"MEMBER",
  "body":"I believe you're seeing some memory issues, but if you're using the latest release, they're not in `ak.Arrays`. (I deployed [1.0.1rc1](https://pypi.org/project/awkward/1.0.1rc1/) last night; it has no differences with respect to the main branch right now.) The entire codebase has no `new` operators anymore (with the exception of some objects for JSON-handling, which you're not doing). All class instances are allocated with `std::make_shared` and all arrays are allocated with `kernel::malloc`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/030eb02d2bd7b4d9077a8bcd8ab0986cb0972121/include/awkward/kernel-dispatch.h#L159-L186\r\n\r\nwhich creates non-GPU arrays as `std::shared_ptrs` with an `array_deleter`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/030eb02d2bd7b4d9077a8bcd8ab0986cb0972121/include/awkward/kernel-dispatch.h#L59-L81\r\n\r\nBoth of these `awkward_malloc` and `awkward_free` functions (for non-GPUs) resolve to:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/030eb02d2bd7b4d9077a8bcd8ab0986cb0972121/src/cpu-kernels/allocators.cpp#L7-L23\r\n\r\nSince this has 100% coverage, we can track every array allocation and deletion. (For class instances, we have to trust C++'s shared memory handling.)\r\n\r\nIn your new example, turning on the print-outs for two passes of the loop prints:\r\n\r\n```\r\nCPU  malloc at 0x55fa5839ca80 (8192 bytes)\r\nCPU  malloc at 0x55fa58403f00 (8192 bytes)\r\nCPU  malloc at 0x55fa58399250 (8192 bytes)\r\nCPU  malloc at 0x55fa57cfd080 (8192 bytes)\r\nCPU  malloc at 0x55fa58407da0 (8192 bytes)\r\nCPU  malloc at 0x55fa58417780 (8192 bytes)\r\nCPU  malloc at 0x55fa58460400 (8192 bytes)\r\nCPU  malloc at 0x55fa583713c0 (8192 bytes)\r\nCPU  free   at 0x55fa58403f00\r\nCPU  free   at 0x55fa5839ca80\r\nCPU  free   at 0x55fa57cfd080\r\nCPU  free   at 0x55fa58399250\r\nCPU  free   at 0x55fa58417780\r\nCPU  free   at 0x55fa58407da0\r\nCPU  free   at 0x55fa583713c0\r\nCPU  free   at 0x55fa58460400\r\nCPU  malloc at 0x55fa58403f00 (8192 bytes)\r\nCPU  malloc at 0x55fa5839ca80 (8192 bytes)\r\nCPU  malloc at 0x55fa58399250 (8192 bytes)\r\nCPU  malloc at 0x55fa57cfd080 (8192 bytes)\r\nCPU  malloc at 0x55fa58407da0 (8192 bytes)\r\nCPU  malloc at 0x55fa58417780 (8192 bytes)\r\nCPU  malloc at 0x55fa58460400 (8192 bytes)\r\nCPU  malloc at 0x55fa583713c0 (8192 bytes)\r\nCPU  free   at 0x55fa5839ca80\r\nCPU  free   at 0x55fa58403f00\r\nCPU  free   at 0x55fa57cfd080\r\nCPU  free   at 0x55fa58399250\r\nCPU  free   at 0x55fa58417780\r\nCPU  free   at 0x55fa58407da0\r\nCPU  free   at 0x55fa583713c0\r\nCPU  free   at 0x55fa58460400\r\n```\r\n\r\nEvery malloc is paired with a free for the same buffer. (Each array has two buffers for the ListOffsetArray `offsets` and the NumpyArray. Since your examples don't use up the 8 kB initial allocation, it's the depth that allocates more, not the length of your examples.)\r\n\r\nIn your original report, you identified a real memory leak, so I made the codebase airtight for all array allocations. (The four JSON-handling nodes should be investigated more deeply at a later date, but they're not relevant here.) Before the fix, I observed linearly increasing memory use, though I had to watch it more than a minute for the trend to be clear the way I was measuring it. After the fix, not only do we have this demonstration that all the allocations are paired with frees, but also I don't observe the linearly increasing memory, even after two minutes. That original error was fixed.\r\n\r\nYou're still seeing memory leaks, but we know (from the above) that it's not any buffer in Awkward Array and I trust C++'s `shared_ptr` handling for class instances. Deleting Python objects _sometimes_ frees them and sometimes you have to wait for the next garbage collection for them to be freed. In this particular case, they do get freed right away (probably because there are no cyclic references).\r\n\r\nMaybe you have a situation in which objects are waiting for the garbage collector, but you have a memory limit on the process that the garbage collector doesn't know about, so the garbage collector doesn't kick in before the process already dies? Anyway, I don't have any evidence of a memory leak in Awkward Array (any more), so I have nothing to follow up on.",
  "created_at":"2020-12-10T16:20:36Z",
  "id":742625391,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MjYyNTM5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-10T16:20:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I see that the awkward arrays are working flawlessly on the C++ level but on the python level there seems to be something odd happening. Overwriting an awkward array 2 million times  (with version 1.0.1rc1) takes up 424 MB and doing the same with numpy array or a list does not even use 1MB...\r\n\r\nI'm afraid that in the end i'll have to rewrite the my whole analysis and abandon the awkward arrays, just because the automatic garbage collection does not work and i haven't found a way to free memory. What would you do in my position? Move on and rewrite everything? ",
  "created_at":"2020-12-11T12:37:43Z",
  "id":743169397,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzE2OTM5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T12:37:43Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"MEMBER",
  "body":"If I were in your situation, I would clone the awkward-1.0 GitHub repo, turn on the memory print-outs, and `pip install .` to try to see them in my own environment. The simplest explaination for the discrepancy is that you're using a different version of the software than you think you are, and that's happened to me countless times. It's a nuisance to have to get a dependency from GitHub and compile from source (be sure to have make, CMake, and a C++ compiler installed), rather than just grabbing a wheel from PyPI, but it's probably a less involved step than rearranging an entire analysis.\r\n\r\nAlso, thinking about this in a wider context, why are you creating millions of Awkward Arrays? Even without explicit leaks, that's not an efficient way to work. There's a significant time overheard in creating an array, so we want to use a small number of large arrays, using array-at-a-time operations (\"vectorization\").\r\n\r\nIf your real code looks like this benchmark, then it should probably be refactored anyway if running time is a concern. If what you're doing can't be reexpressed as a vectorized operation and must be in for loops, then you should consider Numba. You can't create Awkward Arrays inside of a Numba-compiled function, but you can create the equivalent of Python lists for temporary work. (That's what it looks like this function is doing.)",
  "created_at":"2020-12-11T13:06:48Z",
  "id":743182414,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzE4MjQxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T13:06:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski , thanks for your advice. I thought about the structure and if it is possible to use operations on an axis at the time but the issue is that structure that I face is really not trivial. I have a nested structure with vectors of vectors and with an \u201cinstruction\u201d which elements belong together. I added a little example to illustrate what I'm talking about. \r\n\r\n```\r\nEvent 1:\r\n\r\na = [[1,2,3,4],[5],[6,7,8],[9]]\r\n\r\nPulse 1:\r\ninstruction for first level: [0,1]\r\ninstruction for second level: [0,0]\r\n\r\na[[0,1],[0,0]] = [1,5] and now i do some operations on this array and save the result. \r\n\r\nPulse 2:\r\ninstruction for first level: [0,2]\r\ninstruction for second level: [2,1]\r\n\r\na[[0,2],[2,1]] = [3,7] and now I do the same operations on this array and save the result. \r\n```\r\n\r\nSo this I why I\u2019m creating so many arrays. I have around half a million events and per event there are like 10 pulses. I realized that accessing the elements in this fashion is really the limiting memory factor for me. Running something like the following already uses more than 200 MB and keeps growing with more accesses. \r\n\r\n```\r\na = ak.Array([[1,2,3,4],[5],[6,7,8],[9]])\r\n\r\nfor i in range(1000000): \r\n    b = a[[0,1],[0,0]]\r\n```\r\n\r\nDo you also observe this behavior and is it actually what you would expect? If you observe the same thing, I think should switch to ROOT and do it there.  ",
  "created_at":"2020-12-14T14:18:24Z",
  "id":744470368,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDQ3MDM2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-14T14:19:58Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"MEMBER",
  "body":"If the garbage collector stops the world and cleans up before you physically run out of memory, as it's supposed to (but maybe don't if some strange `ulimit` constraints are applied on your system?), then yes, a lot of memory will be used, but it won't error-out due to running out of memory. We've established that the Python reference count goes to zero on these objects and when I run it on my system, it does not run out of memory.\r\n\r\nThe reason I was arguing for array-at-a-time functions or Numba is because of time, not memory usage. Creating a million Awkward Arrays is much slower than creating a million Python lists:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> a = ak.Array([[1,2,3,4],[5],[6,7,8],[9]])   # \"a\" is an Awkward Array\r\n>>> def f(n):\r\n...     for i in range(n):\r\n...         b = a[[0, 1], [0, 0]]    # addressed with an advanced slice\r\n... \r\n>>> starttime = time.time(); f(100000); time.time() - starttime\r\n9.222622871398926\r\n\r\n>>> a = [[1,2,3,4],[5],[6,7,8],[9]]    # \"a\" is a Python list of lists\r\n>>> def g(n):\r\n...     for i in range(n):\r\n...         b = [a[0][0], a[1][0]]    # addressed with explicit indexes\r\n... \r\n>>> starttime = time.time(); g(100000); time.time() - starttime\r\n0.04160714149475098\r\n```\r\n\r\nThat was just 100,000, but already we see that the plain Python lists are 220\u00d7 faster. Creating millions of Awkward Arrays shouldn't be a memory leak, but it's not often tested because it's such an antipattern for _time_ usage.\r\n\r\nI don't have a big picture of what you're trying to do\u2014it might be possible with array-at-a-time functions, but it seems like it's easier to think about this problem in an element-at-a-time way, so I'll show you how to get what you want, at scale, without having to rethink the problem to cast it into an array-at-a-time form.\r\n\r\nYou can use Numba. Let's say you want a 2-D NumPy array of `b`s:\r\n\r\n```python\r\n>>> import numba as nb\r\n>>> import numpy as np\r\n>>> a = ak.Array([[1,2,3,4],[5],[6,7,8],[9]])\r\n>>> @nb.njit\r\n... def h(n):\r\n...     out = np.empty((n, 2), np.int64)\r\n...     for i in range(n):\r\n...         out[i, 0] = a[0][0]   # in Numba, Awkward Arrays have to be accessed as though they were lists\r\n...         out[i, 1] = a[1][0]   # that is, no a[[0, 1], [0, 0]] syntax; do things one at a time\r\n...     return out\r\n... \r\n>>> h(1)    # the first time you call it compiles it; not representative of the eventual speed\r\narray([[1, 5]])\r\n>>> starttime = time.time(); h(100000); time.time() - starttime\r\narray([[1, 5],\r\n       [1, 5],\r\n       [1, 5],\r\n       ...,\r\n       [1, 5],\r\n       [1, 5],\r\n       [1, 5]])\r\n0.001569509506225586\r\n```\r\n\r\nAnd that gives you an additional factor of 26\u00d7 over the Python lists, which is a total of 5800\u00d7 times faster than what you're doing with Awkward Arrays outside of Numba. The Numba implementation does not create intermediate Awkward Arrays: `a[1]` and `a[1][0]` is just accessing elements from Awkward Array's internal buffers, not creating in-memory objects representing those structures\u2014all of the structure manipulation is compile-time logic that creates LLVM instructions to directly get the data at runtime.\r\n\r\nSo this method short-circuits any memory problems: it doesn't create any objects that would need to be deleted (whatever is going wrong in your application to not delete those objects), but it also addresses a many-thousands-of-times speedup that's probably more important than the memory issue itself.\r\n\r\n(Sadly, that's probably how the original memory issue went unnoticed, but based on the arguments I gave previously, I don't think _Awkward Array_ has a memory issue in creating `ak.Array` anymore, though I don't know what's happening on your system to make you run out of memory anyway.)",
  "created_at":"2020-12-14T16:51:14Z",
  "id":744569303,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDU2OTMwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-14T16:51:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Man you are absolutely right! I played a bit with the lists and the numba functions and it instantly gave me a pretty good speed improvement. I decided to transform all the awkward arrays that I get from uproot into lists and only work with lists. Unfortunately numba does not allow for nested lists but even with python functions the analysis is way faster. \r\n\r\nUnfortunately I still had the issue with the continuously growing memory, so I tried to only run the uproot command and I figured out the memory leakage I was seeing, was coming from uproot.iterate and not from the awkward arrays. Luckily you actually solved that in a never version (0.1.2). Thanks for all the help!  ",
  "created_at":"2020-12-15T16:30:49Z",
  "id":745406703,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTQwNjcwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T16:30:49Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, there was a memory leak in `uproot.iterate`. I remember that one! (In Python, \"leak\" is harder to define because if it's in memory, it's accessible by some reference, but we definitely didn't _intend_ to keep data from previous iterations in subsequent iterations.) I'm glad you managed to track this down.\r\n\r\nIf you're writing for loop-style code, lists will be faster than Awkward Arrays. And then Numba will be faster than the pure Python, but Numba doesn't deal with nested structures in lists very well, and the transformation of Python objects into Numba objects can itself be expensive. The optimal pairing is Awkward Arrays with Numba: any Awkward data structure can be viewed in Numba without conversion (i.e. it's an _O(1)_ view, rather than an _O(n)_ conversion). The limitation is that if you're using Numba, Awkward Arrays have to be addressed one element at a time: the array-at-a-time style outside of Numba is mutually exclusive with the element-at-a-time style inside of Numba. This was deliberately added to provide flexibility.\r\n\r\nBut if the speed of Python lists is good enough (keep in mind how your analysis will eventually scale), then stick with it! It's probably simplest. You might also be able to save an unnecessary conversion step if you set `library=\"np\"` (or the global `uproot.default_library`) to get NumPy arrays everywhere, including NumPy arrays of NumPy arrays, which behave like lists (and _not_ like 2D NumPy arrays). The NumPy developers have put more effort into streamlining NumPy objects, and they might have speed comparable to Python lists. Uproot's `library=\"np\"` skips Awkward Arrays entirely.",
  "created_at":"2020-12-15T16:54:56Z",
  "id":745423010,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTQyMzAxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T16:54:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Shit i celebrated to early.... the issue it is still there when i combine uproot and awkward. I tried to bypass awkward but unfortunately if have to deal with vectors of vectors within the root file so when i use `library=\"np` i get back STLVectors. Is there a way to transform them into python lists? ",
  "created_at":"2020-12-15T17:51:15Z",
  "id":745457897,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTQ1Nzg5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T17:51:15Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"MEMBER",
  "body":"I think STLVector objects have a `tolist` method. I know that `__getitem__` works on them. Here's the documentation:\r\n\r\nhttps://uproot.readthedocs.io/en/latest/uproot.containers.STLVector.html\r\n\r\nIf your dataset has STLVectors, then the Awkward Arrays were being produced by converting the STLVectors into Awkward Arrays, so skipping that conversion will be a speedup.",
  "created_at":"2020-12-15T17:58:58Z",
  "id":745462286,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTQ2MjI4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T17:58:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks a lot for your help! I finally managed to get the memory issues under control and run the jobs on the universities cluster without crashing the machines. The solution for me was to completely avoid the awkward arrays and go from the `uproot.STLVectors` directly to nested lists and work with them in python. I still don't know what exactly the problem was but if it is a real problem it will appear again and hopefully i can be then reproduced more consistently. As a learning i take away that the real power of the awkward arrays are vectorized operations and in loops they should be avoided. ",
  "created_at":"2020-12-17T15:01:57Z",
  "id":747491105,
  "issue":567,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzQ5MTEwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-17T15:01:57Z",
  "user":"MDQ6VXNlcjI0NDA4NzA3"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - FYI, the PR should be complete if the tests pass.",
  "created_at":"2020-12-03T16:56:20Z",
  "id":738136940,
  "issue":568,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczODEzNjk0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-03T16:56:20Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna Thanks! Don't worry about the job failures\u2014I'm going to fix them (and a few odds and ends) because I want to get down to zero pull requests to do a `master` \u2192 `main` renaming.",
  "created_at":"2020-12-03T17:20:07Z",
  "id":738154762,
  "issue":568,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczODE1NDc2Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-12-03T17:20:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks @jpivarski !",
  "created_at":"2020-12-04T06:35:06Z",
  "id":738596885,
  "issue":568,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczODU5Njg4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-04T06:35:06Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - should a user-defined parameters be allowed? For example, the following tests would raise a `ValueError`:\r\n```python\r\n   def test_dress():\r\n        class Dummy(ak.highlevel.Array):\r\n            def __repr__(self):\r\n                return \"<Dummy {0}>\".format(str(self))\r\n    \r\n        ns = {\"Dummy\": Dummy}\r\n    \r\n        x = ak.layout.NumpyArray(np.array([1.1, 2.2, 3.3, 4.4, 5.5]))\r\n        x.setparameter(\"__array__\", \"Dummy\")\r\n>       a = ak.Array(x, behavior=ns, check_valid=True)\r\n\r\ntests/test_0032-replace-dressedtype.py:72: \r\n```\r\nand\r\n```python\r\n    def test_dress():\r\n        class Dummy(ak.highlevel.Array):\r\n            def __repr__(self):\r\n                return \"<Dummy {0}>\".format(str(self))\r\n    \r\n        ns = {\"Dummy\": Dummy}\r\n    \r\n        x = ak.layout.NumpyArray(np.array([1.1, 2.2, 3.3, 4.4, 5.5]))\r\n        x.setparameter(\"__array__\", \"Dummy\")\r\n>       a = ak.Array(x, behavior=ns, check_valid=True)\r\n\r\ntests/test_0039-no-hanging-types.py:62: \r\n```\r\n```python\r\n>               raise ValueError(out)\r\nE               ValueError: at layout (NumpyArray): __array__ can not be Dummy\r\nE               \r\nE               (https://github.com/scikit-hep/awkward-1.0/blob/1.0.0rc3/src/libawkward/array/NumpyArray.cpp#L1346)\r\n```",
  "created_at":"2020-12-05T12:02:26Z",
  "id":739242114,
  "issue":571,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTI0MjExNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T12:02:26Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"User-defined types are definitely allowed\u2014that's how we stake a claim for new functionality before it is implemented. In the space of all possible parameter names, only a few (`__array__`, `__record__`, and `__typestr__`) have been given meaning so far, but more will likely be added. For `__array__`, a few values have been given special meaning: `\"string\"`, `\"bytestring\"`, `\"char\"`, `\"byte\"`, `\"categorical\"`, but more will likely be added. Uproot is consistently presenting `std::map` objects as RecordArrays with `__record__` = `\"sorted_map\"` (after sorting the key-value pairs) with the expectation that someday, we'll define behaviors that allow people to use these maps to lookup values by sets of keys (probably using NumPy's `np.searchsorted`). @nsmith- has been using `__doc__` as a way to carry documentation from NanoAOD TBranch titles into Awkward Arrays. The [vector](https://github.com/scikit-hep/vector) project will be adding meanings for `__record__` = `\"LorentzVector\"`, `\"Vector2D\"`, and `\"Vector3D\"`.\r\n\r\nSo the rules you're defining should be of the form \"if `__array__` = `\"string\"`, then require such-and-such,\" but not \"`__array__` must be `null` or such-and-such.\"",
  "created_at":"2020-12-05T16:27:00Z",
  "id":739317283,
  "issue":571,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTMxNzI4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T16:27:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - as discussed, I'm done with this PR for the time being :-)\r\n\r\nNote, the following ASIS implementation is work in progress. The `quick sort` algorithm implemented in `src/cpu-kernels/awkward_NumpyArray_subrange_equal.cpp` does not use the C++ standard library, but it's still not suitable for the GPU. A previously implemented lexicographical ranges comparison is not done yet.\r\n",
  "created_at":"2020-12-22T16:34:32Z",
  "id":749640247,
  "issue":571,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0OTY0MDI0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-22T16:34:32Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Missing `ndim` in the Numba context is an oversight, and won't be too hard to write.\r\n\r\n(I'm going to start using the phrase \"in the Numba context\" from now on\u2014it's much better than the \"in Numbafied functions,\" \"in functions compiled by Numba,\" or the not-really-correct \"in Numba.\")",
  "created_at":"2020-12-04T16:37:39Z",
  "id":738884120,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczODg4NDEyMA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "laugh":1,
   "total_count":2
  },
  "updated_at":"2020-12-04T16:37:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"After a quick debugging session I found the place where it probably should go:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/5996899a390099c9637aafc21961d7a041f1ddf7/src/awkward/_connect/_numba/arrayview.py#L647\r\n\r\nIt's already marked with `???` \ud83d\ude09 \r\n\r\nI am not so familiar with the low level Numba architecture but I'll have a look.\r\n\r\nI also found the `@nb.extending.overload_attribute` decorator, maybe that's a better approach than hooking into attributes. No idea...",
  "created_at":"2020-12-05T12:54:12Z",
  "id":739247308,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTI0NzMwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T12:54:12Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"That `???` is for future additions, so even after adding `ndim`, there will still be a `???` for other attributes we want to enable in Numba.\r\n\r\nI'm not asking you to do it\u2014Numba is a very specialized compiler that wasn't originally designed to be developed beyond the core developers. A few years ago, an extension mechanism was added on, though I still had to delve into the Numba internals to find out how to do basic things, and it was this year reorganized to make extensions easier for outsiders, but that's still evolving. I'm considering giving tutorials on Numba extension development, since there aren't a lot of good materials out there, but I haven't done that yet. That would be for people who have already decided that they want to do significant projects in Numba, like [vector](https://github.com/scikit-hep/vector) or [hist](https://github.com/scikit-hep/hist).\r\n\r\nBut anyway, here's a general sketch of how it goes: to add a new type, attribute, method, or function to Numba, you have to add type inference, possibly a model, and a lowered implementation. The compiler passes over the code two times (that you care about as an extender\u2014other passes are for other purposes), first to derive output types from input types, and the second to generate LLVM instructions from the Python code. This `???` is in the typing pass: what should go here is\r\n\r\n```python\r\nif attr == \"ndim\":\r\n    return numba.intp\r\n```\r\n\r\n(Though I can't remember if this typer should be returning the return type, `numba.intp`, or a type signature, `numba.intp()`. That's something I'd have to look up by examining other code. If it's wrong, you get error messages that don't seem to be related because the Python code that picks up the output of the typer assumes it's the right thing.)\r\n\r\nAfter assigning the type, you would then have to write the code that \"lowers\" Python bytecode into LLVM assembly. Sometimes, you can do that by writing more Python code and asking Numba to lower it for you, like this:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/5996899a390099c9637aafc21961d7a041f1ddf7/src/awkward/_connect/_numba/arrayview.py#L1105-L1171\r\n\r\nIt creates a string of Python code, based on the inputs, and passes that to Numba in place of the more generic Python we start with. Eventually, though, everything has to resolve into LLVM instructions, and for something as simple as `ndim`, I'd just write the LLVM directly. For the granularity of Numba's type system, the number of dimensions of a NumPy array is part of the type, not a value to be encountered at runtime (they didn't have to do it that way\u2014there's a lot of freedom in designing type systems), and I did Awkward Array the same way: the type at compile time knows the dimension of the array. Thus, if you pass another array with a different number of dimensions to the same function, the function must be recompiled. However, it can run faster once compiled.\r\n\r\nSo the lowering of `ndim` would be something like\r\n\r\n```python\r\ncontext.get_constant(numba.intp, datatype.get_the_number_of_dimensions)\r\n```\r\n\r\nThe Python code for getting the number of dimensions from the type information might be involved, but it runs at compile-time and generates an LLVM instruction for a constant integer.\r\n\r\nI mentioned a \"model\" as one of the steps. At runtime, we have LLVM structs instead of Python objects and any runtime information has to be derived from these models. However, the number of dimensions can be determined at compile-time, so we can ignore the Awkward Array's model (called an `ArrayView` in this code).\r\n\r\nIn summary, it's not for the faint of heart, but if you _want_ to learn how to extend Numba, possibly to use it in another project, then I can guide you through this and you'll learn some things you can use in that other project.",
  "created_at":"2020-12-05T16:59:54Z",
  "id":739321072,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTMyMTA3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T16:59:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I was already wondering where your golden source of Numba mechanics is located. Now I understand \ud83d\ude05 \r\n\r\nYes, I already realised that Numba passes over the function multiple times and it makes of course sense, but it's really hard to picture where which types or interpretations are to be passed so that the inference works. The error messages do not help much ;) Therefore I tried to reverse engineer how you do it for other things (fields) but most of them are quite complex array mechanics and not so simple to derive what needs to be created for a simple constant.\r\n\r\n> I'm not asking you to do it\r\n\r\nI know, I just wanted to see if I can somehow help out since you already have a lot of stuff ongoing. However, it seems that it would be yet another largish project for me to dive in and I am not sure if it's a good idea in this time period \ud83d\ude06 ",
  "created_at":"2020-12-05T18:20:37Z",
  "id":739330567,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTMzMDU2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T18:20:37Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"As it turns out, it was simpler than I was expecting:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/pull/578/commits/3d76da9dccade6db7e7d93631c253308fc7bb73c\r\n\r\nAfter adding `if attr == \"ndim\"` to the typing pass, the lowering pass didn't need another decorated function, just an `if attr == \"ndim\"` of its own. And another thing I recently added required me to put `ndim` properties on all the type objects, so there wasn't much to do, after all.\r\n\r\nThere was one subtlety: partitioned arrays are handled separately, so the update had to go into both the ArrayView handler and the PartitionedArrayView handler.",
  "created_at":"2020-12-07T23:09:37Z",
  "id":740236242,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDIzNjI0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T23:09:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks Jim, I bookmark this as a reference for similar issues in future. Hopefully I can contribute then ;)",
  "created_at":"2020-12-08T10:01:04Z",
  "id":740518164,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDUxODE2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T10:01:04Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"If you're interested in learning how to extend Numba, I'll keep that in mind for a possible future tutorial. I just think that the audience will be small.  `:)`",
  "created_at":"2020-12-08T16:08:15Z",
  "id":740715458,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDcxNTQ1OA==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2020-12-08T16:08:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"OK thanks, I do! \ud83d\ude09 \r\n\r\nJust banging my head against the wall since I ran into the next issue: an extremely limited recursion support in Numba, which makes my example above basically impossible to lower. Now trying to wrap my head to unroll it, but this whole high-performance Python-stack is starting to get a bit annoying nowadays \ud83d\ude48  Glad that you created the `awkward` package which makes many things possible though!",
  "created_at":"2020-12-08T16:15:38Z",
  "id":740724801,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDcyNDgwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T16:15:38Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"The biggest problem for recursion is propagating type information. Often, you get into situations in which the return type depends on the argument type, but the argument type depends on the return type. Since Numba is starting from Python, it usually lacks the type annotations that would resolve this circular dependency. It is, however, possible to add type annotations to the `@numba.jit` operator: https://numba.pydata.org/numba-doc/latest/user/jit.html#eager-compilation\r\n\r\nAre you using these?\r\n\r\n(Saying \"type annotations\" suggests that they ought to be Python type annotations, but I don't think Numba knows how to convert Python annotations into Numba annotations. The latter have finer granularity: `int64`, `int32`, `int16`, etc. instead of arbitrary-precision `int`, for instance, but one-way Python annotations \u2192 Numba annotations ought to be possible.)",
  "created_at":"2020-12-08T16:42:42Z",
  "id":740751066,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDc1MTA2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T16:43:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes I have starting looking into them, the problem is that I am not dealing with primitive types but the more complex `awkward.Array`s. In my initial example above you see that the dimension is reduced in each recursion. As you described, it's somehow chocking on the type information provided by the `getiter` functionality. I am already trying to boil it down to an MWE so that I can inspect it more easily but it's hard to reproduce.",
  "created_at":"2020-12-08T16:45:47Z",
  "id":740752976,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDc1Mjk3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T16:45:47Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"By choosing to make `ndim` part of a NumPy array type, Numba made it so that each level of recursion has to be a separate compilation: a function compiled for a 1-dimensional array can't be used for a 2-dimensional array, etc. Such a thing would have to have overloads (recompilations) for each number of array dimensions. Furthermore, the choice of which overload to use would have to be made using type information, not a runtime thing like an `if` statement. If you had\r\n\r\n```python\r\nif array.ndim == 1:\r\n    return do_base_case(array)\r\nelse:\r\n    return do_recursive_step(array)\r\n```\r\n\r\nthen both parts of the `if` statement get _compiled_ for all `ndim` cases, even though some of them will never be reached at runtime. Making `if` statements \"narrow\" the type in the two branches gets complicated fast: you have to recognize that the `if` predicate is fully known at compile-time, and that predicate can be any expression.\r\n\r\nYou want a compile-time `if`, but of course Python doesn't have anything like that.\r\n\r\nWhat _should_ be possible is to use [@numba.generated_jit](https://numba.pydata.org/numba-doc/dev/user/generated-jit.html) to make a compile-time decision based on type and provide different implementations for each type, such as arrays of different dimension. This _should_ be possible\u2014all of the information is there:\r\n\r\n```python\r\n>>> @nb.generated_jit(nopython=True)\r\n... def do_recursive(arg):\r\n...     # arg is a type (instance of nb.types.Array)\r\n...     if arg.ndim == 1:\r\n...         # arg is a value (array at runtime)\r\n...         return lambda arg: arg\r\n...     else:\r\n...         # arg is a value again (different type, though)\r\n...         return lambda arg: do_recursive(arg[0])\r\n... \r\n>>> array = np.arange(2*3*5).reshape(2, 3, 5)\r\n>>> array\r\narray([[[ 0,  1,  2,  3,  4],\r\n        [ 5,  6,  7,  8,  9],\r\n        [10, 11, 12, 13, 14]],\r\n\r\n       [[15, 16, 17, 18, 19],\r\n        [20, 21, 22, 23, 24],\r\n        [25, 26, 27, 28, 29]]])\r\n>>> do_recursive(array)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n...\r\nNotImplementedError: Failed in nopython mode pipeline (step: nopython frontend)\r\ncall to CPUDispatcher(<function do_recursive at 0x7f5286bf6ca0>): unsupported recursion\r\n```\r\n\r\nThat looks like work TBD. Unlike the `if` branches that depend on type, there aren't fundamental issues for this one, it _should_ work. This is where it should be implemented:\r\n\r\nhttps://github.com/numba/numba/blob/1041fa6ee8430471da99b54b3428a673033e7e44/numba/core/typeinfer.py#L1578-L1587\r\n\r\nThe part that is implemented depends on whether a `callframe` can be found in `context.callstack`... I don't know enough about this.",
  "created_at":"2020-12-08T17:39:10Z",
  "id":740789151,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDc4OTE1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T17:39:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ah I see, thanks, that's very helpful. There are some significant differences on how Numba is dealing with type inference compared to Julia, which I am familiar with. Yes I already saw that the dimensionality is part of the type, which requires separate compilations.\r\nThe [@numba.generated_jit](https://numba.pydata.org/numba-doc/dev/user/generated-jit.html) is interesting, I have not seen that before.\r\n\r\nBtw. I opened an issue meanwhile since it seems that parts of the type inference are somehow working, at least in that recursive function but it seems that something might be missing from the awkward side. Not sure though. https://github.com/scikit-hep/awkward-1.0/issues/580\r\n\r\nAnyways, please see all these as low priority, I am already working on it in parallel and I guess it's quite a special use-case anyways. If I fail, I will just implement some hardcoded routines for `ndim={4,3,2}`, which should cover most of our users needs.",
  "created_at":"2020-12-08T17:50:02Z",
  "id":740796580,
  "issue":572,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDc5NjU4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T17:50:02Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"NONE",
  "body":"Same happens with ```ak.firsts(array, axis = -1)```\r\n```\r\nNotImplementedError: ak.firsts with axis=-1\r\n```",
  "created_at":"2020-12-05T17:07:50Z",
  "id":739321946,
  "issue":574,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTMyMTk0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T17:07:50Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"The documentation says that `axis < 0` is allowed because it's copy-pasted documentation for the `axis` parameter generally. It's hard enough to keep docstrings in sync with so much duplication (why, Python, do docstrings have to be string literals?!?), also having them change as features get added would be too much. So `axis < 0` is a missing implementation for `ak.cartesian`, not a bug, despite the fact that it disagrees with the documentation. I want to avoid the situation in which we update the code but forget to update the documentation!\r\n\r\nAt the time when `ak.cartesian` was first written, it would have been difficult to implement `axis < 0`, but it isn't anymore. This is now low-hanging fruit for anyone who's interested or for me if I have a half-hour block to fill (most of that time would be writing the tests).\r\n\r\nIn `ak.cartesian`'s internal `getfunction1` or `getfunction2` (not sure which, or if it needs to be both), the `depth` should be used to convert the `axis` into a `posaxis` with\r\n\r\n```python\r\nposaxis = layout.axis_wrap_if_negative(posaxis)\r\n```\r\n\r\nsimilar to the definition in `ak.to_regular`. The `posaxis` starts being equal to `axis`, but as soon as it can be converted, it is converted. See\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/5996899a390099c9637aafc21961d7a041f1ddf7/src/libawkward/Content.cpp#L1749-L1771\r\n\r\nfor what `axis_wrap_if_negative` does\u2014it was written after `ak.cartesian` and `ak.firsts`, so that's why they don't take advantage of it yet.\r\n\r\nIn order to make `posaxis` \"sticky,\" I made it a one-element list in `ak.to_regular`. Maybe I could have used a Python `global` or `nonlocal` keyword, but I didn't want to do anything that is Python version-dependent: changing a list in place is \"sticky\" in a way that reassigning a variable in a nested function isn't (because variables have function scope in Python). That's why the actual line in `ak.to_regular` is\r\n\r\n```python\r\nposaxis[0] = layout.axis_wrap_if_negative(posaxis[0])\r\n```\r\n\r\nAll of the above applies equally to `ak.firsts` and wherever else negative `axis` is promised but not yet implemented.\r\n\r\n(Thanks for finding this inconsistency! It was left unimplemented to get the most important parts of the project done, and can easily be forgotten. Now we have to clean up and make it agree with the documentation.  `:)`  )",
  "created_at":"2020-12-05T17:16:16Z",
  "id":739323047,
  "issue":574,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTMyMzA0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T17:16:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"That's nice to hear. I can live without it for the moment since I know the depth of my lists. But it'd be definetly a nice thing to have.",
  "created_at":"2020-12-05T17:24:22Z",
  "id":739323898,
  "issue":574,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTMyMzg5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T17:24:22Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"In PR #582, I did what I described above.",
  "created_at":"2020-12-09T00:53:13Z",
  "id":741347835,
  "issue":574,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MTM0NzgzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-09T00:53:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Awesome!",
  "created_at":"2020-12-09T09:17:31Z",
  "id":741642643,
  "issue":574,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MTY0MjY0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-09T09:17:31Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for this long comment. I'm learning a lot. Will add the changes later.\r\n\r\nFor the missing values: Yes, I intended to pass them to the output. This is to undo `ak.flatten` that was called on a masked array.\r\n\r\nI did not include an `axis` parameter, because of the statement \"Such a function wouldn't take an axis\" in #555. Was this parameter discussed somewhere else? Anyhow, I like the idea of having an `axis` parameter in parallel to the ones of `ak.num` and `ak.flatten`. Ultimately I think it would be nice if `unflatten` could take `axis=None` just as `flatten` does. However for that to be possible probably also `num` would need to learn to accept `None`.",
  "created_at":"2020-12-09T17:21:49Z",
  "id":741919711,
  "issue":583,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MTkxOTcxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-09T17:21:49Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"> I did not include an `axis` parameter, because of the statement \"Such a function wouldn't take an axis\" in #555. Was this parameter discussed somewhere else?\r\n\r\nNope, I just forgot! No need to include `axis`, and I'll resolve the conversations about anything to do with `axis`.",
  "created_at":"2020-12-09T17:45:16Z",
  "id":741939466,
  "issue":583,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MTkzOTQ2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-09T17:45:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Marked it a moment too early as \"ready for review\" but now everything should be ready for a review.",
  "created_at":"2020-12-10T19:21:01Z",
  "id":742739214,
  "issue":583,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MjczOTIxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-10T19:21:01Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"I noticed that it didn't have tests, so I added them, and that gave me more things to check. Some of these appeared in the `ak.unflatten` definition, but a few others were in nplike and `to_layout`. Unless you have any complaints, I'll merge this when the tests pass.\r\n\r\nThanks!",
  "created_at":"2020-12-10T20:21:55Z",
  "id":742774978,
  "issue":583,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0Mjc3NDk3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-10T20:21:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Nice! It actually was that issue #585 .",
  "created_at":"2020-12-09T19:05:31Z",
  "id":741984159,
  "issue":586,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MTk4NDE1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-09T19:05:31Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"Why replace `arrayset` with `buffers`? There are some special case arrays that could not be preserved without explicit knowledge of their length, so `length` is now a necessary part of the interface (not just for lazy-reading).\r\n\r\nThe case I noticed was regular, zero-sized dimensions (either as RegularArrays or as the shape of a NumpyArray):\r\n\r\n```python\r\n>>> ak.from_buffers(*ak.to_buffers(ak.Array(np.empty((5, 0, 3)))))\r\n<Array [[], [], [], [], []] type='5 * 0 * 3 * float64'>\r\n\r\n>>> ak.from_buffers(*ak.to_buffers(ak.from_numpy(np.empty((5, 0, 3)), regulararray=True)))\r\n<Array [[], [], [], [], []] type='5 * 0 * 3 * float64'>\r\n```\r\n\r\nBut I had previously overlooked RecordArrays with no fields (or equivalently, zero-field tuples):\r\n\r\n```python\r\n>>> ak.from_buffers(*ak.to_buffers(ak.Array([{}, {}, {}, {}, {}])))\r\n<Array [{}, {}, {}, {}, {}] type='5 * {}'>\r\n>>> ak.from_buffers(*ak.to_buffers(ak.Array([(), (), (), (), ()])))\r\n<Array [(), (), (), (), ()] type='5 * ()'>\r\n```\r\n\r\nThe old `arrayset` interface, without an explicit `length`, either raises exceptions or gets this wrong. It's an exception for NumpyArrays:\r\n\r\n```python\r\n>>> ak.from_arrayset(*ak.to_arrayset(ak.Array(np.empty((5, 0, 3)))))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/operations/convert.py\", line 4171, in from_arrayset\r\n    return from_buffers(\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/operations/convert.py\", line 3727, in from_buffers\r\n    out = _form_to_layout(*(args + (None, None)))\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/operations/convert.py\", line 3417, in _form_to_layout\r\n    array = raw_array.view(dtype).reshape(shape)\r\nValueError: cannot reshape array of size 0 into shape (0,3)\r\n```\r\n\r\nthe wrong answer for RegularArrays (this is supposed to have type `\"5 * 0 * 3 * float64\"`):\r\n\r\n```python\r\n>>> ak.from_arrayset(*ak.to_arrayset(ak.from_numpy(np.empty((5, 0, 3)), regulararray=True)))\r\n<Array [] type='0 * 0 * 3 * float64'>\r\n```\r\n\r\nand RecordArrays with no fields lose their lengths:\r\n\r\n```python\r\n>>> ak.from_arrayset(*ak.to_arrayset(ak.Array([{}, {}, {}, {}, {}])))\r\n<Array [] type='0 * {}'>\r\n>>> ak.from_arrayset(*ak.to_arrayset(ak.Array([(), (), (), (), ()])))\r\n<Array [] type='0 * ()'>\r\n```\r\n\r\nOn top of that, the system for setting the format of key names in `arrayset` was convoluted\u2014several arguments that, when concatenated with or without separators, produced the key\u2014and the `buffers` interface just has a format string to fill (that can be a function returning a string, which has access to the `layout` node for super-customization).\r\n\r\nSince we're living in 1.x times, `ak.to_arrayset`/`ak.from_arrayset` have a formal deprecation process. In 1.0.1rc2 and 1.0.1, they'll start showing warnings that point to `ak.to_buffers`/`ak.from_buffers`. The `arrayset` functions will be removed in the first monthly minor release, **1.1.0** on **February 1, 2021**. (This one has a short fuse because it's not widely used.)\r\n\r\nData pickled with old releases (which goes through `arrayset`) will be supported forever, and they're already being processed by `ak.from_buffers` with the appropriate arguments. If any of those old pickle files fall into the above corner-cases, they'll be misreconstructed just as of old (because the `length` was not saved), but newly pickled data will be correct for all known cases.",
  "created_at":"2020-12-11T21:16:02Z",
  "id":743430896,
  "issue":592,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQzMDg5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T21:16:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@lgray is interested in this! Maybe @nsmith-, too.",
  "created_at":"2020-12-11T19:36:11Z",
  "id":743387347,
  "issue":593,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzM4NzM0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T19:36:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Step 1 in the investigation: it is indeed possible to make pyarrow arrays whose type is non-nullable. You can't say anything about the nullability of the array at top-level, but you can make such statements about nested components.\r\n\r\nWhereas I would ordinarily make a pyarrow ListArray like this:\r\n\r\n```python\r\n>>> import pyarrow\r\n>>> import numpy as np\r\n>>> content = pyarrow.array([1.1, 2.2, 3.3, 4.4, 5.5])\r\n>>> offsets = pyarrow.py_buffer(np.array([0, 3, 3, 5], np.int32))\r\n>>> t0 = pyarrow.list_(content.type)\r\n>>> pyarrow.Array.from_buffers(t0, 3, [None, offsets], children=[content])\r\n<pyarrow.lib.ListArray object at 0x7f6dad2053a0>\r\n[\r\n  [\r\n    1.1,\r\n    2.2,\r\n    3.3\r\n  ],\r\n  [],\r\n  [\r\n    4.4,\r\n    5.5\r\n  ]\r\n]\r\n```\r\n\r\nI can alternatively make them by passing a `pyarrow.field` where the type would ordinarily go. (`t0` is a type, but `t1` and `t2` are field objects.)\r\n\r\n```python\r\n>>> t1 = pyarrow.list_(pyarrow.list_(content.type).value_field)\r\n>>> t2 = pyarrow.list_(pyarrow.list_(content.type).value_field.with_nullable(False))\r\n>>> nullable = pyarrow.Array.from_buffers(t1, 3, [None, offsets], children=[content])\r\n>>> nonnullable = pyarrow.Array.from_buffers(t2, 3, [None, offsets], children=[content])\r\n```\r\n\r\nThese `nullable` and `nonnullable` arrays don't look any different (in their `repr`), but the distinction in type persists:\r\n\r\n```python\r\n>>> nullable.type\r\nListType(list<item: double>)\r\n>>> nonnullable.type\r\nListType(list<item: double not null>)\r\n```\r\n\r\nStep 2: these can be saved to Parquet files and the nullable/non-nullable distinction persists.\r\n\r\n```python\r\n>>> import pyarrow.parquet\r\n>>> pyarrow.parquet.write_table(\r\n...     pyarrow.Table.from_batches([\r\n...         pyarrow.RecordBatch.from_arrays(\r\n...             [nullable, nonnullable],\r\n...             [\"nullable\", \"nonnullable\"])]\r\n...         ),\r\n...     \"testy.parquet\"\r\n... )\r\n```\r\n\r\n```python\r\n>>> import pyarrow.parquet\r\n>>> table = pyarrow.parquet.read_table(\"testy.parquet\")\r\n>>> table\r\npyarrow.Table\r\nnullable: list<item: double>\r\n  child 0, item: double\r\nnonnullable: list<item: double not null>\r\n  child 0, item: double not null\r\n>>> table[\"nullable\"].chunks[0]\r\n<pyarrow.lib.ListArray object at 0x7f7256b32c40>\r\n[\r\n  [\r\n    1.1,\r\n    2.2,\r\n    3.3\r\n  ],\r\n  [],\r\n  [\r\n    4.4,\r\n    5.5\r\n  ]\r\n]\r\n>>> table[\"nonnullable\"].chunks[0]\r\n<pyarrow.lib.ListArray object at 0x7f7256b32d60>\r\n[\r\n  [\r\n    1.1,\r\n    2.2,\r\n    3.3\r\n  ],\r\n  [],\r\n  [\r\n    4.4,\r\n    5.5\r\n  ]\r\n]\r\n>>> table[\"nullable\"].chunks[0].type\r\nListType(list<item: double>)\r\n>>> table[\"nonnullable\"].chunks[0].type\r\nListType(list<item: double not null>)\r\n```\r\n\r\nSo this distinction is expressible in Arrow and Parquet!\r\n\r\nSince I've never checked this attribute, I should be able to entirely transform the `ak.to_arrow` and `ak.to_parquet` functions to include the nullable/non-nullable attribute without modifying the tests, so that would be a good half-way point. Modifying the `ak.from_arrow` and `ak.from_parquet` functions would require the tests to also not include dummy option-type nodes, so the functions and their tests will have to change together.\r\n\r\nBoth list types can accept fields instead of types:\r\n\r\n```python\r\n>>> pyarrow.list_(pyarrow.list_(content.type).value_field.with_nullable(False))\r\nListType(list<item: double not null>)\r\n>>> pyarrow.large_list(pyarrow.list_(content.type).value_field.with_nullable(False))\r\nLargeListType(large_list<item: double not null>)\r\n```\r\n\r\nand everything else takes fields anyway (records and unions). \"Everything else\" excludes `pyarrow.Tensor`, `pyarrow.binary()`, `pyarrow.StringArray`, `pyarrow.large_binary()`, `pyarrow.LargeStringArray`. The inner contents of tensors and strings should all be non-nullable. A possible exception: I don't see a way to do DictionaryArray (i.e. Awkward's \"categorical\"), but I can ignore that for now.\r\n\r\nThe `recurse` function does not need a `nullable` argument in addition to its `mask` argument because the nullability of node `T` is applied at the _parent_ of node `T`, whether that is a list, a struct, or a union.",
  "created_at":"2020-12-14T23:04:44Z",
  "id":744766855,
  "issue":593,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDc2Njg1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-14T23:26:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It should be `Optional[dict]`. I'll do a search through the codebase to find all the places where this happened.\r\n\r\nAdding type annotations everywhere is an option, though I'd have to do Python 2 style for compatibility (yuck).\r\n\r\nI removed the GitHub link from the corner because it was wrong. These are auto-generated files that have no source in GitHub to point to. The closest thing we have is the \"Defined in\" link below the class/function name:\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/101957018-3beec800-3bc6-11eb-949b-9da46b1c6575.png)\r\n\r\nThe \"line\" link goes right to the definition, which has the docstring. It also goes to the right commit.",
  "created_at":"2020-12-11T21:34:11Z",
  "id":743438489,
  "issue":594,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQzODQ4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T21:34:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Since this is a documentation-only fix and I want to release a new release candidate, I think it's okay to commit directly to main (like README and other administrative fixes): https://github.com/scikit-hep/awkward-1.0/commit/68087850eabafd39f0c0de37ed96487fa44cc69f.",
  "created_at":"2020-12-11T21:56:08Z",
  "id":743447586,
  "issue":594,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ0NzU4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T21:56:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I would have thought that this line would include them:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/68087850eabafd39f0c0de37ed96487fa44cc69f/MANIFEST.in#L5\r\n\r\nbut I know that the last time I checked that the source tarball is complete, I didn't check for a samples directory. The effect of it being missing may be different note because before the only tests that used it used Parquet samples _if_ pyarrow is installed (otherwise, those tests are skipped). Now there are JSON files in there that are always tested.\r\n\r\nIt may be that the MANIFEST.in line needs to include\r\n\r\n```\r\ntests/samples/*\r\n```\r\n\r\nbecause the `*.cpp *.py` could be qualifying the `tests` to recursively include, and the sample files don't end with these extensions.\r\n\r\nI can test it, but I'll need to get to a computer Monday morning.",
  "created_at":"2020-12-13T19:05:32Z",
  "id":744053301,
  "issue":596,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDA1MzMwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-13T19:05:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I verified that\r\n\r\n```bash\r\npython setup.py sdist\r\n```\r\n\r\nwithout and with `samples/*` in\r\n\r\n```\r\nrecursive-include tests *.cpp *.py samples/*\r\n```\r\n\r\ndetermines whether the samples are in the source tarball. Today, I had planned to do a 1.0.1 non-release candidate, but I'm going to start with a release candidate to verify the full chain. (I'm applying a new policy in which release candidates\u2014only\u2014can be deleted from PyPI to control my quota usage. The 1.0.1 release has to be exactly right because it will live forever.",
  "created_at":"2020-12-14T17:34:19Z",
  "id":744595391,
  "issue":596,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDU5NTM5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-14T17:34:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Try the source tarball in 1.0.1rc3: https://pypi.org/project/awkward/1.0.1rc3/\r\n\r\n```bash\r\npip install --no-binary awkward \"awkward>=1.0.1rc3\"\r\n```\r\n\r\nI'm working on #597, so the full release is not going out just yet.",
  "created_at":"2020-12-14T18:40:13Z",
  "id":744632711,
  "issue":596,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDYzMjcxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-14T18:40:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I installed it with `--no-binary` on a VM that previously had no Python installed. I don't think this actually runs the tests, but I do know from downloading the tarball directly that the samples files are included and source-installation works:\r\n\r\n```bash\r\nubuntu@ip-172-31-64-157:~$ python3 -m pip install --no-binary awkward \"awkward>=1.0.1rc3\"\r\nCollecting awkward>=1.0.1rc3\r\n  Downloading awkward-1.0.1rc3.tar.gz (815 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 815 kB 14.3 MB/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting numpy>=1.13.1\r\n  Downloading numpy-1.19.4-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.5 MB 33.0 MB/s \r\nBuilding wheels for collected packages: awkward\r\n  Building wheel for awkward (PEP 517) ... done\r\n  Created wheel for awkward: filename=awkward-1.0.1rc3-cp38-cp38-linux_x86_64.whl size=6303076 sha256=258dee1a13545fc78efcce9f18ebd3e3aa993137e3e97c5e4c25b917823e028c\r\n  Stored in directory: /home/ubuntu/.cache/pip/wheels/fe/01/c7/b97ea7f5da917acfbd6ba22d0f228315675d236779760bc178\r\nSuccessfully built awkward\r\nInstalling collected packages: numpy, awkward\r\n  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/home/ubuntu/.local/bin' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\nSuccessfully installed awkward-1.0.1rc3 numpy-1.19.4\r\n```\r\n\r\nWhen I download the tarball onto this machine and run pytest on the `tests` directory it contains (with `samples` within that), all pass, even with all extra dependencies. So I guess this is okay...\r\n\r\nI'm going to release `awkward==1.0.1` (full release).",
  "created_at":"2020-12-14T20:17:32Z",
  "id":744685281,
  "issue":596,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDY4NTI4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-14T20:17:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski Many thanks! I can confirm that as of version 1.0.1, the PyPI tarballs do contain the samples dir. Thanks too for the great library, btw.",
  "created_at":"2020-12-14T21:37:38Z",
  "id":744726519,
  "issue":596,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDcyNjUxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-14T21:37:38Z",
  "user":"MDQ6VXNlcjM1MzI0Njc="
 },
 {
  "author_association":"MEMBER",
  "body":"Since `ak.from_buffers` is the main thing I want to release in 1.0.1, I fixed this one before that release.\r\n\r\nAs it turns out, ListArray (and only ListArray) was missing a stanza that loosens the comparison between expected Form and generated Form, allowing a VirtualArray to be considered the same as the array the VirtualArray generates.\r\n\r\n~~But looking closely at this, it also bothers me that a VirtualArray and its content have the same `form_key`. Those are supposed to be unique. A VirtualArray has nothing to store in the container, like RecordArray, so it should probably have _no_ `form_key`, like RecordArray. I'm going to check to see if that passes tests.~~\r\n\r\nI was mistaken: that never happens.\r\n\r\n(Since this is a format that will have to be supported indefinitely (to read pickled arrays written by any version), it's worth getting it right.)",
  "created_at":"2020-12-14T19:11:22Z",
  "id":744649222,
  "issue":597,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDY0OTIyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-14T19:20:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for checking this! I occasionally check the tests in a 32-bit Linux Docker container, which should perhaps become a CI test.\r\n\r\nIn this particular case, I think the error is here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e1970c0542002725b647b210f595dbd808c53565/src/awkward/operations/convert.py#L4343-L4344\r\n\r\n`numpy.repeat` needs special handling for Windows (the `counts` array has to be 32-bit for _all_ versions of Windows), and I'm 90% sure that the same would apply to 32-bit Linux.\r\n\r\nThe same is probably true here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/98d870a45666706567e8eb1301ccc1e9337d03b0/src/awkward/behaviors/string.py#L158-L159\r\n\r\nthough I suppose the tests didn't catch it. (There are only two places in the codebase where `numpy.repeat` is used because I try to do most of the structure-changing operations in C++, not NumPy. These were for expedience, but the cost is that some NumPy operations are not platform-independent.)",
  "created_at":"2020-12-15T00:57:05Z",
  "id":744922469,
  "issue":600,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NDkyMjQ2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T00:57:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I confirm your PR fixes the failing tests on `i586`. All tests now pass. Thanks a lot.",
  "created_at":"2020-12-16T00:32:27Z",
  "id":745685264,
  "issue":600,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTY4NTI2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-16T00:32:27Z",
  "user":"MDQ6VXNlcjM1MzI0Njc="
 },
 {
  "author_association":"MEMBER",
  "body":"Great!",
  "created_at":"2020-12-16T00:50:56Z",
  "id":745691611,
  "issue":600,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTY5MTYxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-16T00:50:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna Here's another one that I just remembered (because I'm working on #593).",
  "created_at":"2020-12-15T16:36:51Z",
  "id":745410686,
  "issue":601,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTQxMDY4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T16:36:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It is quite fortunate that all the old Arrow tests checked for agreement at the level of Python lists. Changing the nullability/option-type of data at various levels doesn't affect any of those tests (but the new tests require the right types).",
  "created_at":"2020-12-15T20:13:05Z",
  "id":745539647,
  "issue":602,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTUzOTY0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T20:13:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@lgray and @nsmith- I just noticed that Arrow can _finally_ convert records nested within variable-length lists! (That is, particles in events!) I've been waiting for that since\u2014searches email\u2014August 2016.\r\n\r\nBehold:\r\n\r\n```python\r\noriginal = ak.Array(\r\n    [\r\n        [{\"x\": 1, \"y\": 1.1}, {\"x\": 2, \"y\": 2.2}, {\"x\": 3, \"y\": 3.3}],\r\n        [],\r\n        [{\"x\": 4, \"y\": 4.4}, {\"x\": 5, \"y\": 5.5}],\r\n        [],\r\n        [],\r\n        [\r\n            {\"x\": 6, \"y\": 6.6},\r\n            {\"x\": 7, \"y\": 7.7},\r\n            {\"x\": 8, \"y\": 8.8},\r\n            {\"x\": 9, \"y\": 9.9},\r\n        ],\r\n    ]\r\n)\r\n\r\nak.to_parquet(original, os.path.join(tmp_path, \"data.parquet\"))\r\nreconstituted = ak.from_parquet(os.path.join(tmp_path, \"data.parquet\"))\r\nassert reconstituted.tolist() == [\r\n    [{\"x\": 1, \"y\": 1.1}, {\"x\": 2, \"y\": 2.2}, {\"x\": 3, \"y\": 3.3}],\r\n    [],\r\n    [{\"x\": 4, \"y\": 4.4}, {\"x\": 5, \"y\": 5.5}],\r\n    [],\r\n    [],\r\n    [\r\n        {\"x\": 6, \"y\": 6.6},\r\n        {\"x\": 7, \"y\": 7.7},\r\n        {\"x\": 8, \"y\": 8.8},\r\n        {\"x\": 9, \"y\": 9.9},\r\n    ],\r\n]\r\nassert str(reconstituted.type) == '6 * var * {\"x\": int64, \"y\": float64}'\r\n```\r\n\r\nThe part I did was making sure that it comes back without option-types, but it's more of a big deal that we can write and read back types like `N * var * {some record}` now.\r\n\r\nLet me try another one:\r\n\r\n```python\r\n>>> events = ak.Array([\r\n...     {\"MET\": 1, \"muons\": [{\"px\": 1, \"py\": 1}, {\"px\": 2, \"py\": 2}], \"jets\": [{\"x\": 1, \"y\": 1}]},\r\n...     {\"MET\": 2, \"muons\": [{\"px\": 3, \"py\": 3}, {\"px\": 4, \"py\": 4}], \"jets\": [{\"x\": 2, \"y\": 2}]}])\r\n>>> ak.to_parquet(events, \"events.parquet\")\r\n>>> events2 = ak.from_parquet(\"events.parquet\")\r\n>>> events2.tolist()\r\n[{'MET': 1, 'muons': [{'px': 1, 'py': 1}, {'px': 2, 'py': 2}], 'jets': [{'x': 1, 'y': 1}]},\r\n {'MET': 2, 'muons': [{'px': 3, 'py': 3}, {'px': 4, 'py': 4}], 'jets': [{'x': 2, 'y': 2}]}]\r\n>>> events2.type\r\n2 * {\"MET\": int64, \"muons\": var * {\"px\": int64, \"py\": int64}, \"jets\": var * {\"x\": int64, \"y\": int64}}\r\n```\r\n",
  "created_at":"2020-12-15T23:10:33Z",
  "id":745624448,
  "issue":602,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTYyNDQ0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T23:10:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"![image](https://user-images.githubusercontent.com/1068089/102284882-aa02fa00-3efa-11eb-91e2-de2e75b43d70.png)\r\n",
  "created_at":"2020-12-15T23:26:33Z",
  "id":745630694,
  "issue":602,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTYzMDY5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T23:26:33Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I would also like to add a case in conjunction with `ak.num`\r\n```python\r\nform[\"contents\"][\"Muon\"][\"content\"][\"parameters\"][\"__record__\"] = \"Muon\"\r\narr = ak.from_buffers(form, 3, map, lazy=True)\r\ncon = ak.concatenate([arr[\"Electron\"], arr[\"Muon\"]], axis=1)\r\ncon[\"var\"] = 5\r\nprint(\"Getting counts\")\r\nak.num(con)\r\n```\r\nHere the `ak.num` call will also make all contents being loaded, which is also not desirable. This does not happen if `\"var\"` isn't added to the array.",
  "created_at":"2020-12-15T19:57:59Z",
  "id":745531549,
  "issue":603,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTUzMTU0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T19:57:59Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"This is part of defining what `ak.concatenate` is supposed to mean. There are two ways to put arrays one after another: as a partitioning ([ak.partitioned](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partitioned.html)), which doesn't make the arrays contiguous in memory, it just chains them logically, mapping global indexes to local indexes, and [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html), which eagerly makes them contiguous in memory.\r\n\r\nCreating a partitioned array scales with the number of partitions but not the size of the data in those arrays. Concatenating scales with the size of the data in the arrays because it's actually making them contiguous (by column, as everything is only ever contiguous by column). However, partitioning must be at the outermost level of a tree structure. (In Awkward 0, partitioning was allowed anywhere, and it led to numerous gotchas and bugs.)\r\n\r\nThere's a third option, but it's not a good one: a set of RecordArrays could be combined as a UnionArray with tag `0` for the first RecordArray, tag `1` for the second RecordArray, etc. That would ensure that the RecordArrays are combined lazily, but UnionArrays have some limitations, like you can't use them in Numba. So we'll discard that option.\r\n\r\nHere's a better option: instead of having the `ak.concatenate` function merge all partitions as a first step,\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e1970c0542002725b647b210f595dbd808c53565/src/awkward/operations/structure.py#L1094-L1097\r\n\r\nif the `axis == 0` it should string partitioned data together into a bigger partitioning. For that to work consistently, `ak.from_buffers` with `lazy == True` should always return a partitioned array, even if it's only a single partition, rather than returning non-partitioned data as it does now:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e1970c0542002725b647b210f595dbd808c53565/src/awkward/operations/convert.py#L3725-L3732\r\n\r\nSame for `ak.from_parquet`, in which single-partition outputs are currently thwarted:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e1970c0542002725b647b210f595dbd808c53565/src/awkward/operations/convert.py#L2686-L2689",
  "created_at":"2020-12-15T20:01:19Z",
  "id":745533270,
  "issue":603,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NTUzMzI3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-15T20:01:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've been meaning to do this one \"next\" for a whole week now. There were some deeper things to fix along the way, but now this should be good.\r\n\r\nThe reason that concatenating electrons and muons did not cause the [VirtualArrays](https://awkward-array.readthedocs.io/en/latest/ak.layout.VirtualArray.html) to materialize but concatenating electrons and electrons or muons and muons did is because concatenation for `axis != 0` works by first building a [UnionArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.UnionArray.html) with the data interleaved:\r\n\r\n```python\r\n>>> one = ak.Array([[1, 2, 3], [], [4, 5]])\r\n>>> two = ak.Array([[10, 20, 30], [], [40, 50]])\r\n>>> ak.concatenate([one, two], axis=1, merge=False).layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 6 6 10]\" offset=\"0\" length=\"4\" at=\"0x563b8df5cdd0\"/></offsets>\r\n    <content><UnionArray8_64>\r\n        <tags><Index8 i=\"[0 0 0 1 1 1 0 0 1 1]\" offset=\"0\" length=\"10\" at=\"0x563b8de1d8e0\"/></tags>\r\n        <index><Index64 i=\"[0 1 2 0 1 2 3 4 3 4]\" offset=\"0\" length=\"10\" at=\"0x563b8d9af450\"/></index>\r\n        <content tag=\"0\">\r\n            <NumpyArray format=\"l\" shape=\"5\" data=\"1 2 3 4 5\" at=\"0x563b8df849a0\"/>\r\n        </content>\r\n        <content tag=\"1\">\r\n            <NumpyArray format=\"l\" shape=\"5\" data=\"10 20 30 40 50\" at=\"0x563b8df93950\"/>\r\n        </content>\r\n    </UnionArray8_64></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\nand then \"simplifying\" the UnionArray:\r\n\r\n```python\r\n>>> ak.concatenate([one, two], axis=1, merge=True).layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 6 6 10]\" offset=\"0\" length=\"4\" at=\"0x563b8ddf5910\"/></offsets>\r\n    <content><NumpyArray format=\"l\" shape=\"10\" data=\"1 2 3 10 20 30 4 5 40 50\" at=\"0x563b8dea7920\"/></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\nbut the \"`Muon`\" and \"`Electron`\" records are considered different types, so they weren't simplified/merged into the same array buffers, while the \"`Muon`\" and \"`Muon`\" and the \"`Electron`\" and \"`Electron`\" are considered the same type, so they were simplified/merged.\r\n\r\nThe first thing I did was to ensure that if you have partitioned data, which is usually the case for lazy arrays, that `axis=0` concatenation just puts the partitions one after another. Earlier, I had misread your example, thinking they were `axis=0`, which absolutely should do this, at least. If you start non-partitioned data, I do not think that [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) should just be gluing them into partitions\u2014that's why there's a difference between the [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) and [ak.partitioned](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partitioned.html) functions.\r\n\r\nBut you had `axis=1`, which is more subtle. _Some_ of the virtual data must be materialized to perform that operation: the [ListOffsetArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html) offsets, at minimum. In your example, it was also materializing all the fields of the [RecordArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.RecordArray.html), which it really didn't need to do.\r\n\r\nAs I said, it was merging \"`events.Muon`\" and \"`events.Muon`\" because it has the same type as itself. However, if you're concatenating something with itself, I think you'll want to use indirection, rather than an explicit merge, since that indirection would be an [IndexedArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedArray.html), rather than a [UnionArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.UnionArray.html). (There are reasons to avoid UnionArrays, if possible; for example, they can't be used in Numba: #174.) So now there's a new internal function that can determine if two arrays are referentially equal (like a deeply nested version of the Python `is`).\r\n\r\nThat made your examples work \"out of the box,\" without manual intervention, because\r\n\r\n   * \"`events.Muon`\" and \"`events.Electron`\" are different types (because they have a different `__record__` parameter)\r\n   * \"`events.Muon`\" and \"`events.Muon`\" are referentially equal\r\n\r\nHowever, if the first of these hadn't been labeled with a different `__record__` parameter, they'd be merged, causing materialization of all fields. That's not something we can avoid by default because it goes against the meaning of \"concatenate\". If any data, regardless of referential identity or data type, is not simplified/merged by [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html), there would be no way for a user to ask for it!\r\n\r\nTherefore, I added `merge` as an option to `concatenate`, to prevent recursive merging if you really want to. I set it up to always eliminate UnionArray nested within UnionArray (that's bad), but if there are any different arrays with the same type `concatenate`'s intermediate step, setting `merge=False` will make them _not_ be combined.\r\n\r\nIt was also the only way I could make the first example, above, without building the UnionArrays by hand.",
  "created_at":"2020-12-22T01:54:40Z",
  "id":749293049,
  "issue":603,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0OTI5MzA0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-22T01:54:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thank you for this fix and for working on the conda-forge!\r\n\r\nMy default is to \"squash and merge\", and I'll do that now.",
  "created_at":"2020-12-16T18:46:59Z",
  "id":746812139,
  "issue":605,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NjgxMjEzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-16T18:46:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! I think what happened here is that all my testing systems already had this installed.",
  "created_at":"2020-12-19T14:24:39Z",
  "id":748481416,
  "issue":611,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODQ4MTQxNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-12-19T14:24:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna, since the NumPy default is \"quicksort\", I can't do this until a quicksort exists.\r\n\r\nBut we [now have a formal roadmap](https://github.com/scikit-hep/awkward-1.0#roadmap), so if quicksort is available in January or February, then we can set the deprecation for 1.2.0 (April, 2021).",
  "created_at":"2020-12-22T21:22:08Z",
  "id":749781984,
  "issue":615,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0OTc4MTk4NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-12-22T21:22:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As discussed in https://github.com/scikit-hep/awkward-1.0/discussions/614#discussioncomment-235312, this will need a deprecation cycle that starts by a phase of warnings against using `SliceFields` followed by `SliceField` or `SliceFields` now and the new feature would only be added after the deprecation cycle is over (because it uses the same syntax and there are no names to change).\r\n\r\nWe now have a [formal roadmap](https://github.com/scikit-hep/awkward-1.0#roadmap) (#617), so if this is going to go into 1.1.0 (Feb 1, 2021), then the warning has to be installed in a full release (1.0.2) _now_, before the break.",
  "created_at":"2020-12-23T16:11:39Z",
  "id":750368020,
  "issue":618,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MDM2ODAyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-23T16:11:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"My intention was to quickly make a release with the deprecation warning, to give people as much time as possible to transition. I was first going to implement it and then switch it off, so that turning on this feature would be easier when the date arrived.\r\n\r\nHowever, in the course of implementing it, I had a chance to think more deeply about it, particularly what the deprecation message was going to say. In so doing, I noticed that the old behavior is nonsensical: there is _no_ reason why anyone would want\r\n\r\n```python\r\n>>> array[[\"x\", \"y\", \"z\"], \"x\"]\r\n```\r\n\r\nto select from the `[\"x\", \"y\", \"z\"]` record they've just created, rather than looking for a sub-field `\"x\"` within each of the outer `\"x\"`, `\"y\"`, and `\"z\"` fields. In particular,\r\n\r\n```python\r\n>>> array[\"x\", \"y\", \"z\"], \"w\"]\r\n```\r\n\r\nis guaranteed to be an error (in the old behavior) because you've just ensured that the outer record has only fields `\"x\"`, `\"y\"`, and `\"z\"`, so selecting `\"w\"` never makes sense. The same is true if the second field-selector was a list. The old behavior _only_ allows you to make subsets from the set you've just defined. Who would want that?\r\n\r\nOn the other hand, the new behavior of having the second field-selector look inside each of the objects created by the first field-selector has the application you described (@maxgalli) and it's more like what a tuple of row selectors does.\r\n\r\n**So I hereby redefine the old behavior as a bug.** The new behavior is effective immediately (as soon as #619 enters a release).\r\n\r\nIf anyone was _relying_ on the old behavior, I'll point out that they can adapt by simply collapsing \"`[\"x\", \"y\", \"z\"], \"x\"`\" into `\"x\"`. That works regardless of version; they weren't getting anything with this syntax's old behavior that they couldn't get with simpler syntax.",
  "created_at":"2020-12-23T19:59:42Z",
  "id":750451521,
  "issue":618,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MDQ1MTUyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-23T19:59:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna This is a conceptual start on the array-filling interface from Uproot to Awkward Array that we talked about last Tuesday.\r\n\r\nAs a reminder, the motivation is to try to keep Uproot and Awkward Array decoupled. I've made mistakes in the past where Uproot has used Awkward implementation details, which then broke when the wrong versions of Uproot and Awkward Array were used together. Although the new Uproot 4 uses only Awkward's public API, next year's project, in which Uproot offloads ROOT TBaskets to Awkward Array for filling, gets dangerously close to too much coupling. The prototype (scikit-hep/uproot4#96) puts explicit knowledge of how TTrees are serialized in Awkward Array. (In the following, `0x40000000` and 2-byte headers are ROOT details, which should stay in Uproot, not Awkward Array.)\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/257b24602c343e8420c30499ade40bb7bce1c429/src/libawkward/io/uproot.cpp#L64-L91\r\n\r\nThat sort of thing should be exclusively contained within Uproot.\r\n\r\nSo the idea I've been thinking about for decoupling them is to have Uproot create a sequence of ROOT-deserialization instructions that Awkward Array can follow in compiled code. \"Instructions,\" in a general sense, is \"code\"\u2014Uproot has to send Awkward Array a program. Since the motivation for this is performance (currently, Uproot is generating these instructions as Python code, which it executes using Python's `exec`), it wouldn't help much for these \"instructions\" to be something that has to be interpreted (after all, Python is interpreted by a C program, the Python interpreter). Ideally, we might like to just-in-time compile these instructions, but Awkward Array doesn't come with a built-in compiler.\r\n\r\nThat's why I've been thinking about this idea of using Forth. Forth is an extremely simple language\u2014it has _less_ syntax than LISP!\u2014and it's a half-step above assembly language, a half-step below a conventional language like C or Fortran. The following is a great introduction:\r\n\r\nhttps://skilldrick.github.io/easyforth/\r\n\r\nInstead of having proper functions with arguments, it has a single stack and \"words\" that manipulate that stack. It's an untyped language in the sense that the stack has only one type: integers (typically 16-bit, but we'll be using 64-bit integers). Beyond that, we get to decide what the builtin words are and how they should be wired up.\r\n\r\nHere's how I'm thinking it should work: Uproot determines what kind of [Form](https://awkward-array.readthedocs.io/en/latest/ak.forms.Form.html) it needs to fill and how to traverse the ROOT TBaskets to fill them, in the form of a Forth program. Uproot sends the raw TBaskets and the Form to Awkward Array. Awkward Array makes [GrowableBuffers](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1GrowableBuffer.html) for each node in the Form, compiles the Forth program, and runs it. Our version of the Forth language includes builtin words for reading from the TBasket data, possibly putting these values on the stack (as 64-bit integers), or putting them into the GrowableBuffers (as whatever types they need to be). Awkward Array then assembles the array from the GrowableBuffers and the Form and sends it back to Uproot.\r\n\r\nHere's why Forth seems like a good option:\r\n\r\n   * Whereas declarative or functional languages endeavor to be side-effect free, Forth operates entirely through side-effects. Each word pulls a value off of the TBasket (updating its seek position in-place), pushes or pops values on the stack (for scratch-work), or pushes a value onto one of the GrowableBuffers. Everything we _want_ to do is a side-effect, because our main interest here is I/O. Forth seems like a better fit to this task than a declarative/functional language.\r\n   * Forth is simple enough to minimize overheads when evaluating it. Documentation about Forth calls it an \"interpreted\" language, but these words can be mapped 1:1 onto integer instructions for a simple (stack-based) virtual machine. I would call that \"compiled,\" in the same sense as Java, but for an extremely simple machine like NumExpr. The hot loop is a double `while` loop over a `dictionary`, which can be implemented as a `std::vector<std::vector<int>>`. Here's what it looks like in Python:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/16c08f74c4f42262bee04cdfec24ad86d3f64c38/studies/awkward-forth/virtual-machine.py#L157-L190\r\n\r\nThe `if ... else if` can become a `switch` statement, which a C++ compiler knows how to optimize. The `which` and `where` are stacks of integers, which also have optimized data structures in the C++ STL. Even with this minimal runtime environment, we can compile and run programs that only use builtin words:\r\n\r\n```python\r\n>>> vm.do(\"3 2 +\", verbose=True)\r\ninstruction          | stack before instruction\r\n---------------------+-------------------------\r\n3                    | <- top\r\n2                    | 3 <- top\r\n+                    | 3 2 <- top\r\n                     | 5 <- top\r\n```\r\n\r\nPrograms that define and use a function:\r\n\r\n```python\r\n>>> vm.do(\": foo 3 2 + ; foo\", verbose=True)\r\ninstruction          | stack before instruction\r\n---------------------+-------------------------\r\nfoo                  | \r\n  3                  | <- top\r\n  2                  | 3 <- top\r\n  +                  | 3 2 <- top\r\n                     | 5 <- top\r\n```\r\n\r\nNested function definitions:\r\n\r\n```python\r\n>>> vm.do(\": foo : bar 1 + ; 3 2 + ; foo bar\", verbose=True)\r\ninstruction          | stack before instruction\r\n---------------------+-------------------------\r\nfoo                  | \r\n  3                  | <- top\r\n  2                  | 3 <- top\r\n  +                  | 3 2 <- top\r\nbar                  | \r\n  1                  | 5 <- top\r\n  +                  | 5 1 <- top\r\n                     | 6 <- top\r\n```\r\n\r\nAnd recursion (I haven't defined an `if` builtin word yet, so there's no way to stop the recursion gracefully):\r\n\r\n```python\r\n>>> vm.do(\": foo + foo ; 1 2 3 4 5 foo\", verbose=True)\r\ninstruction          | stack before instruction\r\n---------------------+-------------------------\r\n1                    | <- top\r\n2                    | 1 <- top\r\n3                    | 1 2 <- top\r\n4                    | 1 2 3 <- top\r\n5                    | 1 2 3 4 <- top\r\nfoo                  | \r\n  +                  | 1 2 3 4 5 <- top\r\n  foo                | \r\n    +                | 1 2 3 9 <- top\r\n    foo              | \r\n      +              | 1 2 12 <- top\r\n      foo            | \r\n        +            | 1 14 <- top\r\n        foo          | \r\n          +          | 15 <- top\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<string>\", line 160, in do\r\n  File \"<string>\", line 112, in run\r\n  File \"<string>\", line 20, in pop\r\nValueError: stack underflow\r\n```\r\n\r\nThe rest is adding builtin words to make this language usable for ROOT I/O. I'm going to develop this prototype by testing it with complex data from Uproot, to make sure that it has enough builtin words to parse data like that. When it's clear that it will be capable of all the use-cases Uproot requires, we can think about ways to optimize it in C++.",
  "created_at":"2020-12-24T01:39:48Z",
  "id":750698457,
  "issue":620,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MDY5ODQ1Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-12-24T01:46:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Interesting ROOT files to try parsing with Forth:\r\n\r\n```python\r\n>>> import uproot\r\n>>> import skhep_testdata\r\n\r\n>>> # A good place to start.\r\n>>> vector_of_vectors = uproot.open(skhep_testdata.data_path(\"uproot-vectorVectorDouble.root\"))[\"t/x\"]\r\n>>> vector_of_vectors.array().tolist()\r\n[[], [[], []], [[10.0], [], [10.0, 20.0]], [[20.0, -21.0, -22.0]], [[200.0], [-201.0], [202.0]]]\r\n\r\n>>> # Everything in this tree is an interesting data structure.\r\n>>> stl_containers = uproot.open(skhep_testdata.data_path(\"uproot-stl_containers.root\"))[\"tree\"]\r\n\r\n>>> # A nested record with non-trivial stuff in it that isn't overwhelmingly large.\r\n>>> f = uproot.open(skhep_testdata.data_path(\"uproot-issue46.root\"))\r\n>>> f[\"tree/evt\"].array().type\r\n100 * Event[\"ArrayBool\": 10 * bool, \"Bool\": bool, \"N\": int64, \"SliceBool\": var * bool, \"StlVecBool\": var * bool]\r\n\r\n# Like the stl_containers, but a bit larger.\r\n>>> f = uproot.open(skhep_testdata.data_path(\"uproot-issue243-new.root\"))\r\n>>> f[\"triggerList/triggerMap.first\"].array().type\r\n10252 * var * string\r\n>>> f[\"triggerList/triggerMap.second\"].array().type\r\n10252 * var * float64\r\n\r\n>>> # None of the following have been interpreted correctly by the \"interpret\" function.\r\n>>> # Same with uproot-issue434.root, though it looks like Uproot 3 managed it?\r\n>>> f = uproot.open(skhep_testdata.data_path(\"uproot-issue124.root\"))\r\n>>> f[\"KM3NET_TIMESLICE\"].keys()\r\n['KM3NET_TIMESLICE',\r\n 'KM3NET_TIMESLICE/KM3NETDAQ::JDAQPreamble',\r\n 'KM3NET_TIMESLICE/KM3NETDAQ::JDAQTimesliceHeader',\r\n 'KM3NET_TIMESLICE/vector<KM3NETDAQ::JDAQSuperFrame>']\r\n```\r\n\r\nFor some whopping huge examples (performance scaling), there's the `std::vector^N<float>` from the CHEP 2019 studies. I think I have a copy. Otherwise, they need to be regenerated from the code in [this directory](https://github.com/scikit-hep/awkward-1.0/tree/main/studies/chep-2019).",
  "created_at":"2020-12-31T19:54:09Z",
  "id":753057733,
  "issue":620,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzA1NzczMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-31T19:54:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Note to self: this is how to get the raw data for the Forth interpreter to work on.\r\n\r\n```python\r\n>>> vector_of_vectors.basket(0)\r\n<TBasket 0 of 'x' at 0x7843507860d0>\r\n>>> vector_of_vectors.basket(0).data\r\narray([ 64,   0,   0,   6,   0,   9,   0,   0,   0,   0,  64,   0,   0,\r\n        14,   0,   9,   0,   0,   0,   2,   0,   0,   0,   0,   0,   0,\r\n         0,   0,  64,   0,   0,  42,   0,   9,   0,   0,   0,   3,   0,\r\n         0,   0,   1,  64,  36,   0,   0,   0,   0,   0,   0,   0,   0,\r\n         0,   0,   0,   0,   0,   2,  64,  36,   0,   0,   0,   0,   0,\r\n         0,  64,  52,   0,   0,   0,   0,   0,   0,  64,   0,   0,  34,\r\n         0,   9,   0,   0,   0,   1,   0,   0,   0,   3,  64,  52,   0,\r\n         0,   0,   0,   0,   0, 192,  53,   0,   0,   0,   0,   0,   0,\r\n       192,  54,   0,   0,   0,   0,   0,   0,  64,   0,   0,  42,   0,\r\n         9,   0,   0,   0,   3,   0,   0,   0,   1,  64, 105,   0,   0,\r\n         0,   0,   0,   0,   0,   0,   0,   1, 192, 105,  32,   0,   0,\r\n         0,   0,   0,   0,   0,   0,   1,  64, 105,  64,   0,   0,   0,\r\n         0,   0], dtype=uint8)\r\n>>> vector_of_vectors.basket(0).byte_offsets\r\narray([  0,  10,  28,  74, 112, 158], dtype=int32)\r\n```",
  "created_at":"2020-12-31T19:56:56Z",
  "id":753061558,
  "issue":620,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzA2MTU1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-31T19:56:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I actually discovered this with jagged arrays, but put the minimal flattened example above.\r\n```python\r\n>>> a = ak.to_arrow(ak.Array([[1],[],[2,3,4],[5]]))\r\n>>> a\r\n<pyarrow.lib.ListArray object at 0x18fe05288>\r\n[\r\n  [\r\n    1\r\n  ],\r\n  [],\r\n  [\r\n    2,\r\n    3,\r\n    4\r\n  ],\r\n  [\r\n    5\r\n  ]\r\n]\r\n\r\n>>> ak.from_arrow(a[2:])\r\n<Array [[1], []] type='2 * option[var * ?int64]'>\r\n```\r\n\r\nMy use case is putting jagged arrays into pandas dataframes without copying (or making list of lists) by converting awkward to Arrow and then using pyarrow's [fletcher](https://github.com/xhochy/fletcher) pandas ExtensionArray to get stuff like this:\r\n\r\n|    | Jet_pt                                                            |   MET_pt |\r\n|---:|:------------------------------------------------------------------|---------:|\r\n|  0 | [39.65625  29.9375   23.09375  18.0625   17.6875   15.609375]     |  46.0749 |\r\n|  1 | [76.75      61.84375   58.875     52.0625    22.171875  17.28125  |  29.7964 |\r\n|    |  17.09375   15.453125  15.2578125]                                |          |\r\n|  2 | [23.109375  16.453125  15.5703125]                                |  82.0033 |\r\n|  3 | [59.8125    45.3125    35.21875   22.03125   19.046875  15.796875 |  77.7026 |\r\n|    |  15.6328125]                                                      |          |\r\n|  4 | [55.71875   47.84375   44.46875   42.875     25.578125  22.953125 |  29.9007 |\r\n|    |  20.890625  19.5       15.8671875]                                |          |\r\n\r\nAnd slicing on this gives me the expected Arrow arrays, but converting back to awkward in order to actually compute things, fails:\r\n```python\r\n>>> ak.from_arrow(df.iloc[2:][\"Jet_pt\"].values.data)\r\n<Array [[39.7, 29.9, 23.1, ... 16.5, 15.6]] type='3 * option[var * ?float32]'>\r\n```",
  "created_at":"2020-12-29T05:21:12Z",
  "id":751950283,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MTk1MDI4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-29T05:21:56Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"MEMBER",
  "body":"From @trickarcher:\r\n\r\n> Hey Jim, I was going through the Issue #624 and it was related to `ak.from_arrow`.  I was able to reproduce the issue. I tried forming sliced array from buffers in the pyarrow ecosystem, and got a similar failure.\r\n\r\n```python\r\n >>> import awkward as ak\r\n >>> import pyarrow as pa\r\n >>> pa_arr=pa.array([1,2,3,4,5,6,7,8,9,10])\r\n >>> pa_arr\r\n <pyarrow.lib.Int64Array object at 0x7f3a0d009b20>\r\n [\r\n   1,\r\n   2,\r\n   3,\r\n   4,\r\n   5,\r\n   6,\r\n   7,\r\n   8,\r\n   9,\r\n   10\r\n ]\r\n >>> pa_arr.buffers()\r\n [None, <pyarrow.lib.Buffer object at 0x7f3a05b960f0>]\r\n >>> pa_arr[5:].buffers()\r\n [None, <pyarrow.lib.Buffer object at 0x7f3a05b8cd70>]\r\n >>> import numpy as np\r\n >>> pa.Array.from_buffers(pa.from_numpy_dtype(np.int64), 5, pa_arr[5:].buffers())\r\n <pyarrow.lib.Int64Array object at 0x7f3a05b95700>\r\n [\r\n   1,\r\n   2,\r\n   3,\r\n   4,\r\n   5\r\n ]\r\n```\r\n\r\n> However, the sliced buffers are totally different from the original buffers. How can we debug further from here?\r\n\r\nActually, this example and @aminnj's are both showing the same thing: pyarrow's buffers are defined by a pointer, a length, and an offset (much like ours in Awkward, which we call `ptr`, `length`, and `offset` in C++). When you and I wrote `from_arrow`, we assumed that pyarrow's `buffers()` was giving offset-adjusted pointers (like our `data()`), but apparently it was giving non-adjusted pointers (like our `ptr`).\r\n\r\nThat's why `pa.array(np.arange(10))[6:]` is being turned into `ak.Array([0, 1, 2, 3])` instead of `ak.Array([6, 7, 8, 9])`. It has the right buffer and the right length (4), but it should be starting at 6, not starting at 0.\r\n\r\nSo the problem is that we're interpreting the output of pyarrow's `buffers()` incorrectly\u2014we additionally need to get an offset number from somewhere. I don't know where that is, but the way to search for it is to open this up interactively and call `dir` on things. It's a number that would be 6 (items) or 48 (bytes) in the example above, possibly on the pyarrow object, possibly on the Python buffer returned by `buffers()`, possibly somewhere else, but accessible from the pyarrow object.\r\n\r\nWe'll have to check this on pyarrow arrays with missing values, since they do this with a bitmask, which can't be sliced at non-byte boundaries. (In practice, we'd have to build the Awkward Array node with a `length + offset`, then slice the Awkward node by the `offset`. We'd only want to do that if the offset is non-zero, since that would force a conversion of the BitMaskArray into a ByteMaskArray to be able to cut it at arbitrary places, and that's pointless when the place is zero.)\r\n\r\nWe didn't notice it before because we weren't slicing pyarrow arrays before converting them. This is just another illustration of how inadequate even 100% test coverage is! (Remember how I added a bunch of tests to make sure that every line of code on the function is tested? That doesn't help if the thing we're missing is a value, like this offset not being zero!)\r\n\r\nI'm not in a good position to investigate this now, but let me know here if you find something. Thanks!",
  "created_at":"2020-12-29T16:34:34Z",
  "id":752149153,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjE0OTE1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-29T16:34:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I got a chance to look at it on my computer, and I can confirm:\r\n\r\n```python\r\n>>> array = pa.array(np.arange(10))[6:]\r\n>>> array\r\n<pyarrow.lib.Int64Array object at 0x7fcc999f4280>\r\n[\r\n  6,\r\n  7,\r\n  8,\r\n  9\r\n]\r\n>>> mask, content = array.buffers()\r\n>>> mask\r\n>>> content\r\n<pyarrow.lib.Buffer object at 0x7fcc99d053b0>\r\n>>> np.frombuffer(content, dtype=np.int64)\r\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n```\r\n\r\nThe Python buffer object returned by `array.buffers()` hides Arrow's equivalent of an \"offset\" parameter. It also hides the \"length\"; we must be getting that from `len(array)` (which is `4`).",
  "created_at":"2020-12-29T17:56:54Z",
  "id":752183712,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjE4MzcxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-29T17:56:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Here it is: it's an attribute called `offset`, which is attached to the pyarrow array itself:\r\n\r\n```python\r\n>>> array.offset\r\n6\r\n>>> pa.array(np.arange(10)).offset\r\n0\r\n>>> pa.array(np.arange(10))[6:].offset\r\n6\r\n>>> pa.array(np.arange(10))[2:].offset\r\n2\r\n```",
  "created_at":"2020-12-29T18:00:11Z",
  "id":752184883,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjE4NDg4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-29T18:00:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim, thanks for such a fast response. I was playing a bit more yesterday and for flat arrays, found I could do\r\n```python\r\n>>> import pyarrow as pa\r\n>>> import awkward1 as ak\r\n>>> import numpy as np\r\n>>> orig = pa.Array.from_pandas([1,2,3,4,5])\r\n>>> a = orig[2:4]\r\n>>> _, buffer = a.buffers()\r\n>>> np.frombuffer(buffer, dtype=int)\r\narray([1, 2, 3, 4, 5])\r\n>>> np.frombuffer(buffer.slice(a.offset * a.type.bit_width//8), dtype=int)[:len(a)]\r\narray([3, 4, 5])\r\n```\r\nbut I can see how it's not trivial when there's a mask or jaggedness. \r\n\r\nAnyway, as a solution in my current code, I put\r\n```python\r\n        if array_arrow.offset != 0:\r\n            array_arrow = array_arrow.take(np.arange(len(array_arrow)))\r\n```\r\nbefore I try to convert an arrow array into awkward.",
  "created_at":"2020-12-29T18:07:40Z",
  "id":752187725,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjE4NzcyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-29T18:08:53Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"MEMBER",
  "body":"The tests haven't been added yet, but PR #625 now correctly handles these `offsets`:\r\n\r\n```python\r\n>>> arrow_array = ak.to_arrow(ak.Array(np.arange(10)))\r\n>>> ak.from_arrow(arrow_array[6:])\r\n<Array [6, 7, 8, 9] type='4 * int64'>\r\n\r\n>>> arrow_array = ak.to_arrow(ak.Array([[0, 1, 2], [], [3, 4], [5], [6, 7, 8, 9]]))\r\n>>> ak.from_arrow(arrow_array[2:])\r\n<Array [[3, 4], [5], [6, 7, 8, 9]] type='3 * var * int64'>\r\n```",
  "created_at":"2020-12-29T20:22:18Z",
  "id":752228973,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjIyODk3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-29T20:26:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Anyway, as a solution in my current code, I put\r\n> \r\n> ```python\r\n>         if array_arrow.offset != 0:\r\n>             array_arrow = array_arrow.take(np.arange(len(array_arrow)))\r\n> ```\r\n> \r\n> before I try to convert an arrow array into awkward.\r\n\r\nI think you're relying on the fact that the `take` copies the data into a new array that has `offset=0`. If the zero-copy aspect was valuable, you'll want this fix.\r\n\r\nThanks for pointing it out, though! Seeing the wrong answers come out of a function (as opposed to an error or something) is the most disturbing kind of bug.",
  "created_at":"2020-12-29T20:25:32Z",
  "id":752229786,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjIyOTc4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-29T20:25:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Great, I'll follow that PR so I can eventually switch to a truly zero-copy solution! :)",
  "created_at":"2020-12-29T21:59:22Z",
  "id":752255372,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjI1NTM3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-29T21:59:22Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"MEMBER",
  "body":"@trickarcher This looks great! Let me know if you're done and I'll merge the PR. (I don't want to merge it if you are working on something offline.)",
  "created_at":"2020-12-30T16:18:48Z",
  "id":752678260,
  "issue":625,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjY3ODI2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-30T16:18:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I am done with the tests, you can merge it :)",
  "created_at":"2020-12-30T16:20:30Z",
  "id":752678909,
  "issue":625,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MjY3ODkwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-30T16:20:30Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"I must have written this in a hurry. I think it means that the following doesn't work:\r\n\r\n```python\r\n>>> ak.zip([])\r\n```\r\n\r\nand\r\n\r\n```python\r\n>>> ak.zip({})\r\n```\r\n\r\nWhile it might seem silly that someone would try to do that, my stack trace shows that this is coming from Uproot.\r\n\r\nThe zip is not going to know what the length of the array is supposed to be. When coming from Uproot, the length should be set by `num_entries`. This really ought to be fixed in Uproot, not in Awkward Array. (Maybe the `ak.zip` documentation should add the word \"non-empty\".) Transferring the issue...",
  "created_at":"2020-12-11T22:52:15Z",
  "id":1917332518,
  "issue":2993,
  "node_id":"MDEyOklzc3VlQ29tbWVudDE5MTczMzI1MTg=",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T22:52:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 }
]
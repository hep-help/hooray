[
 {
  "author":{
   "login":"artlbv"
  },
  "body":"(I have not really found a solution here but happy to update the issue otherwise)\r\n\r\nI have two awkward arrays, e.g. barrel electrons and endcap electrons -> the same objects with the same variables/fields.\r\n```\r\nbarrel_eles = [[{pt: 43.8, eta: -0.532}, {pt: 39.5, eta: 0.271}, ... [{pt: 11.5, eta: 0.951}]]\r\nendcap_eles = [[], [{pt: 68.8, eta: 1.92}, {pt: 38, eta: 1.6}]]\r\n```\r\n\r\nI want to merge them such that for each row/event I get the concatenated list of objects, e.g.\r\n```\r\nmerge_ele = ak..(barrel_eles, endcap_eles)\r\n...\r\npt: <Array [[43.8, 39.5, 6], [68.8, 38, 11.5]] type='2 * var * float32'>\r\neta: <Array [[-0.532, 0.271, ... 1.6, 0.951]] type='2 * var * float32'>\r\n```\r\n\r\nIs there any built in method in awkward for this?\r\n\r\np.s. I made the above examples by splitting a single array by a criterion (`abs(eta) < 1.5`)\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"ianna"
     },
     "body":"@artlbv - I think, you can get `barrel_eles.pt` and `endcap_eles.pt` as `contents` to make a `ak.contents.UnionArray`. I wonder why is it needed to separate `pt` and `eta`?\r\n```python\r\n>>> barrel = ak.Array([[{\"pt\": 43.8, \"eta\": -0.532}, {\"pt\": 39.5, \"eta\": 0.271}], [{\"pt\": 11.5, \"eta\": 0.951}]])\r\n>>> endcap = ak.Array([[], [{\"pt\": 68.8, \"eta\": 1.92}, {\"pt\": 38, \"eta\": 1.6}]])\r\n>>> l1 = ak.to_layout(barrel.pt)\r\n>>> l2 = ak.to_layout(endcap.pt)\r\n>>> index = ak.index.Index64([0,1,0,1])\r\n>>> tags = ak.index.Index8([np.int8(0),np.int8(0), np.int8(1),np.int8(1)])\r\n>>> union = ak.contents.UnionArray(tags, index, contents={l1, l2})\r\n>>> arr = ak.Array(union)\r\n>>> arr\r\n<Array [[43.8, 39.5], [11.5], ..., [68.8, 38]] type='4 * union[var * float6...'>\r\n\r\n```",
     "createdAt":"2024-01-10T19:21:41Z",
     "number":8085107,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"@artlbv I think what you're asking for is provided by `ak.concatenate`, with the `axis=1` argument:\r\n```python\r\nimport awkward as ak\r\n\r\nbarrel_eles = ak.from_iter([[{\"pt\": 43.8, \"eta\": -0.532}, {\"pt\": 39.5, \"eta\": 0.271}], [{\"pt\": 11.5, \"eta\": 0.951}]])\r\nendcap_eles = ak.from_iter([[], [{\"pt\": 68.8, \"eta\": 1.92}, {\"pt\": 38, \"eta\": 1.6}]])\r\n\r\nresult = ak.concatenate((barrel_eles, endcap_eles), axis=1)\r\n```\r\n\r\nUnder the hood, this will build a union like @ianna demonstrates, but this union will be erased as both arrays `barrel_eles` and `endcap_eles` have mergeable (the same!) types.",
     "createdAt":"2024-01-11T07:19:51Z",
     "number":8090979,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2024-01-10T17:05:46Z",
  "number":2932,
  "title":"Row-level merging of similar arrays",
  "url":"https://github.com/scikit-hep/awkward/discussions/2932"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n* feat: add some simpler `cuda_kernels` by @ManasviGoyal in https://github.com/scikit-hep/awkward/pull/2880\r\n* feat: add `recursive` flag to `Content.to_packed` by @agoose77 in https://github.com/scikit-hep/awkward/pull/2933\r\n* feat: configure Julia version and required packages by @ianna in https://github.com/scikit-hep/awkward/pull/2896\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: include header-only files in sdist by @agoose77 in https://github.com/scikit-hep/awkward/pull/2909\r\n* fix: touching of unions by @agoose77 in https://github.com/scikit-hep/awkward/pull/2906\r\n* fix: lazily project union contents by @agoose77 in https://github.com/scikit-hep/awkward/pull/2913\r\n* fix: support typetracer in `to_packed` for `IndexedOptionArray` by @agoose77 in https://github.com/scikit-hep/awkward/pull/2912\r\n* fix: cartesian nested record by @agoose77 in https://github.com/scikit-hep/awkward/pull/2929\r\n* fix: cast `bool_` to `int8` for positional reducers by @agoose77 in https://github.com/scikit-hep/awkward/pull/2935\r\n* fix: ak.std and ak.var for axis != -1. by @jpivarski in https://github.com/scikit-hep/awkward/pull/2918\r\n\r\n## Other\r\n\r\n* test: only run test for specific Numba version by @agoose77 in https://github.com/scikit-hep/awkward/pull/2921\r\n* test: add PyPy to the test matrix by @jpivarski in https://github.com/scikit-hep/awkward/pull/2927\r\n* ci: update `actions/(upload|download)-artifact` by @agoose77 in https://github.com/scikit-hep/awkward/pull/2904\r\n* ci: group dependabot updates by @henryiii in https://github.com/scikit-hep/awkward/pull/2914\r\n* ci: fix name of artifacts in deploy-cpp by @agoose77 in https://github.com/scikit-hep/awkward/pull/2940\r\n* chore: restore files that generated the type-parser by @jpivarski in https://github.com/scikit-hep/awkward/pull/2926\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/2915\r\n* chore(deps): bump the actions group with 1 update by @dependabot in https://github.com/scikit-hep/awkward/pull/2923\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.5.1...v2.5.2\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.5.2'>Version 2.5.2</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2024-01-12T18:39:14Z",
  "number":2941,
  "title":"Version 2.5.2",
  "url":"https://github.com/scikit-hep/awkward/discussions/2941"
 },
 {
  "author":{
   "login":"ManasviGoyal"
  },
  "body":"These are the 88/147 CUDA kernels that need to be implemented.\r\n- [x] awkward_ByteMaskedArray_numnull\r\n- [x] awkward_ByteMaskedArray_reduce_next_nonlocal_nextshifts_fromshifts_64\r\n- [x] awkward_Index_nones_as_index\r\n- [x] awkward_IndexedArray_fill\r\n- [x] awkward_IndexedArray_flatten_none2empty\r\n- [x] awkward_IndexedArray_index_of_nulls\r\n- [ ] awkward_IndexedArray_local_preparenext_64 (A)\r\n- [x] awkward_IndexedArray_numnull\r\n- [x] awkward_IndexedArray_numnull_parents\r\n- [x] awkward_IndexedArray_numnull_unique\r\n- [x] awkward_IndexedArray_ranges_carry_next_64\r\n- [x] awkward_IndexedArray_ranges_next_64\r\n- [ ] awkward_IndexedArray_unique_next_index_and_offsets_64 (A)\r\n- [x] awkward_ListArray_broadcast_tooffsets\r\n- [ ] awkward_ListArray_combinations (B)\r\n- [x] awkward_ListArray_combinations_length\r\n- [x] awkward_ListArray_fill\r\n- [ ] awkward_ListArray_getitem_jagged_apply\r\n- [x] awkward_ListArray_getitem_jagged_carrylen\r\n- [x] awkward_ListArray_getitem_jagged_descend\r\n- [x] awkward_ListArray_getitem_jagged_numvalid \r\n- [ ] awkward_ListArray_getitem_jagged_shrink\r\n- [x] awkward_ListArray_getitem_next_array_advanced\r\n- [x] awkward_ListArray_getitem_next_at\r\n- [ ] awkward_ListArray_getitem_next_range (C)\r\n- [ ] awkward_ListArray_getitem_next_range_carrylength (C)\r\n- [x] awkward_ListArray_getitem_next_range_counts\r\n- [x] awkward_ListArray_getitem_next_range_spreadadvanced\r\n- [x] awkward_ListArray_localindex\r\n- [x] awkward_ListArray_min_range\r\n- [x] awkward_ListArray_rpad_and_clip_length_axis1\r\n- [ ] awkward_ListArray_rpad_axis1\r\n- [ ] awkward_ListOffsetArray_argsort_strings\r\n- [x] awkward_ListOffsetArray_drop_none_indexes\r\n- [x] awkward_ListOffsetArray_local_preparenext_64\r\n- [x] awkward_ListOffsetArray_reduce_local_nextparents_64\r\n- [ ] awkward_ListOffsetArray_reduce_local_outoffsets_64\r\n- [x] awkward_ListOffsetArray_reduce_nonlocal_maxcount_offsetscopy_64\r\n- [ ] awkward_ListOffsetArray_reduce_nonlocal_nextshifts_64\r\n- [x] awkward_ListOffsetArray_reduce_nonlocal_nextstarts_64\r\n- [ ] awkward_ListOffsetArray_reduce_nonlocal_outstartsstops_64\r\n- [ ] awkward_ListOffsetArray_reduce_nonlocal_preparenext_64\r\n- [x] awkward_ListOffsetArray_rpad_length_axis1\r\n- [x] awkward_ListOffsetArray_toRegularArray\r\n- [x] awkward_NumpyArray_fill\r\n- [ ] awkward_NumpyArray_pad_zero_to_length\r\n- [ ] awkward_NumpyArray_prepare_utf8_to_utf32_padded\r\n- [ ] awkward_NumpyArray_rearrange_shifted\r\n- [ ] awkward_NumpyArray_sort_asstrings_uint8\r\n- [ ] awkward_NumpyArray_subrange_equal\r\n- [ ] awkward_NumpyArray_unique_strings_uint8\r\n- [ ] awkward_NumpyArray_utf8_to_utf32_padded\r\n- [ ] awkward_RecordArray_reduce_nonlocal_outoffsets_64\r\n- [ ] awkward_RegularArray_combinations (B)\r\n- [x] awkward_RegularArray_getitem_next_array_regularize\r\n- [x] awkward_RegularArray_reduce_local_nextparents_64\r\n- [x] awkward_RegularArray_reduce_nonlocal_preparenext_64\r\n- [x] awkward_RegularArray_rpad_and_clip_axis1\r\n- [x] awkward_UnionArray_fillindex\r\n- [x] awkward_UnionArray_fillindex_count\r\n- [x] awkward_UnionArray_filltags\r\n- [x] awkward_UnionArray_filltags_const\r\n- [ ] awkward_UnionArray_flatten_combine (B)\r\n- [ ] awkward_UnionArray_flatten_length (B)\r\n- [ ] awkward_UnionArray_nestedfill_tags_index\r\n- [ ] awkward_UnionArray_regular_index\r\n- [x] awkward_UnionArray_regular_index_getsize\r\n- [x] awkward_UnionArray_simplify\r\n- [x] awkward_UnionArray_simplify_one\r\n- [ ] awkward_argsort\r\n- [x] awkward_index_rpad_and_clip_axis0\r\n- [ ] awkward_reduce_argmax_complex\r\n- [ ] awkward_reduce_argmin_complex\r\n- [ ] awkward_reduce_countnonzero_complex\r\n- [ ] awkward_reduce_max_complex\r\n- [ ] awkward_reduce_min_complex\r\n- [ ] awkward_reduce_prod\r\n- [ ] awkward_reduce_prod_bool_complex\r\n- [ ] awkward_reduce_prod_complex\r\n- [ ] awkward_reduce_sum_bool_complex\r\n- [ ] awkward_reduce_sum_complex\r\n- [ ] awkward_sort\r\n- [x] awkward_sorting_ranges\r\n- [x] awkward_sorting_ranges_length\r\n- [ ] awkward_unique (A)\r\n- [ ] awkward_unique_copy (A)\r\n- [ ] awkward_unique_offsets (A)\r\n- [ ] awkward_unique_ranges (A)\r\n\r\n(A) if condition uses the counter to update the counters\r\n(B) one of the argument is of double pointer type\r\n(C) kernel calls another function\r\n\r\n\r\nAlso, the following 13 kernels that use `atomicAdd()` need to be reimplemented.\r\n\r\n- [ ] awkward_reduce_argmax.cu\r\n- [ ] awkward_reduce_argmax_bool_64.cu\r\n- [ ] awkward_reduce_argmin.cu\r\n- [ ] awkward_reduce_argmin_bool_64.cu\r\n- [ ] awkward_reduce_count_64.cu\r\n- [ ] awkward_reduce_countnonzero.cu\r\n- [ ] awkward_reduce_max.cu\r\n- [ ] awkward_reduce_min.cu\r\n- [ ] awkward_reduce_prod_bool.cu\r\n- [ ] awkward_reduce_sum.cu\r\n- [ ] awkward_reduce_sum_bool.cu\r\n- [ ] awkward_reduce_sum_int32_bool_64.cu\r\n- [ ] awkward_reduce_sum_int64_bool_64.cu",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"ManasviGoyal"
     },
     "body":"The following reducers kernels can be implemented by using a modified `Hillis Steele algorithm`.\r\n\r\n1. awkward_reduce_argmax.cu\r\n2. awkward_reduce_argmax_bool_64.cu\r\n3. awkward_reduce_argmin.cu\r\n4. awkward_reduce_argmin_bool_64.cu\r\n5. awkward_reduce_count_64.cu\r\n6. awkward_reduce_countnonzero.cu\r\n7. awkward_reduce_max.cu\r\n8. awkward_reduce_min.cu\r\n9. awkward_reduce_prod_bool.cu\r\n10. awkward_reduce_sum.cu\r\n11. awkward_reduce_sum_bool.cu\r\n12. awkward_reduce_sum_int32_bool_64.cu\r\n13. awkward_reduce_sum_int64_bool_64.cu\r\n14.  awkward_reduce_argmax_complex\r\n15.  awkward_reduce_argmin_complex\r\n16.  awkward_reduce_countnonzero_complex\r\n17.  awkward_reduce_max_complex\r\n18.  awkward_reduce_min_complex\r\n19.  awkward_reduce_prod\r\n20.  awkward_reduce_prod_bool_complex\r\n21.  awkward_reduce_prod_complex\r\n22.  awkward_reduce_sum_bool_complex\r\n23.  awkward_reduce_sum_complex",
     "createdAt":"2024-01-15T09:15:00Z",
     "number":8130890,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2024-01-15T09:07:55Z",
  "number":2946,
  "title":"Implementing Awkward CUDA Kernels",
  "url":"https://github.com/scikit-hep/awkward/discussions/2946"
 },
 {
  "author":{
   "login":"HealthyPear"
  },
  "body":"### Description of new feature\n\nHello,\r\n\r\nI have found this in your docs\r\nhttps://awkward-array.org/doc/main/user-guide/how-to-convert-buffers.html?highlight=hdf5#saving-awkward-arrays-to-hdf5\r\n\r\nwhich I knew about, unfortunately I am working from a C++ project and since I saw that it should be possible to use this library directly in C++\r\nhttps://awkward-array.org/doc/main/user-guide/how-to-use-header-only-layoutbuilder.html\r\n\r\nI was wondering if it was possible to have the same tutorial but from C++ instead of Python.\r\n\r\nI tried to go through HDF5 docs and a bunch of other libraries of C++ interfaces but I didn't get out much useful info...",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"The HDF5 example in the docs is not a supported format so much as a demonstration. The reason it's not considered a format is because someone could open the HDF5 file and not know what to do with the array buffers; the Awkward library would be needed to make sense of it, and there's no indication in the HDF5 file itself to say that one should use Awkward to do that.\r\n\r\nOn the other hand, a format like Parquet _does_ describe itself as ragged and nested arrays, rather than flat buffers that need an interpreter. If you're producing data in C++, perhaps you'll have an easier time using the [Parquet C++ tools](https://arrow.apache.org/docs/cpp/parquet.html) to write to Parquet files, and then read them in Python using Awkward.\r\n\r\nThe HDF5 example isn't code, it's a demonstration of how to put buffers from the [ak.to_buffers](https://awkward-array.org/doc/main/reference/generated/ak.to_buffers.html) function into an HDF5 file as arrays. The LayoutBuilder [to_buffers](https://github.com/scikit-hep/awkward/blob/327b1bf1fa50ec74414ea191db88903e49a15bfc/header-only/layout-builder/awkward/LayoutBuilder.h#L135-L144) method creates the same kind of buffers that could, in principle, be written to an HDF5 file in C++. There are HDF5 interfaces [in C](https://www.hdfgroup.org/solutions/hdf5/) and [in C++](https://docs.hdfgroup.org/archive/support/HDF5/doc1.8/cpplus_RM/index.html) that could be used for that, but I'd strongly recommend Parquet.\r\n\r\n...Unless your data is strictly rectilinear arrays! In that case, just use HDF5 as-is, without anything from Awkward Array, and read it with [h5py](https://www.h5py.org/) (with NumPy \u2192 Awkward using [ak.from_numpy](https://awkward-array.org/doc/main/reference/generated/ak.from_numpy.html).",
     "createdAt":"2024-01-16T00:03:57Z",
     "number":8157998,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"HealthyPear"
        },
        "body":"Sorry for the delay!\r\n\r\nI am not an expert in data format, so I am probably missing some details that might make my life easier.\r\n\r\nMy use-case is one in which I am writing a piece of software based on Geant4 which at some point should write data as a table with ragged arrays. In a previous-generation code the data format from which data was written in C++ was [XCDF](https://github.com/jimbraun/XCDF), but I wanted to use a more flexible data format that would allow me to save complex metadata together with the list of output events row data (e.g. a table or a histogram of simulated counts) and I know HDF5 allows to do that because it is essentially a small isolated filesystem (and Geant4 has API to write to HDF5.\r\n\r\nDoes Parquet allow this (from [here](https://parquet.apache.org/docs/file-format/metadata/ it seems to me it should be very similar)? Then reading this kind of data in Python (specifically the list of XCDF-like events) would result in a single \"awkward\" table (see #2597 and #2468 I'm the same guy \ud83d\ude05 ).\r\n\r\nthanks!",
        "createdAt":"2024-01-29T13:33:09Z",
        "number":8282729
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2024-01-15T22:34:02Z",
  "number":2954,
  "title":"Write to HDF5 (optionally read) but from C++",
  "url":"https://github.com/scikit-hep/awkward/discussions/2954"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"From this point onward, awkward strictly depends on fsspec.\r\n\r\n## New features\r\n\r\n* feat: turn on CUDA unit tests for working kernels and add some CUDA kernels by @ManasviGoyal in https://github.com/scikit-hep/awkward/pull/2930\r\n* feat: add string builder by @lobis in https://github.com/scikit-hep/awkward/pull/2899\r\n* feat: add to_parquet_row_groups by @zbilodea in https://github.com/scikit-hep/awkward/pull/2979\r\n* feat!: __array__ is no longer allowed on NumpyArray and EmptyArray by @jpivarski in https://github.com/scikit-hep/awkward/pull/2997\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: delete `header-only/tests/test_1542-array-builder.cpp` by @ianna in https://github.com/scikit-hep/awkward/pull/2981\r\n* fix: use `std::tuple_element_t` as an `OptionType` type alias by @ianna in https://github.com/scikit-hep/awkward/pull/2977\r\n* fix: generate error message and tests for CUDA and CPU kernels by @ManasviGoyal in https://github.com/scikit-hep/awkward/pull/2989\r\n* fix: `ak.merge_union_of_records` error message for cases that can't use `axis < 0` by @jpivarski in https://github.com/scikit-hep/awkward/pull/2995\r\n\r\n## Other\r\n\r\n* test: don't fail fast (so pypy doesn't stop the tests) by @jpivarski in https://github.com/scikit-hep/awkward/pull/2956\r\n* test: generate unit tests for cpu and cuda by @ManasviGoyal in https://github.com/scikit-hep/awkward/pull/2938\r\n* test: update test_2799 for Numba 0.59.0 by @jpivarski in https://github.com/scikit-hep/awkward/pull/2998\r\n* docs: add zonca as a contributor for code by @allcontributors in https://github.com/scikit-hep/awkward/pull/2959\r\n* docs: add chrisburr as a contributor for infra by @allcontributors in https://github.com/scikit-hep/awkward/pull/2960\r\n* docs: add zbilodea as a contributor for code by @allcontributors in https://github.com/scikit-hep/awkward/pull/2961\r\n* docs: add raymondEhlers as a contributor for maintenance by @allcontributors in https://github.com/scikit-hep/awkward/pull/2962\r\n* docs: add mloning as a contributor for doc by @allcontributors in https://github.com/scikit-hep/awkward/pull/2963\r\n* docs: add kkothari2001 as a contributor for code, and test by @allcontributors in https://github.com/scikit-hep/awkward/pull/2964\r\n* docs: add jrueb as a contributor for code by @allcontributors in https://github.com/scikit-hep/awkward/pull/2965\r\n* docs: add Moelf as a contributor for doc by @allcontributors in https://github.com/scikit-hep/awkward/pull/2966\r\n* docs: update dependencies by @agoose77 in https://github.com/scikit-hep/awkward/pull/2986\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/2948\r\n* chore(deps): bump the actions group with 3 updates by @dependabot in https://github.com/scikit-hep/awkward/pull/2980\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/2982\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/2991\r\n* chore(deps): bump the actions group with 2 updates by @dependabot in https://github.com/scikit-hep/awkward/pull/2990\r\n* chore: fsspec is now a strict (required) dependency by @jpivarski in https://github.com/scikit-hep/awkward/pull/2996\r\n\r\n## New Contributors\r\n* @lobis made their first contribution in https://github.com/scikit-hep/awkward/pull/2899\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.5.2...v2.6.0\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.6.0'>Version 2.6.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2024-02-02T19:34:28Z",
  "number":3001,
  "title":"Version 2.6.0",
  "url":"https://github.com/scikit-hep/awkward/discussions/3001"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"The purpose of this release is to provide a lower-bound on the fsspec dependency version (#3006).\r\n\r\n## New features\r\n\r\n_(none!)_\r\n\r\n## Bug-fixes and performance\r\n\r\n_(none!)_\r\n\r\n## Other\r\n\r\n* chore(deps): bump the actions group with 2 updates by @dependabot in https://github.com/scikit-hep/awkward/pull/3004\r\n* build: set a minimum fsspec version (see #3002) by @jpivarski in https://github.com/scikit-hep/awkward/pull/3006\r\n\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.6.0...v2.6.1\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.6.1'>Version 2.6.1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2024-02-05T21:35:17Z",
  "number":3008,
  "title":"Version 2.6.1",
  "url":"https://github.com/scikit-hep/awkward/discussions/3008"
 },
 {
  "author":{
   "login":"Star9daisy"
  },
  "body":"Hi, developers of awkward,\r\n\r\nI'm wondering if there is some function like `np.argwhere` to find the element index?\r\n\r\n```python\r\narray = ak.from_iter([[1, 2, 3], [], [4, 5], [6, 7, 8, 9]])\r\n\r\n# If there is that function\r\n# ak.argwhere(array == 1)\r\n# Expected returned value:\r\n# [(0, 0)]\r\n```",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"The best answer to this is probably \"what will you do with that index?\".\r\n\r\nAwkward Array has a powerful ragged indexing system that supports structure-preserving integer/boolean indices. We touch on using these indexing arrays in [our user guide](https://awkward-array.org/doc/main/user-guide/how-to-filter-ragged.html). The semantics of this \"ragged indexing\" are also mentioned [in the API reference](https://awkward-array.org/doc/main/reference/generated/ak.Array.html#ak.Array.__getitem__) (for now).\r\n\r\nFor example, if you need to find the indices of even numbers:\r\n```python\r\nimport awkward as ak\r\narray = ak.Array(\r\n    [\r\n        [1, 2, 3],\r\n        [],\r\n        [4, 5],\r\n        [6, 7, 8, 9],\r\n    ]\r\n)\r\nix = ak.local_index(array)\r\nix_even = ix[array % 2 == 0]\r\n```\r\n\r\nThis `ix_even` array can then be used to index into array (or any array with the same structure). Note that, if you were just interested in filtering `array`, you could skip the step that computes the index, i.e.\r\n```python\r\narray_even = array[array % 2 == 0]\r\n```",
     "createdAt":"2024-02-09T10:52:41Z",
     "number":8417575,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"@agoose77 is right; you might be trying to find a long way to do what could be done with a slice. But assuming that you need positions of matching indexes as tuples, it can be done in your two-dimensional example like this:\r\n\r\n```python\r\nimport numpy as np\r\nimport awkward as ak\r\narray = ak.Array(\r\n    [\r\n        [1, 2, 3],\r\n        [],\r\n        [4, 5],\r\n        [6, 7, 8, 9],\r\n    ]\r\n)\r\n\r\nsecond = ak.local_index(array)[array % 2 == 0]    # [[1], [], [0], [0, 2]], what @agoose77 suggested\r\nfirst = np.arange(len(second))                    # [ 0 ,  1,  2 ,     3 ]\r\nfirst, _ = ak.broadcast_arrays(first, second)     # [[0], [], [2], [3, 3]]\r\nresult = ak.flatten(ak.zip((first, second)))      # [(0, 1), (2, 0), (3, 0), (3, 2)]\r\n```\r\n\r\nThis technique would require a different number of steps for each number of dimensions. Also, if you actually wanted lists (contiguous data) instead of tuples (not contiguous), you can do it by concatenating instead of zipping:\r\n\r\n```python\r\nak.flatten(ak.concatenate((first[..., np.newaxis], second[..., np.newaxis]), axis=-1))\r\n                                                  # [[0, 1], [2, 0], [3, 0], [3, 2]]\r\n```\r\n\r\n(Which one you want depends on how you're going to use it...)",
     "createdAt":"2024-02-09T14:42:59Z",
     "number":8419835,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"Star9daisy"
     },
     "body":"Thank you so much for replying so quickly! Yeah, it's all about what I'm gonna do with these indexes. Let me explain more:\r\n\r\nGiven the output root file from Delphes in Madgraph5, I want to rebuild fatjets according to constituents, which is basically the same thing asked in [stack overflow in 2019](https://stackoverflow.com/questions/58291817/how-to-use-uproot-to-load-referenced-values-trefarray).\r\n\r\n@jpivarski already answered that time. But it's still kind of hard to do it step by step:\r\n1. `awkward` is so different from that time;\r\n2. The \"Tower.Particles\", a `TRefArray`, mentioned in the answer refers to the unique indexes of the branch \"Particles\"; However, the \"Constituents\" refers to \"Tower\" and \"Track\" these two different branch. So it's not that straightforward to use the `TRefArray` directly.\r\n\r\nThe questioner [John Karkas](https://stackoverflow.com/users/12106577/john-karkas) gave some hints in the reply:\r\n> Delphes stores the IDs per event for objects...\r\n\r\nIt hits me that there does exist a subbranch named \"fUniqueID\". After a few try, I find that the \"refs\" of \"FatJet.Constituents\" stores the unique id that corresponds to a `EFlowTrack` or a `EFlowPhoton` or a `EFlowNeutralHadron`. So here's the reason behind my question: I need to find the corresponding indexes in one of three according to the ids so that I can build \"read\" constituents rather than \"refs\".\r\n\r\nI need to search `EFlowTrack` `EFlowPhoton` `EFlowNeutralHadron` to find the location of constituents' id.\r\n\r\nIf it's a \"one element's location in one array\", I would use `np.argwhere` to retrieve the indexes; But it's now more like a \"one array's location in another three array\". To avoid loop as much as possible, I do some search and find `np.isin` function. It is exactly what I want. Here's my final solution:\r\n\r\n```python\r\nimport numpy as np\r\nimport uproot\r\nimport awkward as ak\r\nimport vector\r\n\r\nvector.register_awkward()\r\n\r\nfilepath = \"../data/pp2wz/Events/run_01_decayed_1/tag_1_delphes_events.root\"\r\nevents = uproot.open(f\"{filepath}:Delphes\")\r\n\r\nall_ref_ids = events[\"FatJet.Constituents\"].array()[\"refs\"]\r\nall_tracks = events[\"EFlowTrack.fUniqueID\"].array()\r\nall_photons = events[\"EFlowPhoton.fUniqueID\"].array()\r\nall_neutral_hadrons = events[\"EFlowNeutralHadron.fUniqueID\"].array()\r\n\r\nall_tracks = ak.zip(\r\n    {\r\n        \"pt\": events[\"EFlowTrack.PT\"].array(),\r\n        \"eta\": events[\"EFlowTrack.Eta\"].array(),\r\n        \"phi\": events[\"EFlowTrack.Phi\"].array(),\r\n        \"mass\": events[\"EFlowTrack.Mass\"].array(),\r\n        \"id\": all_tracks,\r\n    },\r\n    with_name=\"Momentum4D\",\r\n)\r\nall_photons = ak.zip(\r\n    {\r\n        \"pt\": events[\"EFlowPhoton.ET\"].array(),\r\n        \"eta\": events[\"EFlowPhoton.Eta\"].array(),\r\n        \"phi\": events[\"EFlowPhoton.Phi\"].array(),\r\n        \"mass\": ak.zeros_like(events[\"EFlowPhoton.ET\"].array()),\r\n        \"id\": all_photons,\r\n    },\r\n    with_name=\"Momentum4D\",\r\n)\r\nall_neutral_hadrons = ak.zip(\r\n    {\r\n        \"pt\": events[\"EFlowNeutralHadron.ET\"].array(),\r\n        \"eta\": events[\"EFlowNeutralHadron.Eta\"].array(),\r\n        \"phi\": events[\"EFlowNeutralHadron.Phi\"].array(),\r\n        \"mass\": ak.zeros_like(events[\"EFlowNeutralHadron.ET\"].array()),\r\n        \"id\": all_neutral_hadrons,\r\n    },\r\n    with_name=\"Momentum4D\",\r\n)\r\n\r\nall_constituents = []\r\n\r\nfor tracks, photons, neutral_hadrons, ref_ids in zip(\r\n    all_tracks, all_photons, all_neutral_hadrons, all_ref_ids\r\n):\r\n    constituents = []\r\n    for ref_id in ref_ids:\r\n        matched_tracks = tracks[np.isin(tracks.id, ref_id)]\r\n        matched_photons = photons[np.isin(photons.id, ref_id)]\r\n        matched_neutral_hadrons = neutral_hadrons[np.isin(neutral_hadrons.id, ref_id)]\r\n\r\n        assert len(ref_id) == (\r\n            len(matched_tracks) + len(matched_photons) + len(matched_neutral_hadrons)\r\n        )\r\n\r\n        constituents.append(\r\n            ak.concatenate([matched_tracks, matched_photons, matched_neutral_hadrons])\r\n        )\r\n\r\n    all_constituents.append(constituents)\r\n\r\nall_constituents = ak.from_iter(all_constituents)\r\n```\r\n\r\nThe `all_constituents` now is:\r\n```\r\n[[],\r\n [[{pt: 0.48, eta: -0.126, phi: 0.389, mass: 0.14, id: 2857}, ..., {...}]],\r\n [[{pt: 9.77, eta: 0.418, phi: -2.74, mass: 0.14, id: 1313}, ..., {...}]],\r\n [[{pt: 0.321, eta: -0.489, phi: -2.66, mass: 0.14, id: 2303}, ..., {...}]],\r\n [[{pt: 0.914, eta: 1.62, phi: 2.06, mass: 0.14, id: 1957}, {...}, ..., {...}]],\r\n [[{pt: 0.688, eta: 1.88, phi: 1.55, mass: 0.14, id: 2178}, {...}, ..., {...}]],\r\n [[{pt: 1.23, eta: -0.111, phi: -1.55, mass: 0.494, id: 3379}, ...], ...],\r\n [[{pt: 2.18, eta: -0.152, phi: 1.75, mass: 0.14, id: 621}, {...}, ..., {...}]],\r\n [[{pt: 0.365, eta: -0.572, phi: -2.08, mass: 0.14, id: 2336}, ..., {...}]],\r\n [[{pt: 4.69, eta: -0.356, phi: -2.85, mass: 0.14, id: 1147}, ..., {...}]],\r\n ...,\r\n [[{pt: 8.91, eta: -0.63, phi: -1.28, mass: 0.14, id: 2677}, ..., {...}], ...],\r\n [[{pt: 0.83, eta: -1.64, phi: 2.98, mass: 0.14, id: 2047}, {...}, ..., {...}]],\r\n [[{pt: 0.23, eta: 1.85, phi: 2.04, mass: 0.14, id: 1388}, {...}, ..., {...}]],\r\n [[{pt: 0.386, eta: -1.13, phi: 2.44, mass: 0.494, id: 2611}, ..., {...}]],\r\n [[{pt: 1.35, eta: -0.959, phi: 2.32, mass: 0.14, id: 3270}, ..., {...}]],\r\n [[{pt: 0.422, eta: -2.32, phi: 2.9, mass: 0.938, id: 3551}, ..., {...}]],\r\n [[{pt: 4.21, eta: 0.422, phi: -1.63, mass: 0.14, id: 1950}, ..., {...}]],\r\n [[{pt: 1.52, eta: -0.3, phi: -1.96, mass: 0.494, id: 1257}, ..., {...}]],\r\n [[{pt: 20, eta: 0.404, phi: 2.64, mass: 0.14, id: 1640}, ..., {...}], ...]]\r\n--------------------------------------------------------------------------------\r\ntype: 100 * var * var * {\r\n    pt: float64,\r\n    eta: float64,\r\n    phi: float64,\r\n    mass: float64,\r\n    id: int64\r\n}\r\n```\r\n\r\nThe type could be explained as: 100 events, var jets, var constituents. The last var indicates the constituents, since they're reclustered to build the fatjet, it's always 1:\r\n\r\n```python\r\nimport fastjet as fj\r\n\r\nparticles = ak.zip(\r\n    {\r\n        \"pt\": all_constituents.pt,\r\n        \"eta\": all_constituents.eta,\r\n        \"phi\": all_constituents.phi,\r\n        \"mass\": all_constituents.mass,\r\n    },\r\n    with_name=\"Momentum4D\",\r\n)\r\n\r\njet_def = fj.JetDefinition(fj.antikt_algorithm, 100.0)\r\ncluster = fj.ClusterSequence(particles, jet_def)\r\njets = cluster.inclusive_jets()\r\n\r\n# To index, for example, the second fatjet in the seventh event\r\n# jets[6, 1, 0]\r\n# the last index is always 0\r\n```\r\n\r\nI know it's still not \"perfect\": I have to loop twice: one for event, and one for fatjets since it's a recluster not a cluster one (I'll post a cluster one to remove one loop). It's not a long time after I use `awkward`, so if you have any better idea, please let me know. Thank you!",
     "createdAt":"2024-02-10T10:27:21Z",
     "number":8426533,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"Star9daisy"
     },
     "body":"This is another version of event loop that collects all constituents of one event and cluster jets:\r\n\r\n```python\r\nall_constituents = []\r\n\r\nfor tracks, photons, neutral_hadrons, ref_ids in zip(\r\n    all_tracks, all_photons, all_neutral_hadrons, all_ref_ids\r\n):\r\n    matched_tracks = tracks[np.isin(tracks.id, ak.flatten(ref_ids))]\r\n    matched_photons = photons[np.isin(photons.id, ak.flatten(ref_ids))]\r\n    matched_neutral_hadrons = neutral_hadrons[\r\n        np.isin(neutral_hadrons.id, ak.flatten(ref_ids))\r\n    ]\r\n\r\n    constituents = ak.concatenate(\r\n        [matched_tracks, matched_photons, matched_neutral_hadrons]\r\n    )\r\n    all_constituents.append(constituents)\r\n```\r\n\r\nThis version don't differentiate constituents of different jets so the type is \"100 * var ...\". Cluster jets as the previous one, then index for example, the second fatjet in the seventh event:\r\n\r\n```\r\njets[6, 1]\r\n```",
     "createdAt":"2024-02-10T10:36:05Z",
     "number":8426594,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Right now we don't have an overload for `np.isin`. You could implement a 2D variant of the kernel in Numba. If you need to scale to larger dimensionality with `axis=-1`, then one can just use `ak.transform` for this.\r\n\r\nUnfortunately it doesn't look like Numba implement an overload for `isin`, which is a shame because there are different performance characteristics according to the method used.\r\n\r\nHere's a trivial kernel\r\n```python\r\nimport numba as nb\r\n\r\n\r\n@nb.njit\r\ndef isin_kernel_2d(needle, haystack, result):\r\n    for i in range(len(needle)):\r\n        for y in haystack[i]:\r\n            if needle[i] == y:\r\n                result[i] = True\r\n                break\r\n        else:\r\n            result[i] = False\r\n\r\nall_track_is_in = isin_kernel_2d(all_tracks.id, ak.flatten(all_ref_ids, axis=-1))\r\n```\r\n\r\nbut you'd really want to do something smarter e.g. at least binary search over a sorted haystack. NumPy implement a table method which can be used if you know the nature of the values in `haystack` (e.g. how dense they are, are they sorted, etc).\r\n\r\nI looked into which functions `pyarrow.compute` provides, in case we can leverage an existing solution, but it appears that they only implement a 1D variant.",
        "createdAt":"2024-02-12T09:15:38Z",
        "number":8439003
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"If each `haystack[i]` is relatively small, the difference between your $\\mathcal{O}(n)$ implementation[^1] (in Numba, above) and a smarter $\\mathcal{O}(\\log n)$ implementation might not even matter. In fact, the $\\mathcal{O}(\\log n)$ implementation implementation might be worse than the $\\mathcal{O}(n)$ implementation.\r\n\r\nWhat's the average size of `haystack[i]`?\r\n\r\nBisection search is $\\mathcal{O}(\\log n)$, but it only works for sorted lists and it's $\\mathcal{O}(n \\log n)$ (i.e. worse) if you have to sort the list. Similarly if you want to do a binary tree search and have to build the binary trees first.\r\n\r\nAnd _even if_ each `haystack[i]` is already sorted and you can immediately do a bisection search over each one of them, the branching code to do that search might thwart compiler optimizations. There's a turn-over point: above some average `haystack[i]` size, better time complexity always wins, but I'd expect that to be at least 50 items or so...[^2]\r\n\r\n[^1]: where $n$ is the length of _each_ `haystack[i]`.\r\n\r\n[^2]: just a guess. I'm tempted to do an experiment, but I have a lot of emails to get through still...",
        "createdAt":"2024-02-12T21:30:17Z",
        "number":8447073
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"So, the nerd-sniping thing again. I wanted to know how much it would matter because we tend to have a lot of problems in which the classical solutions optimize for one big X (one big matrix multiplication or inversion, one big minimization, etc.) and we have many small Xes. This is another case like that.\r\n\r\nHere is code to search for the point at which bisection searches (the classic solution for a big X = needle in a haystack search) wins over the linear search, which has the advantage of compiling down to simpler instructions and memory strides.\r\n\r\n```python\r\nimport time\r\n\r\nimport awkward as ak\r\nimport numpy as np\r\nimport numba as nb\r\n\r\n\r\ndef testy(haystack_size, inv_density=1.5, sample_size=10000000):\r\n    sorted_numbers = np.random.poisson(inv_density, sample_size)\r\n    np.cumsum(sorted_numbers, out=sorted_numbers)\r\n\r\n    offsets = np.arange(0, len(sorted_numbers), haystack_size)\r\n    haystacks = ak.Array(\r\n        ak.contents.ListOffsetArray(\r\n            ak.index.Index64(offsets),\r\n            ak.contents.NumpyArray(sorted_numbers),\r\n        )\r\n    )\r\n\r\n    needles = np.random.randint(0, haystack_size, len(offsets) - 1)\r\n    needles = sorted_numbers[offsets[:-1] + needles]\r\n    needles += np.random.randint(-1, 2, len(needles))  # don't make them all return True\r\n\r\n    @nb.njit\r\n    def ragged_isin(needles, haystacks):\r\n        out = np.zeros(len(needles), dtype=np.bool_)\r\n        for i in range(len(needles)):\r\n            needle = needles[i]\r\n            haystack = haystacks[i]\r\n            for x in haystack:\r\n                if needle == x:\r\n                    out[i] = True\r\n                    break\r\n        return out\r\n\r\n    @nb.njit\r\n    def ragged_bisection_isin(needles, haystacks):\r\n        out = np.zeros(len(needles), dtype=np.bool_)\r\n        for i in range(len(needles)):\r\n            needle = needles[i]\r\n            haystack = haystacks[i]\r\n            left = 0\r\n            right = len(haystack) - 1\r\n            while left <= right:\r\n                mid = (left + right) >> 1\r\n                if haystack[mid] == needle:\r\n                    out[i] = True\r\n                    break\r\n                elif haystack[mid] < needle:\r\n                    left = mid + 1\r\n                else:\r\n                    right = mid - 1\r\n        return out\r\n\r\n    a = ragged_isin(needles, haystacks)\r\n    b = ragged_bisection_isin(needles, haystacks)\r\n\r\n    assert (a == b).all()\r\n\r\n    measurements = []\r\n    for _ in range(10):\r\n        before = time.perf_counter_ns()\r\n        tmp = ragged_isin(needles, haystacks)\r\n        after = time.perf_counter_ns()\r\n        measurements.append(after - before)\r\n\r\n    first_mean = np.mean(measurements) / len(needles)\r\n    first_std = np.std(measurements) / len(needles)\r\n\r\n    measurements = []\r\n    for _ in range(10):\r\n        before = time.perf_counter_ns()\r\n        tmp = ragged_bisection_isin(needles, haystacks)\r\n        after = time.perf_counter_ns()\r\n        measurements.append(after - before)\r\n\r\n    second_mean = np.mean(measurements) / len(needles)\r\n    second_std = np.std(measurements) / len(needles)\r\n\r\n    print(\r\n        f\"haystack_size: {haystack_size} linear: {first_mean:.3f} +- {first_std:.3f} ns, bisection: {second_mean:.3f} +- {second_std:.3f} ns\"\r\n    )\r\n\r\n\r\ntesty(1)\r\ntesty(2)\r\ntesty(5)\r\ntesty(10)\r\ntesty(20)\r\ntesty(50)\r\ntesty(100)\r\ntesty(200)\r\ntesty(500)\r\ntesty(1000)\r\ntesty(2000)\r\ntesty(5000)\r\ntesty(10000)\r\ntesty(20000)\r\ntesty(50000)\r\ntesty(100000)\r\n```\r\n\r\nAnd here are the results:\r\n\r\n```\r\nhaystack_size: 1 linear: 3.582 +- 0.177 ns, bisection: 6.964 +- 0.024 ns\r\nhaystack_size: 2 linear: 4.399 +- 0.143 ns, bisection: 8.410 +- 0.032 ns\r\nhaystack_size: 5 linear: 5.615 +- 0.019 ns, bisection: 11.821 +- 0.064 ns\r\nhaystack_size: 10 linear: 6.889 +- 0.136 ns, bisection: 14.524 +- 0.030 ns\r\nhaystack_size: 20 linear: 9.450 +- 0.093 ns, bisection: 18.199 +- 0.072 ns\r\nhaystack_size: 50 linear: 18.108 +- 0.567 ns, bisection: 23.995 +- 0.394 ns\r\nhaystack_size: 100 linear: 37.471 +- 1.062 ns, bisection: 52.758 +- 0.983 ns\r\nhaystack_size: 200 linear: 93.914 +- 1.529 ns, bisection: 106.149 +- 2.265 ns\r\nhaystack_size: 500 linear: 215.370 +- 11.525 ns, bisection: 146.369 +- 15.453 ns\r\nhaystack_size: 1000 linear: 356.485 +- 6.946 ns, bisection: 125.843 +- 35.119 ns\r\nhaystack_size: 2000 linear: 624.713 +- 36.474 ns, bisection: 174.018 +- 30.987 ns\r\nhaystack_size: 5000 linear: 1304.450 +- 28.181 ns, bisection: 166.940 +- 55.463 ns\r\nhaystack_size: 10000 linear: 2553.567 +- 84.002 ns, bisection: 121.813 +- 83.126 ns\r\nhaystack_size: 20000 linear: 5088.757 +- 398.548 ns, bisection: 163.010 +- 169.516 ns\r\nhaystack_size: 50000 linear: 12526.477 +- 511.572 ns, bisection: 148.117 +- 138.095 ns\r\nhaystack_size: 100000 linear: 23110.413 +- 1033.834 ns, bisection: 209.931 +- 193.116 ns\r\n```\r\n\r\nIt turns over somewhere between 50 and 100. My guess was a good one!\r\n\r\nActually, no it was a lucky first trial of the other parameters. With `inv_density=5.0` (probability of finding a match is much lower), the turn-over point is between 200 and 500:\r\n\r\n```\r\nhaystack_size: 1 linear: 3.668 +- 0.148 ns, bisection: 6.976 +- 0.023 ns\r\nhaystack_size: 2 linear: 3.704 +- 0.134 ns, bisection: 7.670 +- 0.071 ns\r\nhaystack_size: 5 linear: 4.431 +- 0.030 ns, bisection: 11.712 +- 0.062 ns\r\nhaystack_size: 10 linear: 5.698 +- 0.088 ns, bisection: 14.712 +- 0.018 ns\r\nhaystack_size: 20 linear: 9.343 +- 0.770 ns, bisection: 18.905 +- 0.151 ns\r\nhaystack_size: 50 linear: 19.527 +- 1.575 ns, bisection: 25.500 +- 0.370 ns\r\nhaystack_size: 100 linear: 37.332 +- 1.221 ns, bisection: 53.944 +- 0.820 ns\r\nhaystack_size: 200 linear: 87.906 +- 3.063 ns, bisection: 110.730 +- 2.791 ns\r\nhaystack_size: 500 linear: 216.226 +- 15.276 ns, bisection: 154.511 +- 11.501 ns\r\nhaystack_size: 1000 linear: 363.567 +- 14.152 ns, bisection: 169.031 +- 24.139 ns\r\nhaystack_size: 2000 linear: 679.980 +- 26.190 ns, bisection: 183.090 +- 41.826 ns\r\nhaystack_size: 5000 linear: 1586.080 +- 73.245 ns, bisection: 179.635 +- 61.417 ns\r\nhaystack_size: 10000 linear: 3041.679 +- 192.377 ns, bisection: 162.697 +- 88.346 ns\r\nhaystack_size: 20000 linear: 5882.436 +- 186.369 ns, bisection: 136.003 +- 125.089 ns\r\nhaystack_size: 50000 linear: 15706.292 +- 627.261 ns, bisection: 178.308 +- 184.746 ns\r\nhaystack_size: 100000 linear: 30758.378 +- 2171.481 ns, bisection: 212.002 +- 181.188 ns\r\n```\r\n\r\nRemoving the \"`don't make them all return True`\" line (so that the probability of finding a match is 100%), the turn-over point is pretty close to 200:\r\n\r\n```\r\nhaystack_size: 1 linear: 1.127 +- 0.177 ns, bisection: 1.505 +- 0.006 ns\r\nhaystack_size: 2 linear: 4.244 +- 0.120 ns, bisection: 5.321 +- 0.038 ns\r\nhaystack_size: 5 linear: 7.433 +- 0.067 ns, bisection: 9.627 +- 0.067 ns\r\nhaystack_size: 10 linear: 8.991 +- 0.300 ns, bisection: 12.454 +- 0.054 ns\r\nhaystack_size: 20 linear: 11.678 +- 0.395 ns, bisection: 15.544 +- 0.123 ns\r\nhaystack_size: 50 linear: 19.318 +- 0.552 ns, bisection: 22.298 +- 0.413 ns\r\nhaystack_size: 100 linear: 44.263 +- 1.149 ns, bisection: 52.754 +- 1.195 ns\r\nhaystack_size: 200 linear: 108.139 +- 2.209 ns, bisection: 111.008 +- 1.986 ns\r\nhaystack_size: 500 linear: 230.535 +- 7.110 ns, bisection: 160.800 +- 15.092 ns\r\nhaystack_size: 1000 linear: 371.500 +- 28.397 ns, bisection: 169.795 +- 31.917 ns\r\nhaystack_size: 2000 linear: 632.418 +- 90.036 ns, bisection: 100.565 +- 58.754 ns\r\nhaystack_size: 5000 linear: 1212.563 +- 124.871 ns, bisection: 117.561 +- 63.530 ns\r\nhaystack_size: 10000 linear: 2282.108 +- 249.024 ns, bisection: 152.931 +- 126.055 ns\r\nhaystack_size: 20000 linear: 3637.732 +- 239.075 ns, bisection: 118.751 +- 107.246 ns\r\nhaystack_size: 50000 linear: 8607.739 +- 747.016 ns, bisection: 150.027 +- 143.799 ns\r\nhaystack_size: 100000 linear: 17485.448 +- 1070.246 ns, bisection: 200.325 +- 182.548 ns\r\n```\r\n\r\nSo it depends on the probability of finding a match (naturally), but over that whole probability range, from nearly 0% to exactly 100%, the turn over is somewhere in the vicinity of 50 to 500. That order of magnitude.\r\n\r\nIf the average `len(haystack[i])` is under 50, go for the linear search!",
        "createdAt":"2024-02-12T23:12:35Z",
        "number":8447710
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"@jpivarski thank you _so_ much for doing this investigation! Whilst we have to be careful about extrapolating to other hardware etc, it's really nice to be able to point to this as a clear reference for \"if you know your data, you should use X solution\". I'll keep this bookmarked in my mind for future.",
        "createdAt":"2024-02-13T11:29:01Z",
        "number":8452697
       }
      ],
      "totalCount":4
     }
    },
    {
     "author":{
      "login":"Star9daisy"
     },
     "body":"Thank you so much @agoose77 @jpivarski . I've learned a lot from your codes and discussions! \r\n\r\nHere's a summary of all kinds of \"in\" operation that may help me figure out which one I need:\r\n\r\n| Function | Explanation | Results |\r\n|--------|--------|--------|\r\n| `in` | `<scalar> in <array>`  | `<scalar>` |\r\n|`np.isin` | `<1d array> isin <1d array>` |`<1d array>` |\r\n| `ragged_isin` or `isin_kernel_2d` |`<1d array> isin <2d array>` | `<1d array>` |\r\n\r\nThe new proposed `isin` function is applied for the `cluster` case: `<1d array>` is one of `EFlowTrack`, `EFlowPhoton`,  `EFlowNeutralHadron`, `<2d array>` is `FatJet.Constituents` for one event. In this way, it looks like I have to loop over all events. This does not seem to comply with the principles: if I use an array, it's better drop the loop. And the output shape could be better if it is a 2d since the input is 2d.\r\n\r\nThe code snippets help me a lot how to take the advantage of `numba` (which I should have learned long time before...), so here's `my_isin` that I've tried:\r\n\r\n```python\r\n@nb.njit\r\ndef my_isin(array, test_array):\r\n    results = []\r\n\r\n    for record, test_record in zip(array, test_array):\r\n        mask = np.zeros(len(record), dtype=np.bool_)\r\n\r\n        for i in range(len(record)):\r\n            for j in range(len(test_record)):\r\n                if record[i] == test_record[j]:\r\n                    mask[i] = True\r\n                    break\r\n\r\n        results.append(mask)\r\n\r\n    return results\r\n```\r\nIt assumes that inputs are stacked by records. Now the summary table is:\r\n\r\n| Function | Explanation | Results |\r\n|--------|--------|--------|\r\n| `in` | `<scalar> in <array>`  | `<scalar>` |\r\n|`np.isin` | `<1d array> isin <1d array>` |`<1d array>` |\r\n| `ragged_isin` or `isin_kernel_2d` |`<1d array> isin <2d array>` | `<1d array>` |\r\n| `my_isin` | `<2d array> isin <2d array>` |`<2d array>` |\r\n\r\nSince the first dimension means the total number of records, so it essentially is a `np.isin` with a loop. Combined with the background of jets and their constituents, it could be explained as:\r\n\r\nI don't care about which jet the constituents belong to. I want to cluster jets from all of them just like the `Delphes` does.\r\n\r\nI also use the magical command `%%timeit` to test the performance gain without the loop(https://github.com/scikit-hep/awkward/discussions/3015#discussioncomment-8426594):\r\n\r\n| Case | Time |\r\n|--------|--------|\r\n| no loop | 55.1 ms \u00b1 775 \u00b5s |\r\n| with loop | 456 ms \u00b1 5.98 ms | \r\n\r\nBelow is the complete code:\r\n```python\r\nimport awkward as ak\r\nimport fastjet as fj\r\nimport numba as nb\r\nimport numpy as np\r\nimport uproot\r\nimport vector\r\n\r\nvector.register_awkward()\r\n\r\nfilepath = \"../data/pp2wz/Events/run_01_decayed_1/tag_1_delphes_events.root\"\r\nevents = uproot.open(f\"{filepath}:Delphes\")\r\n\r\nall_ref_ids = events[\"FatJet.Constituents\"].array()[\"refs\"]\r\nall_ref_ids = ak.flatten(all_ref_ids, axis=-1) # ---> New: 100 * var * var * int32 -> 100 * var * int32\r\n                                                                         #                    flatten the last dimension to remove jet group.\r\n\r\nall_tracks = events[\"EFlowTrack.fUniqueID\"].array()\r\nall_photons = events[\"EFlowPhoton.fUniqueID\"].array()\r\nall_neutral_hadrons = events[\"EFlowNeutralHadron.fUniqueID\"].array()\r\n\r\nall_tracks = ak.zip(\r\n    {\r\n        \"pt\": events[\"EFlowTrack.PT\"].array(),\r\n        \"eta\": events[\"EFlowTrack.Eta\"].array(),\r\n        \"phi\": events[\"EFlowTrack.Phi\"].array(),\r\n        \"mass\": events[\"EFlowTrack.Mass\"].array(),\r\n        \"id\": all_tracks,\r\n    },\r\n    with_name=\"Momentum4D\",\r\n)\r\nall_photons = ak.zip(\r\n    {\r\n        \"pt\": events[\"EFlowPhoton.ET\"].array(),\r\n        \"eta\": events[\"EFlowPhoton.Eta\"].array(),\r\n        \"phi\": events[\"EFlowPhoton.Phi\"].array(),\r\n        \"mass\": ak.zeros_like(events[\"EFlowPhoton.ET\"].array()),\r\n        \"id\": all_photons,\r\n    },\r\n    with_name=\"Momentum4D\",\r\n)\r\nall_neutral_hadrons = ak.zip(\r\n    {\r\n        \"pt\": events[\"EFlowNeutralHadron.ET\"].array(),\r\n        \"eta\": events[\"EFlowNeutralHadron.Eta\"].array(),\r\n        \"phi\": events[\"EFlowNeutralHadron.Phi\"].array(),\r\n        \"mass\": ak.zeros_like(events[\"EFlowNeutralHadron.ET\"].array()),\r\n        \"id\": all_neutral_hadrons,\r\n    },\r\n    with_name=\"Momentum4D\",\r\n)\r\n\r\n@nb.njit\r\ndef my_isin(array, test_array):\r\n    results = []\r\n\r\n    for record, test_record in zip(array, test_array):\r\n        mask = np.zeros(len(record), dtype=np.bool_)\r\n\r\n        for i in range(len(record)):\r\n            for j in range(len(test_record)):\r\n                if record[i] == test_record[j]:\r\n                    mask[i] = True\r\n                    break\r\n\r\n        results.append(mask)\r\n\r\n    return results\r\n\r\nmatched_tracks = all_tracks[my_isin(all_tracks.id, all_ref_ids)]\r\nmatched_photons = all_photons[my_isin(all_photons.id, all_ref_ids)]\r\nmatched_neutral_hadrons = all_neutral_hadrons[\r\n    my_isin(all_neutral_hadrons.id, all_ref_ids)\r\n]\r\n\r\nall_constituents = ak.concatenate(\r\n    [matched_tracks, matched_photons, matched_neutral_hadrons], axis=1\r\n)\r\n\r\nparticles = ak.zip(\r\n    {\r\n        \"pt\": all_constituents.pt,\r\n        \"eta\": all_constituents.eta,\r\n        \"phi\": all_constituents.phi,\r\n        \"mass\": all_constituents.mass,\r\n    },\r\n    with_name=\"Momentum4D\",\r\n)\r\n\r\njet_def = fj.JetDefinition(fj.antikt_algorithm, 1.0)\r\ncluster = fj.ClusterSequence(particles, jet_def)\r\njets = cluster.inclusive_jets()\r\n\r\nprint(f\"pt: {jets[6, 0].pt}\")\r\nprint(f\"eta: {jets[6, 0].eta}\")\r\nprint(f\"phi: {jets[6, 0].phi}\")\r\nprint(f\"mass: {jets[6, 0].m}\")\r\n\r\n# pt: 563.0124337033961\r\n# eta: 1.0792125821032512\r\n# phi: 2.0480975146837075\r\n# mass: 178.84373299160538\r\n```\r\n\r\n\r\n\r\n",
     "createdAt":"2024-02-15T04:19:34Z",
     "number":8474837,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"Star9daisy"
     },
     "body":"Since `isin` could only determine weather or not \"array\" in the \"test_array\" in elementwise way, inspired by your answers, I improve the function `my_isin` as following:\r\n\r\n```python\r\n@nb.njit\r\ndef find_1d_in_1d(a, b):\r\n    index_array = []\r\n\r\n    for record, test_record in zip(a, b):\r\n        indices = []\r\n        for i in range(len(record)):\r\n            for j in range(len(test_record)):\r\n                if record[i] == test_record[j]:\r\n                    indices.append(i)\r\n                    break\r\n\r\n        index_array.append(indices)\r\n\r\n    return index_array\r\n```\r\n\r\nIn the question example, it can be used like:\r\n```python\r\nimport awkward as ak\r\n\r\narray = ak.Array(\r\n    [\r\n        [1, 2, 3],\r\n        [],\r\n        [4, 5],\r\n        [6, 7, 8, 9],\r\n    ]\r\n)\r\n\r\ntest_array = ak.Array(\r\n    [\r\n        [0, 2],\r\n        [],\r\n        [4],\r\n        [6, 8],\r\n    ]\r\n)\r\n\r\nprint(find_1d_in_1d(array, test_array))\r\n# [[1], [], [0], [0, 2]]\r\n\r\n# Index the corresponding elements in array\r\nprint(array[find_1d_in_1d(array, test_array)])\r\n# [[2], [], [4], [6, 8]]\r\n```\r\n\r\nAnd this is the 2d case:\r\n```python\r\n@nb.njit\r\ndef find_1d_in_2d(a, b):\r\n    index_array = []\r\n\r\n    for record_a, record_b in zip(a, b):\r\n        indices_per_a = []\r\n\r\n        for i in range(len(record_b)):\r\n            indices_per_b = []\r\n            for j in range(len(record_b[i])):\r\n                for k in range(len(record_a)):\r\n                    if record_b[i][j] == record_a[k]:\r\n                        indices_per_b.append(k)\r\n\r\n            indices_per_a.append(indices_per_b)\r\n        index_array.append(indices_per_a)\r\n\r\n    return index_array\r\n```\r\n```python\r\nimport awkward as ak\r\n\r\narray = ak.Array(\r\n    [\r\n        [1, 2, 3],\r\n        [],\r\n        [4, 5],\r\n        [6, 7, 8, 9],\r\n    ]\r\n)\r\n\r\ntest_array = ak.Array(\r\n    [\r\n        [[0, 2], [1, 2, 3]],\r\n        [[]],\r\n        [[4]],\r\n        [[6, 8]],\r\n    ]\r\n)\r\n\r\nprint(find_1d_in_2d(array, test_array))\r\n# [[[1], [0, 1, 2]], [[]], [[0]], [[0, 2]]]\r\n```\r\n\r\nHowever, after I check [link](https://awkward-array.org/doc/main/user-guide/how-to-filter-ragged.html#how-to-filter-with-ragged-arrays), there's no \"fancy index\" in awkward like [numpy](https://numpy.org/doc/stable/reference/generated/numpy.take.html). \r\n\r\nIf `awkward` could support this feature or I could do something about these, it would be better. But I'm not sure this feature is needed in wider cases. In my situation, reclustering constituents of jets also needs the `fastjet` support, so this may be too much to finish though...",
     "createdAt":"2024-02-17T11:00:50Z",
     "number":8500138,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Awkward Array does implement a kind of fancy indexing -- if you build a ragged array with the same structure (ignoring regular vs ragged; you need at least one ragged dimension in the index array), you can pull out particular items from a given dimension. You've linked to the tutorial that demonstrates this.\r\n\r\nI'm aware that you have more questions, but I'm also not entirely sure what you're asking / trying to do. So, let me give a bit more context to hopefully provide some guidance!\r\n\r\nAwkward Array tries to provide tools for you to perform useful analyses over ragged arrays. We don't have all operations, e.g. `np.isin`, and in those cases you'll need to implement them yourself. There are two \"classes\" of ragged operations:\r\n1. Operations that look at the _exterior_ `axis=-1`\r\n2. Operations that look at _interior_ `axis!=-1`\r\n\r\nImplementing custom variants of (1) is trivial if you can write a 2D kernel:\r\n- Implement the operations for the 2D case (where `axis=-1` is `axis=1`).\r\n- Flatten / recurse until you have an array of 2D\r\n- Apply the operation\r\n- Unflatten until you have your original structure.\r\n\r\nYou can perform manual flattening/unflattening operations yourself, or you can use `ak.transform` (which implements the recursion automatically).\r\n\r\nAlternatively, you can just write a Numba function that handles an array of that dimensionality, i.e. a 2D function, a 3D function, etc. It's generally better to just write a 2D kernel and use recursion, though, because of all of the other features that Awkward supports (like option types) - these are free if you use recursion in most cases.\r\n\r\nIf you want to operate along _interior_ axes, then it is more complicated. These are not just \"flatten-transform-unflatten\" operations, these are \"swapaxes-flatten-transform-unflatten-unswapaxes\", which we don't provide any constructs to implement yourself. However, in my experience, nearly _all_ operations that need to be implemented with a custom kernel do not usually involve operations on interior axes. \r\n\r\nSo, if your question is: \"how do I find the indices of elements from one array in another, and use those indices to restructure the array\", then we definitely have constructs for this. You'll need to write the \"is_in\" kernel yourself, but the rest is just [`ak.transform`](https://awkward-array.org/doc/main/reference/generated/ak.transform.html).\r\n\r\n\r\n",
        "createdAt":"2024-02-19T10:37:40Z",
        "number":8515654
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":6
  },
  "createdAt":"2024-02-09T04:11:55Z",
  "number":3015,
  "title":"Any equivalent operation like np.argwhere?",
  "url":"https://github.com/scikit-hep/awkward/discussions/3015"
 }
]
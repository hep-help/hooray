[
 {
  "author_association":"NONE",
  "body":"@jpivarski  Does the [sort-by-upvotes view of the open issues](https://github.com/scikit-hep/awkward-1.0/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc) aggregate the total number of upvotes in the thread, or just the number of upvotes on the initial post?\r\nAt the time if writing, the second result seems to have a single upvote quite a bit down the thread... this behaviour might not be the intended one when using it to prioritize issues?",
  "created_at":"2021-01-04T11:14:15Z",
  "id":753915500,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzkxNTUwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T11:14:43Z",
  "user":"MDQ6VXNlcjY0MTMzMzg2"
 },
 {
  "author_association":"MEMBER",
  "body":"I'll modify the recommended search string to include a filter for nonzero reactions (on the initial comment): [is:issue is:open sort:reactions-+1-desc reactions:>0](https://github.com/scikit-hep/awkward-1.0/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+reactions%3A%3E0+). Only two issues actually have reactions on the initial comment because we've never done this voting thing before.",
  "created_at":"2021-01-04T16:59:54Z",
  "id":754092892,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDA5Mjg5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T16:59:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I'll modify the recommended search string to include a filter for nonzero reactions\r\n\r\nDone: https://github.com/scikit-hep/awkward-1.0/commit/97124efaf9f7f17dc0ae17410ae1113c99f1f147.\r\n\r\nNow if anyone is trying to vote, they can check against this list to see if they've actually done it; this might be enough to see that the reaction has to be on the initial comment (or at least, it would prompt a search, rather than just assuming that it worked).",
  "created_at":"2021-01-04T17:52:38Z",
  "id":754120401,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDEyMDQwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T17:52:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Hi @stevesimmons! Let me know if you're willing and able to do this (date-time types in Awkward Array).\r\n> \r\n> If not, I'll move it out of \"in progress,\" but it will be a high priority item for me. There's evidently a lot of interest.\r\n\r\nI'm afraid I have too many other things on my plate right now and won't be able to work on this. Sorry about that, because this is a feature than lots of people clearly want.\r\n\r\n",
  "created_at":"2021-02-02T21:47:21Z",
  "id":772016805,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MjAxNjgwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-02T21:47:21Z",
  "user":"MDQ6VXNlcjI4MzA0NTk="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for giving it a look! I understand that things come up. Also, this _does_ seem to be the most requested feature right now, so it's high on my priority list, too. I'll feel free to work on an implementation when I get a chance.",
  "created_at":"2021-02-02T21:54:51Z",
  "id":772024234,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MjAyNDIzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-02T21:54:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@drahnreb - please, let me know if this is what you'd expect. Thanks!\r\n```python\r\n    values = {\"time\": [\"20190902093000\", \"20190913093000\", \"20190921200000\"]}\r\n    df = pandas.DataFrame(values, columns=[\"time\"])\r\n    df[\"time\"] = pandas.to_datetime(df[\"time\"], format=\"%Y%m%d%H%M%S\")\r\n    array = ak.layout.NumpyArray(df)\r\n    assert ak.to_list(array) == [\r\n        np.datetime64(\"2019-09-02T09:30:00\"),\r\n        np.datetime64(\"2019-09-13T09:30:00\"),\r\n        np.datetime64(\"2019-09-21T20:00:00\"),\r\n    ]\r\n```",
  "created_at":"2021-06-10T12:34:09Z",
  "id":858582195,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODU4MjE5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T12:34:09Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@ianna - thanks for the huge PR! I could only follow it loosely and have apparently not commented a lot - sorry.\r\n\r\n> let me know if this is what you'd expect\r\n\r\nYes, this looks good to me.\r\n\r\nThe type casting to `int` instead of `datetime.datetime` when `dtype=datetime64[ns]`, I would handle like `numpy`, as mentioned [here](https://github.com/scikit-hep/awkward-1.0/pull/835#discussion_r649397958).\r\n\r\nI pulled your latest version 2aa2ed5377438b0736d93ab7427aec871c4aaeb0 and did a few tests with a `parquet` dataset similar to the one I had when I raised this issue. Awkward does not convert to datetime field correctly at this point of time. \r\n`<Array [1970-01-19T16:32:31.555000000, ... ] type='14709 * ?datetime'>`\r\ninstead of `array(['2021-03-03T06:05:55.000000000', .... ], dtype='datetime64[ns]')`\r\nWhile `pandas` and `pyarrow` (to `timestamp[us]`) do handle it correctly.\r\n\r\nI need to investigate and try to give you a reproducer. I will update you shortly.",
  "created_at":"2021-06-10T18:23:10Z",
  "id":858870332,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODg3MDMzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T18:23:10Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"@drahnreb I just got this message, too late to stop the auto-merge. Is it incorrect in main? (I corrected a few Arrow-related things at the end, which might be relevant.)\r\n\r\nIf it's still broken, fixing it can be a new PR.",
  "created_at":"2021-06-10T19:47:45Z",
  "id":858982740,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODk4Mjc0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T19:47:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski Yes, it's fixed. Thanks, great update!\r\n\r\nNot sure if [this](https://github.com/scikit-hep/awkward-1.0/pull/835#pullrequestreview-680703265) and my comment passed unnoticed.\r\nThis is [yet another way](https://stackoverflow.com/a/21916253/7622769) of dealing with datetime types (besides the confusing and different ways in pandas, numpy and pyarrow including your reported issue). But after giving it a bit of thought it is probably the most concise way. It should be documented though that `ak.to_list()` always creates lists of `np.datetime64` objects independent from - but with the same - input unit.\r\n\r\n```python\r\n# numpy converts to `py_datetime` iff possible and same type...\r\n>>> np_ms = np.asarray([np.datetime64('2019-09-02T09:30:00', 'ms')])\r\n>>> ak_ms = ak.Array(np_ms)\r\n>>> np_ms.tolist()\r\n[datetime.datetime(2019, 9, 2, 9, 30)]\r\n>>> ak_ms.tolist()\r\n[numpy.datetime64('2019-09-02T09:30:00.000000000')]\r\n\r\n# ...but `py_datetime` does not support `ns` scaled units\r\n>>> np_ns = np.asarray([np.datetime64('2019-09-02T09:30:00', 'ns')])\r\n>>> ak_ns = ak.Array(np_ns)\r\n>>> np_ns.tolist()\r\n[1567416600000000000]\r\n>>> ak_ns.tolist()\r\n[numpy.datetime64('2019-09-02T09:30:00.000000000')]\r\n\r\n# ...but `option` typed numpy arrays are of dtype object\r\n>>> np_ms_opt = np.asarray([np.datetime64('2019-09-02T09:30:00', 'ms'), None])\r\n>>> ak_ms_opt = ak.Array(np_ms_opt)\r\n>>> np_ms_opt.tolist()\r\n[numpy.datetime64('2019-09-02T09:30:00.000'), None]\r\n>>> ak_ms_opt.tolist()\r\n[numpy.datetime64('2019-09-02T09:30:00.000000000'), None]\r\n```\r\n\r\nI guess the reasoning for numpy is that a list should preferably hold python native datetime objects if possible/supported.\r\nI do not see a problem here (people are used to unintuitive datetime conversions for a long time). As the current implementation is not vectorized, `ak.to_list()` is also not performance critical? One could argue, why not just converting to pure `int64` (that will prevent convenient arithmetics in pure python and then there is also [`int96` coercing of timestamps](https://issues.apache.org/jira/browse/ARROW-2026))...",
  "created_at":"2021-06-11T00:15:24Z",
  "id":859169528,
  "issue":367,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTE2OTUyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T00:15:24Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"I finally got to this one! The new features are documented, but here's a diff:\r\n\r\n```python\r\nak.to_parquet(array, \"directory-name/filename.parquet\")\r\n```\r\n\r\nstill only makes one (non-partitioned) Parquet file, but\r\n\r\n```python\r\nak.to_parquet.dataset(\"directory-name\")\r\n```\r\n\r\n\"seals\" a directory with the `_common_metadata` and `_metadata` files. My thinking is that a dataset large enough to be split into multiple files will probably be made by independent processes, so collecting metadata is something you'd want to do after the fact, not during the writing.\r\n\r\nNext,\r\n\r\n```python\r\nak.from_parquet(\"directory-name\")\r\n```\r\n\r\nwill now read metadata from the `_metadata` file preferentially, but if it's not there (Pandas does not create one, at least with my version), it will examine the metadata in all the files in that directory, since we can read the metadata without reading the bulk data.\r\n\r\nIf any of the subdirectory names inside the directory of Parquet files match patterns like `[^=]+=[^=]*`, those name-value pairs are interpreted as columns, since they were columns in the original Pandas data. However, there's no type information anywhere, so I assume they're strings, and it would be cumbersome to lazily load them, so they're materialized right away, even if with `lazy=True`. Their memory footprint is minimized using an [IndexedArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedArray.html), and they can be excluded by setting `include_partition_columns=False`.\r\n\r\n\r\n",
  "created_at":"2021-02-05T22:59:43Z",
  "id":774333546,
  "issue":368,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NDMzMzU0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-05T22:59:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This is great. Thank you. I will try later (and will have a closer look at the code and the other issue).",
  "created_at":"2021-02-06T09:24:13Z",
  "id":774432234,
  "issue":368,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NDQzMjIzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-06T09:24:13Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"PR #733 adds a low-level primitive that enables \"group by\" operations. Primarily, it's for many-nested \"group by,\" as most things in Awkward Array are.\r\n\r\nFrom the documentation (not yet online):\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/6ad84136711fd63ba0d106dccc60f4cd33333ebb/src/awkward/operations/structure.py#L300-L332",
  "created_at":"2021-02-12T03:23:22Z",
  "id":777944417,
  "issue":386,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3Nzk0NDQxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-12T03:23:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This should have been linked to #652: it's done now!",
  "created_at":"2021-02-02T16:09:46Z",
  "id":771747083,
  "issue":392,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MTc0NzA4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-02T16:09:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I haven't found enough time for this.\r\nIf anyone stumbles over it and really needs to do this in-mem op, this one-liner might get you started:\r\n\r\n```python\r\nimport pandas as pd\r\nimport awkward as ak\r\nimport pyarrow as pa\r\n\r\nak.from_pandas = lambda df, columns=None, preserve_index=False: ak.from_arrow(pa.Table.from_pandas(df, preserve_index=preserve_index, columns=columns))\r\n\r\na_df = pd.DataFrame([{'one':1, 'two':2}])\r\nawk_arr = ak.from_pandas(a_df)\r\n```\r\n\r\nThis mostly suits my rare pandas to awkward transitions. It is one of my few other awkward \"hacky extensions\" (kind of a cheat sheet)...\r\n",
  "created_at":"2021-04-25T21:51:10Z",
  "id":826395604,
  "issue":405,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNjM5NTYwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-25T21:51:10Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sorry to revive an old issue, but unfortunately, it seems that this issue has resurfaced after updating to the new versions of awkward (1.2.0rc2) and pybind11 (2.6.2). Just trying to pass an awkward array into a c++ function on ubuntu 18.04, I get:\r\n\r\n```\r\n========================================================================================== FAILURES ===========================================================================================\r\n______________________________________________________________________________________ test_numpy_array _______________________________________________________________________________________\r\n\r\n    def test_numpy_array() -> None:\r\n>       pba.awkward_test_numpy_array(ak.Array([1,2,3]).layout)\r\nE       TypeError: awkward_test_numpy_array(): incompatible function arguments. The following argument types are supported:\r\nE           1. (arr: awkward::NumpyArray) -> awkward::NumpyArray\r\nE\r\nE       Invoked with: <NumpyArray format=\"l\" shape=\"3\" data=\"1 2 3\" at=\"0x562381da0240\"/>\r\n\r\n\r\ntests/test_simple.py:6: TypeError\r\n_____________________________________________________________________________________ test_awkward_array ______________________________________________________________________________________\r\n\r\n    def test_awkward_array() -> None:\r\n>       pba.awkward_test(ak.Array([1,2,3]).layout)\r\nE       TypeError: awkward_test(): incompatible function arguments. The following argument types are supported:\r\nE           1. (arr: awkward::Content) -> awkward::Content\r\nE\r\nE       Invoked with: <NumpyArray format=\"l\" shape=\"3\" data=\"1 2 3\" at=\"0x562381dada40\"/>\r\n\r\n\r\ntests/test_simple.py:9: TypeError\r\n=================================================================================== short test summary info ===================================================================================\r\nFAILED tests/test_simple.py::test_numpy_array - TypeError: awkward_test_numpy_array(): incompatible function arguments. The following argument types are supported:\r\nFAILED tests/test_simple.py::test_awkward_array - TypeError: awkward_test(): incompatible function arguments. The following argument types are supported:\r\n====================================================================================== 2 failed in 0.30s ======================================================================================\r\n```\r\n\r\nThe full reproducer is here: https://github.com/raymondEhlers/pybind11-awkward-array-test . It was a few c++ functions in `pybind11_awkward/src/binding.cpp`, and then the tests leading to the failures above are in `tests`. Unfortunately, it's built with poetry, but you should be able to build it with pep 517:\r\n\r\n```bash\r\n# in some virtualenv\r\npip install .\r\npytest tests/test_simple.py\r\n```\r\n\r\nI must be doing something trivially wrong, but I can't seem to find it. Any advice is appreciated! (I can also start a new issue)",
  "created_at":"2021-03-02T17:04:52Z",
  "id":789061311,
  "issue":483,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4OTA2MTMxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-02T17:06:54Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"This has been on the back of my mind for a long time (a lot of things have, actually), and I think I have a solution (in the [jpivarski/SliceVarNewAxis](https://github.com/scikit-hep/awkward-1.0/tree/jpivarski/SliceVarNewAxis) branch, which will be merged into [jpivarski/fixes-for-scipy-prep](https://github.com/scikit-hep/awkward-1.0/tree/jpivarski/fixes-for-scipy-prep), which will be merged into [main](https://github.com/scikit-hep/awkward-1.0)\u2014it was a long digression).\r\n\r\nRegular dimensions of length 1 can now be mixed with variable-length dimensions in an array used as a slice. Regular dimensions of length 1 are created by `np.newaxis` or `None`, which is the NumPy idiom for the same behavior, except that we hadn't allowed it to mix with variable-length dimensions until now. (It's changing what used to be an error into a usable idiom.)\r\n\r\nFor example, if\r\n\r\n```python\r\n>>> array = ak.Array([[[ 0,  1,  2,  3,  4],\r\n...                    [ 5,  6,  7,  8,  9],\r\n...                    [10, 11, 12, 13, 14]],\r\n...                   [[15, 16, 17, 18, 19],\r\n...                    [20, 21, 22, 23, 24],\r\n...                    [25, 26, 27, 28, 29]]])\r\n>>> array\r\n<Array [[[0, 1, 2, 3, 4, ... 26, 27, 28, 29]]] type='2 * var * var * int64'>\r\n```\r\n\r\nis our array and\r\n\r\n```python\r\n>>> slicer = ak.Array([[3, 4], [0, 1, 2, 3]])\r\n>>> slicer\r\n<Array [[3, 4], [0, 1, 2, 3]] type='2 * var * int64'>\r\n```\r\n\r\nis what we're going to use to slice it, notice that the `array` is `2 * var * var * int64` and the `slicer` is `2 * var * int64`. To make them compatible, we need to add a dimension to the `slicer`:\r\n\r\n```python\r\n>>> slicer[:, np.newaxis]\r\n<Array [[[3, 4]], [[0, 1, 2, 3]]] type='2 * 1 * var * int64'>\r\n```\r\n\r\nNow the outermost `2` matches and the innermost `var` matches, but we have a `1` in `slicer` where we have `var` in `array`. This is now legal:\r\n\r\n```python\r\n>>> array[slicer[:, np.newaxis]]\r\n<Array [[[3, 4], [8, 9, ... [25, 26, 27, 28]]] type='2 * var * var * int64'>\r\n>>> array[slicer[:, np.newaxis]].tolist()\r\n[[[3, 4],\r\n  [8, 9],\r\n  [13, 14]],\r\n [[15, 16, 17, 18],\r\n  [20, 21, 22, 23],\r\n  [25, 26, 27, 28]]]\r\n```\r\n\r\nThe same applies for boolean arrays:\r\n\r\n```python\r\n>>> slicer = ak.Array([[False, False, False,  True,  True],\r\n...                    [ True,  True,  True,  True, False]])\r\n>>> array[slicer[:, np.newaxis]]\r\n<Array [[[3, 4], [8, 9, ... [25, 26, 27, 28]]] type='2 * var * var * int64'>\r\n>>> array[slicer[:, np.newaxis]].tolist()\r\n[[[3, 4],\r\n  [8, 9],\r\n  [13, 14]],\r\n [[15, 16, 17, 18],\r\n  [20, 21, 22, 23],\r\n  [25, 26, 27, 28]]]\r\n```\r\n\r\nI _think_ this meets the need you described above. Anyway, I'll close the issue now and if you disagree, you can reply and I'll hear it. Or open a new issue or Discussion. Thanks!",
  "created_at":"2021-02-02T08:47:20Z",
  "id":771471682,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MTQ3MTY4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-02T08:47:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks Jim! We will give it a try.",
  "created_at":"2021-02-02T15:11:23Z",
  "id":771703296,
  "issue":492,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MTcwMzI5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-02T15:11:23Z",
  "user":"MDQ6VXNlcjQxMzQ3ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"See discussion #1027; this feature was ill-conceived and led to unexpected interactions. I'll be removing it in `awkward>1.4.0`; these slices are still possible, but we'll have to explicitly set up the slicer to fit the array to be sliced, rather than taking regular length-1 dimensions to implicitly perform this. Maybe new functions will be needed, which is what you suggested when you first opened the issue.",
  "created_at":"2021-07-21T15:41:17Z",
  "id":884290492,
  "issue":492,
  "node_id":"IC_kwDODBCWws40tTO8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-21T15:41:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@agoose77 This is the issue I was thinking of.\r\n\r\nTo catch up @henryiii on something we talked about in the Awkward-Dask meeting: @martindurant is thinking about a suite of string functions, like everything in `np.char.*` but with UTF-8 capabilities, vectorized across Awkward Arrays. (This might be a killer app for Awkward in industry.) The short path to implementing this is through Rust because Rust has a good UTF-8 library, in which questions like \"what does it mean to capitalize?\" have already been thought through. and it would be higher performance than multi-pass NumPy-based implementations [like this equality operation](https://github.com/scikit-hep/awkward-1.0/blob/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b/src/awkward/behaviors/string.py#L124-L149).\r\n\r\nStrings are a high-level `ak.behavior` in Awkward Array, but foundational ones that should included when one does `pip install awkward` (unlike, say, Lorentz vectors, for which there's `pip install vector awkward`). Rust complicates the build procedure, and a clean way of avoiding that would be to put them in a separate PyPI (and conda-forge) package: `awkward-string-kernels`. Since strings are foundational, `awkward` would _strictly depend_ on `awkward-string-kernels`.\r\n\r\nNaturally, that reminded me of this project, of externalizing the compiled code into a package that `awkward` strictly depends on. An advantage that @martindurant brought up, which I hadn't thought of, is that any potential contributor who doesn't need to touch kernels or C++ would get those binaries from PyPI/conda-forge and be able to operate on a pure Python `awkward` codebase. With v2, that includes a much larger fraction of the codebase.\r\n\r\nAs an overview, the eventual project would involve the following pieces, which could all be separate packages:\r\n\r\n   * `awkward-string-kernels`, as described above; probably a Rust-compiled shared object library with a ctypes interface that a Python part interacts with: that Python provides behaviors for `ak.beahviors`.\r\n   * `awkward-cpu-kernels`, which are \"nearly C,\" using C++ only for numerical type templating. If there's ever any desire to make them pure C, that wouldn't be difficult for most of them (replacing templates with `#define`: yuck!) but would be a problem for the sorting and uniqueness kernels (which use the C++ standard library). I doubt it's necessary, though: C++ compilers are ubiquitous. This part does not use pybind11.\r\n   * `libawkward`: three features are fully written in C++ (11) and rely on it for performance: ArrayBuilder, LayoutBuilder, and AwkwardForth. This part uses pybind11. We had been talking about `awkward-cpu-kernels` and `libawkward` being part of the same package, but the two pieces come apart easily and they differ in how seriously they use C++ features, and only `libawkward` needs pybind11. What do you think about them being separate?\r\n   * `awkward-cuda-kernels`, which should be a separate package because it's optional (the only optional one in this list). It has yet different build constraints: it needs CUDA and nvcc.\r\n   * All the rest of Awkward Array, starting in version 2.0, can be pure Python. `pip install awkward` would pull in everything but `awkward-cuda-kernels` as strict dependencies, and `pip install awkward[cuda]` would pull in CUDA.\r\n\r\nThis pattern suggests generalization: `awkward-cpu-kernels` has a lot of \"nearly C\" functions and a few that make use of the C++ standard library, which are also hard to imagine porting to CUDA, so maybe that splits up into `awkward-cpu-kernels` and `awkward-sorting-kernels`. The `awkward-string-kernels` likewise may be difficult to port to CUDA, so making it granular like this could help to explain which things can be done on the GPU and which can't: i.e. \"`awkward-cuda-kernels` implements all `awkward-cpu-kernels`, but not any of the specialized ones.\"\r\n\r\nEach of the kernels packages could have its own [kernels specification](https://github.com/scikit-hep/awkward-1.0/blob/main/kernel-specification.yml), in the same YAML format, which `awkward` uses to generate ctypes interfaces (with more information than a header file: the \"in vs out\" of each array argument is important).",
  "created_at":"2021-10-28T15:59:24Z",
  "id":953983826,
  "issue":503,
  "node_id":"IC_kwDODBCWws443KNS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T15:59:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and I should probably also mention that each of the packages that the `awkward` package depends on would be pinned to identical version numbers, since the coupling between kernels, ArrayBuilder/LayoutBuilder/AwkwardForth, and downstream Awkward Array behavior is tight, not loose. Since matching versions is important, they would probably all be built from this one GitHub repo, so that they have shared version control.\r\n\r\n(But it would also be advantageous for them to have separate CI, since we don't want to have to wait for recompilation when we're making Python changes, and we're imagining future contributors to be much more active on the Python part. Maybe that adds too much complexity, though.)",
  "created_at":"2021-10-28T16:06:15Z",
  "id":953989437,
  "issue":503,
  "node_id":"IC_kwDODBCWws443Lk9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T16:06:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"More on this later, but a few quick thoughts:\r\n\r\nRust shouldn't be too much of an issue, but it will be a bit of \"good trouble\", I expect. There's been quite some interest in Rust - Brett Cannon, a core dev for CPython and SC member has written the Python Launcher for Unix in Rust. There have also been a few projects moving to Rust. I don't think there have been too many packages on PyPI using Rust, but I expect that to change. We might have to pioneer the usage of Rust here, but it could be very helpful for other projects. (And Rust is a fantastic lowish-level language, I'd take it over C any day :) )\r\n\r\nC++ internals with a C interface is perfectly fine. Don't try to hack together templates by hand with macros. :)\r\n\r\nThis reminds me that different packages from the same file might be an important thing to consider for scikit-build, as well as possibly interfacing to native language build systems (Rust, Go). I expect there will be a general grouping into separate repositories based on build/ci needs, and the (small number) of repos will then have some number of packages in each. Parts that don't get changed as often ideally could have a slower version number than the Python wrappers, etc, which change more frequently. (I wish boost-histogram had been split this way).\r\n",
  "created_at":"2021-10-28T17:14:08Z",
  "id":954042000,
  "issue":503,
  "node_id":"IC_kwDODBCWws443YaQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T17:14:32Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"(About Rust in particular, I strongly wanted to make Awkward 1.0 my first Rust project, since we determined that it had to have a compiled part, but I chose C++ for compatibility reasons. Those compatibility reasons being completely misguided is a large part of why we're moving most of it into Python now. But in a parallel universe somewhere, the kernels, ArrayBuilder, LayoutBuilder, and AwkwardForth are all implemented in Rust. Maybe starting with `awkward-string-kernels` in Rust would be a first step toward reconvergence with that alternate world...)",
  "created_at":"2021-10-28T17:55:43Z",
  "id":954071533,
  "issue":503,
  "node_id":"IC_kwDODBCWws443fnt",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T17:55:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like Pandas calls this `nlargest`: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nlargest.html\r\n\r\n",
  "created_at":"2021-08-02T16:40:34Z",
  "id":891170247,
  "issue":554,
  "node_id":"IC_kwDODBCWws41Hi3H",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-02T16:40:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This was addressed (as much as is possible) in v2.",
  "created_at":"2021-12-07T21:19:00Z",
  "id":988270107,
  "issue":579,
  "node_id":"IC_kwDODBCWws46584b",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-12-07T21:19:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I add myself here, as discussed over email to help with this project.",
  "created_at":"2021-03-17T11:31:03Z",
  "id":801009325,
  "issue":588,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMTAwOTMyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-17T11:31:03Z",
  "user":"MDQ6VXNlcjUyNjgyMzc="
 },
 {
  "author_association":"NONE",
  "body":"Argh, never replied, sorry! A RDataSource to feed awkward arrays into RDF is the correct thing to use, and to output awkward arrays a custom action can be added to RDF using the [Book](https://root.cern/doc/master/classROOT_1_1RDF_1_1RInterface.html#a9ed8313806398d106bfc2390301c0408) method (and a nice alias can be monkey-patched via Python).",
  "created_at":"2021-03-17T11:37:29Z",
  "id":801013053,
  "issue":588,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMTAxMzA1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-17T11:37:29Z",
  "user":"MDQ6VXNlcjEwOTk5MDM0"
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't know about `Book`, and it does look like the right way to get a set of columns out. If it can be made to return a dict of arrays in Python, I can follow up with [ak.from_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_buffers.html). I wasn't thinking that `AsNumpy` would be right because these arrays would have to be a different view of the columns than the user interface actions (such as `Define`) see: the user should see `std::vectors` and  `structs` but `ak.from_buffers` should get one-dimensional arrays of offsets and content (and some metadata that can be used to make the [ak.forms.Form](https://awkward-array.readthedocs.io/en/latest/ak.forms.Form.html) that Awkward Array needs to build the output array.\r\n\r\nThe last action would also need to be non-lazy, so I guess I'll need something after `Book` to trigger execution.",
  "created_at":"2021-03-17T12:30:37Z",
  "id":801043658,
  "issue":588,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMTA0MzY1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-17T12:30:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"`Book` per se is a C++ thing, it cannot return Python dicts, but I guess the idea is that `Book(AwkwardArrayMaker{}, ...)` would inject C++ logic into `RDataFrame`'s event loop (giving you access to multi-threading, laziness, etc) and return a C++ object, and then the monkey-patched Python method can \"decorate\" that `Book` invocation to make it more Pythonic and, if you want, to make it eager, e.g. `df.AsAwkward(\"x\")` would execute something like (I'm simplifying a bit):\r\n\r\n```python\r\ndef AsAwkward(dataframe, column):\r\n  awkArrMaker = AwkwardArrayMaker()\r\n  res = df.Book(awkArrMaker, column).GetValue() # calling GetValue right away makes this eager\r\n  # res is now some C++ struct exposed to Python via PyROOT\r\n  # you can use it from Python or invoke a helper C++ function that converts C++ arrays into awkward arrays, or whatever you want\r\n```",
  "created_at":"2021-03-17T13:25:43Z",
  "id":801077920,
  "issue":588,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMTA3NzkyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-17T13:25:43Z",
  "user":"MDQ6VXNlcjEwOTk5MDM0"
 },
 {
  "author_association":"MEMBER",
  "body":"That is helpful, thanks! I will want to do a part of the translation on the C++ side, in order to make C++ native objects like `std::vectors` and `structs` that would be the user's interface when they're writing `Define`, `Filter`, etc. From a global view, this is how I see the conversion:\r\n\r\n   1. Awkward Array goes through [ak.to_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_buffers.html) in Python to break it down into 1-dimensional arrays of contents and indexes (such as \"offsets\" for lists).\r\n   2. A particular array generates custom C++ code (the code-generation would be in Python) to turn those 1-dimensional arrays into the appropriate C++ objects, such as `std::vectors` and `structs`, with whatever nesting is appropriate for the ak.Array's data type.\r\n   3. Users write `Define`, `Filter`, etc., operating exclusively on C++ objects.\r\n   4. The output C++ objects get converted into a set of new 1-dimensional arrays, again using C++ code generated from the Python side. This would likely be a `Book`.\r\n   5. These 1-dimensional arrays are gathered into a dict and passed into [ak.from_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_buffers.html) on the Python side, probably in a monkey-patched RDataFrame method named `AsAwkward` (for symmetry with `AsNumpy`), to turn the output into Awkward Arrays.\r\n\r\nSteps 1\u20122 are independent of steps 4\u20125. Someone might do one, the other, or both.\r\n\r\nThere's something I hadn't considered about step 4, though: I need to inspect the C++ output types of a node in the RDataFrame DAG from Python. The reflection information must be available for RDataFrame to work, but is it publicly accessible? For instance, given a Python object representing a node in an RDataFrame ([ROOT.RDF.RInterface](https://root.cern/doc/master/classROOT_1_1RDF_1_1RInterface.html)?), is it possible to get the names of defined columns and their types? How are their types represented ([ROOT.TClass](https://root.cern/doc/master/classTClass.html)? if so, what about numbers and STL collections)? That's what I'd need to know to generate the appropriate C++ when an `AsAwkward` node is added to an RInterface.",
  "created_at":"2021-03-17T14:05:50Z",
  "id":801108746,
  "issue":588,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMTEwODc0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-17T14:05:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"RDF has a `GetColumnType(\"x\")` method that returns the type of column `\"x\"` as a `std::string`. Collections take the type `RVec<T>`, where [RVec](https://root.cern/doc/master/classROOT_1_1VecOps_1_1RVec.html) is the numpy-like vector type that RDF users use to manipulate collections.\r\n\r\n`GetColumnType` is not _fast_, but if that ever becomes the bottleneck (I doubt it) there are options.",
  "created_at":"2021-03-17T14:17:08Z",
  "id":801117704,
  "issue":588,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMTExNzcwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-17T14:17:08Z",
  "user":"MDQ6VXNlcjEwOTk5MDM0"
 },
 {
  "author_association":"MEMBER",
  "body":"A string is probably best, anyway. I can parse templates to get all the STL structures, and if I encounter any opaque class names, I can look them up with `ROOT.TClass.GetClass` to get their fields. There might be type aliases (such as `Size_t`), but that can be a lookup table that grows as we encounter more cases (what Uproot does).\r\n\r\nAs for it not being fast, the type lookup, code generation, and Cling compilation are all once-per-array operations, rather than once-per-element, and I generally only worry about the performance of the latter. (I.e. we want the scaling to have a good slope, but are less concerned with the offset: defining the goal to optimize infinitely large arrays.) Since RDF also does a one-time compilation step, this isn't very different from what you do. If I make [ak.forms.Form](https://awkward-array.readthedocs.io/en/latest/ak.forms.Form.html) hashable, to put in a global dict, it can be a once-per-type-of-array operation: repeatedly applying arrays of the same type could reuse previously generated and compiled interfaces.\r\n\r\nOh, and I should make the interface to collections be `RVec` instead of `std::vector`? I did see that `AsNumpy` makes `RVecs` of PyROOT objects when the columns are not numeric types. If `RVec` is the preferred user interface for collections in RDF, I'll do that. (I see now: `RVec` enhances `std::vector` with [np.take](https://numpy.org/doc/stable/reference/generated/numpy.take.html)- and [np.compress](https://numpy.org/doc/stable/reference/generated/numpy.compress.html)-like slicing.)",
  "created_at":"2021-03-17T14:33:35Z",
  "id":801133839,
  "issue":588,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMTEzMzgzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-17T14:33:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"If I may step in and generate a bit of noise, I'd support this on principle alone. I think many analyses are moving forward in frameworks being built on uproot+awkward or RDataFrame, and I agree having interoperability will be a big boon for analyses and analyzers. As I'm careening towards thesis writing and graduation, I'm not sure I have much bandwidth for meaningful contributions, but I'll be watching (and that may perhaps change by the time this gets attention from whoever will work on it). \r\n\r\nI can say that I wish this existed right now, I would definitely have uses for converting from my nearly-complete analysis based on RDF, to awkward, and back again. Interfacing with industry standard tools and already trained models for ML and round-tripping back to root output, without introducing even more bottle-necking intermediate stages, falling back to single-threaded execution, or rewriting things for the complete flattening I'd need using AsNumpy/MakeNumpyDataFrame, would be fantastic. \r\n\r\nWhich raises a question from me, Jim, do you envision this as a bulk operation like AsNumpy, where all columns/rows are held in memory at once, or something that can operate on chunks at a time? That may be something for much later, once a working implementation can take an entire dataset from one to the other and back.",
  "created_at":"2021-03-17T20:40:42Z",
  "id":801428596,
  "issue":588,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMTQyODU5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-17T20:40:42Z",
  "user":"MDQ6VXNlcjM4MjE3Mjc0"
 },
 {
  "author_association":"MEMBER",
  "body":"Good to know there's interest!\r\n\r\nIf it's implemented as a bulk operation, we can get the chunk-at-a-time functionality from that. The natural implementation for partitioned arrays ([ak.partitioned](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partitioned.html)) would be to step through the partitions, running the RDF bridge for each partition at a time. If the partitioned array is also virtual (i.e. it's a lazy array) and the RDF actions fill histograms or write to ROOT files (i.e. do not generate growing in-memory output), then such a process would never have more than one chunk of data in memory (for a suitably chosen lazy cache). That statement has a lot of qualifiers on it, but some examples of \"best practices\" could indicate how to do it within a memory budget.",
  "created_at":"2021-03-17T20:53:34Z",
  "id":801435733,
  "issue":588,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMTQzNTczMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-17T20:53:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thank you for all the work.\r\nDo you know why the `ak.num` call I mentioned in my second message triggers the loading and how to get around that?",
  "created_at":"2021-01-11T13:59:41Z",
  "id":757969304,
  "issue":603,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Nzk2OTMwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T13:59:41Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"The `ak.num` loads only the list offsets, not the particle attribute data, as it should:\r\n\r\n```python\r\n>>> # I loaded the Map, form, and map from your first message.\r\n>>> arr = ak.from_buffers(form, 3, map, lazy=True)\r\n\r\n>>> # Only loads the offsets, not the charge.\r\n>>> ak.num(arr.Muon)\r\nLoading part0-loads_quickly2-offsets\r\n<Array [1, 1, 1] type='3 * int64'>\r\n\r\n>>> # Do it again with concatenation.\r\n>>> con = ak.concatenate([arr[\"Electron\"], arr[\"Muon\"]], axis=1)\r\nLoading part0-loads_quickly1-offsets\r\nLoading part0-loads_quickly2-offsets\r\n>>> ak.num(con)\r\n<Array [2, 2, 2] type='3 * int64'>\r\n\r\n>>> # Just to be sure, printing it out loads the internal data that is shown in the print-out.\r\n>>> con\r\nLoading part0-loads_slowly1-data\r\nLoading part0-loads_slowly2-data\r\n<Array [[{charge: 5}, ... {charge: 5}]] type='3 * var * union[{\"charge\": int32},...'>\r\n```\r\n\r\nDetermining the number of items in each list (what `ak.num` does) requires the list offsets\u2014it has to be loaded in order to compute that quantity\u2014but the inner data is not needed and is not loaded.",
  "created_at":"2021-01-11T14:18:27Z",
  "id":757980176,
  "issue":603,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Nzk4MDE3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T14:18:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I agree with you, but what is weird is that the inner data is loaded by the call if you set a field, for example\r\n```python\r\ncon[\"var\"] = 5\r\n```\r\nbefore the `ak.num` call.",
  "created_at":"2021-01-11T14:26:08Z",
  "id":757984823,
  "issue":603,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Nzk4NDgyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T14:26:08Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"I looked into it, and that investigation snowballed. The reason setting a variable affects the later `ak.num` call is because part of broadcasting the `5` to all elements in `con` slices all the fields in the array by a trivial slice. Because of the delayed slicing that VirtualArrays now have, that operation doesn't cause a read. (You can see that difference by looking at `con.layout` before and after assigning the new field: the other fields go from VirtualArrays with a PythonGenerator to VirtualArrays with a SliceGenerator.)\r\n\r\nThen you'd think that calling `ak.num` with an `axis` above the RecordArray wouldn't touch the contents inside the RecordArray, but the function that determines how to convert a negative `axis` into a positive axis has to descend all the way to the leaves of the tree to find out how deep it is, and that materializes the fields of the RecordArray because the lazily sliced array has no Form. The lazily sliced array has no Form because the Form of anything but a field-slice (only slicing using strings) is hard to predict. The code the computes a slice is rather complex\u2014writing parallel code to predict whether the result of that slice will be a ListArray or a ListOffsetArray, for instance, would at least be hard to keep in sync with the code that actually does the computation (a maintenance problem), but it might also be impossible (it might depend on details of the values in the array, which are not available in the Form).\r\n\r\nIn this case, the `axis` of `ak.num` is non-negative, so we could just check for `axis >= 0` _before_ checking the depth of the array (6d0f7639ccfc71cf6f0b82763a2364f8b0dd760c). I should have left it at that.\r\n\r\nInstead, I looked into why the fields of the RecordArray were getting sliced. It was a trivial slice (i.e. `x[slice] == x`), and there are quite a few situations in which a slice might happen to be trivial, so I added logic that if a slice is trivial, it should not be applied. If we ever had any hope of predicting what `__getitem__` does through an array's Form, it's gone now because this makes that result depend on a value: whether the slice is trivial or not. (You have to look at all the values of the slice and whether it matches the array's length.) This can also be a performance improvement because not applying trivial slices avoids memory copies.\r\n\r\nOnce that was working (PR #643) and I had half-written this response, I started thinking that the calculation of array depth involved in `ak.num`'s handling of a (potentially) negative `axis` shouldn't ever cause a VirtualArray to materialize if it has any Form information. Perhaps we can't predict the exact Form of a sliced array, but that doesn't mean we don't know its depth. Therefore, I changed the depth calculations so that they derive from the arrays themselves, rather than creating a Form and checking that, with extra logic to compute the depth of a sliced VirtualArray from the unsliced VirtualArray (PR #644). Maybe we don't know whether it will be composed of ListArrays or ListOffsetArrays, but we do know how many such things will be nested.\r\n\r\nThe bottom line is that keeping VirtualArrays from being materialized is _inherently_ complex. Awkward Array is not a delayed evaluation formalism like Dask, which postpones every action until a `calculate()` method is called\u2014it is an eager evaluation formalism in which every action returns as much information as the action requires and materializes any virtual arrays it needs to in order to provide that information. We can postpone some of those materializations by adding detailed type information\u2014the Form\u2014to virtual arrays, but then this information has to be propagated through any downstream actions that we also want to keep virtual. In this scheme, the complexity builds the longer we want to postpone materialization. Dask, on the other hand, knows nothing about the type of each of its delayed actions (or even if they will work) until the whole thing gets calculated, but that requires a user-visible `calculate()` method to say when the information is finally needed. Awkward Array wasn't designed to be that way because it wasn't designed to be a delayed evaluation formalism: if you ask for `ak.num`, you have to get the number of items in each list as soon as `ak.num` completes because there's no `calculate()` method for you to specify when you want that. The way forward on this, if more delayed evaluation is needed, is to get Awkward Array to work in Dask. (https://github.com/scikit-hep/awkward-1.0/issues/284#issuecomment-731434583)",
  "created_at":"2021-01-11T22:32:06Z",
  "id":758266803,
  "issue":603,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1ODI2NjgwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T22:32:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - incidentally NumPy `quicksort` has been changed to `introsort`:\r\nhttps://numpy.org/doc/stable/reference/generated/numpy.sort.html\r\n\r\nIt's the same as in AwkwardArray :-)",
  "created_at":"2021-01-22T11:16:40Z",
  "id":765331872,
  "issue":615,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTMzMTg3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T11:16:40Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The Python prototype is done. The C++ prototype will be a new PR. (I'm merging this one to be sure that the `ak.from_buffers` correction gets into the main codebase.)",
  "created_at":"2021-01-04T18:09:27Z",
  "id":754129151,
  "issue":620,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDEyOTE1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T18:09:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Interesting.. I assume on the uproot side of this the form keys don't depend on the partition, right? If so, and if we find they are invariant for ~files with identical branches~ TTrees with identical metadata, then this satisfies the needs for establishing a base form for NanoEvents and then I don't have to worry about maintaining https://github.com/CoffeaTeam/coffea/blob/master/coffea/nanoevents/mapping/uproot.py#L34 for all types of root files (and can drop the `!skip` and `!content` DSL commands as well)",
  "created_at":"2021-01-04T21:02:53Z",
  "id":754217038,
  "issue":620,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDIxNzAzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T21:03:55Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I saw this in the release notes, but I'm having some trouble understanding the full impact on my analysis code. Does this also impact analyses which convert to/from parquet files too? And is it only for slices? (ie. does it also impact masking?). Thanks!",
  "created_at":"2021-01-04T18:50:13Z",
  "id":754149770,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDE0OTc3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T18:50:13Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's worth testing your analysis code on the newly released version. This is a bug that can give you the wrong values when converting an Arrow array to an Awkward Array, and the chain Parquet \u2192 Arrow \u2192 Awkward goes through Arrow.\r\n\r\nHowever, you only get the wrong values if the Arrow arrays have been sliced in `pyarrow`. That's because the zero-copy buffers that `pyarrow` gives us for, say\r\n\r\n```python\r\n>>> import pyarrow as pa\r\n>>> array = pa.array([0, 1, 2, 3, 4, 5])\r\n>>> sliced = array[2:]\r\n>>> sliced\r\n<pyarrow.lib.Int64Array object at 0x7fdb92cc6ca0>\r\n[\r\n  2,\r\n  3,\r\n  4,\r\n  5\r\n]\r\n```\r\n\r\ndo not explicitly exclude the sliced-off parts. That is,\r\n\r\n```python\r\n>>> _, raw_data = sliced.buffers()\r\n>>> np.frombuffer(raw_data, np.int64)\r\narray([0, 1, 2, 3, 4, 5])\r\n```\r\n\r\nstarts with `0` even though `sliced` is supposed to start with `2`. In the Arrow \u2192 Awkward conversion, we should have been looking at the\r\n\r\n```python\r\n>>> sliced.offset\r\n2\r\n```\r\n\r\nto find the right part of `raw_data` to consider as part of the Awkward Array. With the PR, that \"wrong answer bug\" has been fixed.\r\n\r\nNow that I've written this up in a scary way, I _doubt_ it affects conversions from Parquet through Arrow to Awkward Array because I _doubt_ that `pyarrow` arrays get sliced after the conversion from Parquet and before the conversion to Awkward. That's how this bug survived so long: nobody was doing any non-trivial work on the Arrow arrays, so the assumption that `offset == 0` always held. But that's my assumption of how the Parquet \u2192 Arrow conversion works, and I didn't write that code, so it remains only a guess.",
  "created_at":"2021-01-04T19:28:33Z",
  "id":754169390,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDE2OTM5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T19:28:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the explanation - now it's clear. I'll run a quick test to check and report back if something changed in my analysis with the new version, but I suspect that you're right that it shouldn't be an issue for me",
  "created_at":"2021-01-04T20:32:54Z",
  "id":754202290,
  "issue":624,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDIwMjI5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T20:32:54Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for finding this! What we ought to do is check to see if an object to be used as a slice has an `__index__` method and call it to get an integer ([PEP 357](https://www.python.org/dev/peps/pep-0357/)). This should happen near the ball-out point mentioned in the error message:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/1.0.1/src/python/content.cpp#L652\r\n\r\nIt's in C++ (the `toslice` function), where we'd use pybind11 methods to check for the existence of a `__index__` and only try to call it if it exists (otherwise, it would be a segfault).",
  "created_at":"2021-01-01T16:41:39Z",
  "id":753341918,
  "issue":628,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzM0MTkxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-01T16:41:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Correction: the `toslice_part` function.",
  "created_at":"2021-01-01T16:43:21Z",
  "id":753342128,
  "issue":628,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzM0MjEyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-01T16:43:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'd call this a bug report, rather than a feature request.\r\n\r\nI'll get to it as soon as I can, unless there's someone out there who's interested in a quick project and practical introduction to pybind11.",
  "created_at":"2021-01-01T16:46:35Z",
  "id":753342665,
  "issue":628,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzM0MjY2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-01T16:46:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ok, I was not sure whether this is intentional due to keep it the package light-weight or this really being a bug. I'm not really into the backend of `awkward` so I think I cannot help with resolving the issue, but do you want me to translate my example code into some unit-test?",
  "created_at":"2021-01-01T17:23:38Z",
  "id":753347536,
  "issue":628,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzM0NzUzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-01T17:23:38Z",
  "user":"MDQ6VXNlcjE3ODYyMDkw"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the offer, but it'll be easy enough to make a unit test from what you've already shown.\r\n\r\nThis sort of thing would take me about an hour, including the time for running the tests on Azure. I'm just mentioning it as a possible project in case somebody wants a practical introduction to pybind11, but if anybody does, they should speak up so that our implementations don't cross in the mail.\r\n\r\n(I'm writing these comments on my phone because I'm on break.)",
  "created_at":"2021-01-01T17:53:22Z",
  "id":753354617,
  "issue":628,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzM1NDYxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-01T17:53:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I should have done this and included it in 1.0.2rc4! Oh well, I'll do it now.",
  "created_at":"2021-01-04T14:34:33Z",
  "id":754009952,
  "issue":628,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDAwOTk1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T14:34:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Cool: there was already a \"FIXME\" comment on the offending line of code.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/cfac11b07e339ed3f39b27e85c682f4e5a716102/src/python/content.cpp#L411-L415",
  "created_at":"2021-01-04T14:40:46Z",
  "id":754013483,
  "issue":628,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDAxMzQ4Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-04T14:40:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This fixes it: 9b521ced4280bc59392e4e3e4c2922029f1fb97c\r\n\r\nAnything with a `__index__` method _that does not raise exceptions_ can be used in place of an integer in slices. This includes NumPy integer types like `np.int64(2)` and zero-dimensional NumPy arrays of integers like `np.array([2]).reshape(())`.",
  "created_at":"2021-01-04T15:13:31Z",
  "id":754032976,
  "issue":628,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDAzMjk3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T15:13:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Cool, thank you!",
  "created_at":"2021-01-04T15:17:03Z",
  "id":754034953,
  "issue":628,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDAzNDk1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-04T15:17:03Z",
  "user":"MDQ6VXNlcjE3ODYyMDkw"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Better link: https://github.com/scikit-hep/awkward-1.0/pull/606/files#diff-63b9cf483f8a04b770807d6cabd4098708d35ed6d124e8d1c969fcca657d9b32L845-R865",
  "created_at":"2021-01-01T23:27:29Z",
  "id":753403583,
  "issue":629,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzQwMzU4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-01T23:27:29Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, `getitem_fields` _should_ drop parameters:\r\n\r\n```python\r\narray_of_records[[\"just\", \"these\", \"fields\"]]\r\n```\r\n\r\nturns a record array into a record array of fewer fields, so it should no longer allow the same methods (or have the same identity as an `asdf` type of object), which might require a full set of fields.\r\n\r\nBut you've demonstrated that `ak.with_field` is losing parameters and it should not. Having _more_ fields (or the same number of fields, with one of its values replaced) is not a problem for having the same set of methods (or the same type-identity). The `ak.with_field` function is calling a `Content::setitem_field` method:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ea2d88c8df7fe2cfd3a7f8b157846e7c18d0aa1c/src/awkward/operations/structure.py#L661-L669\r\n\r\nBut... the `RecordArray::setitem_field` method _does_ return a RecordArray with the same `parameters_`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ea2d88c8df7fe2cfd3a7f8b157846e7c18d0aa1c/src/libawkward/array/RecordArray.cpp#L429-L472\r\n\r\nThat's weird. I'll have to go into deep debugging mode to figure out where it's being lost. (Not here: https://github.com/scikit-hep/awkward-1.0/blob/ea2d88c8df7fe2cfd3a7f8b157846e7c18d0aa1c/src/awkward/highlevel.py#L1058-L1069 .) There shouldn't be a point where an array is sliced with its full set of fields, which would do nothing except drop parameters, but that's the sort of thing I'd be looking for.",
  "created_at":"2021-01-01T23:45:17Z",
  "id":753405205,
  "issue":629,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzQwNTIwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-01T23:45:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ok, I `git bisect`ed to find that PR was the culprit, but only guessed that line change to be the relevant piece of the PR.",
  "created_at":"2021-01-01T23:50:06Z",
  "id":753405647,
  "issue":629,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzQwNTY0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-01T23:50:06Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I can see how this commit did it (the one you found with a git bisect). The commit was correct: `array[array.fields]` should drop parameters, but I used that in one line of `ak.with_fields` to avoid doubling up a key (i.e. getting two keys named `\"z\"`).\r\n\r\nPR #629 removes that line, and to do so, it builds the output RecordArray manually in Python. The C++ `RecordArray::setitem_field` now appears to be completely unnecessary. I should consider removing it. Another nice feature of building the RecordArray manually is that it's only built once: the `array[array.fields]` built it one time and `RecordArray::setitem_field` built it again, which could be a performance issue for records with many fields (at best a factor of 2).",
  "created_at":"2021-01-02T19:17:38Z",
  "id":753518038,
  "issue":629,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzUxODAzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-02T19:17:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a semi-duplicate of #595, where I was thinking that we'd enumerate them all in a submodule.\r\n\r\nHowever, your idea of doing it outside of the decorator as a default is a different (and probably better) idea. Here's the thing, though: it should automatically call `__array__` on all the arguments that are supposed to be arrays and not on the ones that aren't. Maybe I can inspect the NumPy docstrings?!?",
  "created_at":"2021-01-02T03:04:39Z",
  "id":753422740,
  "issue":630,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzQyMjc0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-02T03:04:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Also worth noting: half of C++'s time is spent dereferencing the `std::vectors` of `std::shared_ptrs` with a virtual method call into the `ForthOutputBuffer` (whose specialized type would not be known at compile time). There isn't any one of these that is much more significant than the others. They can't be turned off because it will _always_ be the case that the number of types of input/outputs will be unknown at compile-time, but considering that the input/output management takes half of the time in a metric where Forth is 3\u00d7 worse, a heavy calculation not dominated by input/output might be 5\u00d7 worse.",
  "created_at":"2021-01-06T19:30:46Z",
  "id":755564271,
  "issue":638,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NTU2NDI3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-06T19:30:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This looks great\u2014removing the recursion in quicksort by adding a stack. But does a finite `kMaxLevels` mean that there are arrays that would be too big to sort? That would be an unwelcome surprise. Is it possible to compute an upper limit maximum number of levels given the length of a list? I would guess that such an upper limit would be logarithmic in that length, and therefore not too large. If it takes _O(n log(n))_ space to sort _n_ elements, I think that would be acceptable...",
  "created_at":"2021-01-05T18:57:53Z",
  "id":754832650,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDgzMjY1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-05T18:57:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This looks great\u2014removing the recursion in quicksort by adding a stack. But does a finite `kMaxLevels` mean that there are arrays that would be too big to sort? That would be an unwelcome surprise. Is it possible to compute an upper limit maximum number of levels given the length of a list? I would guess that such an upper limit would be logarithmic in that length, and therefore not too large. If it takes _O(n log(n))_ space to sort _n_ elements, I think that would be acceptable...\r\n\r\nI think, the algorithm ensures that an array of N entries can handle a set of 2<sup>N</sup> entries: the larger range is pushed into the array and the loop iterates on the smaller range. It still has quadratic time complexity on sorted arrays, but at least it would sort arrays of all possible sizes. ",
  "created_at":"2021-01-06T11:54:54Z",
  "id":755257412,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NTI1NzQxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-06T11:54:54Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> It still has quadratic time complexity on sorted arrays, but at least it would sort arrays of all possible sizes.\r\n\r\nI just read up on the algorithm. If the worst case of _O(n^2)_ is rare, as Wikipedia claims, then that's okay. If this case happens when the arrays are already sorted, then that's not good because running a sorting algorithm \"just to be sure it's sorted\" is a common thing to do. But if the already-sorted case is that _O(n^2)_ worst case, then it's easily fixed: the algorithm could start with a quick _O(n)_ check to determine whether it's sorted, then follow with quicksort if it's not. Most unsorted lists wouldn't even take a full _O(n)_ to see that they're not sorted.\r\n\r\nAnd just to be sure: the actual iteration over data in the arrays is done in cpu-kernels, not libawkward, right?",
  "created_at":"2021-01-06T14:13:37Z",
  "id":755321724,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NTMyMTcyNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-06T14:13:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, the PR is more or less done. Both `sort` and `argsort` support `ascending` and `descending` order. `Stable` is not supported. I wonder if a stable `argsort` is something to work on next. Perhaps, I'll spend some time on it over the week-end.\r\n\r\nNext, I'd like to run some profiling tests and to compare this implementation to an old std-based version. Also, I'm trying to get (an easy) access to a GPU to test the kernels. While there is data dependency between iterations within the `quick_sort` and `quick_argsort` functions, sorting the ranges are independent loop iterations.",
  "created_at":"2021-01-08T16:11:03Z",
  "id":756843077,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Njg0MzA3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-08T16:11:03Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I just have some questions: is quicksort what you get if you call `sort` and `argsort` with `stable=False` now?\r\n> \r\n`Introsort` begins with `quicksort` and if the recursion depth goes more than a particular limit it switches to `Heapsort` to avoid Quicksort\u2019s worse case `O(N2)` time complexity. It also uses insertion sort when the number of elements to sort is quite less.\r\n\r\nSo first it creates a partition. Three cases arises from here.\r\n\r\n* If the partition size is such that there is a possibility to exceed the maximum depth limit then the `Introsort` switches to `Heapsort`. The maximum depth limit is defined as `2*log(N)`.\r\n* If the partition size is too small then `Quicksort` decays to `Insertion Sort`. This cutoff is defined as 16 (due to research). So if the partition size is less than 16 then it will do insertion sort.\r\n* If the partition size is under the limit and not too small (i.e- between 16 and 2*log(N)), then it performs a simple `quicksort`.\r\n\r\n> What is the role of `kMaxLevels`? Is there a maximum size array or maximum size list in an array that can be sorted? What happens if one attempts to sort an arbitrarily large array? The CPU version of this function, at least, must work for any dataset that can fit in memory (tens of GB, with the understanding that some working space will be required, and the time will scale as _O(n log n)_).\r\n\r\nThe algorithm pushes the larger range into the array and the loop ingrates on a smaller range: `kMaxLevels` defines when to stop. BTW, `kMaxLevels` set to 48 allows sorting an array of about 280TB",
  "created_at":"2021-01-09T08:11:31Z",
  "id":757114467,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzExNDQ2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T08:30:32Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> BTW, `kMaxLevels` set to 48 allows sorting an array of about 280TB\r\n\r\nI see! Then it is logarithmic. I agree that 280 TB is technically a limit but practically not a limit, especially since these arrays have to be in RAM. We also use integers without worrying about overflow. (And UnionArrays are limited to 128 distinct tags.) I have no qualms about accepting that kind of artificial limitation.\r\n\r\nThe rules that you specified for switching between different sorting algorithms, are those what your implementation does or what the C++ standard library does?",
  "created_at":"2021-01-09T17:22:48Z",
  "id":757338501,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzMzODUwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T17:22:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > BTW, `kMaxLevels` set to 48 allows sorting an array of about 280TB\r\n> \r\n> I see! Then it is logarithmic. I agree that 280 TB is technically a limit but practically not a limit, especially since these arrays have to be in RAM. We also use integers without worrying about overflow. (And UnionArrays are limited to 128 distinct tags.) I have no qualms about accepting that kind of artificial limitation.\r\n> \r\n> The rules that you specified for switching between different sorting algorithms, are those what your implementation does or what the C++ standard library does?\r\n\r\nThat\u2019s what std does  \ud83d\ude00",
  "created_at":"2021-01-09T22:12:54Z",
  "id":757375034,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzM3NTAzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T22:12:54Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I can merge it now if you think it's ready. It looks ready to me. I would then follow up with making quicksort the default from the high-level interface, and I think you wanted to do a bit more work as another PR, right?",
  "created_at":"2021-01-09T22:19:39Z",
  "id":757375966,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzM3NTk2Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-09T22:19:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I can merge it now if you think it's ready. It looks ready to me. I would then follow up with making quicksort the default from the high-level interface, and I think you wanted to do a bit more work as another PR, right?\r\n\r\nOne question I have is about sorting an array copy rather than an array itself. This can be addressed later, I think.",
  "created_at":"2021-01-09T22:33:18Z",
  "id":757377426,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzM3NzQyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T22:33:18Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry, I missed your response earlier; I hope it's not too late. Do you mean that the `Content::sort` operation sorts the array in-place? None of the Awkward operations are supposed to be in-place: they should all leave the input unchanged.",
  "created_at":"2021-01-09T22:58:15Z",
  "id":757380162,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzM4MDE2Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-09T22:58:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I'm trying to convince myself that this new implementation should be used instead of an std-based one. Because, usually, I'd go with the std whenever possible...\r\nI started with a best case scenario sorting a random one dimensional array. Here is what I get.\r\n\r\nThis PR `sort` implementation:\r\n```python\r\nIn [5]: content = ak.layout.NumpyArray(np.random.rand(100000))                                                                                                                                                                                              \r\n\r\nIn [6]: content                                                                                                                                                                                                                                             \r\nOut[6]: <NumpyArray format=\"d\" shape=\"100000\" data=\"0.289661 0.559196 0.306584 0.27053 0.030667 ... 0.595118 0.326157 0.993009 0.695484 0.356429\" at=\"0x7f8301160000\"/>\r\n\r\nIn [7]: %%timeit \r\n   ...: rand_sorted = content.sort(0, True, True) \r\n   ...:  \r\n   ...:                                                                                                                                                                                                                                                     \r\n18.8 ms \u00b1 157 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\nvs previously implemented using an `std::sort`:\r\n```python\r\nIn [3]: content = ak.layout.NumpyArray(np.random.rand(100000))                                                                                                                                                                                              \r\n\r\nIn [4]: %%timeit \r\n   ...: rand_sorted = content.sort(0, True, True) \r\n   ...:  \r\n   ...:                                                                                                                                                                                                                                                     \r\n22.7 ms \u00b1 231 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```\r\nvs Numpy `sort` implementation (is this the price we pay for not sorting in place?):\r\n```python\r\nIn [8]: nparr = np.random.rand(100000)                                                                                                                                                                                                                      \r\n\r\nIn [9]: %%timeit \r\n    ...: nrand_sorted = np.sort(nparr) \r\n    ...:  \r\n    ...:                                                                                                                                                                                                                                                    \r\n6.13 ms \u00b1 45.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nThis PR sort on a larger array:\r\n```python\r\nIn [12]: content2 = ak.layout.NumpyArray(np.random.rand(1000000))                                                                                                                                                                                           \r\n\r\nIn [13]: %%timeit \r\n    ...: rand_sorted2 = content2.sort(0, True, True) \r\n    ...:  \r\n    ...:                                                                                                                                                                                                                                                    \r\n221 ms \u00b1 3.02 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\nvs using an std::sort-based implementation:\r\n```python\r\nIn [5]: content2 = ak.layout.NumpyArray(np.random.rand(1000000))                                                                                                                                                                                            \r\n\r\nIn [6]: %%timeit \r\n   ...: rand_sorted2 = content2.sort(0, True, True) \r\n   ...:  \r\n   ...:                                                                                                                                                                                                                                                     \r\n326 ms \u00b1 20.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n``` \r\nfinally this PR:\r\n```python\r\nIn [14]: content3 = ak.layout.NumpyArray(np.random.rand(10000000))                                                                                                                                                                                          \r\n\r\nIn [15]: %%timeit \r\n    ...: rand_sorted3 = content3.sort(0, True, True) \r\n    ...:  \r\n    ...:                                                                                                                                                                                                                                                    \r\n2.53 s \u00b1 15.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\nvs previos std-based:\r\n```python\r\nIn [7]: content3 = ak.layout.NumpyArray(np.random.rand(10000000))                                                                                                                                                                                           \r\n\r\nIn [8]: %%timeit \r\n   ...: rand_sorted3 = content3.sort(0, True, True) \r\n   ...:  \r\n   ...:                                                                                                                                                                                                                                                     \r\n5.52 s \u00b1 103 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```",
  "created_at":"2021-01-10T14:22:15Z",
  "id":757484279,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzQ4NDI3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-10T14:22:55Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Before I merge it, I need to know that it doesn't change the inputs (that it's an immutable operation) and that you're happy with it. I think it would be a little difficult to remove a commit, so let's be sure before putting it in.\r\n\r\nOn the performance tests: true ones in which the standard deviation is ten times larger than the mean are inconclusive. They'll need larger samples or repeated runs off the same samples so that caches get filled and the variances settle down. That said, I can't imagine that our own implementation would beat one from the standard library in the common single-big-list case.\r\n\r\nThe motivation for implementing our own sorting library was to control it, to ensure that what happens on the GPU is the same. However, for sorting, the specification is clear: it has to put the elements in the right order. It's understood that we'll use different algorithms on the CPU and GPU to get the same outcome. The only user-visible difference could be in how some-value elements are treated in two unstable `argsorts`, but we can argue that the correct order of these are undefined, if we do have such a difference.\r\n\r\nOne place where performance might favor your hand-written quicksort is in sorting of many small lists. That's a much more likely physics use-case than the single-large-list that sorting is usually optimized for. If you make a large ListOffsetArray full of many small lists (3, 5, 10, 20 elements), you _might_ find that your hand-written quicksort beats the standard library one because this tests a different aspect of the algorithm: how quickly it can set up and tear down as it moves from one list to the next. Surely, the standard library algorithms weren't designed for that, but that's what physicists need (sorting sets of particles in each independent event).\r\n\r\nFinally, though, we don't necessarily want to get rid of the standard library version in favor of the hand-written quicksort: we want to provide the same options as NumPy: [np.sort](https://numpy.org/doc/stable/reference/generated/numpy.sort.html). There, a pure quicksort is the default, but they have two other algorithms (\"stable\" is a synonym) and one of them might correspond with the standard library algorithm. It's that true? (We talked about this, but I don't remember the outcome.) In that case, we'd want both, and I'd introduce the `kind` argument to the high-level function to choose between them.",
  "created_at":"2021-01-10T15:16:32Z",
  "id":757492823,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzQ5MjgyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-10T15:16:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Before I merge it, I need to know that it doesn't change the inputs (that it's an immutable operation) and that you're happy with it. I think it would be a little difficult to remove a commit, so let's be sure before putting it in.\r\n> \r\nIt does not change the input. I have another commit to push removing the comment and unifying `awkward_sort` and `awkward_quick_sort`. They both use the same algorithm, no need for code duplication. Or would you rather keep an std-based `awkward_sort` and have an `awkward_quick_sort` as an option?.\r\n\r\n> On the performance tests: true ones in which the standard deviation is ten times larger than the mean are inconclusive. They'll need larger samples or repeated runs off the same samples so that caches get filled and the variances settle down. That said, I can't imagine that our own implementation would beat one from the standard library in the common single-big-list case.\r\n> \r\nagree.\r\n\r\n> The motivation for implementing our own sorting library was to control it, to ensure that what happens on the GPU is the same. However, for sorting, the specification is clear: it has to put the elements in the right order. It's understood that we'll use different algorithms on the CPU and GPU to get the same outcome. The only user-visible difference could be in how some-value elements are treated in two unstable `argsorts`, but we can argue that the correct order of these are undefined, if we do have such a difference.\r\n> \r\nyes, unfortunately, there is a difference.\r\n\r\n> One place where performance might favor your hand-written quicksort is in sorting of many small lists. That's a much more likely physics use-case than the single-large-list that sorting is usually optimized for. If you make a large ListOffsetArray full of many small lists (3, 5, 10, 20 elements), you _might_ find that your hand-written quicksort beats the standard library one because this tests a different aspect of the algorithm: how quickly it can set up and tear down as it moves from one list to the next. Surely, the standard library algorithms weren't designed for that, but that's what physicists need (sorting sets of particles in each independent event).\r\n> \r\nyes. Also, this is where we can parallelize, I think \r\n> Finally, though, we don't necessarily want to get rid of the standard library version in favor of the hand-written quicksort: we want to provide the same options as NumPy: [np.sort](https://numpy.org/doc/stable/reference/generated/numpy.sort.html). There, a pure quicksort is the default, but they have two other algorithms (\"stable\" is a synonym) and one of them might correspond with the standard library algorithm. It's that true? (We talked about this, but I don't remember the outcome.) In that case, we'd want both, and I'd introduce the `kind` argument to the high-level function to choose between them.\r\n\r\nOk, I think, it's best to have both for the moment. I'll update the PR.\r\n\r\n",
  "created_at":"2021-01-11T09:38:08Z",
  "id":757806577,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzgwNjU3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T09:38:08Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I'm done with this PR. Could you, please, review it? Thanks.",
  "created_at":"2021-01-12T10:11:42Z",
  "id":758552006,
  "issue":639,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1ODU1MjAwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-12T10:11:42Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I manually moved this to a Discussion: https://github.com/scikit-hep/awkward-1.0/discussions/641\r\n\r\nbecause the error is a `ArrowNotImplementedError`, not something I can fix. (You can try the [Arrow JIRA](https://issues.apache.org/jira/projects/ARROW/issues/ARROW-10904?filter=allopenissues).) The discussion would be about ways of working around this (current) Arrow \u2192 Parquet limitation.",
  "created_at":"2021-01-05T18:01:59Z",
  "id":754800818,
  "issue":640,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NDgwMDgxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-05T18:01:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Was this simply missing? If so, thanks for noticing this!\r\n\r\nWhereas `ak.to_buffers` might have an experts-only `persist_virtual` option, `ak.to_arrow` can't (no virtual Arrow).\r\n\r\nAnd, of course, let me know if you're done with this branch and I'll merge it after tests pass.",
  "created_at":"2021-01-06T18:11:45Z",
  "id":755467333,
  "issue":642,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NTQ2NzMzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-06T18:11:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'm done",
  "created_at":"2021-01-06T18:30:40Z",
  "id":755483790,
  "issue":642,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NTQ4Mzc5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-06T18:30:40Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I verified that we still have the 5 ns per instruction.\r\n\r\nBy contrast, Python's virtual machine on compiled bytecodes (similar in _kind_ to what we do) takes ~900 ns per instruction.\r\n\r\n```python\r\n>>> import dis\r\n>>> def f():\r\n...   x = 0\r\n...   for i in range(100000):\r\n...     x = 10 + x + 10 + x + 10 + x + 10 + x + 10 + x + 10\r\n... \r\n>>> # There are 25 Python bytecode instructions in this loop, which gets repeated 100000 times.\r\n>>> dis.dis(f)\r\n  2           0 LOAD_CONST               1 (0)\r\n              2 STORE_FAST               0 (x)\r\n\r\n  3           4 LOAD_GLOBAL              0 (range)\r\n              6 LOAD_CONST               2 (100000)\r\n              8 CALL_FUNCTION            1\r\n             10 GET_ITER\r\n        >>   12 FOR_ITER                48 (to 62)\r\n             14 STORE_FAST               1 (i)\r\n\r\n  4          16 LOAD_CONST               3 (10)\r\n             18 LOAD_FAST                0 (x)\r\n             20 BINARY_ADD\r\n             22 LOAD_CONST               3 (10)\r\n             24 BINARY_ADD\r\n             26 LOAD_FAST                0 (x)\r\n             28 BINARY_ADD\r\n             30 LOAD_CONST               3 (10)\r\n             32 BINARY_ADD\r\n             34 LOAD_FAST                0 (x)\r\n             36 BINARY_ADD\r\n             38 LOAD_CONST               3 (10)\r\n             40 BINARY_ADD\r\n             42 LOAD_FAST                0 (x)\r\n             44 BINARY_ADD\r\n             46 LOAD_CONST               3 (10)\r\n             48 BINARY_ADD\r\n             50 LOAD_FAST                0 (x)\r\n             52 BINARY_ADD\r\n             54 LOAD_CONST               3 (10)\r\n             56 BINARY_ADD\r\n             58 STORE_FAST               0 (x)\r\n             60 JUMP_ABSOLUTE           12\r\n        >>   62 LOAD_CONST               0 (None)\r\n             64 RETURN_VALUE\r\n\r\n>>> begintime = time.time(); f(); print((time.time() - begintime) * 1e9)\r\n2324416875.8392334\r\n>>> 2324416875.8392334 / (25*100000)\r\n929.7667503356934\r\n\r\n>>> begintime = time.time(); f(); print((time.time() - begintime) * 1e9)\r\n2277071237.564087\r\n>>> 2277071237.564087 / (25*100000)\r\n910.8284950256348\r\n\r\n>>> begintime = time.time(); f(); print((time.time() - begintime) * 1e9)\r\n2262091398.2391357\r\n>>> 2262091398.2391357 / (25*100000)\r\n904.8365592956543\r\n```",
  "created_at":"2021-01-16T02:23:28Z",
  "id":761296216,
  "issue":648,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MTI5NjIxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-16T02:23:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is correct. It means that although the data are logically presented as an array of records:\r\n\r\n```\r\n[{\"x\": 1, \"y\": 1.1}, {\"x\": 2, \"y\": 2.2}, {\"x\": 3, \"y\": 3.3}]\r\n```\r\n\r\nthey're internally stored as a struct of arrays:\r\n\r\n```\r\nx: [1, 2, 3]\r\ny: [1.1, 2.2, 3.3]\r\n```\r\n\r\nThis is a general principle for all data structures in Awkward Array (as well as Apache Arrow and Parquet, other \"columnar\" data representations). It has a name, \"AoS vs SoA,\" in the case of records, but this technique can be generalized to variable-length lists, missing data, heterogenous data, etc.\r\n\r\nTo see the columnar form of any Awkward Array in Python, look at its `layout`:\r\n\r\n```python\r\n>>> array.layout\r\n```\r\n\r\nA representation of its in-memory layout is presented in an XML format (intended for readability).",
  "created_at":"2021-01-13T14:22:45Z",
  "id":759480668,
  "issue":649,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTQ4MDY2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T14:22:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I really want to convert this into a Discussion, but it looks like GitHub removed that button! Oh well.",
  "created_at":"2021-01-13T14:24:32Z",
  "id":759481841,
  "issue":649,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTQ4MTg0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T14:24:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the clarification. At least to me it was unclear that the first part in the sentence refers to the semantics and the second to the in-memory data structure.",
  "created_at":"2021-01-13T15:26:06Z",
  "id":759521321,
  "issue":649,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTUyMTMyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T15:26:06Z",
  "user":"MDQ6VXNlcjU0ODg0NDA="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, that helps. I'll do this:\r\n\r\n```diff\r\n--- a/docs-sphinx/index.rst\r\n+++ b/docs-sphinx/index.rst\r\n@@ -157,7 +157,7 @@ Navigation\r\n    * :doc:`ak.layout.RegularArray`: splits its nested content into equal-length lists.\r\n    * :doc:`ak.layout.ListArray`: splits its nested content into variable-length lists with full generality (may use its content non-contiguously, overlapping, or out-of-order).\r\n    * :doc:`ak.layout.ListOffsetArray`: splits its nested content into variable-length lists, assuming contiguous, non-overlapping, in-order content.\r\n-   * :doc:`ak.layout.RecordArray`: represents an array of records in \"struct of arrays\" form.\r\n+   * :doc:`ak.layout.RecordArray`: represents a logical array of records with a \"struct of arrays\" layout in memory.\r\n    * :doc:`ak.layout.Record`: represents a single record (not a subclass of :doc:`ak.layout.Content` in Python).\r\n    * :doc:`ak.layout.IndexedArray`: rearranges and/or duplicates its content by lazily applying an integer index.\r\n    * :doc:`ak.layout.IndexedOptionArray`: same as :doc:`ak.layout.IndexedArray` with missing values as negative indexes.\r\n```\r\n\r\nI only found this text on the landing page for the Sphinx Python API, though, not the Doxygen C++ API.",
  "created_at":"2021-01-13T16:10:51Z",
  "id":759553486,
  "issue":649,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTU1MzQ4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T16:10:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #650. If you have another suggestion for changing the wording (before the tests pass), I'll edit that PR.\r\n\r\nThanks!",
  "created_at":"2021-01-13T16:13:47Z",
  "id":759555305,
  "issue":649,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTU1NTMwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T16:13:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks, that is much clearer!\r\n\r\nI saw it it in the bullet point list here in the doxygen c++ docs: https://awkward-array.readthedocs.io/en/latest/_static/index.html",
  "created_at":"2021-01-13T16:23:06Z",
  "id":759561122,
  "issue":649,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTU2MTEyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T16:23:06Z",
  "user":"MDQ6VXNlcjU0ODg0NDA="
 },
 {
  "author_association":"NONE",
  "body":"Generated from this file here I think: https://github.com/scikit-hep/awkward-1.0/blob/main/docs-doxygen/index.md,\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/pull/651",
  "created_at":"2021-01-13T16:23:53Z",
  "id":759561616,
  "issue":649,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTU2MTYxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T16:25:40Z",
  "user":"MDQ6VXNlcjU0ODg0NDA="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks: my `grep` wasn't wide-ranging enough. I've fixed this instance, too (ddbcd74d66d66084f399b6983137a6aa05b4b844).",
  "created_at":"2021-01-13T16:28:29Z",
  "id":759564525,
  "issue":649,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTU2NDUyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T16:28:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh: our changes crossed in the mail. I added a commit to PR #650 for this file (ddbcd74d66d66084f399b6983137a6aa05b4b844). I'll just use the one PR so that both files are fixed with the same commit in `main`. Thanks, though!",
  "created_at":"2021-01-13T16:30:56Z",
  "id":759566153,
  "issue":651,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTU2NjE1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T16:30:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've already talked with @sjperkins (in GitHub or email? I can't remember), and he's not actively working on this: you won't collide. However, it's good to ping him here to be sure that the work isn't being duplicated.",
  "created_at":"2021-01-13T18:54:46Z",
  "id":759649573,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTY0OTU3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-13T18:54:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski @ianna Yep, I'm not working on it at present. Note that the snippet in https://github.com/scikit-hep/awkward-1.0/issues/392#issuecomment-752348789 may be useful for stamping out C/C++ templates.",
  "created_at":"2021-01-13T20:46:26Z",
  "id":759729190,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1OTcyOTE5MA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-13T20:46:26Z",
  "user":"MDQ6VXNlcjM1MzAyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Note to myself: it seems to me, the minimum amount that is necessary and sufficient to recreate a complex object is the real and imaginary part, plus the metadata:\r\n```json\r\n{\r\n    \"__complex__\": true,\r\n    \"real\": 42,\r\n    \"imag\": 36\r\n}\r\n```\r\nHere is an example in Python:\r\n```python\r\n>>> import json\r\n>>> z = 3 + 8j\r\n>>> type(z)\r\n<class 'complex'>\r\n>>> json.dumps(z)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/yana/anaconda3/lib/python3.8/json/__init__.py\", line 231, in dumps\r\n    return _default_encoder.encode(obj)\r\n  File \"/Users/yana/anaconda3/lib/python3.8/json/encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/Users/yana/anaconda3/lib/python3.8/json/encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/Users/yana/anaconda3/lib/python3.8/json/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type complex is not JSON serializable\r\n```\r\n```python\r\n>>> def encode_complex(z):\r\n...     if isinstance(z, complex):\r\n...         return (z.real, z.imag)\r\n...     else:\r\n...         type_name = z.__class__.__name__\r\n...         raise TypeError(f\"Object of type '{type_name}' is not JSON serializable\")\r\n... \r\n>>> json.dumps(9 + 5j, default=encode_complex)\r\n'[9.0, 5.0]'\r\n>>> json.dumps(z, default=encode_complex)\r\n'[3.0, 8.0]'\r\n>>> s = \"\"\"\r\n... {\r\n...     \"__complex__\": true,\r\n...     \"real\": 42,\r\n...     \"imag\": 36\r\n... }\r\n... \"\"\"\r\n>>> def decode_complex(dct):\r\n...     if \"__complex__\" in dct:\r\n...         return complex(dct[\"real\"], dct[\"imag\"])\r\n...     return dct\r\n... \r\n>>> x = json.loads(s, object_hook=decode_complex)\r\n>>> type(x)\r\n<class 'complex'>\r\n>>> x\r\n(42+36j)\r\n>>> \r\n```",
  "created_at":"2021-01-14T16:50:08Z",
  "id":760321270,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MDMyMTI3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-14T16:50:08Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Note to myself: it seems to me, the minimum amount that is necessary and sufficient to recreate a complex object is the real and imaginary part, plus the metadata:\r\n> \r\n> ```json\r\n> {\r\n>     \"__complex__\": true,\r\n>     \"real\": 42,\r\n>     \"imag\": 36\r\n> }\r\n> ```\r\n\r\nThe reason I think complex numbers will be more straightforward than datatimes is that no new metadata will be needed for them. The NumpyArray class already passes data around as a `std::shared_ptr<void>`, `util::dtype` (enum), `format` (string), and `itemsize`. Arrays of complex numbers fit into this scheme just as easily as any other numbers. NumPy sets a convention that the real and imaginary parts are interleaved in the same buffer with the real part first.\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> array = np.array([1, 1j, 1 + 1j], np.complex64)\r\n>>> array\r\narray([1.+0.j, 0.+1.j, 1.+1.j], dtype=complex64)\r\n>>> array.view(np.uint8)\r\narray([  0,   0, 128,  63,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n         0, 128,  63,   0,   0, 128,  63,   0,   0, 128,  63], dtype=uint8)\r\n>>> array.view(np.float32)\r\narray([1., 0., 0., 1., 1., 1.], dtype=float32)\r\n```\r\n\r\nThe two dtypes that are relevant here are `complex64` (two `float32` numbers) and `complex128` (two `float64` numbers) because of the built-in `float` and `double` in C++. Any others can be indefinitely postponed (like `float16` and `float128`, which only have stubs).\r\n\r\nWhen such arrays come into Awkward Array, they go through Python's buffer interface, which assigns a `format` string. The string for `complex64` is `Zf` and the string for `complex128` is `Zd`.\r\n\r\n```python\r\n>>> memoryview(np.array([1, 1j, 1 + 1j], np.complex64)).format\r\n'Zf'\r\n>>> memoryview(np.array([1, 1j, 1 + 1j], np.complex128)).format\r\n'Zd'\r\n```\r\n\r\nSo, just in case you thought you would need to add attributes to classes or something, the project is simpler than that. All the places where complex numbers could be relevant are already in the code (commented out, I believe). They can be searched with a multifile grep.",
  "created_at":"2021-01-14T17:10:24Z",
  "id":760334035,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MDMzNDAzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-14T17:10:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > Note to myself: it seems to me, the minimum amount that is necessary and sufficient to recreate a complex object is the real and imaginary part, plus the metadata:\r\n> > ```json\r\n> > {\r\n> >     \"__complex__\": true,\r\n> >     \"real\": 42,\r\n> >     \"imag\": 36\r\n> > }\r\n> > ```\r\n> \r\n> The reason I think complex numbers will be more straightforward than datatimes is that no new metadata will be needed for them. The NumpyArray class already passes data around as a `std::shared_ptr<void>`, `util::dtype` (enum), `format` (string), and `itemsize`. Arrays of complex numbers fit into this scheme just as easily as any other numbers. NumPy sets a convention that the real and imaginary parts are interleaved in the same buffer with the real part first.\r\n> \r\n> ```python\r\n> >>> import numpy as np\r\n> >>> array = np.array([1, 1j, 1 + 1j], np.complex64)\r\n> >>> array\r\n> array([1.+0.j, 0.+1.j, 1.+1.j], dtype=complex64)\r\n> >>> array.view(np.uint8)\r\n> array([  0,   0, 128,  63,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n>          0, 128,  63,   0,   0, 128,  63,   0,   0, 128,  63], dtype=uint8)\r\n> >>> array.view(np.float32)\r\n> array([1., 0., 0., 1., 1., 1.], dtype=float32)\r\n> ```\r\n> \r\n> The two dtypes that are relevant here are `complex64` (two `float32` numbers) and `complex128` (two `float64` numbers) because of the built-in `float` and `double` in C++. Any others can be indefinitely postponed (like `float16` and `float128`, which only have stubs).\r\n> \r\n> When such arrays come into Awkward Array, they go through Python's buffer interface, which assigns a `format` string. The string for `complex64` is `Zf` and the string for `complex128` is `Zd`.\r\n> \r\n> ```python\r\n> >>> memoryview(np.array([1, 1j, 1 + 1j], np.complex64)).format\r\n> 'Zf'\r\n> >>> memoryview(np.array([1, 1j, 1 + 1j], np.complex128)).format\r\n> 'Zd'\r\n> ```\r\n> \r\n> So, just in case you thought you would need to add attributes to classes or something, the project is simpler than that. All the places where complex numbers could be relevant are already in the code (commented out, I believe). They can be searched with a multifile grep.\r\n\r\nThanks, @jpivarski ! I've been thinking the metadata is needed to represent complex type in `to_` and `from_json` because the `json` module expects all custom types to be expressed as objects in the JSON standard.",
  "created_at":"2021-01-15T07:50:07Z",
  "id":760716666,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MDcxNjY2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-15T07:50:07Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Thanks, @jpivarski ! I've been thinking the metadata is needed to represent complex type in `to_` and `from_json` because the `json` module expects all custom types to be expressed as objects in the JSON standard.\r\n\r\nOh, that's right: the JSON methods would need to handle this type in a special way. Unfortunately, there's no standard way to encode complex numbers in JSON. If you were to make:\r\n\r\n```python\r\n{\"real\": 1, \"imag\": 0.5}\r\n```\r\n\r\nmean `1+0.5j`, then there could be cases where someone actually wanted a record with these two fields, and they would not want complex numbers. Even embedding some tag like `__complex__: true` in the _JSON_ (I'm not talking about Awkward properties) wouldn't guarantee that someone doesn't actually have and want data like that. The default parsing of JSON must be vanilla.\r\n\r\nThis situation is similar to the one where we wanted to encode floating point NaN and infinity as strings. The default parsing of a string `\"infinity\"` is the string `\"infinity\"`, but we let the user pass arguments to `from_json` that changed how this is parsed. We could do the same thing with complex numbers. Assuming that we'll always want complex numbers to go to or from a record, all that must be established is what those field names are. Unfortunately, not only is there not a standard, but everybody's solution differs: `\"r\"/\"i\"`, `\"R\"/\"I\"`, `\"re\"/\"im\"`, `\"Re\"/\"Im\"`, `\"RE\"/\"IM\"`, `\"real\"/\"imag\"`, `\"Real\"/\"Imag\"`, `\"REAL\"/\"IMAG\"`, `\"real\"/\"imaginary\"`, etc. Here's [one that hopes to become standardized](https://observablehq.com/@kgryte/stdlib-json-serialization-and-deserialization-of-exotic-d) that chose `\"re\"/\"im\"` with an extra `type` field.\r\n\r\nA pretty good rule would be to take `complex_fields` as a 2-tuple of strings in [ak.from_json](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_json.html) and if any JSON object in the data have at least these two fields, interpret it as a complex number. By default, `complex_fields` would be None. By asking for \"at least these two fields,\" we allow the records to have a tag. The same `complex_fields` argument should exist in [ak.to_json](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_json.html) as well.\r\n\r\nBut I would think of JSON reading/writing as one of the last things to worry about in adding complex types. The very first thing that would be needed is to pass these data from NumPy arrays into Awkward Arrays and back into NumPy arrays. Then there are reducers, which have to be handled carefully because a construct like `std::complex<float>` can only exist below and above the `extern \"C\"` layer. In between, it has to be a `void*` or maybe a `float*` and `double*`, depending. The name of the function determines that those numbers are to be reduced as complex numbers, rather than twice as many real numbers.",
  "created_at":"2021-01-15T17:18:59Z",
  "id":761071010,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MTA3MTAxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-15T17:18:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"oh, \"Kernel specializations not sorted in specification\" :-(",
  "created_at":"2021-01-21T16:39:37Z",
  "id":764778015,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NDc3ODAxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-21T16:39:37Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Note (affects all open PRs, of which this is the only one): #645 added a new submodule, which might require special handling in merging. Since I'm not comfortable with advanced git, I just cloned a new copy of the repo with `--recursive` to get everything and now I'm merging it into my PR.",
  "created_at":"2021-01-22T18:19:38Z",
  "id":765599505,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTU5OTUwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T18:19:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think you need to include something in Builder.h for Windows to find \"`complex` in namespace `std`\".",
  "created_at":"2021-01-25T18:29:56Z",
  "id":767021814,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NzAyMTgxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-25T18:29:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - here is a very simple solution :-)\r\n```python\r\n    array = ak.from_json(\r\n        '[{\"r\": 1.1, \"i\": 1.0}, {\"r\": 2.2, \"i\": 2.0}]', complex_record_fields=(\"r\", \"i\")\r\n    )\r\n    assert (array[\"r\"] + array[\"i\"]*1j).tolist() == [(1.1 + 1j), (2.2 + 2j)]\r\n```\r\nThe rest is done.",
  "created_at":"2021-01-26T17:40:01Z",
  "id":767708929,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NzcwODkyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-26T17:40:01Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - got it! it looks like `assert` treats `[(0.25+0j), (0.5+0j), (3.5+0j), (4.5+0j), (5.5+0j)]` as equal to `[0.25, 0.5, 3.5, 4.5, 5.5]`\r\n\r\nHere is the real `ak.to_list` output:\r\n```python\r\n>>> import numpy as np\r\n>>> import awkward as ak\r\n>>> content_float64 = ak.layout.NumpyArray(\r\n... np.array([0.25, 0.5, 3.5, 4.5, 5.5], dtype=np.float64)\r\n... )\r\n>>> content_float64\r\n<NumpyArray format=\"d\" shape=\"5\" data=\"0.25 0.5 3.5 4.5 5.5\" at=\"0x7f8ef843e820\"/>\r\n>>> content_complex64 = ak.values_astype(content_float64, \"complex64\", highlevel=False)\r\n>>> content_complex64\r\n<NumpyArray format=\"Zf\" shape=\"5\" data=\"0.25+0j 0.5+0j 3.5+0j 4.5+0j 5.5+0j\" at=\"0x7f8f18404180\"/>\r\n>>> array_complex64 = ak.layout.UnmaskedArray(content_complex64)\r\n>>> array_complex64\r\n<UnmaskedArray>\r\n    <content><NumpyArray format=\"Zf\" shape=\"5\" data=\"0.25+0j 0.5+0j 3.5+0j 4.5+0j 5.5+0j\" at=\"0x7f8f18404180\"/></content>\r\n</UnmaskedArray>\r\n>>> ak.to_list(content_complex64)\r\n[(0.25+0j), (0.5+0j), (3.5+0j), (4.5+0j), (5.5+0j)]\r\n>>> \r\n```",
  "created_at":"2021-01-27T13:54:17Z",
  "id":768300403,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMwMDQwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T13:54:17Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Remaining to-do:\r\n\r\n   - [x] getting all the tests to work: I re-read the testing suite and it's correctly checking all the things I wanted it to check\r\n   - [ ] https://github.com/scikit-hep/awkward-1.0/pull/652#pullrequestreview-577105992 is described as incomplete (the `builtins.complex` vs `__builtin__.complex` thing), but I don't see how the tests could pass if this were not complete. Is there anything left to do on this item?\r\n   - ~~pretty-print NumpyArrays with complex numbers~~ There's no need to do this.\r\n\r\nSo it sounds like everything might be done. Let me know if that one item above is incomplete. If not, I'll merge this PR.",
  "created_at":"2021-01-27T15:29:21Z",
  "id":768364649,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM2NDY0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T15:29:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Remaining to-do:\r\n> \r\n> * [x]  getting all the tests to work: I re-read the testing suite and it's correctly checking all the things I wanted it to check\r\n> * [ ]  [#652 (review)](https://github.com/scikit-hep/awkward-1.0/pull/652#pullrequestreview-577105992) is described as incomplete (the `builtins.complex` vs `__builtin__.complex` thing), but I don't see how the tests could pass if this were not complete. Is there anything left to do on this item?\r\n> * ~pretty-print NumpyArrays with complex numbers~ There's no need to do this.\r\n> \r\n> So it sounds like everything might be done. Let me know if that one item above is incomplete. If not, I'll merge this PR.\r\n\r\nThanks, @jpivarski ! Yes, there is an open question if the following comparison though it works, could be done better `obj.attr(\"__class__\").attr(\"__name__\").cast<std::string>() == \"complex\"`\r\nIt looks from the builds that like both Python 2 and 3 are happy with it. If you think, it's ok to leave it asis, please, go ahead and merge it. Thanks!",
  "created_at":"2021-01-27T15:47:52Z",
  "id":768378196,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM3ODE5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T15:47:52Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Yes, there is an open question if the following comparison though it works, could be done better `obj.attr(\"__class__\").attr(\"__name__\").cast<std::string>() == \"complex\"`\r\n> It looks from the builds that like both Python 2 and 3 are happy with it.\r\n\r\nI kept wavering on the necessity of this one: it works without it, but if a class happens to be named \"`complex`\" while not being the built-in Python `complex` type, then this would pass when it shouldn't. I just did the fix and will be pushing it soon.\r\n\r\nIt looks like a significant merge will also be needed, so that might be another commit from me. Once those two commits go through and pass tests, I'll merge it.\r\n\r\nThanks!",
  "created_at":"2021-01-27T16:04:23Z",
  "id":768389600,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM4OTYwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:04:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The merge to `main` turned out to be no big deal (something must have been bad in my local git, so I've refreshed it).\r\n\r\n@ianna When the tests pass on this, I'll merge it, or you can do the honors!",
  "created_at":"2021-01-27T16:11:58Z",
  "id":768394527,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM5NDUyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:11:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Both 3.8 and 2.7 worked in my local tests. Once it works in Azure, it can be merged.",
  "created_at":"2021-01-27T16:29:20Z",
  "id":768406210,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODQwNjIxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:29:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> The merge to `main` turned out to be no big deal (something must have been bad in my local git, so I've refreshed it).\r\n> \r\n> @ianna When the tests pass on this, I'll merge it, or you can do the honors!\r\n\r\nThanks @jpivarski ! Please, go ahead and merge it. I'll be in a meeting for an hour or so :-) Thanks!",
  "created_at":"2021-01-27T16:31:04Z",
  "id":768407355,
  "issue":652,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODQwNzM1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:31:04Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Since this PR currently cleans the warnings and nothing else, could you rename it and merge it as-is? That way, I can work with the same warning-free starting point. Thanks!",
  "created_at":"2021-01-18T13:04:04Z",
  "id":762237855,
  "issue":654,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjIzNzg1NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-18T13:04:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Since this PR currently cleans the warnings and nothing else, could you rename it and merge it as-is? That way, I can work with the same warning-free starting point. Thanks!\r\n\r\nOk, I'll wait for the tests to pass",
  "created_at":"2021-01-18T13:10:03Z",
  "id":762240874,
  "issue":654,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjI0MDg3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-18T13:10:03Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The extension of laziness to allow field-slices to remain lazy (`arr[\"a\"]`) has made it possible to enter the `VirtualArray::getitem_next_jagged` function. What these functions ought to do is simple: materialize the array and pass `getitem_next_jagged` down to the materialized array, so I just added that. It looks like the first of four cases has already been reached and implemented, so I did the other three.\r\n\r\nThanks for pointing this out!",
  "created_at":"2021-01-18T17:11:41Z",
  "id":762375857,
  "issue":655,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjM3NTg1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-18T17:11:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - Thanks! Shall I merge it?",
  "created_at":"2021-01-19T15:48:36Z",
  "id":762931234,
  "issue":658,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjkzMTIzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-19T15:48:36Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes! I was going to merge it as soon as tests pass, but I lost track of it among other things.",
  "created_at":"2021-01-19T15:54:47Z",
  "id":762935589,
  "issue":658,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjkzNTU4OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-19T15:54:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Just to be clear, we always \"squash and merge.\" I just pulled an update in main and it looks like 5 commits, whereas I was expecting 1 commit. I'll squash and merge this now.",
  "created_at":"2021-01-19T16:03:29Z",
  "id":762941731,
  "issue":658,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Mjk0MTczMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-19T16:03:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I see\u2014I was too late. I don't plan to change any git history after the fact, but from now on, let's just always \"squash and merge\" so that every PR counts as one commit in the main branch. Thanks!",
  "created_at":"2021-01-19T16:04:53Z",
  "id":762942868,
  "issue":658,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Mjk0Mjg2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-19T16:04:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is the payoff: the reason why AwkwardForth was developed. Uproot will be able to read unsplit data as fast as ROOT does, without any compilation.\r\n\r\n![Uncompressed ROOT files \u2192 Awkward Arrays (warm cache)](https://user-images.githubusercontent.com/1852447/105405568-e65c0100-5bf0-11eb-9616-aae4805af191.png)\r\n\r\n![ZLIB(1) ROOT files \u2192 Awkward Arrays (warm cache)](https://user-images.githubusercontent.com/1852447/105405574-e8be5b00-5bf0-11eb-924b-ed821293dc9e.png)\r\n\r\nPulling data from warm cache (i.e. already in RAM, using [vmtouch](https://hoytech.com/vmtouch/)) is limited only by the memory bus at about 1000 MB/sec\u2014the ROOT test is slower because I'm not using ROOT's BulkIO, which I should do in a future version of this test. When the data are compressed, that becomes the bottleneck at about 100 MB/sec.\r\n\r\nIn past studies, the singly jagged case was about as fast as the non-jagged case because [that \"jagged\" was arrays](https://github.com/scikit-hep/uproot3#uproot), this \"jagged\" is `std::vector`. Skipping that header means a bit of NumPy work, which slows it down, but not as much as doubly jagged or triply jagged (non-NumPy code). These results settle a question I had in my mind: Uproot should use AwkwardForth even for the singly jagged case (so, not just `AsObjects`, but also `AsJagged`).\r\n\r\nHere's what the jagged3 specialized Forth looks like:\r\n\r\n```forth\r\ninput data\r\ninput byte_offsets\r\noutput offsets2 int32\r\noutput offsets1 int32\r\noutput offsets0 int32\r\noutput content float32\r\n\r\n0 offsets2 <- stack\r\n0 offsets1 <- stack\r\n0 offsets0 <- stack\r\n\r\nbegin\r\n  byte_offsets i-> stack\r\n  6 + data seek\r\n  data !i-> stack\r\n  dup offsets2 +<- stack\r\n  0 do\r\n    data !i-> stack\r\n    dup offsets1 +<- stack\r\n    0 do\r\n      data !i-> stack\r\n      dup offsets0 +<- stack\r\n      data #!f-> content\r\n    loop\r\n  loop\r\nagain\r\n```",
  "created_at":"2021-01-21T20:10:25Z",
  "id":764906902,
  "issue":661,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NDkwNjkwMg==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":2,
   "rocket":1,
   "total_count":3
  },
  "updated_at":"2021-01-23T04:13:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Same thing for Avro. The [fastavro](https://fastavro.readthedocs.io/en/latest/) library is a precompiled Python extension, but it doesn't know the data type until runtime and it has to create Python objects, not Awkward Arrays. That's why Awkward Array can read Avro much faster. (A similar story would apply to Protobuf, etc.)\r\n\r\n![Uncompressed Avro files \u2192 Awkward Arrays (warm cache)](https://user-images.githubusercontent.com/1852447/105415834-e4993a00-5bfe-11eb-90ca-18865cf46518.png)\r\n\r\n![LZ4(1) Avro files \u2192 Awkward Arrays (warm cache)](https://user-images.githubusercontent.com/1852447/105415846-e82cc100-5bfe-11eb-95e5-32b1a4ade2ff.png)\r\n\r\nHere's what the jagged3 specialized Forth looks like:\r\n\r\n```forth\r\ninput stream\r\noutput offset0 int32\r\noutput offset1 int32\r\noutput offset2 int32\r\noutput content float32\r\n\r\n0 offset0 <- stack\r\n0 offset1 <- stack\r\n0 offset2 <- stack\r\n\r\n0 do\r\n  stream zigzag-> stack\r\n  dup offset0 +<- stack\r\n  0 do\r\n    stream zigzag-> stack\r\n    dup offset1 +<- stack\r\n    0 do\r\n      stream zigzag-> stack\r\n      dup offset2 +<- stack\r\n      stream #f-> content\r\n      stream b-> stack drop\r\n    loop\r\n    stream b-> stack drop\r\n  loop\r\n  stream b-> stack drop\r\nloop\r\n```",
  "created_at":"2021-01-21T21:42:00Z",
  "id":764959274,
  "issue":661,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NDk1OTI3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-23T04:12:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski super cool! Out of curiosity, I think I am not familiar with what means here 'doubly' and 'triply' jagged?",
  "created_at":"2021-01-22T15:38:22Z",
  "id":765491873,
  "issue":661,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTQ5MTg3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T15:38:37Z",
  "user":"MDQ6VXNlcjcwMTI0MjA="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski super cool! Out of curiosity, I think I am not familiar with what means here 'doubly' and 'triply' jagged?\r\n\r\n   * non-jagged: `float`\r\n   * jagged: `std::vector<float>` (which has different performance in Uproot than `float[nValues]`, the \"jagged\" used by NanoAOD, because the `std::vector` has a header to skip; the [else clause in this code](https://github.com/scikit-hep/uproot4/blob/main/uproot/interpretation/jagged.py#L173-L201))\r\n   * doubly jagged: `std::vector<std::vector<float>>`\r\n   * triply jagged: `std::vector<std::vector<std::vector<float>>>`",
  "created_at":"2021-01-22T15:41:10Z",
  "id":765493949,
  "issue":661,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTQ5Mzk0OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-22T15:41:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"For completeness, here's the Parquet test.\r\n\r\n![Uncompressed Parquet files \u2192 Awkward Arrays (warm cache)](https://user-images.githubusercontent.com/1852447/105568255-9f0f6680-5cfd-11eb-8a8f-1734e48a68c2.png)\r\n\r\n![ZLIB(1) Parquet files \u2192 Awkward Arrays (warm cache)](https://user-images.githubusercontent.com/1852447/105568410-b0a53e00-5cfe-11eb-9429-0b8a8a1afe40.png)\r\n\r\nLook at the code for unpacking repetition levels:\r\n\r\n```forth\r\ninput stream\r\noutput replevels uint8\r\n\r\nstream I-> stack\r\nbegin\r\n  stream varint-> stack\r\n  dup 1 and 0= if\r\n    ( run-length encoding )\r\n    stream {rle_format}-> replevels\r\n    1 rshift 1-\r\n    replevels dup\r\n  else\r\n    ( bit-packed )\r\n    1 rshift 8 *\r\n    stream #{bit_width}bit-> replevels\r\n  then\r\n  dup stream pos 4 - <=\r\nuntil\r\n```\r\n\r\nand then for turning repetition levels into offsets:\r\n\r\n```forth\r\ninput reps\r\noutput offsets0 int32 output offsets1 int32 output offsets2 int32\r\nvariable count0 variable count1 variable count2\r\n\r\nbegin\r\n  reps b-> stack\r\n\r\n  dup 3 = if\r\n    1 count2 +!\r\n  then\r\n  dup 2 = if\r\n    count2 @ offsets2 +<- stack\r\n    1 count2 !\r\n    1 count1 +!\r\n  then\r\n  dup 1 = if\r\n    count2 @ offsets2 +<- stack\r\n    1 count2 !\r\n    count1 @ offsets1 +<- stack\r\n    1 count1 !\r\n    1 count0 +!\r\n  then\r\n  0 = if\r\n    count2 @ offsets2 +<- stack\r\n    1 count2 !\r\n    count1 @ offsets1 +<- stack\r\n    1 count1 !\r\n    count0 @ offsets0 +<- stack\r\n    1 count0 !\r\n  then\r\n\r\n  reps end\r\nuntil\r\n\r\ncount2 @ offsets2 +<- stack\r\ncount1 @ offsets1 +<- stack\r\ncount0 @ offsets0 +<- stack\r\n```\r\n\r\nVery little about this needs to be specialized: it could as easily be a precompiled loop over N offset arrays. It doesn't benefit from specialization the way that ROOT (jagged > 1) and Avro do. Then we're left with the fact that the Forth VM is not as fast as compiled code, especially for a large number of instructions per iteration through the loop.",
  "created_at":"2021-01-23T04:11:19Z",
  "id":765862738,
  "issue":661,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTg2MjczOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-23T04:11:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That's true, `ak.Array.__delitem__` does not exist.\r\n\r\nIf the array is a record at top-level, this is all you'd need:\r\n\r\n```python\r\n>>> a[[x for x in ak.fields(a) if x != \"z\"]]\r\n<Array [{x: 1, y: 4}, ... y: 5}, {x: 3, y: 6}] type='3 * {\"x\": int64, \"y\": int64}'>\r\n```\r\n\r\nThough this has a few issues. One is that an empty list does the wrong thing because we don't know that it's supposed to be a list of strings:\r\n\r\n```python\r\n>>> a[[]]\r\n<Array [] type='0 * {\"x\": int64, \"y\": int64, \"z\": int64}'>\r\n```\r\n\r\nThere doesn't seem to be a way to make this output have type `3 * {}`, like this:\r\n\r\n```python\r\n>>> ak.Array(ak.layout.RecordArray([], [], 3))\r\n<Array [{}, {}, {}] type='3 * {}'>\r\n```\r\n\r\n~~The other issue is that this doesn't apply at the level where the record actually is; it applies at top-level.~~\r\n\r\nNope! Turns out, that's okay. It's hard to remember all of these rules. With a jagged array of records `b`,\r\n\r\n```python\r\n>>> b = ak.unflatten(a, [2, 0, 1])\r\n>>> print(b)\r\n[[{x: 1, y: 4, z: 7}, {x: 2, y: 5, z: 8}], [], [{x: 3, y: 6, z: 9}]]\r\n>>> ak.type(b)\r\n3 * var * {\"x\": int64, \"y\": int64, \"z\": int64}\r\n```\r\n\r\napplying the slice procedure maintains the jagged array structure above the record.\r\n\r\n```python\r\n>>> b[[x for x in ak.fields(b) if x != \"z\"]]\r\n<Array [[{x: 1, y: 4}, ... {x: 3, y: 6}]] type='3 * var * {\"x\": int64, \"y\": int64}'>\r\n```\r\n\r\nThe only thing that keeps the definition of `ak.Array.__delitem__` from being a one-liner is handling the case in which there's only one field and that's the one being removed. To do that right, `getitem_fields` should be exposed to the Python layout level through [content_methods](https://github.com/scikit-hep/awkward-1.0/blob/b5503c5e606fc594fabdc2e352c9432aa767e160/src/python/content.cpp#L1145-L1503) and then `ak.Array.__delitem__` can pass its reduced list of keys directly to that function, rather than as a slice (where there's the ambiguity about an empty list being a list of strings).",
  "created_at":"2021-01-22T20:21:59Z",
  "id":765662029,
  "issue":663,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTY2MjAyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T20:21:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thoughts about `a[\"z\"] = None` ? Should `None` be a special case (create a fully masked array?) or should we broadcast a numpy object array of None? This is somewhat tangential to delitem, I guess.",
  "created_at":"2021-01-22T21:13:04Z",
  "id":765686995,
  "issue":663,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTY4Njk5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T21:13:04Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"`a[\"z\"] = None` is not what I'd expect to remove a field. I'd expect `ak.Array.__delitem__` to do that. There are cases where we want special handling of None, rather than casting it as an array (whatever _that_ turns out to be), such as #487, in which we really do expect `ar != None` to give a boolean array with True everywhere that the option-type `ar` has a missing value.\r\n\r\nActually, I might think that `a[\"z\"] = None` would make `\"z\"` a new field (or replace one that was already there) consisting entirely of missing values. Like this:\r\n\r\n```python\r\n>>> ak.Array(\r\n...     ak.layout.IndexedOptionArray32(\r\n...         ak.layout.Index32(np.full(1000, -1, np.int32)),\r\n...         ak.layout.EmptyArray()\r\n...     )\r\n... )\r\n<Array [None, None, None, ... None, None, None] type='1000 * ?unknown'>\r\n```\r\n\r\nMaybe if `\"z\"` preexisted and it's assigned to be None, maybe then it would pick up the original type of `a[\"z\"]` (by putting it in the `content` instead of the EmptyArray).",
  "created_at":"2021-01-22T21:28:25Z",
  "id":765695355,
  "issue":663,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTY5NTM1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T21:28:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> a[\"z\"] = None is not what I'd expect to remove a field.\r\n\r\nYeah I realized that shortly after typing, and just wanted to highlight its current behavior is a bit troublesome (causes some error)",
  "created_at":"2021-01-22T21:41:55Z",
  "id":765701036,
  "issue":663,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTcwMTAzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T21:41:55Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"With this PR, the ForthMachine releases the Python GIL, so it scales. These were tested on a c5.18xlarge AWS instance (which has 72 cores). The ceiling might be the rate of memory access: my laptop maxes out at 2000 MB/sec.\r\n\r\n![AwkwardForth (future Uproot)](https://user-images.githubusercontent.com/1852447/105643144-cd06bf00-5e53-11eb-956a-380ca3ec53d7.png)\r\n\r\nOf course, Uproot's original deserializer in Python doesn't scale at all.\r\n\r\n![Python (current Uproot)](https://user-images.githubusercontent.com/1852447/105643147-cf691900-5e53-11eb-91b4-2287e85e570b.png)\r\n",
  "created_at":"2021-01-24T21:01:01Z",
  "id":766431271,
  "issue":664,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NjQzMTI3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-24T21:01:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Ran it a few more times, averaged, and got a smoother curve.\r\n\r\n![AwkwardForth (future Uproot) (1)](https://user-images.githubusercontent.com/1852447/105645270-31c81680-5e60-11eb-8c93-3c27b0dec24a.png)\r\n",
  "created_at":"2021-01-24T22:21:54Z",
  "id":766449708,
  "issue":664,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NjQ0OTcwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-24T22:21:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like the (single-threaded) raw data copying rate is 2000 MiB/sec, so it's not surprising that we hit a wall at 5000 MiB/sec.\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import time\r\n>>> array = np.ones(10000*1024**2, np.uint8)\r\n>>> begin = time.time(); array2 = np.copy(array); print(time.time() - begin)\r\n4.9701831340789795\r\n>>> del array2\r\n>>> begin = time.time(); array2 = np.copy(array); print(time.time() - begin)\r\n4.9057841300964355\r\n>>> del array2\r\n>>> begin = time.time(); array2 = np.copy(array); print(time.time() - begin)\r\n4.90533709526062\r\n>>> del array2\r\n>>> begin = time.time(); array2 = np.copy(array); print(time.time() - begin)\r\n4.904132127761841\r\n```",
  "created_at":"2021-01-24T22:28:20Z",
  "id":766450696,
  "issue":664,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NjQ1MDY5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-24T22:28:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It has the same limiting read speed when multiple threads are reading the same array.\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import time\r\n>>> import concurrent.futures\r\n>>> executor = concurrent.futures.ThreadPoolExecutor(2)\r\n>>> begin = time.time(); tmp = list(executor.map(np.copy, [array] * 2)); print(time.time() - begin)\r\n5.315511703491211\r\n>>> del tmp\r\n>>> begin = time.time(); tmp = list(executor.map(np.copy, [array] * 2)); print(time.time() - begin)\r\n5.248187303543091\r\n>>> del tmp\r\n>>> executor = concurrent.futures.ThreadPoolExecutor(4)\r\n>>> begin = time.time(); tmp = list(executor.map(np.copy, [array] * 4)); print(time.time() - begin)\r\n5.308006763458252\r\n>>> del tmp\r\n>>> begin = time.time(); tmp = list(executor.map(np.copy, [array] * 4)); print(time.time() - begin)\r\n5.11659049987793\r\n>>> del tmp\r\n>>> executor = concurrent.futures.ThreadPoolExecutor(8)\r\n>>> begin = time.time(); tmp = list(executor.map(np.copy, [array] * 8)); print(time.time() - begin)\r\n5.400676012039185\r\n>>> del tmp\r\n>>> begin = time.time(); tmp = list(executor.map(np.copy, [array] * 8)); print(time.time() - begin)\r\n5.453733205795288\r\n```\r\n",
  "created_at":"2021-01-24T22:36:19Z",
  "id":766451753,
  "issue":664,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NjQ1MTc1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-24T22:36:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is good; doing it in `studies` for now.",
  "created_at":"2021-01-25T16:58:29Z",
  "id":766958664,
  "issue":668,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Njk1ODY2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-25T16:58:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski All the tests you added pass now. Should we merge this into `studies/` now and then create a separate PR where we integrate it into the main codebase?",
  "created_at":"2021-02-27T20:04:50Z",
  "id":787127879,
  "issue":668,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4NzEyNzg3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-27T20:04:50Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski All the tests you added pass now. Should we merge this into `studies/` now and then create a separate PR where we integrate it into the main codebase?\r\n\r\nYes! You can merge it whenever you're ready.",
  "created_at":"2021-02-27T20:32:00Z",
  "id":787131080,
  "issue":668,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4NzEzMTA4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-27T20:32:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Without the architecture specifically mentioned, we can just change the `CUDA_VERSION` variable in the `cuda-build.sh` script, to switch to a different CUDA version, because of this block - https://github.com/scikit-hep/awkward-1.0/blob/main/cuda-build.sh#L11-L30. The architecture would be the default architecture for that CUDA version.\r\n\r\nIf we specify the architecture and later change `CUDA_VERSION` to a version that is more than 9.0 and which doesn't support `sm_30`, I think there would probably be an error. The architecture would have to be changed as well. This might be unwanted behaviour since I think the architecture choice of `sm_30` or `sm_35` might still be a little arbitrary right now.\r\n\r\nShould we still hardcode the `-arch sm_30` flag?",
  "created_at":"2021-01-25T19:16:54Z",
  "id":767049703,
  "issue":669,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NzA0OTcwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-25T19:16:54Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, fine: we'll just drop the `-arch sm_30` flag. I guess that means I'd accept this PR as-is. Give me a thumbs-up and I'll merge it (so that I don't do it while you're working on anything).",
  "created_at":"2021-01-25T19:20:14Z",
  "id":767051602,
  "issue":669,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NzA1MTYwMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-25T19:20:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I was just about to try to do that dependent project fix. Thanks for beating me to it!",
  "created_at":"2021-01-25T17:05:25Z",
  "id":766963442,
  "issue":670,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Njk2MzQ0Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2021-01-25T17:05:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - FYI, by reducing the number of includes, especially in the headers, total build plus test time on my Mac went down from `1 min 40 sec` to `32 sec`.",
  "created_at":"2021-01-25T17:09:01Z",
  "id":766966027,
  "issue":670,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Njk2NjAyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-25T17:09:01Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - FYI, by reducing the number of includes, especially in the headers, total build plus test time on my Mac went down from `1 min 40 sec` to `32 sec`.\r\n\r\nWow\u2014I'm looking forward to that! I thought most of the compilation time was spent in linking, not repeatedly compiling headers, but maybe.",
  "created_at":"2021-01-25T17:13:19Z",
  "id":766968815,
  "issue":670,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Njk2ODgxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-25T17:13:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! PR #673 should fix it.",
  "created_at":"2021-01-26T17:56:25Z",
  "id":767719301,
  "issue":671,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NzcxOTMwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-26T17:56:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I thought we didn't support partitioned Parquet datasets yet (#368). For issue 1, you can get more information using [ak.validity_error](https://awkward-array.readthedocs.io/en/latest/_auto/ak.validity_error.html) to get the error message. Issue 2 is likely something that I just didn't know about: is that a ChunkedArray with 3 chunks? The Arrow \u2192 Awkward function doesn't attempt to consolidate dictionaries from different chunks, so they could get duplicated, and we'll have to add code to stop that. Issue 3 is probably an oversight due to the fact that IndexedOptionArray does both indexing and option-type, but the test for categoricalness might only be checking for IndexedArrays.\r\n\r\nThese sound like not-too-difficult problems, all of them to be addressed on the Python level, but most will require me to become more knowledgeable about Arrow. You're right that they should probably be dealt with all at once, and probably along with #368 at the same time.",
  "created_at":"2021-01-26T19:19:38Z",
  "id":767769460,
  "issue":674,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Nzc2OTQ2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-26T19:19:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> I thought we didn't support partitioned Parquet datasets yet (#368).\r\n\r\nYes correct, thanks for pointing that out. I missed that. But it doesn't seem to be the root cause for Issue 1 and 3.\r\n\r\n\r\n\r\n> For issue 1, you can get more information using [ak.validity_error](https://awkward-array.readthedocs.io/en/latest/_auto/ak.validity_error.html) to get the error message.\r\n\r\nYes, I looked into that, result is `'at layout (IndexedArray32): __array__ = \"categorical\" requires contents to be unique'` which doesn't seem to be true here as `categorical` is correctly minimized. I've not yet looked further into the code of `ak.is_valid()`...\r\n\r\n\r\n\r\n> Issue 2 is likely something that I just didn't know about: is that a ChunkedArray with 3 chunks? ... we'll have to add code to stop that.\r\n\r\nYes, a) it is a collection of primitives ... b) and could be [flattened](https://arrow.apache.org/docs/python/generated/pyarrow.ChunkedArray.html#pyarrow.ChunkedArray.combine_chunks). The structure is used when dealing with partitioned parquets in particular.\r\n\r\n\r\n> These sound like not-too-difficult problems, all of them to be addressed on the Python level, but most will require me to become more knowledgeable about Arrow.\r\n\r\nI try looking into it by end of the week.",
  "created_at":"2021-01-26T19:39:40Z",
  "id":767780772,
  "issue":674,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Nzc4MDc3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-26T19:41:08Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Resolved via #727",
  "created_at":"2021-02-11T20:36:52Z",
  "id":777772854,
  "issue":674,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3Nzc3Mjg1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T20:36:52Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Could we exclude the `studies` and maybe also `docs-*` directories from these checks? The studies are completely free-form for figuring out the logic of a problem before committing it to mainline code. After having solved the logic, they're left for historical interest and maybe for figuring out a related problem. (I've used old studies in new studies from time to time.) The code in this directory is not necessarily even expected to work, which is why you found a syntax error.\r\n\r\nPerhaps we could remove code from this directory, saying that it's all in history, but in practice, it can be hard to find the last commit that has something you're looking for. GitHub's search doesn't search all of the history, for instance. When I need to find an old study, I don't want it to be super-hard! (I won't necessarily know what it was named.)\r\n\r\nFormatting the studies with Black is not only unnecessary, given what they are and what they're for, but it would add noise to this one PR, for instance, since this scratch-work is far from being properly formatted.",
  "created_at":"2021-01-27T02:49:57Z",
  "id":767977641,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Nzk3NzY0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T02:49:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You know, it's also putting newlines at the ends of SVG files. Those are made by Inkscape, and if edited by Inkscape, it will remove the newline again (ah, but the pre-commit would put it back...).\r\n\r\nInkscape is otherwise pretty good about minimizing diffs, in that it has an internally consistent style and maintains SVG element ids.",
  "created_at":"2021-01-27T02:54:02Z",
  "id":767979154,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Nzk3OTE1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T02:54:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sure, I can exclude svgs and those two directories.",
  "created_at":"2021-01-27T02:55:23Z",
  "id":767979663,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Nzk3OTY2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T02:55:23Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"The test seem to be trailing whitespace in markdown, configuration files, and black in places where I hadn't thought of running it, like the dependent tests. I'm happy with the changes and with having them become automated.\r\n\r\nJust not in the studies directory, and probably not for SVG files.",
  "created_at":"2021-01-27T03:00:39Z",
  "id":767981726,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Nzk4MTcyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T03:00:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Reran with a global exclusion for `^docs.*` and `^studies.*` (regex), and removed the svg file type from the end-of-file fixer.",
  "created_at":"2021-01-27T03:07:37Z",
  "id":767984371,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Nzk4NDM3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T03:07:37Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Now that I'm looking at this on a computer and not a phone, I found one more: \"tests/samples/test-two-arrays.json\" was badly formatted on purpose, to test the JSON-reader's insensitivity to such issues. I see that `*.svg` have been excluded; `*.json` should be excluded as well, or else (or also) anything in the \"tests/samples\" directory, since these are all raw data and may have formatting issues on purpose.",
  "created_at":"2021-01-27T14:13:58Z",
  "id":768312998,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMxMjk5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:13:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As soon as this PR is merged, I'd like all contributors to try running pre-commit. It should be as simple as `brew install pre-commit` / `pipx install pre-commit` / `pip install pre-commit` (whatever you like to use for your main applications), and then just try `pre-commit run -a`. (or `pre-commit run --all-files`; I like descriptive names, but since I type this hundreds of times per day, short is better ;) )\r\n\r\nWe can set it up via CI, but ideally everyone will be able to run locally as well.",
  "created_at":"2021-01-27T15:11:56Z",
  "id":768351944,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM1MTk0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T15:11:56Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I've installed pre-commit and will try\r\n\r\n```bash\r\npre-commit run -a\r\n```\r\n\r\non my next PR (when it's still small and doesn't have many changes).",
  "created_at":"2021-01-27T16:30:02Z",
  "id":768406654,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODQwNjY1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:30:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I scanned them again and I'm 100% happy with the changes. If you're not editing it locally, you can merge it or let me know that I can merge it.",
  "created_at":"2021-01-27T16:32:22Z",
  "id":768408253,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODQwODI1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:32:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":":tada: Nope, not editing locally.",
  "created_at":"2021-01-27T16:34:37Z",
  "id":768409595,
  "issue":675,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODQwOTU5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:34:37Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"My last local build might be missing the GitHub button (which was the point of this!), will check tomorrow.",
  "created_at":"2021-01-27T04:38:29Z",
  "id":768023757,
  "issue":677,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODAyMzc1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T15:01:31Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Will revert to html_theme_options, opened an issue here:  https://github.com/executablebooks/jupyter-book/issues/1194",
  "created_at":"2021-01-27T15:40:06Z",
  "id":768372263,
  "issue":677,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM3MjI2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T15:40:06Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"A lot of these options used to not work outside of `sphinx_config`. JupyterBook has been changing too rapidly\u2014I didn't like how the docs kept breaking because of upstream changes.\r\n\r\nThe configuration parameter that I want most is `show_navbar_depth: 2`, and I see that's still there. The second-tier topics in the left-bar were written to be visible at all times. They used to be grouped by section headers, but that feature was removed from JupyterBook, so my next-best approximation was to make them nested within fake pages whose content is a manually-maintained table of contents for that section, and then the default navbar depth was dropped to 1, making them hidden again. I'm afraid I might have adopted this too early!\r\n\r\nWhen this is ready for review, I should check it out and look at the documentation it produces, since the visual layout is not something that can be caught by a test. (Note: even the \"doctest-awkward (JupyterBook py38-np*)\" is new. Until recently, we had no automated test to ensure that those notebook examples were correct! At least there's that.)",
  "created_at":"2021-01-27T15:50:16Z",
  "id":768379750,
  "issue":677,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM3OTc1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T15:50:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> My last local build might be missing the GitHub button (which was the point of this!), will check tomorrow.\r\n\r\nIn some documentation, I've deliberately removed the \"edit on GitHub\" button because the sources were auto-generated and don't exist in the git repo. Not this one, though: every one of the pages in docs-src are hand-written (Jupytext, sometimes edited in Jupyter, sometimes manually). The \"edit on GitHub\" button would be a good addition, especially since any errors introduced into the code examples will be tested by the \"doctest-awkward (JupyterBook).\"",
  "created_at":"2021-01-27T16:16:24Z",
  "id":768397559,
  "issue":677,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM5NzU1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:16:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Edit on GitHub is already there, the \"repository\" button was the one missing.\r\n\r\n<img width=\"169\" alt=\"Screen Shot 2021-01-27 at 11 29 05 AM\" src=\"https://user-images.githubusercontent.com/4616906/106021601-f5c8c780-6092-11eb-91c6-e0640c44a4bf.png\">\r\n\r\nBut I'd like to see:\r\n\r\n<img width=\"171\" alt=\"Screen Shot 2021-01-27 at 11 29 24 AM\" src=\"https://user-images.githubusercontent.com/4616906/106021612-f7928b00-6092-11eb-894f-b3545cd5694f.png\">\r\n\r\nI'm testing locally, so I'll have to move a few things back until the linked Issue gets resolved in juptyer-book. The idea is to have no visual change expect for the addition of the missing menu item.\r\n",
  "created_at":"2021-01-27T16:30:57Z",
  "id":768407271,
  "issue":677,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODQwNzI3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:30:57Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Eventually we might be able to refactor a bit, but this is simple and adds what I wanted to see. :/ Don't quite know why I was losing the button entirely with the version that put the values in the \"standard\" places, maybe it doesn't merge correctly, or maybe it was my error.",
  "created_at":"2021-01-27T16:45:23Z",
  "id":768416503,
  "issue":677,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODQxNjUwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T16:45:23Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I just checked it out, ran the documentation-generator, and visually inspected it. That new \"repository\" item in the drop-down menu correctly links to the main GitHub page. Also, the left-bar is still expanded, which is what I wanted to maintain.",
  "created_at":"2021-01-28T15:35:05Z",
  "id":769168611,
  "issue":677,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTE2ODYxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T15:35:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! Knowing the layout of the array will be essential. The set of types that Arrow can write to Parquet has grown dramatically, and there used to be issues in which we left unreachable elements in the Arrow array that prevented it from being written to Parquet (instead of failing earlier), so we've seen things _like_ this before.\r\n\r\nIf you pickle your array and it's not too big, you can post it here instead of trying to make the original reproducible.",
  "created_at":"2021-01-27T13:04:15Z",
  "id":768271360,
  "issue":678,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODI3MTM2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T13:04:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"With the current version I can't reproduce the Error nor the array layout that caused it and cannot recover a pickled version of it. I close the issue.",
  "created_at":"2021-02-10T15:27:33Z",
  "id":776791084,
  "issue":678,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3Njc5MTA4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-10T15:27:33Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"I also encountered an Arrow bug when working with pyarrow 2.0 that went away in pyarrow 3.0. (It was an actual bug, though I don't remember the details.) Then, since I needed a LargeListArray fix in pyarrow 3.0, I made that the new minimum version for Awkward. It could be that increasing the Awkward version brought in a better pyarrow.",
  "created_at":"2021-02-10T15:32:39Z",
  "id":776794884,
  "issue":678,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3Njc5NDg4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-10T15:32:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #680 fixes this:\r\n\r\n```python\r\n>>> tmp = ak.mask(data, np.full(length, True))\r\n>>> tmp\r\nLoading part0-loads_very_slowly-data\r\n<Array [{run: 1, test: 1}, ... test: 1}] type='499 * ?{\"run\": uint32, \"test\": in...'>\r\n```\r\n\r\nIt's part of a more general attack on right-broadcasting, though, which is NumPy-style broadcasting that associates elements between arrays from the deepest level up to the shallowest level (and only applies to regular, NumPy-like arrays). The deepest-to-shallowest scan is what was materializing the array. For example, it also caused scikit-hep/uproot4#244.\r\n\r\nRight-broadcasting is needed in functions that generalize NumPy, such as ufuncs and `ak.where`, but others like `ak.mask` use the same internal function to align input arrays (e.g. to left-broadcast, from shallowest to deepest, a scalar in `ak.mask(array, True)`). I'm adopting a new policy of not right-broadcasting except where necessary for NumPy-generalization.",
  "created_at":"2021-01-27T17:30:18Z",
  "id":768448351,
  "issue":679,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODQ0ODM1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T17:30:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right; this is worrisome. I've found examples of overshadowed tests, too. I'm glad that this can now be automated.\r\n\r\nFortunately, it looks like they're not broken overshadowed tests (from the CI so far)!",
  "created_at":"2021-01-27T21:09:07Z",
  "id":768578724,
  "issue":681,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODU3ODcyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T21:09:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Great! (technically, it will be automated very soon, but isn't yet, at least not in CI or by the currently publicly available pre-commit file).",
  "created_at":"2021-01-27T21:10:28Z",
  "id":768579418,
  "issue":681,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODU3OTQxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T23:16:16Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"We should have a procedure for merging these PRs. Should I consider all non-draft PRs from you to be \"ready to merge\" from your perspective and when I press the merge button, I'm confirming it? That is, your PRs are passed by a consensus between the two of us. If it hasn't passed tests yet (also a precondition), I could write \"ready to merge\" for my vote until the auto-merge is set up.",
  "created_at":"2021-01-27T21:19:00Z",
  "id":768584028,
  "issue":681,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODU4NDAyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T21:19:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, if you see a PR from me, if it's not draft, it's ready to be merged if you like it and it passes tests. If it does not pass tests, you'll often see me set it back to draft as I scramble to fix it (usually Python 2. :) ). If I'm making a placeholder or a discussion PR, it will be draft.",
  "created_at":"2021-01-27T23:12:05Z",
  "id":768639488,
  "issue":681,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODYzOTQ4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T23:12:05Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2021-01-27T23:17:35Z",
  "id":768641602,
  "issue":681,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODY0MTYwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T23:17:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Although technically the way these were caught was by \"pointless comparisons,\" they're not pointless\u2014they're attempts at assertions that failed to get the word \"asset\" (or somehow lost it). Similarly, before I added the rule that \"the truth value of an array is ambiguous,\" there had been assertions that always returned true, even if the arrays on both sides of the `==` didn't match. (They merely had to return a non-empty sequence, even if that sequence was full of \"false\" values.) That's probably why NumPy introduced that rule.\r\n\r\nThis automated check has revealed two failing tests. Both contain a trivial thing to fix: the string representation of a floating point number has a trailing `.0`, but one of them _also_ has a substantial error: `ak.from_json(file)` was recently modified to accept a sequence of JSON documents into an array. In this particular file, it has only one document and should be one level less nested than it is\u2014the new rule about accepting a sequence of JSONs shouldn't make a single JSON into an item in an array.\r\n\r\nThis test from way back in the \"0018\" era would have caught this introduced error, but it was missing the word `assert`. It's a great automated check!\r\n\r\nI'll fix this tomorrow when I get to my computer, unless @ianna gets to it first. (This is in the new multi-line JSON handling. If only one full JSON document is observed, it should not be considered the first in a series, but the entirety of that single document.)",
  "created_at":"2021-01-28T04:00:55Z",
  "id":768783740,
  "issue":682,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODc4Mzc0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T04:00:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm using my fork, so `gh pr checkout 682` will check this out if you use the GitHub command line tool without any fuss. If put something in that I think others will be likely to edit, I try to use the main repo, generally, just to simplify collaboration. I didn't know this would actually catch a bug. :)",
  "created_at":"2021-01-28T04:30:49Z",
  "id":768792001,
  "issue":682,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODc5MjAwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T04:30:49Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I've actually had a discussion with the author of this check on special cases, by the way. There is a very rare, special case where you have to use a `# noqa` for it (no questions asked, didn't realize that at first): if you have a comparison *that throws an error* that you want to catch in your tests. But yes, it's fantastic and one of my favorites. :)",
  "created_at":"2021-01-28T04:35:09Z",
  "id":768793244,
  "issue":682,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODc5MzI0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T04:35:09Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I had been thinking that this was a bug introduced by PR #543, but I was wrong. It wasn't _reading_ a single JSON document as though it were a JSONs stream, it was _writing_ a single JSON document with one level too deep nesting.\r\n\r\nI did some forensics on it: there was a time when the `tojson_part` was changed from assuming it was inside a `beginlist()`/`endlist()` pair to creating the `beginlist()`/`endlist()` pair if it's a list type. That means an update in two places: inside `tojson_part` and outside of it in the `Content::tojson` that calls it. ([Here is what it looked like](https://github.com/scikit-hep/awkward-1.0/blob/62afebc2513efaea1792182830d9b18fc92294f9/src/libawkward/Content.cpp#L12-L42) in olden times, just after these functions were introduced, and [here is what it looked like](https://github.com/scikit-hep/awkward-1.0/blob/c5d736219e68fb3e58900e6ae2489b7589779e35/src/libawkward/Content.cpp#L821-L886) right before I put in this fix. The `builder.beginlist()`/`builder.endlist()` was removed from the overload that creates a string to agree with the new definition within `tojson_part`, but it was not removed from the overload that writes to a file.)\r\n\r\nWhen this change was put in, I must have been testing the string output because that's easy to write a test around. I would have been _assuming_ that the file version was okay because there were tests of JSON input and output \"since forever.\" So I was explicitly relying on the test that apparently didn't have the word `assert` before it.",
  "created_at":"2021-01-28T14:23:14Z",
  "id":769087844,
  "issue":682,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTA4Nzg0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T14:23:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's green. :)",
  "created_at":"2021-01-28T14:54:24Z",
  "id":769138088,
  "issue":682,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTEzODA4OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-28T14:54:24Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I couldn't build this locally to check it; it turns out `wheel` is pinned in pyproject.toml to a version of wheel that does not support macOS 11. (It expects the version to have length 2, like 10.15, not 11).\r\n\r\nI think we should unpin, probably just set a minimum (currently building in docker).",
  "created_at":"2021-01-28T21:11:06Z",
  "id":769397604,
  "issue":683,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTM5NzYwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T21:11:06Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> I couldn't build this locally to check it; it turns out `wheel` is pinned in pyproject.toml to a version of wheel that does not support macOS 11.\r\n\r\nI don't think that there was a strong reason for pinning `wheel` to a particular version. It might have just been the version that was current last February.",
  "created_at":"2021-01-28T21:14:22Z",
  "id":769400009,
  "issue":683,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTQwMDAwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T21:14:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`awkward-1.0.2.dist-info/entry_points.txt` looks okay, `awkward-1.0.2.dist-info/METADATA` seems reasonable , I'm going to say this is ready enough. If we are installing and testing the built wheels, then I'm completely happy. We will be soon enough if not. :)",
  "created_at":"2021-01-28T21:42:05Z",
  "id":769418241,
  "issue":683,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTQxODI0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T21:53:17Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"@jpivarski Do you care about being able to run flake8 manually, not through pre-commit? I generally keep black and mypy hand-runnable too, but usually don't bother to keep an ignore list for flake8 and just use pre-commit for it. It wasn't working all that well by hand.",
  "created_at":"2021-01-28T16:32:22Z",
  "id":769208355,
  "issue":685,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTIwODM1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T16:35:01Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski Do you care about being able to run flake8 manually, not through pre-commit? I generally keep black and mypy hand-runnable, but usually don't bother to keep an ignore list for flake8 and just use pre-commit for it. It wasn't working all that well by hand.\r\n\r\nI've been running black and flake8 manually. If the `pre-commit` command is equivalent, I suppose I won't need to. I was just compiling a list of directories to ignore...",
  "created_at":"2021-01-28T16:35:37Z",
  "id":769210548,
  "issue":685,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTIxMDU0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T16:35:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Should not be scanned for code quality:\r\n\r\n   * dependency sub-modules: pybind11, rapidjson, dlpack\r\n   * studies\r\n\r\nShould not be prevented from having `print` statements (in addition to the above):\r\n\r\n   * docs-*\r\n   * dependent-project   (it's an example, not production code)",
  "created_at":"2021-01-28T16:38:38Z",
  "id":769212606,
  "issue":685,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTIxMjYwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T16:38:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"pre-commit can run everything, which frees you from running these by hand (and you can name a hook to run just that one - `pre-commit run -a flake8` will just run the flake8 check). Pre-commit is git aware, so there's no need to build ignore lists of things that are not in git.\r\n\r\nPre-commit is also smart; if you run `pre-commit run`, it only checks changed files, so it's much faster if you just want to check your staged changes.\r\n\r\nSo I could drop the flake8 jobs, and add a general pre-commit job? That will also cut our CI time if the flake8 job really does run on each workflow!",
  "created_at":"2021-01-28T16:39:00Z",
  "id":769212836,
  "issue":685,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTIxMjgzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T16:39:00Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Pre-commit passes, it's only the manual flake8 jobs that are having problems, since they are not using the pre-commit ignores.",
  "created_at":"2021-01-28T16:39:48Z",
  "id":769213359,
  "issue":685,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTIxMzM1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T16:41:41Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, I think that the pre-commit would replace the flake8 step in CI. It's not a significant change in CI time, since most of the time is in compilation.\r\n\r\nThe flake8-print check is an example of something that I wouldn't want to be automatically changed. Print statements in developer tools and _a very few_ places in the production code are intended. The few that this caught in tests were not intended and I'll remove them as part of this PR. (The problem there is that pytest hides stdout by default, so I forget to check for residual debugging code.)",
  "created_at":"2021-01-28T16:45:14Z",
  "id":769216908,
  "issue":685,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTIxNjkwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T16:45:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Print is not bad in tests all the time - sometimes I'll print because the printout is easier to read than the pytest assert failure output - and it only shows up in tests. I didn't actually enable the print add-on in pre-commit (one benefit of pre-commit for flake8 - due to the entry-point based add ons, it depends on what you have installed - using pre-commit forces an exact, repeatable set of extensions).\r\n\r\nI'll add the print check and tell it to be ignored in tests and anywhere else (and check to see what other flake8 extensions might be already included that I missed).",
  "created_at":"2021-01-28T16:49:02Z",
  "id":769219310,
  "issue":685,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTIxOTMxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T16:49:27Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Flake8 _cannot_ auto-correct anything, it's a linter only.",
  "created_at":"2021-01-28T16:50:03Z",
  "id":769219938,
  "issue":685,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTIxOTkzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T16:50:03Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Since @henryiii is done with this PR, I'll merge it, though @reikdas should remember that it has only be tagged as \"don't complain,\" not necessary \"is correct.\"",
  "created_at":"2021-01-28T20:12:08Z",
  "id":769351778,
  "issue":685,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTM1MTc3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-28T20:12:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jrueb - thanks for spotting it! The https://github.com/scikit-hep/awkward-1.0/pull/687 should fix it.",
  "created_at":"2021-01-29T16:48:37Z",
  "id":769919359,
  "issue":686,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTkxOTM1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-29T16:48:37Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the quick fix, @ianna! I'll merge this as soon as the tests pass _unless_ you tell me not to.",
  "created_at":"2021-01-29T16:52:55Z",
  "id":769921753,
  "issue":687,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTkyMTc1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-29T16:52:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Thanks for the quick fix, @ianna! I'll merge this as soon as the tests pass _unless_ you tell me not to.\r\n\r\nThanks, @jpivarski ! I'm scanning through the code to add more checks like that. I'm almost done.",
  "created_at":"2021-01-29T17:09:09Z",
  "id":769931052,
  "issue":687,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTkzMTA1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-29T17:09:09Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I'm done. Please merge it when the tests pass. Thanks",
  "created_at":"2021-01-29T17:17:00Z",
  "id":769935210,
  "issue":687,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTkzNTIxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-29T17:17:00Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Can you turn @jrueb's examples into tests? I'll wait until you're done. At first, I thought this was a very simple fix, and if it's fixing a segfault, I wanted to fast-track it. The next set of updates look more involved, like this bug prompted you to search for similar bugs, and you're finding those. (If they can be expressed as tests, that, too, would be great.)",
  "created_at":"2021-01-29T17:17:59Z",
  "id":769935713,
  "issue":687,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTkzNTcxMw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-29T17:17:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the tests! I'll merge it as soon as these pass.",
  "created_at":"2021-01-29T17:50:35Z",
  "id":769952891,
  "issue":687,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTk1Mjg5MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-29T17:50:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"All fields of the MillionSongDataset can be lazily read.",
  "created_at":"2021-01-30T04:52:38Z",
  "id":770157365,
  "issue":688,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MDE1NzM2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-30T04:52:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"To fix this (PR #691), `__getitem__` has to pass down one more boolean in its recursive descent because there's a distinction between no advanced indexes (slicing with arrays of booleans or integers) and an advanced index with zero length. It doesn't make a different for any values, but it does matter for the type\u2014thanks for catching this!",
  "created_at":"2021-02-01T18:58:36Z",
  "id":771081108,
  "issue":689,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MTA4MTEwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-01T18:58:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"While the `ArrayBuilder` is building arrays, it does uniquely own the data, but then when `snapshot` is called, the passes a reference of its data to a new `Content`. In the initial design, both the `ArrayBuilder` and the `Content` own it from that point onward, so it has to be a `shared_ptr`. The `ArrayBuilder` can continue to append to the buffer while the `Content` uses its reference, though the `Content` assumes that it's only valid up to the `length` when the `snapshot` was taken. If the `ArrayBuilder` exceeds the allocation, it then makes a new buffer for itself and the `Content` ends up being the sole owner of the original. Two `snapshots` taken from the same `ArrayBuilder` might share the same buffer or they might not, depending on whether the `ArrayBuilder` had to grow the allocation in between `snapshots`. (That, by the way, is why we had to roll our own `GrowableBuffer` and couldn't just use `std::vector`. A `std::vector` doesn't provide its buffer as a `shared_ptr` and might delete it at any time.)\r\n\r\nThis policy could be changed for the sake of performance, but 5% or 6% doesn't sound like much to be interested in. Two other possible policies are:\r\n\r\n   * `ArrayBuilder` has a `unique_ptr`, which it _copies_ into a `shared_ptr` in the `snapshot` operation.\r\n   * `ArrayBuilder` has a `unique_ptr`, which it somehow upgrades into a `shared_ptr` when a `snapshot` is taken. Then both the `ArrayBuilder` and the `Content` both have a reference to the same `shared_ptr`. This might be unworkably complex because `ArrayBuilders` would have to work in two different modes: when it has `unique_ptr` and when it has `shared_ptr`.\r\n   * `ArrayBuilder` has a `unique_ptr`, which it upgrades into a `shared_ptr` to give to a `Content` (the `Content` always must have `shared_ptr`), but then the `ArrayBuilder` ceases to exist or ceases to be usable. This new policy would disallow multiple snapshots. That's a loss of a feature, though the most common case is to snapshot only once.\r\n\r\nIf we know ahead of time that the best any of these new policies can give is a 5\u20126% increase in speed, then I doubt it's worthwhile.\r\n\r\nYour metric could be made a bit better with [field_fast](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1ArrayBuilder.html#a950563074bc8987b63196de7d9066041), rather than [field_check](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1ArrayBuilder.html#a81883f6d746612b422a32b7990b9450f). The \"fast\" method checks field name equivalence by comparing pointers (valid for `const char*` strings like these), whereas the \"check\" method does string equality checks each time. A `TypedArrayBuilder` can go one better by requiring fields to be provided in the same order each time\u2014putting a boolean or a floating-point number into a field that's supposed to be an integer would always be an error, so assuming a constant order (which is almost always the case) becomes safer. [The logic that RecordBuilder goes through to verify that you're picking the right field is complex](https://github.com/scikit-hep/awkward-1.0/blob/1bfa9c60ba59d6e549011b287ebbcb49bebe5840/src/libawkward/builder/RecordBuilder.cpp#L408-L452); removing that in the typed case would surely be a win. (That logic starts by guessing that the order will be the same each time, so in the case when it is, it shouldn't suffer too much, but still.)\r\n\r\nAlso, unions can be improved in the `TypedArrayBuilder` case by requiring the user to provide a tag before filling the value, just as records and tuples require the user to provide a field name or number. That's not possible for the untyped `ArrayBuilder` because we don't even know _if_ data at a given level is going to be a union, but a `TypedArrayBuilder` would be initialized with the fact that the data should be a union (and it might not even be a union of different types, or it might be a union of mutual subtypes, like integer and floating-point, so this tag information would be absolutely required).\r\n\r\nThe API of `TypedArrayBuilder` will go a long way toward speeding things up, so chasing this 5\u20126% probably isn't worth it. (We can consider `unique_ptr` and the one-snapshot rule for `TypedArrayBuilder`, though, now that we know it's the biggest bottleneck of `ArrayBuilder`.)",
  "created_at":"2021-02-01T16:27:48Z",
  "id":770981773,
  "issue":690,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MDk4MTc3Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-01T16:27:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - indeed, switching to `field_fast` gives around 4% improvement (see `3.7%` cost of `::compare`) :-)\r\n<img width=\"1717\" alt=\"Screenshot 2021-02-01 at 18 24 47\" src=\"https://user-images.githubusercontent.com/1390682/106494661-f9a58100-64ba-11eb-9e3d-8297522dcff2.png\">\r\n\r\n",
  "created_at":"2021-02-01T17:33:10Z",
  "id":771025838,
  "issue":690,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MTAyNTgzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-01T17:33:10Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"A solution that results in the `NumpyArray::getitem_nothing` that I would generalize from the other `Content::getitem_nothing` cases is\r\n\r\n```c++\r\n  const ContentPtr\r\n  NumpyArray::getitem_nothing() const {\r\n    if (isscalar()) {\r\n      throw std::runtime_error(\r\n        std::string(\"internal error: getitem_nothing on a NumpyArray scalar\")\r\n        + FILENAME(__LINE__));\r\n    }\r\n    std::vector<ssize_t> shape;\r\n    std::vector<ssize_t> strides;\r\n    ssize_t x = itemsize_;\r\n    for (int64_t i = (int64_t)shape_.size() - 1;  i > 0;  i--) {\r\n      if (i == 1) {\r\n        shape.insert(shape.begin(), 0);\r\n      }\r\n      else {\r\n        shape.insert(shape.begin(), shape_[i]);\r\n      }\r\n      strides.insert(strides.begin(), x);\r\n      x *= shape_[i];\r\n    }\r\n    IdentitiesPtr identities;\r\n    if (identities_.get() != nullptr) {\r\n      identities = identities_.get()->getitem_range_nowrap(0, 0);\r\n    }\r\n    return std::make_shared<NumpyArray>(identities,\r\n                                        parameters_,\r\n                                        ptr_,\r\n                                        shape,\r\n                                        strides,\r\n                                        byteoffset_,\r\n                                        itemsize_,\r\n                                        format_,\r\n                                        dtype_,\r\n                                        ptr_lib_);\r\n  }\r\n```\r\n\r\nIt results in\r\n\r\n```python\r\n>>> ak.layout.NumpyArray(np.arange(2*3*5).reshape(2, 3, 5)).getitem_nothing().strides\r\n[40, 8]\r\n>>> ak.layout.NumpyArray(np.arange(2*3*5).reshape(2, 3, 5)).getitem_nothing().shape\r\n[0, 5]\r\n```\r\n\r\njust as\r\n\r\n```python\r\n>>> ak.Array(ak.from_numpy(np.arange(2*3*5).reshape(2, 3, 5), regulararray=True).layout.getitem_nothing())\r\n<Array [] type='0 * 5 * int64'>\r\n```\r\n\r\nHowever, this is not how `NumpyArray::getitem_nothing` is _used_. Scattered throughout Awkward Array is the assumption that `NumpyArray::getitem_nothing` returns a 1-dimensional empty array, so it should just continue doing that. (Remember that this is an internal function; it only needs to be used consistently. It does not need to make sense to users.)\r\n\r\nSo I'm going to close this without actually changing anything.",
  "created_at":"2021-02-17T20:47:01Z",
  "id":780842713,
  "issue":692,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDg0MjcxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T20:47:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!\r\n\r\nI've been finding and fixing a lot of bugs related to PartitionedArrays (the first commit of #699 goes down the line, making it have all the same methods and properties as `Content` subclasses). I've been developing some tutorial examples with lazy Parquet files, so I'm running into them myself and fixing them whenever I see them.\r\n\r\nIn this one, I think it makes sense for Awkward PartitionedArrays to be turned into [Arrow ChunkedArrays](https://arrow.apache.org/docs/cpp/arrays.html#chunked-arrays). However, ChunkedArrays are not turned into PartitionedArrays when round-tripping back (they just get concatenated, since the advantage of laziness is gone after round-tripping through Arrow). I might have to revisit that.",
  "created_at":"2021-02-05T00:06:17Z",
  "id":773685485,
  "issue":702,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MzY4NTQ4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-05T00:06:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Noted. In the meantime (while I compile for Python 2.7), the short story is to use [ak.flatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.flatten.html) if you want to remove a level of structure. If you have more than one level of nested arrays, either call `ak.flatten` multiple times, or pass `axis=None` to completely flatten. This actually flattens all structures, including records, so be sure this is what you want.\r\n\r\n```python\r\n>>> ak.flatten(ak.Array([{\"x\": 1, \"y\": 1.1}, {\"x\": 2, \"y\": 2.2}]), axis=None)\r\n<Array [1, 2, 1.1, 2.2] type='4 * float64'>\r\n```\r\n\r\nNote that flatten also removes missing values (None), so it's meaningful to flatten at `axis=0` if you're trying to get rid of them.\r\n\r\nDepending on what your intentions are, you might not want to flatten\u2014you might actually want to do something like\r\n\r\n```python\r\n>>> array[:, 0]\r\n```\r\n\r\nor\r\n\r\n```python\r\n>>> ak.sum(array, axis=1)\r\n```\r\n\r\nwhich also remove dimensions, but in ways that might correspond to what you actually want. (That's what I imagined most of the text on that page to be about, when I can write it.)\r\n\r\nAlso also, some libraries don't call `np.asarray` on their arguments, so they wouldn't recognize a NumPy-compatible array (e.g. one-dimensional, after flattening). If it's complaining about not recognizing type `awkward.highlevel.Array`, then try wrapping the Awkward Array with `np.asarray`. As a side-benefit, it will raise an exception if your array is not actually compatible with NumPy, alerting you to the fact that you need to make a decision about how you want to flatten it.\r\n\r\nCompilation's done!",
  "created_at":"2021-02-05T23:15:45Z",
  "id":774338822,
  "issue":704,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NDMzODgyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-05T23:15:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks! `ak.flatten` seems to do what I need.",
  "created_at":"2021-02-06T12:50:27Z",
  "id":774472872,
  "issue":704,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NDQ3Mjg3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-06T12:50:27Z",
  "user":"MDQ6VXNlcjU4ODQwNjU="
 },
 {
  "author_association":"MEMBER",
  "body":"Done in #943.",
  "created_at":"2021-06-18T18:32:32Z",
  "id":864209593,
  "issue":704,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDIwOTU5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T18:32:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Not exactly a bug; when I wrote that, it wasn't actually possible to identify an ellipsis in Python 2.7 with pybind11:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ef3587f2aed1e8e8e0b13d863b96885cc08d7156/src/python/content.cpp#L459-L463\r\n\r\nbut since then, the situation has changed:\r\n\r\nhttps://pybind11.readthedocs.io/en/stable/advanced/pycpp/numpy.html#ellipsis\r\n\r\nAwkward builds against pybind11 2.6.2, so we can use ellipsis now! PR #707.",
  "created_at":"2021-02-05T23:25:02Z",
  "id":774341840,
  "issue":705,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NDM0MTg0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-05T23:25:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The semantics of this\u2014what it's _supposed_ to do\u2014could be better thought-out. It was intended for cases in which missing values are present because it was for handling the output of [ak.argmin](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argmin.html) and [ak.argmax](https://awkward-array.readthedocs.io/en/latest/_auto/ak.argmax.html). (Here are some early thoughts about the problem: #203.) The problem is basically this:\r\n\r\n```python\r\n>>> # We want to find the max of one_quantity and look at the corresponding quantity in \"another.\"\r\n>>> one_quantity = ak.Array([[1, 2, 3], [], [5, 4]])\r\n>>> another = ak.Array([[1.1, 2.2, 3.3], [], [5.5, 4.4]])\r\n\r\n>>> # As of Awkward 1, argmax gives one dimensional output with Nones, not singletons/empties.\r\n>>> # (This is required for conformance with NumPy.)\r\n>>> ak.argmax(one_quantity, axis=1)\r\n<Array [2, None, 0] type='3 * ?int64'>\r\n\r\n>>> # Applying that to \"another\" is not what we want, since it applies to the first dimension.\r\n>>> # It rearranges the lists, rather than picking out the corresponding element from each list.\r\n>>> another[ak.argmax(one_quantity, axis=1)]\r\n<Array [[5.5, 4.4], None, [1.1, 2.2, 3.3]] type='3 * option[var * float64]'>\r\n\r\n>>> # ak.singletons gets us to that other format.\r\n>>> ak.singletons(ak.argmax(one_quantity, axis=1))\r\n<Array [[2], [], [0]] type='3 * var * int64'>\r\n\r\n>>> # And we get what we want: the corresponding value from each list.\r\n>>> another[ak.singletons(ak.argmax(one_quantity, axis=1))]\r\n<Array [[3.3], [], [5.5]] type='3 * var * float64'>\r\n\r\n>>> # Which we then have to pair with ak.firsts to turn it back into flat-with-missing-values.\r\n>>> ak.firsts(another[ak.singletons(ak.argmax(one_quantity, axis=1))])\r\n<Array [3.3, None, 5.5] type='3 * ?float64'>\r\n```\r\n\r\nHowever, that's non-obvious and verbose. I don't remember where I first saw it, but some users of Awkward found a much simpler solution:\r\n\r\n```python\r\n>>> # The 'keepdims' argument was included for NumPy compatibility...\r\n>>> ak.argmax(one_quantity, axis=1, keepdims=True)\r\n<Array [[2], [None], [0]] type='3 * var * ?int64'>\r\n\r\n>>> # But it gives us nearly what ak.singletons does, which simplifies the max-by-another problem.\r\n>>> another[ak.argmax(one_quantity, axis=1, keepdims=True)]\r\n<Array [[3.3], [None], [5.5]] type='3 * var * ?float64'>\r\n\r\n>>> # And then we can do the \"ak.firsts\" thing with a simple slice.\r\n>>> another[ak.argmax(one_quantity, axis=1, keepdims=True)][:, 0]\r\n<Array [3.3, None, 5.5] type='3 * ?float64'>\r\n```\r\n\r\nSo in truth, `ak.singletons` and `ak.firsts` aren't really needed to solve the problem they were invented for, and their semantics weren't carefully thought through for other problems.\r\n\r\nFor instance, I wonder if you can get what you want by turning a regular axis created with `np.newaxis` into an irregular one with [ak.from_regular](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_regular.html):\r\n\r\n```python\r\n>>> vals = ak.Array([[43, 15, 10.5], [11.5], [50, 5]])\r\n>>> idx = ak.Array([2, 0, 1])\r\n\r\n>>> # np.newaxis makes a new axis with length 1.\r\n>>> idx[:, np.newaxis]\r\n<Array [[2], [0], [1]] type='3 * 1 * int64'>\r\n\r\n>>> # But we're about to do a jagged slice, so turn the length-1 dimension into a variable-length one.\r\n>>> ak.from_regular(idx[:, np.newaxis])\r\n<Array [[2], [0], [1]] type='3 * var * int64'>\r\n\r\n>>> # Use this to slice \"vals\". It picks out the nth item from each list.\r\n>>> vals[ak.from_regular(idx[:, np.newaxis])]\r\n<Array [[10.5], [11.5], [5]] type='3 * var * float64'>\r\n\r\n>>> # It works exactly the same way if you have any missing data.\r\n>>> idx = ak.Array([2, None, 1])\r\n>>> vals[ak.from_regular(idx[:, np.newaxis])]\r\n<Array [[10.5], [None], [5]] type='3 * var * ?float64'>\r\n\r\n>>> # And that shape is exactly what you want if you're going to be removing this dimension.\r\n>>> vals[ak.from_regular(idx[:, np.newaxis])][:, 0]\r\n<Array [10.5, None, 5] type='3 * ?float64'>\r\n```\r\n\r\nSo maybe `ak.singletons` and `ak.firsts` should be deprecated, and these slicing tricks should be advertised in tutorials on the https://awkward-array.org site. (I _really_ need to write more of that documentation.) If you agree, I'll convert this Issue into a Discussion and then it would remain visible for others to comment on the proposed deprecation and benefit from the ad-hoc documentation above.",
  "created_at":"2021-02-06T19:45:55Z",
  "id":774531704,
  "issue":708,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NDUzMTcwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-06T19:45:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the detailed answer! I need to remember to use `keepdims=True` when the index array comes from other awkward operations. The `np.newaxis` trick works for when I have a handmade index array (that might use -1 instead of None for when the collection is empty).\r\n\r\nMaybe `ak.singletons`/`ak.firsts` is still useful to keep around, given that they can convert the output of `argmax`/`argmin` after they have been computed (at least, with the above alternatives advertised somewhere)? I've also found `ak.firsts` to be a nicer alternative to `ak.pad_none(a, 1)[:,0]` since it turns inner `[]` into `None`)",
  "created_at":"2021-02-06T20:00:48Z",
  "id":774533801,
  "issue":708,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NDUzMzgwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-06T20:00:48Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"MEMBER",
  "body":"But you did raise an important point, that `ak.singletons` and `ak.firsts` are not well thought-through. They don't do what you expected for non-option-type arrays, so they ought to be fixed (given a simple, easily describable rule for what they should do in all circumstances) or removed. Removing is definitely less work!\r\n\r\nI'll convert this into a Discussion to let others weigh in on it. Now would be a good time to schedule them for removal in 1.2.0 or 1.3.0 (see [Roadmap](https://github.com/scikit-hep/awkward-1.0#roadmap)).",
  "created_at":"2021-02-06T20:12:24Z",
  "id":774536544,
  "issue":708,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NDUzNjU0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-06T20:12:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The eventual TypedArrayBuilder will get its type at runtime, right? For it to be useful in Python, it still has to be compiled generically into a binary that we deliver through pip. At runtime and before filling, it can create a fixed number of output buffers and generate some Forth code specialized for its type, but that has to be entirely after the C++ compilation.\r\n\r\nMaybe there were be reason someday for a third kind of ArrayBuilder, which takes templated types and compiles to a specialized machine at C++ compile time (raising compiler errors if it is used incorrectly, rather than slow exceptions), but not enough people are using the C++ interface yet to justify that now.",
  "created_at":"2021-02-08T13:18:59Z",
  "id":775144507,
  "issue":711,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTE0NDUwNw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-08T13:18:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - tested on `7 x 1 000 000 000` `BoolBuilder::boolean`\r\n\r\nInitial change from a runtime type to determining properties of the type at compile time plus dropping the shared pointers\r\nreduces the time from `2.62 min` to `58.85 s`:\r\n<img width=\"1279\" alt=\"Screenshot 2021-02-09 at 18 16 43\" src=\"https://user-images.githubusercontent.com/1390682/107401488-37ca2280-6b03-11eb-96a1-8fcd0be28c12.png\">\r\n<img width=\"1282\" alt=\"Screenshot 2021-02-09 at 18 17 27\" src=\"https://user-images.githubusercontent.com/1390682/107401500-3bf64000-6b03-11eb-99c0-ed31d2f2e562.png\">\r\nSecond step a `TypedGrowableBuffer` (also `unique_ptr` based) is reserving a large enough memory chunk (as a unique pointer `std::unique_ptr<T> ptr = kernel::unique_ptr_malloc<T>`), plus not checking if it needed a resize:\r\n`GrowableBuffer::append` time went down from `48.60 s` to `7.54 s`:\r\n<img width=\"1282\" alt=\"Screenshot 2021-02-09 at 18 27 44\" src=\"https://user-images.githubusercontent.com/1390682/107402725-96dc6700-6b04-11eb-96f3-ae000ee9a3dd.png\">\r\n",
  "created_at":"2021-02-09T17:36:45Z",
  "id":776112177,
  "issue":711,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjExMjE3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T17:36:45Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I suspect that the error is correct for your first issue, though I'm open to improvements in the phrasing of the message. Is it because `jets` has fields with different depths? When you say `jets.pt`, you get a jagged array of numbers\u2014no record structures anywhere\u2014but `jets` is a record that might contain fields with more dimensions than `pt`. The `axis=-1` makes the function apply to the innermost dimension of all fields of the record(s), which might be different depths for different fields.\r\n\r\n```python\r\n>>> array = ak.Array([{\"x\": [1, 2, 3], \"y\": [[1, 2, 3], [], [4, 5]]}])\r\n>>> print(ak.num(array, axis=-1))\r\n[{x: 3, y: [3, 0, 2]}]\r\n>>> print(ak.local_index(array, axis=-1))\r\n[{x: [0, 1, 2], y: [[0, 1, 2], [], [0, 1]]}]\r\n```\r\n\r\nWhereas in NumPy, negative `axis` is a convenience (you don't have to check `array.ndim` to choose an `axis` from the other end), in Awkward Array, it's the only way to express some things. In the above example, `axis=-1` means `axis=1` for *x* and `axis=2` for *y*.\r\n\r\n```python\r\n>>> print(ak.num(array.x, axis=1), ak.num(array.y, axis=2))\r\n[3] [[3, 0, 2]]\r\n>>> print(ak.local_index(array.x, axis=1), ak.local_index(array.y, axis=2))\r\n[[0, 1, 2]] [[[0, 1, 2], [], [0, 1]]]\r\n```\r\n\r\nIn your case, it might be that `jets` has some fields that can't be mutually computed at any `axis=-1` level. (It depends on what's in the `jets` record, which is why it was hard to reproduce.) If you have a set of fields that you want to narrow in on, you could do a [nested projection](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#nested-projection), like this:\r\n\r\n```python\r\n>>> jets[[\"pt\", \"eta\", \"phi\", \"m\"]]\r\n```\r\n\r\nwhich would keep kinematics while dropping the subjet structure or associated leptons or whatever it is that has deeper jaggedness and is preventing the `ak.local_index`. On the other hand, if you're only trying to get a local index, you only need one field, such as `jets.pt`.\r\n\r\n----------------------------\r\n\r\nAs for the second issue, the extra dimension on the strings is the list that is the string itself. Strings are not special objects, they're a special interpretation of lists, but only some functions do special things with them. (Try looking at their `layout`: you'll see that the \"stringiness\" is just a parameterization of the ListArray/ListOffsetArray.) `ak.local_index`, in particular, is generic/naive with respect to strings\u2014there's no mention of strings [in the documentation](https://awkward-array.readthedocs.io/en/latest/_auto/ak.local_index.html).\r\n\r\nBut maybe this is the wrong behavior? I could convert this issue into a Discussion if these two topics are things you want others to chime in on.",
  "created_at":"2021-02-08T19:21:05Z",
  "id":775382121,
  "issue":712,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTM4MjEyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T19:21:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for the explanation. I agree these things aren't really bugs, but feel it would be good to have more error messages/documentation to make the user aware of them, and I agree this issue should be converted to a discussion so other people can suggest how they would expect it to behave.\r\n\r\nFor the first issue, having looked at the jets again, I see there is a field (`muonIdxG`) with `ndim`=3, while the other fields have `ndim`=2, as you suggested. If it's not too hard to implement, it might be nice if the error message in cases like these was something like \"Cannot compute negative index of RecordArray with different depths\" to make this a little clearer.\r\n\r\nFor the second issue, I hadn't realised awkward was aware that strings are lists, as this is not the case in numpy. I feel the current implementation would probably surprise a lot of people, but also some more experienced users might want to be able to access the strings in this way. Maybe one could add a flag for whether to consider strings as lists, or just objects in the array, either in the `local_index` function, or maybe as an option when producing an Array (with the default to just consider them as objects)?\r\n\r\nAlso, maybe index=-1 shouldn't be the default for `local_index`, as a new user expecting it to just behave as in numpy might experience similar pitfalls? However, I guess this is the option people are mostly likely to use, so it would be useful if other people have an opinion on this.",
  "created_at":"2021-02-09T12:23:19Z",
  "id":775899484,
  "issue":712,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTg5OTQ4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T12:23:19Z",
  "user":"MDQ6VXNlcjYwMjkyMjQ3"
 },
 {
  "author_association":"MEMBER",
  "body":"I'll make this a discussion. There are things to do here, but it would be good to get more input.\r\n\r\nNumPy has two ways of representing strings: as fixed-width bytes (unencoded or UTF-32) and as Python objects. The wasted space in fixed-width strings is severe enough that Pandas defaults to Python objects. However, Python objects can only be used at the Python level (no calculations in C++ without making the C++ layer depend on Python headers). Both of these dtypes can be _passed through_ an Awkward Array, but there are many places where I need to do something with the dtype of an array and only support a reasonable set (booleans, numbers, now including complex, and hopefully soon date-times). Fixed-width bytes and Python object pointers are not included in that set, so as a user, it might work at first, but soon you'd run into something unsupported.\r\n\r\nBut considering that both fixed-width bytestrings and Python objects are highly wasteful, I think we should keep using variable-length strings, as Arrow and Parquet do, but make more methods aware of them. Since negative index handling happens in one place, we could make a string's internal dimension never contribute to the \"number of dimensions\" used when calculating a negative `axis`. That can become one of the specializations associated with `__array__ = \"string\"`.\r\n",
  "created_at":"2021-02-09T16:28:21Z",
  "id":776064697,
  "issue":712,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjA2NDY5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T16:28:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The bug is that the double-masking occurred: there's a `simplify_optiontype` that we should be using to ensure that ByteMaskedArray of IndexedOptionArray gets collapsed down to IndexedOptionArray. I think I know where this one is\u2014the `ByteMaskedArray::getitem_field` (`arr.b`) should probably be calling `simplify_optiontype` and it's not. I'll go look.",
  "created_at":"2021-02-08T16:18:08Z",
  "id":775262239,
  "issue":713,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTI2MjIzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T16:18:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Should be fixed in PR #714. (I included your example in the testing suite, along with a VirtualArray version of the same, because changing the output type of `getitem_field` should be reflected in the virtual Form or you'll get a \"generated the wrong Form\" error.)",
  "created_at":"2021-02-08T18:53:58Z",
  "id":775365475,
  "issue":713,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTM2NTQ3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T18:53:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for fixing this so quickly!",
  "created_at":"2021-02-09T11:07:54Z",
  "id":775860418,
  "issue":713,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTg2MDQxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T11:07:54Z",
  "user":"MDQ6VXNlcjYwMjkyMjQ3"
 },
 {
  "author_association":"MEMBER",
  "body":"This is a good idea and I'll do it as a quick-fix, but it goes after the 1.1.0 release today.",
  "created_at":"2021-02-09T16:36:45Z",
  "id":776070584,
  "issue":715,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjA3MDU4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T16:36:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm not seeing this, so I uninstalled my local Awkward and installed from pip (version 1.1.0, which is identical to 1.1.0rc5, but we want the non-RC version to be the working one, anyway).\r\n\r\n```bash\r\n% pip uninstall awkward\r\nWARNING: Skipping awkward as it is not installed.\r\n% pip install awkward\r\nCollecting awkward\r\n  Downloading awkward-1.1.0-cp38-cp38-manylinux2010_x86_64.whl (7.5 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.5 MB 1.7 MB/s \r\nRequirement already satisfied: setuptools in ./miniconda3/lib/python3.8/site-packages (from awkward) (49.6.0.post20210108)\r\nRequirement already satisfied: numpy>=1.13.1 in ./miniconda3/lib/python3.8/site-packages (from awkward) (1.20.0)\r\nInstalling collected packages: awkward\r\nSuccessfully installed awkward-1.1.0\r\n```\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import numpy as np\r\n>>> arr = ak.Array([1, 2, 3])\r\n>>> arr[:, None]\r\n<Array [[1], [2], [3]] type='3 * 1 * int64'>\r\n>>> arr[:, np.newaxis]\r\n<Array [[1], [2], [3]] type='3 * 1 * int64'>\r\n```\r\n\r\nI'm also going to recompile my latest version to see if it's there.",
  "created_at":"2021-02-09T18:16:30Z",
  "id":776138319,
  "issue":718,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjEzODMxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T18:16:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Sorry, I hadn't realised 1.1.0 was available now, but having installed that this also seems to work for me, so I'm not sure why the 1.1.0rc5 didn't work for me",
  "created_at":"2021-02-09T18:28:18Z",
  "id":776146204,
  "issue":718,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjE0NjIwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T18:28:18Z",
  "user":"MDQ6VXNlcjYwMjkyMjQ3"
 },
 {
  "author_association":"MEMBER",
  "body":"I did some work on generalizing the behavior of `np.newaxis` recently; perhaps one of the intermediate versions was involved?",
  "created_at":"2021-02-09T18:30:29Z",
  "id":776147587,
  "issue":718,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjE0NzU4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T18:30:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Having updated awkward a couple of times to try and work out what was going on it seems the problem was I forgot to use \"python3 -m pip\" rather than pip3 to get my user updated version of pip. Sorry to raise it here.",
  "created_at":"2021-02-09T18:41:50Z",
  "id":776154632,
  "issue":718,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjE1NDYzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T18:41:50Z",
  "user":"MDQ6VXNlcjYwMjkyMjQ3"
 },
 {
  "author_association":"MEMBER",
  "body":"1.1.0 had an incomplete SDist which broke Conda (and anyone not getting a wheel), this should fix it, and improve testing so it's less likely to happen again.",
  "created_at":"2021-02-09T21:35:51Z",
  "id":776259833,
  "issue":720,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjI1OTgzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T21:35:51Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"That's a recent one! I just installed flake8-bugbear; I'll be using it in addition to the pre-commit.\r\n\r\nI'm in favor of this PR: as soon as the tests pass, you can merge it.",
  "created_at":"2021-02-09T21:55:25Z",
  "id":776269736,
  "issue":720,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjI2OTczNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T21:55:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Pre-commit runs the flake8 bugbear test. It handles everything for you, just run `pre-commit run -a`, and every check now and in the future will run. :) (though the check-manifest check I made manual because it's rather slow, you have to ask for it with `pre-commit run -a --hook-stage manual` (for all checks include manual ones), or just that check with `pre-commit run -a --hook-stage manual check-manifest` (all pre-commit checks can be run individually by id).",
  "created_at":"2021-02-09T21:57:37Z",
  "id":776270890,
  "issue":720,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjI3MDg5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T21:57:37Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Force pushing that last commit again to see if I can get pre-commit.ci to trigger.",
  "created_at":"2021-02-09T21:58:54Z",
  "id":776271501,
  "issue":720,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjI3MTUwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T21:58:54Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"By the way, to see this bug without Conda, just run:\r\n\r\n\r\n```bash\r\npip install awkward --no-binary awkward\r\n```\r\nFor example, from `docker run -it --rm python:3.9 bash`. Or try a regular pip install from a non-manylinux Linux, like Alpine or ClearLinux. It will die at 95% with \"fatal error: dlpack/dlpack.h: No such file or directory\".\r\n\r\nWill help with https://github.com/conda-forge/awkward-feedstock/pull/64 (once a fixed release is made).\r\n",
  "created_at":"2021-02-09T22:07:26Z",
  "id":776276615,
  "issue":720,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjI3NjYxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T22:07:59Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll try to get this to fail if you do a regular build and install in the tests in the future. If I can't, we can add a test where we make an SDist then install it. But for now, this should fix the problem with Conda and source installs.",
  "created_at":"2021-02-09T22:12:43Z",
  "id":776279370,
  "issue":720,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjI3OTM3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T22:12:43Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> By the way, to see this bug without Conda, just run:\r\n> \r\n> ```shell\r\n> pip install awkward --no-binary awkward\r\n> ```\r\n> \r\n> For example, from `docker run -it --rm python:3.9 bash`. Or try a regular pip install from a non-manylinux Linux, like Alpine or ClearLinux. It will die at 95% with \"fatal error: dlpack/dlpack.h: No such file or directory\".\r\n> \r\n> Will help with [conda-forge/awkward-feedstock#64](https://github.com/conda-forge/awkward-feedstock/pull/64) (once a fixed release is made).\r\n\r\nThe source distribution has been fixed several times in the past; the reason it's broken now is because dlpack was added. It's a recurrent issue.",
  "created_at":"2021-02-09T22:16:14Z",
  "id":776281194,
  "issue":720,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjI4MTE5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T22:16:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This new check should help a lot. The tests I was mentioning should be another big help. I _thought_ the PEP 517 build was supposed to solve this, but for some reason it's not isolating. Will look into this when I work on the build system.",
  "created_at":"2021-02-09T22:19:04Z",
  "id":776282616,
  "issue":720,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjI4MjYxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T22:19:04Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> It's a recurrent issue.\r\n\r\nThis is exactly why check-manifest is a recommendation in the Scikit-HEP developer docs. ;)",
  "created_at":"2021-02-09T22:29:20Z",
  "id":776287709,
  "issue":720,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjI4NzcwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T22:29:20Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"This was a case of the wrong error message. Jagged slices need to \"fit\" the corresponding array at all levels except the last. That was assumed in some parts of the code and failed to be enforced in others. I don't know _exactly_ what was happening to raise the memory error, but it was likely reading off the end of an array to decide how big to make the next array.\r\n\r\nI've instituted checks for array length at each level of `Content::getitem_next_jagged` and homogenized the error messages.\r\n\r\n```python\r\n>>> a = ak.layout.NumpyArray(np.arange(122))\r\n>>> idx = ak.layout.Index64([0, 2, 4, 6, 8, 10, 12])\r\n>>> a = ak.layout.ListOffsetArray64(idx, a)\r\n>>> idx = ak.layout.Index64([0, -1, 1, 2, -1, 3, 4, 5])\r\n>>> a = ak.layout.IndexedOptionArray64(idx, a)\r\n>>> a = ak.Array(a)\r\n>>> a\r\n<Array [[0, 1], None, [2, ... 8, 9], [10, 11]] type='8 * option[var * int64]'>\r\n\r\n>>> # not allowed\r\n>>> a[[[0], None]]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/highlevel.py\", line 1007, in __getitem__\r\n    return ak._util.wrap(self._layout[where], self._behavior)\r\nValueError: cannot fit jagged slice with length 2 into IndexedOptionArray64 of size 8\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.1.1/src/libawkward/array/IndexedArray.cpp#L2770)\r\n\r\n>>> # allowed\r\n>>> a[[[0], None, [], [], [], [], [], []]]\r\n<Array [[0], None, [], ... None, [], [], []] type='8 * option[var * int64]'>\r\n```",
  "created_at":"2021-02-10T18:26:34Z",
  "id":776916735,
  "issue":723,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjkxNjczNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-10T18:26:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think the check if the slice fits now is interfering with some valid slices. For example this doesn't work\r\n```python\r\na[ak.argsort(a)]\r\n```\r\n`cannot fit jagged slice with length 6 into IndexedOptionArray64 of size 8`. It seems it's not counting masked rows.",
  "created_at":"2021-02-11T09:41:50Z",
  "id":777316662,
  "issue":723,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzMxNjY2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T09:41:50Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, it's a different bug that fixing the above reveals.\r\n\r\nStarting with the same `a` (but using `arange` instead of `empty`, so that the numbers are recognizable and repeatable):\r\n\r\n```python\r\n>>> a = ak.layout.NumpyArray(np.arange(122))\r\n>>> idx = ak.layout.Index64([0, 2, 4, 6, 8, 10, 12])\r\n>>> a = ak.layout.ListOffsetArray64(idx, a)\r\n>>> idx = ak.layout.Index64([0, -1, 1, 2, -1, 3, 4, 5])\r\n>>> a = ak.layout.IndexedOptionArray64(idx, a)\r\n>>> a = ak.Array(a)\r\n```\r\n\r\nThe `ak.argsort(a)` is producing an IndexedArray of an IndexedArray (both are IndexedOptionArrays, in particular):\r\n\r\n```python\r\n>>> ak.argsort(a).layout\r\n<IndexedOptionArray64>\r\n    <index><Index64 i=\"[0 -1 1 2 -1 3 4 5]\" offset=\"0\" length=\"8\" at=\"0x56159d603d50\"/></index>\r\n    <content><IndexedOptionArray64>\r\n        <index><Index64 i=\"[0 1 2 3 4 5 -1 -1]\" offset=\"0\" length=\"8\" at=\"0x56159d5f7b10\"/></index>\r\n        <content><ListOffsetArray64>\r\n            <offsets><Index64 i=\"[0 2 4 6 8 10 12]\" offset=\"0\" length=\"7\" at=\"0x56159d5454c0\"/></offsets>\r\n            <content><NumpyArray format=\"l\" shape=\"12\" data=\"0 1 0 1 0 ... 1 0 1 0 1\" at=\"0x56159d5efd90\"/></content>\r\n        </ListOffsetArray64></content>\r\n    </IndexedOptionArray64></content>\r\n</IndexedOptionArray64>\r\n```\r\n\r\nThe `ak.argsort` operation ought to be calling `simplify` on its output before returning, if it's an option-type or indexed type.\r\n\r\n```python\r\n>>> ak.argsort(a).layout.simplify()\r\n<IndexedOptionArray64>\r\n    <index><Index64 i=\"[0 -1 1 2 -1 3 4 5]\" offset=\"0\" length=\"8\" at=\"0x56159d603d50\"/></index>\r\n    <content><ListOffsetArray64>\r\n        <offsets><Index64 i=\"[0 2 4 6 8 10 12]\" offset=\"0\" length=\"7\" at=\"0x56159d5454c0\"/></offsets>\r\n        <content><NumpyArray format=\"l\" shape=\"12\" data=\"0 1 0 1 0 ... 1 0 1 0 1\" at=\"0x56159d606920\"/></content>\r\n    </ListOffsetArray64></content>\r\n</IndexedOptionArray64>\r\n```\r\n\r\nThis doesn't change the meaning of the result (by definition):\r\n\r\n```python\r\n>>> ak.to_list(ak.argsort(a))\r\n[[0, 1], None, [0, 1], [0, 1], None, [0, 1], [0, 1], [0, 1]]\r\n>>> ak.to_list(ak.argsort(a).layout.simplify())\r\n[[0, 1], None, [0, 1], [0, 1], None, [0, 1], [0, 1], [0, 1]]\r\n```\r\n\r\nbut the slicing logic assumes that we have simplified IndexedArrays:\r\n\r\n```python\r\n>>> a[ak.Array(ak.argsort(a).layout.simplify())]\r\n<Array [[0, 1], None, [2, ... 8, 9], [10, 11]] type='8 * option[var * int64]'>\r\n```\r\n\r\nI should probably also add an exception message whenever a slice is constructed with `SliceMissing` inside of `SliceMissing`:\r\n\r\n```python\r\n>>> # ak._ext._slice_tostring is my back-door to see what kind of slice object is made from arrays\r\n>>> print(ak._ext._slice_tostring(ak.argsort(a)))\r\n[missing([0, -1, 1, 2, -1, 3, 4, 5], missing([0, 1, 2, 3, 4, 5], jagged([0, 2, 4, 6, 8, 10, 12], array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]))))]\r\n>>> print(ak._ext._slice_tostring(ak.Array(ak.argsort(a).layout.simplify())))\r\n[missing([0, -1, 1, 2, -1, 3, 4, 5], jagged([0, 2, 4, 6, 8, 10, 12], array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])))]\r\n```\r\n\r\nI should probably also add a validity-check for IndexedArrays inside IndexedArrays, option-type inside option-type, and union-type inside union-type. All of these have to be allowed in intermediate calculations (so I can't refuse them in the constructor, as I probably can for `SliceMissing`), but they shouldn't be considered valid for analysis.\r\n\r\n```python\r\n>>> # This should change.\r\n>>> ak.is_valid(ak.argsort(a))\r\nTrue\r\n```",
  "created_at":"2021-02-11T14:23:26Z",
  "id":777495750,
  "issue":723,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzQ5NTc1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T14:23:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I implemented all of those things in PR #729.",
  "created_at":"2021-02-11T15:06:12Z",
  "id":777552080,
  "issue":723,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzU1MjA4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T15:06:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> You mentioned that it is purely on python level but I think the validation checks were C++ based. If I am mistaken please advise.\r\n\r\nYou're right: sorry about that mistake.",
  "created_at":"2021-02-11T20:16:17Z",
  "id":777761403,
  "issue":727,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3Nzc2MTQwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T20:16:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Looks good. Thanks",
  "created_at":"2021-02-11T20:31:31Z",
  "id":777770017,
  "issue":727,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3Nzc3MDAxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T20:31:31Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributions please add @drahnreb for code and tests",
  "created_at":"2021-02-13T03:28:08Z",
  "id":778555110,
  "issue":727,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3ODU1NTExMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-13T03:28:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @drahnreb for code, tests",
  "created_at":"2021-02-13T03:29:30Z",
  "id":778555262,
  "issue":727,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3ODU1NTI2Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-02-13T03:29:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/738) to add @drahnreb! :tada:",
  "created_at":"2021-02-13T03:29:39Z",
  "id":778555291,
  "issue":727,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3ODU1NTI5MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-02-13T03:29:39Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"This is already effectively happening because slices are delayed.\r\n\r\n```python\r\n>>> form = ak.Array([1, 2, 3]).layout.form\r\n>>> mask = [False] * 30\r\n>>> mask[5] = True\r\n>>> mask[15] = True\r\n>>> mask[16] = True\r\n>>> mask[29] = True\r\n>>> materialized = []\r\n>>> def materialize(i):\r\n...     materialized.append(i)\r\n...     return ak.Array([1, 2, 3])\r\n... \r\n>>> array = ak.partitioned([ak.virtual(materialize, args=(i,), length=3, form=form) for i in range(10)])\r\n>>> materialized\r\n[]\r\n>>> tmp = array[mask] + 1\r\n>>> materialized\r\n[1, 5, 9]\r\n>>> tmp = array + 1\r\n>>> materialized\r\n[1, 5, 9, 0, 2, 3, 4, 6, 7, 8]\r\n```\r\n\r\nThe case I was seeing in the Argo dataset was not over-materialization. The cut on Julian day was a profile-level cut (array of lists-of-booleans) and therefore it had to touch each partition to get the lists. It was an error in the test, not the code, as usual.",
  "created_at":"2021-02-11T21:49:30Z",
  "id":777815061,
  "issue":728,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzgxNTA2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T21:49:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"What would you do instead if axis were available?",
  "created_at":"2021-02-11T17:51:43Z",
  "id":777675385,
  "issue":730,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzY3NTM4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T17:51:43Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, not this example. This isn't adding a jagged dimension to `structure`, it's transporting that to a completely new array. Sorry, I jumped to a conclusion. I'll provide the example I saw.",
  "created_at":"2021-02-11T17:56:33Z",
  "id":777678323,
  "issue":730,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzY3ODMyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T17:56:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"```python\r\noriginal = ak.Array([[1, 2, 3, 4], [], [5, 6, 7], [8, 9]])\r\ndesired = ak.Array([[[1, 2], [3, 4]], [], [[5], [6, 7]], [[8], [9]]])\r\n```\r\n\r\ngiven\r\n\r\n```python\r\ncounts = ak.Array([2, 2, 1, 2, 1, 1])\r\n```\r\n\r\nCurrently, you have to\r\n\r\n```python\r\nak.unflatten(ak.unflatten(ak.flatten(original), counts), [2, 0, 2, 2])\r\n```\r\n\r\nin which the `[2, 0, 2, 2]` is determined somehow from the `ak.num(original)` and the `counts`.\r\n\r\nWe'd like to be able to just\r\n\r\n```python\r\nak.unflatten(original, counts, axis=1)\r\n```\r\n",
  "created_at":"2021-02-11T18:03:26Z",
  "id":777682741,
  "issue":730,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzY4Mjc0MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-11T18:03:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(Note: the Uproot tests also pass. This won't be a surprise when it gets released.)",
  "created_at":"2021-02-12T22:03:29Z",
  "id":778479673,
  "issue":737,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3ODQ3OTY3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-12T22:03:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You found a bug. In general, we use only two exception types in Awkward Array: ValueErrors for user errors (i.e. you have to provide different input) and RuntimeErrors for internal errors. (There aren't many equivalences between C++ exceptions and Python exceptions, but there were these two, and so we choose to use them to make this distinction. The most important thing for you to know is whether it's your error or ours.) So, basically, if it ever says RuntimeError, please report it! (Maybe I should go through the code and add some text like that every single instance of RuntimeError. unfortunately, though, that's a practice that would have to be maintained manually.)\r\n\r\nFor this one in particular, I don't recognize the message, it could be coming from pybind11, rather than our own code. Nevertheless, we should support it because NumPy scalars can look just like ordinary numbers, and raising an error like this can be confusing.",
  "created_at":"2021-02-13T18:04:23Z",
  "id":778654935,
  "issue":740,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3ODY1NDkzNQ==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2021-02-13T18:04:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Median is different from mean, variance, covariances, and even linear fits in that it can't be computed from a small (non-growing) set of numbers. That's what everything in `ak.operations.reducers.*` have in common. However, this property is only really necessary for parallelizing a calculation, and that's not what you get when you call `ak.mean`, `ak.var`, etc., so maybe median _does_ belong in that set.\r\n\r\nIf you're computing a median at `axis=0`, i.e. you have a one-dimensional array and you want to get a scalar result for the median, just pass it into the NumPy function, [np.median](https://numpy.org/doc/stable/reference/generated/numpy.median.html), possibly wrapping the Awkward Array with [np.asarray](https://numpy.org/doc/stable/reference/generated/numpy.asarray.html) so that NumPy recognizes it as an array. (We're going to find a way to fix that: #630.)\r\n\r\nFor computing a median at `axis=1`, I've worked out how you can do it below. After going through it, I can totally sympathize that you had trouble with it! This case is tricky for a number of reasons, and so \"median\" should probably become a library function.\r\n\r\nFirst, start with a small, synthetic example of what we want to solve. Here's an array with variable-length lists at `axis=1`, some of which are empty (so that the solution has to deal with that).\r\n\r\n```python\r\narray = ak.Array([[2.2, 1.1, 0.0], [], [3.3, 4.4], [5.5], [8.8, 6.6, 7.7, 9.9]])\r\n```\r\n\r\nYou're right that the first thing we want to do is sort within each list (i.e. in `axis=1`).\r\n\r\n```python\r\n>>> sorted_array = ak.sort(array, axis=1)\r\n>>> sorted_array.tolist()\r\n[[0.0, 1.1, 2.2], [], [3.3, 4.4], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n```\r\n\r\nWe'll see in a moment that the empty list is going to be a problem. We'll want to find the two indexes that pick out the element or two elements that are closest to the center of each list, but the empty list has no indexes that can pick out an element. So we'll keep on hand a version of the `sorted_array` that doesn't have any empty lists, replacing them with `None` as a placeholder so that it doesn't further complicate the indexing (that is, every list stays _where_ it is, unaffected by the removal of the empties). The [ak.mask](https://awkward-array.readthedocs.io/en/latest/_auto/ak.mask.html) function does this, but it has a shorthand syntax through `.mask[\u00b7]`.\r\n\r\n```python\r\n>>> sorted_without_empties = sorted_array.mask[ak.num(sorted_array) != 0]\r\n>>> sorted_without_empties.tolist()\r\n[[0.0, 1.1, 2.2], None, [3.3, 4.4], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n```\r\n\r\nNow: on with the indexing. To compute a median, we want to average the element at `floor((N - 1) / 2)` and `ceil((N - 1) / 2)`, where `N` is the length of each list. If `N` is even, these are the two elements that are equally close to the middle; if `N` is odd, these are both the same element, exactly in the middle. (Averaging a single number with itself returns itself, so we don't need to introduce an \"if\" statement, or [ak.where](https://awkward-array.readthedocs.io/en/latest/_auto/ak.where.html) to handle both even and odd.)\r\n\r\nWe can do that, starting with [ak.num](https://awkward-array.readthedocs.io/en/latest/_auto/ak.num.html):\r\n\r\n```python\r\n>>> ak.num(sorted_array, axis=1)\r\n<Array [3, 0, 2, 1, 4] type='5 * int64'>\r\n>>> np.floor((ak.num(sorted_array, axis=1) - 1) / 2)\r\n<Array [1, -1, 0, 0, 1] type='5 * float64'>\r\n>>> np.ceil((ak.num(sorted_array, axis=1) - 1) / 2)\r\n<Array [1, -0, 1, 0, 2] type='5 * float64'>\r\n```\r\n\r\n(and you can already see how the indexes for the empty list, `-1` and `-0`, are nonsensical). Although these values are, by construction, integers, they have floating point type and have to be converted to true integers. There's [ak.values_astype](https://awkward-array.readthedocs.io/en/latest/_auto/ak.values_astype.html) for that.\r\n\r\n```python\r\n>>> low_index = ak.values_astype(np.floor((ak.num(sorted_array, axis=1) - 1) / 2), np.int64)\r\n>>> high_index = ak.values_astype(np.ceil((ak.num(sorted_array, axis=1) - 1) / 2), np.int64)\r\n>>> low_index, high_index\r\n(<Array [1, -1, 0, 0, 1] type='5 * int64'>,\r\n <Array [1, 0, 1, 0, 2] type='5 * int64'>)\r\n```\r\n\r\nNow we're _still_ not there because these indexes should apply to the second (jagged) dimension, but here they are in the first dimension, as flat arrays. If you try to just slice with this, it will be trying to find the _list_ at index `1`, then the _list_ at index `-1`, etc.\r\n\r\n```python\r\n>>> sorted_without_empties[low_index].tolist()\r\n[None, [6.6, 7.7, 8.8, 9.9], [0.0, 1.1, 2.2], [0.0, 1.1, 2.2], None]\r\n```\r\n\r\nThere's no error there, but it's not at all what we want. It seems like you ought to be able to do a slice like `[:, low_index]`, but that means requesting indexes `1`, `-1`, `0`, `0`, `1` from _every_ list.\r\n\r\n```python\r\n>>> sorted_without_empties[:, low_index]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/miniconda3/lib/python3.8/site-packages/awkward/highlevel.py\", line 1007, in __getitem__\r\n    return ak._util.wrap(self._layout[where], self._behavior)\r\nValueError: in ListArray64 attempting to get 1, index out of range\r\n```\r\n\r\nThat's also not what we want to do (even if it were possible): we want to get `1` from the _first_ list, skip `-1` (because that's `None`), `0` from the _third_ list, etc. For that, we need the `low_index` and `high_index` to be jagged arrays of integers with the same lengths at all dimensions but the innermost one. It has to be shaped like `[[1], [-1], [0], [0], [1]]`. NumPy's `np.newaxis` does that:\r\n\r\n```python\r\n>>> low_index[:, np.newaxis]\r\n<Array [[1], [-1], [0], [0], [1]] type='5 * 1 * int64'>\r\n```\r\n\r\nthough it's going to be relevant that this inner dimension has fixed-length 1, rather than a variable length that happens to be 1. Arrays with fixed-length dimensions have to reproduce what NumPy does, which often isn't what we want, so some behaviors hinge on whether a dimension is fixed-length or variable length. For instance, NumPy says that fixed-length dimensions should be broadcasted to the length of the other dimension, which is useful in some cases:\r\n\r\n```python\r\n>>> sorted_without_empties[low_index[:, np.newaxis]].tolist()\r\n[[None], [[6.6, 7.7, 8.8, 9.9]], [[0.0, 1.1, 2.2]], [[0.0, 1.1, 2.2]], [None]]\r\n```\r\n\r\nbut not ours. Since the behaviors depend on fixed-length versus variable-length dimensions, there are functions for converting between them: [ak.to_regular](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_regular.html) and [ak.from_regular](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_regular.html).\r\n\r\n```python\r\n>>> ak.from_regular(low_index[:, np.newaxis])\r\n<Array [[1], [-1], [0], [0], [1]] type='5 * var * int64'>\r\n```\r\n\r\nThis one does exactly what we want.\r\n\r\n```python\r\n>>> sorted_without_empties[ak.from_regular(low_index[:, np.newaxis])].tolist()\r\n[[1.1], None, [3.3], [5.5], [7.7]]\r\n```\r\n\r\nNow we can get the low and high values in each list.\r\n\r\n```python\r\n>>> low = sorted_without_empties[ak.from_regular(low_index[:, np.newaxis])]\r\n>>> high = sorted_without_empties[ak.from_regular(high_index[:, np.newaxis])]\r\n>>> low, high\r\n(<Array [[1.1], None, [3.3], [5.5], [7.7]] type='5 * option[var * float64]'>,\r\n <Array [[1.1], None, [4.4], [5.5], [8.8]] type='5 * option[var * float64]'>)\r\n```\r\n\r\nThe median is the average of these two.\r\n\r\n```python\r\n>>> (low + high) / 2\r\n<Array [[1.1], None, [3.85], [5.5], [8.25]] type='5 * option[var * float64]'>\r\n```\r\n\r\nOr, actually, it's the first element of each of these.\r\n\r\n```python\r\n>>> ((low + high) / 2)[:, 0]\r\n<Array [1.1, None, 3.85, 5.5, 8.25] type='5 * ?float64'>\r\n```\r\n\r\nWhew! Given how much work this was, it should be wrapped up as a library function, though it would need to be generalized to apply to any `axis` (by making slices with `slice(None) * ndim + (np.newaxis,)`, rather than `:, np.newaxis`, etc.). All of the other functions in `ak.operations.reducers.*` can handle weights. I'm not really sure what to do with weights in a median...",
  "created_at":"2021-02-15T16:11:43Z",
  "id":779320619,
  "issue":741,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTMyMDYxOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2021-02-15T16:11:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the complete explanation of how to do the indexing properly!",
  "created_at":"2021-02-16T08:05:46Z",
  "id":779660406,
  "issue":741,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTY2MDQwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-16T08:05:46Z",
  "user":"MDQ6VXNlcjMzMzc4MA=="
 },
 {
  "author_association":"MEMBER",
  "body":"When unflattening at `axis>0` or in a PartitionedArray (two recent new features), it's a free choice where to put the zeros: inside the larger structure on the left or on the right. By choosing to put them on the left, I missed this case where you're unflattening at `axis=0` without any partitions: then it's not sensible to put them on the left because there's nothing on the right to take them. (That would also apply to the last list in `axis>0` or the last partition in a PartitionedArray.) So this choice isn't so arbitrary after all: they need to go on the left for the sensible thing to happen in a case like the above. Thanks for pointing this out.",
  "created_at":"2021-02-15T19:36:30Z",
  "id":779414855,
  "issue":742,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTQxNDg1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-15T19:36:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is #701, and I agree that we need a function to trim the unreachable elements from an array. When manipulating these in memory, we want to keep them to avoid duplicating data, but writing them to disk is a copy anyway and at that point, it's time to trim them down. Such an operation should be automatically applied in pickling, for instance, and it should be discussed in the tutorial you're referencing.\r\n\r\nThis was also an issue in Awkward 0: scikit-hep/awkward-0.x#246. As indicated in #701, the `ak.packed` function would be a good addition. You found the same `ak.from_arrow(ak.to_arrow(arr))`` workaround we discussed there, which does some wasteful computations if there are option-types around (not so much otherwise, but it's still crude).",
  "created_at":"2021-02-17T16:40:50Z",
  "id":780687476,
  "issue":746,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDY4NzQ3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T16:40:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The argument was completely ignored. I must have intended this, but forgot. PR #748 fixes that.",
  "created_at":"2021-02-17T17:48:19Z",
  "id":780732329,
  "issue":747,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDczMjMyOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-17T17:48:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"be aware that a better name for `xarray.Dataset.to_array` would probably be `to_dataarray`: it tries to combine all data variables into a single `DataArray`. `Dataset` itself doesn't have something like `to_numpy` (you would have to use `to_array` first), but for `DataArray` the `values` property can be used. I think we're open to adding something like `to_numpy`, though, see pydata/xarray#3245.",
  "created_at":"2021-02-17T23:20:59Z",
  "id":780918994,
  "issue":749,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDkxODk5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T23:20:59Z",
  "user":"MDQ6VXNlcjE0ODA4Mzg5"
 },
 {
  "author_association":"MEMBER",
  "body":"Just to let you know, GPU support won't be ready for some months, and then only on Linux for Nvidia GPUs. You might be able to\r\n\r\n```bash\r\npip install awkward[cuda]\r\n```\r\n\r\nand do some simple things like\r\n\r\n```python\r\n>>> one = ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]], kernels=\"cuda\")\r\n>>> two = ak.Array([100, 200, 300], kernels=\"cuda\")\r\n>>> one, two\r\n(<Array:cuda [[1.1, 2.2, 3.3], ... [4.4, 5.5]] type='3 * var * float64'>,\r\n <Array:cuda [100, 200, 300] type='3 * int64'>)\r\n>>> three = one + two\r\n>>> three\r\n<Array:cuda [[101, 102, 103], ... [304, 306]] type='3 * var * float64'>\r\n>>> ak.flatten(three)\r\n<Array:cuda [101, 102, 103, 304, 306] type='5 * float64'>\r\n>>> ak.to_cupy(ak.flatten(three))\r\narray([101.1, 102.2, 103.3, 304.4, 305.5])\r\n>>> type(ak.to_cupy(ak.flatten(three)))\r\n<class 'cupy.core.core.ndarray'>\r\n```\r\n\r\nbut a lot hasn't been implemented and it's a minefield for segmentation faults. (Throughout the codebase, there's a `// DERIVE` comment indicating all the places we need to determine whether an array is in main memory or on the GPU, instead of just _assuming_ it's in main memory. At least we'll be able to do that systematically, but it hasn't been done yet.)\r\n\r\nI just tried [ak.unflatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.unflatten.html) and put in a minor tweak to make it turn CuPy arrays into jagged arrays (#754), but quickly ran into some segmentation faults when trying to do some math on it. So, just to let you know: it's not ready yet.",
  "created_at":"2021-02-17T22:53:23Z",
  "id":780907831,
  "issue":753,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDkwNzgzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T22:53:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"It's probably a lot of work but indeed would be a pretty useful to be able to accelerate awkward array computations on GPUs !",
  "created_at":"2021-09-30T22:01:04Z",
  "id":931735082,
  "issue":753,
  "node_id":"IC_kwDODBCWws43iSYq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-30T22:01:04Z",
  "user":"MDQ6VXNlcjM1MjI1Njc3"
 },
 {
  "author_association":"MEMBER",
  "body":"PR #757 fixes `ak.num` with `axis=0` on PartitionedArrays (that was how lazy arrays are relevant, so it is an Awkward thing).\r\n\r\nAs for this case:\r\n\r\n> Note that you can apply `ak.num()` to a TTree in other ways, albeit with inconsistent results:\r\n> \r\n> ```python\r\n> >>> ak.num(uproot.open('file.root')['tree'], axis=0)\r\n> 5\r\n> ```\r\n> \r\n> (there are five branches)\r\n\r\nA TTree is not an array. It is iterable, but that's for finding its branches, not its data as an array.\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> tree = uproot.open(skhep_testdata.data_path(\"uproot-Zmumu.root\") + \":events\")\r\n>>> list(tree)\r\n[<TBranch 'Type' at 0x7f536ff9b2b0>,\r\n <TBranch 'Run' at 0x7f536ff9bb50>,\r\n <TBranch 'Event' at 0x7f536ff30190>,\r\n <TBranch 'E1' at 0x7f536ff308b0>,\r\n <TBranch 'px1' at 0x7f536ff30fd0>,\r\n <TBranch 'py1' at 0x7f536ff35730>,\r\n <TBranch 'pz1' at 0x7f536ff35e50>,\r\n <TBranch 'pt1' at 0x7f536ff375b0>,\r\n <TBranch 'eta1' at 0x7f536ff37cd0>,\r\n <TBranch 'phi1' at 0x7f536ff3c430>,\r\n <TBranch 'Q1' at 0x7f536ff3cb50>,\r\n <TBranch 'E2' at 0x7f536ff3f2b0>,\r\n <TBranch 'px2' at 0x7f536ff3f9d0>,\r\n <TBranch 'py2' at 0x7f536ff43130>,\r\n <TBranch 'pz2' at 0x7f536ff43850>,\r\n <TBranch 'pt2' at 0x7f536ff43f70>,\r\n <TBranch 'eta2' at 0x7f536ff466d0>,\r\n <TBranch 'phi2' at 0x7f536ff46df0>,\r\n <TBranch 'Q2' at 0x7f536ff49550>,\r\n <TBranch 'M' at 0x7f536ff49c70>]\r\n```\r\n\r\nAll of the Awkward functions fall back on iterating over their inputs, which in this case, has 20 items.",
  "created_at":"2021-02-18T20:25:51Z",
  "id":781613689,
  "issue":756,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTYxMzY4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T20:25:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"So if they're _all_ scalars, we could remember that fact, turn them into arrays, run the whole process, and (based on the \"are they scalars?\" flag) turn the resulting length-1 RecordArrays into Records?\r\n\r\nWhat if some are scalars and some are not?",
  "created_at":"2021-02-19T22:11:03Z",
  "id":782397295,
  "issue":758,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MjM5NzI5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-19T22:11:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"A motivating example is that, though mixin functions like:\r\n```python\r\n    def r2(self):\r\n        return self.x ** 2 + self.y ** 2\r\n```\r\ncan accept Record or Array objects, functions like\r\n```python\r\n    @awkward.mixin_class_method(numpy.add, {\"TwoVector\"})\r\n    def add(self, other):\r\n        return awkward.zip(\r\n            {\"x\": self.x + other.x, \"y\": self.y + other.y},\r\n            with_name=\"TwoVector\",\r\n        )\r\n```\r\ncannot but would be with this change. Such a Record construction would be valid only if all arguments are scalars. If one argument is an array, in principle the arguments may be broadcast-compatible.\r\n\r\nI would suggest the zip operation checks first if all elements are scalars or ak.Record types, and if so make a new ak.Record. Else, attempt to broadcast the arguments and do the array construction, else fail.",
  "created_at":"2021-02-19T22:13:58Z",
  "id":782400012,
  "issue":758,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MjQwMDAxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-19T22:13:58Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, the flag can be set to `True` if they are _all_ scalars, but in any case, scalars are turned into length-1 arrays. Then if there's a mixture, the non-scalars would naturally broadcast the length-1 arrays if possible. That would handle your three cases.",
  "created_at":"2021-02-19T22:17:09Z",
  "id":782402243,
  "issue":758,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MjQwMjI0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-19T22:17:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #839 implements\r\n\r\n```python\r\nassert isinstance(ak.zip({\"x\": 1, \"y\": 0, \"z\": 0}), ak.Record)\r\n```\r\n\r\nIf any element is not a scalar, the result will be `ak.Array` (and the elements that _are_ scalars will be singletons, which would get broadcasted).",
  "created_at":"2021-04-16T20:52:27Z",
  "id":821557427,
  "issue":758,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTU1NzQyNw==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-04-16T20:52:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"For the last error, try the \"low level interface\" described on the Uproot 3 page: https://github.com/scikit-hep/uproot3#writing-ttrees\r\n\r\nI was talking to someone on Gitter and they ran into a problem with `extend` that wasn't an issue with `newbasket`. Naturally, that's not a good situation, but this will get rewritten into Uproot 4 soon.\r\n\r\n---------------\r\n\r\nAs for the first error, if you could make a small `events` and pickle it, then I'd have something to try. Slicing doesn't make the pickled array any smaller (that's another to-do item: #701), and slicing might affect the bug, anyway. Doing it with a small number of ROOT files and a small number of fields could make it manageable. If it's too big to post here or can't be posted publicly, we may be able to arrange something by email.",
  "created_at":"2021-02-20T00:47:11Z",
  "id":782482508,
  "issue":759,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MjQ4MjUwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-20T00:47:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Well, this might actually be an issue with what gets returned from uproot. I have uploaded a pickle file. The actual field names are not `field1` and `field2`, but rather `file_number` and `num_electrons`.\r\n\r\nThis code confirms that pickling the object as returned from `uproot`, reloading it, and trying to save to parquet reproduces the error, so you should be able to duplicate the error with the attached pickle file.\r\n\r\n```\r\nwith open('events.pickle', 'wb') as f:\r\n    pickle.dump(events, f, pickle.HIGHEST_PROTOCOL)\r\nwith open('events.pickle', 'rb') as f:\r\n    events_3 = pickle.load(f)\r\n    \r\nprint(ak.all(events_3.file_number == events.file_number)) # Prints True\r\nprint(ak.all(events_3.num_electrons == events.num_electrons)) # Prints True\r\n\r\nak.to_parquet(events_3, \"test_1.parquet\") # Raises ValueError: Arrays were not all the same length: 614 vs 615\r\n```\r\n\r\nHowever, in attempting to make the file such that the fields were in fact called `field1` and `field2`, things seem to work.\r\n\r\n```\r\n# create a new object from the original as returned by uproot\r\nevents_1 = ak.zip({\"field1\": events.file_number, \r\n                   \"field2\": events.num_electrons})\r\nprint(ak.all(events_1.field1 == events.file_number)) # Prints True\r\nprint(ak.all(events_1.field2 == events.num_electrons)) # Prints True\r\nak.to_parquet(events_1, \"test.parquet\") # Succeeds\r\n\r\nevents_2 = ak.from_parquet(\"test.parquet\") # Succeeds\r\nprint(ak.all(events_2.field1 == events.file_number)) # Prints True\r\nprint(ak.all(events_2.field2 == events.num_electrons)) # Prints True\r\n```\r\n\r\nSo it looks less like an issue with the structure itself, and more to do with how it's getting returned from uproot? \r\n\r\n[events.txt](https://github.com/scikit-hep/awkward-1.0/files/6013919/events.txt)\r\n",
  "created_at":"2021-02-20T01:47:20Z",
  "id":782518846,
  "issue":759,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MjUxODg0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-20T01:47:20Z",
  "user":"MDQ6VXNlcjg1OTM0ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"As long as `events_3` is valid (check `ak.validity_error(events_3)`), it's not Uproot's fault.\r\n\r\nEven though `events_1` and `events_2` have all the same elements as `events_3`, they can differ in `layout`, and it's probably a specific layout that isn't satisfying some rule for Arrow (Arrow is more strict than Awkward Array). That's why I needed the pickle file, to have exactly the layout that fails.\r\n\r\nI'll take a look; thanks! (It might be Monday, though.)",
  "created_at":"2021-02-20T05:02:14Z",
  "id":782561212,
  "issue":759,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MjU2MTIxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-20T05:02:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #760 fixes it.\r\n\r\nThis is a fun case in which the bug was fixed purely by _removing_ code: [see diff](https://github.com/scikit-hep/awkward-1.0/pull/760/files).\r\n\r\n(At some point, I must have forgotten that the `recurse` function inside of `ak.to_arrow` already converts ListArrays like yours into ListOffsetArrays, which is a necessary step to making Arrow lists. The unnecessary second code path has a bug in it; the original one was right. Removing the extra code path makes the original take over, and it does it right.)\r\n\r\n(The difference in list lengths, 614 vs 615, was hard to see because the missing element was the zero at the beginning of the ListOffsetArray offsets. Therefore, they both ended with 615, which is what draws your eye's attention.)",
  "created_at":"2021-02-20T15:08:06Z",
  "id":782695605,
  "issue":759,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MjY5NTYwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-20T15:08:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is just one tutorial, but anyway that's progress.\r\n\r\nIn addition, it has some bug-fixes that I want to get into main.",
  "created_at":"2021-02-22T15:58:17Z",
  "id":783476843,
  "issue":762,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MzQ3Njg0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-22T15:58:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks; that was just an oversight in the definition of `EmptyArray::argsort_next`.\r\n\r\nThe second example probably should be raising an error because the type is wrong\u2014it certainly does when the array is not empty. It's complicated by the fact that `EmptyArray` declares itself to be an array of integers when used in a slice, but an array of floats when used in other contexts (except `ak.argsort`, after PR #764 goes through), though I think your example has an empty NumpyArray, rather than an EmptyArray. As for that case, I think it would be better for it to be picky about the type, even if the array is empty, but it's pretty low priority.",
  "created_at":"2021-02-22T18:46:36Z",
  "id":783590140,
  "issue":763,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MzU5MDE0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-22T18:46:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna I went through a few versions of this as you can see in the above commits. Now that we know the type in advance, `begin_record` and `end_record` are superfluous because we know we need to march from field x to field y, to field x to field y, etc.:\r\n\r\n```python\r\n# typedbuilder.begin_record()\r\ntypedbuilder.int64(1)\r\ntypedbuilder.float64(1.1)\r\n# typedbuilder.end_record()\r\n\r\n# typedbuilder.begin_record()\r\ntypedbuilder.int64(2)\r\ntypedbuilder.float64(2.2)\r\n# typedbuilder.end_record()\r\n\r\n# typedbuilder.begin_record()\r\ntypedbuilder.int64(3)\r\ntypedbuilder.float64(3.3)\r\n# typedbuilder.end_record()\r\n```\r\n\r\nAlso, `begin_list` is superfluous because when we reach a state of filling a list, we either insert a list item or end the list.\r\n\r\nOh! If we have a list of lists of lists, `begin_list` is necessary because we won't otherwise know which nested list we're closing! Let me go fix that.",
  "created_at":"2021-02-23T23:00:54Z",
  "id":784574519,
  "issue":767,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4NDU3NDUxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-23T23:00:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, one last time: knowing types in advance makes all of the record-related commands irrelevant: `begin_record`, `end_record`, `field` (for picking out a field, we can't do them in arbitrary order easily in Forth, and removing the _capability_ also removes the _requirement_, with its associated speed-up), and `index` (same as `field`, but for tuples).\r\n\r\nOn the other hand, `begin_list` and `end_list` are required because we could have lists directly within lists (which didn't happen in my example), and we need to be able to distinguish `[[[]]]` from `[[]]`.\r\n\r\nThe latest version of the test has a justification for each `pause`, which will help to determine where to put them when generating the Forth code.\r\n\r\nUnion-types will take some thought. Option-types should be straightforward to add.\r\n\r\nI think I can adapt your text into the paper now.",
  "created_at":"2021-02-23T23:22:34Z",
  "id":784588202,
  "issue":767,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4NDU4ODIwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-23T23:22:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a working example that I used to motivate what I wrote in the paper:\r\n\r\n```python\r\nimport awkward.forth\r\nimport awkward as ak\r\nimport numpy as np\r\n\r\nform = ak.forms.Form.fromjson(\"\"\"\r\n{\r\n    \"class\": \"ListOffsetArray32\",\r\n    \"offsets\": \"i32\",\r\n    \"content\": {\r\n        \"class\": \"ListOffsetArray32\",\r\n        \"offsets\": \"i32\",\r\n        \"content\": {\r\n            \"class\": \"ListOffsetArray32\",\r\n            \"offsets\": \"i32\",\r\n            \"content\": {\r\n                \"class\": \"NumpyArray\",\r\n                \"primitive\": \"float32\",\r\n                \"form_key\": \"node3\"\r\n            },\r\n            \"form_key\": \"node2\"\r\n        },\r\n        \"form_key\": \"node1\"\r\n    },\r\n    \"form_key\": \"node0\"\r\n}\r\n\"\"\")\r\n\r\nclass TypedArrayBuilder:\r\n    def __init__(self, form):\r\n        self.form = form\r\n\r\n        def list_node(name, offsets, content_name):\r\n            return \"\"\"\r\n: {name}\r\n  {begin_list} <> if halt then\r\n  0\r\n  begin\r\n    pause\r\n    dup {end_list} = if\r\n      drop\r\n      {offsets} +<- stack\r\n      exit\r\n    else\r\n      {content_name}\r\n      1+\r\n    then\r\n  again\r\n;\r\n\"\"\".format(int64=0, float64=1, begin_list=2, end_list=3, **locals())\r\n\r\n        self.vm = awkward.forth.ForthMachine32(\"\"\"\r\ninput data\r\noutput offsets0 int32\r\noutput offsets1 int32\r\noutput offsets2 int32\r\noutput content float32\r\n\r\n: node3\r\n  {float64} = if\r\n    0 data seek\r\n    data d-> content\r\n  else\r\n    halt\r\n  then\r\n;\r\n\r\n{node2} {node1} {node0}\r\n\r\n0 offsets0 <- stack\r\n0 offsets1 <- stack\r\n0 offsets2 <- stack\r\n\r\n0\r\nbegin\r\n  pause\r\n  node0\r\nagain\r\n\"\"\".format(int64=0, float64=1, begin_list=2, end_list=3,\r\n           node0=list_node(\"node0\", \"offsets0\", \"node1\"),\r\n           node1=list_node(\"node1\", \"offsets1\", \"node2\"),\r\n           node2=list_node(\"node2\", \"offsets2\", \"node3\")))\r\n\r\n        self.data = np.empty(8, np.uint8)\r\n        self.vm.run({\"data\": self.data})\r\n\r\n    def int64(self, x):\r\n        self.data.view(np.int64)[0] = x\r\n        self.vm.stack_push(0)\r\n        self.vm.resume()\r\n\r\n    def float64(self, x):\r\n        self.data.view(np.float64)[0] = x\r\n        self.vm.stack_push(1)\r\n        self.vm.resume()\r\n\r\n    def begin_list(self):\r\n        self.vm.stack_push(2)\r\n        self.vm.resume()\r\n\r\n    def end_list(self):\r\n        self.vm.stack_push(3)\r\n        self.vm.resume()\r\n\r\n    def snapshot(self):\r\n        buffers = {\r\n            \"part0-node0-offsets\": self.vm.outputs[\"offsets0\"],\r\n            \"part0-node1-offsets\": self.vm.outputs[\"offsets1\"],\r\n            \"part0-node2-offsets\": self.vm.outputs[\"offsets2\"],\r\n            \"part0-node3-data\": self.vm.outputs[\"content\"],\r\n        }\r\n        return ak.from_buffers(self.form, self.vm.stack[0], buffers)\r\n\r\n    def debug_step(self):\r\n        print(\"stack: \", builder.vm.stack)\r\n        for k, v in builder.vm.outputs.items():\r\n            print(k + \":\", np.asarray(v))\r\n        print(\"array:\", self.snapshot())\r\n        print()\r\n\r\nbuilder = TypedArrayBuilder(form)\r\nbuilder.debug_step()\r\n\r\nbuilder.begin_list()\r\nbuilder.end_list()\r\nbuilder.debug_step()\r\n\r\nbuilder.begin_list()\r\nbuilder.begin_list()\r\nbuilder.end_list()\r\nbuilder.end_list()\r\nbuilder.debug_step()\r\n\r\nbuilder.begin_list()\r\nbuilder.begin_list()\r\nbuilder.begin_list()\r\nbuilder.end_list()\r\nbuilder.end_list()\r\nbuilder.end_list()\r\nbuilder.debug_step()\r\n\r\nbuilder.begin_list()\r\nbuilder.begin_list()\r\nbuilder.begin_list()\r\nbuilder.float64(1.1)\r\nbuilder.end_list()\r\nbuilder.end_list()\r\nbuilder.end_list()\r\nbuilder.debug_step()\r\n\r\nbuilder.begin_list()\r\nbuilder.begin_list()\r\nbuilder.begin_list()\r\nbuilder.float64(1.1)\r\nbuilder.float64(2.2)\r\nbuilder.float64(3.3)\r\nbuilder.end_list()\r\nbuilder.end_list()\r\nbuilder.end_list()\r\nbuilder.debug_step()\r\n```",
  "created_at":"2021-02-24T01:34:25Z",
  "id":784667797,
  "issue":767,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4NDY2Nzc5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-24T01:34:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi, @trickarcher! Are you going to be using this PR for this week's work or are you starting a new one? I took a look at this and it's not the right lead (what we discussed in the meeting). If you're working on a new one, I'll just close this (or you can). Thanks!",
  "created_at":"2021-03-02T13:35:52Z",
  "id":788913566,
  "issue":768,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4ODkxMzU2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-02T13:35:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I'll close this. Even if I had to use this PR, I'll have to change the title. I'll start this week's work in a new PR.",
  "created_at":"2021-03-02T13:41:33Z",
  "id":788917155,
  "issue":768,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4ODkxNzE1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-02T13:41:33Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"`TypedArrayBuilder` comparison with `ArrayBuilder` on records:\r\n```python\r\nIn [1]: import awkward as ak                                                                                                                           \r\n\r\nIn [2]: form = ak.forms.RecordForm( \r\n   ...: {\"one\": ak.forms.NumpyForm([], 8, \"d\"), \"two\": ak.forms.NumpyForm([], 8, \"d\")} \r\n   ...: )                                                                                                                                              \r\n\r\nIn [3]: builder = ak.layout.TypedArrayBuilder(form)                                                                                                    \r\n\r\nIn [4]: %%timeit -n100000 -r1 \r\n   ...: builder.float64(1.1) \r\n   ...: builder.float64(2.2) \r\n   ...:  \r\n   ...:                                                                                                                                                \r\n6.4 \u00b5s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 100000 loops each)\r\n\r\nIn [5]: array_builder = ak.ArrayBuilder()                                                                                                              \r\n\r\nIn [6]: %%timeit -n100000 -r1 \r\n   ...: with array_builder.record(): \r\n   ...:     array_builder.field(\"one\").real(1.1) \r\n   ...:     array_builder.field(\"two\").real(2.2) \r\n   ...:                                                                                                                                                \r\n15.1 \u00b5s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 100000 loops each)\r\n\r\nIn [7]: array = array_builder.snapshot()                                                                                                               \r\n\r\nIn [8]: array2 = builder.snapshot()                                                                                                                    \r\n\r\nIn [9]: array                                                                                                                                          \r\nOut[9]: <Array [{one: 1.1, two: 2.2}, ... two: 2.2}] type='100000 * {\"one\": float64, \"tw...'>\r\n\r\nIn [10]: ak.Array(array2)                                                                                                                              \r\nOut[10]: <Array [{one: 1.1, two: 2.2}, ... two: 2.2}] type='100000 * {\"one\": float64, \"tw...'>\r\n\r\n\r\n```",
  "created_at":"2021-03-29T20:16:19Z",
  "id":809682234,
  "issue":769,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwOTY4MjIzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-29T20:16:19Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"More than 2\u00d7 faster!",
  "created_at":"2021-03-29T20:28:03Z",
  "id":809689524,
  "issue":769,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwOTY4OTUyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-29T20:28:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> More than 2\u00d7 faster!\r\n\r\njust :-)",
  "created_at":"2021-03-29T20:53:51Z",
  "id":809706546,
  "issue":769,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwOTcwNjU0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-29T20:53:51Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This looks very similar to #815 and still seems to be broken with `1.2.2-1-g9ef4b7bc`",
  "created_at":"2021-04-13T02:02:07Z",
  "id":818375938,
  "issue":770,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODM3NTkzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T02:02:07Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Where we end up with a `Union`, perhaps we should propagate the `__record__` iff. the contents of the union share the same `__record__`?",
  "created_at":"2021-06-15T21:44:30Z",
  "id":861854038,
  "issue":770,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg1NDAzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T21:44:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Maybe the new rule could be,\r\n\r\n   1. if the UnionArray has a parameter with a given name, `purelist_parameter(name)` should return its value;\r\n   2. if the UnionArray does not have this parameter, but every one of its contents has that parameter with the same value, then `purelist_parameter(none)` should return that common value.\r\n\r\nNone of the error cases that raised this issue are still errors, though. In all of those cases, it was wrong for the UnionArray to have existed: a `simplifiy_uniontype` call was missing. The above rule would make UnionArrays transparent to common parameters, but maybe that's bad because then we'd be less likely to catch missing `simplifiy_uniontype` calls, since that's a common symptom.\r\n\r\nAre there any legitimate cases in which you'd have a UnionArray with all contents having the same parameter? I can think of one corner-case: you have an array of records that are all Lorentz vectors (`\"__record__\": \"Momentum4D\"`), but some of them have auxiliary data, like `\"charge\"` because they're electrons, while others don't. Then you'd want the record name to be visible above the UnionArray.",
  "created_at":"2021-06-15T22:06:49Z",
  "id":861867937,
  "issue":770,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg2NzkzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T22:06:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This bug was found by someone that wanted to concatenate electrons and muons into leptons, which share 4-vector and charge attributes, but may have other differing attributes. It would be nice to still do vector math with the leptons collection.",
  "created_at":"2021-06-16T13:50:11Z",
  "id":862397763,
  "issue":770,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MjM5Nzc2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T13:50:11Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"If the electrons and muons have different `__record__` values, then this wouldn't fix it because the UnionArray wouldn't be able to report a single answer for `purelist_parameter(\"__record__\")`. There are a few ways that could be addressed:\r\n\r\n   * Set the `__record__` of both the electrons and muons to `\"Momentum4D\"`, though they can have different attributes beyond the kinematic ones. Since they have different attributes, they wouldn't get merged\u2014they'd still be a UnionArray\u2014but the new rule described above would note that they're all labeled `\"Momentum4D\"` and report that for `purelist_parameter(\"__record__\")`. However, that means that they can't have `__record__` names like `\"Electron\"` or `\"Muon\"` because that spot is taken by `\"Momentum4D\"`. If you wanted to have another specialized method that applies only to muons or is a different function for electrons and muons, you'd be out of luck.\r\n   * Do the above and add yet another rule that interprets a _list_ of strings from `__record__` as a method resolution order. (Remember that parameters can be any JSON, not just strings.) Then the two union contents could be `[\"Electron\", \"Momentum4D\"]` and `[\"Muon\", \"Momentum4D\"]`, resolving particle-specific behaviors first, then Lorentz vector ones if not overridden. This would be a more flexible way than class inheritance way to do class inheritance. (Actually, more similar to Haskell's type classes.)\r\n   * Maybe this would also be needed for the above, but it could be done independently: when attempting to call a method on an array containing unions that have a method with that name defined on all contents of the union, call the method independently on each content and make a union of the result. This will be tricky if the union is not a top-level, if it's nested in some list. If the first two bullet points are not implemented but this one is, it would solve the problem: `\"Electron\"` and `\"Muon\"` both have a method named `pt`, so they're both called, a union is built from the result, and that union would likely simplify to a non-union array. If the first two bullet points are implemented along with this one, it would solve the problem in a slicker way: `\"Electron\"` and `\"Muon\"` don't have to independently implement `pt`, it can be one of the inherited methods from `\"Momentum4D\"`, while they maintain their independent type names.\r\n\r\nSo many possibilities, so little time. Maybe we should start collecting these ideas into PIP/NEP like documents.",
  "created_at":"2021-06-16T15:05:15Z",
  "id":862459089,
  "issue":770,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MjQ1OTA4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T15:05:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> If the electrons and muons have different __record__ values\r\n\r\nThey have the same record value: \"Candidate\" (4-vector plus charge)",
  "created_at":"2021-06-16T15:11:54Z",
  "id":862464466,
  "issue":770,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MjQ2NDQ2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T15:11:54Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Then only the new rule described in https://github.com/scikit-hep/awkward-1.0/issues/770#issuecomment-861867937 would be needed (which is good, because that's the easiest thing out of everything described above). I'll turn that comment into an issue.",
  "created_at":"2021-06-16T15:17:38Z",
  "id":862468850,
  "issue":770,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MjQ2ODg1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T15:17:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As it turns out, the policy described in https://github.com/scikit-hep/awkward-1.0/issues/770#issuecomment-861867937 _is_ the policy that UnionArray _already has_. That's why @nsmith-'s example `c` worked. His example `c2` didn't work because it was incorrectly implemented: all of the UnionArray's `contents` are supposed to have the same `purelist_parameter`, but it was implemented checking their direct `parameters` (for all but `contents[0]`).",
  "created_at":"2021-07-12T22:09:09Z",
  "id":878630514,
  "issue":770,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODYzMDUxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T22:09:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That's a good point. The RecordArray that this makes should allow different-lengthed contents (so that they can be filled non-atomically), users of this constructor are probably intending them to line up. The fact that Pandas raises an error and this constructor was introduced to emulate Pandas only clinches it.\r\n\r\nIt's interesting that the original test for the Pandas-style constructor had unequal length arrays and had to be modified by this PR.",
  "created_at":"2021-03-01T16:53:14Z",
  "id":788102746,
  "issue":771,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4ODEwMjc0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-01T16:53:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you!",
  "created_at":"2021-03-01T17:24:56Z",
  "id":788126214,
  "issue":771,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4ODEyNjIxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-01T17:24:56Z",
  "user":"MDQ6VXNlcjYwMjkyMjQ3"
 },
 {
  "author_association":"MEMBER",
  "body":"It doesn't look like the at-sign automatically links in GitHub: https://github.com/scikit-hep/awkward-1.0/blob/978233d898ed5ffe8801441731199b0641a60e93/README.md\r\n\r\nSince this is going through a whole test cycle anyway, perhaps you should explicitly link it to https://github.com/erezsh",
  "created_at":"2021-03-04T19:56:42Z",
  "id":790890309,
  "issue":773,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDg5MDMwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T19:56:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(If you don't catch this before the auto-merge, never mind. It's fine anyway.)",
  "created_at":"2021-03-04T19:57:22Z",
  "id":790890722,
  "issue":773,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDg5MDcyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T19:57:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Something peculiar is going on. I checked out raymondEhlers/pybind11-awkward-array-test, compiled it, and ran the tests, but they passed for me (on Linux):\r\n\r\n```\r\n% pytest -vv -rs tests\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.8, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /home/jpivarski/miniconda3/bin/python\r\ncachedir: .pytest_cache\r\nrootdir: /tmp/pybind11-awkward-array-test\r\nplugins: anyio-2.1.0\r\ncollected 2 items                                                              \r\n\r\ntests/test_simple.py::test_numpy_array PASSED                            [ 50%]\r\ntests/test_simple.py::test_awkward_array PASSED                          [100%]\r\n\r\n============================== 2 passed in 0.26s ===============================\r\nIn function for NumpyArray\r\nIn function\r\n% python\r\nPython 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import awkward\r\n>>> awkward.__version__\r\n'1.2.0rc2'\r\n```\r\n\r\nIt looks like your pybind11 submodule is pointing at the same commit as Awkward Array's (8de7772), but I wasn't able to check it out that way:\r\n\r\n```\r\n% git clone --recursive https://github.com/raymondEhlers/pybind11-awkward-array-test.git\r\nCloning into 'pybind11-awkward-array-test'...\r\nremote: Enumerating objects: 63, done.\r\nremote: Counting objects: 100% (63/63), done.\r\nremote: Compressing objects: 100% (32/32), done.\r\nremote: Total 63 (delta 26), reused 58 (delta 21), pack-reused 0\r\nUnpacking objects: 100% (63/63), 36.85 KiB | 539.00 KiB/s, done.\r\nSubmodule 'pybind11' (git@github.com:pybind/pybind11.git) registered for path 'pybind11'\r\nCloning into '/tmp/pybind11-awkward-array-test/pybind11'...\r\nWarning: Permanently added the RSA host key for IP address '140.82.112.4' to the list of known hosts.\r\ngit@github.com: Permission denied (publickey).\r\nfatal: Could not read from remote repository.\r\n\r\nPlease make sure you have the correct access rights\r\nand the repository exists.\r\nfatal: clone of 'git@github.com:pybind/pybind11.git' into submodule path '/tmp/pybind11-awkward-array-test/pybind11' failed\r\nFailed to clone 'pybind11'. Retry scheduled\r\nCloning into '/tmp/pybind11-awkward-array-test/pybind11'...\r\ngit@github.com: Permission denied (publickey).\r\nfatal: Could not read from remote repository.\r\n\r\nPlease make sure you have the correct access rights\r\nand the repository exists.\r\nfatal: clone of 'git@github.com:pybind/pybind11.git' into submodule path '/tmp/pybind11-awkward-array-test/pybind11' failed\r\nFailed to clone 'pybind11' a second time, aborting\r\n```\r\n\r\nSo I did the test by recursively copying mine, from the scikit-hep/awkward-1.0 directory. That might be a clue to why it succeeded for me.\r\n\r\nSince it might be related to the issue, I tried a recursive checkout of scikit-hep/awkward-1.0 and there were no problems.\r\n\r\nThanks for including the minimally reproducing repo!",
  "created_at":"2021-03-02T20:05:06Z",
  "id":789178141,
  "issue":774,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4OTE3ODE0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-02T20:05:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for taking a look! Indeed something quite strange is going on.\r\n\r\nI'm guessing that submodule is failing because I used ssh for the pybind11 submodule instead of https?\r\n\r\n```\r\n[submodule \"pybind11\"]\r\n    path = pybind11\r\n    url = git@github.com:pybind/pybind11.git\r\n```\r\n\r\nFor completeness, I cloned the reproducer and awkward in a clean directory, copied pybind11 from awkward to the reproducer, rebuilt, and the tests still fail :-(\r\n\r\nSince the tests passed on your system, I tried out a Debian machine with python 3.9, and the tests passed (I don't think they did in the past, so I suppose this is progress). So I suppose it's somehow a bad combination of ubuntu 18.04, gcc (Ubuntu 7.5.0-3ubuntu1~18.04), and python version (3.7.7 from pyenv with `--enable-shared`).\r\n\r\nAt some point I got it working after the previous pybind11 upgrade, so it must be possible, but I suppose I'll have to write it off as system specific (it's a cluster, so I'm not fully in control, but will figure something out). Thanks for your help, and sorry for the noise!",
  "created_at":"2021-03-02T20:58:25Z",
  "id":789210854,
  "issue":774,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4OTIxMDg1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-02T21:30:46Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"If I make any progress I'll post some updates here. I'm not looking for any action at this point, but just documenting for anyone else who gets into trouble.\r\n\r\nFor now, on the debian machine, I've managed to get the reproducer repo to pass the tests, but fail the full package. So in principle, I should be able to isolate it. But of course, easier said than done.\r\n\r\nI now think it may be something about PEP 517 builds vs native poetry builds. Totally unclear why this should matter, but it seems to pretty consistently fail with PEP 517 builds in a clean environment (ie: `poetry shell; pip install .`), while `poetry install` in a clean environment seems to work",
  "created_at":"2021-03-03T23:07:09Z",
  "id":790137900,
  "issue":774,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDEzNzkwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-03T23:07:09Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi @jpivarski, I am facing a problem while indexing. In the test function if I do a `2 * array.y[0][1] or 2 * array.y[2][0]`, `JAX` gets the wrong values.I don't think `JAX` is the culprit here, since it always passes around the `aux_data` without changing it. ",
  "created_at":"2021-03-03T12:32:28Z",
  "id":789682288,
  "issue":775,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4OTY4MjI4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-03T12:32:28Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"I pushed a correction to the text directly to master. This link went to a stub for documentation that hadn't been written, then the structure got reorganized.\r\n\r\nThanks for catching it!",
  "created_at":"2021-03-05T22:28:13Z",
  "id":791754385,
  "issue":777,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MTc1NDM4NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-03-05T22:28:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That might be what gets reported as the segfault, but I doubt that it's the classname function itself that's at fault:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/5999fbd39793a036802b525f34c1dfb3d43547c2/src/libawkward/array/NumpyArray.cpp#L510-L513\r\n\r\n(It's not defined with an ampersand or anything.) But that's what's terrible about segfaults\u2014they don't give useful debugging info. When I get a segfault, I bisect the code with deliberate exceptions. Throwing an exception before the segfault tells you that the code up to that point is okay; not getting to the exception because of the segfault is not informative. I keep moving the exception until I find the cause.\r\n\r\nIn this case, almost nothing is happening in this module. I could try it out Monday morning (or afternoon, or Tuesday, as things get pushed back), but this is the methodology that I'd use.",
  "created_at":"2021-03-08T02:23:33Z",
  "id":792415549,
  "issue":778,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MjQxNTU0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-08T02:23:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I didn't yet look into the awkward code base, but you're absolutely right - it seems unlikely to come from the class name itself. I suppose that's what I was hoping for, in that something so simple shouldn't be causing problems, which may make it easier to isolate.\r\n\r\nThanks for the suggestions on debugging strategy. I didn't yet embark on debugging in awkward itself because the code structure is still a bit opaque to me, and I haven't had time to sit down and try to understand it detail. I will try to find some time, but not sure I'll get there in the next day or so. I've tried so many variations and tests trying to resolve #774 that at this point, I'm mainly hoping that it's not due to a stupid mistake in how I'm building the package that I've somehow managed to integrate into my reproducer :-)\r\n\r\nIn some other tests, working with gdb had been helpful, but here I clearly need to recompile awkward with debug symbols. Just for completeness (it's really not helpful), here is what I get for the bt when calling python directly (ie. not through pytest):\r\n\r\n```\r\n(gdb) bt\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50\r\n#1  0x00007fa9f07e3535 in __GI_abort () at abort.c:79\r\n#2  0x00007fa9f083a508 in __libc_message (action=action@entry=do_abort, fmt=fmt@entry=0x7fa9f094528d \"%s\\n\") at ../sysdeps/posix/libc_fatal.c:181\r\n#3  0x00007fa9f0840c1a in malloc_printerr (str=str@entry=0x7fa9f094343b \"free(): invalid pointer\") at malloc.c:5341\r\n#4  0x00007fa9f084242c in _int_free (av=<optimized out>, p=<optimized out>, have_lock=<optimized out>) at malloc.c:4165\r\n#5  0x00007fa9d6f59a82 in ?? () from /home/rehlers/code/quarantine/pybind11-awkward-array-test-name/pybind11_awkward/_src.cpython-39-x86_64-linux-gnu.so\r\n#6  0x00007fa9d6f639b5 in ?? () from /home/rehlers/code/quarantine/pybind11-awkward-array-test-name/pybind11_awkward/_src.cpython-39-x86_64-linux-gnu.so\r\n#7  0x00007fa9d6f606fe in ?? () from /home/rehlers/code/quarantine/pybind11-awkward-array-test-name/pybind11_awkward/_src.cpython-39-x86_64-linux-gnu.so\r\n#8  0x00007fa9f0c849c3 in cfunction_call (func=0x7fa9d73f1c20, args=<optimized out>, kwargs=<optimized out>) at Objects/methodobject.c:539\r\n#9  0x00007fa9f0c3c58a in _PyObject_MakeTpCall (tstate=0x55757a578170, callable=0x7fa9d73f1c20, args=<optimized out>, nargs=1, keywords=0x0) at Objects/call.c:191\r\n#10 0x00007fa9f0bfb49b in _PyObject_VectorcallTstate (kwnames=<optimized out>, nargsf=<optimized out>, args=<optimized out>, callable=<optimized out>, tstate=<optimized out>)\r\n    at ./Include/cpython/abstract.h:116\r\n#11 _PyObject_VectorcallTstate (kwnames=<optimized out>, nargsf=<optimized out>, args=<optimized out>, callable=<optimized out>, tstate=<optimized out>) at ./Include/cpython/abstract.h:103\r\n#12 PyObject_Vectorcall (kwnames=<optimized out>, nargsf=<optimized out>, args=<optimized out>, callable=<optimized out>) at ./Include/cpython/abstract.h:127\r\n...\r\n```",
  "created_at":"2021-03-08T16:18:04Z",
  "id":792870133,
  "issue":778,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5Mjg3MDEzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-08T16:19:53Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi! What is the status of this? Is it still an ongoing investigation?",
  "created_at":"2021-04-08T17:04:25Z",
  "id":815989426,
  "issue":778,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTk4OTQyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T17:04:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi! Unfortunately, this is still an open issue. The current status is the same - I can reproduce it in a container via singularity, but haven't had time to go further. It took me 2-3 days of on and off work on this just to get it isolated, so while I appreciate your debugging suggestions, it's been hard to find more time to investigate when it's moving that slowly. I can perhaps spend part of a day on the weekend, but if I can't sort it out by then, I'll most likely put my time into trying to find a (potentially very hacky) workaround rather than a fix, which I hope will come to a faster resolution",
  "created_at":"2021-04-09T12:56:22Z",
  "id":816662279,
  "issue":778,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjY2MjI3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:20:11Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I gave it a shot this weekend, but didn't make any meaningful progress. Reproducing it is still hit or miss, even with the container. So maybe it's compiler specific, or there's some contamination that I don't understand, or I'm introducing a problem somehow in compiling or linking my code, or something else..?\r\n\r\nIn any case, my plan from here is to try to work around it by dropping passing awkward arrays directly into c++. Instead, I'll just iterate over the arrays in python and pass numpy arrays into c++. It would have been nice to use the existing code since it works on some platforms, but I guess this probably won't be a bottleneck for jet finding, so perhaps it will be just fine.\r\n\r\nThanks for your help. I don't foresee any progress here, so you can close it if you'd like",
  "created_at":"2021-04-12T14:22:30Z",
  "id":817855497,
  "issue":778,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNzg1NTQ5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T14:27:25Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, thanks for looking into this. I find that the recommendation of passing pure NumPy arrays through pybind11 is general enough for many situations. For jet-finding, that should be on the order of 4 arrays (5 for jaggedness). The only issue would be if you didn't know the structure, but even then, there would be [ak.to_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_buffers.html)/[ak.from_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_buffers.html) to turn general arrays into nothing but NumPy, assuming you knew the `from_buffers` protocol well enough to use them on the C++ side.",
  "created_at":"2021-04-12T18:32:43Z",
  "id":818034607,
  "issue":778,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODAzNDYwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T18:32:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'm not sure I fully understand your comment, so let me just ask one question to ensure I don't go too far off base. What I had in mind was something as simple as (schematically):\r\n\r\n```python\r\nevents = particles = ak.Array({\r\n    \"px\": [[99.0, 4.0, -99.0], [0.1, -0.1, 0]],\r\n    \"py\": [[0.1, -0.1, 0], [99.0, 4.0, -99.0]],\r\n    \"pz\": [[0, 0, 0], [0, 0, 0]],\r\n    \"E\": [[100.0, 5.0, 99.0], [100.0, 5.0, 99.0]],\r\n})\r\n\r\njets_px = []\r\nfor event in events:\r\n    result = my_cpp_func_to_perform_jet_finding(\r\n        np.asarray(event.px),\r\n        np.asarray(event.py),\r\n        np.asarray(event.pz),\r\n        np.asarray(event.E),\r\n    ))\r\n    # Some map of numpy arrays\r\n    jets_px.append(result[\"px\"])\r\n    # And so on for the rest.\r\n# Build the output, presumably based around offsets and np.concatenate each momentum component...\r\n```\r\n\r\nAdmittedly, I haven't actually written this yet, and I'm not a pybind11 expert, but I was hoping to let it take care of the details of passing arrays in and out. So I didn't foresee any use of to_buffers/from_buffers. Is this too simplistic? (I know this is getting pretty far out of scope for you and your work on awkward, so I won't keep up on this topic after this question. But any advice is greatly appreciated!)",
  "created_at":"2021-04-12T19:28:46Z",
  "id":818074964,
  "issue":778,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODA3NDk2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T19:35:13Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"I was talking in generality: you have a simple enough, fixed structure that you don't need to_buffers/from_buffers. Forget that\u2014I was just saying that even fully general data structures don't _really_ need the C++ interface, something I've been thinking a lot about recently.\r\n\r\nOne thing about your case: you'll have to flatten the px/py/pz/E before turning it into a NumPy array. You can do it only once with the high-level [ak.flatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.flatten.html) function, or you can delve into the low-level [ListArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListArray.html) or [ListOffsetArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html)\u2014if you do any slices of the array, you don't know which type it will be, so [ak.flatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.flatten.html) is easier).\r\n\r\nTo turn the results into an Awkward Array, you can [ak.unflatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.unflatten.html) them, given an array of counts, or construct a [ListOffsetArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html), given an array of offsets.",
  "created_at":"2021-04-12T19:53:37Z",
  "id":818105259,
  "issue":778,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODEwNTI1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T19:53:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for clarifying, and thanks as always for all of the advice!",
  "created_at":"2021-04-13T14:07:47Z",
  "id":818765367,
  "issue":778,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODc2NTM2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T14:07:47Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi @jpivarski , this code is ready for review. ",
  "created_at":"2021-03-24T16:15:03Z",
  "id":805961542,
  "issue":779,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNTk2MTU0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-24T16:15:22Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"This is looking good! When you integrate it into the main codebase (next PR), put most of the code in an `ak._connect._jax` submodule that only gets called from `ak.highlevel`.",
  "created_at":"2021-03-24T16:37:45Z",
  "id":805978442,
  "issue":779,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNTk3ODQ0Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-03-24T16:37:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This goes against the design of what a record is supposed to be: records are not dicts. Specifically, the set of keys in a dict is an aspect of its value, not its type, but the fields of a record are a part of its type: a record with fields `x` and `y` differs as much from a record with fields `x`, `y`, and `z` as `bool` differs from `float`. Two dicts with different sets of keys, on the other hand, have the same type, whether we're talking about Python's runtime types or Python code that has been type-checked by mypy. This matters because all items in an array have the same type (though possibly an enumerated union). It would be better to think of Awkward records as a thing like a C struct than a C++ `std::map`.\r\n\r\nSo what in Awkward Array _is_ like a `std::map`? I had to answer that question when reading `std::map` data from ROOT files in Uproot:\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> tree = uproot.open(skhep_testdata.data_path(\"uproot-stl_containers.root\"))[\"tree\"]\r\n\r\n>>> tree[\"map_int32_int16\"]\r\n<TBranchElement 'map_int32_int16' at 0x7f4ee84e8280>\r\n\r\n>>> tree[\"map_int32_int16\"].typename\r\n'std::map<int32_t, int16_t>'\r\n\r\n>>> tree[\"map_int32_int16\"].array()\r\n<Array [[(1, 1)], [(1, 1), ... (4, 4), (5, 5)]] type='5 * [var * (int64, int64),...'>\r\n\r\n>>> tree[\"map_int32_int16\"].array().type\r\n5 * [var * (int64, int64), parameters={\"__array__\": \"sorted_map\"}]\r\n```\r\n\r\nThe primary thing about a mapping (Python dict or C++ `std::map`) is that it provides a way to quickly find a value corresponding to a unique key. We can represent sets of key-value pairs as sorted lists of 2-tuples in Awkward Array, as though we used `std::vector<std::pair<K, V>>` in C++. (Fun fact: that's how ROOT serializes `std::maps`.)\r\n\r\nThe property of being able to search a mapping quickly is a behavior, and Awkward Array's type system is designed for mere representation, not behavior (just as ROOT I/O's is designed for serialization\u2014hence, they don't care about the difference between a `std::map<K, V>` and a `std::vector<std::pair<K, V>>`, either). In Awkward Array, behaviors are overlaid by naming array and record types with `parameters` and then mixing in class methods from the `ak.behavior` dict ([see documentation](https://awkward-array.readthedocs.io/en/latest/ak.behavior.html)). I've set aside the name `\"sorted_map\"` to mean \"provide a lookup function from sorted keys to values,\" although it hasn't been implemented. It would be a behavior similar to the one that makes lists of `uint8` act like strings:\r\n\r\n```python\r\n>>> ak.Array([\"This\", \"is\", \"an\", \"array\", \"of\", \"strings.\"]).layout\r\n<ListOffsetArray64>\r\n    <parameters>\r\n        <param key=\"__array__\">\"string\"</param>\r\n    </parameters>\r\n    <offsets><Index64 i=\"[0 4 6 8 13 15 23]\" offset=\"0\" length=\"7\" at=\"0x559b66ff90e0\"/></offsets>\r\n    <content><NumpyArray format=\"B\" shape=\"23\" data=\"84 104 105 115 105 ... 105 110 103 115 46\" at=\"0x559b66ff1090\">\r\n        <parameters>\r\n            <param key=\"__array__\">\"char\"</param>\r\n        </parameters>\r\n    </NumpyArray></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\nbut for mappings. I could convert this issue into a request to implement `\"sorted_map\"`, but records will never have field names that are not strings.\r\n\r\n------------------------\r\n\r\nFor now, though, the way to do that would be to use [np.searchsorted](https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html) to get the closest indexes to a given key, then check to see if the closest is exactly equal, then use those indexes on the values. That technique is used in a few different places in Awkward Array's own implementation, such as this one in `ak.unflatten`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/5999fbd39793a036802b525f34c1dfb3d43547c2/src/awkward/operations/structure.py#L2052-L2063\r\n\r\nThe `positions` are the indexes of `inneroffsets` where `inneroffsets` are cloests to `outeroffsets` (doing `side=\"right\"` and subtracting `1` ensures that the indexes are all greater than or equal to `0` and less than `len(inneroffsets)`). Then, if the nearest is also exactly equal\u2014what you want when key-matching\u2014the application `inneroffsets[positions]` must be equal to `outeroffsets` everywhere. Think of this `inneroffsets` as the sorted set of keys in your dict and this `outeroffsets` as the numbers you're hoping to find in that dict. After verifying that `inneroffsets[positions]` is equal to `outeroffsets` everywhere, `values[positions]` are the matched values (no equivalent in the above example).",
  "created_at":"2021-03-08T23:35:08Z",
  "id":793170895,
  "issue":780,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MzE3MDg5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-08T23:35:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for the detailed explanation and for providing an alternate solution! Thinking of awkward arrays as structs instead of maps definitely makes it easier to understand.",
  "created_at":"2021-03-09T12:14:52Z",
  "id":793800053,
  "issue":780,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MzgwMDA1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-09T12:14:52Z",
  "user":"MDQ6VXNlcjQ1NzcwMDIx"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - thanks! I've addressed all the comments. I think, I'm done with this PR.\r\nThanks!",
  "created_at":"2021-03-10T10:28:16Z",
  "id":795212756,
  "issue":781,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5NTIxMjc1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-10T10:28:16Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The default `axis` of [ak.flatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.flatten.html) is `axis=1`, and a one-dimensional array doesn't have such an axis (it is out of range).\r\n\r\nThe main purpose of this function is to reduce the dimension of an array by one, which can't be done to a one-dimensional array. If you want to reduce the dimension of _any_ array _to_ one, that's `ak.flatten` with `axis=None` (following NumPy conventions).\r\n\r\n```python\r\n>>> ak.flatten(ak.Array([-0.0848, -0.00858, 0.13, -0.0658]), axis=None)\r\n<Array [-0.0848, -0.00858, 0.13, -0.0658] type='4 * float64'>\r\n```",
  "created_at":"2021-03-11T23:03:11Z",
  "id":797108078,
  "issue":782,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5NzEwODA3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-11T23:03:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"There's a bug here, but your \"simplest example\" was very lucky\u2014all of the variations that I tried did not reproduce this error. You actually need a list-of-record-of-list-of-record to do it (though record doesn't have to have two fields):\r\n\r\n```python\r\n>>> array = ak.Array([[], [{\"x\": [{\"y\": 1}]}]])\r\n>>> ak.to_parquet(array, \"array.parquet\")\r\n>>> ak.from_parquet(\"array.parquet\").tolist()\r\n[[], [{'x': [{'y': 1}]}]]\r\n>>> ak.from_parquet(\"array.parquet\", lazy=True).tolist()\r\n[[], [{'x': []}]]\r\n```\r\n\r\nMoreover, you need the first entry to be an empty list. Here is the same type/form, but without starting with an empty list, the problem isn't visible:\r\n\r\n```python\r\n>>> array = ak.Array([[{\"x\": [{\"y\": 1}]}]])\r\n>>> ak.to_parquet(array, \"array.parquet\")\r\n>>> ak.from_parquet(\"array.parquet\").tolist()\r\n[[{'x': [{'y': 1}]}]]\r\n>>> ak.from_parquet(\"array.parquet\", lazy=True).tolist()\r\n[[{'x': [{'y': 1}]}]]\r\n```\r\n\r\nI tried putting the empty list in the inner dimension, and encountered another, possibly related, error:\r\n\r\n```python\r\n>>> array = ak.Array([[{\"x\": []}, {\"x\": [{\"y\": 1}]}]])\r\n>>> ak.to_parquet(array, \"array.parquet\")\r\n>>> ak.from_parquet(\"array.parquet\").tolist()\r\n[[{'x': []}, {'x': [{'y': 1}]}]]\r\n>>> ak.from_parquet(\"array.parquet\", lazy=True).tolist()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/highlevel.py\", line 439, in tolist\r\n    return ak.operations.convert.to_list(self)\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/operations/convert.py\", line 943, in to_list\r\n    return [to_list(x) for x in array]\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/operations/convert.py\", line 943, in <listcomp>\r\n    return [to_list(x) for x in array]\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/operations/convert.py\", line 943, in to_list\r\n    return [to_list(x) for x in array]\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/operations/convert.py\", line 943, in <listcomp>\r\n    return [to_list(x) for x in array]\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/operations/convert.py\", line 946, in to_list\r\n    return to_list(array.layout)\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/operations/convert.py\", line 955, in to_list\r\n    return {n: to_list(x) for n, x in array.fielditems()}\r\nValueError: generated array does not have sufficient length: expected2 but generated 1\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.2.0rc2/src/libawkward/virtual/ArrayGenerator.cpp#L39)\r\n```\r\n\r\nso it has something to do with empty lists. Putting an empty list between two good values is also symptomatic (for the value _after_ the empty list):\r\n\r\n```python\r\n>>> array = ak.Array([[{\"x\": [{\"y\": 1}]}], [], [{\"x\": [{\"y\": 2}]}]])\r\n>>> ak.to_parquet(array, \"array.parquet\")\r\n>>> ak.from_parquet(\"array.parquet\").tolist()\r\n[[{'x': [{'y': 1}]}], [], [{'x': [{'y': 2}]}]]\r\n>>> ak.from_parquet(\"array.parquet\", lazy=True).tolist()\r\n[[{'x': [{'y': 1}]}], [], [{'x': []}]]\r\n```\r\n\r\nExplicitly materializing the lazy version gives a hint of what's going on: the outer list offsets are wrongfully applied to the inner list.\r\n\r\n```python\r\n>>> not_lazy = ak.from_parquet(\"array.parquet\")\r\n>>> from_lazy = ak.materialized(ak.from_parquet(\"array.parquet\", lazy=True))\r\n>>> not_lazy.tolist()\r\n[[{'x': [{'y': 1}]}], [], [{'x': [{'y': 2}]}]]\r\n>>> from_lazy.tolist()\r\n[[{'x': [{'y': 1}]}], [], [{'x': []}]]\r\n\r\n>>> not_lazy.layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 1 1 2]\" offset=\"0\" length=\"4\" at=\"0x7fdc0dc00140\"/></offsets>\r\n    <content><RecordArray length=\"2\">\r\n        <field index=\"0\" key=\"x\">\r\n            <ListOffsetArray64>\r\n                <offsets><Index64 i=\"[0 1 2]\" offset=\"0\" length=\"3\" at=\"0x7fdc0dc001c0\"/></offsets>\r\n                <content><RecordArray length=\"2\">\r\n                    <field index=\"0\" key=\"y\">\r\n                        <NumpyArray format=\"l\" shape=\"2\" data=\"1 2\" at=\"0x7fdc0dc00080\"/>\r\n                    </field>\r\n                </RecordArray></content>\r\n            </ListOffsetArray64>\r\n        </field>\r\n    </RecordArray></content>\r\n</ListOffsetArray64>\r\n\r\n>>> from_lazy.layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 1 1 2]\" offset=\"0\" length=\"4\" at=\"0x7fdc0d800140\"/></offsets>\r\n    <content><RecordArray length=\"2\">\r\n        <field index=\"0\" key=\"x\">\r\n            <ListOffsetArray64>\r\n                <offsets><Index64 i=\"[0 1 1 2]\" offset=\"0\" length=\"4\" at=\"0x7fdc0d800140\"/></offsets>\r\n                <content><RecordArray length=\"2\">\r\n                    <field index=\"0\" key=\"y\">\r\n                        <NumpyArray format=\"l\" shape=\"2\" data=\"1 2\" at=\"0x7fdc0d400080\"/>\r\n                    </field>\r\n                </RecordArray></content>\r\n            </ListOffsetArray64>\r\n        </field>\r\n    </RecordArray></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\nThe inner list is not just `[0 1 1 2]` when it's supposed to be `[0 1 2]`, but it also has the same pointer: `0x7fdc0d800140`. That's definitely not right.\r\n\r\nThis could explain why examples without two levels of lists didn't exhibit the symptom, and records might be needed because the virtual-reading stops at each level of record arrays.\r\n\r\nI'll try to figure out what is happening here. Thanks for reporting!",
  "created_at":"2021-03-12T16:52:11Z",
  "id":797617021,
  "issue":783,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5NzYxNzAyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-12T16:52:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #784 fixes all of the examples above.\r\n\r\nFundamentally, it's because Parquet doesn't have the offsets array/content array distinction that Awkward Array and Arrow have\u2014instead, it only saves the deepest leaf-nodes with [a single indexing scheme](https://blog.twitter.com/engineering/en_us/a/2013/dremel-made-simple-with-parquet.html) to indicate offsets at all levels above it. To lazily produce offsets arrays anyway, we have to pick a field from the record (the first, though that's an implementation detail) and read that deepest content to get its indexes and make offsets.\r\n\r\nIn your case, there were two offsets arrays above the content. I had considered that and \"unpacked\" it to make the right offsets at each level, but they had the same cache key. So that's why we ended up with identically the same offsets for both levels: when it should have been fetching the second one, it found the first one in the cache and couldn't distinguish it. The essential correction was this:\r\n\r\n```diff\r\n-                samplekey = \"{0}:off:{1}[{2}]\".format(\r\n-                    lazy_cache_key, sampleform.form_key[4:], row_group\r\n+                samplekey = \"{0}:off:{1}:{2}[{3}]\".format(\r\n+                    lazy_cache_key,\r\n+                    sampleform.form_key[4:],\r\n+                    \".\".join(\"\" if x is None else x for x in unpack),\r\n+                    row_group,\r\n                )\r\n```\r\n\r\nI just had to put a bit more information into that cache key so that the two levels of offsets don't look the same. Unfortunately, that meant fixing a lot of tests that were hard-coded against the original cache keys. (The point of those tests was to verify that lazy reading reads the right thing at the right time, but to do so, I looked at which keys were in the cache.)",
  "created_at":"2021-03-12T18:02:27Z",
  "id":797660353,
  "issue":783,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5NzY2MDM1Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-03-12T18:02:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Do we need every combination of 32 bits and 64 bits on every supported Python for the tests on Windows? Couldn't a sampling be used?\r\n\r\nIt has sometimes been just a sampling, but then we needed more tests for something, so the full matrix was put back in. It can be reduced again. The main thing that it catches are integer width mismatches in C++, which are sometimes just a warning and sometimes an error. (I haven't checked for warnings in a while.) It can also catch Numba errors, as some generated code works with `numba.intp` versus `numba.int64`. Neither of these have been in active development recently, so the matrix can be scaled back, especially as Python 3.9 is being added.\r\n\r\nI've managed to get a 32-bit Linux Docker image working on my local machine to _diagnose_ 32-bit errors once they've been found by the Windows builds. It would be nice to integrate that someday, but not something I want to do right now.",
  "created_at":"2021-03-23T16:21:58Z",
  "id":805040875,
  "issue":786,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNTA0MDg3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-23T16:21:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think it's a good idea to have a couple of 32 bit tests, just not for every Python version. :) Is this ready to merge, or would you like me to scale it back a bit in this PR?",
  "created_at":"2021-03-23T17:56:31Z",
  "id":805109789,
  "issue":786,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNTEwOTc4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-23T17:56:31Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Cleaned up one of them (32-bit 3.8), that should be fairly comprehesive but a little faster",
  "created_at":"2021-03-23T18:12:33Z",
  "id":805121055,
  "issue":786,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNTEyMTA1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-23T18:12:33Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Somehow, GitHub hasn't gotten the message that Windows 32-bit Python 3.8 isn't included, but maybe it will after merging. I think it's fine to merge now. I'll do so if I don't hear back after a while.",
  "created_at":"2021-03-23T19:40:11Z",
  "id":805181718,
  "issue":786,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNTE4MTcxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-23T19:40:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It needs to be removed from the required checks. It thinks it will eventually show up.",
  "created_at":"2021-03-23T19:41:14Z",
  "id":805182350,
  "issue":786,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNTE4MjM1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-23T19:41:14Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I dropped it from the required checks.",
  "created_at":"2021-03-23T20:01:12Z",
  "id":805198155,
  "issue":786,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNTE5ODE1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-23T20:01:12Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Great, thanks!",
  "created_at":"2021-03-23T20:01:40Z",
  "id":805198439,
  "issue":786,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNTE5ODQzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-23T20:01:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right; this is a bug. The reason this happened goes pretty deep, too. Most operations on Awkward Arrays only rewrite indexes; they don't have to modify data in the tree structure down to the leaves. Exceptions to this are called \"carrying\" because the first application (in the implementation of slices) looked to me like carrying a digit in arithmetic, except that it was carrying a whole array. Naturally, carrying is more expensive than not carrying.\r\n\r\nAlmost a year ago now, @ianna put in an important optimization that replaced carrying of RecordArrays with creating an [IndexedArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedArray.html) node above the RecordArray. Carrying a RecordArray is especially expensive because it might have a lot of fields\u2014you'd have to rewrite every one of those fields. Moreover, some of those fields might be unread data on disk (NanoEvents is lazy), so carrying every field in a RecordArray might initiate a lot of extra disk-reading. @ianna's optimization instead creates a new IndexedArray that _represents_ its content in a different order without actually changing the order of its content. This optimization couldn't be applied in all cases, but it could be applied in some.\r\n\r\nHowever, there was one tiny mistake:\r\n\r\n```diff\r\n# src/libawkward/array/RecordArray.cpp\r\n@@ -941,7 +941,7 @@ namespace awkward {\r\n    }\r\n    if (allow_lazy) {\r\n      return std::make_shared<IndexedArray64>(identities,\r\n-                                             parameters_,\r\n+                                             util::Parameters(),\r\n                                              carry,\r\n                                              shallow_copy());\r\n    }\r\n```\r\n\r\nThe RecordArray's parameters (e.g. it's name) was being copied from the RecordArray to the new IndexedArray node, so the name now appeared in two places. When you call `ak.with_name`, you _are_ replacing the RecordArray's name, but it's hidden under the IndexedArray that still has the old name.\r\n\r\nThat's why you see a difference depending on whether the array has been modified or not. The modified array would formally require a carry operation, and an IndexedArray is inserted instead. The unmodifed array doesn't even formally require a carry.\r\n\r\nBefore:\r\n\r\n```python\r\n>>> one = ak.Array([[{\"x\": 1}], [], [{\"x\": 2}]], with_name=\"One\")\r\n>>> two = ak.Array([[{\"x\": 1.1}], [], [{\"x\": 2.2}]], with_name=\"Two\")\r\n>>> ak.with_name(ak.concatenate([one, two], axis=1), \"All\").type          # record name change is visible\r\n3 * var * union[All[\"x\": int64], All[\"x\": float64]]\r\n>>> ak.with_name(ak.concatenate([one[1:], two[1:]], axis=1), \"All\").type  # record name change is hidden\r\n2 * var * union[One[\"x\": int64], Two[\"x\": float64]]\r\n```\r\n\r\nAfter:\r\n\r\n```python\r\n>>> ak.with_name(ak.concatenate([one, two], axis=1), \"All\").type\r\n3 * var * union[All[\"x\": int64], All[\"x\": float64]]\r\n>>> ak.with_name(ak.concatenate([one[1:], two[1:]], axis=1), \"All\").type\r\n2 * var * union[All[\"x\": int64], All[\"x\": float64]]\r\n```\r\n\r\nThat was one fix, and it might be the only one that's relevant to your case because electrons and muons might be unmergeable. (They might have to be a UnionArray of two RecordArrays, rather than a single RecordArray.) That's not the case in my minimal example, though: the `\"x\"` fields can be merged to a single `float64` field. So in PR #789, I also added a second pass to `ak.with_name` to check for instances where unions can be merged.\r\n\r\n```python\r\n>>> ak.with_name(ak.concatenate([one, two], axis=1), \"All\").type\r\n3 * var * All[\"x\": float64]\r\n>>> ak.with_name(ak.concatenate([one[1:], two[1:]], axis=1), \"All\").type\r\n2 * var * All[\"x\": float64]\r\n```\r\n\r\nIf it's possible to remove UnionArrays like this, it's better to do so because some features don't work with UnionArrays (Numba and JAX integration, for example). If your electrons and muons have different sets of fields, preventing their merger, you can select a common subset of fields like this:\r\n\r\n```python\r\n>>> my_electrons = electrons[[\"pt\", \"eta\", \"phi\", \"charge\"]]\r\n```\r\n\r\nto make them mergeable. Records with the same field names pointing to mergeable field types _and_ the same record name are mergeable; the \"union\" should go away. (On the other hand, if you don't use any features that don't work for UnionArrays, don't worry about this step.)",
  "created_at":"2021-03-25T13:46:21Z",
  "id":806772858,
  "issue":788,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNjc3Mjg1OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-03-25T13:46:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi @jpivarski, the tests are queued right now, but this is ready for review!",
  "created_at":"2021-04-09T15:50:08Z",
  "id":816777399,
  "issue":793,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjc3NzM5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T15:50:08Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry, @lukasheinrich, I meant to ask you to be a reviewer, not an assignee of this PR, and just to check the tests (the last file in the diff).",
  "created_at":"2021-04-09T17:02:22Z",
  "id":816822189,
  "issue":793,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjgyMjE4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T17:02:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @trickarcher  - apologies for the delay in the review!\r\n\r\nI did some basic tests and a lot of them look very good and are functional. in my own testing I came across this issue:\r\n\r\n\r\n```python\r\nimport awkward as ak\r\nimport numpy as np\r\nimport pytest\r\n\r\njax = pytest.importorskip(\"jax\")\r\njax.config.update(\"jax_platform_name\", \"cpu\")\r\nak.jax.register()\r\n\r\n\r\ndef func_numpyarray_1(x):\r\n    return ak.Array(x[2:3] ** 2)\r\n\r\n\r\ntest_input = ak.Array([1.,2.,3.,4.])\r\nv,f = jax.vjp(func_numpyarray_1,test_input)\r\nprint(v,type(v),repr(v))\r\n\r\n(grads,) = f(ak.Array([1.0]))\r\nprint(grads)\r\n ```\r\n\r\ngives\r\n\r\n```\r\n[0] <class 'awkward.highlevel.Array'> <Array [0] type='1 * float32'>\r\n[0, 0, 0, 0]\r\n```\r\n\r\nwhile leaving out the wrapping `ak.Array` in the test function allows the gradients to flow.. is this something thats fixable within the scope of this PR? Note also that the value itself is zero which is unexpected\r\n\r\nI also noticed that the value of `v` here is run-dependent, sometimes it's `0` somtimes `-0` sometimes `inf` or `-inf` .. is this accessing uninitialized memory?\r\n\r\n\r\nfor reference here the non-wrapping versino\r\n\r\n```python\r\nimport awkward as ak\r\nimport numpy as np\r\nimport pytest\r\n\r\njax = pytest.importorskip(\"jax\")\r\njax.config.update(\"jax_platform_name\", \"cpu\")\r\nak.jax.register()\r\n\r\n\r\ndef func_numpyarray_1(x):\r\n    return x[2:3] ** 2\r\n\r\n\r\ntest_input = ak.Array([1.,2.,3.,4.])\r\nv,f = jax.vjp(func_numpyarray_1,test_input)\r\nprint(v,type(v),repr(v))\r\n\r\n(grads,) = f(ak.Array([1.0]))\r\nprint(grads)\r\n```\r\n\r\ngiving\r\n\r\n```\r\n[9] <class 'awkward.highlevel.Array'> <Array [9] type='1 * float32'>\r\n[0, 0, 6, 0]\r\n```\r\n\r\n\r\n",
  "created_at":"2021-04-18T20:34:32Z",
  "id":822056712,
  "issue":793,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjA1NjcxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-18T20:43:48Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"NONE",
  "body":"here is another test case that seems to break (diffing through a `ak.Record`) but I'm not sure whethr that falls under the lilmitations we already identified\r\n\r\n```\r\n#works\r\ndef func_numpyarray_1(x):\r\n    return x\r\n\r\narr = ak.Array([1.0,2.0])\r\nprint(jax.jvp(func_numpyarray_1,(arr,),(arr,)))\r\n\r\n\r\n#does not work\r\ndef func_numpyarray_1(x):\r\n    return x['x']\r\n\r\nrec = ak.Record({'x': [1.0,2.0], 'y': [3.0]})\r\nprint(jax.jvp(func_numpyarray_1,(rec,),(rec,)))\r\n\r\n#works\r\ndef func_numpyarray_1(x):\r\n    return x['x']\r\n\r\narr = ak.Array([{'x': [1.0,2.0], 'y': [3.0]}])\r\nprint(jax.jvp(func_numpyarray_1,(arr,),(arr,)))\r\n```",
  "created_at":"2021-04-18T21:18:09Z",
  "id":822063339,
  "issue":793,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjA2MzMzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-18T21:27:18Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"MEMBER",
  "body":"We discussed it at our meeting and concluded that this PR is in a good state, provided that a high-level interface is added (as another PR) to wrap potential scalars (numbers, None, and `ak.Record`) into one-element arrays, run the JAX function, then unwrap them by taking the zeroth element. (These can be slices: `input[np.newaxis]` to add a dimension, `output[0]` to remove it.)\r\n\r\n```python\r\nak.jax.jacobian\r\nak.jax.jacfwd\r\nak.jax.grad\r\nak.jax.hessian\r\n```\r\n\r\n@trickarcher had one more thing to add to the PR before declaring it done, though.",
  "created_at":"2021-04-19T17:14:06Z",
  "id":822635449,
  "issue":793,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjYzNTQ0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-19T17:14:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski Apparantly there was nothing to be done since CuPy doesn't run on CI builds. You can merge the PR once the tests pass. Thank You!",
  "created_at":"2021-04-19T17:20:53Z",
  "id":822639770,
  "issue":793,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjYzOTc3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-19T17:20:53Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"MEMBER",
  "body":"@trickarcher Okay, thanks!",
  "created_at":"2021-04-19T17:37:47Z",
  "id":822650013,
  "issue":793,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjY1MDAxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-19T17:37:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the report!\r\n\r\nI believe that the correct place to catch and fix this edge-case is in PR #795: we should let the size-1 RegularArray go through the calculation (that's how `ak.cartesian` works\u2014by broadcasting) but if the result is all zero-length because one of the arrays in the Cartesian product was, then the size-1 RegularArrays that would have been broadcasted if they weren't empty should then be reduced to empty arrays. It works for your example (before and after collapsing the dimensions with `nested=False`) and works for all other tested cases. As a change, it applies in a very limited set of cases.",
  "created_at":"2021-03-29T17:45:26Z",
  "id":809578869,
  "issue":794,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwOTU3ODg2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-29T17:45:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know what you want `dict(ak_array)` to do. Technically, what happens is that the `dict` constructor iterates over what it's given and tries to cast each item as a 2-tuple to use as key-value pairs. Some Awkward Arrays will have that structure and some won't.\r\n\r\nAre you wanting to turn the Awkward Array into Python lists and dicts? That's [ak.to_list](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_list.html) or `ak_array.tolist()` as a method (following NumPy's [naming convention](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.tolist.html)).\r\n\r\n```python\r\n>>> pets\r\n<Array [{dog: [[1, 2], [5]]}] type='1 * {\"dog\": var * var * float64}'>\r\n>>> pets.tolist()\r\n[{'dog': [[1.0, 2.0], [5.0]]}]\r\n```",
  "created_at":"2021-03-31T16:38:48Z",
  "id":811229248,
  "issue":797,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxMTIyOTI0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-31T16:38:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The only problem that I see is that the output type is so different from what NumPy returns. But then again, NumPy's output can only ever make sense for regular arrays: the tuple of `i` coordinates and `j` coordinates is only useful if the two line up, which they wouldn't fit an irregular array.\r\n\r\nSince the output is so different, maybe this new feature should be a new function with a new name? NumPy has already taken both one-argument `where` and `nonzero` for this same purpose (with the same output type: a tuple of aligned arrays), but maybe something like `argtrue`?",
  "created_at":"2021-04-06T13:19:45Z",
  "id":814113691,
  "issue":801,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNDExMzY5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-06T13:19:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes you're right, the non-tuple version should have another name such as `argtrue`.\r\n\r\nHow about `ak.where` generalizing for variable length arrays, as long as the depth remains the same for every subarray?\r\n```python\r\nA = ak.Array([[False, True], [True, False, True]])\r\nak.where(A)\r\n>>> (<Array [0, 1, 1] type='3 * int64'>, <Array [1, 0, 2] type='3 * int64'>)\r\n```\r\nWe then get the expected:\r\n```python\r\nA[ak.where(A)]\r\n>>> <Array [True, True, True] type='3 * bool'>\r\n```\r\nAnother example with depth three:\r\n```python\r\nak.where(ak.Array([[[False, True], [True]], [[False, False], [True, False, True]]]))\r\n>>> (<Array [0, 0, 1, 1] type='4 * int64'>, \r\n...  <Array [0, 1, 1, 1] type='4 * int64'>, \r\n...  <Array [1, 0, 0, 2] type='4 * int64'>)\r\n```",
  "created_at":"2021-04-07T09:26:56Z",
  "id":814762447,
  "issue":801,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNDc2MjQ0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-07T09:26:56Z",
  "user":"MDQ6VXNlcjI0OTgyMDI5"
 },
 {
  "author_association":"MEMBER",
  "body":"That would work, though it would take some thinking to figure out how to implement this.\r\n\r\nIt could also be `ak.nonzero`, as that should have the same behavior.",
  "created_at":"2021-04-07T13:14:01Z",
  "id":814904441,
  "issue":801,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNDkwNDQ0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-07T13:14:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Absolutely!  `ak.where` with one argument could just call that then",
  "created_at":"2021-04-07T13:30:39Z",
  "id":814916120,
  "issue":801,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNDkxNjEyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-07T13:30:39Z",
  "user":"MDQ6VXNlcjI0OTgyMDI5"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lgray - thanks for spotting it! The https://github.com/scikit-hep/awkward-1.0/pull/803 should fix it.",
  "created_at":"2021-04-07T08:38:32Z",
  "id":814723048,
  "issue":802,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNDcyMzA0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-07T08:38:32Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"fixed via #803",
  "created_at":"2021-04-07T16:55:42Z",
  "id":815070573,
  "issue":802,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTA3MDU3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-07T16:55:42Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@henryiii - it's strange that the CI pre-commit disagrees with my local one:\r\n```python\r\n(base) yana@iannas-macbook-pro-2 awkward-1.0 % pre-commit run --all                  \r\nCheck for added large files..............................................Passed\r\nCheck for case conflicts.................................................Passed\r\nCheck for merge conflicts................................................Passed\r\nCheck for broken symlinks............................(no files to check)Skipped\r\nCheck Yaml...............................................................Passed\r\nDebug Statements (Python)................................................Passed\r\nFix End of Files.........................................................Passed\r\nMixed line ending........................................................Passed\r\nFix requirements.txt.....................................................Passed\r\nTrim Trailing Whitespace.................................................Passed\r\nblack....................................................................Passed\r\nflake8...................................................................Passed\r\n(base) yana@iannas-macbook-pro-2 awkward-1.0 % git status\r\nOn branch ianna/bugfix-issue-802\r\nnothing to commit, working tree clean\r\n```",
  "created_at":"2021-04-07T08:41:51Z",
  "id":814725337,
  "issue":803,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNDcyNTMzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-07T08:41:51Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I've found out hard to ensure that local and remote pre-commits run the same set of checks. It has something to do with what you have installed. In this case, it's checking the .clang-format YAML, a test I've never seen before. Nevertheless, it's probably a real error on the file.",
  "created_at":"2021-04-07T13:11:04Z",
  "id":814900858,
  "issue":803,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNDkwMDg1OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-07T13:11:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I've found out hard to ensure that local and remote pre-commits run the same set of checks. It has something to do with what you have installed. In this case, it's checking the .clang-format YAML, a test I've never seen before. Nevertheless, it's probably a real error on the file.\r\n\r\nIndeed there was a duplicate `BinPackParameters`",
  "created_at":"2021-04-07T13:19:33Z",
  "id":814908456,
  "issue":803,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNDkwODQ1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-07T13:19:33Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna & @jpivarski, _most_ of the time, pre-commit pins quite well and it is very reproducible. However, that may not quite always be the case, as you've seen above; also this can change for clang-format (since the docker images are not pinned). I think this might be that `.clang-format` has started counting as a yaml file. Maybe your version of pre-commit itself is older? That's not \"pinned\" like the checks are.",
  "created_at":"2021-04-07T20:42:50Z",
  "id":815249106,
  "issue":803,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTI0OTEwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-07T20:42:50Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, here it is: https://github.com/pre-commit/identify/commit/9bb588c195843a3256a26a3503d7eebf15f6456d\r\n\r\nYou have an older pre-commit/identify than the CI has.",
  "created_at":"2021-04-07T20:46:05Z",
  "id":815251240,
  "issue":803,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTI1MTI0MA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-07T20:46:05Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lgray - it works if an `axis=0` argument is given to `ak.cartesian`:\r\n```python\r\n    muon = ak.Array([[{\"pt\": 1.}],[]], with_name=\"muon\")\r\n    electron = ak.Array([[],[{\"pt\": 1.}]], with_name=\"electron\")\r\n\r\n    muon = muon[muon.pt > 5]\r\n    electron = electron[electron.pt > 5]\r\n\r\n    leptons = ak.concatenate([muon,electron], axis=1)\r\n    candidate = ak.firsts(leptons)\r\n    assert ak.to_list(ak.Array(candidate)) == [ None, None ]\r\n\r\n    result = ak.cartesian([candidate, candidate], axis=0)\r\n    assert ak.to_list(result) == [(None, None), (None, None), (None, None), (None, None)]\r\n\r\n```\r\nI'm not sure if `ak.cartesian` needs to check on an empty input and (re-)set its axis? @jpivarski ? ",
  "created_at":"2021-04-08T07:26:08Z",
  "id":815520973,
  "issue":805,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTUyMDk3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T07:26:08Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lgray and @jpivarski - The default `axis` for `ak.cartesian` is `1`. Perhaps,  a `ValueError` message such as in https://github.com/scikit-hep/awkward-1.0/pull/808 be sufficient? Thanks.",
  "created_at":"2021-04-08T09:49:11Z",
  "id":815621744,
  "issue":805,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTYyMTc0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T09:49:11Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"`axis=0` follows a different code path, and the error\r\n\r\n```\r\nlocal variable 'numoutputs' referenced before assignment\r\n```\r\n\r\nit's about Python taking an unexpected code path. If the operation is invalid, then catching it with a ValueError would be appropriate. Did this array have 1 dimension? If so, then yes, `axis != 0` should be an error. It's just hitting a programming logic error before discovering that.",
  "created_at":"2021-04-08T11:52:43Z",
  "id":815707944,
  "issue":805,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTcwNzk0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T11:52:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"`axis=1` is valid for the actual arrays, but they appear as if they were empty:\r\n```python\r\n<IndexedOptionArray64>\r\n    <index><Index64 i=\"[-1 -1]\" offset=\"0\" length=\"2\" at=\"0x7f8d7d4fdbb0\"/></index>\r\n    <content><UnionArray8_64>\r\n        <tags><Index8 i=\"[]\" offset=\"0\" length=\"0\" at=\"0x000000000000\"/></tags>\r\n        <index><Index64 i=\"[]\" offset=\"0\" length=\"0\" at=\"0x000000000000\"/></index>\r\n        <content tag=\"0\">\r\n            <IndexedArray64>\r\n                <index><Index64 i=\"[]\" offset=\"0\" length=\"0\" at=\"0x000000000000\"/></index>\r\n                <content><RecordArray length=\"1\">\r\n                    <parameters>\r\n                        <param key=\"__record__\">\"muon\"</param>\r\n                    </parameters>\r\n                    <field index=\"0\" key=\"pt\">\r\n                        <NumpyArray format=\"d\" shape=\"1\" data=\"1\" at=\"0x7f8d7d90d400\"/>\r\n                    </field>\r\n                </RecordArray></content>\r\n            </IndexedArray64>\r\n        </content>\r\n        <content tag=\"1\">\r\n            <IndexedArray64>\r\n                <index><Index64 i=\"[]\" offset=\"0\" length=\"0\" at=\"0x000000000000\"/></index>\r\n                <content><RecordArray length=\"1\">\r\n                    <parameters>\r\n                        <param key=\"__record__\">\"electron\"</param>\r\n                    </parameters>\r\n                    <field index=\"0\" key=\"pt\">\r\n                        <NumpyArray format=\"d\" shape=\"1\" data=\"1\" at=\"0x7f8d7d898c00\"/>\r\n                    </field>\r\n                </RecordArray></content>\r\n            </IndexedArray64>\r\n        </content>\r\n    </UnionArray8_64></content>\r\n</IndexedOptionArray64>\r\n<IndexedOptionArray64>\r\n    <index><Index64 i=\"[-1 -1]\" offset=\"0\" length=\"2\" at=\"0x7f8d7d4fdbb0\"/></index>\r\n    <content><UnionArray8_64>\r\n        <tags><Index8 i=\"[]\" offset=\"0\" length=\"0\" at=\"0x000000000000\"/></tags>\r\n        <index><Index64 i=\"[]\" offset=\"0\" length=\"0\" at=\"0x000000000000\"/></index>\r\n        <content tag=\"0\">\r\n            <IndexedArray64>\r\n                <index><Index64 i=\"[]\" offset=\"0\" length=\"0\" at=\"0x000000000000\"/></index>\r\n                <content><RecordArray length=\"1\">\r\n                    <parameters>\r\n                        <param key=\"__record__\">\"muon\"</param>\r\n                    </parameters>\r\n                    <field index=\"0\" key=\"pt\">\r\n                        <NumpyArray format=\"d\" shape=\"1\" data=\"1\" at=\"0x7f8d7d90d400\"/>\r\n                    </field>\r\n                </RecordArray></content>\r\n            </IndexedArray64>\r\n        </content>\r\n        <content tag=\"1\">\r\n            <IndexedArray64>\r\n                <index><Index64 i=\"[]\" offset=\"0\" length=\"0\" at=\"0x000000000000\"/></index>\r\n                <content><RecordArray length=\"1\">\r\n                    <parameters>\r\n                        <param key=\"__record__\">\"electron\"</param>\r\n                    </parameters>\r\n                    <field index=\"0\" key=\"pt\">\r\n                        <NumpyArray format=\"d\" shape=\"1\" data=\"1\" at=\"0x7f8d7d898c00\"/>\r\n                    </field>\r\n                </RecordArray></content>\r\n            </IndexedArray64>\r\n        </content>\r\n    </UnionArray8_64></content>\r\n</IndexedOptionArray64>\r\n```",
  "created_at":"2021-04-08T12:14:30Z",
  "id":815739831,
  "issue":805,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTczOTgzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T12:14:30Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"As long as it has the right number of dimensions, empty arrays should be allowed, though the result of the operation might be empty.\r\n\r\nThe array that @lgray is trying to `ak.cartesian` is one-dimensional:\r\n\r\n```python\r\n>>> candidate\r\n<Array [None, None] type='2 * ?union[muon[\"pt\": float64], electron[\"pt\": float64]]'>\r\n>>> ak.cartesian([candidate, candidate])\r\n# ... (fails)\r\n```\r\n\r\nWith the default `axis=1`, `ak.cartesian` _should_ fail for one-dimensional arrays:\r\n\r\n```python\r\n>>> simpler = ak.Array([1, 2, 3])\r\n>>> ak.cartesian([simpler, simpler])\r\n# ... (also fails, as it should, though the error message could be more clear\r\n```\r\n\r\n(Note: the error message is the same for `simpler = ak.Array([None, None])`, as it should be, though again it could be clearer.)\r\n\r\nAlthough the \"referenced before assignment\" error is a programming error (the sort of thing a static type checker would catch, actually), it does fail on the cases that are supposed to fail and succeeds on the cases that are supposed to succeed. @ianna, you showed that it works when `axis=0`, as a one-dimensional array should, and if I change `candidate` into a two-dimensional array, it also works:\r\n\r\n```python\r\n>>> candidate[np.newaxis]\r\n<Array [[None, None]] type='1 * 2 * ?union[muon[\"pt\": float64], electron[\"pt\": f...'>\r\n>>> ak.cartesian([candidate[np.newaxis], candidate[np.newaxis]])\r\n<Array [... ), (None, None), (None, None)]] type='1 * var * (?union[muon[\"pt\": f...'>\r\n```\r\n\r\nSo that unconsidered code path _and_ the unclear error message could both be fixed by an up-front check that throws a good error message. Just before\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/1492c4906df9f5bbf7b4f244de543188563015a4/src/awkward/operations/structure.py#L2897\r\n\r\nthere could be a check to ensure that `posaxis` (the `axis` parameter after negative-value handling) is in the correct range for the `purelist_depth` of all the arguments. (The arguments can have different depths from each other, but the chosen `posaxis` has to exist for all of them.) Doing this would get `ak.argcartesian` for free, because it's defined in terms of `ak.cartesian`.\r\n",
  "created_at":"2021-04-08T13:39:36Z",
  "id":815832985,
  "issue":805,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTgzMjk4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T13:39:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@ianna @jpivarski \r\n\r\nUnfortunately #808 is not the solution to this problem, especially since the arrays are the exact same form in a passing case vs. a failing case.\r\n\r\nThe two arrays I am trying to cross in the full data look like:\r\n```\r\n[[FatJet], [FatJet], [FatJet], [FatJet], [], ... [], [], [], [], [FatJet], [], []]\r\n[None, None, None, None, None, None, None, ... None, None, None, None, None, None]\r\n```\r\nwhere the first is a non-option list record array, and the second is an option union record array.\r\naxis = 1 here should always be viable since the second broadcasts into the first.\r\n\r\nThe error occurs when the second is all \"Nones\".\r\nThis appears to extend to the case where any of the input arrays to the cartesian product are all Nones?\r\n\r\nHere's a better minimal example.\r\n\r\n```python3\r\nimport awkward as ak\r\n\r\ndef mre(lepcut=6., jetcut=1.5):\r\n    muon = ak.Array([[{\"pt\": 1.}],[]], with_name=\"muon\")\r\n    electron = ak.Array([[],[{\"pt\": 5.}]], with_name=\"electron\")\r\n\r\n    muon = muon[muon.pt > lepcut]\r\n    electron = electron[electron.pt > lepcut]\r\n\r\n    jet = ak.Array([[{\"pt\": 2.0}],[{\"pt\":4.0}]])\r\n    jet = jet[jet.pt > jetcut]\r\n\r\n    leptons = ak.concatenate([muon,electron], axis=1)\r\n    candidate = ak.firsts(leptons)\r\n\r\n    x = ak.cartesian([candidate, jet])\r\n    print(x)\r\n\r\nif __name__ == \"__main__\":\r\n    mre(2.5, 1.5)\r\n    mre()\r\n```\r\n\r\nwhich outputs with awkward 1.2.1:\r\n```\r\n[None, [({pt: 5}, {pt: 4})]]\r\nTraceback (most recent call last):\r\n  File \"mre.py\", line 21, in <module>\r\n    mre()\r\n  File \"mre.py\", line 16, in mre\r\n    x = ak.cartesian([candidate, jet])\r\n  File \"/Users/lagray/miniconda3/envs/coffea-jetmet/lib/python3.7/site-packages/awkward/operations/structure.py\", line 3090, in cartesian\r\n    layouts, getfunction3, behavior, right_broadcast=False, pass_depth=True\r\n  File \"/Users/lagray/miniconda3/envs/coffea-jetmet/lib/python3.7/site-packages/awkward/_util.py\", line 1037, in broadcast_and_apply\r\n    out = apply(broadcast_pack(inputs, isscalar), 0, user)\r\n  File \"/Users/lagray/miniconda3/envs/coffea-jetmet/lib/python3.7/site-packages/awkward/_util.py\", line 799, in apply\r\n    outcontent = apply(nextinputs, depth + 1, user)\r\n  File \"/Users/lagray/miniconda3/envs/coffea-jetmet/lib/python3.7/site-packages/awkward/_util.py\", line 752, in apply\r\n    outcontent = apply(nextinputs, depth, user)\r\n  File \"/Users/lagray/miniconda3/envs/coffea-jetmet/lib/python3.7/site-packages/awkward/_util.py\", line 717, in apply\r\n    for i in range(numoutputs)\r\nUnboundLocalError: local variable 'numoutputs' referenced before assignment\r\n```",
  "created_at":"2021-04-08T13:46:26Z",
  "id":815838527,
  "issue":805,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTgzODUyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T13:54:10Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"> The two arrays I am trying to cross in the full data look like:\r\n> \r\n> ```\r\n> [[FatJet], [FatJet], [FatJet], [FatJet], [], ... [], [], [], [], [FatJet], [], []]\r\n> [None, None, None, None, None, None, None, ... None, None, None, None, None, None]\r\n> ```\r\n> \r\n> where the first is a non-option list record array, and the second is an option union record array.\r\n> axis = 1 here should always be viable since the second broadcasts into the first.\r\n\r\nOkay, that's different. `axis=1` is valid because the arrays are two-dimensional after broadcasting. (Broadcasting happens during the `ak.cartesian` operation itself, as @nsmith- showed me once that Cartesian product can be implemented in terms of broadcasting, and so that's how it's implemented.)\r\n\r\nHere are some tests with simple types of the same dimensionality:\r\n\r\n```python\r\n>>> one_dimensional = ak.Array([1.1, 2.2, 3.3, 4.4, 5.5])\r\n>>> two_dimensional = ak.Array([[1], [2], [], [], [3]])\r\n>>> ak.broadcast_arrays(one_dimensional, two_dimensional)\r\n[<Array [[1.1], [2.2], [], [], [5.5]] type='5 * var * float64'>, <Array [[1], [2], [], [], [3]] type='5 * var * int64'>]\r\n>>> ak.cartesian([one_dimensional, two_dimensional])\r\n<Array [[(1.1, 1)], [(2.2, ... [], [(5.5, 3)]] type='5 * var * (float64, int64)'>\r\n\r\n>>> none_dimensional = ak.Array([None, None, None, None, None])\r\n>>> ak.broadcast_arrays(none_dimensional, two_dimensional)\r\n[<Array [None, None, None, None, None] type='5 * option[var * bool]'>, <Array [None, None, None, None, None] type='5 * option[var * int64]'>]\r\n>>> ak.cartesian([none_dimensional, two_dimensional])\r\n<Array [None, None, None, None, None] type='5 * option[var * (bool, int64)]'>\r\n```\r\n\r\nSo my idea of putting a general comparison of array dimension and `axis` before any processing isn't a good one, as it would exclude valid cases of one-dimensional \u00d7 two-dimensional that broadcast to two-dimensional.\r\n\r\nGoing back to your `candidate`, it does fail with the UnboundLocalError when I try mix `candidate` with a simple two-dimensional array:\r\n\r\n```python\r\n>>> candidate\r\n<Array [None, None] type='2 * ?union[muon[\"pt\": float64], electron[\"pt\": float64]]'>\r\n>>> ak.cartesian([candidate, ak.Array([[1, 2, 3], []])])\r\n# ...\r\n# UnboundLocalError: local variable 'numoutputs' referenced before assignment\r\n```\r\n\r\n_but also,_ it fails in the same way if we try to explicitly broadcast it:\r\n\r\n```python\r\n>>> ak.broadcast_arrays(candidate, ak.Array([[1, 2, 3], []]))\r\n# ...\r\n# UnboundLocalError: local variable 'numoutputs' referenced before assignment\r\n```\r\n\r\nSo this isn't an error with `ak.cartesian` and the fix shouldn't be in `ak.cartesian`. It's a corner case in broadcasting.\r\n\r\n",
  "created_at":"2021-04-08T14:08:01Z",
  "id":815855292,
  "issue":805,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTg1NTI5Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-08T14:08:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Both of these are problems with mixing high-level and low-level arrays.\r\n\r\nIn the first example, `array` is a high-level array whose `layout` is partitioned. The fact that `array` is partitioned is not visible from the `array` level; you'd only know it if you delved into the `layout` (or used [ak.partitions](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partitions.html) to get the length of each). To access the actual partitions, you could do `array.layout.partitions`, but this would be a low-level view.\r\n\r\nIn the second example, you created a low-level `IrregularlyPartitionedArray`, which has no `layout` because it is a layout. If wrapped in an `ak.Array` constructor, it would behave like an unpartitioned high-level array.\r\n\r\nYou might be trying to iterate over partitions. There isn't actually a function for that (other than using the numbers from [ak.partitions](https://awkward-array.readthedocs.io/en/latest/_auto/ak.partitions.html) to make slices in a `for` loop; see below). I've been wondering if there need to be more tools like this.\r\n\r\n```python\r\n>>> array = ak.repartition(np.arange(100), 10)\r\n>>> # high-level array\r\n>>> array\r\n<Array [0, 1, 2, 3, 4, ... 95, 96, 97, 98, 99] type='100 * int64'>\r\n>>> # low-level partitions\r\n>>> array.layout.partitions\r\n[\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"0 1 2 3 4 5 6 7 8 9\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"10 11 12 13 14 15 16 17 18 19\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"20 21 22 23 24 25 26 27 28 29\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"30 31 32 33 34 35 36 37 38 39\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"40 41 42 43 44 45 46 47 48 49\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"50 51 52 53 54 55 56 57 58 59\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"60 61 62 63 64 65 66 67 68 69\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"70 71 72 73 74 75 76 77 78 79\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"80 81 82 83 84 85 86 87 88 89\" at=\"0x562fa91d9a70\"/>,\r\n    <NumpyArray format=\"l\" shape=\"10\" data=\"90 91 92 93 94 95 96 97 98 99\" at=\"0x562fa91d9a70\"/>\r\n]\r\n>>> # number of entries in each partition\r\n>>> ak.partitions(array)\r\n[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\r\n>>> # cumbersome way to iterate over partitions\r\n>>> start = 0\r\n>>> for count in ak.partitions(array):\r\n...     stop = start + count\r\n...     print(repr(array[start:stop]))\r\n...     start = stop\r\n... \r\n<Array [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] type='10 * int64'>\r\n<Array [10, 11, 12, 13, 14, ... 16, 17, 18, 19] type='10 * int64'>\r\n<Array [20, 21, 22, 23, 24, ... 26, 27, 28, 29] type='10 * int64'>\r\n<Array [30, 31, 32, 33, 34, ... 36, 37, 38, 39] type='10 * int64'>\r\n<Array [40, 41, 42, 43, 44, ... 46, 47, 48, 49] type='10 * int64'>\r\n<Array [50, 51, 52, 53, 54, ... 56, 57, 58, 59] type='10 * int64'>\r\n<Array [60, 61, 62, 63, 64, ... 66, 67, 68, 69] type='10 * int64'>\r\n<Array [70, 71, 72, 73, 74, ... 76, 77, 78, 79] type='10 * int64'>\r\n<Array [80, 81, 82, 83, 84, ... 86, 87, 88, 89] type='10 * int64'>\r\n<Array [90, 91, 92, 93, 94, ... 96, 97, 98, 99] type='10 * int64'>\r\n```\r\n\r\nThe biggest difference that partitions make is that every Awkward operation applies separately to each partition, returning a new partitioned array. the statement in the documentation is that these are not interface-visible differences (in the high-level view), but can be performance differences.",
  "created_at":"2021-04-07T20:38:16Z",
  "id":815246015,
  "issue":806,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTI0NjAxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-07T20:38:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Let me check this with the full example of problematic data.",
  "created_at":"2021-04-08T12:58:04Z",
  "id":815802987,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTgwMjk4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T12:58:04Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"I hadn't gotten this far in my email before writing https://github.com/scikit-hep/awkward-1.0/issues/805#issuecomment-815832985, but this solution seems too focused on the specific problem. It's checking for EmptyArray and IndexedOptionArray node types, when the programming error happened because `posaxis >= purelist_depth` shouldn't be allowed*, and therefore we (probably I) hadn't been considering what would happen in some specific case of that. Simple cases get stopped by the fact that they can't broadcast, and this more complex case missed a code block altogether (and hence, a variable didn't get defined).\r\n\r\nIf the wrong dimensionality is excluded up-front (before [line 2897](https://github.com/scikit-hep/awkward-1.0/blob/1492c4906df9f5bbf7b4f244de543188563015a4/src/awkward/operations/structure.py#L2897)), then the subsequent code can continue to be naive (and we get a clearer and more uniform error message for all such cases). I'd rather have a general fix that expresses a rule about what `axis` is allowed to be, given an array's dimension, than a specific check for layout node types that fixes this one case. The latter would be harder to understand and maintain in the future (and `ak.cartesian` is complicated enough!).\r\n\r\n*I haven't thought deeply about where the cut-off should be. I _think_ `0 <= posaxis < purelist_depth` is the right boundary, since a one-dimensional array has `purelist_depth == 1` and only `axis=0` is allowed for that case. @ianna, we can keep the tests you wrote for this PR, but just change the code to express this general rule about the dimension of the input arrays and `axis`.",
  "created_at":"2021-04-08T13:54:02Z",
  "id":815844737,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTg0NDczNw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-08T13:54:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna, disregard the above: see https://github.com/scikit-hep/awkward-1.0/issues/805#issuecomment-815855292.",
  "created_at":"2021-04-08T14:08:57Z",
  "id":815855988,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTg1NTk4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T14:08:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm not sure whether I should post here or on issue #805, but since everyone involved is subscribed to both, I'll do it here.\r\n\r\nFirst of all, disregard my request to exclude everything but `0 <= posaxis < purelist_depth` for `purelist_depth` of all inputs to `ak.cartesian` because that would exclude some good cases, like the one @lgray showed with a low-dimension array that gets broadcasted up to match the dimensionality of a high-dimension array. It _might_ be appropriate to require `0 <= posaxis < purelist_depth` for `purelist_depth` of _any_ (rather than _all_) inputs to `ak.cartesian`, but that would require some thought and doesn't address the real issue here.\r\n\r\nSorry for the confusion!\r\n\r\nAnyway, the real problem is in broadcasting itself, as `ak.broadcast_arrays` raises the same error message. It could be fixed like this:\r\n\r\n```diff\r\n--- a/src/awkward/_util.py\r\n+++ b/src/awkward/_util.py\r\n@@ -683,6 +683,7 @@ def broadcast_and_apply(  # noqa: C901\r\n                 [(str(i), combos.dtype) for i in range(len(tagslist))]\r\n             ).reshape(length)\r\n \r\n+            numoutputs = None\r\n             tags = nplike.empty(length, dtype=np.int8)\r\n             index = nplike.empty(length, dtype=np.int64)\r\n             outcontents = []\r\n@@ -691,7 +692,6 @@ def broadcast_and_apply(  # noqa: C901\r\n                 tags[mask] = tag\r\n                 index[mask] = nplike.arange(nplike.count_nonzero(mask))\r\n                 nextinputs = []\r\n-                numoutputs = None\r\n                 i = 0\r\n                 for x in inputs:\r\n                     if isinstance(x, uniontypes):\r\n@@ -707,15 +707,20 @@ def broadcast_and_apply(  # noqa: C901\r\n                     assert numoutputs == len(outcontents[-1])\r\n                 numoutputs = len(outcontents[-1])\r\n \r\n-            tags = ak.layout.Index8(tags)\r\n-            index = ak.layout.Index64(index)\r\n+            if numoutputs is None:\r\n+                return tuple(\r\n+                    x[0:0] if isinstance(x, ak.layout.Content) else x for x in inputs\r\n+                )\r\n \r\n-            return tuple(\r\n-                ak.layout.UnionArray8_64(\r\n-                    tags, index, [x[i] for x in outcontents]\r\n-                ).simplify()\r\n-                for i in range(numoutputs)\r\n-            )\r\n+            else:\r\n+                tags = ak.layout.Index8(tags)\r\n+                index = ak.layout.Index64(index)\r\n+                return tuple(\r\n+                    ak.layout.UnionArray8_64(\r\n+                        tags, index, [x[i] for x in outcontents]\r\n+                    ).simplify()\r\n+                    for i in range(numoutputs)\r\n+                )\r\n \r\n         elif any(isinstance(x, optiontypes) for x in inputs):\r\n             mask = None\r\n```\r\n\r\nMoving `numoutputs` out of the `for` loop makes its assertion in the loop stronger:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/1492c4906df9f5bbf7b4f244de543188563015a4/src/awkward/_util.py#L706-L708\r\n\r\nbut previously, it had no strength at all\u2014it was set to None immediately before checking for None, so that assertion happened in unreachable code, clearly not what was intended. So I think it's safe to say that I had intended `numoutputs = None` to be outside the loop (back when I was thinking deeply about broadcasting).\r\n\r\nIf `numoutputs` hasn't been set, then `outcontents` is an empty list. A UnionArray can't have zero contents, so the output has to be something other than a UnionArray. (Also, since the UnionArray that got created in the original code was immediately `.simplified`, it wasn't necessary for the output to have union-type.)\r\n\r\nAlso if `numoutputs` hasn't been set, then we know that `len(combos) == 0` and therefore `len(tagslist) == 0` and `length == 0`. So the output arrays are definitely going to be empty for this case (and wrapped as arrays of all None in a recursive step above this one). Instead of creating EmptyArrays, I sliced all input Contents to zero length so that they would retain their types:\r\n\r\n```python\r\n>>> ak.broadcast_arrays(candidate, ak.Array([[1, 2, 3], []]))\r\n[\r\n    <Array [None, None] type='2 * ?union[muon[\"pt\": float64], electron[\"pt\": float64]]'>,\r\n    <Array [None, None] type='2 * option[var * int64]'>\r\n]\r\n```\r\n\r\nAfter quite a few iterations on this comment, that seems to be the right solution.\r\n\r\n@ianna, does this pass your tests (with _no alterations_ to `ak.cartesian`)?",
  "created_at":"2021-04-08T14:53:23Z",
  "id":815889632,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTg4OTYzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T14:53:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, @ianna, since I have the files open, I can test it myself. It seems that everything is passing (after changing the test to not require a specific message). The next commit is the solution as described above.\r\n\r\n@lgray, does it fix your problem?",
  "created_at":"2021-04-08T15:01:21Z",
  "id":815896100,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTg5NjEwMA==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-04-08T15:01:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'll check in a moment on the full data just to make sure. Thanks!",
  "created_at":"2021-04-08T15:02:56Z",
  "id":815897322,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTg5NzMyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T15:02:56Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \r\n\r\nRunning the mre in #805 now yields:\r\n```\r\n[None, [({pt: 5}, {pt: 4})]]\r\nTraceback (most recent call last):\r\n  File \"mre.py\", line 21, in <module>\r\n    mre()\r\n  File \"mre.py\", line 16, in mre\r\n    x = ak.cartesian([candidate, jet])\r\n  File \"/Users/lagray/miniconda3/envs/coffea-jetmet/lib/python3.7/site-packages/awkward/operations/structure.py\", line 3092, in cartesian\r\n    assert isinstance(out, tuple) and len(out) == 1\r\nAssertionError\r\n```\r\n\r\n",
  "created_at":"2021-04-08T15:36:53Z",
  "id":815923154,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTkyMzE1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T15:36:53Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, that's because I checked `ak.broadcast_apply` and didn't check `ak.cartesian` directly (and @ianna's tests somehow missed this, too). The assertion is because `ak.broadcast_apply` expects as many arrays out as went in, `ak.cartesian` expects one array out, regardless of how many arrays went in, and my handling of the empty case has as many arrays coming out as in.\r\n\r\nTo get it right, I'll need to recurse down into the empty arrays, though that's just a formal thing to get the right number of empty arrays out as the operation requires (for _all_ operations that use `ak._util.broadcast_and_apply`, which are many).\r\n\r\nHowever, that raises the question of what _should_ the output type be? I can do this, for instance:\r\n\r\n```diff\r\n--- a/src/awkward/_util.py\r\n+++ b/src/awkward/_util.py\r\n@@ -708,9 +708,16 @@ def broadcast_and_apply(  # noqa: C901\r\n                 numoutputs = len(outcontents[-1])\r\n \r\n             if numoutputs is None:\r\n-                return tuple(\r\n-                    x[0:0] if isinstance(x, ak.layout.Content) else x for x in inputs\r\n-                )\r\n+                nextinputs = []\r\n+                for x in inputs:\r\n+                    if isinstance(x, uniontypes):\r\n+                        nextinputs.append(x[0:0].project(0))   # first type in the union\r\n+                    elif isinstance(x, ak.layout.Content):\r\n+                        nextinputs.append(x[0:0])\r\n+                    else:\r\n+                        nextinputs.append(x)\r\n+                return apply(nextinputs, depth, user)\r\n+\r\n             else:\r\n                 tags = ak.layout.Index8(tags)\r\n                 index = ak.layout.Index64(index)\r\n```\r\n\r\nin which case,\r\n\r\n```python\r\n>>> ak.cartesian([candidate, ak.Array([[1, 2, 3], []])])\r\n<Array [None, None] type='2 * option[var * (muon[\"pt\": float64], int64)]'>\r\n```\r\n\r\nIt \"unfairly\" picks the first type in the union to set the type for the empty output by doing a `.project(0)` (we know that every union has at least one content).\r\n\r\nIf the `candidate` array contained non-missing electrons and muons, the output type would have electrons and muons; if the `candidate` array contained non-missing electrons (but the muons were all missing), the output type would have electrons only; etc. Since this `candidate` array has everything missing, maybe it ought to not have any of those types. Instead of projecting the UnionArray to its first content, we could replace it with an EmptyArray, but that would change the number of dimensions and can interfere with broadcasting further down the recursive chain (not in this case). Any dimensions that are shared by all members of the union should be copied:\r\n\r\n```diff\r\n--- a/src/awkward/_util.py\r\n+++ b/src/awkward/_util.py\r\n@@ -708,9 +708,34 @@ def broadcast_and_apply(  # noqa: C901\r\n                 numoutputs = len(outcontents[-1])\r\n \r\n             if numoutputs is None:\r\n-                return tuple(\r\n-                    x[0:0] if isinstance(x, ak.layout.Content) else x for x in inputs\r\n-                )\r\n+                def copy_listtypes(contents):\r\n+                    if (\r\n+                        all(isinstance(x, ak.layout.RegularArray) for x in contents)\r\n+                        and all(x.size == contents[0].size for x in contents)\r\n+                    ):\r\n+                        return ak.layout.RegularArray(\r\n+                            copy_listtypes([x.content for x in contents]),\r\n+                            contents[0].size,\r\n+                            0,\r\n+                        )\r\n+                    elif all(isinstance(x, listtypes) for x in contents):\r\n+                        return ak.layout.ListOffsetArray64(\r\n+                            ak.layout.Index64(nplike.empty(0, dtype=np.int64)),\r\n+                            copy_listtypes([x.content for x in contents]),\r\n+                        )\r\n+                    else:\r\n+                        return ak.layout.EmptyArray()\r\n+\r\n+                nextinputs = []\r\n+                for x in inputs:\r\n+                    if isinstance(x, uniontypes):\r\n+                        nextinputs.append(copy_listtypes(x.contents))\r\n+                    elif isinstance(x, ak.layout.Content):\r\n+                        nextinputs.append(x[0:0])\r\n+                    else:\r\n+                        nextinputs.append(x)\r\n+                return apply(nextinputs, depth, user)\r\n+\r\n             else:\r\n                 tags = ak.layout.Index8(tags)\r\n                 index = ak.layout.Index64(index)\r\n```\r\n\r\nAnd now\r\n\r\n```python\r\n>>> ak.cartesian([candidate, ak.Array([[1, 2, 3], []])])\r\n<Array [None, None] type='2 * option[var * (bool, int64)]'>\r\n```\r\n\r\nThe \"bool\" comes from EmptyArray and might be another, unrelated issue.\r\n\r\n```python\r\n>>> ak.cartesian([ak.Array([None, None]), ak.Array([[1, 2, 3], []])])\r\n<Array [None, None] type='2 * option[var * (bool, int64)]'>\r\n```\r\n\r\nFor now, I'm going to focus on this issue, since it seems they can be solved indepenently.\r\n\r\nI'm going to add some tests and commit this.",
  "created_at":"2021-04-08T16:17:04Z",
  "id":815957804,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTk1NzgwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T16:17:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"About that \"bool\":\r\n\r\n```python\r\n>>> ak.broadcast_arrays(candidate, ak.Array([[1, 2, 3], []]))\r\n[\r\n    <Array [None, None] type='2 * option[var * bool]'>,\r\n    <Array [None, None] type='2 * option[var * int64]'>,\r\n]\r\n>>> ak.broadcast_arrays(ak.Array([None, None]), ak.Array([[1, 2, 3], []]))\r\n[\r\n    <Array [None, None] type='2 * option[var * bool]'>,\r\n    <Array [None, None] type='2 * option[var * int64]'>,\r\n]\r\n```\r\n\r\nIt doesn't have anything to do with `ak.cartesian`; the by-product of broadcasting an EmptyArray makes it boolean for some reason. Probably the right type is \"unknown,\" but I'm not sure and I'm not going to try to solve that now.",
  "created_at":"2021-04-08T16:20:39Z",
  "id":815960387,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTk2MDM4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T16:20:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"To me the output array should have the most covering type. Even in the case of selecting all electrons or all muons it should return union[electron, muon] as the underlying type, since as a user you have defined the input array as such and would not expect it to lose that type.",
  "created_at":"2021-04-08T16:23:17Z",
  "id":815962236,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTk2MjIzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T16:39:40Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"> To me the output array should have to most covering type. Even in the case of selecting all electrons or all muons it should return union[electron, muon] as the underlying type, since as a user you have defined the input array as such and would not expect it to lose that type.\r\n\r\nMost operations that can simplify UnionArrays do simplify them. For example, integers and booleans are considered different types, but since they both have a `~` operation (bitwise invert), it can be applied across a union of integers and booleans:\r\n\r\n```python\r\n>>> mixed = ak.Array([1, 2, False, True, True, 3])\r\n>>> mixed\r\n<Array [1, 2, False, True, True, 3] type='6 * union[int64, bool]'>\r\n>>> ~mixed\r\n<Array [-2, -3, True, False, False, -4] type='6 * union[int64, bool]'>\r\n```\r\n\r\nBut when you focus down on a subset that happens to consist of only one type, an operation on it simplifies the UnionArray.\r\n\r\n```python\r\n>>> mixed[[0, 1, -1]]\r\n<Array [1, 2, 3] type='3 * union[int64, bool]'>\r\n>>> ~mixed[[0, 1, -1]]\r\n<Array [-2, -3, -4] type='3 * int64'>           # not union[int64, bool]\r\n>>> ~mixed[[2, 3, 4]]\r\n<Array [True, False, False] type='3 * bool'>    # not union[int64, bool]\r\n```\r\n\r\nArguably, the first step, in which we sliced the union to get only integers, should have already been simplified and maybe that's a bug. Exactly the same thing, in which a user sliced to get only non-missing data, has repeatedly been reported as a bug (#487, #490).\r\n\r\nHere's another example where we'd really want the unions to be simplified but they aren't (and that, I think, is incorrect):\r\n\r\n```python\r\n>>> muon = ak.Array([[{\"pt\": 1.0}], []], with_name=\"muon\")\r\n>>> electron = ak.Array([[], [{\"pt\": 1.0}]], with_name=\"electron\")\r\n>>> leptons = ak.concatenate([muon, electron], axis=1)\r\n>>> leptons\r\n<Array [[{pt: 1}], [{pt: 1}]] type='2 * var * union[muon[\"pt\": float64], electro...'>\r\n>>> leptons.pt\r\n<Array [[1], [1]] type='2 * var * union[float64, float64]'>\r\n```\r\n\r\nThat ought to be `2 * var * float64`.\r\n\r\nIn a statically typed environment, the full type information would be carried through because different values could instantiate any of the possible types. Here, though, each typed array is also a set of values; we know how to constrict types. Constricting types does not reduce functionality\u2014the restricted type can be used anywhere the general type could have been used\u2014but the opposite is not necessarily true.\r\n\r\nIn the case of issue #806 specifically (and maybe I should have led with this), it wouldn't be possible to avoid constricting the types. On the next recursive level down, the union-ness needs to be removed or else this would be an infinite recursion. The whole issue comes up because\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/1492c4906df9f5bbf7b4f244de543188563015a4/src/awkward/_util.py#L681-L689\r\n\r\nrecurses down for all unique combinations of types that exist in the data: this is what would remove electron types if electrons never appeared in the data. It was an issue in your example because _no_ combinations appeared in the data, and we had to deal with that. Following the existing logic, which minimizes type explosion when broadcasting UnionArrays, the case of no combinations should be some EmptyArray thing.\r\n\r\nHowever, I guess one could argue that we _shouldn't_ avoid type explosion: if you want to take a Cartesian product of arrays with types `union(A, B)` and `union(C, D, E)` and get type `union((A, C), (A, D), (A, E), (B, C), (B, D), (B, E))`, regardless of whether the output arrays contain any such combinations, then we would write the loop differently and it would never get a no-combination case. But this goes against the grain of the \"simplify unions wherever possible\" philosophy that is applied throughout the codebase (and not applied consistently enough, as the above examples show).",
  "created_at":"2021-04-08T16:55:23Z",
  "id":815983541,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTk4MzU0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T16:55:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"So the answer I would give is somewhere in between, and perhaps not easily implementable.\r\n\r\nFor primitive types I think the simplifying is fine, largely you don't care where something that you've stripped down to being a float64, int, bool matters that much. You have factored out the bookkeeping or there was none to begin with.\r\n\r\nHowever for something that is given a `__record__` name specifically, keeping the provenance of unions begins to make some sense because you'd like to be able to look at some object in a vacuum and tell what it is.\r\nAs a concrete example, there is a significant physics difference between some `leptons`, which is a union of electrons, muons, and taus, but consists only selected electrons somehow, variable being:\r\n`?union[muon electron tau]`, and `?electron`\r\n\r\nOn a first reading of those types you'd think leptons was just a list of electrons in the latter case... And from the programming perspective it absolutely is, but from physics/analysis perspective it is very different. \r\n\r\nSo really: type allowing explosion for user defined types and simplification for primitive types.\r\n\r\nPerhaps this is just splitting hairs and we should tell users to be careful about bookkeeping?",
  "created_at":"2021-04-08T17:19:05Z",
  "id":815999257,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNTk5OTI1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T17:22:57Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Anyway, with this fix both the mre and the full example execute without failure.",
  "created_at":"2021-04-08T17:29:51Z",
  "id":816006548,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjAwNjU0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T17:29:51Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"This does it\u2014I'm going to merge now.\r\n\r\nThanks @ianna for starting this! It got pretty deep into the broadcasting logic, though.",
  "created_at":"2021-04-08T18:04:30Z",
  "id":816028219,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjAyODIxOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-08T18:04:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> As a concrete example, there is a significant physics difference between some `leptons`, which is a union of electrons, muons, and taus, but consists only selected electrons somehow, variable being:\r\n> `?union[muon electron tau]`, and `?electron`\r\n> \r\n> On a first reading of those types you'd think leptons was just a list of electrons in the latter case... And from the programming perspective it absolutely is, but from physics/analysis perspective it is very different.\r\n\r\nThe granularity of a type system is a choice, designed with a particular purpose in mind. When I said that Femtocode and OAMap \"started with a schema and fit values into that afterward\" and Awkward Array \"starts with the values and infers a type from that,\" I meant I was switching to this choice of putting values first, types second. The type that describes an array of values can be any type that correctly generalizes every value in the array. There's no maximum granularity other than \"Any,\" but there's a minimum granularity, which is the type that correctly matches all the values in the array and nothing more. This is a \"shrink-wrapped\" type: if you take an array of \"muons OR electrons\" and remove all the electrons, the minimal type for the result is just \"muons.\"\r\n\r\nAwkward Arrays don't have a history of what has happened to them, so if you do some operation on \"leptons\" that removes all the electrons (and taus, I guess), then what you have really is \"electrons.\" In a statically typed environment, in which you don't know what values you'll encounter, you _can't_ shrink-wrap it because you don't know if you'll encounter only electrons or not.\r\n\r\nI guess what I'm saying is that Awkward Array types are dynamic types\u2014you can't use them as type-safety for a program whose values are to be determined in the future. Maybe we should define some MyPy types for Awkward Arrays that would do that, but that's not what `array.type` is.\r\n\r\nAlso, I guess what I'm saying is that this was a design decision made in 2018 that is sufficiently fundamental that we won't be able to change it. As I've shown, there are cases that fall short of this \"ideal\" that I would consider bugs to fix, though from a type-safety point of view it's the opposite. Since every array does have a natural minimum type, it should be shrink-wrapped with `uniontype_simplify` to avoid UnionType nodes with empty content arrays.\r\n\r\nFinally, this was the design decision that ended up being useful: the biggest problem with Femtocode and OAMap was improving the user experience\u2014users had to carry around explicit Schemas and try (and fail) to fit data into them\u2014it didn't encourage tinkering the way I wanted it to. As with all dynamically typed things, now we can tinker, but the problem is scaling it up and discovering corner-cases...\r\n\r\n(Note the \"without runtime errors\" in the [description of Femtocode](https://github.com/diana-hep/femtocode#query-language-details). That would be possible with early, highly granular typing, but it comes at a cost of getting something half-working quickly. It's the same trade-off as in Rust: if it compiles, it will run forever. If it compiles... I've been in favor of very static type-checking and very dynamic at different times\u2014I've never found a way to reconcile the trade-offs.)",
  "created_at":"2021-04-08T18:40:25Z",
  "id":816052693,
  "issue":808,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjA1MjY5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-08T18:40:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna, could you take a look at this? I don't know how `ak.argsort` ended up returning non-integers\u2014I couldn't reproduce it with simpler examples than the one @lgray prepared.",
  "created_at":"2021-04-08T22:12:48Z",
  "id":816264607,
  "issue":811,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjI2NDYwNw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-08T22:12:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lgray - thanks for reporting it. The https://github.com/scikit-hep/awkward-1.0/pull/812 should fix it.",
  "created_at":"2021-04-09T07:47:36Z",
  "id":816488590,
  "issue":811,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjQ4ODU5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T07:47:36Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"fixed via #812 ",
  "created_at":"2021-04-09T15:18:15Z",
  "id":816757393,
  "issue":811,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjc1NzM5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T15:18:15Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Cool - I'll try this out momentarily.",
  "created_at":"2021-04-09T14:06:25Z",
  "id":816707979,
  "issue":812,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjcwNzk3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:06:25Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hmm hold on, still getting the same error despite rebuilding with this patch.",
  "created_at":"2021-04-09T14:20:38Z",
  "id":816717844,
  "issue":812,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjcxNzg0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:20:38Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK - that was my fault, stale install. Looks good. Let me test one more thing.",
  "created_at":"2021-04-09T14:29:34Z",
  "id":816723713,
  "issue":812,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjcyMzcxMw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-09T14:29:34Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Yes, if the length is zero, `ak.argsort` should not be returning a copy of the input array, as this won't have integer type. The method you have of creating an empty NumpyArray should work, though this should too:\r\n> \r\n> ```python\r\n> return NumpyArray(Index64(0))\r\n> ```\r\n> \r\n> which would allocate nothing, rather than one element that isn't viewed.\r\n> \r\n> But now it has me wondering if Indexes and NumpyArrays with no allocation (their array is a `nullptr`, pointing at the `0` address in memory) are correctly converted to NumPy arrays. It would be weird if there were any issues with that, as we've been using that technique for so long in so many places and never encountered any problems with it. I ought to do a test.\r\n\r\nyes, this works and is shorter:\r\n```python\r\n      return std::make_shared<NumpyArray>(Index64(0));\r\n```",
  "created_at":"2021-04-09T14:30:27Z",
  "id":816724336,
  "issue":812,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjcyNDMzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:30:27Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK - another test on a different file fails for a different reason now. :-)\r\n\r\nI'll post another issue once I get to the bottom of that.",
  "created_at":"2021-04-09T14:36:50Z",
  "id":816728604,
  "issue":812,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjcyODYwNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-09T14:36:50Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Please go ahead, merge, and mint an rc so I can test this over the whole input fileset.",
  "created_at":"2021-04-09T14:37:18Z",
  "id":816728862,
  "issue":812,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjcyODg2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:37:18Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Please go ahead, merge, and mint an rc so I can test this over the whole input fileset.\r\n\r\nThanks! I'll do it as soon as the tests pass. Looking forward to seeing a new issue ;-)",
  "created_at":"2021-04-09T14:40:16Z",
  "id":816730747,
  "issue":812,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjczMDc0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:40:16Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I agree. If `ak.zeros_like`, `ak.ones_like`, and `ak.empty_like` had a `dtype` argument, it would be implemented by simply running the function as usual and replacing the output with a call to [ak.values_astype](https://awkward-array.readthedocs.io/en/latest/_auto/ak.values_astype.html).\r\n\r\nHere's where the three \"like\" functions are defined (in terms of each other):\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/4a860e775502f9adb953524c35c5a2de8f7a3181/src/awkward/operations/structure.py#L1050-L1240\r\n\r\nI'd welcome a pull request that does this. (Note that `values_astype` is [defined in the same module](https://github.com/scikit-hep/awkward-1.0/blob/4a860e775502f9adb953524c35c5a2de8f7a3181/src/awkward/operations/structure.py#L4164-L4207), so its name does not need to be qualified.)",
  "created_at":"2021-04-09T14:06:30Z",
  "id":816708032,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjcwODAzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:06:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Neat, I'd love to give that a try, but I don't know the etiquette. Is it normal to include updates to the unit tests in the same pull request when adding a feature? ",
  "created_at":"2021-04-09T14:16:47Z",
  "id":816715226,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjcxNTIyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:16:47Z",
  "user":"MDQ6VXNlcjEyOTk2NzYz"
 },
 {
  "author_association":"MEMBER",
  "body":"Including a test would be ideal; the naming convention for test files is derived from either the issue number (0813) or the PR number (some PRs aren't associated with an explicit issue). That way, you don't need to modify any other test files, but you can use them as an example of format. Other than that, it's a normal PR process.\r\n\r\nSince this is a Python edit, you don't need to use the localbuild.py script described for incremental development in the [CONTRIBUTING.md](https://github.com/scikit-hep/awkward-1.0/blob/main/CONTRIBUTING.md), you can use\r\n\r\n```\r\npython -m pip install -v -e .\r\n```\r\n\r\nwhich I just learned about. The `-e` means that when you edit a Python file, it immediately shows up in the installed library. I need to add a description of that to CONTRIBUTING.md.",
  "created_at":"2021-04-09T14:36:07Z",
  "id":816728144,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjcyODE0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:36:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks, that makes sense. Also CONTRIBUTING.md is very helpful, that preempted a lot of the mistakes I would have made. I will give it a go now.",
  "created_at":"2021-04-09T14:51:55Z",
  "id":816739564,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjczOTU2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T14:51:55Z",
  "user":"MDQ6VXNlcjEyOTk2NzYz"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"For what it's worth, `python -m pip install -v -e .` didn't work for me but `python localbuild.py --pytest tests` did work. \r\nHere is the tail end of the install;\r\n```\r\n    [100%] Built target awkward\r\n    Install the project...\r\n    -- Install configuration: \"Release\"\r\n    -- Installing: /home/henry/Programs/awkward-1.0/build/lib.linux-x86_64-3.9/awkward/libawkward-static.a\r\n    -- Installing: /home/henry/Programs/awkward-1.0/build/lib.linux-x86_64-3.9/awkward/libawkward.so\r\n    -- Installing: /home/henry/Programs/awkward-1.0/build/lib.linux-x86_64-3.9/awkward/libawkward-cpu-kernels.so\r\n    -- Installing: /home/henry/Programs/awkward-1.0/build/lib.linux-x86_64-3.9/awkward/libawkward-cpu-kernels-static.a\r\n    -- Installing: /home/henry/Programs/awkward-1.0/build/lib.linux-x86_64-3.9/awkward/_ext.cpython-39-x86_64-linux-gnu.so\r\n    error: can't copy 'build/lib.linux-x86_64-3.9/awkward.cpython-39-x86_64-linux-gnu.so': doesn't exist or not a regular file\r\nERROR: Command errored out with exit status 1: /home/henry/Programs/anaconda3/envs/awkddev/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/home/henry/Programs/awkward-1.0/setup.py'\"'\"'; __file__='\"'\"'/home/henry/Programs/awkward-1.0/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps Check the logs for full command output.\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/home/henry/Programs/anaconda3/envs/awkddev/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 189, in _main\r\n    status = self.run(options, args)\r\n  File \"/home/henry/Programs/anaconda3/envs/awkddev/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 178, in wrapper\r\n    return func(self, options, args)\r\n  File \"/home/henry/Programs/anaconda3/envs/awkddev/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 391, in run\r\n    installed = install_given_reqs(\r\n  File \"/home/henry/Programs/anaconda3/envs/awkddev/lib/python3.9/site-packages/pip/_internal/req/__init__.py\", line 80, in install_given_reqs\r\n    requirement.install(\r\n  File \"/home/henry/Programs/anaconda3/envs/awkddev/lib/python3.9/site-packages/pip/_internal/req/req_install.py\", line 764, in install\r\n    install_editable_legacy(\r\n  File \"/home/henry/Programs/anaconda3/envs/awkddev/lib/python3.9/site-packages/pip/_internal/operations/install/editable_legacy.py\", line 49, in install_editable\r\n    call_subprocess(\r\n  File \"/home/henry/Programs/anaconda3/envs/awkddev/lib/python3.9/site-packages/pip/_internal/utils/subprocess.py\", line 258, in call_subprocess\r\n    raise InstallationSubprocessError(proc.returncode, command_desc)\r\npip._internal.exceptions.InstallationSubprocessError: Command errored out with exit status 1: /home/henry/Programs/anaconda3/envs/awkddev/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/home/henry/Programs/awkward-1.0/setup.py'\"'\"'; __file__='\"'\"'/home/henry/Programs/awkward-1.0/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps Check the logs for full command output.\r\nRemoved build tracker: '/tmp/pip-req-tracker-4lxgyewy'\r\n```\r\n\r\nNot that is really matters.",
  "created_at":"2021-04-09T15:30:01Z",
  "id":816765221,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjc2NTIyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T15:30:01Z",
  "user":"MDQ6VXNlcjEyOTk2NzYz"
 },
 {
  "author_association":"MEMBER",
  "body":"I've never tried `pip install -e .` with Awkward Array before; thanks for being the guinea pig! I had assumed that it would just work, and depending on how much it did incrementally, we might have been able to retire the localbuild.py script. Oh well, the localbuild.py script does work, even though it's non-standard (and it's only for developers\u2014the wheel-building and deployment is a standard `pip install`).",
  "created_at":"2021-04-09T16:36:36Z",
  "id":816806935,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjgwNjkzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T16:36:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"So I've been looking at this in https://github.com/scikit-hep/awkward-1.0/pull/814/files/82eb2f2ba94a06a553b14c9afc7449de7891a57a..63c8a0cf9db0e31407d16ef5c99838b4c64f21ec\r\nand I realise that `np.zeros_like(awkward_array)` will convert a bytestring or string like; `b\"dog\"` goes to `b\"0\"` or `\"dog\"` to `\"0\"`. As in, they remain bytestrings or strings, but take the new value. Conversely, if `ak.values_astype` is called on an array containing strings or bytestrings it specifically ignores the bytestrings and strings. Only things that are not bytestrings or strings get converted. \r\n\r\nThis means that a simply calling the values_astype function when the dtype parameter is set gives weird behaviour for arrays with strings or bytestrings in. The value gets changed, but the type does not. For example;\r\n```python3\r\nIn [0]: cat = ak.Array([5.6, b\"meow\"])\r\n\r\nIn [1]: np.zeros_like(cat, dtype=int)\r\nOut[1]: <Array [0, b''] type='2 * union[int64, bytes]'>\r\n\r\nIn [2]: np.ones_like(cat, dtype=int)\r\nOut[2]: <Array [1, b'1'] type='2 * union[int64, bytes]'>\r\n```\r\nWould the best solution be to add an arguments to `ak.values_astype` called `include_string` and `include_bytes`, which, if switched on would cause it to act on strings and bytes? May I include this in the same pull request?",
  "created_at":"2021-04-11T14:56:11Z",
  "id":817321055,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNzMyMTA1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-11T14:57:37Z",
  "user":"MDQ6VXNlcjEyOTk2NzYz"
 },
 {
  "author_association":"MEMBER",
  "body":"The string behavior (odd as it is) was made to agree with what NumPy does in its `astype` method. What does `np.ones_like(array, dtype=int)` do to a NumPy `array` of strings?\r\n\r\nIf it's different, the right way to handle it is to put most of the implementation of `values_astype` into a hidden function, `_values_astype`, with an extra argument so that `zeros_like`/`ones_like`/`empty_like` have a \"back door\" to specify a different behavior. But the public `ak.values_astype` shouldn't make this back door visible.\r\n\r\nThanks again and sorry for this unforeseen complication!",
  "created_at":"2021-04-11T15:31:09Z",
  "id":817326279,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNzMyNjI3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-11T15:31:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"No worries, writing an implementation is an excellent way to find hidden complexity in a simple idea. I think that is great fun.\r\n\r\nSo actually, that isn't the behaviour I get for numpy's `astype`. With numpy (versions 1.17.4 and 1.20.2) I get;\r\n```python3\r\n>> import numpy as np\r\n>> say = np.array(['4', '3', '1', '1', '0'])\r\n>> say.astype(int)\r\narray([4, 3, 1, 1, 0])\r\n```\r\nWhere as from awkward (version from github, 1.3.0rc1) I get;\r\n```python3\r\n>> import awkward as ak\r\n>> say = ak.from_iter(['4', '3', '1', '1', '0'])\r\n>> ak.values_astype(say, int)\r\n<Array ['4', '3', '1', '1', '0'] type='5 * string'>\r\n```\r\nLooking at the code in `values_astype`, it looks pretty intentional, so I wondered if the reason for calling it `values_astype` rather than just `astype` was that it only acts on numerics?",
  "created_at":"2021-04-12T06:49:35Z",
  "id":817535221,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNzUzNTIyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T06:49:35Z",
  "user":"MDQ6VXNlcjEyOTk2NzYz"
 },
 {
  "author_association":"MEMBER",
  "body":"I think we put in some effort to make `ak.zeros_like` do the right things for string types but we didn't do the same for `ak.values_astype`. That's not why it's called \"`values_astype`\" instead of just \"`astype`,\" the reason is because it doesn't try to coerce lists or records into a chosen dtype like `int`; it descends to the leaves of the tree and applies it there (a consideration that NumPy doesn't have to make). We briefly considered calling it \"`leaves_astype`,\" but that would be confusing, especially if the surrounding context involves ROOT TTrees.\r\n\r\nIt may be too much to try to fix `ak.values_astype` so that passing `dtype=int` changes strings representing numbers into the corresponding integers. That would go deep into the C++. For this project, it would be best to assume that `values_astype` has the correct behavior, use it, and someday we'll fix `values_astype`.",
  "created_at":"2021-04-12T14:04:19Z",
  "id":817839909,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNzgzOTkwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T14:04:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Here is the pull request for this; https://github.com/scikit-hep/awkward-1.0/pull/814\r\nI'm not sure if it goes to the right branch, and if the tests I wrote have the idiomatic form?\r\n\r\nAlso, I cannot figure out how to link it to this issue, sorry.",
  "created_at":"2021-04-13T14:16:01Z",
  "id":818771836,
  "issue":813,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODc3MTgzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T14:16:31Z",
  "user":"MDQ6VXNlcjEyOTk2NzYz"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Added the tests, not sure if my testing is idiomatic? ",
  "created_at":"2021-04-13T13:03:02Z",
  "id":818718264,
  "issue":814,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODcxODI2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T13:03:02Z",
  "user":"MDQ6VXNlcjEyOTk2NzYz"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sorry for the noise earlier, but here is a simple example:\r\n```python\r\nimport numpy as np\r\nimport awkward as ak\r\n\r\n\r\n@ak.mixin_class(ak.behavior)\r\nclass Point:\r\n    @ak.mixin_class_method(np.add, {\"Point\"})\r\n    def point_add(self, other):\r\n        return ak.zip(\r\n            {\"x\": self.x + other.x, \"y\": self.y + other.y}, with_name=\"Point\",\r\n        )\r\n\r\n\r\n@ak.mixin_class(ak.behavior)\r\nclass Point2(Point):\r\n    pass\r\n\r\n\r\ndef make(name):\r\n    return ak.zip(\r\n        {\r\n            \"x\": ak.Array([[1, 2], [3]]),\r\n            \"y\": ak.Array([[1, 2], [3]]),\r\n        },\r\n        with_name=name,\r\n    )\r\n\r\na = ak.zip(\r\n    {\r\n        \"x\": ak.Array([1, 1]),\r\n        \"y\": ak.Array([1, 1]),\r\n    },\r\n    with_name=\"Point\",\r\n)\r\nb, c = make(\"Point2\"), make(\"Point\")\r\na + b\r\nb + c\r\na + c\r\nd = ak.concatenate([b, c], axis=1)\r\na + d\r\n\r\ne = ak.concatenate([b[b.x < 0], c[c.x < 0]], axis=1)\r\na + e\r\n```\r\n\r\nproduces\r\n```\r\nTraceback (most recent call last):\r\n  File \"unionnone.py\", line 43, in <module>\r\n    a + e\r\n  File \"/usr/local/lib/python3.8/site-packages/numpy/lib/mixins.py\", line 21, in func\r\n    return ufunc(self, other)\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward/highlevel.py\", line 1379, in __array_ufunc__\r\n    return ak._connect._numpy.array_ufunc(ufunc, method, inputs, kwargs)\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward/_connect/_numpy.py\", line 206, in array_ufunc\r\n    out = ak._util.broadcast_and_apply(\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward/_util.py\", line 1066, in broadcast_and_apply\r\n    out = apply(broadcast_pack(inputs, isscalar), 0, user)\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward/_util.py\", line 828, in apply\r\n    outcontent = apply(nextinputs, depth + 1, user)\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward/_util.py\", line 880, in apply\r\n    outcontent = apply(nextinputs, depth + 1, user)\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward/_util.py\", line 662, in apply\r\n    return apply(nextinputs, depth, user)\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward/_util.py\", line 737, in apply\r\n    return apply(nextinputs, depth, user)\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward/_util.py\", line 644, in apply\r\n    return apply(nextinputs, depth, user)\r\n  File \"/Users/ncsmith/src/awkward-1.0/awkward/_util.py\", line 958, in apply\r\n    raise ValueError(\r\nValueError: cannot broadcast records in this type of operation\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.2.0rc2/src/awkward/_util.py#L960)\r\n```",
  "created_at":"2021-04-09T16:04:52Z",
  "id":816787221,
  "issue":815,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjc4NzIyMQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-09T16:07:47Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Great - it's good to have the same error being emitted with different array structures with the same trigger (empty lists).",
  "created_at":"2021-04-09T16:28:43Z",
  "id":816802134,
  "issue":815,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjgwMjEzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T16:28:43Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #816 fixes @nsmith-'s example, which is easier to reproduce, and is now [added to the tests](https://github.com/scikit-hep/awkward-1.0/blob/d11ac8f158cd0bec6d98f5e69d756e7e8e861960/tests/test_0815-broadcast-union-types-to-all-possibilities.py). (I'm going to try your example now, @lgray.)\r\n\r\nThe last line in @nsmith-'s example becomes\r\n\r\n```python\r\n>>> a + e\r\n<PointArray [[], []] type='2 * var * Point[\"x\": int64, \"y\": int64]'>\r\n```\r\n\r\nIn this fix, I backed off on the \"design choice\" I was talking about yesterday: this is a real problem; we can't have `+` stop working. I guess broadcasting can't be a type-shrinkwrapping operation, though others can and should be. Broadcasting is somewhat invisible\u2014arrays automatically change shape to fit the operation they're needed in\u2014maybe that's why it should be treated differently.\r\n\r\n[The change](https://github.com/scikit-hep/awkward-1.0/pull/816/commits/d11ac8f158cd0bec6d98f5e69d756e7e8e861960) replaces an iteration over `np.unique(combos)` with an `all_combos` that is constructed with `itertools.product`. (It's not an _O(n)_ iteration in the length of arrays _n_, but _O(t1*t2*t3* ...)_ in the number of tags _ti_ in each union involved in the broadcast.) In my non-empty test case,\r\n\r\n```python\r\n>>> c = ak.concatenate([\r\n...     ak.with_parameter([1, 2, 3], \"__array__\", \"one\"),\r\n...     ak.with_parameter([1.1, 2.2, 3.3], \"__array__\", \"two\"),\r\n... ])\r\n>>> c\r\n<Array [1, 2, 3, 1.1, 2.2, 3.3] type='6 * union[int64[parameters={\"__array__\": \"...'>\r\n>>> c.type\r\n6 * union[int64[parameters={\"__array__\": \"one\"}], float64[parameters={\"__array__\": \"two\"}]]\r\n>>> ak.broadcast_arrays(c, c)\r\n[\r\n    <Array [1, 2, 3, 1.1, 2.2, 3.3] type='6 * union[int64[parameters={\"__array__\": \"...'>,\r\n    <Array [1, 2, 3, 1.1, 2.2, 3.3] type='6 * union[int64[parameters={\"__array__\": \"...'>,\r\n]\r\n```\r\n\r\nThe unique combos are `(0, 0)` and `(1, 1)` because the union-tags of `c` and `c` line up. All of the combos are `(0, 0)`, `(0, 1)`, `(1, 0)`, and `(1, 1)`, including two cases that never happen in these arrays. The old code iterated through the unique combos, recursively descending on each, producing two `contents` in the output UnionArray. The new code iterates through all combos, recursively descending on each (though some of these descents are through empty arrays, after masking the combo with an array of all combos), producing four `contents` in the output UnionArray. (I just checked, and they get `simplified` away, but the fact that it recursively descended into those empty arrays passes the type through to the final arrays.)\r\n\r\nThe bit I added yesterday in PR #808 is gone now, since the `numoutputs is None` case is now impossible. To get to this `if` branch, there must be at least one UnionArray, and every UnionArray must have at least one content, so the `for` loop over `all_combos` must have at least one iteration.",
  "created_at":"2021-04-09T18:40:54Z",
  "id":816883643,
  "issue":815,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjg4MzY0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T18:40:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I ran @lgray's example and got a bunch of warnings about duplicate branches in Uproot, but it ran without errors.",
  "created_at":"2021-04-09T18:47:27Z",
  "id":816887174,
  "issue":815,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxNjg4NzE3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-09T18:47:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah,\r\n\r\n```python\r\n>>> ak.unflatten(a, [2, 2, 2, 2], axis=1)\r\n<Array [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] type='2 * var * var * int64'>\r\n```\r\n\r\nworks but the broadcasting in\r\n\r\n```python\r\n>>> ak.unflatten(a, 2, axis=1)\r\n```\r\n\r\nis failing.",
  "created_at":"2021-04-12T18:41:08Z",
  "id":818041458,
  "issue":819,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODA0MTQ1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T18:41:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Fixed in PR #820.",
  "created_at":"2021-04-12T18:48:54Z",
  "id":818046739,
  "issue":819,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODA0NjczOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T18:48:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for fixing this so quickly!",
  "created_at":"2021-04-13T09:38:35Z",
  "id":818599559,
  "issue":819,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODU5OTU1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T09:38:35Z",
  "user":"MDQ6VXNlcjYwMjkyMjQ3"
 },
 {
  "author_association":"MEMBER",
  "body":"Guessing from https://github.com/scikit-hep/awkward-1.0/runs/2290370917 , I'd say macOS Python 3.7 is missing too.",
  "created_at":"2021-04-12T19:42:28Z",
  "id":818083498,
  "issue":821,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODA4MzQ5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T19:42:28Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"When I work on the build infrastructure, one key piece is to only upload if all jobs complete successfully, via a deploy pipeline (Azure) or a `needs:` job (GHA). That element was part of the original azure-wheel-helpers, but seems to have been removed here. This is what it was supposed to protect against.\r\n\r\n@jpivarski maybe you can retrigger the two failed jobs?",
  "created_at":"2021-04-12T19:45:47Z",
  "id":818089267,
  "issue":821,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODA4OTI2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T19:45:47Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I've retriggered, and I would be in favor of this \"all or nothing\" logic.",
  "created_at":"2021-04-12T20:13:51Z",
  "id":818147933,
  "issue":821,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODE0NzkzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T20:13:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Retriggered, which solves the immediate issue. Should we leave this PR open until the \"all or nothing\" logic is (re)added?\r\n\r\nhttps://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=6207&view=results",
  "created_at":"2021-04-13T12:24:12Z",
  "id":818693494,
  "issue":821,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODY5MzQ5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T12:24:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll be adding that anyway when following the Scikit-HEP developer guidelines, so I think this is closable.",
  "created_at":"2021-04-14T18:07:38Z",
  "id":819725751,
  "issue":821,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxOTcyNTc1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-14T18:07:38Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- brought this up once (and I can't find the Awkward issue number), that `ak.sum` can't be overridden with custom behaviors the way that `np.add` can. At the time, I thought it would be difficult because whereas ufuncs like `np.add` are all handled at Python-level, `ak.sum` is a deep C++ routine. Now that I think about it again, though, that's not actually a blocker\u2014we could just check for custom overrides of `ak.sum` before entering C++. I'm not sure why I didn't think of that last time.\r\n\r\nHowever, there are some purely logical issues with it: overrides would _only_ be possible if `axis=-1`. Any other `axis` would mix values from different events, and I don't know how you would even express an override function for that. So if this behavioral override only applies to `axis=-1`, what should happen when `axis != -1`? Normal behavior? (That could be very surprising!) An error message? (Probably.)\r\n\r\nSo it's something to consider, but it would be a feature request on Awkward Array. I can transfer this issue there.",
  "created_at":"2021-04-12T22:50:11Z",
  "id":818293355,
  "issue":824,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODI5MzM1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T22:50:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the prompt answer @jpivarski! Is there a reason why the \"manual\" fix above (wrapping it in `ak.Record(.., with_name = ...)` didn't make the `.mass` property available? - happy to transfer this to awkward",
  "created_at":"2021-04-12T22:55:13Z",
  "id":818295476,
  "issue":824,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODI5NTQ3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T22:55:13Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"NONE",
  "body":"(though I notice `np.sum` is not preserving the type either) but I guess it's a ufunc vs non-ufunc issue)\r\n\r\n\r\n```\r\n>>> np.sum(arr,axis=-1)\r\n<Record {x: 3, y: 3.3, z: 0.3, t: 6.6} type='{\"x\": float64, \"y\": float64, \"z\": f...'>\r\n```",
  "created_at":"2021-04-12T23:01:34Z",
  "id":818297741,
  "issue":824,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODI5Nzc0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T23:02:37Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"MEMBER",
  "body":"> Thanks for the prompt answer @jpivarski! Is there a reason why the \"manual\" fix above (wrapping it in `ak.Record(.., with_name = ...)` didn't make the `.mass` property available?\r\n\r\nBecause of a long-standing but until now, never-noticed bug in `ak.behavior` handling for Records. Look at this thing: https://github.com/scikit-hep/awkward-1.0/pull/825/commits/3b694d6d35098ebd2db2615cfa7eb7e38aea2990 ! Assigning the name and updating the class have been out of order for maybe a year at least.\r\n\r\n> (though I notice `np.sum` is not preserving the type either) but I guess it's a ufunc vs non-ufunc issue)\r\n> \r\n> ```\r\n> >>> np.sum(arr,axis=-1)\r\n> <Record {x: 3, y: 3.3, z: 0.3, t: 6.6} type='{\"x\": float64, \"y\": float64, \"z\": f...'>\r\n> ```\r\n\r\n`np.sum` and `ak.sum` are the same thing; we use [NEP 18](https://numpy.org/neps/nep-0018-array-function-protocol.html) to override `np.sum` with our own, when there are any Awkward Arrays in the arguments. But `ak.sum` doesn't pass through the parameters, including the record name, because it reduces a dimension\u2014what had been XYZ probably doesn't exist now.\r\n\r\nEspecially if the `axis` is not `-1`:\r\n\r\n<img src=\"https://user-images.githubusercontent.com/1852447/114474503-01208f00-9bbc-11eb-9542-5bb244d8690f.png\" width=\"400\">\r\n\r\nThat mixes values from very different places.\r\n\r\n```python\r\n>>> array = ak.Array([[2, 3, 5], [], [None, 7], [11]])\r\n>>> ak.prod(array, axis=-1)\r\n<Array [30, 1, 7, 11] type='4 * int64'>\r\n>>> ak.prod(array, axis=-2)\r\n<Array [22, 21, 5] type='3 * int64'>\r\n```\r\n",
  "created_at":"2021-04-12T23:24:34Z",
  "id":818306718,
  "issue":824,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODMwNjcxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-12T23:24:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'm not sure if I made an issue, but the way I dealt with it in coffea vectors was to add a mixin method specifically to do the axis=-1 reduction: https://github.com/CoffeaTeam/coffea/blob/fca8e13faa8c8dac51ec4216abf9f83a9b0405c4/coffea/nanoevents/methods/vector.py#L512-L524",
  "created_at":"2021-04-13T01:50:16Z",
  "id":818371949,
  "issue":824,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODM3MTk0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T01:50:16Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!\r\n\r\n@all-contributors please add @HenryDayHall for code",
  "created_at":"2021-04-13T15:36:32Z",
  "id":818833668,
  "issue":826,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODgzMzY2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T15:36:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/827) to add @HenryDayHall! :tada:",
  "created_at":"2021-04-13T15:36:41Z",
  "id":818833767,
  "issue":826,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODgzMzc2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T15:36:41Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"The first issue is fixed in #829; see the test. I simply hadn't known about Arrow's `DataType(null)` and I'm glad the assertion caught it before it became a more confusing error later on. The appropriate translation of this type would be `?unknown`.\r\n\r\nAs for the second, I'm not sure what the symptom is. Here are some arrays with nullability on one or two levels, converted into StructArrays and Tables\u2014are any of these different from what you expect?\r\n\r\n```python\r\n>>> ak.to_arrow(ak.Array([{\"x\": 1}, {\"x\": None}, {\"x\": 3}]))\r\n<pyarrow.lib.StructArray object at 0x7fd9202439a0>\r\n-- is_valid: all not null\r\n-- child 0 type: int64\r\n  [\r\n    1,\r\n    null,\r\n    3\r\n  ]\r\n>>> ak.to_arrow(ak.Array([{\"x\": 1}, {\"x\": None}, None, {\"x\": 3}]))\r\n<pyarrow.lib.StructArray object at 0x7fd920243a00>\r\n-- is_valid:\r\n  [\r\n    true,\r\n    true,\r\n    false,\r\n    true\r\n  ]\r\n-- child 0 type: int64\r\n  [\r\n    1,\r\n    null,\r\n    null,\r\n    3\r\n  ]\r\n>>> ak.to_arrow_table(ak.Array([{\"x\": 1}, {\"x\": None}, {\"x\": 3}]))\r\npyarrow.Table\r\nx: int64\r\n>>> ak.to_arrow_table(ak.Array([{\"x\": 1}, {\"x\": None}, None, {\"x\": 3}]))\r\npyarrow.Table\r\n: struct<x: int64>\r\n  child 0, x: int64\r\n```",
  "created_at":"2021-04-13T16:28:43Z",
  "id":818872703,
  "issue":828,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODg3MjcwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T16:28:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks I saw. Great! Incredibly fast again.\r\n\r\nI need to investigate the second part a bit more (I was fiddling on low-level and built a \"valid\" awkward Array but was unable to save it via Arrow due to invalid Arrow/Parquet fields).\r\n\r\nAs tests passed and your fix works perfectly, I close this issue.",
  "created_at":"2021-04-13T17:07:09Z",
  "id":818899407,
  "issue":828,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxODg5OTQwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-13T17:07:09Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"This is harder than I thought it might be. I'll have to get back to it.",
  "created_at":"2021-04-16T23:02:26Z",
  "id":821704457,
  "issue":832,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTcwNDQ1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T23:02:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You know, if your goal is plotting, you can [ak.flatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.flatten.html) the array. That will get rid of the missing values in addition to getting rid of the lists, which you'd have to do for a histogramming function, anyway.",
  "created_at":"2021-04-16T23:07:30Z",
  "id":821714629,
  "issue":832,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTcxNDYyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T23:07:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"`ak.flatten()` doesn't remove `None` (I wouldn't have run into this issue if it did):\r\n\r\n```python\r\n>>> ak.flatten(ak.Array([[1], [2, None]]))\r\n<Array [1, 2, None] type='3 * ?int64'>\r\n```",
  "created_at":"2021-04-17T10:02:06Z",
  "id":821799393,
  "issue":832,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTc5OTM5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-17T10:02:06Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"NONE",
  "body":"Ah, so `ak.flatten` seems to only remove `None`s that are on the `axis-1` axis:\r\n\r\n```python\r\n>>> ak.flatten(ak.Array([[[0]], None, [[1], None], [[2, None]]]), axis=1)\r\n<Array [[0], [1], None, [2, None]] type='4 * option[var * ?int64]'>\r\n>>> ak.flatten(ak.Array([[[0]], None, [[1], None], [[2, None]]]), axis=2)\r\n<Array [[0], None, [1], [2, None]] type='4 * option[var * ?int64]'>\r\n```\r\n\r\nOther than `axis=0`, which removes `None`s on that axis:\r\n\r\n```python\r\n>>> ak.flatten(ak.Array([[[0]], None, [[1], None], [[2, None]]]), axis=0)\r\n<Array [[[0]], [[1], None], [[2, None]]] type='3 * var * option[var * ?int64]'>\r\n```\r\n\r\nIt looks like `axis=None` does what I want:\r\n\r\n```python\r\n>>> ak.flatten(ak.Array([[[0]], None, [[1], None], [[2, None]]]), axis=None)\r\n<Array [0, 1, 2] type='3 * int64'>\r\n```\r\n\r\nI was actually assuming that `axis=None` was the default like the reducers `sum`, `min`, `max`, etc. Wouldn't `axis=None` for `ak.flatten` be more consistent with those and with `np.ndarray.flatten`? My guess would be that the most common use of `ak.flatten`  is for histogramming anyway (it certainly is for me--I don't think I've ever used it for anything else).",
  "created_at":"2021-04-17T10:19:00Z",
  "id":821801275,
  "issue":832,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTgwMTI3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-17T10:35:44Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"I'm rethinking it because you're not the first person to say they expected the default `axis` of `ak.flatten` to be `None`. I would expect it to be `1` for consistency with functional programming, but then, I'd want the `axis` of the reducers like `ak.sum` to be `1` also but they're constrained by NumPy's behavior.\r\n\r\nIt would be hard to change\u2014a parameter default isn't the sort of thing that can go through a deprecation cycle, unless we change the name of \"flatten\" (and that's already a good name). This would be a good thing to ask about as a Discussion, under the \"deprecation\" category: whether we should change the default `axis` of `ak.flatten`, to get some feedback.",
  "created_at":"2021-04-17T12:45:47Z",
  "id":821817263,
  "issue":832,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTgxNzI2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-17T12:45:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, there was an idea about that: the name `ak.ravel`, which means \"completely flatten\" in NumPy, too.",
  "created_at":"2021-04-17T19:47:03Z",
  "id":821877563,
  "issue":832,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTg3NzU2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-17T19:47:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski weighing in here - when reading jagged branches of a tree that contain 0 or more entries, I noticed that I was always `flatten`ing / `fill_none`ing along the `axis=1` direction. I can now see why most people may well prefer having this as the default. It's only when I want to remove all jaggedness that I find myself preferring `axis=None` as a default.",
  "created_at":"2021-04-19T13:16:07Z",
  "id":822458316,
  "issue":832,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjQ1ODMxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-19T13:16:33Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Is there some sort of guarantee that this is the same CMake build? Some of these things are formatting (multi-line function calls, for instance), but some look substantial. Or if they were manual, intended edits, I'll defer to your expertise that they're the right things to do.\r\n\r\nThe tests pass, and that's good. The testing procedure currently doesn't test a deployment, though. It's possible for a change to successfully pass the build tests but then not create a manylinux audited wheel, for instance. I'm trying to think through all the things that _might_ go wrong, despite the success of the tests.",
  "created_at":"2021-04-19T17:21:24Z",
  "id":822640125,
  "issue":833,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjY0MDEyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-19T17:21:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The first commit are all manual, intentional edits. The second commit should be all automated formatting. I am somewhat hoping this is being tested; cibuildwheel would explicitly test the built wheel and make sure it also passes tests in a fresh environment; my old scripts do not do that.",
  "created_at":"2021-04-19T18:25:12Z",
  "id":822680072,
  "issue":833,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjY4MDA3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-19T18:25:12Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It appears the test was failing due to a missing `\"dtype\"` attribute. The check if it is present is added in `content.cpp#2226`. \r\n```python\r\n____________________________________________________________ test_sort_zero_length_arrays _____________________________________________________________\r\n\r\n    def test_sort_zero_length_arrays():\r\n        array = ak.layout.IndexedArray64(\r\n>           ak.layout.Index64([]), ak.layout.NumpyArray([1, 2, 3])\r\n        )\r\nE       AttributeError: 'list' object has no attribute 'dtype'\r\n\r\ntests/test_0074-argsort-and-sort.py:493: AttributeError\r\n\r\n```",
  "created_at":"2021-04-22T07:57:06Z",
  "id":824627460,
  "issue":835,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNDYyNzQ2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-22T07:57:06Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"I would be very interested on the `datetime` support. Thanks!",
  "created_at":"2021-04-23T16:51:00Z",
  "id":825785738,
  "issue":835,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNTc4NTczOA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-23T16:51:15Z",
  "user":"MDQ6VXNlcjI1NzI2MjQ1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> * [ ]  precision loss: for example, when a `np.datetime64(\"2020-05\")` is converted to seconds `np.datetime64(\"2020-05-01T20:56:24.000000\")`\r\n\r\n```python\r\n>>> dt = np.datetime64(\"2020-05\")\r\n>>> dt.dtype\r\ndtype('<M8[Y]')\r\n>>> dt2 = np.datetime64(\"2020-05-01T20:56:24.000000\")\r\n>>> dt2\r\nnumpy.datetime64('2020-05-01T20:56:24.000000')\r\n>>> dt2.astype(np.datetime64(1, 'M'))\r\nnumpy.datetime64('2020-05')\r\n```\r\n\r\nI think it could be left as is. @jpivarski ?\r\n\r\nIf not, possible solutions are to:\r\n1. convert all incoming `np.datetime64` to `'s'` - **what if a user may want to keep given units?**\r\n```python\r\n>>> dt.astype(np.int64)\r\n604\r\n>>> dt2.astype(np.int64)\r\n1588366584000000\r\n>>> dt.astype(np.datetime64(1, 'us')).astype(np.int64)\r\n1588291200000000\r\n>>> dt.astype(np.datetime64(1, 's')).astype(np.int64)\r\n1588291200\r\n```\r\n2. recalculate it later:\r\n```python\r\n>>> def leap_years_between(start,end):\r\n...     if start < end:\r\n...             return leap_years_before(end) - leap_years_before(start + 1)\r\n...     else:\r\n...             raise ValueError\r\n... \r\n>>> def leap_years_before(year):\r\n...     if year > 0:\r\n...             year = year - 1\r\n...             return (year / 4) - (year / 100) + (year / 400)\r\n...     else:\r\n...             raise ValueError\r\n... \r\n```\r\n",
  "created_at":"2021-06-03T15:47:30Z",
  "id":853973082,
  "issue":835,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1Mzk3MzA4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-03T15:47:30Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> I think it could be left as is. @jpivarski ?\r\n\r\nI don't see anything wrong here. Maybe you should ask the potential users of datetimes?",
  "created_at":"2021-06-03T15:50:36Z",
  "id":853975229,
  "issue":835,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1Mzk3NTIyOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2021-06-03T15:50:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski -here is an example where a pyarrow `date64` array is converted correctly to `NumpyArray`, but not to `Array` due to the units mismatch. I'm not sure how realistic it is.\r\n```python\r\n>>> parr = pa.array([datetime.datetime(2002, 1, 23), datetime.datetime(2019, 2, 20)], type=pa.date64())\r\n>>> parr.type\r\nDataType(date64[ms])\r\n>>> narr = ak.layout.NumpyArray(parr)\r\n>>> narr\r\n<NumpyArray format=\"M8[D]\" shape=\"2\" data=\"Wed Jan 23 00:00:00 2002\r\n Wed Feb 20 00:00:00 2019\r\n\" at=\"0x000109071000\"/>\r\n>>> array = ak.Array(parr)\r\n>>> array\r\n<Array [1970-01-01T00:16:51.744000000, ... ] type='2 * datetime'>\r\n```",
  "created_at":"2021-06-10T14:35:26Z",
  "id":858676967,
  "issue":835,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODY3Njk2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T14:35:26Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> but is there a way to get timezone information in datetime (e.g. `datetime[tz='UTC']`)?\r\n\r\nNumpy deprecated storing timezone information (at the end of [this paragraph](https://numpy.org/doc/stable/reference/arrays.datetime.html#basic-datetimes)). Overall, I would say a feature that is not really relevant. Though, `pyarrow` [does support timezones](https://arrow.apache.org/docs/python/generated/pyarrow.timestamp.html) and any persisted data will lose this information.\r\n\r\n(/cc @jpivarski )",
  "created_at":"2021-06-10T17:52:02Z",
  "id":858838992,
  "issue":835,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODgzODk5Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-10T17:53:09Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna To answer your question, this is not an intended use-case (and it works \"by accident\" because this pyarrow array happens to be flat and interpretable as a buffer):\r\n\r\n```python\r\n>>> parr = pa.array([datetime.datetime(2002, 1, 23), datetime.datetime(2019, 2, 20)], type=pa.date64())\r\n>>> parr.type\r\nDataType(date64[ms])\r\n>>> narr = ak.layout.NumpyArray(parr)\r\n```\r\n\r\nThe proper way to convert pyarrow data into Awkward Arrays is with `ak.from_arrow`.\r\n\r\nThe issues with pyarrow were because we rely on `to_pandas_dtype` to get a NumPy dtype (it's not well named) and this method is completely wrong for a lot of units. I'm going to file a bug report to the Arrow project.\r\n\r\nThe appearance of correct times in the low-level NumpyArray view was due to another bug\u2014the two bugs cancelled\u2014which was that the `scale` from datetime_util was an integer, but sometimes you need to scale down, not just up. I've promoted that quantity to `double` type (including the kernel that it's used in).\r\n\r\nI think it's done! I noticed that we can't take `datetime` objects in the `ak.from_iter` constructor, but neither can NumPy (it makes a `dtype=\"O\"` array). That might be nice to have, but it would be for another PR someday.\r\n\r\nCongrats! This was a long time coming!",
  "created_at":"2021-06-10T19:10:15Z",
  "id":858932233,
  "issue":835,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODkzMjIzMw==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":3,
   "total_count":3
  },
  "updated_at":"2021-06-10T19:10:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Here's the Arrow issue: https://issues.apache.org/jira/browse/ARROW-13040\r\n\r\nUntil we start taking whichever version has this fix as a minimum (Arrow 5? 6?), we'll have to use special cases:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/a5880b930b581bea95cee94c659764d540c72b5d/src/awkward/operations/convert.py#L2591-L2608\r\n\r\nand\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/a5880b930b581bea95cee94c659764d540c72b5d/src/awkward/operations/convert.py#L2819-L2829",
  "created_at":"2021-06-10T19:24:34Z",
  "id":858950992,
  "issue":835,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODk1MDk5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T19:24:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's not your mistake: booleans have to be treated specially for Arrow/Parquet because they're stored as bits, rather than bytes, and that code path didn't check for fixed-size dimensions != 1. That's why there's a length disagreement: the outer dimension is 1 and the inner dimension (what it's trying to copy into the buffer whose bytes will be packed into bits) is 2. I'm committing a fix now.",
  "created_at":"2021-04-15T17:01:09Z",
  "id":820586055,
  "issue":836,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMDU4NjA1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-15T17:01:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"NumPy doesn't always show the whole array, either, but its cut-off is much higher: at 1000 entries or so. Because of its regular structure, NumPy can also cut the array in a mood logical place, whereas there's no guarantee that an Awkward Array is going to have a non-misleading place to cut.\r\n\r\nIndependent of that, I also wanted the string representations to fit on one line\u2014the maximum length of the line was chosen so that it wouldn't create scroll bars on GitHub or StackOverflow. That makes it possible to just paste examples without making the code examples hard to navigate. They could, like NumPy, have multiple lines that never exceed a certain width, but that garbles dicts and lists of arrays. I'm surprised there aren't more complaints about that with NumPy\u2014it can be very hard to read.\r\n\r\nAnother consideration in creating string representations is that the array might contain virtual arrays and we don't want to trigger materialization too easily. Any array elements that get printed must be materialized (usually that means reading from disk or over a network), and the virtual arrays are usually set up such that each record field is independent and there are large row chunks that are independent. Since the string representation includes both ends of the array, the first and last chunk will be read, and if there's only room for two or three fields, then only those will be read. Making the string representation wider will mean more up-front data reading.\r\n\r\nThe `tolist` method will show everything (and IPython-like things will format it nicely), but for large arrays, you don't want to read everything and show everything.\r\n\r\nPerhaps the right thing to do is to give you more control over the representation. The `str` and `repr` functions don't take arguments, but maybe there can be a global parameter (or a context manager) that affects the allowed length of the strings. Or maybe a `show` method that takes arguments. It would be very easy to increase the length, it would be somewhat harder to add line breaks to keep the line width under a maximum (you'd have to get into `ak._util.minimally_touching_string`), and I don't know how one would ensure that the \"...\" only appears in logically meaningful places (i.e. balances parentheses). Currently, the algorithm adds tokens until the length would exceed a maximum\u2014to keep the parentheses balanced, you'd have to change the granular unit from tokens to syntactic groups. In many cases, there would be nothing to show because the smallest syntactic unit is too large.\r\n\r\nI'm going to label this a feature request.",
  "created_at":"2021-04-16T13:43:18Z",
  "id":821187384,
  "issue":838,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTE4NzM4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T13:43:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The problem here is that it's choosing confusing truncation points based solely on the string. It should not truncate across different level lists anymore than it should truncate the middle of a number. If truncating deep in a list is required, it should truncate each, like `<Array [[0], [], [0, 1, 2, ...], ..., [0], [], []] type='7 * var * int64'>` for example. One \"property-based\" test would be the number of `[` should match the number of `]` in the repr for all reprs.",
  "created_at":"2021-04-17T03:54:08Z",
  "id":821760783,
  "issue":838,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTc2MDc4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-17T03:54:08Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"PS: The test I mentioned only catches the first example, the inside lists being merged in truncation would not be caught by that test. Edit, actually I didn't read your final paragraph very well, looks like you already mentioned this. Oh, well, now it's here twice.",
  "created_at":"2021-04-17T03:55:11Z",
  "id":821760881,
  "issue":838,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTc2MDg4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-17T03:57:52Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"NONE",
  "body":"Just to add a bit to the list of unexpected behavior here--the ellipsis can even occur before any field names or data, which looks very strange to me:\r\n\r\n```python\r\n>>> type(a)\r\nawkward.highlevel.Record\r\n\r\n>>> a.tolist()\r\n{'nMuon': 2,\r\n 'Muon_pt': [10.763696670532227, 15.736522674560547],\r\n 'Muon_eta': [1.0668272972106934, -0.563786506652832],\r\n 'Muon_phi': [-0.03427272289991379, 2.5426154136657715],\r\n 'Muon_mass': [0.10565836727619171, 0.10565836727619171],\r\n 'Muon_charge': [-1, -1]}\r\n\r\n>>> a\r\n<Record ... 0.106], Muon_charge: [-1, -1]} type='{\"nMuon\": uint32, \"Muon_pt\": va...'>\r\n\r\n>>> print(a)\r\n... Muon_phi: [-0.0343, 2.54], Muon_mass: [0.106, 0.106], Muon_charge: [-1, -1]}\r\n```\r\n\r\nI would expect an ellipsis to be at the end or in the middle, but never at the beginning...",
  "created_at":"2021-08-26T11:37:04Z",
  "id":906328905,
  "issue":838,
  "node_id":"IC_kwDODBCWws42BXtJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-26T11:37:04Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, so I'm finally addressing this, but in the context of Awkward 2.x, rather than 1.x. PR #1122 introduces high-level `Array` and `Record` for v2, and one of the first things these wrappers need is `__repr__`/`__str__`. We have enough experience with the v1 `__repr__`/`__str__` to know that hiding structure for the sake of screen space is a problem\u2014structure is one of the most important features.\r\n\r\n(Historical note: Awkward 0.x ignored screen space as well. It cut the output to prevent large arrays from inundating the terminal, but it did so at the \"number of objects\" level, which can use a surprisingly large or small amount of screen space. From our experience of _v0_, we learned that getting output that fits in a GitHub or StackOverflow preformatted block without scrolling is a big deal!)\r\n\r\nWe can preserve screen space _and_ structure by allowing for more than one ellipsis (`...`). The decision of where to put the ellipses (plural) is an underconstrained problem\u2014with multiple lists taking up the slack, one could take more slack than another. However, assuming that each list is roughly the same size, we can ask a list to (greedily) try to use all the available space when there's only one of them and ask each list to try to use half of the available space when there's more than one of them, and that would have at least the first two using most of the space with an ellipsis for any remaining. Such an algorithm only wastes space if the list sizes are very unbalanced. A better algorithm, one that globally optimizes the use of space, would have to look ahead non-deterministically\u2014it would have to be some sort of fit, which is too complicated for just printing arrays on the screen. The greedy algorithm described above is good enough.\r\n\r\nHere's what it looks like, using an array in v1 and v2 (both are accessible in `main`; version 2.0 is being developed in a submodule).\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import awkward._v2.highlevel\r\n>>> from awkward._v2.tmp_for_testing import v1_to_v2\r\n>>> import numpy as np\r\n\r\n>>> layout = ak.from_iter([[0.0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]], highlevel=False)\r\n\r\n>>> ak.highlevel.Array(layout)\r\n<Array [[0, 1.1, 2.2], ... 6.6, 7.7, 8.8, 9.9]] type='5 * var * float64'>\r\n\r\n>>> ak._v2.highlevel.Array(v1_to_v2(layout))\r\n<Array [[0, 1.1, 2.2], [...], ..., [6.6, 7.7, 8.8, 9.9]] type='5 * var * flo...'>\r\n```\r\n\r\nThe v1 array has a single ellipsis, but it only hints at the structure with unbalanced brackets. The v2 array has two ellipses, and the brackets are guaranteed to be balanced, so the structure is more clear.\r\n\r\nHere's a bigger example that demonstrates the new multi-line `show` method:\r\n\r\n```python\r\n>>> layout = ak.from_iter([\r\n...     {\"x\": i * 1.1, \"y\": np.arange(i).tolist()} for i in range(100)\r\n... ], highlevel=False)\r\n\r\n>>> ak.highlevel.Array(layout)\r\n<Array [{x: 0, y: []}, ... 95, 96, 97, 98]}] type='100 * {\"x\": float64, \"y\": var...'>\r\n\r\n>>> ak._v2.highlevel.Array(v1_to_v2(layout))\r\n<Array [{x: 0, y: []}, {...}, ..., {x: 109, y: [...]}] type='100 * {x: float...'>\r\n\r\n>>> ak._v2.highlevel.Array(v1_to_v2(layout)).show(type=True)\r\ntype: 100 * {x: float64, y: var * int64}\r\n[{x: 0, y: []},\r\n {x: 1.1, y: [0]},\r\n {x: 2.2, y: [0, 1]},\r\n {x: 3.3, y: [0, 1, 2]},\r\n {x: 4.4, y: [0, 1, 2, 3]},\r\n {x: 5.5, y: [0, 1, 2, 3, 4]},\r\n {x: 6.6, y: [0, 1, 2, 3, 4, 5]},\r\n {x: 7.7, y: [0, 1, 2, 3, 4, 5, 6]},\r\n {x: 8.8, y: [0, 1, 2, 3, 4, 5, 6, 7]},\r\n {x: 9.9, y: [0, 1, 2, 3, ..., 5, 6, 7, 8]},\r\n ...,\r\n {x: 100, y: [0, 1, 2, 3, ..., 88, 89, 90]},\r\n {x: 101, y: [0, 1, 2, 3, ..., 89, 90, 91]},\r\n {x: 102, y: [0, 1, 2, 3, ..., 90, 91, 92]},\r\n {x: 103, y: [0, 1, 2, 3, ..., 91, 92, 93]},\r\n {x: 105, y: [0, 1, 2, 3, ..., 92, 93, 94]},\r\n {x: 106, y: [0, 1, 2, 3, ..., 93, 94, 95]},\r\n {x: 107, y: [0, 1, 2, 3, ..., 94, 95, 96]},\r\n {x: 108, y: [0, 1, 2, 3, ..., 95, 96, 97]},\r\n {x: 109, y: [0, 1, 2, 3, ..., 96, 97, 98]}]\r\n```\r\n\r\nThe new algorithm is guaranteed to satisfy two constraints: (1) the brackets are balanced, however deeply it manages to go, and (2) the output fits within the space budget, unless that budget is less than 5 (the length of `\"[...]\"`). The following demonstrates that using a `limit_cols` that gradually decreases from 80 to 0.\r\n\r\n```python\r\n>>> layout = ak.from_iter([[0.0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]], highlevel=False)\r\n>>> a = ak._v2.highlevel.Array(v1_to_v2(layout))\r\n>>> for width in range(80, -1, -1):\r\n...     print(\"-\" * width)\r\n...     print(a.show(limit_rows=1, limit_cols=width, stream=None))\r\n... \r\n--------------------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n-------------------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n------------------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n-----------------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n----------------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n---------------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n--------------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n-------------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n------------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n-----------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n----------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n---------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n--------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n-------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n------------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n-----------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], [], [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n----------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], ..., [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n---------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], ..., [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n--------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], ..., [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n-------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], ..., [5.5], [6.6, 7.7, 8.8, 9.9]]\r\n------------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], ..., [6.6, 7.7, 8.8, 9.9]]\r\n-----------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, 4.4], ..., [6.6, 7.7, 8.8, 9.9]]\r\n----------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, ...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n---------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, ...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n--------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, ...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n-------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, ...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n------------------------------------------------------\r\n[[0, 1.1, 2.2], [3.3, ...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n-----------------------------------------------------\r\n[[0, 1.1, 2.2], [...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n----------------------------------------------------\r\n[[0, 1.1, 2.2], [...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n---------------------------------------------------\r\n[[0, 1.1, 2.2], [...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n--------------------------------------------------\r\n[[0, 1.1, 2.2], [...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n-------------------------------------------------\r\n[[0, 1.1, 2.2], [...], ..., [6.6, 7.7, 8.8, 9.9]]\r\n------------------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, 7.7, 8.8, 9.9]]\r\n-----------------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, 7.7, 8.8, 9.9]]\r\n----------------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, 7.7, ..., 9.9]]\r\n---------------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, 7.7, ..., 9.9]]\r\n--------------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, 7.7, ..., 9.9]]\r\n-------------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, 7.7, ..., 9.9]]\r\n------------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, 7.7, ..., 9.9]]\r\n-----------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ..., 9.9]]\r\n----------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ..., 9.9]]\r\n---------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ..., 9.9]]\r\n--------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ..., 9.9]]\r\n-------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ..., 9.9]]\r\n------------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ...]]\r\n-----------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ...]]\r\n----------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ...]]\r\n---------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ...]]\r\n--------------------------------\r\n[[0, 1.1, 2.2], ..., [6.6, ...]]\r\n-------------------------------\r\n[[0, 1.1, 2.2], ..., [...]]\r\n------------------------------\r\n[[0, 1.1, 2.2], ..., [...]]\r\n-----------------------------\r\n[[0, 1.1, 2.2], ..., [...]]\r\n----------------------------\r\n[[0, 1.1, 2.2], ..., [...]]\r\n---------------------------\r\n[[0, 1.1, 2.2], ..., [...]]\r\n--------------------------\r\n[[0, 1.1, 2.2], ...]\r\n-------------------------\r\n[[0, 1.1, 2.2], ...]\r\n------------------------\r\n[[0, ..., 2.2], ...]\r\n-----------------------\r\n[[0, ..., 2.2], ...]\r\n----------------------\r\n[[0, ..., 2.2], ...]\r\n---------------------\r\n[[0, ..., 2.2], ...]\r\n--------------------\r\n[[0, ..., 2.2], ...]\r\n-------------------\r\n[[0, ...], ...]\r\n------------------\r\n[[0, ...], ...]\r\n-----------------\r\n[[0, ...], ...]\r\n----------------\r\n[[0, ...], ...]\r\n---------------\r\n[[0, ...], ...]\r\n--------------\r\n[[...], ...]\r\n-------------\r\n[[...], ...]\r\n------------\r\n[[...], ...]\r\n-----------\r\n[...]\r\n----------\r\n[...]\r\n---------\r\n[...]\r\n--------\r\n[...]\r\n-------\r\n[...]\r\n------\r\n[...]\r\n-----\r\n[...]\r\n----\r\n[...]\r\n---\r\n[...]\r\n--\r\n[...]\r\n-\r\n[...]\r\n\r\n[...]\r\n```\r\n\r\nSince this is getting merged into v2 now, I'll close this issue so that I don't forget to later. v2 isn't usable for data analysis, but when it is, this will be one of the enticements to switch over to it.\r\n\r\n(Note: the v1 \u2192 v2 differences are much smaller than the v0 \u2192 v1 differences and we won't be changing PyPI names like we did last time. The new transition will be much easier, I promise!)",
  "created_at":"2021-10-21T22:13:34Z",
  "id":949042273,
  "issue":838,
  "node_id":"IC_kwDODBCWws44kTxh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-21T22:13:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If you're interested in comparing them, the old algorithm walks over tokens from the left and from the right, adding them to the output until adding one more would exceed the budget. Tokens can be individual brackets, such as `[` and `]`, individual numbers, strings, a key, etc. Thus, it can't cut in the middle of a number, but it can cut in the middle of a structure. That's what you all were complaining about.\r\n\r\nHere's the old code:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/a440328f8097d22c2ba053fd117fed543829afc0/src/awkward/_util.py#L1557-L1770\r\n\r\nThe new algorithm walks from the left and from the right of each list, but only from the left for records. It walks over the tree and thus can guarantee bracket matching. Instead of having a single space budget applied to one forward iterator and one backward iterator, it has a space budget that it subdivides to children. As described in the previous comment, it divides up the budget greedily: if one list uses it up, there's nothing left for the next one. If a list has only one element, the whole budget is given to that list; if there's more than one, half of the remaining budget is given to each, which gives exponentially decreasing weight to items in the list: the first-visited gets the most, the second-visited gets less, the third-visited gets less, on down in powers of 2. When walking over a list, the order of visiting alternates between the left-to-right and the right-to-left, so lists put most of the emphasis on the first and last element.\r\n\r\nHere's the new code:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/6eb1e63edcf7b617fca9de5d551d6083f35a77a9/src/awkward/_v2/_prettyprint.py#L12-L239\r\n\r\nTests are done; time to merge the PR and close this issue.",
  "created_at":"2021-10-21T22:27:27Z",
  "id":949049039,
  "issue":838,
  "node_id":"IC_kwDODBCWws44kVbP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-21T22:27:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nope, it's not going to be as easy as modifying `flatten`. There's too much that hasn't been thought through.",
  "created_at":"2021-04-16T23:01:21Z",
  "id":821702224,
  "issue":840,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTcwMjIyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T23:01:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The tests are failing on platforms without numba, because the `__matmul__` indirectly depends upon `numba`. Do you have any suggestions here? I could add \r\n```\r\nnumba = pytest.importorskip(\"numba\")\r\n```\r\nto manually register the numba dependency, but this won't scale as more features have numbafied implementations.",
  "created_at":"2021-04-22T16:49:59Z",
  "id":825011444,
  "issue":846,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNTAxMTQ0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-22T16:50:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"`pytest.importorskip(\"numba\")` is a good thing to add (without the assignment because flake8 would complain about the variable not being used).\r\n\r\nThere are a lot of tests that only work if Numba exists. Matrix multiplication is the only feature in which that is hidden\u2014users don't have to manually import Numba to use it, so failures due to the lack of Numba could be a surprise\u2014but if this is a popular feature, it should be ported to the C++ layer, anyway.\r\n\r\n(This feature was originally written to answer an email. I didn't want to put a lot of time into something without knowing that it would be needed for analysis. Yours is the first use-case that I know about.)",
  "created_at":"2021-04-22T16:56:21Z",
  "id":825018002,
  "issue":846,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNTAxODAwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-22T16:56:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"OK, I'll modify the test accordingly. I should have put this comment in the PR, but it's been a long day :weary: \r\n\r\nI understand that the feature is not really a \"production one\", and I can see why it makes sense to keep things high level unless required. I'm not personally going to advocate for that yet, so once this issue is closed all is fine by me!",
  "created_at":"2021-04-22T17:16:47Z",
  "id":825038602,
  "issue":846,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNTAzODYwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-22T17:16:47Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"LGTM :+1: ",
  "created_at":"2021-04-22T15:29:08Z",
  "id":824943407,
  "issue":847,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNDk0MzQwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-22T15:29:08Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @agoose77 for test, code",
  "created_at":"2021-04-22T15:31:44Z",
  "id":824945482,
  "issue":847,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNDk0NTQ4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-22T15:31:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/848) to add @agoose77! :tada:",
  "created_at":"2021-04-22T15:31:53Z",
  "id":824945593,
  "issue":847,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNDk0NTU5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-22T15:31:53Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for merging :tada: ",
  "created_at":"2021-04-28T06:09:47Z",
  "id":828173047,
  "issue":847,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyODE3MzA0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-28T06:09:47Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"It only went so long because I hadn't noticed that it wasn't already merged. The fact that you had to fix a bug disabled the auto-merge (as it should, I think). So in my mind, it was merged\u2014I was surprised to see it in the list of open PRs.",
  "created_at":"2021-04-28T13:15:54Z",
  "id":828446542,
  "issue":847,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyODQ0NjU0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-28T13:15:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Oh wow, that's a good idea from a security perspective actually.",
  "created_at":"2021-04-28T13:30:13Z",
  "id":828456613,
  "issue":847,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyODQ1NjYxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-28T13:30:13Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks Jim :)",
  "created_at":"2021-04-22T16:44:00Z",
  "id":825006798,
  "issue":848,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNTAwNjc5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-22T16:44:00Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna, the ByteMaskedArray node is somehow being lost. I don't think this has anything in particular to do with using the `.mask` mechanism; it's probably an issue with ByteMaskedArray as opposed to IndexedOptionArray.",
  "created_at":"2021-04-26T14:04:31Z",
  "id":826862399,
  "issue":849,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNjg2MjM5OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-26T14:04:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@agoose77 - thanks for spotting the bug! https://github.com/scikit-hep/awkward-1.0/pull/850 should fix it.",
  "created_at":"2021-04-26T15:41:09Z",
  "id":826940583,
  "issue":849,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNjk0MDU4Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-04-26T15:41:09Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"fixed via #850",
  "created_at":"2021-04-28T13:33:35Z",
  "id":828459005,
  "issue":849,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyODQ1OTAwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-28T13:33:35Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"That is a _really_ nice delta! \r\n\r\n![image](https://user-images.githubusercontent.com/1248413/116111057-2fb76300-a6ae-11eb-84ff-89e575d8af62.png)\r\n\r\nThanks for fixing this so quickly.",
  "created_at":"2021-04-26T15:41:14Z",
  "id":826940642,
  "issue":850,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyNjk0MDY0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-26T15:41:14Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, but the part that's slower is the constant-time part. (The lines are horizontal with respect to array size.) This would only be an issue if the ufuncs were being called many times on small arrays, which is the pattern we recommend against. (By \"columnar,\" we mean that a single invocation of the function scales well with the size of the array.) By the time it starts to have a linear rise, the slope seems to be about the same as NumPy's slope\u2014which would only make sense, since it's NumPy that's actually running the ufunc on flattened data.\r\n\r\nThe performance goals are to have a good slope for the _O(n)_ part and for the _O(1)_ part be short on human timescales (well under 1 second), for interactivity. More build-up and tear-down has to be done before and after actually running the ufunc in order to streamline the ufunc itself: the _O(1)_ time is worse than it would be in a for-loop style implementation, for the sake of streamlining the _O(n)_ part. This will work badly for scripts that try to do tight loops that a ufunc many times, but that's an anti-pattern in NumPy, too.\r\n\r\nIf the algorithm can't be expressed in a columnar way, we recommend using Numba for that part. The implementation of Awkward Array access in Numba is completely different from the ufunc implementation, minimizing the build-up and tear-down of each iteration because this _is_ expected to be used in hot loops.",
  "created_at":"2021-04-28T17:28:55Z",
  "id":828639919,
  "issue":852,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyODYzOTkxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-28T17:28:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the reply. I now understand what your goal is performance wise. The big difference between the constant times Numpy and Awkward have made me a bit worried, but I also think I need to improve my code.\r\n\r\nBut there is still one thing: I don't think the slopes in case of masked arrays agree. Maybe I'm mistaken, but isn't there a factor of 10 between Numpy and Awkward here? For example for a length of 10**9 Awkward needs 17 s while Numpy only needs 1.3 s. These 15 s difference can't be just build-up and tear-down, or are they?\r\n```\r\nfor i in [10, 10**4, 10**6, 10**7, 10**8, 10**9]:\r\n    a = ak.Array(np.full(i, True)).mask[np.full(i, True)]\r\n    %timeit ~a\r\n\r\n381 \u00b5s \u00b1 4.36 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n419 \u00b5s \u00b1 929 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n4.9 ms \u00b1 12.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n131 ms \u00b1 660 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n1.42 s \u00b1 14.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n16.9 s \u00b1 443 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n\r\nfor i in [10, 10**4, 10**6, 10**7, 10**8, 10**9]:\r\n    a = ma.masked_array(np.full(i, True), mask=np.full(i, True))\r\n    %timeit ~a\r\n\r\n26.6 \u00b5s \u00b1 2.15 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n26.5 \u00b5s \u00b1 205 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n166 \u00b5s \u00b1 718 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n2.47 ms \u00b1 7.79 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n133 ms \u00b1 711 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n1.34 s \u00b1 2.09 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```",
  "created_at":"2021-04-28T20:48:44Z",
  "id":828768880,
  "issue":852,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyODc2ODg4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-28T20:53:42Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"Ah, I was eyeballing it from the plot. Yes, the masked arrays are scaling differently because the constant-time part is 0.05 seconds.\r\n\r\nOne thing that `ak.Array(np.full(i, True)).mask[np.full(i, True)]` does that `ma.masked_array(np.full(i, True), mask=np.full(i, True))` doesn't do when a ufunc is applied is that the Awkward version projects the non-masked-out data into a new array of only good values, applies the ufunc, and then un-projects back to the original shape. This prevents the ufunc from being executed on masked-out values. The NumPy version applies the ufunc to all data, whether masked-out or not.\r\n\r\nProjecting accounts for half the execution time, as shown below. Presumably, un-projecting accounts for the other half. (I don't remember, offhand, how to do that from Python.)\r\n\r\n```python\r\n>>> starttime = time.time(); tmp = ~a; print(time.time() - starttime)\r\n0.8057053089141846\r\n>>> starttime = time.time(); tmp = a.layout.project(); print(time.time() - starttime)\r\n0.4267578125\r\n```\r\n\r\nThis wasn't done for performance; it was done because an operation might fail to run on invalid values, though I was thinking about operations that fail for structural reasons when I instituted that policy. (For example, suppose you are doing a slice that takes the first element of each list, and have masked out all the empty lists. You wouldn't want it to fail with an error message saying, \"You can't get the first element of an empty list\" when your array _looks like_ it has no empty lists, the empties being hidden by a mask.) Ufuncs are a little different because failure to compute a function like `np.sqrt` returns `nan`, possibly with a warning. If we let ufuncs get executed on masked-out values, the worst that would happen is that you'd see a warning saying \"invalid value encountered\" when there are no invalid values in the _logical_ array (they're hidden behind a mask).\r\n\r\nSuppose we do that\u2014suppose we skip the projection and un-projection for ufuncs (only) in [BitMaskedArrays](https://awkward-array.readthedocs.io/en/latest/ak.layout.BitMaskedArray.html) and [ByteMaskedArrays](https://awkward-array.readthedocs.io/en/latest/ak.layout.ByteMaskedArray.html), but not [IndexedOptionArrays](https://awkward-array.readthedocs.io/en/latest/ak.layout.IndexedOptionArray.html), which have to project for structural reasons. (IndexedOptionArrays are the most common way to implement missing values, but `.mask` makes ByteMaskedArrays, which are equivalent to NumPy's masked arrays.)\r\n\r\nThat's PR #853, and you _do_ get that order of magnitude back.\r\n\r\n```python\r\n>>> starttime = time.time(); tmp = ~a; print(time.time() - starttime)   # Awkward\r\n0.07140612602233887\r\n>>> starttime = time.time(); tmp = ~b; print(time.time() - starttime)   # NumPy\r\n0.08176493644714355\r\n```\r\n\r\nHowever, I'm still on the fence about allowing it because it will lead to \"invalid value encountered\" warnings for arrays that don't look like they have any invalid values. For instance,\r\n\r\n```python\r\n>>> array = ak.Array([1.1, -2.2, 3.3, -4.4, 5.5])\r\n\r\n>>> array.mask[array >= 0]\r\n<Array [1.1, None, 3.3, None, 5.5] type='5 * ?float64'>\r\n\r\n>>> np.sqrt(array.mask[array >= 0])\r\n/home/jpivarski/irishep/awkward-1.0/awkward/_connect/_numpy.py:160: RuntimeWarning: invalid value encountered in sqrt\r\n  result = getattr(ufunc, method)(\r\n<Array [1.05, None, 1.82, None, 2.35] type='5 * ?float64'>\r\n```\r\n\r\nThat will be hard to explain. It's also difficult to make this performance optimization opt-in because it's not a method you're calling that you can pass an additional argument to, it's `~`.",
  "created_at":"2021-04-28T22:37:24Z",
  "id":828826464,
  "issue":852,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyODgyNjQ2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-28T22:37:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This indeed not an easy decision. I'm a bit in favor of raising warnings instead of copying the data, but I also see the downsides.\r\nIs there a way to keep only the projected version of the array plus its mask? Basically a special case of the IndexedOptionArray. If the masked array was represented this way, one could avoid this issue. But maybe that has downsides as well.",
  "created_at":"2021-04-29T11:17:01Z",
  "id":829144888,
  "issue":852,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyOTE0NDg4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-29T11:17:01Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"Well, it would likely be slower with IndexedOptionArrays, especially for ufuncs with more than one option-type argument (e.g. if it was `logical_and` instead of `logical_not`) because then the projection must _always_ be applied to ensure that the contents of both IndexedOptionArrays line up. At least with masking (by bits or bytes), the content has the same order as the logical array.\r\n\r\nMaybe there's a shortcut that's not quite as fast as the above but still avoids the most complicated gymnastics if all of the option-type arrays are (bit or byte) masked arrays.\r\n\r\nNo, it puts almost all of the time back:\r\n\r\n```python\r\n>>> starttime = time.time(); tmp = ~a; print(time.time() - starttime)\r\n0.48636579513549805\r\n```\r\n\r\nIt's really the projection that takes all that time, specifically this:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/6d33c18815ac39b9436ffddc493f7ab12b498858/src/awkward/_util.py#L744-L747\r\n\r\nNumPy's masked arrays are faster because they don't do that.\r\n\r\n```python\r\n>>> array = np.ma.masked_array([1.1, -2.2, 3.3, -4.4, 5.5], mask=[False, True, False, True, False])\r\n>>> array\r\nmasked_array(data=[1.1, --, 3.3, --, 5.5],\r\n             mask=[False,  True, False,  True, False],\r\n       fill_value=1e+20)\r\n>>> np.sqrt(array)\r\n<stdin>:1: RuntimeWarning: invalid value encountered in sqrt\r\nmasked_array(data=[1.0488088481701516, --, 1.816590212458495, --,\r\n                   2.345207879911715],\r\n             mask=[False,  True, False,  True, False],\r\n       fill_value=1e+20)\r\n```\r\n\r\nSo that's the trade-off. NumPy's masked arrays are simple enough that if you encounter something like the above, you can pretty quickly figure out why you get the warning. Awkward Arrays are more complex, so the warning would be more mysterious. You might not know, for instance, whether you have a ByteMaskedArray (and would therefore get the warning) or an IndexedOptionArray (and would not).",
  "created_at":"2021-04-29T13:03:44Z",
  "id":829219815,
  "issue":852,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyOTIxOTgxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-29T13:03:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"After some thought, I'm leaning strongly toward not including this optimization. Awkward missing values are different from NumPy masked arrays: they have extra features that NumPy does not (e.g. preventing calculation on invalid quantities). That feature comes with a performance cost. Apart from hardware accelerators, like the future GPU support, Awkward Array won't be faster than NumPy (and with GPU support, it won't be faster than CuPy). In some cases, it may be as fast as NumPy (or CuPy), if it doesn't provide any features beyond NumPy. In this case, it does, and that's why you're seeing slower performance.\r\n\r\nIf you have rectilinear data and want to give up the \"don't calculate on invalid values\" feature for performance, then switching to NumPy for that part of the calculation would be a good thing to do. If you don't have rectilinear data but want to control the loop for performance, that's what the Numba interface is for. The performance goals for the Awkward Numba extension were very different from the ufunc extension: as you've seen, the ufunc extension freely makes new arrays to ensure that operations are not performed on invalid data (which includes more complex cases than masked values in a masked array), but the Numba extension reduces an Awkward Array into a minimalist pointer called ArrayView and hard-codes data access by type in Numba's JIT-compilation.\r\n\r\nIt seems wrong to partially give up a feature\u2014preventing calculation on invalid values in Bit/ByteMaskedArrays, but not, for instance, slicing values from empty lists\u2014introducing surprising (and wrong) warning messages for the sake of performance when there's another way to get that performance. Unless you have strong objections, I'll be closing PR #853 and this issue.",
  "created_at":"2021-04-30T16:38:07Z",
  "id":830217507,
  "issue":852,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMDIxNzUwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-30T16:38:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The build fail, if it even reproduces itself, can't be related to the code change. I also [tested on main](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=6343&view=results) and there were no issues. Somehow, the reference count is coming up wrong in Numba in Python 3.6 (Mac and Linux).",
  "created_at":"2021-04-29T14:36:54Z",
  "id":829292958,
  "issue":853,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyOTI5Mjk1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-29T14:36:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This and the other inplace operators are very important because they avoid creating temporaries.",
  "created_at":"2021-05-01T09:08:19Z",
  "id":830588885,
  "issue":854,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMDU4ODg4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-01T09:08:19Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"Awkward Arrays are not mutable: #529. At least, the functions provided in the `awkward` package do not change arrays in place, but if an array is viewing memory owned elsewhere, by NumPy, for instance, then NumPy can change that memory in place ([docs](https://awkward-array.org/how-to-convert-numpy.html#mutability-of-awkward-arrays-from-numpy)).\r\n\r\nGiven that long-standing design choice, it would not be appropriate to implement any in-place operators, unless it is to override the error message.\r\n\r\nAlso, if the array has non-trivial structure (the reason you'd be using Awkward Array and not NumPy), it's not just one temporary array that's being created. In general, you can't control the number of temporary arrays at this level because the temporaries that need to be created depends on the exact layout of the array and the operation to perform. The way to control memory and CPU more tightly is to use Numba. When an Awkward Array is passed to Numba, no new arrays are created that you don't make yourself in the JIT'ed function, just a pointer to the existing arrays.\r\n\r\nI haven't been able to find the issue number, but we also talked about doing `np.asarray` inside a JIT'ed function to mutate it in-place, and I remember that the resolution of that was that it worked. Yes, just verified it:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import numpy as np\r\n>>> import numba as nb\r\n>>> @nb.njit\r\n... def f(array):\r\n...   for subarray in array:\r\n...     asnumpy = np.asarray(subarray)\r\n...     for i in range(len(asnumpy)):\r\n...       asnumpy[i] += 10\r\n... \r\n>>> array = ak.Array([[1, 2, 3], [], [4, 5]])\r\n>>> f(array)\r\n>>> array\r\n<Array [[11, 12, 13], [], [14, 15]] type='3 * var * int64'>\r\n```\r\n\r\nIn conclusion: implementing `&=` would go against a fundamental design choice that we've talked about several time before, but if you want to wrap Awkward Arrays with an interface that supports all the in-place operators and depends on Numba, you're free to do so. Such a package would have a different design philosophy than Awkward Array, since the JIT'ed functions you'd have to generate for each data structure is a very different kind of implementation than Awkward's implementation. Instead of making new buffers to build new array-tree structures with structural sharing, you'd be keeping the same buffers and array-tree structure and changing the values. The former does not require JIT; the latter does require JIT if it's going to be implemented for an infinite set of data types (e.g. the monoid generated by arbitrarily many levels of jaggedness).",
  "created_at":"2021-05-01T15:27:06Z",
  "id":830649834,
  "issue":854,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMDY0OTgzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-01T15:27:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Considering that the only awkward arrays that I am using are jagged arrays which have a very simple structure, it is very difficult to accept this argument. Why make arrays immutable? Why not differe tiate between arrays that must be immutable and allow others to be mutable? Your design choice collides with reality...",
  "created_at":"2021-05-01T16:06:46Z",
  "id":830655119,
  "issue":854,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMDY1NTExOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-01T16:06:46Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"Because most users don't use only simple jagged arrays, and introducing an exception would be confusing. But if you're only using one-level jagged arrays, then providing that functionality with a wrapper becomes even simpler\u2014you don't have to generate code to JIT-compile; the function I wrote above would be sufficient (well, one for each operator).\r\n\r\nMainly, though, we've talked about this multiple times. Mutability becomes confusing to _use_ (it's an _interface_ problem, not implementation) when all data types are considered. That's why, after experimenting with mutability in early versions, I concluded that it would bring more pain to include it. What you want is something specialized that clashes with the general library, so you can write it! You can make mutable jagged arrays a subclass of `ak.Array`, give it all the `__iadd__`, etc. methods, say\r\n\r\n```python\r\nak.behavior[\"mutable-jagged\"] = MutableJaggedArray\r\n```\r\n\r\nand set the `\"__array__\"` parameter of the ListArray/ListOffsetArray to `\"mutable-jagged\"`. Then, every time it's instantiated, it will have `MutableJaggedArray` type and have all the `__iadd__`, etc. methods.\r\n\r\nDon't keep asking me to put things in the general library that I've already found to be troublesome. (We've talked about it before, but by \"troublesome,\" I mean that users can't easily determine whether something is a view or a copy because, unlike a NumPy array, which is either all-view or all-copy, an Awkward Array is a tree of nodes, some of which may be views while others are copies. It gets complicated very quickly. That wouldn't happen if you only had simple jagged arrays, but most users have more complex arrays. I regularly work with people who have wide records, subrecords (usually representing systematic variations), index pointers, multiple levels of jaggedness, the four different option-types, and some people even use union-types, though I recommend against union types, if they're possible to avoid. And a back-door exception, like scikit-hep/awkward-0.x/pull/167, muddies the water too much: I had to keep explaining why this case worked and others didn't. \"No mutability in the general library\" is an easy enough rule that most people accept when they hear about it. _Especially_ since it's possible to wrap it with that functionality: casting as NumPy is a back door to mutability that can be built into a convenience library, if you limit the interface to data types in which mutability is not confusing.)",
  "created_at":"2021-05-01T17:39:38Z",
  "id":830667394,
  "issue":854,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMDY2NzM5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-01T17:39:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hey, I just checked the `JupyterBook doctest` logs, and there's no other warnings.",
  "created_at":"2021-05-03T17:10:15Z",
  "id":831400640,
  "issue":855,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMTQwMDY0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-03T17:10:15Z",
  "user":"MDQ6VXNlcjM5ODc4Njc1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@essoca - thanks for spotting it! Indeed it was a missing case. It should be fixed in https://github.com/scikit-hep/awkward-1.0/pull/858 \r\nThanks!",
  "created_at":"2021-05-07T12:12:42Z",
  "id":834319829,
  "issue":857,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDMxOTgyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T12:12:42Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Thanks! You solved this before I got a chance to read the issue.\r\n> \r\n> I'll merge it, because it looks complete in itself.\r\n\r\nthis issue reminded me how important @SantamRC summer project is :-)",
  "created_at":"2021-05-07T15:08:15Z",
  "id":834497192,
  "issue":858,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDQ5NzE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T15:08:15Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I see the error in the Docker environment, but not in my local setup. There's something going on here with versions.\r\n\r\n   * I noticed that there were a lot of developments since the last Awkward release (new 1.x versions are regularly released every two months, but in between that they're ad-hoc. So I triggered another release, which should be available on PyPI in less than an hour.\r\n   * I noticed that Arrow updated their major release number _again_, from 3.0.0 to 4.0.0. (They've adopted a very rapid versioning scheme!) Since this involves Parquet writing (which goes through Arrow), that's relevant.\r\n\r\nI could test the second point right away by updating my pyarrow package from 3.0.0 to 4.0.0 (I should be testing the latest, anyway). There were no problems there.\r\n\r\nThis new Awkward release ([1.3.0rc4](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.3.0rc4) includes a fix to the handling of regular dimensions in boolean arrays (#837). You might have independently discovered this same bug. If so, the new release should fix it, or [installing Awkward from GitHub](https://adamj.eu/tech/2019/03/11/pip-install-from-a-git-repository/). I'm trying the latter in the Docker container right now.",
  "created_at":"2021-05-10T13:54:10Z",
  "id":836730508,
  "issue":859,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNjczMDUwOA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-05-10T13:54:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It works in the Docker container if the latest Awkward is installed.\r\n\r\n```\r\nroot@3701fd712778:/# python3 -m pip install git+https://github.com/scikit-hep/awkward-1.0.git\r\nCollecting git+https://github.com/scikit-hep/awkward-1.0.git\r\n  Cloning https://github.com/scikit-hep/awkward-1.0.git to /tmp/pip-req-build-6ga7pmxl\r\n  Running command git clone -q https://github.com/scikit-hep/awkward-1.0.git /tmp/pip-req-build-6ga7pmxl\r\n  Running command git submodule update --init --recursive -q\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nRequirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.8/dist-packages (from awkward==1.3.0rc4) (1.20.2)\r\nRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from awkward==1.3.0rc4) (45.2.0)\r\nBuilding wheels for collected packages: awkward\r\n  Building wheel for awkward (PEP 517) ... done\r\n  Created wheel for awkward: filename=awkward-1.3.0rc4-cp38-cp38-linux_x86_64.whl size=8107834 sha256=d35368e8761cb11683aa05a028065fa9d43e7f7a92e0d12c22df06dd4d288079\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ffnf6_da/wheels/1d/0a/7d/c67b26366bf441e34371c6b286943132d3d8bb628ee67511c1\r\nSuccessfully built awkward\r\nInstalling collected packages: awkward\r\n  Attempting uninstall: awkward\r\n    Found existing installation: awkward 1.2.2\r\n    Uninstalling awkward-1.2.2:\r\n      Successfully uninstalled awkward-1.2.2\r\nSuccessfully installed awkward-1.3.0rc4\r\nWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\r\nroot@3701fd712778:/# python3\r\nPython 3.8.5 (default, Jan 27 2021, 15:41:15) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import awkward as ak\r\n>>> ak.__version__\r\n'1.3.0rc4'\r\n>>> \r\nroot@3701fd712778:/# python3 ~/problem.py \r\nAwkward version 1.3.0rc4\r\nSave success\r\n```\r\n\r\nThe second check would be to see if 1.3.0rc4 also solves the issue. ([See progress toward deployment here.](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=6362&view=results))\r\n\r\nI've decided that the numbering of pre-releases and bug-fix releases between the every-two-months release is confusing: pre-releases are numbered 1.3.0rcX, whereas bug-fixes are numbered 1.2.Y. After 1.3.0, I'll start numbering the pre-releases as 1.3.1rcX so that version number order will be chronological order.\r\n\r\n(The reason I need to do pre-candidates is because real releases have to be kept forever, since someone might pin to them, but pre-releases can be deleted. Storage space in PyPI is limited. However, since this regular-dimension, boolean array bug in `ak.to_arrow` was discovered twice, maybe this `1.3.0rc4 should also be released as 1.2.3, after you confirm that it fixes the error.)",
  "created_at":"2021-05-10T14:04:55Z",
  "id":836742823,
  "issue":859,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNjc0MjgyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-10T14:04:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"[Awkward 1.3.0rc4 has been released](https://pypi.org/project/awkward/1.3.0rc4/).",
  "created_at":"2021-05-10T14:36:21Z",
  "id":836778293,
  "issue":859,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNjc3ODI5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-10T14:36:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Can confirm, this issue is fixed in the new release. Many thanks!",
  "created_at":"2021-05-10T14:37:57Z",
  "id":836780141,
  "issue":859,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNjc4MDE0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-10T14:37:57Z",
  "user":"MDQ6VXNlcjEyOTk2NzYz"
 },
 {
  "author_association":"MEMBER",
  "body":"(Auto-merge is on, so this will be merged as soon as the tests pass. It will stop if any fall or if you commit something else before it's done.)",
  "created_at":"2021-05-10T12:03:25Z",
  "id":836612872,
  "issue":860,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNjYxMjg3Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-05-10T12:03:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Ah, I see. Nothing works as well as planned.",
  "created_at":"2021-05-11T11:46:10Z",
  "id":838330682,
  "issue":860,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzODMzMDY4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-11T11:46:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, this was a bug and #864 fixes it; thank you!",
  "created_at":"2021-05-12T12:53:57Z",
  "id":839748009,
  "issue":863,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzOTc0ODAwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-12T12:53:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Is the only unsupported field type datetime and timedelta? If so, then #835 would solve it. Nevertheless, I can check to see if it's building Forms for columns that aren't in the requested set of columns and prevent that, since they won't be used later. Thanks for catching this.",
  "created_at":"2021-05-16T22:45:07Z",
  "id":841888260,
  "issue":865,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MTg4ODI2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-16T22:45:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Is the only unsupported field type datetime and timedelta?  If so, then #835 would solve it.\r\n\r\nYes, great PR. I saw. Those field types seem to be the only ones affected. At least from what I experienced.\r\n\r\n> Nevertheless, I can check to see if it's building Forms for columns that aren't in the requested set of columns and prevent that, since they won't be used later.\r\n\r\nGreat, I think PR #867 should fix it.\r\n\r\n",
  "created_at":"2021-05-17T08:27:04Z",
  "id":842128545,
  "issue":865,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjEyODU0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-17T08:27:04Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"The second example is not exactly working, since `union[int64, int64]` needs to be simplified to `int64`.",
  "created_at":"2021-05-16T22:50:35Z",
  "id":841888913,
  "issue":866,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MTg4ODkxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-16T22:50:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Should be fixed in #870; the test is your example above.",
  "created_at":"2021-05-17T18:04:11Z",
  "id":842524539,
  "issue":866,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjUyNDUzOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-05-17T18:04:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@drahnreb thanks for contributing a fix! I'm not a maintainer, but I know that the general protocol is to add a test for any bug found. Is that something that you could do here?",
  "created_at":"2021-05-17T10:03:09Z",
  "id":842194470,
  "issue":867,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjE5NDQ3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-17T10:03:09Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like this takes a loop that was originally used to check for non-matching strings in the `columns` argument and adds to it a rewrite of `schema`, which is guaranteed to exist because one of the three paths above it,\r\n\r\n   * `if isinstance(source, str) and os.path.isdir(source)`\r\n   * `if not isinstance(source, str) and isinstance(source, Iterable)`\r\n   * `if multimode is None`\r\n\r\nhas to trigger, and each one defines it. (At the moment, I don't see what would happen if the iterable in the middle one has zero files. It looks like `schema` would be None, which would certainly raise an error somewhere. That might be a case that needs an explicit error message, though.)\r\n\r\n------------------\r\n\r\nAs for testing, it's a good idea, especially when it's easy. This case isn't easy, since you need to have a file that would trigger the bug, and that file would have to be created by or added to the testing suite (a frequent problem for testing Uproot). The test has to fail without the fix and succeed with it, which would be possible with a file containing datetimes until #835 is merged. Then it would succeed with or without the fix\u2014a temporary test. The fact that @drahnreb's workflow is unblocked by this fix _is_ a temporary test. The value of having a similar test in the testing suite is to make sure that the bug never regresses, but it wouldn't have the power to make such a guarantee after #835 is merged.\r\n\r\nMeanwhile, this is a heavily trafficked code path: every call to `ak.from_parquet` has to go through this point. It's testing in the sense of being sure there are no simple errors like misspelled variables.\r\n\r\nSo if you find a way to add a test (e.g. create a small Parquet file with a datetime column using pyarrow directly, with appropriate `pytest.importorskip`), then that would be great. But given the above, I'm not going to require it as a condition for merging the PR.\r\n\r\n@drahnreb, let me know if you're done.",
  "created_at":"2021-05-17T12:20:31Z",
  "id":842279609,
  "issue":867,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjI3OTYwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-17T12:20:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> It looks like this takes a loop that was originally used to check for non-matching strings in the `columns` argument and adds to it a rewrite of `schema`, which is guaranteed to exist because one of the three paths above it,\r\n> has to trigger, and each one defines it.\r\n\r\nSorry for hijacking it, but it looked so tempting with the given conditions and because of its proximity to the `columns is None` redefinition that has similar meaning (I hesitate to comment to keep your neat code style and it looked verbose enough).\r\n\r\n> The test has to fail without the fix and succeed with it, which would be possible with a file containing datetimes until #835 is merged. Then it would succeed with or without the fix\u2014a temporary test. The fact that @drahnreb's workflow is unblocked by this fix _is_ a temporary test. The value of having a similar test in the testing suite is to make sure that the bug never regresses, but it wouldn't have the power to make such a guarantee after #835 is merged.\r\n\r\nFor the exact same reason I did not include a test. I wanted to avoid any \"intermediate\" test as this PR is already in the pipeline and couldn't think of an appropriate alternative.\r\n\r\n> So if you find a way to add a test (e.g. create a small Parquet file with a datetime column using pyarrow directly, with appropriate `pytest.importorskip`), then that would be great.\r\n\r\nWe could try purposefully corrupt one column of a written parquet and try to read the file partially (with/without the corrupt one). But this sounds complicated, is fragile in its setup (we would need to find a way to corrupt the column metadata so that the column chunk for _one_ row group is lost) and begs the question of what is being tested (recover a corrupt file vs. testing against this issue).\r\n\r\n> @drahnreb, let me know if you're done.\r\n\r\nI'm done, thanks for your feedback \ud83d\udc4d\ud83c\udffc \r\n",
  "created_at":"2021-05-17T16:56:11Z",
  "id":842481998,
  "issue":867,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjQ4MTk5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-17T16:56:11Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! Then I'll accept it as-is.",
  "created_at":"2021-05-17T16:59:30Z",
  "id":842484215,
  "issue":867,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjQ4NDIxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-17T16:59:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"See #881 for the issue description :) If you agree that this is an expected use case (i.e. arguments to ufuncs are not array-castable) then I can push the test here. Otherwise, perhaps `vector` needs to provide a cast for `VectorObjectND`?",
  "created_at":"2021-05-28T10:41:23Z",
  "id":850327725,
  "issue":868,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDMyNzcyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-28T10:47:59Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, it's bumped up in priority; thanks!\r\n\r\nJust to be sure you know about it, the [ak.pad_none](https://awkward-array.readthedocs.io/en/latest/_auto/ak.pad_none.html) reference documentation has a few examples.",
  "created_at":"2021-05-18T17:00:29Z",
  "id":843363696,
  "issue":872,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MzM2MzY5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-18T17:00:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks! I found that too and it also seemed helpful!",
  "created_at":"2021-05-18T17:02:25Z",
  "id":843365106,
  "issue":872,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MzM2NTEwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-18T17:02:25Z",
  "user":"MDQ6VXNlcjM5NzE4MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Done in #943.",
  "created_at":"2021-06-18T20:40:41Z",
  "id":864263450,
  "issue":872,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDI2MzQ1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T20:40:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks @jpivarski !",
  "created_at":"2021-06-21T11:32:55Z",
  "id":864961062,
  "issue":872,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDk2MTA2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-21T11:32:55Z",
  "user":"MDQ6VXNlcjM5NzE4MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, I'm not sure. For one thing, ByteBehavior and CharBehavior can't be subclasses of bytes and str, respectively, because Python doesn't allow it (bytes and str are not normal classes and they have a different memory layout than Awkward Arrays; their methods wouldn't work). However, we can catch the output of `ak.Array.__getitem__`, `ak.Record.__getitem__`, and `ak.ArrayBuilder.__getitem__` to turn it into bytes and str.\r\n\r\nOn the one hand, this breaks the notion that Awkward types \"`byte`\" and \"`char`\" are just one-dimensional arrays of `uint8` with special behaviors. It also introduces extra copying: the bytes and str objects have a copy of the underlying data, rather than a view. That's more than a performance thing, it's a semantics difference if somebody was relying on the mutability of the underlying data.\r\n\r\nOn the other hand, having actual bytes and str objects is super-convenient. If \"`byte`\" and \"`char`\" are thought of as fundamental types like numbers, this is as natural as having `__getitem__` return Python int and float (which is what it does). On that last point, it's going backward from what @lukasheinrich wanted: to have `__getitem__` return a zero-dimensional object with `ak.Array` type, just as NumPy numbers are (sometimes!) zero-dimensional arrays. (This is something our approach to JAX integration needed, but we might be taking a different approach.) Finally, this is what happens in Numba-compiled functions: \"`byte`\" and \"`char`\" are returned as (lowered) bytes and str.\r\n\r\nActually, this should only ever be relevant for small-sampling, in which you're extracting a single string, rather than performing a vectorized operation.\r\n\r\nEven if the PR passes all tests, I'm going to leave this open for discussion for a while. I'm not sure it's the right thing to do, but I'm convinceable.",
  "created_at":"2021-05-19T18:25:17Z",
  "id":844358443,
  "issue":873,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NDM1ODQ0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-19T18:25:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yeah, handling this in `__getitem__` as in the PR seems reasonable to me. This is also consistent with the current behavior of `__iter__`, which *does* yield `str` (despite `__getitem__` not doing so) for reasons I don't entirely understand...",
  "created_at":"2021-05-20T23:20:21Z",
  "id":845545059,
  "issue":873,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTU0NTA1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-20T23:20:21Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, the fact that `__iter__` does makes the case stronger. (It's because `__iter__` is a non-recursive version of `tolist`\u2014that was the training there.)",
  "created_at":"2021-05-21T10:30:16Z",
  "id":845853511,
  "issue":873,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTg1MzUxMQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-05-21T10:30:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"See discussion on #873. Don't merge this PR until we're sure it's what we want to do!",
  "created_at":"2021-05-19T18:28:17Z",
  "id":844360597,
  "issue":874,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NDM2MDU5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-19T18:28:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nobody has spoken up on #873, but I've become convinced. NumPy returns a custom type,\r\n\r\n```python\r\n>>> type(numpy.array([\"one\", \"two\", \"three\"])[0])\r\n<class 'numpy.str_'>\r\n```\r\n\r\nbut this type behaves like a Python `str`:\r\n\r\n```python\r\n>>> numpy.array([\"one\", \"two\", \"three\"])[0] + numpy.array([\"one\", \"two\", \"three\"])[1]\r\n'onetwo'\r\n```\r\n\r\nand _not_ like an array. In fact, `np.str_` is a direct subclass of `str` with a few number-like features:\r\n\r\nhttps://github.com/numpy/numpy/blob/75f852edf94a7293e7982ad516bee314d7187c2d/numpy/__init__.pyi#L3418-L3432\r\n\r\nNamely,\r\n\r\n```python\r\n>>> numpy.array([\"one\", \"two\", \"three\"])[0].real\r\n'one'\r\n>>> numpy.array([\"one\", \"two\", \"three\"])[0].imag\r\n''\r\n```\r\n\r\nSo sure, we'll return Python string objects from `__getitem__`.",
  "created_at":"2021-06-15T21:34:05Z",
  "id":861848924,
  "issue":874,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg0ODkyNA==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-06-15T21:34:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"On issue 1, `ak.layout.EmptyArray` converts to a `float64` array because empty NumPy arrays are assumed to be `float64`. This logic is reproduced in the `to_arrow` function, but it doesn't have to be:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/3dbf8c87d78d98332b6629607841d8610dcca072/src/awkward/operations/convert.py#L2033-L2034\r\n\r\nCan we instead make a length-zero `pyarrow.Array` with null type? Is this appropriate?\r\n\r\n```python\r\n>>> pyarrow.Array.from_buffers(pyarrow.null(), 0, [pyarrow.py_buffer(np.array([]))])\r\n<pyarrow.lib.NullArray object at 0x7fe7bae72700>\r\n0 nulls\r\n```\r\n\r\nOr this?\r\n\r\n```python\r\n>>> pyarrow.Array.from_buffers(pyarrow.null(), 0, [None])\r\n<pyarrow.lib.NullArray object at 0x7fe7bae819a0>\r\n0 nulls\r\n```\r\n\r\nProbably the latter, because\r\n\r\n```python\r\n>>> null_table.to_batches()[0].column(0).buffers()\r\n[None]\r\n```\r\n\r\n---------------------------\r\n\r\nOn issue 2, it's because an `arrow_type` of `DataType(null)` gets converted into a NumPy kind `\"O\"` dtype with\r\n\r\n```python\r\n>>> arrow_type = pq.ParquetFile(\"null_typed_table.parquet\").schema_arrow.field(\"null_col\").type\r\n>>> arrow_type\r\nDataType(null)\r\n>>> arrow_type.to_pandas_dtype()\r\n<class 'numpy.object_'>\r\n```\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/3dbf8c87d78d98332b6629607841d8610dcca072/src/awkward/operations/convert.py#L3178-L3180\r\n\r\nThis null is an exceptional case that apparently shouldn't be checked through `to_pandas_dtype`. This works:\r\n\r\n```python\r\n>>> arrow_type == pyarrow.null()\r\nTrue\r\n```",
  "created_at":"2021-05-24T17:50:11Z",
  "id":847223304,
  "issue":875,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NzIyMzMwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-24T17:50:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll fix 'em.",
  "created_at":"2021-05-24T19:16:47Z",
  "id":847274379,
  "issue":875,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NzI3NDM3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-24T19:16:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #878 does it: see the tests. Cheers!",
  "created_at":"2021-05-24T19:41:02Z",
  "id":847287486,
  "issue":875,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NzI4NzQ4Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-05-24T19:41:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I confirm:\r\n\r\n```python\r\n>>> i = ak.Array([0, 1, 2, 3, 4, 5])\r\n>>> x = ak.Array([1.1, 2.2, 3.3, 4.4, 5.5, 6.6])\r\n>>> x[i[3:4].mask[[True]]]\r\n<Array [1.1] type='1 * ?float64'>\r\n>>> x[i[3:4][[True]]]\r\n<Array [4.4] type='1 * float64'>\r\n```\r\n\r\nThe problem isn't in converting this index (`i`) into a slice:\r\n\r\n```python\r\n>>> ak._ext._slice_tostring(i[3:4].mask[[True]])\r\n'[missing([0], array([3]))]'\r\n>>> ak._ext._slice_tostring(i[3:4][[True]])\r\n'[array([3])]'\r\n>>> ak._ext._slice_tostring(i[3:4])\r\n'[array([3])]'\r\n```\r\n\r\nbut apparently, the `SliceMissing` is ignoring an offset at some level. I'm looking into it.",
  "created_at":"2021-05-24T18:13:11Z",
  "id":847236096,
  "issue":876,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NzIzNjA5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-24T18:13:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It also doesn't require the target array (`x`) to be flat:\r\n\r\n```python\r\n>>> i = ak.Array([0, 1, 2])\r\n>>> x = ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]])\r\n>>> x[i[2:][[True]]]\r\n<Array [[4.4, 5.5]] type='1 * var * float64'>\r\n>>> x[i[2:].mask[[True]]]\r\n<Array [[1.1, 2.2, 3.3]] type='1 * option[var * float64]'>\r\n```",
  "created_at":"2021-05-24T18:21:23Z",
  "id":847240277,
  "issue":876,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NzI0MDI3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-24T18:21:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Almost a year ago, all of the kernel functions were converted from taking array offsets as arguments to just being given the correct pointer position. To make this easier, an `Index64::data` method was added that returns the pointer position with offset accounted for, but this was a corner case that still had `.ptr().get()` instead of `.data()`.\r\n\r\nc8821f7 fixes it, but I'm going to look for any other instances of the old idiom. It's disturbing to get the wrong answer from a slice\u2014thanks a lot for finding this and isolating it so it could be easily fixed!",
  "created_at":"2021-05-24T19:05:02Z",
  "id":847268132,
  "issue":876,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NzI2ODEzMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-05-24T19:05:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Can this please be integrated into a bugfix release?",
  "created_at":"2021-06-01T15:23:08Z",
  "id":852214807,
  "issue":876,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MjIxNDgwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-01T15:23:08Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, it's time for a new minor release: [1.3.0](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.3.0).\r\n\r\n[Track its progress here.](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=6444&view=results)",
  "created_at":"2021-06-01T17:18:22Z",
  "id":852300058,
  "issue":876,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MjMwMDA1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-01T17:18:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Awesome :D",
  "created_at":"2021-06-01T17:27:43Z",
  "id":852308937,
  "issue":876,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MjMwODkzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-01T17:27:43Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I know that the following is broadcasted and is working as expected (to get `unknown` typed):\r\n`what=[None]`\r\nor\r\n`what=[None for _ in range(len(test))]` (`if len(test) != 0`)\r\n\r\nBut the above should still raise a ValueError early (if not broadcasted to one of those alternatives).\r\n\r\n\r\n",
  "created_at":"2021-05-25T19:29:23Z",
  "id":848202824,
  "issue":879,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODIwMjgyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T19:29:23Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, if `what is None` on the line you linked to, then the appropriate thing to convert `what` into is\r\n\r\n```python\r\n>>> what = ak.layout.IndexedOptionArray64(\r\n...     ak.layout.Index64(nplike.full(len(base), -1, np.int64)),\r\n...     ak.layout.EmptyArray()\r\n... )\r\n>>> what\r\n<IndexedOptionArray64>\r\n    <index><Index64 i=\"[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\" offset=\"0\" length=\"10\"/></index>\r\n    <content><EmptyArray/></content>\r\n</IndexedOptionArray64>\r\n>>> ak.Array(what)\r\n<Array [None, None, None, ... None, None, None] type='10 * ?unknown'>\r\n```\r\n\r\nIf you could make a PR, I'd really appreciate it. Thanks!",
  "created_at":"2021-05-25T19:43:32Z",
  "id":848210784,
  "issue":879,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODIxMDc4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T19:43:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Will do, including test.",
  "created_at":"2021-05-25T19:47:34Z",
  "id":848212960,
  "issue":879,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODIxMjk2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T19:47:34Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski is your suggestion here to special-case the `what is None` argument to `with_field` such that it is broadcast to the array shape? \r\n\r\nEDIT: I always forget the UK is a bit ahead of the states! I've made a PR #880 where we can discuss this :)\r\n",
  "created_at":"2021-05-28T09:30:40Z",
  "id":850286195,
  "issue":879,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDI4NjE5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-28T10:02:03Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"(I want to make sure that @drahnreb is on this thread.)\r\n\r\n------\r\n\r\nYes, this is what I meant. Also, with this fix, the following should correctly at a column of Nones:\r\n\r\n```python\r\nx[\"z\"] = None\r\n```\r\n\r\nSince that's a synonym for the `ak.with_field` (changing the `ak.Array` wrapper in place).",
  "created_at":"2021-05-28T11:07:04Z",
  "id":850340604,
  "issue":880,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDM0MDYwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-28T11:07:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for mentioning me and the PR.\r\n\r\nThe bugfix looks good to me. I would want extend the code coverage for testing. I have prepared an improvement. I have an improvement ready on your branch, could you unprotect the `bugfix-879` branch on your fork for me so I could include it right in this PR?",
  "created_at":"2021-05-28T17:46:33Z",
  "id":850573883,
  "issue":880,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDU3Mzg4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-28T17:46:33Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"> I have an improvement ready on your branch, could you unprotect the `bugfix-879` branch on your fork for me so I could include it right in this PR?\r\n\r\nThis is something @agoose77 has to do. (I was confused at first.)",
  "created_at":"2021-05-28T17:58:35Z",
  "id":850579696,
  "issue":880,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDU3OTY5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-28T17:58:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes, sorry for the confusion, it was meant to be addressed to @agoose77 ",
  "created_at":"2021-05-28T18:01:31Z",
  "id":850581304,
  "issue":880,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDU4MTMwNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-05-28T18:01:31Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"```sh\r\n\u276f git status\r\nOn branch agoose77-bugfix-879\r\nYour branch is up to date with 'agoose77/bugfix-879'.\r\n\u276f git add tests/test_0879-non-primitive-with-field.py\r\n\u276f git commit -m \"extend tests with specific required dtypes. included suggested direct assignment.\"\r\n[agoose77-bugfix-879 42a245d] extend tests with specific required dtypes. included suggested direct assignment.\r\n 1 file changed, 15 insertions(+), 5 deletions(-)\r\n\u276f git remote -v\r\nagoose77\tgit@github.com:agoose77/awkward-1.0.git (fetch)\r\nagoose77\tgit@github.com:agoose77/awkward-1.0.git (push)\r\norigin\tgit@github.com:drahnreb/awkward-1.0.git (fetch)\r\norigin\tgit@github.com:drahnreb/awkward-1.0.git (push)\r\nupstream\tgit@github.com:scikit-hep/awkward-1.0.git (fetch)\r\nupstream\tgit@github.com:scikit-hep/awkward-1.0.git (push)\r\n\u276f git push agoose77 HEAD:bugfix-879\r\nERROR: Permission to agoose77/awkward-1.0.git denied to drahnreb.\r\nfatal: Could not read from remote repository.\r\n\r\nPlease make sure you have the correct access rights\r\nand the repository exists.\r\n```\r\n\r\nSorry, @agoose77 I still can't push to your fork to add to this PR. (I thought your thumbs up suggested that it should work..., if not let me know to find another solution.)",
  "created_at":"2021-05-29T14:01:45Z",
  "id":850838532,
  "issue":880,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDgzODUzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-29T14:01:45Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@drahnreb I've added you as a collaborator, it should work now!",
  "created_at":"2021-05-29T14:37:28Z",
  "id":850843912,
  "issue":880,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDg0MzkxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-29T14:37:28Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> @drahnreb I've added you as a collaborator, it should work now!\r\n\r\nThanks. It worked and is included here. Tests should pass. From my side ready to be merged, elsewise comments welcome... \ud83d\ude42",
  "created_at":"2021-05-29T14:57:52Z",
  "id":850846970,
  "issue":880,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDg0Njk3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-29T14:57:52Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Looks good to me. Are you ready for this to be merged, @agoose77 ?",
  "created_at":"2021-05-29T21:05:35Z",
  "id":850898744,
  "issue":880,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDg5ODc0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-29T21:05:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I haven't run the tests as introduced by @drahnreb, but looks good to me! I'll merge. ",
  "created_at":"2021-05-30T10:13:59Z",
  "id":850975021,
  "issue":880,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDk3NTAyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-30T10:13:59Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Or it needs to be\r\n\r\n```python\r\n [<Array [[1,2,3], [4]] type='2 * var * int64]'>,\r\n <Array [[None, None, None], [6]] type='2 * var * ?int64'>]\r\n```",
  "created_at":"2021-05-28T12:39:12Z",
  "id":850389592,
  "issue":882,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDM4OTU5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-28T12:42:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Or it needs to be\r\n> \r\n> ```python\r\n>  [<Array [[1,2,3], [4]] type='2 * var * int64]'>,\r\n>  <Array [[None, None, None], [6]] type='2 * var * ?int64'>]\r\n> ```\r\n\r\n\r\n\r\nYes, I suppose if the option doesn't take precedence then it needs to be like this. ",
  "created_at":"2021-05-28T12:42:11Z",
  "id":850391346,
  "issue":882,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDM5MTM0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-28T12:42:11Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I was looking at this with the intention of fixing it, but instead, I became convinced that the current behavior is correct.\r\n\r\nThe purpose of broadcasting is to put N arrays into the same form so that some operation can be computed on them, such as (but not limited to) applying a ufunc. NumPy doesn't have missing values above the numeric level (and even then, `np.ma.MaskedArrays` are not super-standard), so NumPy doesn't provide guidance and we can choose a behavior that is suited to our needs.\r\n\r\nBroadcasting these two arrays lets the None in the right array knock out a whole list in the left array:\r\n\r\n```python\r\n>>> ak.broadcast_arrays(ak.Array([[1, 2, 3], [4]]), ak.Array([None, 6]))\r\n[<Array [None, [4]] type='2 * option[var * int64]'>,\r\n <Array [None, [6]] type='2 * option[var * int64]'>]\r\n```\r\n\r\nbut that's because combining them in an operation like `+` (binary ufunc) would have no information for the whole first list on the left.\r\n\r\n```python\r\n>>> ak.Array([[1, 2, 3], [4]]) + ak.Array([None, 6])\r\n<Array [None, [10]] type='2 * option[var * int64]'>\r\n```\r\n\r\nAlternative answers like `[[None, None, None], [10]]` don't really make sense (the None on the right is in the position of the whole first list\u2014I don't see why it should be \"exploded down\" one level).\r\n\r\nGiven that broadcasting is to support N-array operations, the current behavior is correct. I suppose you were thinking that broadcasting should leave at least one of the arrays intact, but I don't see why that should be a constraint (and it might not be compatible with the intention of supporting N-array operations).",
  "created_at":"2021-07-13T21:13:20Z",
  "id":879407139,
  "issue":882,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTQwNzEzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T21:13:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Also worth knowing: if you run\r\n\r\n```bash\r\npre-commit run -a\r\n```\r\n\r\nbefore pushing your commit, then you'll know if it will pass the pre-commit.ci test. That can be useful because pre-commit will change your code if it can (mostly to adhere to standard formatting rules). When pre-commit changes the files on GitHub, you have to `git pull` the changes locally, which can get messy if you've already started modifying them for the next step. (`git stash` before `git pull` is a good way to deal with that, if the changes aren't on the same lines.)\r\n\r\nI think this is in the [CONTRIBUTING.md](https://github.com/scikit-hep/awkward-1.0/blob/main/CONTRIBUTING.md) document.",
  "created_at":"2021-05-28T18:55:35Z",
  "id":850606280,
  "issue":884,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MDYwNjI4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-28T18:55:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I can't comment the line number on mobile, but should `filedloc` be `fieldloc`? :) ",
  "created_at":"2021-05-31T13:15:56Z",
  "id":851484979,
  "issue":884,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MTQ4NDk3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-03T06:47:59Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> I can't comment the line number on mobile, but should `filedloc` be `fieldloc`?\r\n\r\nYes. `:)`",
  "created_at":"2021-05-31T13:51:18Z",
  "id":851505774,
  "issue":884,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MTUwNTc3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-31T13:51:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I've taken a look at this, and it's a bit more fiddly than I first anticipated. I'm keeping some notes here, in case anyone gets a chance to work on it before I do.\r\n\r\n- The new `parquet_dataset` function ignores the order of the row groups ([due to `unordered_map`](https://github.com/apache/arrow/blob/133b1a904bf7fc1d24343c306a2279e27d4ebe6d/cpp/src/arrow/dataset/file_parquet.cc#L891-L915))\r\n   - Need to use `pyarrow.parquet.ParquetDataset` *or* compose dataset manually by reading metadata first. I think the latter option is most future-proof\r\n- Handling URIs *and* filepaths will be a logistical challenge. I propose instead to only handle paths, and allow user to specify the filesystem.\r\n- fsspec expects POSIX paths, Arrow file systems convert to POSIX, and  currently we regularise paths according to the host OS. We need to be clear about when we have fs paths vs host paths.\r\n- `dataset` is a very general API, and if we're not careful we'll have lots of tricky cases to handle. \r\n\r\nAwkward already does some boilerplate like finding the `_metadata` file for datasets, and in order to maintain the same interface we're going to need to do the same thing. \r\n\r\nSo I think the way we need to tackle this is:\r\n- Switch on `is_file_like(source)` to use new or old dataset\r\n- Define default filesystem (`arrow.fs.LocalFileSystem`)\r\n   - not sure if this should be parameter or in `options`\r\n- Define four path-like cases for new API:\r\n   -  `str` (single file)\r\n   - `str` (directory)\r\n   - `str` (directory with `_metadata`)\r\n   - `Iterable[str]`\r\n   - other (pass through to `arrow.dataset`)\r\n- convert paths to POSIX (to avoid platform-specific pain)\r\n\r\nHere's a hacky implementation https://github.com/scikit-hep/awkward-1.0/pull/949\r\n",
  "created_at":"2021-06-23T12:50:10Z",
  "id":866806990,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjgwNjk5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T12:50:10Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> ignores the order of the row groups\r\n\r\nThis is surprising. I am assuming dask handles this already, might be a template.\r\n\r\n> Handling URIs and filepaths will be a logistical challenge\r\n\r\nLet fsspec do t for you?\r\n\r\n> fsspec expects POSIX paths\r\n\r\nFor local files, windows paths work fine. Yes, they may be converted to posix internally, but you shouldn't have to worry about that.\r\n\r\n> dataset is a very general API, and if we're not careful we'll have lots of tricky cases to handle.\r\n\r\nI'd love to know what you mean, what the downsides are/might be.\r\n\r\n> Define default filesystem \r\n\r\nlet fsspec do it? If you're going to be doing other file things elsewhere in the project (i.e., where you don't explicitly require arrow), this seems like a good idea. Also, fsspe's local filesystem is guaranteed to be compatible with the other filesystems for s3, gcs..., that it will give you.\r\n\r\n> - other\r\n\r\nI would make a minimal file-like check, error otherwise. Probably more `hasattr(obj, \"read\")` than `isinistance(obj, os.IOBase)`.",
  "created_at":"2021-06-23T13:03:46Z",
  "id":866816514,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjgxNjUxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T13:03:46Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"(sorry, misread - that was \"other\" for path-like things, not file-like. Be aware that people will want to pass pathlib.Path objects at some point. fsspec is OK with this)",
  "created_at":"2021-06-23T13:05:05Z",
  "id":866817587,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjgxNzU4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T13:05:05Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"So, the issue is whether we rely on fsspec. Arrow has a superset of file systems (which do overlap with fsspec), so this is a design choice to be made.\r\n\r\nWe also need to normalize the paths because we need to combine them, and fsspec / arrow don't implement a join API.\r\n\r\nThe TLDR is that this is a bit messy because of the need to support the existing API. I also stumbled across a few bugs when running the test suite which make me think this is not going to be as easy as I had hoped.",
  "created_at":"2021-06-23T13:16:32Z",
  "id":866826818,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjgyNjgxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T13:17:12Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> We also need to normalize the paths because we need to combine them, and fsspec / arrow don't implement a join API.\r\n\r\nEasy to do! fsspec is at your service for this kind of thing. We release all the time.\r\n\r\n> Arrow has a superset of file systems (which do overlap with fsspec),\r\n\r\nThe other way around [surely](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations). Arrow only supports local, s3 and hdfs.",
  "created_at":"2021-06-23T13:23:33Z",
  "id":866832388,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjgzMjM4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T13:23:33Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Arrow implements a wrapper over fsspec's file systems, so technically a superset ;) We have a dependency on Arrow because we have to, but I'm not sure whether we can depend upon `fsspec` internally vs support it via Arrow's wrapper file system. I think currently we're going via the latter route.",
  "created_at":"2021-06-23T13:25:47Z",
  "id":866834035,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjgzNDAzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T13:25:47Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"fsspec has no dependencies of its own, so it a very light dependency. It would be a natural place to add things you might need, like `join`. It already provides tools for URL inference and such that will be important. Dask uses it for this reason (and dask will, of course, be an important engine for awkward).\r\n\r\nI am surprised that you explicitly depend on arrow. I understand there is overlap between awkward and arrow and that to/from conversions will be important to many, but I would still have thought is optional.",
  "created_at":"2021-06-23T13:29:53Z",
  "id":866837029,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjgzNzAyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T13:29:53Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> fsspec has no dependencies of its own, so it a very light dependency. It would be a natural place to add things you might need, like `join`. It already provides tools for URL inference and such that will be important. Dask uses it for this reason (and dask will, of course, be an important engine for awkward).\r\n> \r\n> I am surprised that you explicitly depend on arrow. I understand there is overlap between awkward and arrow and that to/from conversions will be important to many, but I would still have thought is optional.\r\n\r\nSorry, let me be more clear! We have a soft (runtime) dependency upon Arrow in order to implement Arrow/Parquet reading, but it's not required to install.\r\n\r\nClearly we need to make some decisions here, and I'm not in the right headspace for that today! I'll take a look back here at some point this/next week. Thanks for the input so far!",
  "created_at":"2021-06-23T13:46:47Z",
  "id":866850527,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Njg1MDUyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T13:47:27Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Currently, Awkward Array's only required dependency is NumPy (1.13.1+). 90% of its functionality or more depends on NumPy\u2014clearly, that needs to be a strict dependency.\r\n\r\nThe second most relevant library in terms of Awkward Array functionality is Numba: there's quite a lot of code for lowering array iteration in Numba. However, Numba is not a strict dependency: Awkward's Numba extensions are loaded through Numba's entry point if and when Numba is imported. It's probably still true that the majority of Awkward Array users don't use Numba, though I encourage it a lot.\r\n\r\nThe third most relevant library is probably pyarrow, though it only gets invoked in `ak.from_arrow`, `ak.to_arrow`, `ak.to_arrow_table`, `ak.from_parquet`, and `ak.to_parquet`. pyarrow is also not a strict dependency, and we only attempt to import it when somebody calls one of those functions. Although it's a soft dependency, it's a versioned soft dependency: Arrow is developing rapidly, and we require some of its newest features: [the minimum is pyarrow 2.0.0](https://github.com/scikit-hep/awkward-1.0/blob/d9b8082314911041acc7b1c1b63313d66c2d3315/src/awkward/operations/convert.py#L1953-L1974).\r\n\r\nThere are a number of other libraries that are used in very specific functions, if they're available (e.g. `ak.to_pandas` requires Pandas). They're all soft dependencies, with an import triggered by the function call, and a message explaining how to install it with pip or conda if it's not there.\r\n\r\nUsing fsspec to get access to lots of remote file systems would be great, though I only see it coming in through `ak.from_parquet`, and then there are cases in which it is not needed: local files, S3, and HDFS. When should we invoke the \"You can do this, but please install fsspec\" error? On _any_ call of `ak.from_parquet`? The majority of users of this function (right now) are probably using local files. On any non-local file? Or on anything that's not local, S3, or HDFS? It could be coarse or fine-grained.\r\n\r\nFor the user experience, there's an advantage to grouping dependencies to ask for them all at once (coarse), rather than the \"death by a thousand cuts\" of install this, get a little further, install that, get a little further, etc. We don't have that problem yet because Awkward's soft dependencies are in different functions: `ak.to_pandas` needs Pandas and `ak.from_arrow` needs Arrow, but neither of these functions need both. If `ak.from_parquet` _sometimes_ needs both, maybe it should _always_ need both, so that nobody's workflow gets interrupted twice by the same `ak.*` function call. This would be like [Uproot's soft dependency on lz4](https://github.com/scikit-hep/uproot4/blob/511f2d6312e7ce02211801fc5c8da46ab691064e/src/uproot/extras.py#L147-L167), which also requires xxhash because all ROOT files with an LZ4-compressed block also have an xxhash checksum to check. The error message asking users to install lz4 asks them to install xxhash at the same time, saving an extra step.\r\n\r\nSince fsspec has no dependencies, I'm inclined to do that for the `ak.from_parquet`/`ak.to_parquet` pair (likely used together, so the same rule should apply for simplicity). Calling either one of these functions should require pyarrow 2.0+ and fsspec at the same time, and the error message should say how to import them both with pip or import them both with conda.\r\n\r\nHowever, `ak.from_arrow`, `ak.to_arrow`, and `ak.to_arrow_table` really don't need fsspec and shouldn't be asking for it. Currently, these and the `ak.from_parquet`/`ak.to_parquet` pair all use the same helper function, so that would have to be refactored with an argument (e.g. `require_fsspec=False`). Awkward Array users who only read and write local files don't, strictly speaking, need fsspec, but we should ask for it anyway to keep the soft dependencies from getting too fine-grained.\r\n\r\nThis could be elevated to a general rule: any `ak.*` function that might require _any_ of a set of third-party packages must attempt to import (with \"how to install\" on failure) _all_ of those third-party packages at the beginning of the function, before any other error-handling. This case is the first that satisfies the rule. I might be over-eager in defining this rule, since we might find before this PR is done that s3fs is only needed if the filesystem is S3, rados is only needed if the filesystem is Ceph, etc., and then `ak.from_parquet` would (according to this rule) require a whole lot of third-party packages, most of them never used by a given user. If that's the case, I'll back down. On the other hand, if these libraries (s3fs, rados, etc.) are required at the time that the filesystem object is created by fsspec, not at the time that it's used by pyarrow to read the file, then that would be ideal: it would communicate the fact that such-and-such a library is needed because of the requested filesystem, not because of `ak.from_parquet`.",
  "created_at":"2021-06-23T14:55:23Z",
  "id":866909935,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjkwOTkzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T14:55:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I all depends on whether you want fsspec to handle your URLs for you and generate filesystem objects. If you need changes in the thrist party dependency, you will find fsspec much faster to respond than arrow. \r\nI would argue that the future of awkward has to be in great part in the cloud, with data on cloud servers or various flavours. I realise that's not as relevant to actual HEP, who likely like HPC systems and mounted disks, but I really want to see awkward as the de-facto nested processing engine for python. But you might want to load root files remotely too, no?\r\n\r\n(by the way, all users are working with local files right now, because that's the only thing that works! :) )",
  "created_at":"2021-06-23T15:59:27Z",
  "id":866964455,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Njk2NDQ1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T15:59:27Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> I would argue that the future of awkward has to be in great part in the cloud, with data on cloud servers or various flavours. [\u2026] I really want to see awkward as the de-facto nested processing engine for python.\r\n\r\nAgree. If there is a way awkward can get fsspec compliant I would love to see this. I am using Azure and support for it is limited right now.\r\n\r\nMaybe this could be a small step towards the concept of a dask integration that @jpivarski presented in this talk: [An Awkward Dask Collection](https://github.com/jpivarski-talks/2021-05-21-dasksummit-awkward-collection)?\r\n\r\nEye gazing this issue and recent changes in #935 etc. I\u2018m eager to help.",
  "created_at":"2021-06-23T16:31:10Z",
  "id":866987916,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Njk4NzkxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T16:31:10Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Certainly, dask integration is coming! It will take some planning and development. Splitting awkward processing tasks into daskable chunks and accessing remote filesystems are separate concerns, but they will, of course, play well together. ",
  "created_at":"2021-06-23T16:34:25Z",
  "id":866990330,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Njk5MDMzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T16:34:25Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I'd be fine with saying that every Awkward function that might take a URL should require fsspec. I think the only such functions that might take URLs right now are `ak.from_parquet` and `ak.to_parquet` (assuming we can write to URLs).\r\n\r\nThe `ak.from_json` function can take a filename, though it's a bad API because the same string can also be a string of JSON text. The C++ layer that this is based on is being rewritten in Python, which would make `ak.from_json(data)` exactly equivalent to `ak.from_iter(json.loads(data))` or `ak.from_iter(json.load(open(data)))`, in which case, we can deprecate `ak.from_json`/`ak.to_json` with a stub telling people how to write that one-liner.\r\n\r\nOther than that, no other functions deal with files. Zarr integration is planned. That would be a second case, and I suppose fsspec would be needed for that, too.",
  "created_at":"2021-06-23T16:43:18Z",
  "id":866996213,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Njk5NjIxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T16:43:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Further down the line, I would suggest also considering avro, xml and maybe other nested-schema formats, as well as line-serialised arrow. I'm not sure how much of that could be directly deferred to arrow, we will see.\r\n\r\nZarr integration only for >=v3, right?",
  "created_at":"2021-06-23T16:47:35Z",
  "id":866998981,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Njk5ODk4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T16:47:35Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"For Avro, I can generate AwkwardForth that's considerably faster than the fastavro library (https://arxiv.org/abs/2102.13516). Awkward Array can become the fastest way to read Avro data without an extra compilation step. I have some background in Avro and could write the Avro schema \u2192 AwkwardForth function, though that's a matter of time and would probably have to be motivated by a use-case.\r\n\r\nI think line-serialized Arrow should go through pyarrow. (Don't we already get it for free? Isn't there a pyarrow function to load line-serialized Arrow as a `pyarrow.array`?)\r\n\r\nI've talked about Zarr v3 because we have a chance of adding a \"must be read by Awkward\" flag to the format as some kind of extension. I know that it's possible to serialize into and out of v2 now (using `ak.from_buffers`/`ak.to_buffers`), but it's the issue of forcing the collection of buffers to be interpreted as an `ak.Array` that I'm thinking of. The same could be said of HDF5; there's no flag saying that the group of 1-dimensional datasets is a low-level view that needs to be interpreted in a particular way. I haven't made any steps toward influencing the Zarr v3 specification, other than talking about it, though.",
  "created_at":"2021-06-23T17:16:33Z",
  "id":867017768,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzAxNzc2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T17:16:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Awkward Array can become the fastest way to read Avro data\r\n\r\nYes, what I was thinking exactly. Fastavro is very dict-oriented, so no good for this. My library [uavro](https://github.com/martindurant/uavro) might also be a good starting point, for being more python-oriented (but completely ignoring nesting!). The avro format is actually quite nice to work with, so long as the in-memory buffers can grow as the data is read.\r\n\r\n> adding a \"must be read by Awkward\"\r\n\r\nAgreed, I thought this was still the plan, and it sounds good - we can be in a position to add our extension when v3 comes out.\r\n\r\n> Don't we already get it for free (arrow serialisation)\r\n\r\nProbably yes, but I imagine we'd want to at least make an explicit example of this.",
  "created_at":"2021-06-23T17:37:21Z",
  "id":867031473,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzAzMTQ3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T17:37:21Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"(btw: fastparquet contains a new very fast thrift reader that would also be a good starting point for avro, since the two are so similar. That code makes nested python dicts, but it's all in the same field roughly. I don't think we care particularly about thrift-serialised data, but we could cover all avro/thrift/protobuf/msgpack for only a little extra effort)",
  "created_at":"2021-06-23T17:40:37Z",
  "id":867033534,
  "issue":886,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzAzMzUzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T17:40:37Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"NONE",
  "body":"This is the only ticket I see mentioning protobuf. What is the current/expected state of support for such formats, especially protobuf?",
  "created_at":"2021-09-30T08:21:32Z",
  "id":931014057,
  "issue":886,
  "node_id":"IC_kwDODBCWws43fiWp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-30T08:21:32Z",
  "user":"MDQ6VXNlcjEwMDE3Nzg="
 },
 {
  "author_association":"MEMBER",
  "body":"@deeplook I wasn't planning on `ak.from_protobuf` or `ak.to_protobuf`, but that would be a good application of AwkwardForth (since it's not a columnar format). It's not on any road maps but can be if there's a strong enough use-case for it. The place to start would be to file a feature request.",
  "created_at":"2021-09-30T12:20:41Z",
  "id":931270789,
  "issue":886,
  "node_id":"IC_kwDODBCWws43ghCF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-30T12:20:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I'm not familiar with AwkwardForth, but I'm interested in operating on nested structures in ways like [flatten-dict](https://github.com/ianlini/flatten-dict), but on binary formats, especially protobuf, with some NumPy-like interface and the performance promised by Awkward. ;) ",
  "created_at":"2021-09-30T12:59:07Z",
  "id":931297969,
  "issue":886,
  "node_id":"IC_kwDODBCWws43gnqx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-30T12:59:07Z",
  "user":"MDQ6VXNlcjEwMDE3Nzg="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Right, this thread seems to have meandered quite far from the original intent! I do agree that we should have in mind a list of specific IO that the project is interested in, to go into an explicit issue discussing this. \r\nFor consideration:\r\n- text\r\n  - json\r\n  - yaml\r\n  - xml\r\n  - text lines plus nested regex? (e.g., log files)\r\n- columnar binary\r\n  - parquet\r\n  - orc\r\n  - zarr (v3, or v2 with our own convention)\r\n- row binary\r\n  - protobuf\r\n  - avro\r\n  - thrift\r\n  - msgpack?\r\n- arrow services\r\n  - flight",
  "created_at":"2021-09-30T13:01:12Z",
  "id":931299550,
  "issue":886,
  "node_id":"IC_kwDODBCWws43goDe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-30T13:01:12Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Improving feature parity with NumPy is definitely desirable, and I'd be grateful for any contributions like the above.\r\n\r\nSince you already have an implementation, I'll just tell you where it goes: it's a reducer, so it belongs in `src/awkward/operations/reducers py`, immediately after `min` and `max`. The order of functions in the file matters because it determines the order of functions in the side-bar of the documentation. Like looking for books in a library, a user might be looking for `ak.min` and `ak.max`, but see `ak.ptp` right next to it and go for that.\r\n\r\nYou already know about the `implements` decorator that works make `ak.ptp` override `np.ptp`, so that's good.\r\n\r\nYou can copy most of the docstring from `min`, weigh gives a standard explanation of the reducer arguments. If you want to add an example illustrating that this is a difference and mention that \"ptp\" stands for \"peak to peak,\" that would be helpful. Seen someone clicks on the documentation for `ak.ptp`, it will be the only thing on the page\u2014they won't also be seeing the other reducers for context.\r\n\r\nIt should also have a test, numbered fit this issue or the PR, that includes comparisons of flat `ak.Arrays` with their corresponding `np.ndarrays`, since they have to behave the same when they coincide. You should make sure that the results are the same for all `axis` values, since `axis != -1` cuts across nested arrays in surprising ways.",
  "created_at":"2021-06-02T12:15:36Z",
  "id":852976915,
  "issue":889,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1Mjk3NjkxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-02T12:15:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks Jim. Let me know if these \"discussion issues\" are better done after creating a PR in future :)",
  "created_at":"2021-06-02T12:18:22Z",
  "id":852978651,
  "issue":889,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1Mjk3ODY1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-02T12:18:22Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"No, this is exactly right: an issue labeled \"feature\" is the first step in adding a feature. I'd also accept an appropriate PR without the feature request issue, but raising the issue first gives us a chance to talk it over first, so that you don't put a lot of effort into a PR and then find out that I can't accept it for some reason.\r\n\r\nAlso, I gave all the details in my previous response though I know you're aware of some of them because someone else might discover this thread in the future when they want to add a feature. It's hard to get exactly the right information into the documentation or CONTRIBUTING.md that each person would need, but if I get enough information out there in the form of answering specific questions like this one, the chances are better that someone who needs it will find it.",
  "created_at":"2021-06-02T12:55:35Z",
  "id":853003178,
  "issue":889,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MzAwMzE3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-02T12:55:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> No, this is exactly right: an issue labeled \"feature\" is the first step in adding a feature. I'd also accept an appropriate PR without the feature request issue, but raising the issue first gives us a chance to talk it over first, so that you don't put a lot of effort into a PR and then find out that I can't accept it for some reason.\r\n\r\nThis was my line of reasoning, too. Good to know :)\r\n\r\n> Also, I gave all the details in my previous response though I know you're aware of some of them because someone else might discover this thread in the future when they want to add a feature. It's hard to get exactly the right information into the documentation or CONTRIBUTING.md that each person would need, but if I get enough information out there in the form of answering specific questions like this one, the chances are better that someone who needs it will find it.\r\n\r\nMakes complete sense to me - there's nothing better than as-it-happened documention imo.\r\n\r\n---\r\n\r\nOne issue with my above `ptp` implementation is that there is no \"zero-overhead\" way to implement an `initial` value. Although NumPy doesn't implement such a parameter for `ptp`, I think it makes more sense for Awkward where sub-arrays can be empty. We can't pull the identity out of the calls to `min` and `max` without setting `mask_identity=True`, and then running `fill_none`. Do you think it's reasonable for a reducer to call `fill_none` internally to fulfil this case?\r\n",
  "created_at":"2021-06-02T13:05:33Z",
  "id":853010447,
  "issue":889,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MzAxMDQ0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-02T13:23:17Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, if `np.ptp` doesn't have an `initial` parameter, we don't _need_ to add one. Maybe they don't have one because it doesn't make a lot of sense in this case.\r\n\r\nAs for performance, this is already doing the `min` and `max` as separate passes, which could be more efficient as a single pass, though that would involve a whole new Reducer (C++ class), which is a lot of boilerplate.\r\n\r\nWe can't get guidance from NumPy about what to do with empty lists. By default (just subtracting) `min` and `max`, they'd be None. If it makes more sense for them to be zero, then adding a third pass with `fill_none` would be the right thing to do. It's fine for a first implementation of a feature to be less efficient than possible. If a faster version is needed, we establish the interface with this implementation and revamp it later with a specialized Reducer in the future.",
  "created_at":"2021-06-02T13:22:35Z",
  "id":853023406,
  "issue":889,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MzAyMzQwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-02T13:22:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"On the performance, the initial implementation can be *slower*, after all, `np.ptp` is also a simple difference IIRC.\r\n\r\nThis is what the above would look like:\r\n\r\n```python\r\nptp = ak.max(\r\n    arr, axis=axis, keepdims=keepdims\r\n) - ak.min(\r\n    arr, axis=axis, keepdims=keepdims\r\n)\r\nif not mask_identity:\r\n    return ak.fill_none(ptp, initial=initial)\r\nreturn ptp\r\n```\r\n\r\nIs it reasonable to just let `initial=0` in the `ptp` function signature?",
  "created_at":"2021-06-02T13:29:04Z",
  "id":853028487,
  "issue":889,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MzAyODQ4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-02T13:29:35Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"The idea of `initial` is different from the idea of a `fill_value`. `initial` limits what min and max values `ak.min` and `ak.max` can \"see\"; it's literally\r\n\r\n```python\r\ninitial = ???\r\nfor x in data:\r\n    if x < initial:\r\n        minimum = x\r\nreturn minimum\r\n```\r\n\r\nSo a data range can get truncated to an `initial` value even if it's not empty; if all of the values are greater than the `initial` (for `ak.min`, for instance). The `fill_value` only applies if `len(data)` is actually `0`. So the above implementation is fine, but it probably shouldn't be called `initial`. Perhaps it should be called `fill_value`, or maybe it should not even be configurable. For a \"peak to peak\" difference of no data, only None or zero make sense. (Zero wouldn't distinguish `len(data) == 0` from `len(data) == 1`; the latter would always have a peak-to-peak of zero.)\r\n\r\nMy preference would be to not have it be configurable. If `mask_identity` is True, then you get Nones, otherwise, you get zeros.",
  "created_at":"2021-06-02T13:42:56Z",
  "id":853039326,
  "issue":889,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MzAzOTMyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-02T13:42:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ah my mistake - I had thought that it was an override-able identity.  I was leaning towards letting it be non-configurable, so I'll take your preference on board. ",
  "created_at":"2021-06-02T13:48:47Z",
  "id":853043817,
  "issue":889,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MzA0MzgxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-02T13:48:47Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! This looks like something I can merge as-is.",
  "created_at":"2021-06-02T16:13:24Z",
  "id":853160805,
  "issue":891,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MzE2MDgwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-02T16:13:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ianna check out the C kernel code once. Haven't added the assert line for testing but do tell me if I need to add anything more",
  "created_at":"2021-06-10T12:55:33Z",
  "id":858597487,
  "issue":893,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODU5NzQ4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T12:55:33Z",
  "user":"MDQ6VXNlcjUyNjM1Nzcz"
 },
 {
  "author_association":"MEMBER",
  "body":"@SantamRC Is this PR still active, or is it superseded by #971?",
  "created_at":"2021-07-12T18:27:01Z",
  "id":878497740,
  "issue":893,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODQ5Nzc0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T18:27:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @SantamRC Is this PR still active, or is it superseded by #971?\r\n\r\nNo this PR is not active anymore\r\n\r\n\r\n",
  "created_at":"2021-07-13T12:09:21Z",
  "id":879032663,
  "issue":893,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTAzMjY2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T12:09:21Z",
  "user":"MDQ6VXNlcjUyNjM1Nzcz"
 },
 {
  "author_association":"MEMBER",
  "body":"I think you needed to `git checkout main; git pull` your directory before making this branch.",
  "created_at":"2021-06-03T14:45:17Z",
  "id":853925841,
  "issue":896,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1MzkyNTg0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-03T14:45:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Now this branch pretty much matches main: https://github.com/scikit-hep/awkward-1.0/compare/f5b2985...eb54a77\r\n\r\nBut be sure to\r\n\r\n```bash\r\ngit checkout main\r\ngit pull\r\ngit checkout -b NEW-BRANCH\r\n```\r\n\r\nwhenever you want to create a `NEW-BRANCH` in the future. A lot of other things get added to the main branch while you're working on these PRs.",
  "created_at":"2021-06-03T18:33:22Z",
  "id":854088904,
  "issue":896,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NDA4ODkwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-03T18:34:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I added a bunch of tests in `tests/test_0896-content-classes-refactoring.py` that are currently all marked `pytest.mark.skip`. After you implement the classes needed for a given test, you can remove the \"skip\" so that it gets tested. You'll need to implement the constructor, `__len__`, `_getitem_at`, `_getitem_range`, and `_getitem_field` for a given class to make the tests for that class succeed.",
  "created_at":"2021-06-03T19:00:11Z",
  "id":854104221,
  "issue":896,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NDEwNDIyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-03T19:00:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a suggested/starter implementation, which passes the above tests. However, you should replace the assertions with proper error messages.\r\n\r\n```python\r\nclass Content(object):\r\n    def __getitem__(self, where):\r\n        if isinstance(where, numbers.Integral):\r\n            return self._getitem_at(where)\r\n        elif isinstance(where, slice) and where.step is None:\r\n            return self._getitem_range(where)\r\n        elif isinstance(where, str):\r\n            return self._getitem_field(where)\r\n        elif isinstance(where, Iterable) and all(isinstance(x, str) for x in where):\r\n            return self._getitem_fields(where)\r\n        else:\r\n            raise AssertionError(where)\r\n\r\n\r\nclass EmptyArray(Content):\r\n    def __init__(self):\r\n        pass\r\n\r\n    def __len__(self):\r\n        return 0\r\n\r\n    def _getitem_at(self, where):\r\n        raise AssertionError()\r\n\r\n    def _getitem_range(self, where):\r\n        return EmptyArray()\r\n\r\n    def _getitem_field(self, where):\r\n        raise IndexError(\"field \" + repr(where) + \" not found\")\r\n\r\n\r\nclass NumpyArray(Content):\r\n    def __init__(self, data):\r\n        # must be an array, but not necessarily NumPy (e.g. any nplike)\r\n        self.data = data\r\n\r\n    def __len__(self):\r\n        return len(self.data)\r\n\r\n    def _getitem_at(self, where):\r\n        out = self.data[where]\r\n        if isinstance(out, np.ndarray) and len(out.shape) != 0:\r\n            return NumpyArray(out)\r\n        else:\r\n            return out\r\n\r\n    def _getitem_range(self, where):\r\n        return NumpyArray(self.data[where])\r\n\r\n    def _getitem_field(self, where):\r\n        raise IndexError(\"field \" + repr(where) + \" not found\")\r\n\r\n\r\nclass RegularArray(Content):\r\n    def __init__(self, content, size, zeros_length=0):\r\n        assert isinstance(content, Content)\r\n        assert isinstance(size, numbers.Integral)\r\n        assert isinstance(zeros_length, numbers.Integral)\r\n        assert size >= 0\r\n        if size != 0:\r\n            length = len(content) // size  # floor division\r\n        else:\r\n            assert zeros_length >= 0\r\n            length = zeros_length\r\n        self.content = content\r\n        self.size = size\r\n        self.length = length\r\n\r\n    def __len__(self):\r\n        return self.length\r\n\r\n    def _getitem_at(self, where):\r\n        if where < 0:\r\n            where += len(self)\r\n        assert 0 <= where < len(self)\r\n        return self.content[(where) * self.size : (where + 1) * self.size]\r\n\r\n    def _getitem_range(self, where):\r\n        start, stop, step = where.indices(len(self))\r\n        zeros_length = stop - start\r\n        start *= self.size\r\n        stop *= self.size\r\n        return RegularArray(self.content[start:stop], self.size, zeros_length)\r\n\r\n    def _getitem_field(self, where):\r\n        return RegularArray(self.content[where], self.size, self.length)\r\n\r\n\r\nclass ListArray(Content):\r\n    def __init__(self, starts, stops, content):\r\n        assert isinstance(starts, Index) and starts.T in (np.int32, np.uint32, np.int64)\r\n        assert isinstance(stops, Index) and starts.T == stops.T\r\n        assert isinstance(content, Content)\r\n        assert len(stops) >= len(starts)  # usually equal\r\n        self.starts = starts\r\n        self.stops = stops\r\n        self.content = content\r\n\r\n    def __len__(self):\r\n        return len(self.starts)\r\n\r\n    def _getitem_at(self, where):\r\n        if where < 0:\r\n            where += len(self)\r\n        assert 0 <= where < len(self)\r\n        return self.content[self.starts[where] : self.stops[where]]\r\n\r\n    def _getitem_range(self, where):\r\n        start, stop, step = where.indices(len(self))\r\n        starts = self.starts[start:stop]\r\n        stops = self.stops[start:stop]\r\n        return ListArray(starts, stops, self.content)\r\n\r\n    def _getitem_field(self, where):\r\n        return ListArray(self.starts, self.stops, self.content[where])\r\n\r\n\r\nclass ListOffsetArray(Content):\r\n    def __init__(self, offsets, content):\r\n        assert isinstance(offsets, Index) and offsets.T in (\r\n            np.int32,\r\n            np.uint32,\r\n            np.int64,\r\n        )\r\n        assert isinstance(content, Content)\r\n        assert len(offsets) != 0\r\n        self.offsets = offsets\r\n        self.content = content\r\n\r\n    def __len__(self):\r\n        return len(self.offsets) - 1\r\n\r\n    def _getitem_at(self, where):\r\n        if where < 0:\r\n            where += len(self)\r\n        assert 0 <= where < len(self)\r\n        return self.content[self.offsets[where] : self.offsets[where + 1]]\r\n\r\n    def _getitem_range(self, where):\r\n        start, stop, step = where.indices(len(self))\r\n        offsets = self.offsets[start : stop + 1]\r\n        if len(offsets) == 0:\r\n            offsets = [0]\r\n        return ListOffsetArray(offsets, self.content)\r\n\r\n    def _getitem_field(self, where):\r\n        return ListOffsetArray(self.offsets, self.content[where])\r\n\r\n\r\n# don't really make Record a dict: follow the documentation on\r\n# https://awkward-array.readthedocs.io/en/latest/ak.layout.Record.html\r\nclass Record(dict):\r\n    pass\r\n\r\n\r\nclass RecordArray(Content):\r\n    def __init__(self, contents, recordlookup, length=None):\r\n        assert isinstance(contents, list)\r\n        if length is None:\r\n            assert len(contents) != 0\r\n            length = min([len(x) for x in contents])\r\n        assert isinstance(length, numbers.Integral)\r\n        for x in contents:\r\n            assert isinstance(x, Content)\r\n            assert len(x) >= length\r\n        assert recordlookup is None or isinstance(recordlookup, list)\r\n        if isinstance(recordlookup, list):\r\n            assert len(recordlookup) == len(contents)\r\n            for x in recordlookup:\r\n                assert isinstance(x, str)\r\n        self.contents = contents\r\n        self.recordlookup = recordlookup\r\n        self.length = length\r\n\r\n    def __len__(self):\r\n        return self.length\r\n\r\n    def _getitem_at(self, where):\r\n        if where < 0:\r\n            where += len(self)\r\n        assert 0 <= where < len(self)\r\n        record = [x[where] for x in self.contents]\r\n        if self.recordlookup is None:\r\n            return Record(zip((str(x) for x in range(len(record))), record))\r\n        else:\r\n            return Record(zip(self.recordlookup, record))\r\n\r\n    def _getitem_range(self, where):\r\n        start, stop, step = where.indices(len(self))\r\n        if len(self.contents) == 0:\r\n            start = min(max(start, 0), self.length)\r\n            stop = min(max(stop, 0), self.length)\r\n            if stop < start:\r\n                stop = start\r\n            return RecordArray([], self.recordlookup, stop - start)\r\n        else:\r\n            return RecordArray(\r\n                [x[start:stop] for x in self.contents],\r\n                self.recordlookup,\r\n                stop - start,\r\n            )\r\n\r\n    def _getitem_field(self, where):\r\n        if self.recordlookup is None:\r\n            try:\r\n                i = int(where)\r\n            except ValueError:\r\n                pass\r\n            else:\r\n                if i < len(self.contents):\r\n                    return self.contents[i][: len(self)]\r\n        else:\r\n            try:\r\n                i = self.recordlookup.index(where)\r\n            except ValueError:\r\n                pass\r\n            else:\r\n                return self.contents[i][: len(self)]\r\n        raise IndexError(\"field \" + repr(where) + \" not found\")\r\n\r\n\r\nclass IndexedArray(Content):\r\n    def __init__(self, index, content):\r\n        assert isinstance(index, Index) and index.T in (np.int32, np.uint32, np.int64)\r\n        assert isinstance(content, Content)\r\n        self.index = index\r\n        self.content = content\r\n\r\n    def __len__(self):\r\n        return len(self.index)\r\n\r\n    def _getitem_at(self, where):\r\n        if where < 0:\r\n            where += len(self)\r\n        assert 0 <= where < len(self)\r\n        return self.content[self.index[where]]\r\n\r\n    def _getitem_range(self, where):\r\n        return IndexedArray(self.index[where.start : where.stop], self.content)\r\n\r\n    def _getitem_field(self, where):\r\n        return IndexedArray(self.index, self.content[where])\r\n\r\n\r\nclass IndexedOptionArray(Content):\r\n    def __init__(self, index, content):\r\n        assert isinstance(index, Index) and index.T in (np.int32, np.int64)\r\n        assert isinstance(content, Content)\r\n        self.index = index\r\n        self.content = content\r\n\r\n    def __len__(self):\r\n        return len(self.index)\r\n\r\n    def _getitem_at(self, where):\r\n        if where < 0:\r\n            where += len(self)\r\n        assert 0 <= where < len(self)\r\n        if self.index[where] < 0:\r\n            return None\r\n        else:\r\n            return self.content[self.index[where]]\r\n\r\n    def _getitem_range(self, where):\r\n        return IndexedOptionArray(self.index[where.start : where.stop], self.content)\r\n\r\n    def _getitem_field(self, where):\r\n        return IndexedOptionArray(self.index, self.content[where])\r\n\r\n\r\nclass ByteMaskedArray(Content):\r\n    def __init__(self, mask, content, valid_when):\r\n        assert isinstance(mask, Index) and mask.T == np.int8\r\n        assert isinstance(content, Content)\r\n        assert isinstance(valid_when, bool)\r\n        assert len(mask) <= len(content)\r\n        self.mask = mask\r\n        self.content = content\r\n        self.valid_when = valid_when\r\n\r\n    def __len__(self):\r\n        return len(self.mask)\r\n\r\n    def _getitem_at(self, where):\r\n        if where < 0:\r\n            where += len(self)\r\n        assert 0 <= where < len(self)\r\n        if self.mask[where] == self.valid_when:\r\n            return self.content[where]\r\n        else:\r\n            return None\r\n\r\n    def _getitem_range(self, where):\r\n        start, stop, step = where.indices(len(self))\r\n        return ByteMaskedArray(\r\n            self.mask[start:stop],\r\n            self.content[start:stop],\r\n            valid_when=self.valid_when,\r\n        )\r\n\r\n    def _getitem_field(self, where):\r\n        return ByteMaskedArray(\r\n            self.mask, self.content[where], valid_when=self.valid_when\r\n        )\r\n\r\n\r\nclass BitMaskedArray(Content):\r\n    def __init__(self, mask, content, valid_when, length, lsb_order):\r\n        assert isinstance(mask, Index) and mask.T == np.uint8\r\n        assert isinstance(content, Content)\r\n        assert isinstance(valid_when, bool)\r\n        assert isinstance(length, numbers.Integral) and length >= 0\r\n        assert isinstance(lsb_order, bool)\r\n        assert len(mask) <= len(content)\r\n        self.mask = mask\r\n        self.content = content\r\n        self.valid_when = valid_when\r\n        self.length = length\r\n        self.lsb_order = lsb_order\r\n\r\n    def __len__(self):\r\n        return self.length\r\n\r\n    def _getitem_at(self, where):\r\n        if where < 0:\r\n            where += len(self)\r\n        assert 0 <= where < len(self)\r\n        if self.lsb_order:\r\n            bit = bool(self.mask[where // 8] & (1 << (where % 8)))\r\n        else:\r\n            bit = bool(self.mask[where // 8] & (128 >> (where % 8)))\r\n        if bit == self.valid_when:\r\n            return self.content[where]\r\n        else:\r\n            return None\r\n\r\n    def _getitem_range(self, where):\r\n        # In general, slices must convert BitMaskedArray to ByteMaskedArray.\r\n        bytemask = np.unpackbits(\r\n            self.mask, bitorder=(\"little\" if self.lsb_order else \"big\")\r\n        ).view(np.bool_)\r\n        start, stop, step = where.indices(len(self))\r\n        return ByteMaskedArray(\r\n            bytemask[start:stop],\r\n            self.content[start:stop],\r\n            valid_when=self.valid_when,\r\n        )\r\n\r\n    def _getitem_field(self, where):\r\n        return BitMaskedArray(\r\n            self.mask,\r\n            self.content[where],\r\n            valid_when=self.valid_when,\r\n            length=self.length,\r\n            lsb_order=self.lsb_order,\r\n        )\r\n\r\n\r\nclass UnmaskedArray(Content):\r\n    def __init__(self, content):\r\n        assert isinstance(content, Content)\r\n        self.content = content\r\n\r\n    def __len__(self):\r\n        return len(self.content)\r\n\r\n    def _getitem_at(self, where):\r\n        return self.content[where]\r\n\r\n    def _getitem_range(self, where):\r\n        return UnmaskedArray(self.content[where])\r\n\r\n    def _getitem_field(self, where):\r\n        return UnmaskedArray(self.content[where])\r\n\r\n\r\nclass UnionArray(Content):\r\n    def __init__(self, tags, index, contents):\r\n        assert isinstance(tags, Index) and tags.T == np.int8\r\n        assert isinstance(index, Index) and index.T in (np.int32, np.intU32, np.int64)\r\n        assert isinstance(contents, list)\r\n        assert len(index) >= len(tags)  # usually equal\r\n        self.tags = tags\r\n        self.index = index\r\n        self.contents = contents\r\n\r\n    def __len__(self):\r\n        return len(self.tags)\r\n\r\n    def _getitem_at(self, where):\r\n        if where < 0:\r\n            where += len(self)\r\n        assert 0 <= where < len(self)\r\n        return self.contents[self.tags[where]][self.index[where]]\r\n\r\n    def _getitem_range(self, where):\r\n        start, stop, step = where.indices(len(self))\r\n        return UnionArray(self.tags[start:stop], self.index[start:stop], self.contents)\r\n\r\n    def _getitem_field(self, where):\r\n        return UnionArray(self.tags, self.index, [x[where] for x in self.contents])\r\n```",
  "created_at":"2021-06-03T19:41:40Z",
  "id":854128943,
  "issue":896,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NDEyODk0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-03T19:41:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's a Python 2.7 issue. I'll take a closer look right before our meeting (in 7 minutes).",
  "created_at":"2021-06-10T12:53:49Z",
  "id":858596320,
  "issue":896,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODU5NjMyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T12:53:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`__repr__` should return the XML form.",
  "created_at":"2021-06-10T14:00:30Z",
  "id":858648268,
  "issue":896,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODY0ODI2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T14:00:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"No more problems with Python 2.7. Go ahead and finish off this PR!",
  "created_at":"2021-06-10T16:14:45Z",
  "id":858757137,
  "issue":896,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODc1NzEzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T16:14:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif I renamed a few things and fixed a few others.\r\n\r\nThe main thing, going forward, is that the \"sanity checking\" in the `__init__` of these classes should raise TypeError or ValueError with useful messages, rather than blank assertions. (The documentation was written that way for clarity, to avoid cluttering the screen with error-handling when its main purpose was to explain the rules.)",
  "created_at":"2021-06-16T02:17:11Z",
  "id":861983674,
  "issue":896,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTk4MzY3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T02:17:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"More of a policy choice than a bug-fix, but a good idea.",
  "created_at":"2021-12-07T22:14:03Z",
  "id":988299713,
  "issue":898,
  "node_id":"IC_kwDODBCWws466EHB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-07T22:14:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2021-06-07T17:44:15Z",
  "id":856136007,
  "issue":899,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NjEzNjAwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-07T17:44:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I'm closing this for now, as I don't have time to work on it!",
  "created_at":"2021-07-20T14:36:03Z",
  "id":883445370,
  "issue":900,
  "node_id":"IC_kwDODBCWws40qE56",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T14:36:03Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! I appreciate having fewer open PRs, since I have to scan all of them when asking myself, \"Is there anything wait for me to review?\"\r\n\r\nThe issue is still there and the issue has a pointer to this closed PR, so whenever we take this up again in the future, the test you wrote will be something we can copy into the new PR for it.",
  "created_at":"2021-07-20T15:19:24Z",
  "id":883479205,
  "issue":900,
  "node_id":"IC_kwDODBCWws40qNKl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T15:19:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It looks like the issue is probably the `NumpyArray` has both strides set and an already-strided copy of the data:\r\n```pycon\r\n>>> layout = ak.from_numpy(table[..., 0], highlevel=False)\r\n>>> layout\r\n<NumpyArray format=\"d\" shape=\"2\" strides=\"16\" data=\"0 2\" at=\"0x0000055575d0\"/>\r\n```\r\nPerhaps this is just a wild goose chase, though; the data repr might just be wrong. Especially given that this *only* fails inside Numba.",
  "created_at":"2021-06-08T15:47:31Z",
  "id":856886161,
  "issue":903,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1Njg4NjE2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-08T16:03:29Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The `strides` value does not agree inside the numba function vs outside.",
  "created_at":"2021-06-08T16:07:53Z",
  "id":856901828,
  "issue":903,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NjkwMTgyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-08T16:07:53Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The line in question is https://github.com/scikit-hep/awkward-1.0/blob/cb24e1b0a9c6ab6c10199f1db40035e55c35c7ab/src/awkward/_connect/_numba/arrayview.py#L1231-L1235, just need to work out what's going on here :)",
  "created_at":"2021-06-08T16:20:28Z",
  "id":856910855,
  "issue":903,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NjkxMDg1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-08T16:21:08Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I think I've hit a wall with how much I understand about Numba + Awkward Numba. I'm not entirely clear whose responsibility it is to pass in the strides; I think we might need to add a `strides` field to the ArrayViewType, but I'm not confident on that!",
  "created_at":"2021-06-08T16:44:06Z",
  "id":856926752,
  "issue":903,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NjkyNjc1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-08T16:44:06Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right: the strides are assumed to be C-contiguous, which makes the for-loop steps constant for compiler optimization. But it went bad because they were just _assumed_ to be contiguous, not forced to be. Here's a simpler test case:\r\n\r\n```python\r\n    @numba.njit\r\n    def f1(x):\r\n        return x[0], x[1]\r\n\r\n    array = ak.Array(np.arange(4).reshape(2, 2)[:, 0])\r\n\r\n    assert f1.py_func(array) == (0, 2)\r\n    assert f1(array) == (0, 2)   # would be (0, 1) with the bug\r\n```\r\n\r\nThe spot that you located is in the lowering of the `np.asarray` function: changing that would only affect that one function (and the actual strides are not known at that point in the code!). That wouldn't have done it.\r\n\r\nInstead, we should use `nplike.ascontiguousarray` instead of `nplike.asarray` when the array is converted into an ArrayView. This happens only once and is cached. Performance will be better for C-contiguous arrays because other arrays have to be converted into C-contiguous, but most arrays are C-contiguous, and we get to apply this compiler optimization (constant strides).\r\n\r\nThanks for looking into it! The Numba lowering is admittedly hard to read because it was so heavily optimized (e.g. all that pointer arithmetic).",
  "created_at":"2021-06-08T20:35:37Z",
  "id":857113418,
  "issue":903,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzExMzQxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-08T20:35:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I realise the complexity would increase, but wouldn't this be something that the user should enforce rather than the numba layer taking a potential contiguous copy? Unless ... do we already take a copy (of non-NumpyArray contents) in order to erase the indirection from non-structural layouts? (guesswork here)",
  "created_at":"2021-06-09T07:24:58Z",
  "id":857455740,
  "issue":904,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzQ1NTc0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-09T08:40:57Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Copying NumpyArrays to make them contiguous is pretty common, perhaps more common that you think, and perhaps that's why it took so long to find this case (because most of the arrays that got this far _were_ contiguous). NumpyArray in C++ has a [contiguous](https://github.com/scikit-hep/awkward-1.0/blob/3f36053f0a2d40c6920aa27c65d9fb08e0677495/src/libawkward/array/NumpyArray.cpp#L3818-L3932) method to turn whatever we have into a contiguous array because nearly all operations except the ones that emulate NumPy either need that assumption are are greatly simplified by it.\r\n\r\nIt would be possible to iterate over non-contiguous arrays in Numba, but either the Numba type would have to carry the stride information (so that the iteration step size can be inserted into the compiled code as a compile-time constant) or the ArrayViewModel would have to be expanded to have this information at runtime. Carrying the stride data in the type would generate as-efficient compiled code, but it would mean that each array with a different stride would require a new compilation, and then some applications would end up spending a lot more time compiling. Carrying the stride data in the ArrayViewModel would slow down everything at runtime, not just the arrays that need non-trivial strides. The ArrayViewModel is like a \"cursor\" pointing to a part of the array tree; it's has the same memory layout for all types because it has to hold all information that hasn't been \"burned into\" the compilation. Most data in Numba are passed by value (to avoid a lot of memory management), meaning this cursor has to be copied in its entirety from one function to another; thus, we want it to be small. Including the strides in the runtime object would mean every cursor would be a bit bigger\u2014they would need at least one more pointer, a pointer to all the stride values in the array-tree.\r\n\r\nSo it's not a clear-cut case of complexity vs optimization. Adding non-trivial strides in the Numba implementation would definitely increase the complexity, but it wouldn't always make it faster, and in quite a few cases would make it slower (including cases without non-trivial strides, in some possible implementations). But even if it were a clear-cut case of complexity vs optimization, I wouldn't want to implement it right away. In the case discussed here, 1D non-contiguous arrays are copied into contiguous arrays before entering Numba. It is also the case that nD NumpyArrays are copied into contiguous arrays before entering Numba, by virtue of turning n\u20121 dimensions of the NumpyArray into RegularArrays, to avoid the extra complexity of dealing with multidimensional NumpyArray nodes in Numba. If anything, that case would have to be dealt with before the more-rare case of 1D non-contiguous arrays.",
  "created_at":"2021-06-09T12:34:50Z",
  "id":857656262,
  "issue":904,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzY1NjI2Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-09T12:34:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think the cause here is the various `searchsorted` calls. https://github.com/scikit-hep/awkward-1.0/blob/3f36053f0a2d40c6920aa27c65d9fb08e0677495/src/awkward/operations/structure.py#L2056-L2061\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/3f36053f0a2d40c6920aa27c65d9fb08e0677495/src/awkward/operations/structure.py#L2108-L2110\r\n\r\nThere is another case with `axis=0`, where\r\n```python3\r\narray = ak.Array([0, 2, 2, 2, 2, 2])\r\nlengths = ak.Array([1, 0, 4, 0, 0, 0])\r\nak.unflatten(array, lengths)\r\n```\r\nfails",
  "created_at":"2021-06-09T12:37:24Z",
  "id":857657946,
  "issue":905,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzY1Nzk0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-09T12:38:00Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"\"Getting zeros right\" was an active effort at one point\u2014the fact that the counts can contain zeros was not ignored. The this that's supposed to do this is called `current_offsets`. Unfortunately, the implementation of this zeros-handling is stateful: you have to be aware of how the value of `current_offsets` _changes_ through the algorithm. (Lots of print-outs!)\r\n\r\nThere are some ambiguities in where to put the zeros when partitioning is involved: the same zero-length list could just as well go at the end of one partition or the beginning of another. The current choice was made to make it easier to not lose any zero-length lists at the end of the array (zeros at the end of `counts` are easy to miss/have to be handled carefully).",
  "created_at":"2021-06-09T12:43:05Z",
  "id":857661698,
  "issue":905,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzY2MTY5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-09T12:43:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ah, I can now see that I didn't notice that the problem of arranging zero-length lists is degenerate in the nested axis case; as you say, without a size constraint the zero-length lists can be placed in either partition.\r\n\r\nHowever, in this case, the meaning of `[0 0 0]` is lost during the unflattening. I don't think *that* is undefined - they could sit in either partition, e.g. \r\n```\r\n[\r\n    [[], [], []],\r\n    [[0], [], [2, 2, 2, 2, 2]],\r\n]\r\n```\r\nor\r\n\r\n```\r\n[\r\n    [],\r\n    [[], [], [], [0], [], [2, 2, 2, 2, 2]],\r\n]\r\n```\r\nor any intermediate configuration thereof, but there should still (in my opinion) be three lists *somewhere*?",
  "created_at":"2021-06-09T12:52:50Z",
  "id":857668098,
  "issue":905,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzY2ODA5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-09T12:57:14Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh yeah, it's not just an ambiguity between partitions, but also between nested lists if you're unflattering inside of existing lists, like with `axis!=1`. But they should appear somewhere, and you may have found a case in which the `current_offsets` mechanism is losing empties. It was probably only tested with a single empty list\u2014is it a situation in which only multiple, consecutive empty lists are being lost?",
  "created_at":"2021-06-09T12:59:27Z",
  "id":857672734,
  "issue":905,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzY3MjczNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-09T12:59:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski it looks that way (for `axis != 0`).",
  "created_at":"2021-06-09T13:00:22Z",
  "id":857673365,
  "issue":905,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzY3MzM2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-09T13:01:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Still, for my use case, perhaps I'll need to roll a custom version `unflatten` that has a predetermined set of rules for arranging these empty lists.",
  "created_at":"2021-06-09T13:02:13Z",
  "id":857674786,
  "issue":905,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzY3NDc4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-09T13:02:13Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Though old, this was still a bug and had to be fixed! (I'm putting the same corrections into the commented out code that's waiting to be ported to v2, so that it doesn't regress. Also, there's a test and all tests will be ported, too.",
  "created_at":"2021-12-07T21:46:09Z",
  "id":988284697,
  "issue":905,
  "node_id":"IC_kwDODBCWws466AcZ",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-12-07T21:46:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the report! Yes, `pyarrow.lib.FixedSizeListType` is new to me; it will need to be added as a case to convert. The Awkward equivalent is `ak.layout.RegularArray`, so this will be straightforward. (The testing might need to restrict to a limited set of pyarrow versions if it's very new.)",
  "created_at":"2021-06-09T18:44:30Z",
  "id":857971749,
  "issue":906,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1Nzk3MTc0OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-09T18:44:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This will probably need better test coverage, but I'm not entirely familiar with pyarrow, so I welcome any inputs.",
  "created_at":"2021-06-10T06:34:03Z",
  "id":858351228,
  "issue":907,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODM1MTIyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T06:34:03Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, so the implementation was there, but it was wrong? Simply untested? Then @VasilyFomin's example would serve as a test:\r\n\r\n```python\r\npyarrow = pytest.importorskip(\"pyarrow\")\r\n\r\ntpe = pyarrow.struct([\r\n    pyarrow.field('x', pyarrow.list_(pyarrow.float64(), 2)),\r\n    pyarrow.field('y', pyarrow.list_(pyarrow.float64(), 2)),\r\n])\r\narr = pyarrow.array([{'x': [1.0, 2.0], 'y': [3.0, 4.0]}], type=tpe)\r\nassert ak.from_arrow(arr).tolist() == [{\"x\": [1.0, 2.0]}, {\"y\": [3.0, 4.0]}]\r\n```",
  "created_at":"2021-06-10T12:39:29Z",
  "id":858586020,
  "issue":907,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODU4NjAyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T12:39:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Oh, so the implementation was there, but it was wrong? Simply untested? Then @VasilyFomin's example would serve as a test:\r\n> \r\n> ```python\r\n> pyarrow = pytest.importorskip(\"pyarrow\")\r\n> \r\n> tpe = pyarrow.struct([\r\n>     pyarrow.field('x', pyarrow.list_(pyarrow.float64(), 2)),\r\n>     pyarrow.field('y', pyarrow.list_(pyarrow.float64(), 2)),\r\n> ])\r\n> arr = pyarrow.array([{'x': [1.0, 2.0], 'y': [3.0, 4.0]}], type=tpe)\r\n> assert ak.from_arrow(arr).tolist() == [{\"x\": [1.0, 2.0]}, {\"y\": [3.0, 4.0]}]\r\n> ```\r\n\r\nThere was no switch case to handle the (new?) PyArrow type, or a test case, but the logic is very similar to the existing types. I added a test case that copies @VasilyFomin's example",
  "created_at":"2021-06-10T12:41:11Z",
  "id":858587285,
  "issue":907,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODU4NzI4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T12:41:26Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"@agoose77 Thanks for all of this! I only want to add a test to ensure that the type is right (fixed-length, not `var`) and a `pytest.mark.skip` for old pyarrow versions, if there are any versions we support that don't have this feature (I need to check).\r\n\r\nI've checked out your repo to make that one change, and then I'll do an auto-merge, but this would be much easier if you made pull requests off the main repo instead of your forked repo. (I wouldn't have to recompile it to test a Python change, for instance.) I just bumped up your permissions from \"Write\" to \"Maintain\" in case that was necessary. Thanks!",
  "created_at":"2021-06-10T16:09:06Z",
  "id":858753088,
  "issue":907,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODc1MzA4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T16:09:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks @jpivarski, I didn't consider that it would be easier for me to work on branches on the upstream, I'll do that from now on. There isn't much documentation/information about PyArrow, so this implementation is based upon my understanding + the test which passed first time. It's not a complex idea, a fixed length buffer though, so I am not too worried. Feel free to merge at will!",
  "created_at":"2021-06-10T16:12:45Z",
  "id":858755692,
  "issue":907,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODc1NTY5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T16:12:45Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"The FixedSizeListType has [existed since Arrow 0.16.0](https://github.com/apache/arrow/commit/1500d39aeccc5ed0137424fa806965103c50bc3a), and we require >= 2.0.0, so I don't need to put any test skips here.",
  "created_at":"2021-06-10T16:28:35Z",
  "id":858767546,
  "issue":907,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODc2NzU0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T16:28:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I'll have a look - it shouldn't be too difficult to implement.",
  "created_at":"2021-06-11T10:02:16Z",
  "id":859465386,
  "issue":909,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTQ2NTM4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T10:02:16Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@drahnreb and @jpivarski - It looks like most packages I looked at have a default unit for an integer value since epoch - `pyarrow`, for example uses `ms`, `pandas` - `ns`. If **no unit** is passed to a function, it is assumed that the integer value is given in these default units. \r\n\r\n`np.datetime64` is ambiguous. We should be able to specify units. Please, let me know if you have any suggestions?",
  "created_at":"2021-06-11T16:32:25Z",
  "id":859702133,
  "issue":909,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTcwMjEzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T16:32:25Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Quick note on Arrow, since I looked into it yesterday: they have a different default unit depending on whether the type is 32-bit or 64-bit!\r\n\r\n```python\r\n>>> pyarrow.date32()\r\nDataType(date32[day])\r\n>>> pyarrow.date64()\r\nDataType(date64[ms])\r\n```\r\n\r\nNumPy's datetime/timedelta dtypes have units, the corresponding Python type does not.\r\n\r\n```python\r\n>>> np.dtype(\"M8[s]\")\r\ndtype('<M8[s]')\r\n>>> np.dtype(\"M8[s]\").type\r\n<class 'numpy.datetime64'>\r\n\r\n>>> np.dtype(\"M8[ns]\")\r\ndtype('<M8[ns]')\r\n>>> np.dtype(\"M8[ns]\").type\r\n<class 'numpy.datetime64'>\r\n```\r\n\r\nAs a Python type (`np.datetime64`), the unit information is stored in the Python _value_, rather than the type.\r\n\r\n```python\r\n>>> np.datetime64(5, \"s\")\r\nnumpy.datetime64('1970-01-01T00:00:05')\r\n>>> np.datetime64(5, \"ns\")\r\nnumpy.datetime64('1970-01-01T00:00:00.000000005')\r\n```\r\n\r\nBut that's not possible for arrays of values (and you'd want it to be constant across an array), so there has to be this dtype/type distinction.\r\n\r\nWhen a user passes something (string, type, whatever) to the argument of `ak.values_astype`, it gets converted into a NumPy dtype, partly to ensure that that's possible before getting too deep into the code. After that regularization step, the object is a dtype and must have a unit.\r\n\r\nAh, I see: this is the problem.\r\n\r\n```python\r\n>>> np.dtype(np.datetime64)\r\ndtype('<M8')\r\n```\r\n\r\nSimply passing the type to the dtype constructor doesn't give it a unit. Still, there must be some unit assumed. Let me see...\r\n\r\n```python\r\n>>> np.array([1, 2, 3], dtype=np.dtype(np.datetime64))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nValueError: Converting an integer to a NumPy datetime requires a specified unit\r\n>>> np.array([1, 2, 3], dtype=np.datetime64)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nValueError: Converting an integer to a NumPy datetime requires a specified unit\r\n>>> dir(np.dtype(np.datetime64))\r\n```\r\n\r\nI was going to try to learn the unit from some examples, but NumPy isn't letting me construct examples. As such, we can refuse to admit datetime/timedelta dtypes without units for the same reasons.\r\n\r\nWhen you use NumPy's `astype` with a datetime without units, it just doesn't change the units.\r\n\r\n```python\r\n>>> np.array([1, 2, 3], \"M8[D]\")\r\narray(['1970-01-02', '1970-01-03', '1970-01-04'], dtype='datetime64[D]')\r\n>>> np.array([1, 2, 3], \"M8[D]\").astype(\"M8[s]\")\r\narray(['1970-01-02T00:00:00', '1970-01-03T00:00:00',\r\n       '1970-01-04T00:00:00'], dtype='datetime64[s]')\r\n>>> np.array([1, 2, 3], \"M8[D]\").astype(\"M8\")\r\narray(['1970-01-02', '1970-01-03', '1970-01-04'], dtype='datetime64[D]')\r\n>>> np.array([1, 2, 3], \"M8[D]\").astype(np.datetime64)\r\narray(['1970-01-02', '1970-01-03', '1970-01-04'], dtype='datetime64[D]')\r\n```\r\n\r\nWe can't do that because we don't know how deep it's going to be, unless we implement this logic in the NumpyArray class (in C++). That's the only place where we'd know what the current unit is, in order to not change it.\r\n\r\n```python\r\n>>> np.array([1, 2, 3]).astype(\"M8[D]\")\r\narray(['1970-01-02', '1970-01-03', '1970-01-04'], dtype='datetime64[D]')\r\n>>> np.array([1, 2, 3]).astype(\"M8\")\r\nValueError: Cannot convert a NumPy datetime value other than NaT with generic units\r\n>>> np.array([1, 2, 3]).astype(np.datetime64)\r\nValueError: Cannot convert a NumPy datetime value other than NaT with generic units\r\n```\r\n\r\nYeah, the only valid case where a unitless datetime/timedelta (I haven't explicitly checked the timedeltas!) is when you're changing a type that is already a datetime/timedelta, which is to say, when it's not changing at all. That's not a very interesting/useful case. We can require the dtypes to have units.",
  "created_at":"2021-06-11T17:43:51Z",
  "id":859740297,
  "issue":909,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTc0MDI5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T17:43:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes, a datetime coercion would need to include units but the python type objects `np.datetime64` are unitless (generic datetimes that only work on `NaT` e.g. `np.array('NaT', dtype=np.datetime64)`) until further specified as numpy dtypes.\r\nNumpy stores this time unit in the metadata (in the original [PEP 4](https://numpy.org/neps/nep-0004-datetime-proposal3.html) it used to be implemented in the metadata property of a `np.dtype('m8[ns]').metadata` that is `None` in my case and moved to the .name/.str props...) to complement datetime dtypes in their expressiveness without changing the underlying structure.\r\n\r\nBoth short string notations ought to default to `[us]` as unit:\r\n`M{m}8` is equivalent to `M{m}8[us]`.\r\n\r\nAdditionally, you can set the units of datetimes/timedeltas with the _long string notation_ (I use `'timedelta64[ns]'` here, but they have coherent behavior in numpy):\r\n```python\r\n>>> np.dtype('m8[ns]').name\r\n'timedelta64[ns]'\r\n>>> np.dtype('timedelta64[ns]')\r\ndtype('<m8[ns]')\r\n>>> type(np.dtype('<m8[ns]')) is type(np.dtype('m8[ns]')) is type(np.dtype('timedelta64[ns]'))\r\nTrue\r\n\r\n# np.core._multiarray_umath.DATETIMEUNITS  # NULL PyCapsule obj, unfortunately the C++ enum doesn't get broadcasted into python space\r\n# business days 'B' are deprecated:\r\n>>> np_base_units = (\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \"us\", \"ns\", \"ps\", \"fs\", \"as\")\r\n>>> [np.array([1]).astype(f'timedelta64[{unit}]') for unit in np_base_units]\r\n[array([1], dtype='timedelta64[Y]'),\r\n array([1], dtype='timedelta64[M]'),\r\n array([1], dtype='timedelta64[W]'),\r\n array([1], dtype='timedelta64[D]'),\r\n array([1], dtype='timedelta64[h]'),\r\n array([1], dtype='timedelta64[m]'),\r\n array([1], dtype='timedelta64[s]'),\r\n array([1], dtype='timedelta64[ms]'),\r\n array([1], dtype='timedelta64[us]'),\r\n array([1], dtype='timedelta64[ns]'),\r\n array([1], dtype='timedelta64[ps]'),\r\n array([1], dtype='timedelta64[fs]'),\r\n array([1], dtype='timedelta64[as]')]\r\n\r\n>>> [np.array([1]).astype(f'datetime64[{unit}]') for unit in np_base_units]\r\n[array(['1971'], dtype='datetime64[Y]'),\r\n array(['1970-02'], dtype='datetime64[M]'),\r\n array(['1970-01-08'], dtype='datetime64[W]'),\r\n array(['1970-01-02'], dtype='datetime64[D]'),\r\n array(['1970-01-01T01'], dtype='datetime64[h]'),\r\n array(['1970-01-01T00:01'], dtype='datetime64[m]'),\r\n array(['1970-01-01T00:00:01'], dtype='datetime64[s]'),\r\n array(['1970-01-01T00:00:00.001'], dtype='datetime64[ms]'),\r\n array(['1970-01-01T00:00:00.000001'], dtype='datetime64[us]'),\r\n array(['1970-01-01T00:00:00.000000001'], dtype='datetime64[ns]'),\r\n array(['1970-01-01T00:00:00.000000000001'], dtype='datetime64[ps]'),\r\n array(['1970-01-01T00:00:00.000000000000001'], dtype='datetime64[fs]'),\r\n array(['1970-01-01T00:00:00.000000000000000001'], dtype='datetime64[as]')]\r\n```\r\n\r\nAs a side note (I barely used it): modulo event-units  can also be combined with any derived units and both dtypes in long and short notation:\r\n```python\r\n>>> np.array([1]).astype('m8[10s/2]')\r\narray([1], dtype='timedelta64[5000ms]')\r\n>>> np.array([1]).astype('datetime64[100as/1]')\r\narray(['1970-01-01T00:00:00.000000000000000100'],\r\n      dtype='datetime64[100as]')\r\n\r\n```\r\n",
  "created_at":"2021-06-12T12:14:37Z",
  "id":860045386,
  "issue":909,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDA0NTM4Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-12T12:14:37Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@drahnreb - Could you, please, check https://github.com/scikit-hep/awkward-1.0/pull/916? Thanks.",
  "created_at":"2021-06-15T11:34:49Z",
  "id":861422705,
  "issue":909,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTQyMjcwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T11:34:49Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> @drahnreb - Could you, please, check #916? Thanks.\r\n\r\nGreat, lgtm. Thanks, @ianna!",
  "created_at":"2021-06-15T16:14:45Z",
  "id":861635362,
  "issue":909,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTYzNTM2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T16:14:45Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"I think you may be right, though I'm confused by the examples that use the layout itself (through `run_lengths`) to generate counts.\r\n\r\nThe one high-level point I can make is that IndexedArrays must be invisible at high-level. It's essentially applying a lazy \"`carry`\" (non-negative integer array slice) at some point in the tree, and whatever happens, such as `unflatten`, should not depend on whether the \"`carry`\" is lazily applied or eagerly applied.\r\n\r\nThe intended implementation would have eagerly applied the \"`carry`\" during the first recursive descent. If a new option, off by default, needs to be added to `ak.util.recursively_apply` to implement that, that's okay.",
  "created_at":"2021-06-11T11:20:19Z",
  "id":859511170,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTUxMTE3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T11:20:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi Jim, thanks for chiming in. The `run_lengths` on content just skips the outer `IndexedArray`. This *works* because the `IndexedArray` doesn't modify the ordering *within* sublists. Usually the user would be running this on the actual Array, but I was being a bit lazy.\r\n\r\nI agree that `unflatten` should *not* care about the `IndexedArray` vs `carry`, and indeed `run_lengths` behaves as expected - it respects all of the layouts. It's just the current implementation for `unflatten` that doesn't. \r\n\r\nI've been playing around with a kind of simplification that merges successive layouts together. For example\r\n```python3\r\nlayout = ak.layout.IndexedArray64(\r\n    ak.layout.Index64([1, 0]),\r\n    ak.layout.ListOffsetArray64(\r\n        ak.layout.Index64([0, 2, 4]),\r\n        ak.layout.ListOffsetArray64(\r\n            ak.layout.Index64([0, 3, 7, 9, 12]),\r\n            ak.layout.NumpyArray(([0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 3, 3])),\r\n        ),\r\n    ),\r\n)\r\n```\r\ncan be simplified top down (once) to become (via project)\r\n```python3\r\nlayout1 = ak.layout.ListArray64(\r\n    ak.layout.Index64([2, 0]),\r\n    ak.layout.Index64([4, 2]),\r\n    ak.layout.ListOffsetArray64(\r\n        ak.layout.Index64([0, 3, 7, 9, 12]),\r\n        ak.layout.NumpyArray(([0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 3, 3])),\r\n    ),\r\n)\r\n```\r\nand\r\n```python3\r\nlayout2 = ak.layout.ListOffsetArray64(\r\n    ak.layout.Index64([0, 2, 4]),\r\n    ak.layout.ListOffsetArray64(\r\n        ak.layout.Index64([0, 2, 5, 8, 12]),\r\n        ak.layout.IndexedArray64(\r\n            ak.layout.Index64([7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5, 6]),\r\n            ak.layout.NumpyArray(([0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 3, 3])),\r\n        ),\r\n    ),\r\n)\r\n```\r\n\r\nI don't know whether this approach is the \"right\" one - it avoids copying the contents (which is good for a record array), but involves creating a number of intermediate layouts.",
  "created_at":"2021-06-11T11:36:02Z",
  "id":859518887,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTUxODg4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T11:38:44Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"So here's an incomplete simplifier that handles this particular layout (and makes a copy whilst it's at it). It's probably wrong e.g. with the assertion, but it's a PoC rather than production code. I don't know whether this approach is the best one. The idea here is that by moving all reordering to the inner-most level, then the flattened counts will correlate to the layout. One way to do this is:\r\n\r\n* Move all `IndexedArray` routines to the inner buffer\r\n* Change `ListArray`s to `ListOffsetArray`s.\r\n\r\n```python3\r\n\r\ndef replace_content(layout, content):\r\n    def getfunction(this, depth):\r\n        if this is layout:\r\n            return\r\n        return lambda: content\r\n\r\n    return ak._util.recursively_apply(layout, getfunction)\r\n\r\n\r\ndef simplify(layout):\r\n    if hasattr(layout, \"project\"):\r\n        return simplify(layout.project())\r\n\r\n    # Now we don't have an IndexedArray\r\n    if not hasattr(layout, \"content\"):\r\n        return layout\r\n\r\n    # We now have only listtypes\r\n    assert isinstance(layout, ak._util.listtypes)\r\n\r\n    # ListArrays can be converted to another case (reduce dimension of problem)\r\n    if isinstance(layout, ak.layout.ListArray64):\r\n        layout = layout.toListOffsetArray64(True)\r\n        return simplify(layout)\r\n\r\n    # When arrays are\r\n    # - structural\r\n    # - contiguous\r\n    # then we can't optimise any further, and should move on\r\n    if isinstance(layout, (ak.layout.ListOffsetArray64, ak.layout.RegularArray)):\r\n        return replace_content(layout, simplify(layout.content))\r\n    \r\n    raise NotImplementedError\r\n```\r\n\r\nYou know much better what is going on under the hood with respect to copying, RecordArrays, etc. Do you think this approach is the right one here? Is this kind of multi-layout modification possible w.r.t all of the possible layouts (including partitioned arrays)?\r\n",
  "created_at":"2021-06-11T12:33:52Z",
  "id":859549638,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTU0OTYzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T12:39:48Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"We shouldn't simplify more than we need to. For example, replacing ListArrays with ListOffsetArrays doesn't have anything to do with the current problem, and ListArrays exist to avoid excessive reordering.\r\n\r\nI think your current problem with `unflatten` is because the non-list nodes directly above the node whose length changes need to be simplified: e.g. IndexedArrays at this level only need to be projected.\r\n\r\nIt's better to make a chain of `elif isinstance(layout, node_classes)` than to test for the existence of attributes like `hasattr(layout, \"project\")`. IndexedOptionArray also has a `project` method and it doesn't do what you want: it removes None values.\r\n\r\nUnfortunately for making things local and factorizable, implementing any method generally requires the author to know about all node types. OOP is usually against that, but there was no way to avoid it and do what Awkward Array does. This is the Nth iteration on the set of node types and very final\u2014the only possible new node type is RedirectArray (#178), but that may be unnecessary now that the C++ is being translated into Python (thanks to the garbage collector).\r\n\r\nSo this problem can be fixed by considering each of the possible node types just above the node that changes its length: IndexedArrays need to be projected, etc.",
  "created_at":"2021-06-11T13:06:08Z",
  "id":859568385,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTU2ODM4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T13:06:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> We shouldn't simplify more than we need to. For example, replacing ListArrays with ListOffsetArrays doesn't have anything to do with the current problem, and ListArrays exist to avoid excessive reordering.\r\n\r\nYes, in my PoC I just convert things to reduce the number of combinations to consider.\r\n\r\n> So this problem can be fixed by considering each of the possible node types just above the node that changes its length: IndexedArrays need to be projected, etc.\r\n\r\nFor posterity, I'll add that it's both length and ordering that we need to handle here. \r\n\r\n> I think your current problem with unflatten is because the non-list nodes directly above the node whose length changes need to be simplified: e.g. IndexedArrays at this level only need to be projected.\r\n\r\n@jpivarski that's true - we can just project the `IndexedArray`s because the buffers below the axis depth won't be at risk of being copied.\r\n\r\n> It's better to make a chain of elif isinstance(layout, node_classes) than to test for the existence of attributes like hasattr(layout, \"project\"). IndexedOptionArray also has a project method and it doesn't do what you want: it removes None values.\r\n\r\nNoted, thanks. Old habits die hard I suppose :) I noticed while writing the visitor that it would be useful if the `apply` method of `recursively_apply` were available to call by `getfunction`, in the case where it wants to operate at multiple depths sequentially. Perhaps it should be an optional argument to `getfunction`?",
  "created_at":"2021-06-11T13:20:53Z",
  "id":859577488,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTU3NzQ4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T13:22:42Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> I noticed while writing the visitor that it would be useful if the `apply` method of `recursively_apply` were available to call by `getfunction`, in the case where it wants to operate at multiple depths sequentially. Perhaps it should be an optional argument to `getfunction`?\r\n\r\nMaybe I'm thinking of `ak._util.broadcast_and_apply`, but I think there's a way to pass down user data. I used that elsewhere to make it applicable at multiple depths. There were other cases (`ak.combinations`) in which we wanted to act at multiple levels and I did it through `getfunctions` that launch new `ak._util.broadcast_and_apply`, though that got complicated fast.\r\n\r\n`ak._util.broadcast_and_apply` and `ak._util.recursively_apply` are purely internal functions. We can add whatever we need to them to handle new use-cases, though it has to not break existing users (within our codebase) of these functions. That is, the API can be ugly, but it has to keep working.",
  "created_at":"2021-06-11T14:17:44Z",
  "id":859614488,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTYxNDQ4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T14:17:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, there is a `pass_user` which is currently used for the `axis` parameter. I will add a `pass_apply` argument to `recursively_apply` so that the getfunction can call the implementation for sub-trees. It will be `False` by default to ensure that existing use cases are not broken!",
  "created_at":"2021-06-11T14:21:29Z",
  "id":859616839,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTYxNjgzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T14:21:29Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski once #912 is ready to go, what do you think we should do here? Might feeling is that unless you actively care about what's going on under the hood, the average user should not need to be aware that `unflatten` *might* produce the wrong results if their array has experienced a particular set of transformations. This leads me to conclude that `packed` should be called by unflatten, right up to the axis that is being unflattened. This *does* make `unflatten` more expensive, but I think that's an inevitability rather than an implementation issue. ",
  "created_at":"2021-06-14T08:27:19Z",
  "id":860494445,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDQ5NDQ0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T08:27:19Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Possibly producing wrong results is always bad. If `unflatten` has to become more expensive (doing a `_packed` pass on it) to ensure that it is correct, then that's what needs to be done. It possibly only needs to be done if you see an IndexedArray in `unflatten` though: that's a decision that can be made there, and the backdoor in `_packed` could be much less general than an `axis` argument, it could be a `is_shallow` argument, saying to only pack at the top node level or the top dimension (always easier than trying to limit an operation to one axis somewhere deep within a structure). Such a thing might fix the error in `unflatten` without having to rewrite the array all the way down to the leaves of the tree.",
  "created_at":"2021-06-14T14:36:27Z",
  "id":860736358,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDczNjM1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T14:36:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I *think* it is caused by any layout that re-orders / resizes the internal array - `unflatten` introduces a new layout over the internal one, which uses offsets derived from the counts array. So, any layouts at a higher level that re-arrange things, e.g. ListArray, or ListOffsetArray with `offsets[0]!=0` break things.\r\n\r\nBecause unflatten can be called for any depth, the \"fix\" needs to be able to apply to any depth. I think the fact that UnionArray's break the axis wrapping assumption may not be a problem ... because unflatten can't handle unions above the axis if my assumptions are correct.",
  "created_at":"2021-06-14T14:39:32Z",
  "id":860738708,
  "issue":910,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDczODcwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T14:42:59Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"yes, it looks like only `ak.layout.NumpyArray` with `M8[ns]` format is affected:\r\n```python\r\n>>> array = ak.layout.NumpyArray(df_ns)\r\n>>> array\r\n<NumpyArray format=\"M8[ns]\" shape=\"3 1\" strides=\"8 0\" data=\"0x 00f00cda 1795c015 00f07118 7af5c315 00807ca2 7f8cc615\" at=\"0x7ffb72d685e0\"/>\r\n>>> array[0]\r\n<NumpyArray format=\"M8[ns]\" shape=\"1\" strides=\"0\" data=\"2019-09-02T09:30:00\" at=\"0x7ffb72d685e0\"/>\r\n>>> ak.to_list(array)\r\n[[1567416600000000000], [1568367000000000000], [1569096000000000000]]\r\n>>> ak.to_list(array[0])\r\n[1567416600000000000]\r\n```\r\n\r\n`PyArrow` converts a NumPy `datetime64` array to a `TimestampArray`:\r\n```python\r\n>>> numpy_array = np.array([\"2020-07-27T10:41:11\", \"2019-01-01\", \"2020-01-01\"], \"datetime64[ns]\")\r\n>>> numpy_array\r\narray(['2020-07-27T10:41:11.000000000', '2019-01-01T00:00:00.000000000',\r\n       '2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\r\n>>> nparr = pyarrow.array(numpy_array)\r\n>>> nparr\r\n<pyarrow.lib.TimestampArray object at 0x7ffb68a6d820>\r\n[\r\n  2020-07-27 10:41:11.000000000,\r\n  2019-01-01 00:00:00.000000000,\r\n  2020-01-01 00:00:00.000000000\r\n]\r\n```\r\n",
  "created_at":"2021-06-11T16:19:27Z",
  "id":859693341,
  "issue":911,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTY5MzM0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T16:19:27Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Are we required to reproduce NumPy's `tolist` behavior?\r\n\r\nI would say no. When it comes to timestamp handling everyone is doing it differently and even inconsistently within the same package. With my [latest comment](https://github.com/scikit-hep/awkward-1.0/issues/367#issuecomment-859169528) I opted for a most coherent solution (that introduces a new way - but for the good...).\r\nList of py_obj or not I think you should always expect the same datatype (be it `np.datetime` or `int` etc.) no matter what unit.\r\n\r\n\r\n> Are all datetimes turned into integers, or just the ns resolution ones?\r\n\r\nLike @ianna described, this is just the case for `ns` resolution. `datetime.datetime` does not support `ns` (or even beyond).\r\n<details>\r\n  <summary>comment in numpy/core/src/multiarray/_datetime.h</summary>\r\n\r\n```c++\r\n    # /*\r\n    # * Converts a datetime into a PyObject *.\r\n    # *\r\n    # * For days or coarser, returns a datetime.date.\r\n    # * For microseconds or coarser, returns a datetime.datetime.\r\n    # * For units finer than microseconds, returns an integer.\r\n    # */\r\n    # NPY_NO_EXPORT PyObject *\r\n    # convert_datetime_to_pyobject(npy_datetime dt, PyArray_DatetimeMetaData *meta);\r\n\r\n```\r\n\r\n[Link](https://github.com/numpy/numpy/blob/fb215c76967739268de71aa4bda55dd1b062bc2e/numpy/core/src/multiarray/_datetime.h#L272) and the same [applies for timedelta.](https://github.com/numpy/numpy/blob/fb215c76967739268de71aa4bda55dd1b062bc2e/numpy/core/src/multiarray/_datetime.h#L282)\r\n\r\n</details>\r\n\r\n> What about timedelta objects, which are more number-like than datetimes (in the sense that they have a meaningful zero)?\r\n\r\nSimilar, just that `datetime[Y]` and `datetime[M]` is converted into int too - just like anything with precision greater than `datetime[us]`. (I think the first might be a `numpy` bug)\r\n\r\n\r\n> what does pyarrow's `to_pyobj` do?\r\n\r\nOf the four precision that `pyarrow` supports `(\"s\", \"ms\", \"us\", \"ns\")`, `to_pylist` converts to either `datetime.datetime` objects when possible otherwise to `pa.Timestamp`.\r\n\r\n> I'm inclined to reproduce NumPy's behavior, strange as it is, but could we check it first? For both datetime and timedelta, and for different resolutions?\r\n\r\nI compiled quick snippet of conversions for various combinations (could be partially included in tests, if necessary and a behavior is reproduced).\r\n\r\n<details>\r\n  <summary>Code</summary>\r\n\r\n```python\r\n    import numpy as np\r\n    # general datetime units; neglecting modulo event units e.g. [60s]//10\r\n    np_base_units = (\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \"us\", \"ns\", \"ps\", \"fs\", \"as\")\r\n\r\n    np_dts = {bu: np.datetime64('1970', bu) for bu in np_base_units}  #-01-01T01:01:01\r\n    np_tds = {bu: np.timedelta64(1, bu) for bu in np_base_units}\r\n\r\n    # NaT\r\n    np_special_units = ('NaT', \"\", None)\r\n    np_dt_spec = {u: np.datetime64(u) for u in np_special_units}\r\n    np_td_spec = {u: np.timedelta64(u) for u in np_special_units}\r\n    \r\n    py_scalar_from_np_dt = {unit: timeobj.item() for unit, timeobj in np_dts.items()}\r\n    py_scalar_from_np_td = {unit: timeobj.item() for unit, timeobj in np_tds.items()}\r\n\r\n    # tolist converts to the \"nearest compatible builtin python type\" via .item()\r\n    dt_arrays = {unit: np.asarray(timeobj) for unit, timeobj in np_dts.items()}\r\n    dt_arrays['mixed'] = np.asarray(list(py_scalar_from_np_dt.values()))\r\n    td_arrays = {unit: np.asarray(timeobj) for unit, timeobj in np_tds.items()}\r\n    td_arrays['mixed'] = np.asarray(list(py_scalar_from_np_td.values()))\r\n\r\n    py_list_from_np_dt = {unit: timearr.tolist() for unit, timearr in dt_arrays.items()}\r\n    py_list_from_np_td = {unit: timearr.tolist() for unit, timearr in td_arrays.items()}\r\n\r\n\r\n    import pyarrow as pa\r\n    pa_base_units = (\"s\", \"ms\", \"us\", \"ns\")\r\n\r\n    pa_t32s = [pa.time32(u) for u in pa_base_units[:2]]\r\n    pa_t64s = [pa.time64(u) for u in pa_base_units[2:]]\r\n    pa_ts = [pa.timestamp(u) for u in pa_base_units]\r\n    \r\n    import datetime\r\n    np_from_pa_ts = {pa.array(\r\n            [datetime.datetime(2002, 1, 23), datetime.datetime(2019, 2, 20)],\r\n            type=t,\r\n        ).to_numpy(False).dtype: pa.array(\r\n            [datetime.datetime(2002, 1, 23), datetime.datetime(2019, 2, 20)],\r\n            type=t,\r\n        ).to_numpy(False) for t in pa_ts}\r\n    np_from_pa_opt = {pa.array(\r\n            [None, datetime.datetime(2002, 1, 23)],\r\n            type=t,\r\n        ).to_numpy(False).dtype: pa.array(\r\n            [None, datetime.datetime(2002, 1, 23)],\r\n            type=t,\r\n        ).to_numpy(False) for t in pa_ts}\r\n\r\n    def try_numpy_roundtrip(pt):\r\n        try: return pa.array([pt]).to_numpy(False)\r\n        except BaseException as e: return str(e)\r\n    pa_dts = {repr(pt): try_numpy_roundtrip(pt) for pt in np_dts.values()}\r\n    pa_tds = {repr(pt): try_numpy_roundtrip(pt) for pt in np_tds.values()}\r\n\r\n    def try_to_py_obj(pt):\r\n        try: return pa.array([pt]).to_pylist()\r\n        except BaseException as e: return str(e)\r\n    py_list_from_pa_dts = {repr(pt): try_to_py_obj(pt) for pt in np_dts.values()}\r\n    py_list_from_pa_tds = {repr(pt): try_to_py_obj(pt) for pt in np_tds.values()}\r\n\r\n    # dtype conversion from numpy\r\n    def try_from_numpy(np_type):\r\n        try: return pa.from_numpy_dtype(np_type)\r\n        except BaseException as e: return str(e)\r\n    \r\n    pa_conv_dts = {unit + f'_{repr(np_dt)}': try_from_numpy(np_dt) for unit, np_dt in np_dts.items()}\r\n    pa_conv_dt_special = {str(unit) + f'_{repr(np_dt)}': try_from_numpy(np_dt) for unit, np_dt in np_dt_spec.items()}\r\n\r\n    pa_conv_tds = {unit + f'_{repr(np_td)}': try_from_numpy(np_td) for unit, np_td in np_tds.items()}\r\n    pa_conv_td_special = {str(unit) + f'_{repr(np_td)}': try_from_numpy(np_td) for unit, np_td in np_td_spec.items()}\r\n\r\n    import pandas as pd\r\n    pd_ts = pd.DataFrame.from_dict([np_dts], orient='columns')\r\n    # ignored:  pd_ts = pd.DataFrame.from_dict([np_dts['m']], orient='columns').astype('datetime64[M]')\r\n    pd_ts_dtypes = pd_ts.dtypes\r\n    pa_table = pa.Table.from_pandas(pd_ts)\r\n    table_schema = pa_table.schema\r\n    py_dict_from_pa_table = pa_table.to_pydict()\r\n\r\n    ... # ellipsis for debug breakpoint\r\n```\r\n\r\n</details>\r\n\r\nIt's quite a mess... Honestly, as long as int conversions work and data can be loaded, I'd say most applications are feasible.\r\n(in the python space - besides the arithmetics with datetime data that this great PR enabled in the awkward space!)",
  "created_at":"2021-06-12T04:15:18Z",
  "id":859994906,
  "issue":911,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTk5NDkwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-12T04:20:58Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"@drahnreb, I see that https://github.com/scikit-hep/awkward-1.0/issues/367#issuecomment-859169528 describes two different options for what `tolist` could do:\r\n\r\n   * return `datetime.datetime` objects, but there are resolutions it doesn't handle\r\n   * return `np.datetime64` objects (scalars), but Nones from option-type (or worse from union-type) would make these arrays read back into NumPy as `dtype=\"O\"`.\r\n\r\nAs you say above, it should consistently return only one type, and I agree wholeheartedly. The lack of some resolutions for `datetime.datetime`, I believe, rules that out. I think it's fine to always return `np.datetime64` objects from `tolist`, even though there are cases that _if read back into NumPy_ would be read back as `dtype=\"O\"`. You could say the same thing about\r\n\r\n```python\r\n>>> np.array(ak.Array([1, 2, 3, None, None, 6]).tolist())\r\narray([1, 2, 3, None, None, 6], dtype=object)\r\n```\r\n\r\nSo I think having `tolist` always return `np.datetime64` from an `ak.Array` of `np.datetime64` (or `np.timedelta64` from an `ak.Array` of `np.datetime64`) is best. I see no reason to be consistent with this:\r\n\r\n```python\r\n>>> np.array([1, 2, 3], \"M8[us]\").tolist()\r\n[datetime.datetime(1970, 1, 1, 0, 0, 0, 1), datetime.datetime(1970, 1, 1, 0, 0, 0, 2), datetime.datetime(1970, 1, 1, 0, 0, 0, 3)]\r\n>>> np.array([1, 2, 3], \"M8[ns]\").tolist()\r\n[1, 2, 3]\r\n```\r\n\r\n(Also unlike other NumPy functions, we haven't been attempting to make `tolist` behave like NumPy's `tolist`: consider, for instance, NumPy masked arrays, which return a `masked` object, whereas we would return `None`. Maybe we should have followed pyarrow's `to_pyobj` from the beginning and matched all the types there\u2014and we still can, since we haven't use the name \"`to_pyobj`\" yet.)\r\n\r\nSo let's just return `np.datetime64` (and `np.timedelta64`) objects from `tolist` uniformly. Does that work for you, @drahnreb?",
  "created_at":"2021-06-14T15:14:10Z",
  "id":860766039,
  "issue":911,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDc2NjAzOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-14T15:14:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna and I agreed to close this: the desired behavior is in main.",
  "created_at":"2021-06-15T12:14:01Z",
  "id":861447213,
  "issue":911,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTQ0NzIxMw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-15T12:14:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> So let's just return `np.datetime64` (and` np.timedelta64`) objects from `tolist` uniformly.  Does that work for you, @drahnreb?\r\n\r\nYes, that works, @jpivarski. \ud83d\udc4d\ud83c\udffc\r\n\r\n> Maybe we should have followed pyarrow's to_pyobj from the beginning and matched all the types there\u2014and we still can, since we haven't use the name \"to_pyobj\" yet.\r\n\r\nI assume you mean [`pyarrow.NumericArray.to_pylist`](https://arrow.apache.org/docs/python/generated/pyarrow.NumericArray.html#pyarrow.NumericArray.to_pylist) (and `py_dict` on Tables) which acts on a scalar level via `.as_py()`.\r\nIf `to_pyobj` gets implemented I\u2018d _maybe_ suggest the coercion type `int` (and `None` instead of the generic time types for eg. `NaT`) as pure python objects.\r\nThus, drop unit information (one would usually know the uniform array\u2019s unit) but enforce consistent behavior of the method independent of underlying different units (given uniformity along axis).\r\n\r\nSimply to prevent this (lost information, or sloppy behavior at day unit):\r\n```python\r\n>>> supported_py_datetime_units = (\"D\", \"M\", \"h\", \"m\", \"s\", \"us\")\r\n>>> _np_times = np.datetime64, np.timedelta64\r\n>>> for unit in supported_py_datetime_units:\r\n        for _method in _np_times:\r\n            arr = np.array([_method(1, unit)])\r\n            print(repr(arr))\r\n            try:\r\n                parr = pa.array(arr)\r\n                pa_type = parr.type\r\n                py_type = type(parr.to_pylist()[0])\r\n            except pa.lib.ArrowNotImplementedError as e:\r\n                pa_type = f'{e}: {unit}'\r\n                py_type = '-'\r\n\r\n            print('  -> pyarrow type: ', pa_type)\r\n            print('  -> python type: ', py_type)\r\n\r\n        print('\\n')\r\n\r\narray(['1970-01-02'], dtype='datetime64[D]')\r\n  -> pyarrow type:  date32[day]\r\n  -> python type:  <class 'datetime.date'>\r\narray([1], dtype='timedelta64[D]')\r\n  -> pyarrow type:  Unsupported timedelta64 time unit: D\r\n  -> python type:  -\r\n\r\n\r\narray(['1970-02'], dtype='datetime64[M]')\r\n  -> pyarrow type:  Unsupported datetime64 time unit: M\r\n  -> python type:  -\r\narray([1], dtype='timedelta64[M]')\r\n  -> pyarrow type:  Unsupported timedelta64 time unit: M\r\n  -> python type:  -\r\n\r\n\r\narray(['1970-01-01T01'], dtype='datetime64[h]')\r\n  -> pyarrow type:  Unsupported datetime64 time unit: h\r\n  -> python type:  -\r\narray([1], dtype='timedelta64[h]')\r\n  -> pyarrow type:  Unsupported timedelta64 time unit: h\r\n  -> python type:  -\r\n\r\n\r\narray(['1970-01-01T00:01'], dtype='datetime64[m]')\r\n  -> pyarrow type:  Unsupported datetime64 time unit: m\r\n  -> python type:  -\r\narray([1], dtype='timedelta64[m]')\r\n  -> pyarrow type:  Unsupported timedelta64 time unit: m\r\n  -> python type:  -\r\n\r\n\r\narray(['1970-01-01T00:00:01'], dtype='datetime64[s]')\r\n  -> pyarrow type:  timestamp[s]\r\n  -> python type:  <class 'datetime.datetime'>\r\narray([1], dtype='timedelta64[s]')\r\n  -> pyarrow type:  duration[s]\r\n  -> python type:  <class 'datetime.timedelta'>\r\n\r\n\r\narray(['1970-01-01T00:00:00.000001'], dtype='datetime64[us]')\r\n  -> pyarrow type:  timestamp[us]\r\n  -> python type:  <class 'datetime.datetime'>\r\narray([1], dtype='timedelta64[us]')\r\n  -> pyarrow type:  duration[us]\r\n  -> python type:  <class 'datetime.timedelta'>\r\n\r\n```\r\n\r\nGenerally, working with time gets non-trivial easily and in this case the sdtlib `datetime.datetime` is a limiting factor. Requirements for time data are very use case specific, but I would say a) arithmetics are not done in the python space b) scientific arithmetics tend to require precisions beyond the provided `datetime` units c) `datetime.datetime` is left with the task of \"formatting\" and d) you can work your way around it with a much simpler type or convert back.\r\nAs such, I currently do not see why a list of maybe richter but not guaranteed `datetime.datetime` obj would be preferred ever (maybe pickling)?",
  "created_at":"2021-06-15T15:53:37Z",
  "id":861618794,
  "issue":911,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTYxODc5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T15:53:37Z",
  "user":"MDQ6VXNlcjI1ODgzNjA3"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks @drahnreb, I did mean `to_pylist` and `to_pydict`. (I didn't look up and check the name before writing it.)\r\n\r\nSo Arrow returns `datetime.datetime` objects, too... sometimes. Well, we're satisfying criteria (a) and (b) at least.\r\n\r\nI had another thought about this, which is that `np.datetime64`/`np.timedelta64` can easily be converted into `datetime.datetime`/`datetime.timedelta`:\r\n\r\n```python\r\n>>> numpy.datetime64(1, \"s\").item()\r\ndatetime.datetime(1970, 1, 1, 0, 0, 1)\r\n>>> numpy.timedelta64(1, \"s\").item()\r\ndatetime.timedelta(seconds=1)\r\n```\r\n\r\nbut the reverse isn't true. We strictly depend on NumPy, so it's definitely available, and that makes returning `np.datetime64`/`np.timedelta64` more powerful than returning `datetime.datetime`/`datetime.timedelta`. If you have a list of NumPy datetimes, you can write a very easy list comprehension to turn them into standard library datetimes, but it's not as easy the other way around. `np.datetime64(stdlib_datetime)` is not much harder than `np_datetime.item()`, but dealing with inconsistent types is.\r\n\r\nSo I think we all agree that Awkward Array's `tolist` should continue returning NumPy `np.datetime64`/`np.timedelta64`, as it is in main.",
  "created_at":"2021-06-15T19:38:24Z",
  "id":861779116,
  "issue":911,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTc3OTExNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-15T19:38:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I'm going to be working on this but please feel free to make suggestions at any time!",
  "created_at":"2021-06-11T14:33:17Z",
  "id":859624791,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTYyNDc5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T14:33:17Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I'm not sure how to handle `VirtualArray`. It could just pass through to avoid realising it, but equally, it would be confusing if you had to trigger a load in order for `simplify` to traverse into it. Perhaps using the same flag `materialize=True` as `recursive_walk` (but default to `True`) would be the right thing here?",
  "created_at":"2021-06-11T14:38:24Z",
  "id":859628101,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTYyODEwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T14:45:34Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> Perhaps a flag `realise_virtual` (or something)\r\n\r\nThe word that we use for that is \"materialize,\" though I think it would always be true for broadcasting or recursively applying. Operations should be expected to make a lazy array non-lazy. That's what this does:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/94de4e5112ad3a2d5fc9c2ec0fc29e242543a0d6/src/awkward/_util.py#L1312-L1313\r\n",
  "created_at":"2021-06-11T14:45:52Z",
  "id":859633214,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTYzMzIxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T14:45:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > Perhaps a flag `realise_virtual` (or something)\r\n> \r\n> The word that we use for that is \"materialize,\" though I think it would always be true for broadcasting or recursively applying. Operations should be expected to make a lazy array non-lazy. That's what this does:\r\n> \r\n> https://github.com/scikit-hep/awkward-1.0/blob/94de4e5112ad3a2d5fc9c2ec0fc29e242543a0d6/src/awkward/_util.py#L1312-L1313\r\n\r\nGreat. I sort of agree. I think maybe we don't add the flag for that reason, also because `recursively_apply` doesn't define the same arg.",
  "created_at":"2021-06-11T14:47:57Z",
  "id":859634596,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTYzNDU5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T14:47:57Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"This `simplify` function looks very similar to one we've wanted for a while that would be called `ak.packed`: #746. I think you should make this into that: packing does simplify, but it also removes inaccessible elements.\r\n\r\nIn this comment, I'll write a list of rules that `ak.packed` should have. Since you'd be doing something on just about every node, it would probably make more sense to not try to shoehorn it into `ak._util.recursively_apply`. That function is for avoiding boilerplate when you only want to affect one node level; here, you'll be affecting them all.\r\n\r\n   * **EmptyArray:** unchanged\r\n   * **NumpyArray:** converted to contiguous, if not already\r\n   * **RegularArray:** truncate the `content` to `len(original) * size` with special handling (pass through) if `size == 0`\r\n   * **ListArray:** convert `toListOffsetArray64(true)` (the `true` means starting at zero)\r\n   * **ListOffsetArray:** convert `toListOffsetArray64(true)` (it's a pass-through if it's already true that `offsets[0] == 0`)\r\n   * **RecordArray:** truncate all the `contents` to `len(original)`\r\n   * **IndexedArray:** `project()` it\r\n   * **IndexedOptionArray:** convert `toByteMaskedArray` if the `content` has `PrimitiveType`; otherwise, we want to project the `content` such that the non-negative `index` values become simple counting... the `index` should end up looking like `0, 1, 2, -1, 3, -1, -1, -1, 4, 5...`. That will take some thought.\r\n   * **ByteMaskedArray:** convert `toIndexedOptionArray` if the `content` does not have `PrimitiveType`. Doing so will naturally lead to the right kind of `index`. If not changing the type (because the `content` has `PrimitiveType`), at least truncate the `content` length to `len(original)`.\r\n   * **BitMaskedArray:** convert `toIndexedOptionArray` if the `content` does not have `PrimitiveType`. If not changing the type, at least truncate the `content` length to `len(original)`.\r\n   * **UnmaskedArray:** unchanged (but recursively descend, of course)\r\n   * **UnionArray:** simplify the `index` by `project`ing each of the `contents`\r\n   * **VirtualArray:** materialize and recursively descend\r\n\r\nBefore applying the above rules, option-type nodes (IndexedOptionArray, ByteMaskedArray, BitMaskedArray, UnmaskedArray) should be simplified and UnionArrays should be simplified. This prevents double-masking and unnecessary unions. In principle, we shouldn't be seeing these double-masked or unnecessary unions, but bugs happen and a function like this should be defensive against that.\r\n\r\nAll of the above rules are intended to make pickled and otherwise serialized data as small as they can be, without resorting to compression techniques (like run-length encoding, variable-length encoding) or packing bytes into bits (we'll leave them as bits if they're bits; otherwise no).",
  "created_at":"2021-06-11T14:50:32Z",
  "id":859636351,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTYzNjM1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T15:21:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This `simplify` function looks very similar to one we've wanted for a while that would be called `ak.packed`: #746. I think you should make this into that: packing does simplify, but it also removes inaccessible elements.\r\n\r\nSure, I had the same thoughts as I pushed the PR. Let's do it.\r\n\r\n> In this comment, I'll write a list of rules that `ak.packed` should have. Since you'd be doing something on just about every node, it would probably make more sense to not try to shoehorn it into `ak._util.recursively_apply`. That function is for avoiding boilerplate when you only want to affect one node level; here, you'll be affecting them all.\r\n\r\nPart of the logic for using `recursively_apply` is that I don't want to re-implement the type-aware tree logic that it already implements. What are you thoughts there?",
  "created_at":"2021-06-11T14:52:50Z",
  "id":859637918,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTYzNzkxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T14:53:08Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> Part of the logic for using `recursively_apply` is that I don't want to re-implement the type-aware tree logic that it already implements. What are you thoughts there?\r\n\r\nThe reasons one might want to do that are (a) to simplify/reduce boilerplate and/or (b) keep all \"knowledge\" of how to recurse in one place.\r\n\r\nSome functions are made simpler with `recursively_apply` because they only do something specialized for one node type, all others \"pass through\" in a way that is very similar from one function to another. As long as we were talking about that (e.g. in `unflatten`), that's a good argument to use and possibly expand `recursively_apply`. But for this case, as you'll see when I finish the comment above, there will be something non-trivial to do for nearly every node type. Trying to shoehorn _this_ function into `recursively_apply` would only make it more complex, not less complex. It would be like trying to read some callback-heavy Javascript code. Sometimes, a straight-through, top-to-bottom Fortran style is more appropriate\u2014whatever makes it easier to read, and that depends on context. So motivation (a) doesn't apply to this one.\r\n\r\nFor (b), that ship has sailed. The \"knowledge\" of what kinds of children each node type has is scattered throughout the codebase, mostly for the reason that centralizing it would make the code harder to read, and (you may have seen) some parts are already hard enough. Because of this lack of DRY centralization, we have lost the ability to easily add a new node type. That's something I accepted at the beginning because of how previous attempts went, and therefore put in an effort to get the set of node types right, knowing that it would be hard to add new ones in the future.\r\n\r\nFor examples of the ship sailing, look at convert.py.",
  "created_at":"2021-06-11T15:02:37Z",
  "id":859644673,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTY0NDY3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T15:02:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Ping (because GitHub doesn't email us about edited comments, only new comments). I've posted a list of rules in https://github.com/scikit-hep/awkward-1.0/pull/912#issuecomment-859636351.",
  "created_at":"2021-06-11T15:23:11Z",
  "id":859657541,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTY1NzU0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T15:23:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > Part of the logic for using `recursively_apply` is that I don't want to re-implement the type-aware tree logic that it already implements. What are you thoughts there?\r\n> \r\n> The reasons one might want to do that are (a) to simplify/reduce boilerplate and/or (b) keep all \"knowledge\" of how to recurse in one place.\r\n\r\nYou're right. It's a combination of the two, but taking a step back and thinking about it - there is no saving on code here, it's just trampolining into the same function which implements most of the types. I'll move over to a recursion when I am back at it. ",
  "created_at":"2021-06-11T15:33:53Z",
  "id":859664230,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTY2NDIzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T16:00:49Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Before applying the above rules, option-type nodes (IndexedOptionArray, ByteMaskedArray, BitMaskedArray, UnmaskedArray) should be simplified and UnionArrays should be simplified. This prevents double-masking and unnecessary unions. In principle, we shouldn't be seeing these double-masked or unnecessary unions, but bugs happen and a function like this should be defensive against that.\r\n\r\nI'm not yet familiar with all of these types (though the docs make their usage clear). Are you suggesting that this should be a two-pass process, with the first pass operating upon option types only?",
  "created_at":"2021-06-11T16:13:16Z",
  "id":859689415,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTY4OTQxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T16:13:16Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> Are you suggesting that this should be a two-pass process, with the first pass operating upon option types only?\r\n\r\nNo, it can be a single pass, but when you encounter, say, an IndexedOptionArray, do\r\n\r\n```python\r\nelif isinstance(layout, ak._util.indexedoptiontypes):\r\n    if isinstance(layout.content, ak._util.optiontypes):   # the bad case we want to protect against\r\n        return recurse(layout.simplify())\r\n    # apply the rules for IndexedOptionArray\r\n```\r\n\r\nThis won't infinitely recurse because `layout.simplify()` will reduce the number of levels of option-types containing option-types by 1. Eventually, the option-type node won't contain an option-type node: it will eventually reach that state because it's possible to turn any option-type of option-type into just an option-type. But that also means that it's essential to guard this recursion with `isinstance(layout.content, ak._util.optiontypes)`.\r\n\r\nFor unions, it will be somewhat trickier because some unions can be simplified and others can't. There's a rule that I can help you with when you get to it: a UnionArray can be simplified when all of its `contents` are `mergeable` with each other. (Actually, you'd be able to work that out, I believe.)\r\n\r\nAlso note that we can `isinstance` groups of classes with similar properties using\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/94de4e5112ad3a2d5fc9c2ec0fc29e242543a0d6/src/awkward/_util.py#L66-L105",
  "created_at":"2021-06-11T17:58:36Z",
  "id":859748062,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTc0ODA2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-11T17:58:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> For unions, it will be somewhat trickier because some unions can be simplified and others can't. There's a rule that I can help you with when you get to it: a UnionArray can be simplified when all of its `contents` are `mergeable` with each other. (Actually, you'd be able to work that out, I believe.)\r\n> \r\n> Also note that we can `isinstance` groups of classes with similar properties using\r\n> \r\n> https://github.com/scikit-hep/awkward-1.0/blob/94de4e5112ad3a2d5fc9c2ec0fc29e242543a0d6/src/awkward/_util.py#L66-L105\r\n\r\nFor unions, I think the behaviour that we want is:\r\n1. to lift any immediate child unions into the parent union\r\n1. to merge any mergable contents of the new top-level union\r\n1. to compact the array contents and their associated indices. \r\nDoes this match your expectations?\r\n\r\n(1.) is already handled by `layout.simplify`, but doesn't behave as I would expect where the array types aren't compatible (e.g `float` vs `int`).  I wonder if this needs to be done manually instead.\r\n(2.) this is also (I think) handled by `layout.simplify`, but doesn't behave well as discussed. Effectively, I think we want to find the partition of the set of contents with the smallest cardinality?",
  "created_at":"2021-06-12T16:51:28Z",
  "id":860079190,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDA3OTE5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-12T16:51:28Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> For unions, I think the behaviour that we want is:\r\n> \r\n> 1. to lift any immediate child unions into the parent union\r\n> 2. to merge any mergable contents of the new top-level union\r\n> 3. to compact the array contents and their associated indices.\r\n>    Does this match your expectations?\r\n\r\nYes, this is right. And UnionArray's `simplify` does this\u2014the merging of ints and floats (and complex) is intentional: it's a choice to consider them subtypes of each other. (I can't conceive of a real-world advantage of a \"union of ints and floats\" over \"just floats.\" There's a potential distinction in precision, but again, I can't conceive of a _real-world_ case where you'd want to mix them functionally without mixing them numerically.)\r\n\r\nJust using UnionArray's `simplify` before going in and packing the contents is fine. What I thought would be tricky would be determining if `simplify` would change anything before calling it. If you always call `simplify` and recurse on the result of the simplification, it would be an infinite recursive loop. I was talking about how to write the guard.\r\n\r\nThe guard is easier for option-types, since option-type's `simplify` just ensures that two option-type nodes aren't double-stacked, nothing more. It's easy to write the guard for that.",
  "created_at":"2021-06-12T17:26:05Z",
  "id":860083742,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDA4Mzc0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-12T17:26:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, I need to think about this a bit more. I'm not sure whether the\nconversion is safe though. How do we handle converting uint64 to floats?\nI'm afk right now but the thought occurred to me that the maxint is bigger\nthan the 2^53-1.\n\nOn Sat, 12 Jun 2021, 18:26 Jim Pivarski, ***@***.***> wrote:\n\n> For unions, I think the behaviour that we want is:\n>\n>    1. to lift any immediate child unions into the parent union\n>    2. to merge any mergable contents of the new top-level union\n>    3. to compact the array contents and their associated indices.\n>    Does this match your expectations?\n>\n> Yes, this is right. And UnionArray's simplify does this\u2014the merging of\n> ints and floats (and complex) is intentional: it's a choice to consider\n> them subtypes of each other. (I can't conceive of a real-world advantage of\n> a \"union of ints and floats\" over \"just floats.\" There's a potential\n> distinction in precision, but again, I can't conceive of a *real-world*\n> case where you'd want to mix them functionally without mixing them\n> numerically.)\n>\n> Just using UnionArray's simplify before going in and packing the contents\n> is fine. What I thought would be tricky would be determining if simplify\n> would change anything before calling it. If you always call simplify and\n> recurse on the result of the simplification, it would be an infinite\n> recursive loop. I was talking about how to write the guard.\n>\n> The guard is easier for option-types, since option-type's simplify just\n> ensures that two option-type nodes aren't double-stacked, nothing more.\n> It's easy to write the guard for that.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-hep/awkward-1.0/pull/912#issuecomment-860083742>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAJQZHO3TWORBCON6LXLNE3TSOKDTANCNFSM46Q3NZEQ>\n> .\n>\n",
  "created_at":"2021-06-12T21:17:17Z",
  "id":860109985,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDEwOTk4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-12T21:17:17Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> Yes, I need to think about this a bit more. I'm not sure whether the conversion is safe though. How do we handle converting uint64 to floats? I'm afk right now but the thought occurred to me that the maxint is bigger than the 2^53-1.\r\n\r\nIt's actually the same conversion as NumPy. (We let them decide the \"right\" way to merge numbers and then match it.)",
  "created_at":"2021-06-12T22:16:48Z",
  "id":860120285,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDEyMDI4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-12T22:16:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hmm. I can see that NumPy provide such a function, it just seems like a last-resort because of the lack of representation for large ints.",
  "created_at":"2021-06-13T14:30:37Z",
  "id":860220332,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDIyMDMzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-13T14:30:37Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski how would you feel about exposing `referentially_equal`, e.g. as `__eq__`? I think this would be useful, and would avoid us having to test for simplification. Looking at `simplify_uniontype`, it seems that all I need to test for is whether any contents are also union arrays, right? The merge ability is just a secondary optimisation for the contents of these nested union arrays, but from the top-level perspective, `simplify_uniontype` will *always* merge the nested union's contents with its own.\r\n\r\nFurthermore, I don't think there a recursion risk here. If `simplify` returns a non UnionArray type, then we recurse on the result. Otherwise, we treat the simplification as a success (it doesn't matter if it actually did anything), and simplify the contents.",
  "created_at":"2021-06-13T14:48:38Z",
  "id":860223118,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDIyMzExOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-13T15:23:40Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"To implement the truncation step, I've added a private `truncate` helper that introduces a new `IndexedArray64` that wraps the existing content. Is this a bad idea? My thought process is that we then do not need to implement a new routine that truncates each type - Awkward already knows how to do this.",
  "created_at":"2021-06-13T15:25:16Z",
  "id":860228619,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDIyODYxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-13T19:40:51Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Why do we care whether the content has `PrimitiveType` in the masked types?",
  "created_at":"2021-06-13T16:59:46Z",
  "id":860241413,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDI0MTQxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-13T16:59:46Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Now to push the tests...",
  "created_at":"2021-06-13T17:51:38Z",
  "id":860247626,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDI0NzYyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-13T17:51:38Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I've made a mistake in the `IndexedOptionArray` case which I need to fix. I also need to make changes to stop mutating existing properties of layouts in-place.",
  "created_at":"2021-06-13T19:27:49Z",
  "id":860258823,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDI1ODgyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-13T19:27:49Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"OK, this is looking like it is nearly ready for a review. I have not yet implemented the axis / depth argument. I *don't* think we want to restrict this to only a single axis if `depth is not None`. Instead, I would probably want to define the upper bound (i.e. simplify to a given inclusive depth). The `depth = None` case can be used to simplify the entire structure. Do you think this is reasonable?\r\n\r\nAs you can imagine, the reason that I want to pass a depth parameter is that for routines like `unflatten`, we do not need any simplification below the axis of unflattening, so it would be unnecessary to invoke the copies etc that would be required to flatten the entire subtree.",
  "created_at":"2021-06-14T06:22:28Z",
  "id":860408068,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDQwODA2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T06:28:59Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Additionally, the tests are probably not quite finished - I don't thoroughly check that identities and parameters are preserved, or that the contents are always simplified too. Perhaps some input as to how much testing we want to do would be helpful.",
  "created_at":"2021-06-14T06:23:40Z",
  "id":860408785,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDQwODc4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T06:23:40Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"When implementing the depth parameter (I settled on `to_axis` because it's up-to-and-including the axis), I observed that `UnionArray` raises an exception when asked to wrap `axis=-1` for the following layout\r\n```python3\r\na = ak.layout.NumpyArray(np.arange(4))\r\nb = ak.layout.RegularArray(ak.layout.NumpyArray(np.arange(12)), 3)\r\nlayout = ak.layout.UnionArray8_64(\r\n    ak.layout.Index8([1, 1,  0, 0]), ak.layout.Index64([0,1, 0,1]), [a, b]\r\n)\r\nlayout.axis_wrap_if_negative(-1)\r\n```\r\n\r\nI expect this to just return `-1`, because each array has its own concept of `axis=-1`. Do you have any thoughts here?",
  "created_at":"2021-06-14T07:29:56Z",
  "id":860453346,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDQ1MzM0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T07:53:05Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I think `ak.packed` should always apply to all levels: no `axis` (depth) parameter. I can't imagine why anyone would want to partially pack an array: it's an \"invisible\" feature (like virtualness, partitioning, categoricals with IndexedArray, etc.). It would be hard to even know that it has been applied to one axis and not another\u2014it's something one would want to apply to a whole array before `ak.to_buffers` or to reduce the indirection in an array and make it faster to access.\r\n\r\n> * **PartitionedArray** concatenate partitions?\r\n\r\nI think it would be more useful to leave the partitions as partitions. `ak.repartition` generalizes the process of concatenating them, so it would be good if `ak.packed` and `ak.repartition` can be used independently. Since `ak.packed` is important for storage, we don't want it to interfere with another storage-related property.",
  "created_at":"2021-06-14T13:53:38Z",
  "id":860703242,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcwMzI0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T13:53:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I was motivated to work on this by #910. In that case, we don't need to flatten every axis unless the user passes `axis=-1`, and indeed, one wouldn't want to flatten additional axes unless required to, because it introduces many copies of the data. For example, if I wanted to add structure to a wide RecordArray, `ak.unflatten` would then potentially require copying all of the branches (to project them).",
  "created_at":"2021-06-14T13:56:18Z",
  "id":860705226,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcwNTIyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T13:56:18Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> `UnionArray` raises an exception when asked to wrap `axis=-1` for the following layout\r\n\r\nNegative axes can't have a meaningful value in a union array with different depths. But this should be moot if you don't implement an `axis` argument.",
  "created_at":"2021-06-14T13:57:00Z",
  "id":860705795,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcwNTc5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T13:57:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > `UnionArray` raises an exception when asked to wrap `axis=-1` for the following layout\r\n> \r\n> Negative axes can't have a meaningful value in a union array with different depths. But this should be moot if you don't implement an `axis` argument.\r\n\r\nI thought the `UnionArray` was supposed to return the negative axis value in such cases?",
  "created_at":"2021-06-14T13:59:00Z",
  "id":860707395,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcwNzM5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T13:59:00Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"#910 is about unflatten, an analysis function that takes an `axis` argument to know what dimension to turn into two dimensions. `ak.packed` is a whole different category of function: it produces an array that has the same high-level meaning as the input array with different low-level/internal structure. That's why I say that its changes are \"invisible,\" like virtualness, partitioning, and categoricals. Whereas `ak.unflatten` needs an `axis` to make its high-level-visible change in the right place, an `axis` argument in `ak.packed` wouldn't be very useful. Which is good news because it's easier to not implement it!",
  "created_at":"2021-06-14T14:01:06Z",
  "id":860709033,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcwOTAzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T14:01:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski we might be crossing wires here. I reported #910 because I noticed that there was a bug that entirely depended upon the (invisible) layout of the array, i.e. the `counts` argument used the layout from outside-in, whereas the `getfunction` only operates upon the internal content. This leads to incorrect structures for any arrays that have undergone some degree of manipulation.\r\n\r\nThe `ak.packed` function solves that problem by removing these problematic layouts, and ultimately moving them recursively down one level. As `ak.unflatten` needs something **like** `ak.packed`, it makes sense to me to implement an `axis` parameter (although we might make the user-default `axis=None`) so that `unflatten` can call `ak.packed` without changing the structure (or being slower) more than it needs to be.",
  "created_at":"2021-06-14T14:07:25Z",
  "id":860713899,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcxMzg5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T14:07:25Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> I thought the `UnionArray` was supposed to return the negative axis value in such cases?\r\n\r\nThe definition is here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/98f4174041b1c9b72365da9ca8d371f8cdeb845f/src/libawkward/Content.cpp#L1686-L1711\r\n\r\nIt can only give a meaningful result if \"min depth\" is equal to \"max depth.\" If they're not equal but it can keep recursing, then this function returns its input so that it can try again on the next recursive step. That's how it can work in a record array whose fields have different depths: the minimum depth and maximum depth would differ for all nodes above or at the RecordArray, but once a branch of the recursion enters one field, it gets a single depth value for that branch (and maybe some other recursive branch reaches another value). That allows `axis=-1` to mean a different deepest level for the `x` field of a record than the `y` field of a record, for example.\r\n\r\nUnion arrays, on the other hand, are a \"sum type,\" not a \"product type\" (the type of an instance is `T1 or T2`, not `T1 and T2`), so everything in the union array has to count as the same depth or die trying. The exception gets raised by the `if (mindepth + axis == 0)`, which is the base case of this recursion if `mindepth == maxdepth` is never reached.",
  "created_at":"2021-06-14T14:10:06Z",
  "id":860716019,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcxNjAxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T14:10:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Union arrays, on the other hand, are a \"sum type,\" not a \"product type\" (the type of an instance is `T1 or T2`, not `T1 and T2`), so everything in the union array has to count as the same depth or die trying. The exception gets raised by the `if (mindepth + axis == 0)`, which is the base case of this recursion if `mindepth == maxdepth` is never reached.\r\n\r\nAh, I see. I was thinking about the product type rules being equivalent to the sum type. Thanks for clarifying that!",
  "created_at":"2021-06-14T14:12:03Z",
  "id":860717601,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcxNzYwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T14:12:03Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> The `ak.packed` function solves that problem by removing these problematic layouts, and ultimately moving them recursively down one level. As `ak.unflatten` needs something **like** `ak.packed`, it makes sense to me to implement an `axis` parameter (although we might make the user-default `axis=None`) so that `unflatten` can call `ak.packed` without changing the structure (or being slower) more than it needs to be.\r\n\r\nI see: you need it as a utility to solve the problem in `ak.unflatten`. In that case, the `axis` that it takes to solve the `ak.unflatten` problem doesn't need to be a part of the public API, which can simplify things by not having to handle negative `axis` values, for instance. If you define `_packed` as the general function that takes arguments you need for `ak.unflatten`, the user-visible `packed` function can expose only a subset of those arguments. Then the extra arguments in `_packed` don't need to follow the general patterns, like the ability to interpret negative `axis` (which can be hard).\r\n\r\nThis was done in convert.py; I think `_to_arrow` is distinguished from `to_arrow` so that `to_parquet` would have a backdoor to controlling the process more, in a way that we don't want to make available to users.",
  "created_at":"2021-06-14T14:15:19Z",
  "id":860720050,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcyMDA1MA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-14T14:15:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I see: you need it as a utility to solve the problem in `ak.unflatten`. In that case, the `axis` that it takes to solve the `ak.unflatten` problem doesn't need to be a part of the public API, which can simplify things by not having to handle negative `axis` values, for instance. If you define `_packed` as the general function that takes arguments you need for `ak.unflatten`, the user-visible `packed` function can expose only a subset of those arguments. Then the extra arguments in `_packed` don't need to follow the general patterns, like the ability to interpret negative `axis` (which can be hard).\r\n> \r\n> This was done in convert.py; I think `_to_arrow` is distinguished from `to_arrow` so that `to_parquet` would have a backdoor to controlling the process more, in a way that we don't want to make available to users.\r\n\r\nFab, I've made these changes, and I *think* it's ready for review. I modified the `truncate` helper to immediately project the `IndexedArray64` rather than recurse; otherwise, the `axis` parameter could prevent this from being performed. ",
  "created_at":"2021-06-14T15:02:03Z",
  "id":860756523,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDc1NjUyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T15:02:12Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Sorry for the git noise. I rebased to drop the first commit.",
  "created_at":"2021-06-14T15:14:30Z",
  "id":860766297,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDc2NjI5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T15:14:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"OK, since making the discussed changes, I added a missing pass-through case for `PartitionedArray`, and another for `Record`",
  "created_at":"2021-06-14T20:33:23Z",
  "id":860973188,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDk3MzE4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T20:33:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I just made `ak.packed` a preprocessing step for pickling, which resolves scikit-hep/awkward-0.x#246 and #701. Also, I think there's some alliterative possibilities there.\r\n\r\nI added documentation (that I'm not 100% sure will render correctly; I'm just crossing my fingers and will look at awkward-array.org when it's done). I don't think you have any other changes to make, so I'm enabling auto-merge. If there's something else you want me to add, let me know right away and I can stop the process!",
  "created_at":"2021-06-14T22:00:05Z",
  "id":861023188,
  "issue":912,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTAyMzE4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T22:00:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, this is one can be closed",
  "created_at":"2021-06-30T16:11:13Z",
  "id":871537564,
  "issue":913,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTUzNzU2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T16:11:13Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif I'm done setting up this PR. I think you won't have to write any tests at all, and the implementation will be considerably shorter than these tests.\r\n\r\nUse the original C++ as a guide, with the translations described in the checklist. Those translations should be enforced by the tests. The dependency or is: UnknownType and NumpyType first, then the rest; EmptyForm, NumpyForm, and RegularForm first, then the rest.\r\n\r\nIf any test looks odd, like it breaks the pattern in a non-natural way, it might be a typo from me. Please ask me about any test you're unsure of!",
  "created_at":"2021-06-12T01:49:29Z",
  "id":859979960,
  "issue":914,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTk3OTk2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-12T01:49:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Windows can be excluded from tests that involve `float16`, `float128`, and `complex256`, since its NumPy doesn't have these types, and Python 2.7 and 3.5 can be excluded from tests that involve string representations of multiple parameters, since they don't have stable dict keys.\r\n\r\nThere are booleans for checking these things:\r\n\r\n```python\r\n>>> ak._util.win\r\nFalse\r\n>>> ak._util.py27\r\nFalse\r\n>>> ak._util.py35\r\nFalse\r\n```",
  "created_at":"2021-06-22T22:46:00Z",
  "id":866386024,
  "issue":914,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjM4NjAyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T22:46:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - would you like me to revert the API as well? Thanks!",
  "created_at":"2021-06-12T16:03:02Z",
  "id":860073211,
  "issue":915,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDA3MzIxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-12T16:03:02Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - would you like me to revert the API as well? Thanks!\r\n\r\nActually, I don't know what you mean by this. I'd like it to be datetime64 and timedelta64 everywhere, as it is in NumPy.",
  "created_at":"2021-06-12T17:30:33Z",
  "id":860084256,
  "issue":915,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDA4NDI1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-12T17:30:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > @jpivarski - would you like me to revert the API as well? Thanks!\r\n> \r\n> Actually, I don't know what you mean by this. I'd like it to be datetime64 and timedelta64 everywhere, as it is in NumPy.\r\n\r\nShall I revert the function names such as:\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/98f4174041b1c9b72365da9ca8d371f8cdeb845f/src/libawkward/builder/ArrayBuilder.cpp#L102",
  "created_at":"2021-06-13T05:16:49Z",
  "id":860153100,
  "issue":915,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDE1MzEwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-13T05:16:49Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, no, these should stay as they are because ArrayBuilder method names don't specify the number of bits in their types. (The names in this PR are to be consistent with `int8`, `float32`, etc., but none of the method names in ArrayBuilder look like that.)",
  "created_at":"2021-06-13T16:09:35Z",
  "id":860235027,
  "issue":915,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDIzNTAyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-13T16:09:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Sorry for the ping everyone - I was multitasking and managed to click everyone's names.",
  "created_at":"2021-06-14T13:41:02Z",
  "id":860693948,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDY5Mzk0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T13:41:02Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This breaks some existing tests, because this PR changes the behaviour of `ak.fill_none`. I believe it *should* break this behaviour because we *should* specify an axis. One possible addition might be to generalise to `axis=None`, which might be a safer default than `axis=1`.",
  "created_at":"2021-06-14T13:58:13Z",
  "id":860706713,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcwNjcxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T13:58:13Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski how do you feel about the axis parameter? I think we *should* provide `axis=None`, and I would expect it to recursively `fill_none`. This will change the behaviour of the function. If we implement that behaviour, I'll probably want to merge the part of #912 that enables `getfunction` to re-enter.",
  "created_at":"2021-06-14T14:20:52Z",
  "id":860724344,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDcyNDM0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T14:21:02Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"This is looking good!\r\n\r\nAs for the default, the most conservative thing to do is to make the default `axis` equivalent to the old behavior. People can start using explicit `axis` or not, but if they change nothing, nothing changes.\r\n\r\nHowever, if the old default really is unexpected, we can change the behavior through the deprecation mechanism. This part of the README describes upcoming breaking changes: https://github.com/scikit-hep/awkward-1.0#roadmap We can have the new default `axis` take effect in version 1.4.0 on August 1, 2021 by advertising it here and putting a warning in the function:\r\n\r\n```python\r\nPLACEHOLDER = a singleton that isn't None\r\n\r\ndef fill_none(arguments, axis=PLACEHOLDER, more arguments):\r\n    if axis is PLACEHOLDER:\r\n        if ak.deprecations_as_errors:\r\n            raise ValueError(\"ak.fill_none needs an explicit axis because the default will change in version 1.4.0\")\r\n        warnings.warn(\r\n            \"\"\"In version 1.4.0 (target date: August 1, 2021), the default axis for fill_none will be XXX\r\n(Set ak.deprecations_as_errors = True to get a stack trace now.)\"\"\",\r\n            FutureWarning,\r\n        )\r\n        axis=XXX\r\n    normal code follows\r\n```\r\n\r\nThis is similar to what `ak._util.deprecate` does, but this case is a little different because it's changing the default of an argument, not removing functionality.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/98f4174041b1c9b72365da9ca8d371f8cdeb845f/src/awkward/_util.py#L49-L63\r\n\r\nIt's not obvious to me that the old default is surprising enough to require changing, but you're a better judge of that than I am. Anyway, if it needs to be changed, this is how we can do it, giving people a few months to prepare for it.",
  "created_at":"2021-06-14T14:57:00Z",
  "id":860752270,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDc1MjI3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T14:57:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks Jim.\r\n\r\n> It's not obvious to me that the old default is surprising enough to require changing, but you're a better judge of that than I am. Anyway, if it needs to be changed, this is how we can do it, giving people a few months to prepare for it.\r\nTo be honest, I'm not *entirely* sure what the best option is. I feel slightly stronger in favour of deprecating the existing first-wins implementation because there are two possible scenarios:\r\n1. The user has only a single axis of `None`s\r\n2. The user has multiple axes of `None`s\r\n\r\nIn (1) the user can also provide an explicit axis. In (2) the user would need to repeatedly call `fill_none`. If we make `axis=None` recursive, then (2) becomes simpler.\r\n\r\nHowever, I can see the benefit to the existing behaviour in the event that the user doesn't know where the `None`s are coming from. My gut feeling here is that you *should* know where None's are coming from; they're an important part of the schema. Moreover, if you notice that your data have `None`s, then you work out where they are coming from and fix them explicitly. I'll add the deprecation notice now; you can always nuke it in the code review :smile: \r\n",
  "created_at":"2021-06-14T15:08:47Z",
  "id":860761858,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDc2MTg1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T15:08:47Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski OK, I've taken a step back and realised that the current implementation won't facilitate an `axis` parameter, only a `start_depth` parameter; `IndexedArray` switches according to whether it is an option array cls or not, if it is, then it removes the option, otherwise it delegates to the content `fillna`. This is where the apparent first-wins behaviour comes from. \r\n\r\nThis complicates things a little. I think that existing implementation in `main` is still undesirable long-term as it does not permit users to `fill_none` at a depth beyond an existing `None` e.g.\r\n```python3\r\nak.layout.IndexedOptionArray64(\r\n  ak.layout.Index64([0, 1, -1, 2]),\r\n  ak.layout.ListOffsetArray64(\r\n    ak.layout.Index64([0, 3, 6, 9, 12]),\r\n    ak.layout.IndexedOptionArray64(\r\n      ak.layout.Index64([0, 1, 2, 3, 4, 5, -1, 7, 8, 9, 10, 11]),\r\n      ak.layout.NumpyArray(np.arange(12))\r\n    )\r\n  )\r\n)\r\n```\r\n\r\nSo, what we want to do (I think) is circumvent  `fillna`'s recursion. I think the quickest solution here is to perform `fillna` only on the optiontypes, effectively, \r\n```python3\r\ndef getfunction(layout, depth, posaxis):\r\n    # Check axis is correct\r\n    ...\r\n\r\n   # Invoke fillna\r\n    if isinstance(layout, ak._utils.optiontypes):\r\n        layout.fillna()\r\n```\r\n",
  "created_at":"2021-06-14T15:52:09Z",
  "id":860795026,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDc5NTAyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T15:53:07Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> The old behavior was `axis=0`, which is a default some functions have (`ak.concatenate`, `ak.unflatten`, and `ak.is_none`, which is perhaps the most relevant for comparison with `ak.fill_none`).\r\n> \r\n> I see some value in making the default `axis=None` since that's what people have sometimes expected (I remember some old issues about that), but I don't think the expectation is strong enough to warrant the pain of a deprecation cycle. If someone asks, \"Why didn't it get the nested `Nones`?\" we can just say, \"Use `axis=None`\" and that would probably be the end of the conversation. The documentation for this function should have examples with `axis=some int` and `axis=None` for users to hopefully notice.\r\n> \r\n> So if you're asking me to make an executive decision, I'll do it: make the default `axis` of this function `0`, and then the old tests will work without modification.\r\n> \r\n> Has the `axis=None` case not been implemented? I don't see it here or in the tests. Would that be hard to add (because `recursively_apply` has to be applied to the output of an `apply`)?\r\n\r\nThanks for reading through that wall of text!\r\n\r\nI'm really trying to say two things in one go:\r\n1. The existing recursive behaviour cannot be restricted to single axes (e.g. `axis=i`)\r\n2. The existing behaviour requires the user to know the underlying layout tree.\r\n3. It would be nice to be able to remove *all* axes (e.g. `axis=None`)\r\n\r\nAt the moment, I don't think that the behaviour is `axis=0`, it's more like `axis > 0 && is_optional(axis)`. I propose that a better set of behaviours would be:\r\n\r\n* Default `axis=SENTINEL` which removes the first optiontype it finds\r\n* `axis=i` which operates only on that axis\r\n* `axis=None` which removes *all* option types.\r\n\r\nI'm not in favour of having a special sentinel value, so I don't know what the best API is here.",
  "created_at":"2021-06-14T16:12:56Z",
  "id":860810189,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDgxMDE4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T16:12:56Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"If this would be simpler without using `fillna` at all (i.e. avoid the sentinel), then go for it. I think this function was written before `recursively_apply` existed.\r\n\r\nThe best API would be one that\r\n\r\n   * takes `axis=#` to be the only list depth where None values are replaced with the replacement\r\n   * takes `axis=None` to apply the replacement at all levels (using that new `pass_apply`)\r\n\r\nI just tried a simple example and it _looked_ like the current behavior is `axis=0`. If it's not exactly that, then we could declare its deviation from `axis=0` to be a bug that we can change without a deprecation cycle. Whatever it was, if it wasn't equivalent to one of the two cases above, it wasn't intended. Then, since the old behavior was _closest to_ `axis=0`, we could make that the default and most code would continue to work as-is. (The code that stops working was depending on a bug; we can make an argument that _that_ code should be fixed.)",
  "created_at":"2021-06-14T16:21:05Z",
  "id":860815629,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDgxNTYyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T16:21:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This PR (which just delays the call to `fillna` until the axis agrees with the depth) still demonstrates the first wins behaviour on this array:\r\n```python\r\nlayout = ak.layout.RegularArray(\r\n    ak.layout.IndexedOptionArray64(\r\n        ak.layout.Index64([0, 1, -1, 2, 3]),\r\n        ak.layout.ListOffsetArray64(\r\n            ak.layout.Index64([0, 3, 6, 9, 12]),\r\n            ak.layout.IndexedOptionArray64(\r\n                ak.layout.Index64([0, 1, 2, 3, 4, 5, -1, 7, 8, 9, 10, 11]),\r\n                ak.layout.NumpyArray(np.arange(12)),\r\n            ),\r\n        ),\r\n    ),\r\n    2,\r\n)\r\n\r\n```\r\n\r\n```python\r\n>>> ak.to_list(layout)\r\n... [[[0, 1, 2], [3, 4, 5]], [None, [None, 7, 8]]]\r\n>>> ak.fill_none(layout, 1, axis=0).tolist()\r\n... [[[0, 1, 2], [3, 4, 5]], [1, [None, 7, 8]]]\r\n>>> ak.fill_none(layout, 1, axis=1).tolist()\r\n[[[0, 1, 2], [3, 4, 5]], [1, [None, 7, 8]]]\r\n```\r\n\r\nThis behaviour can be prevented by checking that the type is an optiontype before calling `fillna` from Python. \r\n\r\nOf course, it is another question entirely as to whether we just remove fillna from the C++ layer entirely (or just ignore it for this PR) and re-implement it in Python. It sounds like that *might* align with the idea that we're lifting some of the C++ layer into Python now (apart from the kernels).\r\n\r\nLastly, longer-term I wonder how much of this multi-type aware code should be sitting as free functions vs member functions on types. This is a design decision I wouldn't feel comfortable tackling right now, but it is interesting!",
  "created_at":"2021-06-14T18:55:06Z",
  "id":860916547,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDkxNjU0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T18:56:28Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"OK, this PR now implements the described behaviour, with a default of `None`. The example above now behaves as follows:\r\n```python\r\n>>> ak.to_list(layout)\r\n... [[[0, 1, 2], [3, 4, 5]], [None, [None, 7, 8]]]\r\n>>> ak.fill_none(layout, 1, axis=0).tolist()\r\n... [[[0, 1, 2], [3, 4, 5]], [None, [None, 7, 8]]]\r\n>>> ak.fill_none(layout, 1, axis=1).tolist()\r\n... [[[0, 1, 2], [3, 4, 5]], [1, [None, 7, 8]]]\r\n>>> ak.fill_none(layout, 1, axis=None).tolist()\r\n... [[[0, 1, 2], [3, 4, 5]], [1, [1, 7, 8]]]\r\n>>> ak.fill_none(layout, 1).tolist()\r\n... [[[0, 1, 2], [3, 4, 5]], [1, [1, 7, 8]]]\r\n```\r\n\r\n\r\n I made this decision because after modifying the behaviour to *only* affect the specified axis, it was not clear whether `axis=0` would give the right result anymore. I had to change an existing use of `fill_none` to reflect the new default.\r\n\r\nIt might be that this needs to be in the next major release (if using SemVer) instead of minor, but I'll defer to you on that one :smile: ",
  "created_at":"2021-06-14T20:07:27Z",
  "id":860958823,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDk1ODgyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T20:07:45Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"You can see from my above commits that I wavered between `axis=0` and `axis=None` defaults. In the end, I think we *do* need a deprecation notice (and have implemented one) - the new behaviour is more explicit, so it will miss existing cases. This also means that the old tests don't fail, so we can leave them in until the deprecation period expires!\r\n\r\nNew behaviour:\r\n```python\r\n>>> ak.to_list(layout)\r\n... [[[0, 1, 2], [3, 4, 5]], [None, [None, 7, 8]]]\r\n>>> ak.fill_none(layout, 1, axis=0).tolist()\r\n... [[[0, 1, 2], [3, 4, 5]], [None, [None, 7, 8]]]\r\n>>> ak.fill_none(layout, 1, axis=1).tolist()\r\n... [[[0, 1, 2], [3, 4, 5]], [1, [None, 7, 8]]]\r\n>>> ak.fill_none(layout, 1, axis=None).tolist()\r\n... [[[0, 1, 2], [3, 4, 5]], [1, [1, 7, 8]]]\r\n>>> ak.fill_none(layout, 1).tolist()\r\n... [[[0, 1, 2], [3, 4, 5]], [1, [None, 7, 8]]]\r\n```\r\n",
  "created_at":"2021-06-14T20:55:21Z",
  "id":860989197,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDk4OTE5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T21:20:02Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hopefully this is the *last* commit, which cleans things up by moving the deprecated implementation into a new private function.  Additionally, I removed the `PartitionedArray` case in this PR because in the new explicit axis code it's no longer required; it's handled by the `getfunction` logic automatically.\r\n\r\n\r\nThanks for **all** your help on this @jpivarski, I know it's been a bit of an intense PR.",
  "created_at":"2021-06-14T21:10:39Z",
  "id":860997313,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MDk5NzMxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-14T21:10:39Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I was about to disagree with the introduction of a deprecation/breaking change, but now I see why. I had thought that the effective axis (in the main branch) is `axis=0`:\r\n\r\n```python\r\n>>> ak.fill_none(ak.Array([[None, 2], None, [4, None]]), 10)\r\n<Array [[None, 2], 10, [4, None]] type='3 * union[var * ?int64, int64]'>\r\n```\r\n\r\nbut if this were one level deeper, it still applies at the first level that has option-type.\r\n\r\n```python\r\n>>> ak.fill_none(ak.Array([[[None, 2], None, [4, None]]]), 10)\r\n<Array [[[None, 2], 10, [4, None]]] type='1 * var * union[var * ?int64, int64]'>\r\n```\r\n\r\nThat's not any particular `axis`, so you're right that some users of the old system would be broken by making the new default `axis=0` and others would not. There is no value we can make the new default `axis` that will not break _somebody's_ code. But the old way was wrong (before the general pattern of how these functions should work had been fully set), it needs to break.\r\n\r\nWe have a choice. The new default could be `axis=0` (like `ak.is_none`) or `axis=None` (I think this might be the usual expectation). What do you think? We'll be stuck with what we choose now. Do you want to open a discussion under Deprecations?\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/discussions/categories/deprecations",
  "created_at":"2021-06-14T22:17:11Z",
  "id":861031080,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTAzMTA4MA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-14T22:17:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The failing tests are caused by the new warning that's raised by this PR, with `deprecations_as_errors`. Should I modify the old tests that use `fill_none` to permit warnings?",
  "created_at":"2021-06-15T19:16:55Z",
  "id":861766284,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTc2NjI4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T19:19:18Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Since the behavior has changed, it is necessary to change the old tests. (Even though the tests are largely historical and labeled by PR or issue number, it's not uncommon to change them. Sometimes it's because the old tests were testing too much\u2014more than we guaranteed, such as the exact string representation of a repr. But in a case like this, you _know_ you're changing the behavior, so it's a good sign that the old tests had enough coverage to fail!)\r\n\r\nI just did a grep, and there are 10 cases of `ak.fill_none`, all in tests/test_0072-fillna-operation.py, tests/test_0590-allow-regulararray-size-zero.py, and tests/test_0634-fill_none-with-record.py. Probably setting `axis=None` would be sufficient for these; they probably didn't test multiple levels of missing values.",
  "created_at":"2021-06-15T20:06:08Z",
  "id":861796012,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTc5NjAxMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-15T20:06:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Please don't remove `axis=None` as an option, considering that it works and produces valid arrays! Maybe it's less useful than other cases (because it makes union arrays), but it has positive usefulness (union arrays are good for some things).",
  "created_at":"2021-06-15T20:57:52Z",
  "id":861826209,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTgyNjIwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T20:57:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Please don't remove `axis=None` as an option, considering that it works and produces valid arrays! Maybe it's less useful than other cases (because it makes union arrays), but it has positive usefulness (union arrays are good for some things).\r\n\r\nOK, I concede that I might have been a little *too* opinionated here. I'll revert the change :)",
  "created_at":"2021-06-15T21:14:23Z",
  "id":861835674,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTgzNTY3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T21:14:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"From #929:\r\n> Our deprecation function and/or the custom one for ak.fill_none's axis argument should recommend this line of code instead of leaving it to the user to come up with that.\r\n> Also, the warning for ak.fill_none's axis should recommend the new default, since that's the best way to silence the warning when they see it.\r\n\r\nYes, I have now removed the custom `deprecated` for `fill_none`'s old behaviour in favour of the existing decorator. I'll modify this in turn if we merge #929 first!\r\n\r\nI will add a note about the new default now!",
  "created_at":"2021-06-15T21:48:09Z",
  "id":861855899,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg1NTg5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T21:48:09Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"@agoose77 Is this PR done? I'm trying to figure out what is waiting for me and what can be closed. The last message (27 days ago) references #929, which was merged.\r\n\r\nI'll start by bring this branch up to date and seeing if the tests still work.",
  "created_at":"2021-07-12T18:25:22Z",
  "id":878496652,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODQ5NjY1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T18:25:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I *think* it's done. We have left it with axis=-1 as a future default, and\nfor now it incurs a deprecation warning when no axis is given, falling back\nto axis=None\n\nOn Mon, 12 Jul 2021, 19:25 Jim Pivarski, ***@***.***> wrote:\n\n> @agoose77 <https://github.com/agoose77> Is this PR done? I'm trying to\n> figure out what is waiting for me and what can be closed. The last message\n> (27 days ago) references #929\n> <https://github.com/scikit-hep/awkward-1.0/pull/929>, which was merged.\n>\n> I'll start by bring this branch up to date and seeing if the tests still\n> work.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-hep/awkward-1.0/pull/917#issuecomment-878496652>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAJQZHLFN7TNJJRXXGZB3FLTXMXSVANCNFSM46VHXRUA>\n> .\n>\n",
  "created_at":"2021-07-12T19:01:54Z",
  "id":878519062,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODUxOTA2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T19:01:54Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"It [relies on the API](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=7108&view=logs&j=bb1c2637-64c6-57bd-9ea6-93823b2df951&t=8994636e-e094-5103-be26-64e72a4b5d0c&l=1856) of \"recursively_apply\", but that changed in another PR.",
  "created_at":"2021-07-12T19:06:22Z",
  "id":878522362,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODUyMjM2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T19:06:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Good catch; I'd forgotten that we removed that flag. I'll fix this up tomorrow BST.",
  "created_at":"2021-07-12T21:37:44Z",
  "id":878614779,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODYxNDc3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T21:37:44Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I've fixed the removal of `pass_apply` and am directly using the new transformer helper. I have a couple of questions about code-style \u2014 I don't normally use exhaustive `if` statements (which I recall you noting in a previous code review), so let me know if there's still some style elements that you'd like to see changed here.",
  "created_at":"2021-07-13T09:34:59Z",
  "id":878934476,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODkzNDQ3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T09:34:59Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski thanks for the cleanups (in particular `_fill_none_deprecated`), I must have missed that when pushing.",
  "created_at":"2021-07-13T16:29:41Z",
  "id":879231448,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTIzMTQ0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T16:29:41Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I just turned on warnings-as-errors (though it took a bit to figure out how to do that in pytest, now that we're doing it the standard way; notes to in the warning message have been updated).\r\n\r\nSee the [changes to Roadmap](https://github.com/scikit-hep/awkward-1.0/blob/c7f6e8227ceff6ca1a1c426c63e95ca25245ec9f/README.md#roadmap): this is how the deprecation is going to look. I think you were considering another backward-incompatible change; it would have to go here, too. The fact that all backward-incompatible changes have to be listed here puts a damper on how many there can be.\r\n\r\nI've changed the once-every-two-month releases into monthly releases, and I don't plan to do many in-between releases anymore. Regular users are going to see the deprecation warning for the first time after August 1, and people who don't update more frequently than every two months (there are quite a few of them) won't see it at all\u2014the new behavior could take them by surprise. However, there has to be some threshold frequency. (This is basically saying that people who don't update more frequently than every two months should expect surprising changes.)",
  "created_at":"2021-07-13T16:39:00Z",
  "id":879237587,
  "issue":917,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTIzNzU4Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-13T16:39:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Actually noticed the issue when i tried to use an http file-like object from `fsspec`, so #886 could be related\r\n\r\n```\r\npython -m http.server\r\n```\r\n```pycon\r\n>>> import fsspec\r\n>>> import awkward as ak\r\n>>> of = fsspec.open(\"http://localhost:8000/array.parquet\", \"rb\")\r\n>>> f = of.open()\r\n>>> array = ak.from_parquet(f)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/nikolai/.local/lib/python3.9/site-packages/awkward/operations/convert.py\", line 3624, in from_parquet\r\n    single_file = pyarrow.parquet.ParquetFile(filename)\r\n  File \"/home/nikolai/.local/lib/python3.9/site-packages/pyarrow/parquet.py\", line 217, in __init__\r\n    self.reader.open(source, use_memory_map=memory_map,\r\n  File \"pyarrow/_parquet.pyx\", line 947, in pyarrow._parquet.ParquetReader.open\r\n  File \"pyarrow/io.pxi\", line 1473, in pyarrow.lib.get_reader\r\n  File \"pyarrow/io.pxi\", line 1466, in pyarrow.lib.get_native_file\r\nTypeError: Cannot convert bytes to pyarrow.lib.NativeFile\r\n```\r\n\r\nEdit: actually the python http server would not work since it does not support byte ranges, but one could spawn an nginx or similar with docker, e.g.\r\n```\r\ndocker run --rm --name nginx -v $(pwd):/usr/share/nginx/html:ro -p 8000:80 nginx\r\n```",
  "created_at":"2021-06-15T09:25:16Z",
  "id":861340205,
  "issue":921,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTM0MDIwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T09:43:36Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@nikoladze you're right that the current implementation *appears* to be unable to handle `File`-like objects due to that `Iterable` check, which was introduced in #706. \r\n\r\nAddressing this and #886 is going to take a little more time that I have available right now unfortunately. Is this something that you need and can't work around?\r\n\r\n",
  "created_at":"2021-06-15T09:54:41Z",
  "id":861360469,
  "issue":921,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTM2MDQ2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T09:54:41Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'd like to run some tests with parquet files over http (also making use of streaming I/O, not downloading the file). For this it should be sufficient to have the file-like objects passed through correctly to `pyarrow.parquet.ParquetFile`.\r\n",
  "created_at":"2021-06-15T10:11:00Z",
  "id":861372574,
  "issue":921,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTM3MjU3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T10:11:00Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"You can try the [`bugfix-from-parquet-file-like`](https://github.com/scikit-hep/awkward-1.0/tree/bugfix-from-parquet-file-like) branch if you want to perform some tests.",
  "created_at":"2021-06-15T10:17:24Z",
  "id":861377211,
  "issue":921,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTM3NzIxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T10:18:25Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"That seems to work for now. Thanks!",
  "created_at":"2021-06-15T11:28:37Z",
  "id":861419215,
  "issue":921,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTQxOTIxNQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2021-06-15T11:28:37Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"Then I think we should merge the PR! I'll go approve it.",
  "created_at":"2021-06-15T18:42:38Z",
  "id":861744547,
  "issue":921,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTc0NDU0Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-15T18:42:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This code feels a little ugly, which I think derives from the fact that `to_layout` is almost redundant - the reason I include it is to set `allow_record=False`. It won't take much convincing from a reviewer for me to replace `to_layout` with an explicit check for `ak.Record`. Despite this, I wonder whether keeping it is a good idea, given that we explicitly control both `allow_record` and `allow_other`.",
  "created_at":"2021-06-15T09:35:03Z",
  "id":861347200,
  "issue":922,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTM0NzIwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T09:35:03Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> A redundant `to_layout` is fine. One of the first lines in `to_layout` is `if isinstance(obj, ak.layout.Content): return obj`, so it becomes a pass-through.\r\n> \r\n> It looks like this is what you needed! I'm in favor of merging this PR. Can you help me out: which PRs can be closed now because of this?\r\n\r\nOnly this PR, but in case you meant issues, at the moment, only #910\r\nThe other issues with `unflatten` are more to do with how zero lengths are handled.",
  "created_at":"2021-06-15T18:41:00Z",
  "id":861743423,
  "issue":922,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTc0MzQyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T18:41:00Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - this is technical - renaming `TypedArrayBuilder` to a `LayoutBuilder`.",
  "created_at":"2021-06-16T13:26:09Z",
  "id":862378691,
  "issue":924,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MjM3ODY5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T13:26:09Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"If you like that diagram, I'd quickly trace over it in SVG before adding it. (It would take less than 5 minutes, but SVG and its derivatives, PDF and PNG, render better.)\r\n\r\nThe diagram is hard to interpret in the context of variable length dimensions, though. Maybe it could be modified to be more like this:\r\n\r\n![](https://github.com/scikit-hep/awkward-1.0/raw/main/docs-img/diagrams/example-reduction.png)\r\n\r\nbut with less information?",
  "created_at":"2021-06-15T19:43:27Z",
  "id":861782111,
  "issue":926,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTc4MjExMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T19:43:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and I think we need to avoid the word \"column\" for an array axis. \"Column\" is often used for record fields, which is a very different thing from an axis at any level.\r\n\r\nFor example, all axes of an array are converted into a Pandas index (its \"rows\"), but it's the record fields that become Pandas columns:\r\n\r\n```python\r\n>>> ak.to_pandas(\r\n...     ak.Array([\r\n...         [{\"x\": 1, \"y\": 1.1}, {\"x\": 2, \"y\": 2.2}, {\"x\": 3, \"y\": 3.3}],\r\n...         [],\r\n...         [{\"x\": 4, \"y\": 4.4}, {\"x\": 5, \"y\": 5.5}]\r\n...     ])\r\n... )\r\n                x    y\r\nentry subentry        \r\n0     0         1  1.1\r\n      1         2  2.2\r\n      2         3  3.3\r\n2     0         4  4.4\r\n      1         5  5.5\r\n```",
  "created_at":"2021-06-15T19:47:42Z",
  "id":861784591,
  "issue":926,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTc4NDU5MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-15T19:47:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> If you like that diagram, I'd quickly trace over it in SVG before adding it. (It would take less than 5 minutes, but SVG and its derivatives, PDF and PNG, render better.)\r\n> \r\n> The diagram is hard to interpret in the context of variable length dimensions, though. Maybe it could be modified to be more like this:\r\n> \r\n> ![](https://github.com/scikit-hep/awkward-1.0/raw/main/docs-img/diagrams/example-reduction.png)\r\n> \r\n> but with less information?\r\n\r\nYes, I'll update the snapshot. BTW, the axis are off by 1 :-)\r\n```python\r\n>>> array = ak.Array([[2, 3, 5], [], [None, 7], [11]])\r\n>>> array\r\n<Array [[2, 3, 5], [], [None, 7], [11]] type='4 * var * ?int64'>\r\n>>> ak.prod(array)\r\n2310\r\n>>> ak.prod(array, axis=0)\r\n<Array [22, 21, 5] type='3 * int64'>\r\n>>> ak.prod(array, axis=1)\r\n<Array [30, 1, 7, 11] type='4 * int64'>\r\n>>> ak.prod(array, axis=2)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/yana/Projects/PR925/awkward-1.0/awkward/operations/reducers.py\", line 395, in prod\r\n    layout.prod(axis=axis, mask=mask_identity, keepdims=keepdims), behavior\r\nValueError: axis=2 exceeds the depth of the nested list structure (which is 2)\r\n```",
  "created_at":"2021-06-16T09:08:51Z",
  "id":862190855,
  "issue":926,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MjE5MDg1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T09:08:51Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"<img width=\"355\" alt=\"Screenshot 2021-06-16 at 11 48 15\" src=\"https://user-images.githubusercontent.com/1390682/122198003-08546980-ce99-11eb-83a6-3e6aebb64ce0.png\">\r\nis this one better?",
  "created_at":"2021-06-16T09:50:53Z",
  "id":862218793,
  "issue":926,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MjIxODc5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T09:50:53Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, that new diagram is much clearer, and thanks for the correction to the old one. I'll go fix it now (6a1c16f3378deeda234890c0c20c0ff89b1a70f1).",
  "created_at":"2021-06-16T14:05:04Z",
  "id":862410072,
  "issue":926,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MjQxMDA3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T14:07:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm syncing this with `main` so that you get \"modernized\" JupyterBook pages to fill.\r\n\r\nFor writing these pages, note that you can start up a `jupyter lab` and right-click on the `.md` file you want to edit to \"open as notebook.\" That lets you write the Jupytext (`.md`) as though it were a notebook, but the only things that gets saved to the file are the input cells (input code and markdown). The outputs are re-evaluated in testing and in production, which acts as a check to ensure they're up to date. (Also, we like Jupytext sources because git diffs are readable, unlike ipynb files.)\r\n\r\nFor local testing, you can do `jupyter-book build docs-src`.",
  "created_at":"2021-06-18T15:07:26Z",
  "id":864105056,
  "issue":926,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDEwNTA1Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-18T15:07:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - given that this `array` is the same type as `another_array` -- `9 * {\"x\": float64, \"y\": var * int64}`:\r\n```python\r\n>>> array = ak.Array(\r\n...         [\r\n...             {\"x\": 0.0, \"y\": []},\r\n...             {\"x\": 11.11, \"y\": [1]},\r\n...             {\"x\": 2.2, \"y\": [0, 2]},\r\n...             {\"x\": 33.33, \"y\": [3, -1, 10]},\r\n...             {\"x\": 4.4, \"y\": [-2, 0, 4, 14]},\r\n...             {\"x\": 5.5, \"y\": [25, 15, 5]},\r\n...             {\"x\": 6.6, \"y\": [26, 6]},\r\n...             {\"x\": 77.77, \"y\": [77]},\r\n...             {\"x\": 8.8, \"y\": []},\r\n...         ]\r\n...     )\r\n>>>\r\n```\r\n\r\n```python\r\nanother_array = ak.Array({\"x\": [0.0, 11.11, 2.2, 33.33, 4.4, 5.5, 6.6, 77.77, 8.8], \"y\": [[], [1], [0, 2], [3, -1, 10], [-2, 0, 4, 14], [25, 15, 5], [26, 6], [77], []]})\r\n```\r\nshouldn't sorted array be also the same type `9 * {\"x\": float64, \"y\": var * int64}`, such as?\r\n```python\r\nassert ak.to_list(ak.sort(array)) == [\r\n        {'x': 0.0, 'y': []},\r\n        {'x': 2.2, 'y': [1]},\r\n        {'x': 4.4, 'y': [0, 2]},\r\n        {'x': 5.5, 'y': [-1, 3, 10]},\r\n        {'x': 6.6, 'y': [-2, 0, 4, 14]},\r\n        {'x': 8.8, 'y': [5, 15, 25]},\r\n        {'x': 11.11, 'y': [6, 26]},\r\n        {'x': 33.33, 'y': [77]},\r\n        {'x': 77.77, 'y': []}\r\n    ]\r\n```\r\n\r\nCurrently sorting of a `RecodArray` returns a `{\"x\": 9 * float64, \"y\": 9 * var * int64}` type with the following layout that seems to me unnecessarily complicated:\r\n```python\r\n<Record at=\"0\">\r\n    <RecordArray length=\"1\">\r\n        <field index=\"0\" key=\"x\">\r\n            <RegularArray size=\"9\">\r\n                <content><NumpyArray format=\"d\" shape=\"9\" data=\"0 2.2 4.4 5.5 6.6 8.8 11.11 33.33 77.77\" at=\"0x7f995cd86eb0\"/></content>\r\n            </RegularArray>\r\n        </field>\r\n        <field index=\"1\" key=\"y\">\r\n            <RegularArray size=\"9\">\r\n                <content><ListOffsetArray64>\r\n                    <offsets><Index64 i=\"[0 0 1 3 6 10 13 15 16 16]\" offset=\"0\" length=\"10\" at=\"0x7f995d008200\"/></offsets>\r\n                    <content><NumpyArray format=\"l\" shape=\"16\" data=\"1 0 2 -1 3 ... 15 25 6 26 77\" at=\"0x7f995cd85a10\"/></content>\r\n                </ListOffsetArray64></content>\r\n            </RegularArray>\r\n        </field>\r\n    </RecordArray>\r\n</Record>\r\n```\r\nI'd like to change it to a simpler `9 * {\"x\": float64, \"y\": var * int64}` type:\r\n```python\r\n<RecordArray length=\"9\">\r\n    <field index=\"0\" key=\"x\">\r\n        <NumpyArray format=\"d\" shape=\"9\" data=\"0 2.2 4.4 5.5 6.6 8.8 11.11 33.33 77.77\" at=\"0x7f845bdb70b0\"/>\r\n    </field>\r\n    <field index=\"1\" key=\"y\">\r\n        <ListOffsetArray64>\r\n            <offsets><Index64 i=\"[0 0 1 3 6 10 13 15 16 16]\" offset=\"0\" length=\"10\" at=\"0x7f846c861600\"/></offsets>\r\n            <content><NumpyArray format=\"l\" shape=\"16\" data=\"1 0 2 -1 3 ... 15 25 6 26 77\" at=\"0x7f845bdbb2b0\"/></content>\r\n        </ListOffsetArray64>\r\n    </field>\r\n</RecordArray>\r\n```\r\nAccessing the fields gives the same result as before:\r\n```python\r\n>>> sorted_array.x\r\n<Array [0, 2.2, 4.4, 5.5, ... 11.1, 33.3, 77.8] type='9 * float64'>\r\n>>> sorted_array.y\r\n<Array [[], [1], [0, 2], ... [6, 26], [77], []] type='9 * var * int64'>\r\n``` ",
  "created_at":"2021-06-21T14:19:35Z",
  "id":865071953,
  "issue":926,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTA3MTk1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-21T14:46:29Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorting only applies to numbers: `ak.sort` does not sort the records themselves. No comparison operators (`==`, `<=`, `<`, etc.) have been defined for records (or lists, for that matter), so we can't say what it means for an array of records to be in order.\r\n\r\nTherefore, we can't sort `5 * {x: int64, y: float64}` (a simpler case) without sorting the _x_ values separately from the _y_ values. If the return type is `5 * {x: int64, y: float64}`, users would be very surprised to learn that the records are \"broken.\" That is,\r\n\r\n```\r\n[\r\n    {\"x\": 5, \"y\": 1.1},\r\n    {\"x\": 4, \"y\": 2.2},\r\n    {\"x\": 3, \"y\": 3.3},\r\n    {\"x\": 2, \"y\": 4.4},\r\n    {\"x\": 1, \"y\": 5.5},\r\n]\r\n```\r\n\r\nwould become\r\n\r\n```\r\n[\r\n    {\"x\": 1, \"y\": 1.1},\r\n    {\"x\": 2, \"y\": 2.2},\r\n    {\"x\": 3, \"y\": 3.3},\r\n    {\"x\": 4, \"y\": 4.4},\r\n    {\"x\": 5, \"y\": 5.5},\r\n]\r\n```\r\n\r\nThe output records aren't (can't be) the same records as the input records. At least if the output has type `{x: 5 * int64, y: 5 * float64}`, that's an indication that the result is not what you think it might be.\r\n\r\nAlternatively, we could _refuse_ to sort data with records in it if even this output is too hard to understand. I don't think we need to do this, though.",
  "created_at":"2021-06-21T16:02:43Z",
  "id":865156418,
  "issue":926,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTE1NjQxOA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-21T16:02:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"closed in favour of https://github.com/scikit-hep/awkward-1.0/pull/948",
  "created_at":"2021-06-23T11:02:38Z",
  "id":866741506,
  "issue":926,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Njc0MTUwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T11:02:38Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"This is good. I like the use of a standard library call, rather than a global variable in Awkward.\r\n\r\nOur deprecation function and/or the custom one for `ak.fill_none`'s `axis` argument should recommend this line of code instead of leaving it to the user to come up with that.\r\n\r\nAlso, the warning for `ak.fill_none`'s `axis` should recommend the new default, since that's the best way to silence the warning when they see it.",
  "created_at":"2021-06-15T21:44:04Z",
  "id":861853822,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg1MzgyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T21:44:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski are you happy for this to merge? I'll reply to your other points on #917 ",
  "created_at":"2021-06-15T21:46:26Z",
  "id":861855093,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg1NTA5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T21:46:26Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"The recommendations for `ak.fill_none`'s `axis` can go on #917, but `ak._util.deprecate` is missing a message to the user about calling `warnings.simplefilter(\"error\", FutureWarning)`. (Maybe we need a subclass of `FutureWarning` called `AwkwardDeprecationWarning`, so that such a filter doesn't eliminate too much?)\r\n\r\nIt's true that the `ak._util.deprecate` isn't currently in use, but just to avoid forgetting to add such a message later.",
  "created_at":"2021-06-15T21:54:18Z",
  "id":861860645,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg2MDY0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T21:54:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> AwkwardDeprecationWarning\r\n\r\nOK, I'll add a note about raising warnings as `Exceptions` using `simplefilter`. It's a really unfortunate choice of name because `simplefilter` doesn't really filter - it changes the handling for (a category of) warnings.\r\n\r\nI'm not sure about adding a custom warning type: there are already some useful ones:\r\n```python\r\nIn [1]: DeprecationWarning.__mro__\r\nOut[1]: (DeprecationWarning, Warning, Exception, BaseException, object)\r\n\r\nIn [2]: FutureWarning.__mro__\r\nOut[2]: (FutureWarning, Warning, Exception, BaseException, object)\r\n````\r\n\r\nIf we use the built-in ones, the only potential problem is if the user makes `DeprecationWarning` raise an Error, and some of our code (or dependencies) implements such a warning. I'd recommend your guidance on this. Worst case is, we make an ugly diamond inheritance:\r\n```python\r\nIn [6]: class AwkwardDeprecationWarning(DeprecationWarning, AwkwardWarning):\r\n   ...:     pass\r\n   ...:\r\n   ...:\r\n\r\nIn [7]: AwkwardDeprecationWarning.__mro__\r\nOut[7]:\r\n(__main__.AwkwardDeprecationWarning,\r\n DeprecationWarning,\r\n __main__.AwkwardWarning,\r\n Warning,\r\n Exception,\r\n BaseException,\r\n object)\r\n```",
  "created_at":"2021-06-15T22:01:02Z",
  "id":861865160,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg2NTE2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T22:01:02Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Python's DeprecationWarning is hidden from ordinary users: it gets reported in testing, but not when a library is used interactively. That's why we have to use or inherit from FutureWarning.\r\n\r\nThe only problem I see with diamond inheritance is being able to guess the method resolution order, but exception classes rarely have any custom methods. So that inheritance diagram looks fine to me. However,\r\n\r\n> If we use the built-in ones, the only potential problem is if the user makes `DeprecationWarning` raise an Error, and some of our code (or dependencies) implements such a warning.\r\n\r\nWouldn't that be desirable? If a user makes `DeprecationWarning` raise an error, then they want our deprecations to raise errors, too.\r\n\r\nOr\u2014change the name in all of the above to FutureWarning. I guess for clarity, ours should be named AwkwardFutureWarning, too (so it's easy to guess that it inherits from FutureWarning).",
  "created_at":"2021-06-15T22:22:17Z",
  "id":861874300,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg3NDMwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T22:22:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Python's DeprecationWarning is hidden from ordinary users: it gets reported in testing, but not when a library is used interactively. That's why we have to use or inherit from FutureWarning.\r\n\r\nWe can have Awkward register a default filter for the warning level for its own package to solve this:\r\n> module is a string containing a regular expression that the module name must match. The expression is compiled to be case-sensitive.\r\n\r\nThat way we can use the appropriate warning type\r\n\r\n> The only problem I see with diamond inheritance is being able to guess the method resolution order, but exception classes rarely have any custom methods. So that inheritance diagram looks fine to me. However,\r\n> \r\n> > If we use the built-in ones, the only potential problem is if the user makes `DeprecationWarning` raise an Error, and some of our code (or dependencies) implements such a warning.\r\n> \r\n> Wouldn't that be desirable? If a user makes `DeprecationWarning` raise an error, then they want our deprecations to raise errors, too.\r\n\r\nI was thinking if someone that Awkward uses for something internal emits a warning, that would then break our code that uses it. However, I hadn't seen the `module` filter as above- this enables us to scope the `raise` functionality to just Awkward. So, actually, we can just use the built-in warning types!\r\n\r\n> Or\u2014change the name in all of the above to FutureWarning. I guess for clarity, ours should be named AwkwardFutureWarning, too (so it's easy to guess that it inherits from FutureWarning).\r\n\r\n",
  "created_at":"2021-06-15T22:26:37Z",
  "id":861876133,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg3NjEzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T22:27:10Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> However, I hadn't seen the `module` filter as above- this enables us to scope the `raise` functionality to just Awkward. So, actually, we can just use the built-in warning types!\r\n\r\nOh, okay, then let's use that.",
  "created_at":"2021-06-15T22:34:19Z",
  "id":861879170,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg3OTE3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T22:34:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"OK, this should do it.",
  "created_at":"2021-06-15T22:40:33Z",
  "id":861881458,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg4MTQ1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T22:40:33Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Great! I suppose the \"default\" filter means that they'll see the DeprecationWarning, then?\r\n\r\nAnother example of an obvious name ;) Yes, by default Python has the following filters\r\n```\r\ndefault::DeprecationWarning:__main__\r\nignore::DeprecationWarning\r\nignore::PendingDeprecationWarning\r\nignore::ImportWarning\r\nignore::ResourceWarning\r\n```\r\nwhich ignores `DeprecationWarning`s unless they're raised in `__main__` (`default` enables them for `__main__`).",
  "created_at":"2021-06-15T22:56:04Z",
  "id":861886936,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTg4NjkzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T22:56:04Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks! ",
  "created_at":"2021-06-15T23:40:57Z",
  "id":861903203,
  "issue":929,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MTkwMzIwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-15T23:40:57Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"A lot of things are somewhere between a bug and a feature. This one gets lots of labels!",
  "created_at":"2021-06-16T15:19:34Z",
  "id":862470374,
  "issue":930,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MjQ3MDM3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-16T15:19:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"See https://github.com/scikit-hep/awkward-1.0/issues/770#issuecomment-878630514 for an explanation.",
  "created_at":"2021-07-12T22:09:23Z",
  "id":878630643,
  "issue":930,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODYzMDY0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T22:09:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This was not a new feature or a change in policy: it was a matter of correctly implementing the existing policy.",
  "created_at":"2021-07-12T22:10:34Z",
  "id":878631257,
  "issue":930,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODYzMTI1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T22:10:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"~~OK, this is a fiddly one. I think what's happening is that `read_row_group` is being given the wrong column name; the column name seems to be `\".list.item...\"` whereas Arrow uses `\"\"`~~",
  "created_at":"2021-06-17T10:06:06Z",
  "id":863109652,
  "issue":932,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MzEwOTY1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T16:26:07Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This is being tracked in https://issues.apache.org/jira/browse/ARROW-13151",
  "created_at":"2021-06-30T18:18:32Z",
  "id":871626904,
  "issue":932,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTYyNjkwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T18:18:55Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"This was fixed in some version of Arrow. By now (Arrow 6.0.0), it's no longer a problem.",
  "created_at":"2021-12-07T21:18:10Z",
  "id":988269673,
  "issue":932,
  "node_id":"IC_kwDODBCWws4658xp",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "hooray":1,
   "total_count":2
  },
  "updated_at":"2021-12-07T21:18:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Noting my issue #886 here",
  "created_at":"2021-06-18T00:40:11Z",
  "id":863648934,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2MzY0ODkzNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2021-06-18T00:40:11Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a good point: the implementation might simplify a lot by using pyarrow's dataset function (which I hadn't known about).",
  "created_at":"2021-06-18T12:38:41Z",
  "id":864008276,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDAwODI3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T12:38:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"In this PR I was planning to refactor the code and then take a step back and look at it. Perhaps it might be better to just rewrite it from scratch for the new API.",
  "created_at":"2021-06-18T12:44:13Z",
  "id":864011282,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDAxMTI4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T12:44:13Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I wouldn't be opposed, but take a good look at the old implementation\u2014there's a lot going on there.\r\n\r\nYou definitely don't need to (or want to) rewrite the part that builds Forms with VirtualArrays and turns Parquet columns into Awkward Arrays. That's complicated because Parquet doesn't have lists with `starts`, `stops`, or `offsets`: it has \"definition and repetition levels,\" which are a more bottom-up way of representing jaggedness. Unfortunately, this means that if you want to make a jagged array of records with virtual fields, you have to read one of the columns just to get the `offsets`. The bookkeeping involved in doing that is not something you want to get into, and it's orthogonal to the actual problem of managing datasets (multi-file data).\r\n\r\nThe code that handles datasets is too WET, as you've noticed, and one big function, rather than being broken down. That, along with @martindurant's suggestion if using pyarrow's function, would be definite improvements. But try to limit your scope to just that, or else this would quickly balloon.",
  "created_at":"2021-06-18T13:30:33Z",
  "id":864041150,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDA0MTE1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T13:30:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> You definitely don't need to (or want to) rewrite the part that builds Forms with VirtualArrays and turns Parquet columns into Awkward Arrays. That's complicated because Parquet doesn't have lists with `starts`, `stops`, or `offsets`: it has \"definition and repetition levels,\" which are a more bottom-up way of representing jaggedness. Unfortunately, this means that if you want to make a jagged array of records with virtual fields, you have to read one of the columns just to get the `offsets`. The bookkeeping involved in doing that is not something you want to get into, and it's orthogonal to the actual problem of managing datasets (multi-file data).\r\n\r\nYeah, \"from scratch\" is a slight exaggeration of my intentions. Having separated into routines, it's not as unruly as I first suspected, so I think we should be fine moving forward from here.",
  "created_at":"2021-06-22T07:48:23Z",
  "id":865682812,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTY4MjgxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T07:48:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski this is probably ready for some first-stage review if there's any going spare!",
  "created_at":"2021-06-22T14:43:47Z",
  "id":866045821,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjA0NTgyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T14:43:47Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ah drat, some tests were being skipped locally. I'll check those out.",
  "created_at":"2021-06-22T14:46:57Z",
  "id":866048456,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjA0ODQ1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T14:46:57Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"OK, I've fixed the issues with the tests :relieved: ",
  "created_at":"2021-06-22T15:31:50Z",
  "id":866088495,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjA4ODQ5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T15:31:50Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I just tested `ak.from_parquet` on the Million Song Dataset with\r\n\r\n   * directory containing `_common_metadata` and `_metadata`\r\n   * directory without these files\r\n   * a list of files\r\n   * one file\r\n\r\nand it works beautifully!\r\n\r\nThe only thing I'm explicitly asking for before merging is `(isinstance(obj, Iterable) and isinstance(obj, Sized))`. The name of the superclass is up to you; consider my feedback as you like.",
  "created_at":"2021-06-22T16:16:43Z",
  "id":866129153,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjEyOTE1Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-06-22T16:16:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Can you load directly from s3? ",
  "created_at":"2021-06-22T16:18:08Z",
  "id":866130246,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjEzMDI0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:18:08Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Can you load directly from s3?\r\n\r\nI haven't tried that (because I don't know how it would be done in pyarrow by itself, if an S3 library has to be installed or what\u2014I haven't read the documentation).\r\n\r\nMy test was on a local file.",
  "created_at":"2021-06-22T16:19:19Z",
  "id":866131107,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjEzMTEwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:19:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"You can maybe supply a path like \"s3://bucket/path\"; or else make a filesystem\r\n```\r\nimport fsspec\r\nfs = fsspec.filesystem(\"s3\", anon=True)\r\n```\r\nand pass it as `fs=fs` in the calls to pyarrow. This would be a big deal!",
  "created_at":"2021-06-22T16:20:55Z",
  "id":866132295,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjEzMjI5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:20:55Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"(the latter invocation needs s3fs)",
  "created_at":"2021-06-22T16:21:58Z",
  "id":866133031,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjEzMzAzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:21:58Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"That would be a great feature, and not too hard to implement via the route of passing `fs=fs` to pyarrow. (That is, not attempting to parse the URL and be aware of specific backends, but letting fsspec and pyarrow do that.)\r\n\r\nThis PR is a big change that supposed to be an interface-preserving refactor, so maybe in a new, much smaller PR?",
  "created_at":"2021-06-22T16:33:08Z",
  "id":866145264,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE0NTI2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:33:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@martindurant the plan is to remove all of the `_XXXReader` implementations apart from the file-like reader, and replace them with the new dataset API. That will be a tiny PR, and as @jpivarski, should be quite straightfoward :crossed_fingers:. I had your bug report in mind when writing this PR.",
  "created_at":"2021-06-22T16:49:15Z",
  "id":866156697,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE1NjY5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:49:15Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK, keep me updated!\r\n\r\n> apart from the file-like reader\r\n\r\nYou mean filesystem, right? Because reading parquet involves doing various folder listings and such.",
  "created_at":"2021-06-22T16:51:47Z",
  "id":866158699,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE1ODY5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:54:26Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> You mean filesystem, right? Because reading parquet involves doing various folder listings and such.\r\n\r\nSorry, didn't mean to edit your reply.\r\n\r\n\r\nCurrently we support passing in a file object (but not a file descriptor actually, which we perhaps should), which the new dataset API doesn't handle. For this reason, we'll need to keep a legacy API when we move to `arrow.dataset.dataset`",
  "created_at":"2021-06-22T16:54:39Z",
  "id":866160851,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE2MDg1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:54:46Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I think it would be fine if passing a file-like object means you can't use the dataset API, but passing a string means you can. The main use-case that I can think of for passing a file-like object is `BytesIO`, which would necessarily have to be small data, hence a single \"file.\"",
  "created_at":"2021-06-22T16:56:48Z",
  "id":866162459,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE2MjQ1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:56:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski when I started this PR, one of my intentions was to merge the code-paths between lazy and eager reading. I found a bug in the lazy reading that didn't happen for the eager (see #932). I think now that things are much cleaner, maybe we don't need to do that. I suppose in either case, that's for another PR!",
  "created_at":"2021-06-22T17:00:29Z",
  "id":866165039,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE2NTAzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T17:05:42Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"OK, done :crossed_fingers: ",
  "created_at":"2021-06-22T17:01:43Z",
  "id":866165993,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE2NTk5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T17:01:43Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"If you want to use in-memory temporary \"files\", you might want to use MemoryFileSystem from fsspec\r\n```\r\nfs = fsspec.filesystem(\"memory\")\r\n```\r\nwhich is fully compatible with other filesystems and, therefore, pyarrow. In the context of Dask, we never pass around actual open files.",
  "created_at":"2021-06-22T17:02:10Z",
  "id":866166305,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE2NjMwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T17:02:10Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ah, I never checked the FS implementations in `fsspec`. That's very snazzy, I should probably consider supporting ffspec in my own work.\r\n\r\nRE the file-object - it's just that we currently support it (and as a user, I sometimes have an open file that I want to use!)",
  "created_at":"2021-06-22T17:04:10Z",
  "id":866167726,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE2NzcyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T17:05:27Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"That makes a lot of sense for Dask, since an open file handle is a great example of a non-serializable, non-transferrable object. Trying to pass them around in a distributed environment would cause a lot of pain!\r\n\r\nHowever, Awkward Array is not, by itself, a distributed processor; it's a library that could be used inside a distributed processor. There are enough people using it in a single-thread/single-machine setting that they'd be confused about a file-reading function not being able to take a file-like object as an argument. Python has a long history of passing file-like objects to functions like `json.load` as the primary API. I even got a request for adding this to Uproot: https://github.com/scikit-hep/uproot4/issues/98 There, the use-case was something I hadn't thought of: they had files in a tarball and wanted to use Python's `tarfile` to iterate over them.\r\n\r\nI don't remember when `ak.from_parquet` acquired this ability to read file-like objects, but I wouldn't want to remove it. If it causes trouble for people using Awkward Array in a distributed setting\u2014i.e. with Dask\u2014then we'll explain that file-like objects are for small, non-distributed cases and they should be loading it by filename (and fsspec). That wouldn't be specific to Awkward Array, but would apply to anything in a distributed setting. That's also why the file-like object interface can miss out on this ability to read datasets (I don't see how the interface would make any sense for datasets, anyway).\r\n\r\n(Uproot users of file-like objects don't get any of Uproot's parallel-reading features, which is a similar restriction.)",
  "created_at":"2021-06-22T17:20:02Z",
  "id":866179708,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE3OTcwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T17:20:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I agree in theory, and fastparquet supports file-like objects too; obviously, I've made something of a career in providing file-like objects to other libraries exactly for this reason. Indeed, I would have told your user to use fsspec's tar handling...\r\n\r\nHowever, parquet is special in its directory-as-a-dataset approach, so at least that space needs good testing, both for local files and remote, with file-like objects (from fsspec or otherwise), *and* filesystem objects.",
  "created_at":"2021-06-22T17:33:17Z",
  "id":866189016,
  "issue":935,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjE4OTAxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T17:33:17Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"This is good, thanks! But I think you didn't intend to check in src/awkward/operations/test-parquet.ipynb, right?",
  "created_at":"2021-06-18T12:35:33Z",
  "id":864006586,
  "issue":937,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDAwNjU4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T12:35:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This is good, thanks! But I think you didn't intend to check in src/awkward/operations/test-parquet.ipynb, right?\r\n\r\nOh drat :facepalm: . Working late :sleeping: ",
  "created_at":"2021-06-18T12:36:37Z",
  "id":864007183,
  "issue":937,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDAwNzE4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T12:36:37Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski fixed!",
  "created_at":"2021-06-18T12:38:50Z",
  "id":864008359,
  "issue":937,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDAwODM1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T12:39:00Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Partitions don't have to have the same form, but they do have to have the same type. If one partition is a ListOffsetArray same the next is a ListArray, that ought to be okay for everything downstream. Mixing either of these list node types with RegularArray is not okay, and if it was doing that, that would be a bug.\r\n\r\nA lot of the PartitionedArray code is already in Python, largely because it was true last to be written. It does have a C++ part, but the C++ part is used minimally, almost all of the operations (including repartition,I think) is implemented in src/awkward/partition.py.",
  "created_at":"2021-06-18T12:33:07Z",
  "id":864005266,
  "issue":939,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDAwNTI2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T14:58:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Positions don't have to have the same form, but they do have to have the same type. If one partition is a ListOffsetArray same the next is a ListArray, that ought to be okay for everything downstream. Mixing either of these list node types with RegularArray is not okay, and if it was doing that, that would be a bug.\r\n\r\n\r\nI'll check the finer grained cases here and report back.\r\n\r\n> It does have a C++ part, but the C++ part is used minimally, almost all of the operations (including repartition,I think) is implemented in src/awkward/partition.py.\r\n\r\nRight I observed the same. So, is the plan to lift existing C++ layouts into the Python layer and leave just the kernels at the C++ level?",
  "created_at":"2021-06-18T14:44:38Z",
  "id":864090455,
  "issue":939,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDA5MDQ1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T14:44:38Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> Right I observed the same. So, is the plan to lift existing C++ layouts into the Python layer and leave just the kernels at the C++ level?\r\n\r\nThat's exactly right, though I would refer to the kernels level as \"mostly C\" because the ABI is `extern C` and the only C++ feature they use is function templates for integer specialization (something that could have been a C macro, but it's safer as a template). So the current 3-layer structure is C \u2192 C++ \u2192 Python and the Awkward 2.0 structure is C \u2192 Python \u2192 Python. Since both of the last two levels are Python, they may merge, losing their distinctive identity.\r\n\r\nThe conceptual distinction between the middle and high layers (currently C++ and Python) is weak, especially for partitioned arrays. The original intention was for the high-level layer to be a thin wrapper around the middle layer, but it was so much effort to add C++ methods that we added a lot of functionality in the Python layer directly, especially for partitioned arrays. Instead of translating the C++ `ak._ext.PartitionedArray` and `ak._ext.IrregularlyPartitionedArray` classes to Python, we'll just merge their functionality (which isn't much) directly into the Python `ak.partition.PartitionedArray` and `ak.partition.IrregularlyPartitionedArray`. Partitioning is decoupled from everything else because they are (deliberately) not subclasses of Content, so we have more freedom to make them strictly two-level: C \u2192 Python. That can even happen independently of or before the general Awkward 2.0 transition.",
  "created_at":"2021-06-18T15:29:41Z",
  "id":864119189,
  "issue":939,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDExOTE4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T15:29:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This was really informative. Thanks Jim, things are clearer now. ",
  "created_at":"2021-06-19T20:44:23Z",
  "id":864461470,
  "issue":939,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDQ2MTQ3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-19T20:44:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"This is moot because PartitionedArray won't be ported to v2.",
  "created_at":"2021-12-07T21:21:22Z",
  "id":988271366,
  "issue":939,
  "node_id":"IC_kwDODBCWws4659MG",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-12-07T21:21:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Per #942 this is not correct!",
  "created_at":"2021-06-18T14:42:37Z",
  "id":864089155,
  "issue":941,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDA4OTE1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T14:42:37Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Wait\u2014the `cache=\"new\"` and `cache={}` options are a necessary part of the `with_cache API. If there's something wrong with then, they need to be fixed, but they can't be removed.\r\n\r\nThe weak reference is a concession to the C++ layer that will be replaced with a regular reference when it's translated into Python. It prevents cyclic references through the C++ that Python's garbage collector is unable to identify. So for now, we have to live with the weak reference.\r\n\r\nThe way this is supposed to work is that any reference to the cache must be retained within the scope of the `with_cache` function, since the VirtualArray/ArrayCache isn't holding it. Before the function returns, it creates an `ak.Array` that _does_ hold the reference, strongly.\r\n\r\nIf you were using the function with `highlevel=False`, that mechanism would have broken. Is that what happened? It could be that we have to disable `highlevel=False` in this function\u2014it's just not going to work, considering how the reference is held only by the `ak.Array` object. The `highlevel=False` will have to wait until the C++ layer is translated into Python.\r\n\r\nWith `highlevel=False`, `cache=\"new\"` and `cache={}` will fall, but so would `cache=anything_constructed_on_the_spit()`, since its reference only exists in the argument list and will be fine after the `with_cache` function returns.\r\n\r\nWith `highlevel=False`, you can still use this for making low-level layouts. You have to pull the layout out of the unwanted `ak.Array`, but that `ak.Array` has to live long enough for the final `ak.Array` to pick up strong references to the caches.",
  "created_at":"2021-06-18T13:02:25Z",
  "id":864021786,
  "issue":942,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDAyMTc4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T13:02:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> The way this is supposed to work is that any reference to the cache must be retained within the scope of the with_cache function, since the VirtualArray/ArrayCache isn't holding it. Before the function returns, it creates an ak.Array that does hold the reference, strongly.\r\n\r\nRight\r\n> If you were using the function with highlevel=False, that mechanism would have broken. Is that what happened? It could be that we have to disable highlevel=False in this function\u2014it's just not going to work, considering how the reference is held only by the ak.Array object. The highlevel=False will have to wait until the C++ layer is translated into Python.\r\nYes, this is likely the culprit.\r\n\r\nOK, then I'll repush this branch with the fix for #940. ",
  "created_at":"2021-06-18T13:07:54Z",
  "id":864025264,
  "issue":942,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDAyNTI2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T13:11:11Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski OK, corrected my oversight. I've removed the `highlevel` argument, and I think*think* fixed the cache handling so that it matches the docstring. Could you give that a quick once over?",
  "created_at":"2021-06-18T14:41:45Z",
  "id":864088598,
  "issue":942,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDA4ODU5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T14:41:45Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Just removing the `highlevel` parameter should do it, but eventually somebody's going to ask, \"Why doesn't this function have a `highlevel` argument? Here, I'll add one.\"\r\n\r\nTo ensure that it doesn't just look like an oversight, let's leave the argument in (in other words, revert almost everything in this PR) and just add an error message if `highlevel is not True`. That makes the `highlevel` argument useless, but it does communicate the fact that it was removed on purpose, and the error message would say, \"ak.with_cache cannot allow highlevel=False because the only strong reference to caches are held by ak.Array objects; VirtualArrays only hold weak references, which would go out of scope with this function. This will be fixed in Awkward 2.0, when VirtualArrays are reimplemented in Python and can safely hold strong references to caches. For now, use highlevel=True and extract the layout from the high-level array\" (with good formatting so that that long explanation isn't on one long line).\r\n\r\nThis PR then just becomes a prevention of `highlevel=False` with an explanation of why it is temporarily disabled.",
  "created_at":"2021-06-18T14:56:19Z",
  "id":864097768,
  "issue":942,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDA5Nzc2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T14:56:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski is be tempted to just put this information in a comment, but as it is planned to support this in future, we might as well keep the API stable. I'll make the changes at some point :) ",
  "created_at":"2021-06-18T19:07:59Z",
  "id":864225353,
  "issue":942,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDIyNTM1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T19:07:59Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I just took care of it. Thanks for opening this!",
  "created_at":"2021-06-18T19:21:03Z",
  "id":864230843,
  "issue":942,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDIzMDg0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T19:21:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski from the docstring, isn't the cache=None case supposed to unset the caches entirely? ",
  "created_at":"2021-06-19T00:05:22Z",
  "id":864325461,
  "issue":942,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDMyNTQ2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-19T00:05:31Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I think you might be right about that. I might have just committed a mistake. I didn't look carefully enough at the logic of it, I assumed that the old logic was right.\r\n\r\nIs it now in disagreement with the documentation?",
  "created_at":"2021-06-19T15:08:02Z",
  "id":864418435,
  "issue":942,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDQxODQzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-19T15:08:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Again, could be wrong - long week. The existing implementation has a case\nfor which the existing cache is left in place. As fair as I can tell, the\ncache should either be replaced or removed, with no pass through condition.\n\nOn Sat, 19 Jun 2021, 16:08 Jim Pivarski, ***@***.***> wrote:\n\n> I think you might be right about that. I might have just committed a\n> mistake. I didn't look carefully enough at the logic of it, I assumed that\n> the old logic was right.\n>\n> Is it now in disagreement with the documentation?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-hep/awkward-1.0/pull/942#issuecomment-864418435>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAJQZHLWZPP7G6MPLXK73A3TTSXF3ANCNFSM465STQYA>\n> .\n>\n",
  "created_at":"2021-06-19T20:43:31Z",
  "id":864461365,
  "issue":942,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDQ2MTM2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-19T20:43:31Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Enough for now. Merge and pick it up again next week.",
  "created_at":"2021-06-18T21:13:34Z",
  "id":864277066,
  "issue":943,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NDI3NzA2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-18T21:13:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@SantamRC Is this PR still active, or is it superseded by #971?",
  "created_at":"2021-07-12T18:21:14Z",
  "id":878494181,
  "issue":944,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODQ5NDE4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T18:21:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @SantamRC Is this PR still active, or is it superseded by #971?\r\n\r\nThis PR is not active anymore\r\n",
  "created_at":"2021-07-13T12:08:41Z",
  "id":879032196,
  "issue":944,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTAzMjE5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T12:08:41Z",
  "user":"MDQ6VXNlcjUyNjM1Nzcz"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - should the following `argsort` feature be considered a bug?\r\n\r\nTake an example where `argsort` is used to generate indices to get a sorted array:\r\n```python\r\n>>> import awkward as ak\r\n>>> array = ak.Array([-5, 4, 1, -1, 30])\r\n>>> index = ak.argsort(array)\r\n>>> array[index]\r\n<Array [-5, -1, 1, 4, 30] type='5 * int64'>\r\n>>> ak.sort(array)\r\n<Array [-5, -1, 1, 4, 30] type='5 * int64'>\r\n```\r\nI'd assume the same should work for an array containing `None`s:\r\n```python\r\n>>> array = ak.Array([None, None, 1, -1, 30])\r\n>>> index = ak.argsort(array)\r\n>>> array[index]\r\n<Array [None, None, 1, None, None] type='5 * ?int64'>\r\n>>> ak.sort(array)\r\n<Array [-1, 1, 30, None, None] type='5 * ?int64'>\r\n```\r\nwhile it is possible to get sorted list from:\r\n```python\r\n>>> ak.to_list(array.layout.content[index])\r\n[-1, 1, 30, None, None]\r\n```",
  "created_at":"2021-06-22T15:51:42Z",
  "id":866108689,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjEwODY4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T15:51:42Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, that's wrong. It looks like `ak.argsort` is returning type `?int64`, rather than `int64`. Is this a manifestation of the fact that 1-dimensional arrays were not handled until recently? Does the same thing happen when argsorting deeper lists?\r\n\r\nSince `ak.sort` puts missing values at the end (`None` is after all numbers), `ak.argsort` should put the index positions of those missing values last, just as it would with any other value.",
  "created_at":"2021-06-22T15:58:19Z",
  "id":866114206,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjExNDIwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T15:58:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Yes, that's wrong. It looks like `ak.argsort` is returning type `?int64`, rather than `int64`. Is this a manifestation of the fact that 1-dimensional arrays were not handled until recently? Does the same thing happen when argsorting deeper lists?\r\n> \r\n> Since `ak.sort` puts missing values at the end (`None` is after all numbers), `ak.argsort` should put the index positions of those missing values last, just as it would with any other value.\r\n\r\nYes, it's the same for deeper lists.",
  "created_at":"2021-06-22T16:05:09Z",
  "id":866119712,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NjExOTcxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T16:05:09Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I've linked this PR to issue #967 because merging this will resolve that. Was this PR intended to solve another issue, too? If so, we should use the \"Linked issues\" to ensure that the corresponding issues gets cleared when this is done. I find the closing keywords, such as \"fixes #XYZ\" to be pretty unreliable (I'll pick the wrong tense or something), but that \"Linked issues\" section is great.",
  "created_at":"2021-06-30T18:54:49Z",
  "id":871649352,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTY0OTM1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T18:54:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I've linked this PR to issue #967 because merging this will resolve that. Was this PR intended to solve another issue, too? If so, we should use the \"Linked issues\" to ensure that the corresponding issues gets cleared when this is done. I find the closing keywords, such as \"fixes #XYZ\" to be pretty unreliable (I'll pick the wrong tense or something), but that \"Linked issues\" section is great.\r\n\r\nThanks! I think I never opened the issues this PR is trying to solve...",
  "created_at":"2021-06-30T18:57:41Z",
  "id":871651101,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTY1MTEwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T18:57:41Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - should this case be different, e.g. keeping `None`?\r\n```python\r\n    array = ak.Array([[3, 2, 1], [], None, [4, 5]])\r\n\r\n    assert ak.to_list(ak.argsort(array, axis=1)) == [\r\n        [2, 1, 0],\r\n        [],\r\n        None,\r\n        [0, 1],\r\n    ]\r\n```\r\nor should it be:\r\n```python\r\n   [\r\n        [2, 1, 0],\r\n        [],\r\n        0,  # or 2?\r\n        [0, 1],\r\n    ]\r\n```",
  "created_at":"2021-06-30T22:02:10Z",
  "id":871755792,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTc1NTc5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T22:02:10Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - should this case be different, e.g. keeping `None`?\r\n> \r\n> ```python\r\n>     array = ak.Array([[3, 2, 1], [], None, [4, 5]])\r\n> \r\n>     assert ak.to_list(ak.argsort(array, axis=1)) == [\r\n>         [2, 1, 0],\r\n>         [],\r\n>         None,\r\n>         [0, 1],\r\n>     ]\r\n> ```\r\n\r\nThe \"None\" is a level above the one where the sorting is happening, so yes, it stays.\r\n\r\nFor `axis=-1`, it seems pretty clear what sorting should do: the bottom-most lists get sorted (or argsorted, returning integer positions, even positions of Nones), but everything above this lowest level is left as-is. That's why this list-level None is fine.",
  "created_at":"2021-06-30T23:04:07Z",
  "id":871781572,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTc4MTU3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T23:04:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - ok, the `None`s are replaced by their indices in `argsort`. I was wondering if it should be an option for `argsort`? \r\n\r\nThe `nan` values in an array would prevent correct sorting, so this is fixed as well. There are two (possibly one kernel function) to write - the `for` loops are marked with `FIXME`, but this is for tomorrow.",
  "created_at":"2021-07-01T16:48:47Z",
  "id":872400021,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MjQwMDAyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-01T16:48:47Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna I see in the tests that this is doing the right thing. This PR represents an important bug-fix (handling None at `axis=0` and in `argsort`), so it would be great to include it in 1.4.0, but the time for that is constrained by PyHEP. Perhaps this PR should go into 1.4.0 now (since it concerns user-visible changes) and the refactoring to put the inline loops into kernels can be done in another PR that gets merged after 1.4.0 (since it concerns important, but not user-visible, code organization)? Do you think it's in a stable state for that, notwithstanding the cleanup issues that need to eventually happen?\r\n\r\n> I was wondering if it should be an option for `argsort`?\r\n\r\nI think that would complicate the interface too much. It's not really a useful feature (someone could mask the output with `ak.is_none` to get the same effect).",
  "created_at":"2021-07-01T18:03:48Z",
  "id":872445765,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MjQ0NTc2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-01T18:03:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna Let me know if I can merge this PR as-is and include it in 1.4.0. Thanks!",
  "created_at":"2021-07-01T18:27:46Z",
  "id":872460221,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MjQ2MDIyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-01T18:27:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @ianna Let me know if I can merge this PR as-is and include it in 1.4.0. Thanks!\r\n\r\n@jpivarski - if the last commit succeeds, that's all done. Please, merge it ",
  "created_at":"2021-07-01T18:30:30Z",
  "id":872461716,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MjQ2MTcxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-01T18:30:30Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"That was fast! I checked the diff, and it looks good, too. The kernel headers and ctypes should now be auto-generated, and if they're not, a failed test will tell us that. I'll enable auto-merge.",
  "created_at":"2021-07-01T18:34:26Z",
  "id":872463867,
  "issue":946,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MjQ2Mzg2Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-01T18:34:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I'm doe with this for today. Please, have a look. Thanks!",
  "created_at":"2021-06-23T15:33:44Z",
  "id":866942155,
  "issue":948,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Njk0MjE1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T15:33:44Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - thanks! You\u2019ve got a point. The reference documentation is indeed more tutorial-like and since these pages refer to it I thought it is not necessary repeating it. It\u2019s sort of like \u201cok, I\u2019ve got an idea what it does and if I want to know more, I\u2019ll check the reference guide\u201d. I\u2019ll give it another try if you don\u2019t mind.",
  "created_at":"2021-06-23T21:05:51Z",
  "id":867158953,
  "issue":948,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzE1ODk1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T21:05:51Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - as discussed - only the pictures are left in this PR.",
  "created_at":"2021-12-21T14:46:51Z",
  "id":998837853,
  "issue":948,
  "node_id":"IC_kwDODBCWws47iQ5d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-21T14:46:51Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#948](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (fe00d9b) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `1.53%`.\n> The diff coverage is `83.23%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_reducers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19yZWR1Y2Vycy5weQ==) | `96.48% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `56.27% <51.72%> (+0.82%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `59.75% <63.23%> (+0.44%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.19% <63.58%> (-0.36%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `69.54% <74.19%> (+0.26%)` | :arrow_up: |\n| ... and [31 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/948/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-12-21T14:57:28Z",
  "id":998846130,
  "issue":948,
  "node_id":"IC_kwDODBCWws47iS6y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-21T15:34:28Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"They're pretty common patterns. The `maybe_wrap_like` is only going to be applicable in cases with a single array argument and if the `behavior` is not needed (and extracted) earlier in the function, but the helper function creep doesn't bother me for these well-named functions.\r\n\r\nI see helper functions as being a balance between avoiding repetitive (WET, non-DRY) code on the one hand versus the indirection of having to look up what all of these helper functions do on the other. There are cases when I'd actually prefer WET over DRY, and that is if the thing that you're Writing Every Time is short enough to be easily read as an \"idiom\" and it's so generic that the name is not immediately recognizable. Basically, the optimization metric is minimizing the time needed to understand a bit of code, not minimizing the number of characters in the code or any code-golf thing like that.\r\n\r\nThere's also been a lot more effort in beautifying the public programming interface that data analyst users will be interacting with every day in Jupyter than the internal machinations that we use to implement it, since we can improve the latter at any time (as you're doing right now).",
  "created_at":"2021-06-23T19:52:48Z",
  "id":867114717,
  "issue":952,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzExNDcxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T19:52:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> They're pretty common patterns. The maybe_wrap_like is only going to be applicable in cases with a single array argument and if the behavior is not needed (and extracted) earlier in the function, but the helper function creep doesn't bother me for these well-named functions.\r\n\r\nI thought about making arrays support an iterable of Array or an Array, but I'm not sure whether that's just making things more complicated for a small number of cases.\r\n\r\n> There's also been a lot more effort in beautifying the public programming interface that data analyst users will be interacting with every day in Jupyter than the internal machinations that we use to implement it, since we can improve the latter at any time (as you're doing right now).\r\n\r\nI agree on these principles. For me, this function is beneficial more in that it reduces the likelihood of programming errors (though these kinds of constructs usually fail loudly), and should make it easier to read the code because this will be a frequently seen idiom. \r\n\r\nThis PR is definitely take it or leave it - it doesn't add any new features ;)",
  "created_at":"2021-06-23T20:12:12Z",
  "id":867126100,
  "issue":952,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzEyNjEwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T20:12:12Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Does that mean it's done? Because I approve! Give me a checkmark and I'll enable auto-merge.",
  "created_at":"2021-06-23T20:23:18Z",
  "id":867132673,
  "issue":952,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzEzMjY3Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-23T20:23:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PS: As a safety feature, cibuildwheel runs from a different directory, so that doesn't pick up a local copy by accident. But the current form of the tests require only running from one directory (the repo directory). This enables them to be run anywhere.",
  "created_at":"2021-06-24T13:48:33Z",
  "id":867651246,
  "issue":953,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzY1MTI0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-24T13:48:33Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Only one test file assumed a directory? I'm surprised. But if the tests pass from a different directory, then that must have been it.\r\n\r\nThanks!",
  "created_at":"2021-06-24T14:38:20Z",
  "id":867690051,
  "issue":953,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzY5MDA1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-24T14:38:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Note I am not running with any extras, like numba, etc, so this was just for the basic tests. (Adding a few extras might not be a bad idea, but we'll start with this).",
  "created_at":"2021-06-24T14:56:31Z",
  "id":867703620,
  "issue":953,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzcwMzYyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-24T14:56:31Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"FYI, the current pybind/cmake_example was heavily influenced by awkward's setup.py, so it's coming full circle. :)",
  "created_at":"2021-06-23T21:51:52Z",
  "id":867185286,
  "issue":954,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzE4NTI4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T21:51:52Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> The only thing that sticks out to me is a mixture of += and append\r\n\r\nI had intended to make all the one-item additions an append, but got missed. Might try to clean that up later, then.",
  "created_at":"2021-06-24T19:06:38Z",
  "id":867883999,
  "issue":954,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Nzg4Mzk5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-24T19:06:38Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2021-06-23T21:13:50Z",
  "id":867163340,
  "issue":955,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzE2MzM0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-23T21:13:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Be sure to let me know when you're done. (\"Done\" meaning time to enable auto-merge and fight with Azure to get the MacOS tests to pass.)",
  "created_at":"2021-06-24T16:31:26Z",
  "id":867785392,
  "issue":957,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Nzc4NTM5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-24T16:31:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It's all done. ",
  "created_at":"2021-06-24T16:37:25Z",
  "id":867789029,
  "issue":957,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Nzc4OTAyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-24T16:37:25Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"Copying from Slack, `Content.__getitem__` should be roughly:\r\n\r\n   1. check int\r\n   2. check slice\r\n   3. check string\r\n   4. check non-tuple iterable of strings\r\n   5. ...\r\n   6. check ak.Array, pull out its layout and call `__getitem__` on that so that it can be handled by rule 7 or 8\r\n   7. check NumpyArray (to enter into `_getitem_array`)\r\n   8. check generic Content (other cases, like jagged arrays and missing values)\r\n   9. check tuple (for the generic `_getitem_next`)\r\n   10. check remaining Iterable by calling nplike.`asarray` on it, wrapping it in NumpyArray and passing it into `__getitem__` again so that it gets picked up by rule 7 and goes into `_getitem_array`\r\n   11. TypeError\r\n\r\n_(roughly)_",
  "created_at":"2021-06-24T22:28:04Z",
  "id":867994176,
  "issue":959,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2Nzk5NDE3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-24T22:28:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"For testing, you can alternate between old-style and new-style arrays. The `ak._v2.tmp_for_testing` module defines `v1v2_equal`, which you can use to see if an old-style and a new-style array are exactly equal, and `v1_to_v2`/`v2_to_v1` for conversions.\r\n\r\nWhen your project is done, we'll be removing the `ak._v2.tmp_for_testing` module as well as the old-style arrays, so while they're useful for ensuring that your new functions are compatible with the old ones, the final tests will have to be written in such a way that we can remove the old stuff and not lose the tests.\r\n\r\nYou can do this by developing with direct comparisons (`v1v2_equal`), then make the final tests look like\r\n\r\n```python\r\nresult = your_v2_array.something()\r\nassert ak.to_list(result) = [...]\r\nassert result.form.tolist() == {...}\r\nassert v1v2_equal(result, v2_to_v1(your_v2_array).something())\r\n```\r\n\r\nwhere you get `[...]` and `{...}` from the old-style array, and they're plain Python data. When we remove the old-style arrays, we can remove the last assertion, but the other two completely check the correctness of the array (values and detailed type).",
  "created_at":"2021-06-24T23:54:56Z",
  "id":868072413,
  "issue":959,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODA3MjQxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-24T23:54:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"All Content subclass constructors (e.g. calling `ListArray`, `IndexedArray`, etc.) need to pass in the appropriate `identifiers` and `parameters`. We can't define `identifiers` yet, but I'll make utility functions to do the right thing when they're not all `None` (and pass through `None` for now). Passing through or creating empty `parameters` is very important, so the next PR should get started doing that.\r\n\r\nAdditionally, I'll make the very early (low numbered) tests start testing the new code soon.\r\n\r\nThis PR was in very good shape before I started messing with it. A few things were simplified by using kernels, rather than NumPy functions, which is something that we can do now, due to #966.\r\n\r\nI'll try to get some underlying infrastructure done before you come back to this next week.",
  "created_at":"2021-07-19T23:31:26Z",
  "id":882931121,
  "issue":959,
  "node_id":"IC_kwDODBCWws40oHWx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-19T23:31:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Ahh, actions don't run from a fork if not already in the repo. I can move the branch to the repo and try again.\r\n\r\nPS: Unrelated, but the it seems GitHub has new link previews for cross-linked PRs. :)",
  "created_at":"2021-06-24T17:10:28Z",
  "id":867809944,
  "issue":960,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NzgwOTk0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-24T17:10:28Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I triggered the 1.4.0rc1 release ([which is huge](https://github.com/scikit-hep/awkward-1.0/releases/tag/1.4.0rc1)) and [it's starting on Azure](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=6972&view=results). I guess the switch from Azure to GitHub Actions is here, in this unmerged PR. That's okay\u2014I just misunderstood.\r\n\r\nAnyway, if the support for a different set of platforms is in main (and therefore the 1.4.0rc1 that's getting built and deployed now), then it should be 1.4.0 regardless of whether it was built on GitHub Actions or Azure. The important thing for the version number is to have a clear dividing line when a different set of things is supported. if this PR is just changing where it gets built, not which systems it's deploying wheels for, then this PR does not need to have a transitional version number: it can be 1.4.1 or 1.5.0rc1.\r\n\r\n(On the side, I wish I knew what to do about bug-fix releases, such as 1.4.1, that come after next-version prereleases, such as 1.5.0rc1. In the past, I've been interleaving them, but I think it's terrible: 1.4.0, 1.5.0rc1, 1.4.1, 1.5.0rc2, 1.4.2, ... It seems very wrong to me that the chronological and incremental increases are not monotonic with version number. At the moment, though, it doesn't look like that's what's going to happen here, but I'd like to have a plan in place, in case you have any recommendations.)",
  "created_at":"2021-06-25T18:03:24Z",
  "id":868740988,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODc0MDk4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-25T18:03:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This could go into r2 / 1.4.0 final, still? This contains PyPy & Universal2 binaries that would be good to ship. Changing the set of binaries in an RC shouldn't be an issue - pip only looks at final releases in the normal solve.\r\n\r\nDo you have answers for the final two points? (Manylinux1 for 3.7 and 3.8, where should we start with ARM). I need to test this on Apple Silicon, drop the build-on-all-PRs setting, and add a token to GitHub's Secrets.",
  "created_at":"2021-06-25T18:47:41Z",
  "id":868764795,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODc2NDc5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-25T18:47:41Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Interleaving versions is fine - if you sort chronologically, that's just what's going to happen. PyPI sorts by version number, so it looks fine there. GitHub Releases sorts chronologically - it has to, there's no rules about versions there. You could call your releases \"bill\" and \"bob\" and it would be just as happy.",
  "created_at":"2021-06-25T18:50:45Z",
  "id":868766409,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODc2NjQwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-25T18:50:45Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"( I should point out there are no code changes at all in this PR, those have already been made. So it seems like a fine change to go into a new RC, IMO)",
  "created_at":"2021-06-25T18:51:42Z",
  "id":868766931,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODc2NjkzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-25T18:51:42Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Looks like I need to adapt the CMake build to target Apple Silicon, the Universal2 wheel just has x86 arch in it. Working on it.",
  "created_at":"2021-06-25T19:07:34Z",
  "id":868775513,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODc3NTUxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-25T19:07:34Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> This contains PyPy & Universal2 binaries that would be good to ship.\r\n\r\nIn that case, let's try to get this PR into 1.4.0rc2. I'm not at all worried about changing the set of supported releases within the release candidates, only with the final 1.4.0. If it goes in two batches (worst case), then it would be 1.5.0. Just so that there's a clear line for figuring out dependencies.\r\n\r\nI'll be out of touch for the next day, starting right now (camping with family), so we'll get back to this on Monday.\r\n\r\nAs for your two questions,\r\n\r\n> * [ ]  What should the range of ARM builds be? 3.6+? 3.7+?\r\n\r\nFrom a programming perspective, the steep threshold is between 3.5 and 3.6, largely because of the unstable dict order. From 3.6 onward, it's all the same as far as Awkward Array compatibility is concerned. If 3.6 has a lot fewer users on ARM than 3.7, that would be a reason to not bother with it (reasons from outside the codebase, rather than reasons from inside the codebase). But I can't provide guidance on that; I don't have usage statistics.\r\n\r\n> * [ ]  Should we enable manylinux1 3.7 and 3.8? Looking back at that list, there's still 7% of users with an old pip version (that doesn't officially support Python 3.7) on Python 3.7. What are we aiming for here?\r\n\r\n\"7% of users\" sounds like a large fraction to me. Personally, I would cut a threshold somewhere between 0.1% and 1%. The 1-in-a thousand or 1-in-a hundred user doesn't have to be catered to (should still be possible to install, but we don't need to make special wheels for them), but 7% of users is a lot. If we have to draw a hard line, make it 1%.",
  "created_at":"2021-06-25T19:14:07Z",
  "id":868778975,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODc3ODk3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-25T19:14:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Guess I'll work on this slowly, GH seems to be having networking issues. \"Internal Server Error\" trying to checkout awkward from GitHub. \ud83e\udd23",
  "created_at":"2021-06-25T20:36:25Z",
  "id":868819777,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODgxOTc3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-25T20:36:25Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"NumPy on PyPy Linux is failing to load - maybe the NumPy PyPy wheel is not actually manylinux2010 compatible? Going to guess it's their problem, not ours, and disable testing for now. We should probably check the PyPy Linux (or even macOS) wheels manually (I'm on an AS machine currently, so no local PyPy or docker)",
  "created_at":"2021-06-26T03:20:53Z",
  "id":868938488,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODkzODQ4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-26T03:20:53Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Tested the wheels locally on Apple Silicon, they work fine. Should we do Universal + x86, or Arm + x86? Eventually, we should get multitagging and then we can have all three tags on a single universal wheel, if we want to. But that's on my todo list.\r\n\r\nCurrently, the set of all the wheels (which can be downloaded from the PR) is about 350 MB.",
  "created_at":"2021-06-26T03:24:00Z",
  "id":868938902,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODkzODkwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-26T06:15:27Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> * [ ]  Drop the build-on-PR\r\n\r\nLet me know when you think it's ready. (I'm not really sure what is meant by this checkbox.) I'd like to do another release candidate with this PR included to test-run the new deployment. We have all week to go through a few cycles of tests, but it should be stable by the end of the week with a non-RC release so that it's ready for new users from PyHEP. Thanks!",
  "created_at":"2021-06-28T14:33:12Z",
  "id":869737450,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2OTczNzQ1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-28T14:33:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I'm not really sure what is meant by this checkbox\r\n\r\nCurrently this builds on _all_ PRs, which is overkill for building wheels. Maybe building a small subset of wheels would be a good idea, though, on all PRs? I do that in boost-histogram.\r\n\r\nI'll drop this and then it is ready. If you have a PR that changes the build and might affect wheels, you can manually trigger this from the GHA web interface on local branches.",
  "created_at":"2021-06-28T15:45:32Z",
  "id":869795566,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2OTc5NTU2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-28T15:45:32Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"FYI, PyPy is untested, due to NumPy not working in the \"classic\" pypy manylinux image, so I'd advertise it as experimental support currently. It should be easy to test locally. cibuildwheel 2.0 will support pypy in any manylinux image 2010+, so we could likely enable testing then if a newer image fixes it.",
  "created_at":"2021-06-28T15:48:37Z",
  "id":869798000,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2OTc5ODAwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-28T15:48:37Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> Error: Unable to resolve action `actions/checkout@v2cp36-manylinux_x86_64`, unable to find version `v2cp36-manylinux_x86_64`\r\n\r\nI haven't been able to find any references to this string anywhere: \"v2cp36-manylinux_x86_64\". Is it a GitHub Action module, a Docker image, or what?\r\n\r\n**Edit:** But it looks like you fixed it.",
  "created_at":"2021-06-28T16:54:02Z",
  "id":869846374,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2OTg0NjM3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-28T17:04:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The syntax is `uses: <github_repo>@<tag>`, and I pasted the manylinux repo name onto it by accident. :)",
  "created_at":"2021-06-28T17:42:39Z",
  "id":869883244,
  "issue":961,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2OTg4MzI0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-28T17:42:39Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ianna Jim mentioned that you might have an idea of where I should look to fix this. \r\n\r\nI *think* the issue might be that we assume any `ListOffsetArrayOf<int64_t>` has `offsets[0] = 0` because we always convert other `ListOffsetArrayOf` with `toListOffsetArray64(True)`.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/main/src/libawkward/array/ListOffsetArray.cpp#L1892-L1908 \r\n\r\n",
  "created_at":"2021-06-30T14:25:52Z",
  "id":871452736,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTQ1MjczNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T14:25:52Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @ianna Jim mentioned that you might have an idea of where I should look to fix this.\r\n> \r\n> I _think_ the issue might be that we assume any `ListOffsetArrayOf<int64_t>` has `offsets[0] = 0` because we always convert other `ListOffsetArrayOf` with `toListOffsetArray64(True)`.\r\n> \r\n> https://github.com/scikit-hep/awkward-1.0/blob/main/src/libawkward/array/ListOffsetArray.cpp#L1892-L1908\r\n\r\n@agoose77 - thanks for spotting it! Here it is - `argsort` assumes that the offsets do not change: https://github.com/scikit-hep/awkward-1.0/blob/ffb4ec846d4d5ca26a0c337ce2662bc0670efbbe/src/libawkward/array/ListOffsetArray.cpp#L2045 \r\n",
  "created_at":"2021-06-30T14:45:22Z",
  "id":871468512,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTQ2ODUxMg==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-06-30T14:45:22Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@agoose77 - I just checked, it's the same for `sort`",
  "created_at":"2021-06-30T14:53:01Z",
  "id":871474790,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTQ3NDc5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T14:53:01Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks @ianna, I'll make a note to address these next week.",
  "created_at":"2021-06-30T14:55:24Z",
  "id":871476842,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTQ3Njg0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T14:55:24Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Thanks @ianna, I'll make a note to address these next week.\r\n\r\ngreatly appreciated! \r\n\r\nThe fix should be simple. The content is contiguous, so are the offsets. The following shift works, but a new kernel function is needed, I think:\r\n\r\n```diff \r\n(base) yana@iannas-macbook-pro-1 awkward-1.0 % git diff\r\ndiff --git a/src/libawkward/array/ListOffsetArray.cpp b/src/libawkward/array/ListOffsetArray.cpp\r\nindex 6d7adb2..ec56698 100644\r\n--- a/src/libawkward/array/ListOffsetArray.cpp\r\n+++ b/src/libawkward/array/ListOffsetArray.cpp\r\n@@ -1824,9 +1824,14 @@ namespace awkward {\r\n \r\n       outcontent = outcontent.get()->carry(outcarry, false);\r\n \r\n+      Index64 outoffsets(offsets_.length());\r\n+      for (int64_t i = 0; i < offsets_.length(); i++) {\r\n+        outoffsets.data()[i] = offsets_.data()[i] - offsets_.data()[0];\r\n+      }\r\n+\r\n       ContentPtr out = std::make_shared<ListOffsetArray64>(Identities::none(),\r\n                                                            parameters_,\r\n-                                                           offsets_,\r\n+                                                           outoffsets,\r\n                                                            outcontent);\r\n       return out;\r\n     }\r\n@@ -1856,9 +1861,14 @@ namespace awkward {\r\n         negaxis, util::make_starts(offsets_), nextparents,\r\n         offsets_.length() - 1, ascending, stable);\r\n \r\n+      Index64 outoffsets(offsets_.length());\r\n+      for (int64_t i = 0; i < offsets_.length(); i++) {\r\n+        outoffsets.data()[i] = offsets_.data()[i] - offsets_.data()[0];\r\n+      }\r\n+\r\n       ContentPtr out = std::make_shared<ListOffsetArray64>(Identities::none(),\r\n                                                            parameters_,\r\n-                                                           offsets_,\r\n+                                                           outoffsets,\r\n                                                            outcontent);\r\n       return out;\r\n     }\r\n``` \r\n```python\r\n>>> import awkward as ak\r\n>>> import numpy as np\r\n>>> layout = ak.layout.ListOffsetArray64(\r\n...     ak.layout.Index64([1, 4]),\r\n...     ak.layout.NumpyArray(np.arange(64)),\r\n... )\r\n>>> sarr = ak.sort(layout, highlevel=False)\r\n>>> sarr\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 3]\" offset=\"0\" length=\"2\" at=\"0x7ff1834ed110\"/></offsets>\r\n    <content><NumpyArray format=\"l\" shape=\"3\" data=\"1 2 3\" at=\"0x7ff18348f920\"/></content>\r\n</ListOffsetArray64>\r\n>>>\r\n```",
  "created_at":"2021-06-30T15:11:05Z",
  "id":871489488,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTQ4OTQ4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T15:11:05Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for the advice! Is it possibly a good idea to make use of the existing `toListOffsetArray64(true)` vs writing a new kernel function to normalise the offsets?",
  "created_at":"2021-06-30T15:43:39Z",
  "id":871515106,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTUxNTEwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T15:43:39Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Thanks for the advice! Is it possibly a good idea to make use of the existing `toListOffsetArray64(true)` vs writing a new kernel function to normalise the offsets?\r\n\r\nIt will not help because the resulting sorted content is a copy of the original array's and is trimmed.",
  "created_at":"2021-06-30T15:49:57Z",
  "id":871521082,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTUyMTA4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T15:49:57Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"ha, it's already there: \r\n```python\r\nIndex64 outoffsets = compact_offsets64(true);\r\n```",
  "created_at":"2021-06-30T15:58:13Z",
  "id":871527388,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTUyNzM4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T15:58:13Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> ha, it's already there:\r\n> \r\n> ```python\r\n> Index64 outoffsets = compact_offsets64(true);\r\n> ```\r\n\r\nHaha, well this PR just became considerably easier!",
  "created_at":"2021-06-30T15:59:41Z",
  "id":871528474,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTUyODQ3NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-06-30T15:59:41Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"thanks, @agoose77! fixed - piggyback on https://github.com/scikit-hep/awkward-1.0/pull/946 ",
  "created_at":"2021-06-30T16:09:45Z",
  "id":871536441,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTUzNjQ0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T16:09:45Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Great, thanks for fixing this Ianna.",
  "created_at":"2021-06-30T16:36:40Z",
  "id":871560018,
  "issue":967,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTU2MDAxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T16:36:40Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Tell me if this work-around works: do\r\n\r\n```python\r\nimport awkward._io\r\n```\r\n\r\nbefore `f[\"E/Evt/trks/trks.rec_stages\"].array()` in your example. If so, it's just that the module hasn't been explicitly imported. I guess the right place to do that (least potential for error) is in awkward/_connect/_uproot.py, so I'll do that before 1.4.0 goes out. I don't know what might have _changed_ here to make it work before and not now.\r\n\r\nAlso, this alternate code path isn't permanent. It was a first experiment that led to the development of AwkwardForth, and now AwkwardForth needs to be integrated into Uproot. Once that's done, this experimental back-door will be removed. From your perspective, nothing should change, including performance; I'm just giving you a heads-up.",
  "created_at":"2021-06-30T18:43:52Z",
  "id":871642649,
  "issue":968,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTY0MjY0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T18:43:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes, `import awkward._io` works around `:)`\r\n\r\nEdit: Thanks! Looking forward to `AwkwardForth`!",
  "created_at":"2021-06-30T19:42:20Z",
  "id":871677855,
  "issue":968,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTY3Nzg1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T19:43:11Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"In that case, I'll be sure to include #970 in 1.4.0 so that you won't need that work-around for long.",
  "created_at":"2021-06-30T20:17:12Z",
  "id":871699388,
  "issue":968,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTY5OTM4OA==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-06-30T20:17:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I think the fix for this should look like\r\n\r\n```python3\r\n    def is_below_axis(layout, depth, axis):\r\n        if axis >= 0:\r\n            # Depth starts at 1\r\n            return depth-1 > axis\r\n        else:\r\n            # We only consider ourselves to have passed the union depth \r\n            # if we cannot process any union members, i.e.\r\n            # if `abs(axis)` is farther than the farthest leaf\r\n            min_depth, max_depth = layout.minmax_depth\r\n            return max_depth + axis < 0\r\n        \r\n    def apply(layout, depth, posaxis):\r\n        if posaxis is not None:\r\n            # If a particular axis was given, do not proceed past that axis\r\n            if is_below_axis(layout, depth, posaxis):\r\n                return layout\r\n ```\r\n\r\nThis is fiddly stuff, does it look right to you? The only problem here is that we don't handle the case where the initial layout is not deep enough; `is_below_axis` (by design) returns `True` (and doesn't raise an `Exception`) for any axis ~ \u221e. I could add a check outside of the recursion that axis does not exceed the `max_depth` of the outer layout.\r\n\r\nAlternatively, we drop the `axis` parameter. I don't know whether this is a good idea - the fix doesn't seem complicated, and I think we might benefit from not over simplifying. But, perhaps simpler is better?",
  "created_at":"2021-06-30T17:55:21Z",
  "id":871612081,
  "issue":969,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTYxMjA4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T18:33:20Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"To explain the above modified code snippet, consider this example:\r\n```python3\r\nlayout = ak.layout.ListOffsetArray64(\r\n    ak.layout.Index64(np.r_[1, 3]),\r\n    ak.layout.UnionArray8_64(\r\n        ak.layout.Index8(np.r_[0, 1, 0, 1]),\r\n        ak.layout.Index64(np.r_[0, 0, 1, 1]),\r\n        [\r\n            ak.layout.ListOffsetArray64(\r\n                ak.layout.Index64(np.r_[1, 4, 8]), ak.layout.NumpyArray(np.arange(128))\r\n            ),\r\n            ak.layout.ListOffsetArray64(\r\n                ak.layout.Index64(np.r_[1, 3, 5]),\r\n                ak.layout.ListOffsetArray64(\r\n                    ak.layout.Index64(np.r_[1, 3, 6, 9, 12, 16, 18, 25]),\r\n                    ak.layout.NumpyArray(np.arange(64) - 32),\r\n                ),\r\n            ),\r\n        ],\r\n    ),\r\n)\r\n```\r\nFor `axis=2`, the effective depth is actually `depth=3`. When walking the tree, we wait until we hit that depth\r\n\r\n* `ListOffsetArray64(depth=1)`\r\n   * `UnionArray8_64(depth=2)` \r\n   * `ListOffsetArray64(depth=2)`\r\n      * `NumpyArray(depth=3)`\r\n   * `ListOffsetArray64(depth=2)`\r\n      * `ListOffsetArray64(depth=3)`\r\n         * ~~`NumpyArray(depth=4)`~~\r\n      \r\nand then stop processing. This means that the final `NumpyArray(depth=4)` is not processed.\r\nFor negative axes, the behaviour is different - we work backwards from the leaves. This means that for `axis=-2` we see:\r\n* `ListOffsetArray64(depth=1, neg_depth=?)`\r\n   * `UnionArray8_64(depth=2, neg_depth=?)` \r\n   * `ListOffsetArray64(depth=2, neg_depth=-2)`\r\n      * ~~`NumpyArray(depth=3, neg_depth=-1)`~~\r\n   * `ListOffsetArray64(depth=2, neg_depth=-3)`\r\n      * `ListOffsetArray64(depth=3, neg_depth=-2)`\r\n         * ~~`NumpyArray(depth=4, neg_depth=-1)`~~\r\n\r\ni.e. we don't process the leaves at `neg_depth=-1`.",
  "created_at":"2021-06-30T18:11:35Z",
  "id":871622507,
  "issue":969,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTYyMjUwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T18:12:44Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"(I'm thinking this through; chances are I'm saying stuff you already know or have already figured out...)\r\n\r\n`ak.packed` doesn't have an `axis` for simplicity\u2014because of what the function does, I don't think anyone would ever want to pack only one dimension. So this is only about the internal `_packed`, which is used by `ak.unflatten`.\r\n\r\nAn `axis` shouldn't be considered valid on a UnionArray unless it can be used in some way by every `content` in that union. Your examples have a list-type in the first `content` and a list-list-type in the second `content`, so the second should be able to take one more possible negative `axis` than the first. That is, if the `axis` of the function refers to a numeric level, the first could take `axis=-1` and `axis=-2` and the second could take `axis=-1`, `axis=-2`, and `axis=-3`. If the function's `axis` refers to a point between dimensions, like where to put the new counts in `ak.unflatten`, then it would be just `axis=-1` for the first and `axis=-1` or `axis=-2` for the second.\r\n\r\nThe `depth` counts up from the bottom, so `depth` is either equal to the negation of a negative `axis` or off by one (consistently). Since `minmax_depth` gives you both the minimum and maximum and the UnionArray has to work for _all_ `content` branches, I think it's the _minimum_ that you want to compare against a negated `axis`, not the maximum, as in your code example. When you ask for `minmax_depth` of a UnionArray, the minimum is the one that's more restrictive in what negative axes are allowed.\r\n\r\nI'll try drawing one of these diagrams, too:\r\n\r\n```\r\n                               min max depth   allowed axis?\r\n------------------------------------------------------------\r\nlist-type                      3   4           0 (only)\r\n\u2502\r\n\u2514\u2500\u2500 union-type                 2   3           none!\r\n    \u2502\r\n    \u251c\u2500\u2500 list-type              2   2           1, -2\r\n    \u2502   \u2514\u2500\u2500 numeric-type       1   1           2, -1\r\n    \u2502\r\n    \u2514\u2500\u2500 list-type              3   3           1, -3\r\n        \u2514\u2500\u2500 list-type          2   2           2, -2\r\n            \u2514\u2500\u2500 numeric-type   1   1           3, -1\r\n```\r\n\r\nI'm confident about the `minmax_depth` values because I computed them in a terminal. That's just what you get when you call the function. I'm uncertain about the \"allowed axis?\" column\u2014what I've written is my intuition about what should happen, but it could be different for different kinds of functions (some apply _at_ a dimension and others apply _above_ a dimension) and I couldn't find any examples of functions that work! UnionArrays break a lot of functionality. If there is some function that successfully passes an `axis` through a UnionArray, I don't know offhand which it is. RecordArrays and negative `axis` generally work.\r\n\r\nBut anyway, the \"allowed axis?\" column represents the theory: negative `axis` should count up from the bottom and as such, it can be different in different branches of a UnionArray or RecordArray (the only two layout types that can have more than one `content`). If it's ever ambiguous, it's not allowed, so a negative axis can't be applied above a split-point with different depths.\r\n\r\nI'm starting to remember\u2014it might have been an across-the-board decision to make negative `axis` never work through UnionArrays. Unions are different from records because in a RecordArray, every element of the array has all branches, but in a UnionArray, each array element has one of the possible branches. I think that the decision was that interpreting `axis` differently on an array element-by-array element basis was too subtle/not understandable enough to be a good policy. Maybe you shouldn't work so hard to get this one to accept negative `axis`.",
  "created_at":"2021-06-30T19:34:41Z",
  "id":871673400,
  "issue":969,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTY3MzQwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T19:34:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for the detailed reply, the diagram is much clearer than mine. You're correct that this *does not* matter for `ak.packed`, only `ak.unflatten` (because we choose to optimise the case).\r\n\r\nThis entire issue is really about the fact that neither positive nor negative axes are well defined for layouts with unions containing differing numbers of list type layouts. I made a decision in the original implementation & this issue. In my mind, there are really two separate problems:\r\n\r\n## What negative axes \"mean\" (in general)\r\nI wasn't aware that we deliberately disallow ambiguous allowed axes. This makes sense, and changes the interpretation of `minmax`. I was using `max_depth` because we are using negative depths whilst traversing the tree from the root. In this way, I interpret the meaning of negative axes for `_packed` to be \r\n\r\n> pack each layout until we reach the given axis, and then stop. \r\n\r\nWith this interpretation, we expect any layout which contains some `content` that has sufficient depth (`max_depth`) to be visited, which is why I use `max_depth` in the condition.\r\n\r\nThe question is whether this interpretation is valid. Effectively, unions with (`min != max`) contents require us to choose whether out-of-bounds axes (e.g. an `axis` which is too large for one union content, but is valid for another, or an `axis` which is too negative for one content, but not the other) are legal or not. In the case of `_packed`, I think this is an oversight of mine - I arbitrarily chose negative depths to be interpreted in terms of the deepest branch, but one could equally take the minimum. \r\nPerhaps it is better to consider only the minimum, in the same way that one treats the positive case (as you suggest).\r\n\r\n## How `_packed` should be fixed\r\n`_packed` takes an axis so that it can prepare an array for `unflatten`. This function is different to many of the other Awkward functions that take axes, because it operates *above* the given axis, which makes the above ambiguity a problem. \r\n\r\nWhat we *want* to happen is that each layout is packed as it is visited on our way to unflattening at a given depth. Then we defer the meaning of `axis` to that which is defined for `unflatten`. I think this is the right solution. Either we move the recursion from `_packed` into `packed`, and have `_packed` perform a single packing operation, or we replace `axis` with `depth`, which defines the recursion depth. I think the latter is actually going to be simpler than having to implement the recursion in `packed`.",
  "created_at":"2021-06-30T21:00:57Z",
  "id":871724183,
  "issue":969,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTcyNDE4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T21:06:32Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"What you propose sounds like the right solution, to the extent that you need to make `ak.unflatten` work not just for data with unions, but with different list-depth unions. (\"Sounds like\" to the degree that I've understood it.)\r\n\r\nIf you don't have an explicit need for this case and if the solution would be hard to understand even when it works, then it may not be worthwhile. If you do decide that it's not worthwhile, at least it should raise a good error message.",
  "created_at":"2021-06-30T21:07:12Z",
  "id":871728008,
  "issue":969,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTcyODAwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T21:07:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think we're on the same page. Having taken a step back, unpacking above a depth really only makes sense for root-defined axes (aka depths) rather than allowing negative axes, so removing that feature isn't really a problem from a doing-it-the-right-way perspective.",
  "created_at":"2021-06-30T21:18:50Z",
  "id":871734926,
  "issue":969,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTczNDkyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T21:18:50Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ianna just running `python dev/generate-strategies.py` and then running `pytest hypothesis-tests-spec` will run it. I am working on producing more better values and adding other IndexedArray kernel functions to this script ",
  "created_at":"2021-07-01T21:55:41Z",
  "id":872574117,
  "issue":971,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MjU3NDExNw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-01T21:55:41Z",
  "user":"MDQ6VXNlcjUyNjM1Nzcz"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ianna please have a look at the input data in the json file (the error cases and the normal ones)",
  "created_at":"2021-07-02T21:03:56Z",
  "id":873255362,
  "issue":971,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MzI1NTM2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-02T21:03:56Z",
  "user":"MDQ6VXNlcjUyNjM1Nzcz"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Continuing work in #1044 ",
  "created_at":"2021-08-02T21:02:51Z",
  "id":891330468,
  "issue":971,
  "node_id":"IC_kwDODBCWws41IJ-k",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-02T21:02:51Z",
  "user":"MDQ6VXNlcjUyNjM1Nzcz"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Yes, that's all it needed. I'll try to get this into 1.4.0, too.\r\n\r\nOK. I've modified the test to actually check the result of `ak.mask`, but it still passes.",
  "created_at":"2021-07-01T21:36:20Z",
  "id":872566188,
  "issue":976,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MjU2NjE4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-01T21:36:20Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - the last commit was a minor cleanup, so I assume the tests will pass. Please, have a look when you have time. Thanks!",
  "created_at":"2021-08-03T16:03:58Z",
  "id":891970056,
  "issue":977,
  "node_id":"IC_kwDODBCWws41KmII",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-08-03T16:03:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I ran into this in the same scenario as the one that you describe. I'm still quite new to the space, but I wonder if what we need here is some kind of `coerce_to_form` function that can transform one Awkward Array to another with the given form, e.g.\r\n```python3\r\nform = ...\r\nfor batch in batch_generator():\r\n    as_form = ak.coerce_to_form(batch, form)\r\n    ak.to_parquet(as_form, ...)\r\n```\r\nFor auto-generated forms this could be\r\n```python3\r\nfrom itertools import chain\r\n\r\nbatches = iter(batch_generator())\r\npacked = (ak.packed(b) for b in batches)\r\n\r\nfirst = next(packed)\r\nform = first.layout.form\r\n\r\nfor batch in chain([first], packed):\r\n    as_form = ak.coerce_to_form(batch, form)\r\n    ak.to_parquet(as_form, ...)\r\n```\r\nThis would have benefits in other areas such as partitioned arrays. ",
  "created_at":"2021-07-06T16:56:10Z",
  "id":874926297,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NDkyNjI5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T17:53:22Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"yes I agree that type of coercion would be perfect!",
  "created_at":"2021-07-06T16:57:33Z",
  "id":874927192,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NDkyNzE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T16:57:33Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"MEMBER",
  "body":"There is type information associated with Arrow and Parquet, such as \"this is an array of nullable lists.\" That type information is at the same level of abstraction as Awkward types, so the above would be something like `1000 * option[var * float32]`. The high-level type information doesn't say whether the nullability is implemented by a BitMaskedArray or an UnmaskedArray, but if the type says \"nullable\" and the mask is None, then it becomes an UnmaskedArray to provide that option-type in Awkward without creating a mask.\r\n\r\nIf your application relies on high-level types, then one Parquet file with nullable type and an existing mask and another Parquet file with the same schema and a missing mask should be viewed as the same high-level type and everything works the same with both files. However:\r\n\r\n   * If the Parquet file without a mask also has a non-nullable type, then they'll come out in Awkward Array as different types. In this case, the Parquet files are heterogeneous. The producer of these files (maybe `ak.to_parquet`?) ought to produce homogeneous files\u2014all with the same schema.\r\n   * If there's a bug in `ak.from_arrow` or `ak.from_parquet` that is ignoring the type specification, translating arrays with no mask as non-option-type (neither BitMaskedArray nor UnmaskedArray), then this is a problem that should get fixed, not hidden by a coersion function.\r\n   * If your application relies on low-level Forms, rather than high-level Types, then you'll need a coersion function. You'll want to pick the right file to sample for the Form, to get one that's general enough. Or maybe there should be another function that produces the most general _Arrow_ Forms from Types. For instance, given an OptionType, the most general _Arrow_ Form is a BitMaskedForm, but there are non-Arrow/Parquet contexts in which IndexedOptionForm would be more general. The Form you want to generate for reading Arrow/Parquet would have all UnmaskedForms expanded to BitMaskedForms, but that functionality is specific to Arrow/Parquet sources.\r\n\r\nThere are applications that would require exact Forms to be matching. Numba, for instance, compiles specialized code for each Form\u2014an abstract Type is not enough information to generate low-level IR. Lazy arrays would also require it, since VirtualArray checks to see if the generated array matches its expected Form (if the VirtualArray was given an expected Form). The agreement must be at the level of Forms so that lazy arrays can be used in Numba.\r\n\r\nImplementing `ak.coerce_to_form` for VirtualArrays in such a way that they don't immediately materialize the VirtualArray would be tricky. We have all the pieces we need: the new Form and a function that would convert the array, so the VirtualArray's Form gets replaced by the new Form and its generator function would get chained with the function that converts the array (i.e. new generator is `lambda: convert(old_generator())`). But VirtualArrays can generate arrays containing VirtualArrays, so the translation would have to be lazily applied recursively. (The reason we want recursive lazy arrays is to read Parquet files with nested StructArrays or StructArrays in Arrow Tables. We don't want access of an outer field to cause all inner fields to be read.)\r\n\r\nGiven that `ak.coerce_to_form` is tricky for VirtualArrays and expanding to the most general Form for Arrow/Parquet is specific to Arrow/Parquet, maybe all that's needed for _this_ application is to have a flag in `ak.from_arrow` and `ak.from_parquet` to expand all data to the most general Form? For Arrow/Parquet, that means\r\n\r\n   * no UnmaskedArrays; always create `np.zeros(int(np.ceil(np.log2(length))), np.uint8)` whenever a mask is not provided but the type is nullable;\r\n   * all 32-bit nodes (e.g. Arrow ListArray) should be converted into 64-bit nodes (Arrow LargeListArray) to allow for one of them to be 64-bit.\r\n\r\nI can't think of any other Type-preserving differences that Arrow or Parquet could have. The advantage of making it a boolean flag for `ak.from_arrow` and `ak.from_parquet`, other than the fact that the choice of Form would be Arrow/Parquet-specific anyway, is that this integrates it into the code that makes all of the (recursive) VirtualArray generators. Adding to this code that has already solved the recursive-laziness problem would be easier than writing new code that would have to solve it again.",
  "created_at":"2021-07-06T17:55:35Z",
  "id":874965678,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NDk2NTY3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T17:55:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I wonder whether we only need to solve the writing component of the problem. As long as writing uses a common schema, then upon reading these arrays will share a common schema. At least initially, existing incompatible arrays can be re-written to apply a common schema. Therefore, if we were to define some kind of coercion function, it would be \"fine\" to materialise virtual arrays in the same way that we do for `unflatten` and other routines.\r\n\r\n I ran into this \"issue\" in the writing stage, when I was re-partitioning data with slightly different forms. There are some additional things we might need to address here, such as in-determinism of Record field names.",
  "created_at":"2021-07-06T18:07:56Z",
  "id":874973990,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NDk3Mzk5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T18:10:04Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"our use-case is converting a large dataset of O(10k) files O(100TB) from ROOT to parquet. It would be great to really ensurre that all files have the same low-level type as otherwise we'd be baking in the file-boundaries which we perhaps do not want to preserve (perhaps we want to merge parquet files later, etc)\r\n\r\nI think the problem of finding the right low-level type could be factorized. Often we do know what that type should be from other considerations",
  "created_at":"2021-07-06T18:23:01Z",
  "id":874983829,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NDk4MzgyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T18:23:01Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"MEMBER",
  "body":"`pyarrow.parquet.ParquetFileWriter` won't want to give you the same low-level type in each file\u2014what I'm calling its Form. It will want to optimize things by not writing bit-masks when zero entries in a row group (or even page?) have missing values in practice, and it could be hard to swim against that tide. However, it should give the same high-level type\u2014what I'm calling its Type (capital T, [this class](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1Type.html)) and Parquet and Arrow call schemas (two different schema languages, but equivalent to each other). As producer of the files, you should ensure that they have the same high-level schemas, but don't worry about the low-level questions of whether a mask is actually instantiated; let pyarrow optimize. As a consumer of the files, Awkward Array must map identical schemas to identical high-level types. That's how it ought to work, and any failure to do so is either a bug or a not-yet-implemented case.\r\n\r\nBut if a process downstream of your Parquet \u2192 Awkward reader additionally needs the low-level Forms to be the same, then that's something we should either have a special `ak.coerce_to_form` function for or a flag in `ak.from_arrow`/`ak.from_parquet` to expand all low-level data to fit the most general Form for the given Type/schema. Since all of your files have the same schema, such a flag would make them have the same Form, despite pyarrow's file optimizations. (We can \"puff up\" the in-memory format from \"slimmed down\" files.) I favor the `ak.from_arrow`/`ak.from_parquet` flag approach because it would be so much easier to get it right.\r\n\r\nTools that repartition a collection of Parquet files with the same schema know how to adjust for low-level differences in optimization. Things like including or excluding masks are only the tip of the iceberg for libparquet: it mixes all sorts of optimizations, like run-length encodings, variable-width encodings, and odd-bitlength encodings in seemingly random domains (the file-writer decides to run-length encode 5 values, variable-width the next 14, encode the next 8 with 5 bits each, etc.). Since libparquet assumes that all of these fine-granularity optimizations are going to be present anyway, it shouldn't have any problem with adjusting which parts need to get a mask and which don't. (It also needs to rewrite the page minimum/page maximum metadata, so the whole can of worms is open anyway.) That's why converting them into Awkward Arrays will (without a flag to say otherwise) sometimes produce BitMaskedArray nodes and sometimes UnmaskedArray nodes, but these represent the same high-level Type, which should be one-to-one with the Parquet schema.",
  "created_at":"2021-07-06T19:06:08Z",
  "id":875011475,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTAxMTQ3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T19:06:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks @jpivarski - so it seems like for our usecase having a `ak.coerce_to_type` would be suffficient then and we won't need a  `ak.coerce_to_form`. Perhaps @nikoladze can confirm. \r\nOr are you saying a `ak.coerce_to_type`  would not even be necessary?",
  "created_at":"2021-07-06T19:33:46Z",
  "id":875027943,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTAyNzk0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T19:33:46Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm saying that a \"coerce to Type\" _should_ not be necessary, if there are no bugs or missing features in Awkward. There _should_ be a one-to-one relationship between Parquet schemas and Awkward Types, if nothing is broken. Some partitions/row-groups/pages having masks while others don't is not a difference in Parquet schemas and it should not be a difference in Awkward Types because BitMaskedArray vs UnmaskedArray is not a difference in Types (just Forms).\r\n\r\nThe second question that @nikoladze can confirm is whether your application needs all Forms to be the same or if same Types is sufficient. If you need all the Forms to be the same, that would require new functionality, such as the `same_form=True` flag in `ak.from_arrow`/`ak.from_parquet`. (It would be worthwhile even if your application doesn't need it, since lazy arrays in Numba would need constant Forms and this has never been addressed.)\r\n\r\nIf this seems too academic (the language is reminding me of a philosophy book I've been reading recently), the reason there's a distinction between Types and Forms is because Types are all that a data analyst would usually care about: \"could these values be missing or is that impossible?\" but Forms describe how arrays are laid out in memory: \"are they masked out with bits or bytes? Negative values in an indexed array? Or are they formally nullable but in this particular array, none are actually missing?\" Numba needs the latter if it compiles a function that will be reaching into memory addresses, expecting to find the right kind of data, and the arrays with those memory addresses will be created after compilation is finished. Normal processing of same-Type, different-Form data runs different code for each Form, but each code path does \"the same thing\" as far as a data analyst is concerned (identifying missing values or whatever).",
  "created_at":"2021-07-06T20:10:46Z",
  "id":875049151,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTA0OTE1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T20:10:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"```python\r\n>>> numpyarray = ak.layout.NumpyArray(np.array([1.1, 2.2, 3.3, 4.4, 5.5]))\r\n>>> one = ak.Array(\r\n...     ak.layout.BitMaskedArray(\r\n...         ak.layout.IndexU8(np.array([0], np.uint8)),\r\n...         numpyarray,\r\n...         valid_when=False,\r\n...         length=5,\r\n...         lsb_order=True,\r\n...     )\r\n... )\r\n>>> two = ak.Array(\r\n...     ak.layout.UnmaskedArray(\r\n...         numpyarray\r\n...     )\r\n... )\r\n>>> one\r\n<Array [1.1, 2.2, 3.3, 4.4, 5.5] type='5 * ?float64'>\r\n>>> two\r\n<Array [1.1, 2.2, 3.3, 4.4, 5.5] type='5 * ?float64'>\r\n>>> ak.is_none(one)\r\n<Array [False, False, False, False, False] type='5 * bool'>\r\n>>> ak.is_none(two)\r\n<Array [False, False, False, False, False] type='5 * bool'>\r\n>>> one.type\r\n5 * ?float64\r\n>>> two.type\r\n5 * ?float64\r\n>>> one.layout.form\r\n{\r\n    \"class\": \"BitMaskedArray\",\r\n    \"mask\": \"u8\",\r\n    \"content\": \"float64\",\r\n    \"valid_when\": false,\r\n    \"lsb_order\": true\r\n}\r\n>>> two.layout.form\r\n{\r\n    \"class\": \"UnmaskedArray\",\r\n    \"content\": \"float64\"\r\n}\r\n```\r\n\r\n`one` and `two` have the same Type but different Forms. When we compute `ak.is_none`, it invokes a different code path for the BitMaskedArray (unpack those bits into bytes to make an array of booleans) as UnmaskedArray (generate an array of all False values), but data analysts shouldn't care which one is happening, since `one` and `two` represent the same data with the same Type.\r\n\r\nIf fully materialized data for `one` and `two` were passed one at a time into a Numba JIT-compiled function, Numba would compile two specializations of the function, though a user wouldn't need to be aware of this. ~~If a VirtualArray that produces `one` is used to compile the function and then a VirtualArray that produces `two` is used in it,~~ **Nevermind:** I thought that would be a problem, but Numba would still compile two specializations of the function since those two VirtualArrays would have different Forms.\r\n\r\nSo what could matter if the Forms are different among the partitions of a dataset? I can't think of anything\u2014the one example I thought would matter doesn't, in fact, matter.",
  "created_at":"2021-07-06T20:21:55Z",
  "id":875056052,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTA1NjA1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T20:21:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I think I'm more concerned about this setting\r\n\r\n```\r\nIn [18]: a = ak.Array([{'jets': [{'eta': 1.0, 'phi': 2.0}], 'electrons': [{'e': 1.23, 'pid': 11}]}])\r\n\r\nIn [19]: b = ak.Array([{'jets': [{'phi': 1.0, 'eta': 2.0}], 'electrons': []}])\r\n\r\nIn [20]: ak.type(a)\r\nOut[20]: 1 * {\"jets\": var * {\"eta\": float64, \"phi\": float64}, \"electrons\": var * {\"e\": float64, \"pid\": int64}}\r\n\r\nIn [21]: ak.type(b)\r\nOut[21]: 1 * {\"jets\": var * {\"phi\": float64, \"eta\": float64}, \"electrons\": var * unknown}\r\n```\r\n\r\nthe two arrays would both fit into  the type/schema `N * {\"jets\": var * {\"eta\": float64, \"phi\": float64}, \"electrons\": var * {\"e\": float64, \"pid\": int64}}` but the arrow schemas are different\r\n\r\n```\r\nIn [29]: ak.to_arrow(a).type\r\nOut[29]: StructType(struct<jets: large_list<item: struct<eta: double not null, phi: double not null> not null> not null, electrons: large_list<item: struct<e: double not null, pid: int64 not null> not null> not null>)\r\n\r\nIn [30]: ak.to_arrow(b).type\r\nOut[30]: StructType(struct<jets: large_list<item: struct<phi: double not null, eta: double not null> not null> not null, electrons: large_list<item: double not null> not null>)\r\n```\r\n\r\nbeing able to normalize the two is I think what we're after\r\n\r\n",
  "created_at":"2021-07-06T20:39:48Z",
  "id":875066302,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTA2NjMwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T20:42:31Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"MEMBER",
  "body":"Your `a` and `b` have different Types: \"unknown\" is not the same thing as those records. Those are different Parquet schemas because they're different Types. But `ak.from_iter` applied to untyped Python data is not expected to give the same Types.\r\n\r\n```python\r\n>>> a = ak.Array([{'jets': [{'eta': 1.0, 'phi': 2.0}], 'electrons': [{'e': 1.23, 'pid': 11}]}])\r\n>>> b = ak.Array([{'jets': [{'phi': 1.0, 'eta': 2.0}], 'electrons': []}])\r\n>>> a.type == b.type\r\nFalse\r\n```\r\n\r\nIf your data are coming from ROOT/Uproot, rather than `ak.from_iter`, then they'll have the same Types because the Types are derived from the C++ types. Even if a whole file has no electrons, Uproot will assign a record Type to the empty data. (It will be represented by a length-0 RecordArray, not an EmptyArray.)\r\n\r\n----------------------------------\r\n\r\nHowever, @agoose77 found a counterexample that we've been looking at in Gitter: https://gitter.im/Scikit-HEP/awkward-array\r\n\r\nThe `ak.to_buffers` function complains if the partitions of a PartitionedArray have different Forms, but they should be allowed to have different Forms if they have the same Type.\r\n\r\n```python\r\n>>> onetwo = ak.Array(ak.partition.IrregularlyPartitionedArray([one.layout, two.layout]))\r\n>>> onetwo\r\n<Array [1.1, 2.2, 3.3, 4.4, ... 3.3, 4.4, 5.5] type='10 * ?float64'>\r\n>>> pickle.dumps(onetwo)\r\nValueError: the Form of partition 1:\r\n\r\n    {\r\n    \"class\": \"UnmaskedArray\",\r\n    \"content\": {\r\n        \"class\": \"NumpyArray\",\r\n        \"itemsize\": 8,\r\n        \"format\": \"d\",\r\n        \"primitive\": \"float64\",\r\n        \"form_key\": \"node1\"\r\n    },\r\n    \"form_key\": \"node0\"\r\n}\r\n\r\ndiffers from the first Form:\r\n\r\n    {\r\n    \"class\": \"BitMaskedArray\",\r\n    \"mask\": \"u8\",\r\n    \"content\": {\r\n        \"class\": \"NumpyArray\",\r\n        \"itemsize\": 8,\r\n        \"format\": \"d\",\r\n        \"primitive\": \"float64\",\r\n        \"form_key\": \"node1\"\r\n    },\r\n    \"valid_when\": false,\r\n    \"lsb_order\": true,\r\n    \"form_key\": \"node0\"\r\n}\r\n```\r\n",
  "created_at":"2021-07-06T20:49:52Z",
  "id":875071735,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTA3MTczNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T20:53:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"right, but say I have 1000 events of type `{\"jets\": var * {\"eta\": float64, \"phi\": float64}, \"electrons\": var * {\"e\": float64, \"pid\": int64}}`\r\n\r\nsome of those events could be like my `b` (i.e. events without electrons, just by chance)..  If I *know* what electrons look like, shouldn't I be able to change `unknown` to `{\"e\": float64, \"pid\": int64}`\r\n\r\nit seems like we somehow implicitly rely on files being large enough to \"saturate\" the type, i.e. to eventually encounter a event with electrons ",
  "created_at":"2021-07-06T20:52:08Z",
  "id":875073115,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTA3MzExNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T20:54:54Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"MEMBER",
  "body":"Uproot derives the Awkward Type from the C++ type. (I hope it's not \"losing\" this information along the way.)\r\n\r\nIf you're getting data from iterating over Python objects, this will be a performance bottleneck that you should know about. But along these lines, we have a LayoutBuilder as an alternative to the ArrayBuilder that `ak.from_iter` uses that takes an explicit Form and only allows you to build arrays compatible with that Form.\r\n\r\nHaving a way to coerce the Type (as in the case of \"unknowns\") or Form (as in the case of BitMaskedArray vs UnmaskedArray) would also have good applications. They'd be generalizations of [ak.values_astype](https://awkward-array.readthedocs.io/en/latest/_auto/ak.values_astype.html).",
  "created_at":"2021-07-06T20:59:04Z",
  "id":875077184,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTA3NzE4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-06T20:59:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I should probably summarize here what i tried to do and what the obstacles were that i found. The plan was to merge multiple input ROOT files from ATLAS DAOD_PHYSLITE into a single parquet file. To do that i essentially copied over the code from `ak.to_parquet` and modified it such that it doesn't close the `ParquetWriter`, but instead takes a writer as an argument to append to an open parquet file. That works fine as long as the schema of the chunks is the same.\r\n\r\nI think i actually did not have problems where i had the same type, but a different form/schema. It's just that i need to fill in missing fields and then the second problem was that the ordering of fields matters (the record fields in the schema are numbered).\r\n\r\nThe way these ROOT files are written, it can happen e.g. that if there was no single Electron fulfilling the criteria of the `AnalysisElectrons` collection then the corresponding branches won't be written at all. Here the most natural thing would be to take the form of that collection from another file and just make empty lists (that is, i think, also what our custom merging routines for these ROOT files do). I could do that maybe with `ak.from_buffers` or manual layout building, using an offset array filled with 0s and lot's of empty numpy arrays at the leaves.\r\nSo i guess what we would need are convenience functions to do these things, e.g. if i have one array with Electrons and another one without i would like to get a merged array where the second have has only empty lists of Electrons according to the form of the first array.\r\n\r\nThe second problem (ordering of Record fields matters) is probably easier to solve, currently i just sort the record fields by name and call `ak.zip` with an `OrderedDict`.",
  "created_at":"2021-07-07T11:33:46Z",
  "id":875528980,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTUyODk4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T11:34:15Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The key ordering issue I mentioned above should be distinct from anything related to parquet; it follows from the ordering in the JSON form not being guaranteed. Python 3.7+ does guarantee insertion ordering of dicts, so you may well not need `OrderedDict` there (if you target modern Python).",
  "created_at":"2021-07-07T11:43:01Z",
  "id":875534223,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTUzNDIyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T11:43:01Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Python 3.7+ does guarantee insertion ordering of dicts, so you may well not need `OrderedDict` there (if you target modern Python).\r\n\r\ngood to know, thanks!",
  "created_at":"2021-07-07T15:26:13Z",
  "id":875700229,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTcwMDIyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T15:26:13Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"3.6+, actually. This looms large for me because it's often the reason that the Python 2.7 and 3.5 tests fail.",
  "created_at":"2021-07-07T15:28:54Z",
  "id":875702353,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTcwMjM1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T15:28:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think the ordering was only guaranteed in 3.7 and above, although that would mean that it's still there in 3.6!\r\n",
  "created_at":"2021-07-07T16:45:54Z",
  "id":875761195,
  "issue":981,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTc2MTE5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T17:11:40Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi @fleble, thanks for filing a report!\r\n\r\nFrom reading the documentation, I would expect that for the unmasked case, you should see the same result as the masked case.  Semantically, the empty lists are still `lists`, and so the dimension exists for the reduction. So, it doesn't look like the intended behaviour to me. It appears as though empty arrays are behaving like they are missing entirely. I don't anticipate this behaviour, but perhaps my expectations are wrong. \r\n\r\nIf this is a bug, then it might be an issue with the kernel for the output offset calculations e.g. https://github.com/scikit-hep/awkward-1.0/blob/a70a535a818d54c6dcdb60711dff09a283441541/src/cpu-kernels/awkward_ListOffsetArray_reduce_nonlocal_outstartsstops_64.cpp but I don't know enough about this layer to be able to confidently say anything. @ianna or @jpivarski will know whether this is intended behaviour.",
  "created_at":"2021-07-07T10:29:08Z",
  "id":875489344,
  "issue":982,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTQ4OTM0NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-07T10:29:48Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@fleble - thanks for reporting this! I'd expect the latter output, because when `axis=0`, the following array `min`\r\n```\r\n[\r\n [\r\n  [1, 2, 3], [], [4, 3, 2]\r\n ],\r\n [ \r\n  [4, 5, 6], [], [2, 3, 4]\r\n ]\r\n]\r\n```\r\nI think, it should be:\r\n```\r\n  [1, 2, 3], [], [2, 3, 2]\r\n```",
  "created_at":"2021-07-07T16:20:21Z",
  "id":875742536,
  "issue":982,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTc0MjUzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T16:20:21Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I think you're right. I think it should _not_ be moving that empty list.\r\n\r\nHere's a similar example in NumPy:\r\n\r\n```python\r\n>>> np_array = np.array([\r\n...     [[1, 2, 3], [9, 9, 9], [4, 3, 2]],\r\n...     [[4, 5, 6], [9, 9, 9], [2, 3, 4]]\r\n... ])\r\n>>> np.min(np_array, axis=0)\r\narray([[1, 2, 3],\r\n       [9, 9, 9],\r\n       [2, 3, 2]])\r\n>>> np_array = np.array([\r\n...     [[1, 2, 3], [0, 0, 0], [4, 3, 2]],\r\n...     [[4, 5, 6], [0, 0, 0], [2, 3, 4]]\r\n... ])\r\n>>> np.min(np_array, axis=0)\r\narray([[1, 2, 3],\r\n       [0, 0, 0],\r\n       [2, 3, 2]])\r\n```\r\n\r\nAnd here's an example where the list is not empty, but has one element:\r\n\r\n```python\r\n>>> ak_array = ak.Array([\r\n...     [[1, 2, 3], [9], [4, 3, 2]],\r\n...     [[4, 5, 6], [9], [2, 3, 4]]\r\n... ])\r\n>>> ak.min(ak_array, axis=0)\r\n<Array [[1, 2, 3], [9], [2, 3, 2]] type='3 * var * ?int64'>\r\n```\r\n\r\nActually, that last one clinches it: the length-0 list should _not_ be moving, as a length-1 list does not.",
  "created_at":"2021-07-07T16:43:15Z",
  "id":875759339,
  "issue":982,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTc1OTMzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T16:43:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for the fast replies! Glad to help by reporting this bug and thank you for the development of this package!\r\nI guess you will be posting here when the bug is fixed?",
  "created_at":"2021-07-08T09:01:20Z",
  "id":876264710,
  "issue":982,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NjI2NDcxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-08T09:01:20Z",
  "user":"MDQ6VXNlcjY5OTA1MDM1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Thank you for the fast replies! Glad to help by reporting this bug and thank you for the development of this package!\r\n> I guess you will be posting here when the bug is fixed?\r\n\r\nYes, this issue will be linked to a PR addressing it. Thanks again!",
  "created_at":"2021-07-08T09:05:03Z",
  "id":876267506,
  "issue":982,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NjI2NzUwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-08T09:05:03Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"This was super-tricky and got at the heart of how nonlocal reducers (i.e. `ak.min` or any other reducer with `axis != -1`) work. It slipped through the previous tests because the empty lists have to be aligned with one another\u2014usually less symmetric tests are better, but this case is only triggered if all of the empty inner lists are combined into a resultant empty list. My old tests were so asymmetric that the empty lists never aligned like this, and a quantity that I needed to put the empty lists in the right place in the output is computed from the maximum of an old case that I had handled and this one, so it only becomes relevant when one exceeds the other. I wasn't expecting this rabbit hole to lead so deep!\r\n\r\nShould be fixed now, though.",
  "created_at":"2021-07-14T01:09:02Z",
  "id":879505188,
  "issue":982,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTUwNTE4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-14T01:09:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Related discussion: https://github.com/scikit-hep/awkward-1.0/discussions/710",
  "created_at":"2021-07-07T20:41:55Z",
  "id":875918430,
  "issue":983,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTkxODQzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T20:41:55Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"ak.firsts and ak.singletons aren't fully thought through, at least for their action on more than one dimension. Their behavior can be changed if it's going to make them more consistent with other functions.\r\n\r\nYou've been their biggest proponent (I wanted to remove them some time ago, but you convinced me otherwise). I'd say you're free to refine their definitions for ndim > 1.",
  "created_at":"2021-07-07T22:58:12Z",
  "id":875987507,
  "issue":983,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTk4NzUwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T22:58:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@nsmith- are you advocating for `keepdims=True` to produce empty lists instead of `[None]` with a **reducer**? If so, such a change would be a breaking one, unless we add *another* parameter to the reducers. Of course, that doesn't answer the question of whether it should be done!\r\n\r\nI can *clearly* see why the existing behaviour for `keepdims=True` is perhaps non-ideal. Given that, I considered the following possible behaviours:\r\n\r\n* `[None]` -  introduces an option type *and* increases the length of the empty list.\r\n* `[]` - does not introduce an option type, does not increase the length of the empty list.\r\n* `None` - introduces an option type, entirely masks the empty list\r\n\r\nDeciding between these options is not trivial (in my opinion), and I've reconsidered my position several times in the course of writing this. \r\n\r\n**Disclaimer** I've been thinking about this for probably too long of an evening, so read at your peril.\r\n\r\nFor irregular arrays, it seems that we need to understand our expectations of a reducer, namely\r\n* should a reducer always produce fixed-size lists in the reduction axis (for `keepdims=True`)?\r\n\r\nOne example of where the answer is no is for Awkward advanced indexing + `argmin/max`; we need to keep the dimensionality of the lists in order for the lookup to work. For this purpose, returning `[None]` is not ideal; semantically we want to ignore empty lists in the index, whereas `[None]` includes them (albeit as `None`) (as you demonstrate). \r\n\r\nNow, to choose between `[]` and `None`, I do not have a strong conclusion either way. By choosing `None` over `[]` there is some symmetry between `keepdims=False` (which necessarily introduces `None` empty sublists) and `keepdims=True`. But this *isn't* a compelling case. \r\n\r\nI'm sure @jpivarski has some more concrete ideas about the *best* behaviour here, because my mind is exploding trying to keep up with all of the possible use cases / scenarios.",
  "created_at":"2021-07-08T21:19:14Z",
  "id":876752629,
  "issue":983,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3Njc1MjYyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-08T21:29:38Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> mind is exploding\r\n\r\nThis happens a lot. I go by intuition but its not always right. One helpful exercise is to consider the regular case:\r\n```python\r\na = np.array([[3, 1, 2], [4, 5, 3], [1, 2, 3]])\r\n```\r\nIf I want to select the min element index of the inner list, probably the most straightforward is\r\n```python\r\na[[0, 1, 2], np.argmin(a, axis=1)]\r\n```\r\n(of course if we really were just interested in `a` then `a.min(axis=1)` would be enough, here we're assuming we want to look at some other value `b` with the same shape)\r\nFor awkward, that won't work\r\n```python\r\n>>> a[[0, 1, 2], ak.argmin(a, axis=1)]\r\nValueError: cannot mix missing values in slice with NumPy-style advanced indexing\r\n```\r\nsince the argmin produces an optiontype `3 * ?int64`. We could remove that (e.g. `ak.fill_none(ak.argmin(a, axis=1), -1)`) and it would work. Probably for regular dimensions an explicit simplification of the optiontype should be done since the reducer will never return null. (I guess this is more of a bug than my original issue)\r\n\r\nBut anyway the same will not work for our original irregular array:\r\n```python\r\n>>> a = ak.Array([[3, 1, 2], [4, 5], []])\r\n>>> a[[0, 1, 2], ak.argmin(a, axis=1)]\r\nValueError: cannot mix missing values in slice with NumPy-style advanced indexing\r\n```\r\nso we have to provide some alternative means of indexing. The good news is we have awkward-style advanced indexing, and all three possibilities work:\r\n```python\r\n>>> a[ [[1], [0], [None]] ]\r\n<Array [[1], [4], [None]] type='3 * var * ?int64'>\r\n>>> a[ [[1], [0], []] ]\r\n<Array [[1], [4], []] type='3 * var * int64'>\r\n>>> a[ [[1], [0], None] ]\r\n<Array [[1], [4], None] type='3 * option[var * int64]'>\r\n```\r\ncorresponding to `ak.argmin(a, axis=1, keepdims=True)`, `ak.singletons(ak.argmin(a, axis=1))`, and some hypothetical alternative. (I guess `ak.singletons(ak.argmin(a, axis=1)).mask[ak.num(a)>0]`). My take is that option 2 is the best, partly because it has the simplest layout, but I don't have much of an argument beyond that.",
  "created_at":"2021-07-09T14:48:19Z",
  "id":877243261,
  "issue":983,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NzI0MzI2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-09T14:48:19Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"One argument against `[None]` is that it is the only one that produces masked values after flattening on axis 1:\r\n```\r\n>>> ak.flatten([[1], [4], [None]], axis=1)\r\n<Array [1, 4, None] type='3 * ?int64'>\r\n\r\n>>> ak.flatten([[1], [4], []], axis=1)\r\n<Array [1, 4] type='2 * int64'>\r\n\r\n>>> ak.flatten([[1], [4], None], axis=1)\r\n<Array [1, 4] type='2 * int64'>\r\n```",
  "created_at":"2021-07-09T15:24:02Z",
  "id":877266697,
  "issue":983,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NzI2NjY5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-09T15:24:02Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@nsmith- I'd say that's an argument in *favour* of `[None]`, because the structure remains compatible with the original array. Consider\r\n```python\r\n>>> ak.flatten([[1], [4], [None], [6]], axis=1)\r\n<Array [1, 4, None, 6] type='4 * ?int64'>\r\n>>> ak.flatten([[1], [4], [], [6]], axis=1)\r\n<Array [1, 4, 6] type='3 * int64'>\r\n```",
  "created_at":"2021-07-09T15:34:04Z",
  "id":877272951,
  "issue":983,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NzI3Mjk1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-09T15:34:04Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I disagree, flatten almost always alters the broadcast-compatibility of arrays. Meanwhile `ak.firsts` can maintain the structure (in both cases).",
  "created_at":"2021-07-09T15:37:07Z",
  "id":877274884,
  "issue":983,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NzI3NDg4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-09T15:37:07Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It should be - it just takes the code from `flatten` and removes the axis-dependent code paths. Hopefully this paves the way for tinkering with `flatten` down the line by changing `axis=None` to mean `axis=\"records\"`.",
  "created_at":"2021-07-15T14:54:42Z",
  "id":880762024,
  "issue":985,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MDc2MjAyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-15T14:54:42Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Possibly. I've been thinking that since I saw a form on another repo. Some of our issue templates have text for the user to read, which must be deleted when writing the actual issue. A form would separate that text from the text box where the user writes, and trust would be somewhat cleaner, probably easier to read.",
  "created_at":"2021-07-09T11:28:59Z",
  "id":877118917,
  "issue":986,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NzExODkxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-09T11:28:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It also allows you to add the triple tick marks and language highlighting or traceback highlighting automatically. :)",
  "created_at":"2021-07-10T15:51:47Z",
  "id":877659262,
  "issue":986,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NzY1OTI2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-11T13:47:41Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It also adds drop downs!",
  "created_at":"2021-07-11T13:47:58Z",
  "id":877803091,
  "issue":986,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NzgwMzA5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-11T13:47:58Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"The textbox with the `pytb` setting actually uses a monospace font, very nice attention to detail.",
  "created_at":"2021-07-12T02:59:28Z",
  "id":877931754,
  "issue":986,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NzkzMTc1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T02:59:28Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"OK, I've implemented equivalents of our existing issue forms using the GitHub Issue Forms schema. This is just a first draft!",
  "created_at":"2021-07-09T17:18:43Z",
  "id":877337235,
  "issue":987,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NzMzNzIzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-09T17:18:43Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Could I make the case for restoring some of the structured inputs, but making them optional? As a user that is comfortable with making bug reports, these templates will improve my workflow by pre-defining a structure and adding the formatting etc as required. Additionally, by having these fields, our GitHub Issues will have a more consistent style to them.\r\n\r\nI can see your point about users unfamiliar with creating Issues, but I wonder whether reducing the need for them to understand Markdown is a good thing here (by using rendered outputs)? So, in other words:\r\n\r\n- Awkward version\r\n- Description + any extra info (required)\r\n- Code to reproduce (required), python\r\n- Traceback (optional), pytb\r\n\r\nWould this be a better compromise?\r\n\r\nMerge as is if you remain unconvinced ;)",
  "created_at":"2021-07-12T18:24:31Z",
  "id":878496143,
  "issue":987,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODQ5NjE0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T18:24:31Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"(I've disabled auto-merge so that we can talk about this.)\r\n\r\nMaking them optional doesn't remove them from the screen, and I think the main issue is with a novice user **seeing something that looks complicated**. I think most people will be coming to this in the following context: \"I ran into something that doesn't work; I'm going to be a good citizen and report it before I get back to my real work.\" If they see a bewildering array of widgets, that's going to slow them down, even if most of those widgets don't have a red star.\r\n\r\nI can't emphasize enough how much of a step up these forms are in that they put our instructions to users outside of the text box where they write back to us. That's huge. The fact that the form also provides the ability to further structure the feedback would matter to big projects that get a lot of very similar requests, such as [PyPI's size limit requests](https://github.com/pypa/pypi-support/issues/new?assignees=&labels=limit+request&template=limit-request.yml&title=Limit+Request%3A+PROJECT_NAME+-+000+MB). For something like that, you really want \"Project URL?\" \"Which limit?\" \"How big?\", etc. to be in separate boxes because there are enough of them and they're routine enough that you'd want to automate some parts of that procedure. However, bugs in Awkward Array are idiosyncratic; each issue is addressed manually.\r\n\r\nWith separate boxes for \"Code to reproduce?\"  and \"Traceback?\", people would have a hard time explaining problems that involve more than one code block, e.g. \"If I do _this_, I get _that_, but if I do _this other thing_, I get _that other thing_. It would also make it hard to explain issues that are not error messages but wrong answers (or suspected wrong answers). For an issue-filter who wants to get back to the physics project they were working on, having to decide what to do with a square peg and a round hole is extra hassle.",
  "created_at":"2021-07-12T18:49:00Z",
  "id":878511262,
  "issue":987,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODUxMTI2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T18:49:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I can't really argue against your concern for adding too much structure. It's a shame that we can't mark the template as optional. \r\n\r\nI haven't changed my mind on the benefits of having a standard template, but we can always change it down the line as this isn't an API that we have to maintain :) let's merge your changes and revisit this again in future if needs be! \r\n",
  "created_at":"2021-07-12T18:59:56Z",
  "id":878517793,
  "issue":987,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODUxNzc5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T18:59:56Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, sounds good. Let's start with this, as we can always change it, adding more structure, in the future.",
  "created_at":"2021-07-12T19:01:03Z",
  "id":878518464,
  "issue":987,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODUxODQ2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T19:01:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @matthewfeickert for maintenance",
  "created_at":"2021-09-09T16:26:08Z",
  "id":916251752,
  "issue":988,
  "node_id":"IC_kwDODBCWws42nORo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T16:26:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/1086) to add @matthewfeickert! :tada:",
  "created_at":"2021-09-09T16:26:17Z",
  "id":916251867,
  "issue":988,
  "node_id":"IC_kwDODBCWws42nOTb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T16:26:17Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"Since `ak.ravel` exists, we can go ahead with changing the `ak.flatten` with `axis=None` behavior to preserve records. For reasons described below, we can go straight to `axis=None`, without the intermediary `axis=\"records\"`. (That would represent two deprecations, one to warn about `axis=None` and the other to remove `axis=\"records\"`, and that's too complicated for something that can be done without a deprecation cycle, for reasons described below.)\r\n\r\nDocumentation will have to change at the same time as the feature itself (i.e. in this PR). `ak.flatten` with `axis=None` is mentioned in:\r\n\r\n   * the `ak.run_lengths` docs, but what's said here will remain valid with the new behavior\r\n   * the `ak.flatten` docs: _needs updates_\r\n   * the `how-to-restructure-flatten.md` tutorial: _needs updates_\r\n   * the `how-to-restructure-pad.md` tutorial: _needs updates_\r\n   * notebook documentation in docs-jupyter, but that's dated (2020) and doesn't need to be updated\r\n\r\nSince the new behavior would use an interface that currently has another meaning, there can't be a gradual deprecation cycle. However, this would break things in a way that draws attention to itself. If someone had been using `axis=None` without records, nothing changes, but if they had records, then instead of squashing all numbers into a one-dimensional NumPy-like array, they'll get a record, whose uses are more restricted than a one-dimensional array, so it will show up as an exception. If it shows up in an exception, the user would be able to investigate and find out that the behavior has changed (rather than just getting wrong answers or something).\r\n\r\nThe breaking change has to go in at a [minor version boundary](https://github.com/scikit-hep/awkward-1.0#roadmap), though we're mostly only doing minor version boundaries now. If we have any reason to release a 1.4.x, we'll have to disable the change.",
  "created_at":"2021-07-15T15:24:06Z",
  "id":880789192,
  "issue":990,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MDc4OTE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-15T15:24:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"See https://github.com/scikit-hep/awkward-1.0/issues/770#issuecomment-878630514 for an explanation.",
  "created_at":"2021-07-12T22:09:31Z",
  "id":878630715,
  "issue":991,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3ODYzMDcxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-12T22:09:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I fixed this when I was scanning for instances of `fill_none` without an `axis`.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/pull/917/commits/218f1510a18d494d220940600e4f3dd12d2425cc#diff-a3b1e9523d57fff0b3a1aaa1602078a7e413dd05cfc4781e3addbdaaa4c71f16\r\n\r\nIf the replacement value is a number (`0`), then the `axis` must be `-1` or else you'll get a union. Only `axis=-1` has numbers; all other levels have lists and records.\r\n\r\nI see that the intention here is to make the point-to-point difference for empty lists zero, though `ak.min` and `ak.max` return `None` for empty lists. Even if the min/max reduction is performed at some `axis != -1`, their output will be `None` at `axis == -1` anywhere that zero values are included in a given min/max. So as a post-processing step, filling the nones with `axis=-1` is correct, regardless of the `axis` used in the reduction.",
  "created_at":"2021-07-13T16:49:41Z",
  "id":879244452,
  "issue":992,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTI0NDQ1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T16:49:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"My concern here is where we call ptp on an array that already contains none, shouldn't these nones remain unchanged? ",
  "created_at":"2021-07-13T17:44:29Z",
  "id":879279103,
  "issue":992,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTI3OTEwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T17:44:29Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"At lowest level (`axis=-1`), they ought to have been eaten by the reducer. Let me try to come up with examples.",
  "created_at":"2021-07-13T17:50:20Z",
  "id":879282859,
  "issue":992,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTI4Mjg1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T17:50:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, there are two problems here. First, though, the theory:\r\n\r\n![](https://raw.githubusercontent.com/scikit-hep/awkward-1.0/main/docs-img/diagrams/example-reduction.png)\r\n\r\n```python\r\narray = ak.Array([\r\n    [   0,    1, None      ],\r\n    [None,    3            ],\r\n    [                      ],\r\n    None,\r\n    [   4,    5, None,    6],\r\n])\r\n\r\n######## Using ak.sum:\r\n\r\nassert ak.sum(array, axis=-1).tolist() == [\r\n    1,         # 0 + 1\r\n    3,         # 3\r\n    0,         # list is empty\r\n    None,      # list is missing\r\n    15,        # 4 + 5 + 6\r\n]\r\n\r\nassert ak.sum(array, axis=-2).tolist() == [\r\n        4,    9,    0,    6,\r\n    # 0+4\r\n    #     1+3+5\r\n    #         no data\r\n    #                     6\r\n]\r\n\r\n######## Using ak.min:\r\n\r\nassert ak.min(array, axis=-1).tolist() == [\r\n    0,         # min([0, 1])\r\n    3,         # min([3])\r\n    None,      # list is empty\r\n    None,      # list is missing\r\n    4,         # min([4, 5, 6])\r\n]\r\n\r\nassert ak.min(array, axis=-2).tolist() == [\r\n        0,    1, None,    6,\r\n    # 0,4\r\n    #     1,3,5\r\n    #          no data\r\n    #                     6\r\n]\r\n```\r\n\r\nOne problem is that the two Nones in `ak.min(array, axis=-1)` would get conflated. This is what you were concerned about and you were right to be concerned about it. The application of `ak.fill_none` should be at the lowest level, but after reduction, we can't tell the difference between the lowest level and the second-to-lowest level.\r\n\r\nWe can fix that by using `keepdims=True` in `ak.min` and `ak.max`, then applying the `ak.fill_none` to `axis=-1`.\r\n\r\n```python\r\n>>> ak.min(array, axis=-1, keepdims=True)\r\n<Array [[0], [3], [None], None, [4]] type='5 * option[var * ?int64]'>\r\n\r\n>>> ak.fill_none(ak.min(array, axis=-1, keepdims=True), 999, axis=-1)\r\n<Array [[0], [3], [999], None, [4]] type='5 * option[var * int64]'>\r\n\r\n>>> ak.min(array, axis=-2, keepdims=True)\r\n<Array [[0, 1, None, 6]] type='1 * var * ?int64'>\r\n\r\n>>> ak.fill_none(ak.min(array, axis=-2, keepdims=True), 999, axis=-1)\r\n<Array [[0, 1, 999, 6]] type='1 * var * int64'>\r\n```\r\n\r\nOf course, `ak.ptp` would be filling nones with `0`, not `999`, but this is for illustration and to distinguish from the first row/column. Everywhere that there's a 999 is a place where the maximum of an empty list would be subtracted by the minimum of the same empty list: horizontally in the `axis=-1` case and vertically in the `axis=-2` case.\r\n\r\nIf `keepdims=False` in the `ak.ptp` function, then we have to remove that extra dimension manually:\r\n\r\n```python\r\n>>> ak.fill_none(ak.min(array, axis=-1, keepdims=True), 999, axis=-1)[..., 0]\r\n<Array [0, 3, 999, None, 4] type='5 * ?int64'>\r\n```\r\n\r\nThe other problem is that reducers are producing double-masks (`??`):\r\n\r\n```python\r\n>>> ak.min(array, axis=-1)\r\n<Array [0, 3, None, None, 4] type='5 * ??int64'>\r\n```\r\n\r\nwhich would have prevented `ak.fill_none` from taking effect if we didn't `keepdims=True`. Although `keepdims=True` works around that, this, too, should be fixed. I'll open a PR.",
  "created_at":"2021-07-13T18:19:13Z",
  "id":879301502,
  "issue":992,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTMwMTUwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T18:19:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It's a clever trick to use keepdims in order to distinguish the masks, thanks for raising it!\r\n",
  "created_at":"2021-07-13T18:27:34Z",
  "id":879306800,
  "issue":992,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTMwNjgwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T18:28:18Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Filtering out the methods that start with underscore is optional for Jupyter/IPython tab completion, but declaring in `__dir__` that an object has, for example, a `__mro__` attribute would be wrong. In general, `type(self)` can have different attributes than `self`, so advertising `type(self)`'s attributes in `self`'s `__dir__` is technically wrong. However, all non-underscored attributes of a class are considered bound to the object as well (e.g. all of the properties, methods, classmethods, and staticmethods), so I don't anything wrong could be claimed if we limit it to the non-underscored ones.\r\n\r\nWhy am I mixing in `dir(type(self))` at all? You make subclasses of ak.Array and ak.Record with special attributes, but as subclasses, `__dir__` is already overloaded to not see those attributes. The standard `__dir__` is called on ak.Array/ak.Record, but this is already too far up the inheritance tree to see the special attributes you've made. That's why we were losing them before.\r\n\r\nAlternatively, I suppose I could walk up the MRO asking for the `__dict__` at each level, but that's just the sort of thing that feels fragile in Python because there are all sorts of different ways for a Python object to get \"effective\" attributes. `__slots__`, for instance.\r\n\r\nThis method will not include ad-hoc attributes added to a particular array/record instance, but that's not a good pattern anyway, considering how easily instances are destroyed and created. As defined, `__dir__` describes attributes relevant for an array's _type_.\r\n\r\nThanks for the pointer to `_ipython_key_completions_`. According to the documentation, it defers to `__dir__` if there is no `_ipython_key_completions_`, and that's how I would want to define this, anyway. The need to expand `__dir__` came up for a non-Jupyter reason (identifying that an array of vectors has \"px\", \"py\", \"pz\", \"E\" without actually computing these properties in scikit-hep/fastjet). I was asking you because I thought this might impact NanoEvents.\r\n\r\nBut if nothing looks dangerous for you, give me a thumbs-up and I'll merge it. Thanks!",
  "created_at":"2021-07-13T20:22:09Z",
  "id":879376238,
  "issue":993,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTM3NjIzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T20:22:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> According to the documentation, it defers to `__dir__` if there is no `_ipython_key_completions_`\r\n\r\nEr, one is for completing getattr, and the other for getitem I thought?\r\nOtherwise things look ok, I missed the point about dir(type(self)) returning some inappropriate attributes.",
  "created_at":"2021-07-13T21:07:03Z",
  "id":879403597,
  "issue":993,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTQwMzU5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T21:07:03Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Er, one is for completing getattr, and the other for getitem I thought?\r\n\r\nOh, I missed that (two separate paragraphs in the IPython docs). Yes, adding an explicit `_ipython_key_completions_` to return `fields` would make it more useful. I'll add that to this PR and then enable auto-merge.\r\n\r\nThanks for checking!",
  "created_at":"2021-07-13T21:20:08Z",
  "id":879410909,
  "issue":993,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTQxMDkwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-13T21:20:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The absolute positions are valid when they are passed in to `awkward_NumpyArray_reduce_adjust_starts_64`, but once the `starts` are removed, the result is incorrect.\r\n\r\nI need to further my understanding here. From reading the docs & source, I believe that for `reduce_next`\r\n* `starts` holds the start indices of each sublist e.g.\r\n  ```\r\n  [0,3,5]\r\n  ```\r\n* `parents` holds the sublist index for each element in the flattened array, e.g. \r\n  ```\r\n  [[0, 0, 0], [1,1], [2]]\r\n  ```\r\n\r\nHaving re-written this comment a few times, the main question I have is whether `starts` should be defined as *absolute* indices into the current content, or relative ones (to `starts[0]`), i.e. should the final result be `argmax_offset - start` or should it be `argmax_offset - (start - start_0)`? Is this established anywhere? I am under the impression at the moment that it is not specified, and is assumed to be absolute.\r\n\r\nIn the latter case, the bug for this particular test is in `awkward_NumpyArray_reduce_adjust_starts_64`, which takes `starts` as absolute. The fix would be something like\r\n\r\n```C++\r\n\r\nERROR awkward_NumpyArray_reduce_adjust_starts_64(\r\n  int64_t* toptr,\r\n  int64_t outlength,\r\n  const int64_t* parents,\r\n  const int64_t* starts) {\r\n\r\n  if (outlength > 0) {\r\n    int64_t origin = starts[parents[toptr[0]]];\r\n    for (int64_t k = 0;  k < outlength;  k++) {\r\n      int64_t i = toptr[k];\r\n      if (i >= 0) {\r\n        int64_t parent = parents[i];\r\n        int64_t start = starts[parent];\r\n        toptr[k] += -(start - origin);\r\n      }\r\n    }\r\n  }\r\n  return success();\r\n}\r\n```\r\n\r\nFrom reading other areas of the codebase, like `IndexedArray::reduce_next` which leverages `IndexedArray_reduce_next_fix_offsets_64`, it seems as though this is the expectation.",
  "created_at":"2021-07-14T13:31:18Z",
  "id":879894622,
  "issue":998,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTg5NDYyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-14T16:34:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This PR is one of two solutions according to my understanding of Awkward. I'm going to close it in favour of completing that discussion first.",
  "created_at":"2021-07-14T15:54:52Z",
  "id":880011114,
  "issue":999,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MDAxMTExNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-14T15:54:52Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Issue 1000! :tada:\r\n\r\nI'll be looking at the issues and PRs associated with nonzero starts soon. It's probably related to the code I was looking at last night.",
  "created_at":"2021-07-14T16:45:09Z",
  "id":880047565,
  "issue":1000,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MDA0NzU2NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-07-14T16:45:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't remember what I thought I was waiting for, but whatever it was, it's better to get this merged now than let it get more out of date.",
  "created_at":"2021-08-09T22:12:06Z",
  "id":895585627,
  "issue":1003,
  "node_id":"IC_kwDODBCWws41YY1b",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T22:12:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've converted this into a Discussion: https://github.com/scikit-hep/awkward-1.0/discussions/1010 (with a curiously binary number).",
  "created_at":"2021-07-16T14:35:58Z",
  "id":881497342,
  "issue":1004,
  "node_id":"IC_kwDODBCWws40ipT-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-16T14:35:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- This one had been waiting too long.",
  "created_at":"2021-07-15T17:12:50Z",
  "id":880872273,
  "issue":1005,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MDg3MjI3Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-15T17:12:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This looks right.\r\n> \r\n> Keeping identities and parameters on all node types is also important (though unrelated to the bug-fix, might as well fix them now).\r\n\r\nMy thoughts too. Not sure how that snuck through the original commit; originally I ensured these were all preserved.",
  "created_at":"2021-07-15T19:52:48Z",
  "id":880970539,
  "issue":1008,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MDk3MDUzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-15T19:52:48Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I'm done with this PR now! :rocket: ",
  "created_at":"2021-07-16T08:32:22Z",
  "id":881275099,
  "issue":1008,
  "node_id":"IC_kwDODBCWws40hzDb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-16T08:32:22Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"In principle, the NumpyForm constructor should complain about `itemsize <= 0`, or even any `itemsize` that disagrees with the data type. (There are a lot of redundancies in v1 NumpyForm/NumpyArray's dtype handling, in large part because I discovered platform dependencies in the Python buffer's `format` after development was well underway.) The v2 constructor and `from_json` don't ask for the `itemsize`, since it can be derived from dtype.\r\n\r\nForm JSON with `\"itemsize\": 0` is not legal, though I suppose the check against it hadn't been implemented.",
  "created_at":"2021-07-15T20:52:46Z",
  "id":881005674,
  "issue":1009,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MTAwNTY3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-15T20:52:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> In principle, the NumpyForm constructor should complain about `itemsize <= 0`, or even any `itemsize` that disagrees with the data type. (There are a lot of redundancies in v1 NumpyForm/NumpyArray's dtype handling, in large part because I discovered platform dependencies in the Python buffer's `format` after development was well underway.) The v2 constructor and `from_json` don't ask for the `itemsize`, since it can be derived from dtype.\r\n> \r\n> Form JSON with `\"itemsize\": 0` is not legal, though I suppose the check against it hadn't been implemented.\r\n\r\nAh, I'm not being clear enough in the discussion here - `itemsize` is referring to the `dtype.itemsize`, which counts the size of the inner sub array. That *can* be zero, if any of the inner dims are zero.",
  "created_at":"2021-07-15T21:03:08Z",
  "id":881011586,
  "issue":1009,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MTAxMTU4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-15T21:03:08Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"If we're talking about a dtype's `itemsize` because some element in `dtype.shape` is zero, that's supposed to be captured in the `inner_shape`. (The `inner_shape` is exactly the `dtype.shape`; in other words, the array's `shape[1:]`.)\r\n\r\nA zero could be hidden in the `inner_shape` and could lead to an untested error. It seems you've found one, then.",
  "created_at":"2021-07-15T21:09:27Z",
  "id":881015192,
  "issue":1009,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MTAxNTE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-15T21:09:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, this is what I'm talking about. Given that it's legal for NumPy arrays to have such a dtype (or at least, be described by one), the only error here is that we currently implicitly expect the provided buffer to have non-zero length. This assumption fails in the case that `to_buffers` was called with something like `np.zeros((3, 0))`.",
  "created_at":"2021-07-15T21:14:12Z",
  "id":881017774,
  "issue":1009,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MTAxNzc3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-15T21:14:12Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think this is ready to test :wrench: and ship :rocket:, unless you have any style concerns.",
  "created_at":"2021-07-15T21:14:30Z",
  "id":881017925,
  "issue":1009,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MTAxNzkyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-15T21:14:46Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"It's totally legal for a NumPy array to have zero size due to a zero-length dimension hidden somewhere in its shape.\r\n\r\nSince you've given me the go-ahead and tests pass, I'll merge this now.",
  "created_at":"2021-07-15T22:20:14Z",
  "id":881047948,
  "issue":1009,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg4MTA0Nzk0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-15T22:20:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1011](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0e71384) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `1.50%`.\n> The diff coverage is `83.23%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_reducers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19yZWR1Y2Vycy5weQ==) | `96.48% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `56.27% <51.72%> (+0.82%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `59.75% <63.23%> (+0.44%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.19% <63.58%> (-0.36%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `69.54% <74.19%> (+0.26%)` | :arrow_up: |\n| ... and [31 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1011/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-12-15T18:00:05Z",
  "id":995031977,
  "issue":1011,
  "node_id":"IC_kwDODBCWws47Tvup",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T21:53:54Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, this one's ready too. pybind11 2.7+ is pickier about being explicit with the tuple constructor. On 32 bit Windows `py::ssize_t` is a 32 bit integer.",
  "created_at":"2021-12-16T19:52:10Z",
  "id":996147048,
  "issue":1011,
  "node_id":"IC_kwDODBCWws47X_9o",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T21:46:08Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> Okay, this one's ready too. pybind11 2.7+ is pickier about being explicit with the tuple constructor. On 32 bit Windows `py::ssize_t` is a 32 bit float.\r\n\r\n32-bit _float_? Integer, right?\r\n\r\nThanks for this as well. I'll enable auto-merge, now that the required tests have been updated (and I see that they're right in the list above).",
  "created_at":"2021-12-16T20:48:07Z",
  "id":996182773,
  "issue":1011,
  "node_id":"IC_kwDODBCWws47YIr1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T20:48:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The requirement to do 3.10 must have crossed in the mail. The previous run of the test didn't have them. It's up to date now; something just had to get flushed.",
  "created_at":"2021-12-16T20:50:40Z",
  "id":996184358,
  "issue":1011,
  "node_id":"IC_kwDODBCWws47YJEm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T20:50:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, I meant integer. \ud83d\ude43\r\n\r\nRestarted the CI, it knows it needs the new checks but they need to run.",
  "created_at":"2021-12-16T21:44:27Z",
  "id":996217186,
  "issue":1011,
  "node_id":"IC_kwDODBCWws47YRFi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T21:44:27Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski once tests pass, this is good on my end!",
  "created_at":"2021-07-19T17:48:13Z",
  "id":882739324,
  "issue":1013,
  "node_id":"IC_kwDODBCWws40nYh8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-19T17:48:13Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This should be ready to go, subject to any changes you might feel necessary.",
  "created_at":"2021-07-19T18:00:58Z",
  "id":882747131,
  "issue":1016,
  "node_id":"IC_kwDODBCWws40nab7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-19T18:00:58Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I *think* the bug is that the getfunction doesn't expect an `ndim>1` array, and doesn't set `numpy_to_regular=True` which would otherwise guarantee that it would never receive ndim arrays. \r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/2c679f385cb9cbd7af013958c54c0e0be8d8837c/src/awkward/operations/structure.py#L1369-L1373\r\n\r\nThe fix would then be either:\r\n1. To check that the dimensions are correct\r\n   ```python3\r\n   def getfunction(inputs):\r\n       if all(isinstance(x, ak.layout.NumpyArray) and x.ndim == 1 for x in inputs):\r\n           return lambda: tuple(inputs)\r\n       else:\r\n           return None\r\n   ```\r\n   and if not, the `broadcast_and_apply` helper will convert the `NumpyArray` to a `RegularArray` on the next loop\r\n2. Alternatively set `numpy_to_regular=True` in the call to `broadcast_and_apply`.\r\n\r\nI'm not sure whether the purpose of `numpy_to_regular` is clear to me, could you elaborate @jpivarski ? It seems like it's used to ensure that there any `NumpyArray`s that are passed to the `getfunction` have `ndim == 1` (which is why setting it to `True` would solve this bug. It doesn't affect the recursion routine itself, as later in the recursion body we have\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/2c679f385cb9cbd7af013958c54c0e0be8d8837c/src/awkward/_util.py#L750-L757\r\n\r\nThe result itself is always converted to a `RegularArray` in either case.",
  "created_at":"2021-07-19T21:45:31Z",
  "id":882882378,
  "issue":1017,
  "node_id":"IC_kwDODBCWws40n7dK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-19T22:44:47Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Handling layout details like NumpyArray with ndim > 1 is considered the particular getfunction's responsibility. That means that no one has experienced this bug before in `ak.broadcast_arrays` specifically. (It would be different in other high-level functions.) Maybe that should be collected into a \"regulararray=True\" option on `broadcast_and_apply`, though?",
  "created_at":"2021-07-19T23:23:39Z",
  "id":882926362,
  "issue":1017,
  "node_id":"IC_kwDODBCWws40oGMa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-19T23:23:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"If tests pass, I'm done with this PR :)",
  "created_at":"2021-07-20T07:51:27Z",
  "id":883178346,
  "issue":1019,
  "node_id":"IC_kwDODBCWws40pDtq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T07:51:27Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This (now) should be good to go after the tests :mag: pass",
  "created_at":"2021-07-20T13:55:13Z",
  "id":883413731,
  "issue":1021,
  "node_id":"IC_kwDODBCWws40p9Lj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T14:01:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski ready to go if you are happy with the changes.",
  "created_at":"2021-07-20T14:59:44Z",
  "id":883463356,
  "issue":1021,
  "node_id":"IC_kwDODBCWws40qJS8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T14:59:44Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"At a glance, it looks like this is being triggered from \r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/e258f4f2a252e599c48549f1d294e05b2b65b020/src/libawkward/array/ListOffsetArray.cpp#L796-L812\r\n\r\n",
  "created_at":"2021-07-20T15:03:17Z",
  "id":883466194,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40qJ_S",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T15:03:17Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"The history of that exception was that I was reviewing some of @nsmith-'s corrections to jagged slicing, and I came upon this function that didn't look to me like it could ever be reached. (I couldn't construct an example that would trigger it.) I thought it might be dead code, but instead of removing it, I put this FIXME error message in, so that an example that triggers it would be reported and we can use that example to understand how that code path is reached.\r\n\r\nIf it were Python, we could just put an exception at that point and look at the stack trace. Since it's C++, we'll have to do a bit more work, but it's still possible.\r\n\r\nThanks!",
  "created_at":"2021-07-20T15:24:45Z",
  "id":883483201,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40qOJB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T15:24:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I will look into debugging this with Valgrind / gdb; I've not touched any of the compilation code yet, so it will be interesting to see whether CLion can handle the CMake project.",
  "created_at":"2021-07-20T15:28:12Z",
  "id":883485766,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40qOxG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T15:28:12Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"This code is slated for conversion into Python (which is why I have Python stack traces in mind), but if this is a common enough case\u2014that I somehow wasn't able to make up\u2014then waiting for v2 may be too long.\r\n\r\nIf it depends crucially on the \"list-of-IndexedArray-of-X\" and not, for instance, on \"list-of-IndexedOptionArray-of-X\", then that point in the code could simply `project()` the IndexedArray.",
  "created_at":"2021-07-20T15:38:25Z",
  "id":883493070,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40qQjO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T15:38:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I can also trigger this with a more conventional layout:\r\n```python3\r\ny = ak.Array([\r\n    [\r\n        [1,2,3,4],\r\n        [5,6,7,8],\r\n    ]\r\n])\r\ny = ak.to_regular(y,axis=-1)\r\n```\r\n\r\nCould you help me to understand what this particular overload is supposed to do? By name, `SliceVarNewAxis` seems like `np.newaxis`, but there is also `SliceNewAxis`, so there is clearly more going on here.",
  "created_at":"2021-07-20T15:54:37Z",
  "id":883504863,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40qTbf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T15:54:37Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"SliceVarNewAxis had to be added later, as an alternate form of SliceNewAxis. They're very similar, made to distinguish between `np.newaxis` applied to a regular axis vs a variable axis? I'll have to look it up.",
  "created_at":"2021-07-20T16:04:36Z",
  "id":883512399,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40qVRP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T16:04:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"SliceNewAxis and SliceVarNewAxis shouldn't be relevant because none of these slices involve any `np.newaxis`.\r\n\r\nBut there's definitely something murky here.\r\n\r\n```python\r\n>>> y = ak.to_regular(ak.Array([[[1, 2, 3, 4], [5, 6, 7, 8]]]), axis=-1)\r\n>>> y\r\n<Array [[[1, 2, 3, 4], [5, 6, 7, 8]]] type='1 * var * 4 * int64'>\r\n>>> y.layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 2]\" offset=\"0\" length=\"2\" at=\"0x56448d7a6940\"/></offsets>\r\n    <content><RegularArray size=\"4\">\r\n        <content><NumpyArray format=\"l\" shape=\"8\" data=\"1 2 3 4 5 6 7 8\" at=\"0x56448d7aa960\"/></content>\r\n    </RegularArray></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\nand\r\n\r\n```python\r\n>>> t = ak.argmax(y, axis=-1, keepdims=True)\r\n>>> t\r\n<Array [[[3], [3]]] type='1 * var * 1 * ?int64'>\r\n>>> t.layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 2]\" offset=\"0\" length=\"2\" at=\"0x56448d7a9b30\"/></offsets>\r\n    <content><RegularArray size=\"1\">\r\n        <content><ByteMaskedArray valid_when=\"false\">\r\n            <mask><Index8 i=\"[0 0]\" offset=\"0\" length=\"2\" at=\"0x56448d7a9bb0\"/></mask>\r\n            <content><NumpyArray format=\"l\" shape=\"2\" data=\"3 3\" at=\"0x56448d7a9ab0\"/></content>\r\n        </ByteMaskedArray></content>\r\n    </RegularArray></content>\r\n</ListOffsetArray64>\r\n```\r\n\r\nFirst of all, this is the simplest and most natural example that triggers the bug:\r\n\r\n```python\r\n>>> y[t]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/highlevel.py\", line 996, in __getitem__\r\n    tmp = ak._util.wrap(self.layout[where], self._behavior)\r\nRuntimeError: FIXME ListArrayOf<T>::SliceVarNewAxis. 2021-02-10 Was this left over from development? If so, it's not getting tested. If anyone out there encounters this error, please report it so that we can properly validate this code path and include it in the tests. https://github.com/scikit-hep/awkward-1.0/issues/new?assignees=&labels=bug+%28unverified%29&template=bug-report.md&title=\r\n```\r\n\r\nIs it the fact that `t` has an option-type? No.\r\n\r\n```python\r\n>>> t2 = ak.fill_none(t, 999, axis=-1)\r\n>>> t2\r\n<Array [[[3], [3]]] type='1 * var * 1 * int64'>\r\n>>> t2.layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 2]\" offset=\"0\" length=\"2\" at=\"0x56448d7a9b30\"/></offsets>\r\n    <content><RegularArray size=\"1\">\r\n        <content><NumpyArray format=\"l\" shape=\"2\" data=\"3 3\" at=\"0x56448d7ad660\"/></content>\r\n    </RegularArray></content>\r\n</ListOffsetArray64>\r\n>>> y[t2]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/highlevel.py\", line 996, in __getitem__\r\n    tmp = ak._util.wrap(self.layout[where], self._behavior)\r\nRuntimeError: FIXME ListArrayOf<T>::SliceVarNewAxis. 2021-02-10 Was this left over from development? If so, it's not getting tested. If anyone out there encounters this error, please report it so that we can properly validate this code path and include it in the tests. https://github.com/scikit-hep/awkward-1.0/issues/new?assignees=&labels=bug+%28unverified%29&template=bug-report.md&title=\r\n```\r\n\r\nDoes it depend on this depth of nesting? Yes.\r\n\r\n```python\r\n>>> y[0]\r\n<Array [[1, 2, 3, 4], [5, 6, 7, 8]] type='2 * 4 * int64'>\r\n>>> t[0]\r\n<Array [[3], [3]] type='2 * 1 * ?int64'>\r\n>>> y[0][t[0]]\r\n<Array [[4, 4, 4, 4], [8, 8, 8, 8]] type='2 * var * ?int64'>\r\n```\r\n\r\nBut\u2014murkiness\u2014it's not okay if the slicer (`t`) does not have potentially missing values.\r\n\r\n```python\r\n>>> y[0][t2[0]]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/highlevel.py\", line 996, in __getitem__\r\n    tmp = ak._util.wrap(self.layout[where], self._behavior)\r\nValueError: in RegularArray attempting to get 3, index out of range\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.4.0/src/cpu-kernels/awkward_RegularArray_getitem_next_array_regularize.cpp#L19)\r\n```\r\n\r\nOh, but I guess that's because this has become a strictly regular array problem, and it's deferring to NumPy rules.\r\n\r\n```python\r\n>>> y0 = ak.to_numpy(y[0])\r\n>>> t20 = ak.to_numpy(t2[0])\r\n>>> y0\r\narray([[1, 2, 3, 4],\r\n       [5, 6, 7, 8]])\r\n>>> t20\r\narray([[3],\r\n       [3]])\r\n>>> y0[t20]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nIndexError: index 3 is out of bounds for axis 0 with size 2\r\n```\r\n\r\nThat's the design\u2014the potentially missing values makes the slicer be interpreted as a nested slice\u2014but a bit counter-intuitive because it looks like a small change from the case in which the values are not formally nullable. But NumPy does not ever do nested slices; each dimension has to be a separate array in a tuple, and any multidimensionalness in the slicer is taken to reshape the output. Both of those considerations presuppose rectilinear data.\r\n\r\nSo this \"murkiness,\" at least, is understood. It's a poor fit to advanced NumPy slicing features and the idea of variable-length data. But the FIXME error message should not be encountered. `y[t2]` may be the simplest case that demonstrates it, and the fact that `y[0][t[0]]` doesn't demonstrate it seems like a major clue.",
  "created_at":"2021-07-20T16:38:14Z",
  "id":883535504,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40qa6Q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T16:38:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, it seems that this code path is indeed coming from `ListOffsetArrayOf<T>`, given this call stack:\r\n```\r\nawkward::ListArrayOf<long>::getitem_next_jagged ListArray.cpp:2016\r\nawkward::Content::getitem_next_jagged Content.cpp:1480\r\nawkward::ListOffsetArrayOf<long>::getitem_next_jagged ListOffsetArray.cpp:811\r\nawkward::RegularArray::getitem_next RegularArray.cpp:1554\r\nawkward::Content::getitem_next Content.cpp:1449\r\nawkward::Content::getitem Content.cpp:1398\r\ngetitem<awkward::ListOffsetArrayOf<long> > content.cpp:819\r\npybind11::detail::argument_loader<awkward::ListOffsetArrayOf<long> const&, pybind11::object const&>::call_impl<pybind11::object, pybind11::object (*&)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&), 0ul, 1ul, pybind11::detail::void_type>(pybind11::object (*&)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&), std::integer_sequence<unsigned long, 0ul, 1ul>, pybind11::detail::void_type&&) && cast.h:2042\r\npybind11::detail::argument_loader<awkward::ListOffsetArrayOf<long> const&, pybind11::object const&>::call<pybind11::object, pybind11::detail::void_type, pybind11::object (*&)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&)>(pybind11::object (*&)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&)) && cast.h:2014\r\npybind11::cpp_function::initialize<pybind11::object (*&)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&), pybind11::object, awkward::ListOffsetArrayOf<long> const&, pybind11::object const&, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11::object (*&)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&), pybind11::object (*)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::operator()(pybind11::detail::function_call&) const pybind11.h:192\r\npybind11::cpp_function::initialize<pybind11::object (*&)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&), pybind11::object, awkward::ListOffsetArrayOf<long> const&, pybind11::object const&, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11::object (*&)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&), pybind11::object (*)(awkward::ListOffsetArrayOf<long> const&, pybind11::object const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) pybind11.h:170\r\npybind11::cpp_function::dispatcher pybind11.h:767\r\ncfunction_call 0x0000564ccb04b145\r\n_PyObject_MakeTpCall 0x0000564ccb031bb6\r\n_PyObject_VectorcallTstate abstract.h:116\r\n_PyObject_VectorcallTstate abstract.h:103\r\nmethod_vectorcall 0x0000564ccb08bfda\r\n_PyObject_VectorcallTstate abstract.h:118\r\nvectorcall_unbound 0x0000564ccb098359\r\nvectorcall_method 0x0000564ccb098359\r\nslot_mp_subscript 0x0000564ccb098359\r\nPyObject_GetItem 0x0000564ccb073987\r\n_PyEval_EvalFrameDefault 0x0000564ccb0ca7e6\r\n_PyEval_EvalFrame pycore_ceval.h:40\r\nfunction_code_fastcall 0x0000564ccb09803b\r\n_PyFunction_Vectorcall 0x0000564ccb09803b\r\n_PyObject_VectorcallTstate abstract.h:118\r\nvectorcall_unbound 0x0000564ccb09803b\r\nvectorcall_method 0x0000564ccb09803b\r\nslot_mp_subscript 0x0000564ccb09803b\r\nPyObject_GetItem 0x0000564ccb073987\r\n_PyEval_EvalFrameDefault 0x0000564ccb0ca7e6\r\n_PyEval_EvalFrame pycore_ceval.h:40\r\n_PyEval_EvalCode 0x0000564ccb024560\r\n_PyEval_EvalCodeWithName 0x0000564ccb109227\r\nPyEval_EvalCodeEx 0x0000564ccb109269\r\nPyEval_EvalCode 0x0000564ccb10928b\r\nrun_eval_code_obj 0x0000564ccb13bdc9\r\nrun_mod 0x0000564ccb176894\r\npyrun_file 0x0000564ccafffe4a\r\npyrun_simple_file 0x0000564ccb17ca02\r\nPyRun_SimpleFileExFlags 0x0000564ccb17ca02\r\npymain_run_file 0x0000564ccb17d0d5\r\npymain_run_python 0x0000564ccb17d0d5\r\nPy_RunMain 0x0000564ccb17d0d5\r\nPy_BytesMain 0x0000564ccb17d229\r\n__libc_start_main 0x00007ff2ed7ed565\r\n_start 0x0000564ccb0f63a1\r\n```",
  "created_at":"2021-07-20T20:09:30Z",
  "id":883662288,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40q53Q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T20:09:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The line that introduces the `SliceVarNewAxis` is https://github.com/scikit-hep/awkward-1.0/blob/eb61e07f52e06708d6c6d4238014218fd5288eb7/src/libawkward/array/RegularArray.cpp#L971\r\n\r\nThis is called from `ListOffsetArray::asslice` for my toy layout `y` in \r\nhttps://github.com/scikit-hep/awkward-1.0/blob/eb61e07f52e06708d6c6d4238014218fd5288eb7/src/libawkward/array/ListOffsetArray.cpp#L1189\r\n\r\nAs to the interpretation of what this means ... I'm not quite up to speed there yet.",
  "created_at":"2021-07-20T20:41:02Z",
  "id":883694327,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40rBr3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T20:41:36Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"So, as per Gitter, I determined that `SliceVarNewAxis` is an undocumented (unsurprising given how much has been added recently!) feature that supports broadcasting 1-size dimensions in jagged advanced indices. \r\n\r\nI.e. \r\n```python3\r\n>>> array = ak.Array(\r\n...     [[[1.1, 2.2, 3.3], [3, 4], [4.4, 5.5], [6.6, 7.7], [8, 6], [7.7, 8.8, 9.9]]]\r\n... )\r\n>>> array\r\n<Array [... 7.7], [8, 6], [7.7, 8.8, 9.9]]] type='1 * var * var * float64'>\r\n>>> ix = ak.Array([[0, -1]])\r\n>>> ix\r\n<Array [[0, -1]] type='1 * var * int64'>\r\n>>> array[ix]\r\n<Array [... 1.1, 2.2, 3.3], [7.7, 8.8, 9.9]]] type='1 * var * var * float64'>\r\n>>> array[ix[np.newaxis, ...]]\r\n<Array [... [6.6, 7.7], [8, 6], [7.7, 9.9]]] type='1 * var * var * float64'>\r\n```\r\n\r\nClearly, `array[ix]` is taking the first and last subarrays, whereas `array[ix[np.newaxis, ...]]` is picking the first and last elements across all subarrays.\r\n\r\nThis is being raised for my test case because `t` is derived from a `RegularArray`, and so its `size=1` dimension (from `keepdims=True`) is also regular. This is why the bug is not triggered when the `keepdims=True` dimension is irregular in `y`.\r\n\r\nGiven my current understanding, I think we actually want to remove this exception altogether - isn't this method required?",
  "created_at":"2021-07-20T21:57:13Z",
  "id":883733719,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40rLTX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T22:24:58Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"If removing the exception altogether returns the right result, then that's the best way to resolve this issue. The FIXME was added because I wasn't sure whether the whole function could ever be reached. If it can be reached and it does the right thing, then the function should stay and the exception should be removed. If it can be reached and it does the wrong thing, then it will unfortunately require more scrutiny.",
  "created_at":"2021-07-20T22:35:07Z",
  "id":883750223,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40rPVP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T22:35:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"In disabling this exception, the result fails with\r\n```pytb\r\nValueError: cannot fit jagged slice with length 1 into ListArray64 of size 2\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.4.0/src/libawkward/array/ListArray.cpp#L1827)\r\n```\r\nI *think* the problem is in the translation from the `SliceVarNewAxis` to `SliceJagged` in https://github.com/scikit-hep/awkward-1.0/blob/5bb81f0a103277495b391723d61e86c6bac9c74b/src/libawkward/array/ListArray.cpp#L2036-L2050\r\n\r\nGiven these two examples:\r\n```python3\r\n\r\ndef test_list_jagged_t():\r\n    y = ak.Array(\r\n        ak.layout.ListOffsetArray64(\r\n            ak.layout.Index64(np.r_[0, 2]),\r\n            ak.layout.ListArray64(\r\n                ak.layout.Index64(np.r_[0, 4]),\r\n                ak.layout.Index64(np.r_[4, 8]),\r\n                ak.layout.NumpyArray(\r\n                    np.array([0, 1, 2, -1, 0, 1, 2, 3])\r\n                )\r\n            )\r\n        )\r\n    )\r\n    t = ak.argmax(y, axis=-1, keepdims=True, mask_identity=False)\r\n\r\n    print(y[t])\r\n\r\n\r\ndef test_list_regular_t():\r\n    y = ak.Array(\r\n        ak.layout.ListOffsetArray64(\r\n            ak.layout.Index64(np.r_[0, 2]),\r\n            ak.layout.ListArray64(\r\n                ak.layout.Index64(np.r_[0, 4]),\r\n                ak.layout.Index64(np.r_[4, 8]),\r\n                ak.layout.NumpyArray(\r\n                    np.array([0, 1, 2, -1, 0, 1, 2, 3])\r\n                )\r\n            )\r\n        )\r\n    )\r\n    t = ak.to_regular(ak.argmax(y, axis=-1, keepdims=True, mask_identity=False), axis=-1)\r\n\r\n    print(y[t])\r\n```\r\n\r\nthe `where` argument in `Content::getitem` corresponding to `t` in `test_list_jagged_t` is\r\n```python3\r\n[jagged([0, 2], jagged([0, 1, 2], array([2, 3])))]\r\n```\r\n\r\nconversely, for `test_list_regular_t`, `where` is initially\r\n```python3\r\n[jagged([0, 2], newaxis(array([2, 3])))]\r\n```\r\n\r\n `ListArrayOf<T>::getitem_next_jagged` then expands the `SliceVarNewAxis` into a `SliceJagged`, producing\r\n```python3\r\njagged([0, 2], array([2, 2]))\r\n```\r\nwhich would be equivalent to an initial `where` of\r\n```python\r\n[jagged([0, 2], jagged([0, 2], array([2, 2])))]\r\n```\r\n\r\nGiven that I am not fully up to speed with the complexity here, I have made some guesses that might be wrong. I have observed that `Content::getitem` wraps the content in a single list with `RegularArray`. I don't know why this is, but it means that at each level of `getitem_next_jagged`, the current slice corresponds to child of the layout, i.e. \r\n```\r\n[jagged([0, 2], jagged([0, 1, 2], array([2, 3])))]\r\n          ___________/               \\_______________\r\n        /                                             \\\r\nListArray[1]::getitem_next_jagged  ListArray[2]::getitem_next_jagged\r\n```\r\n\r\nFor this inner `ListArray`, the offsets are wrong for the regular case. The logic that does this is quite simple:\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/5bb81f0a103277495b391723d61e86c6bac9c74b/src/libawkward/array/ListArray.cpp#L2048\r\n\r\nWe are just taking the offsets off this parent array, whereas we need to use the offsets of the child.\r\n\r\nAt this point, I think my lack of knowledge about the design at play means I am unable to make any informed decision as to how to proceed!\r\n\r\nThis feature is technically not documented anywhere, so it's partially hidden behind a compile-time feature flag (namely, a big loud exception)! As such, you probably *could* leave this until v2.\r\n\r\nIn the mean-time, I'll ensure that any jagged lookups don't have regular 1-dims.",
  "created_at":"2021-07-21T09:05:12Z",
  "id":884022340,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40sRxE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-21T11:11:15Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"@agoose77  I've been investigating this issue. Normally, the mixed variable and regular slicer that you made would be declared invalid: all-variable dimensions triggers Awkward advanced indexing and all-regular triggers NumPy advanced indexing; a mixture would be confusing, so it's not allowed.\r\n\r\nHowever, it sneaks through because length-1 regular arrays in a slice are interpreted as SliceVarNewAxis: https://github.com/scikit-hep/awkward-1.0/pull/694/files#diff-a63810de74c2520ec41382cece2d156993c47ba9eb69772ce6b10a8262536e22\r\n\r\nThe use of the word \"newaxis\" is a little misleading; this is not representing a `np.newaxis` object in a slice, but a length-1 regular axis _that was probably made_ by a `np.newaxis`. These tests demonstrate its use: https://github.com/scikit-hep/awkward-1.0/pull/694/files#diff-822ebabcc1ec64f9f91037a24a786bc869db7a9ebd334cf14189f5d8c0149988 (the `np.newaxis` modifies the `slicer`, which is then used to slice the `array`. SliceVarNewAxis objects are created when slicing `array`, not when slicing `slicer`.\r\n\r\nThis feature was added in a rush to prepare a tutorial that never happened (https://github.com/jpivarski-talks/2021-02-09-propose-scipy2021-tutorial/blob/main/prep/million-song.ipynb). It was the most minimal way I could see to add features that were necessary to do the analysis in that tutorial. The idea was that this is replicating a NumPy feature\u2014boradcasting length-1 dimensions in slices\u2014in Awkward advanced indexing. But Awkward advanced indexing fits a nested structure in the slicer to the nested structure that you're slicing, whereas NumPy advanced indexing slices each dimension by a different array, and all of those arrays in the tuple are broadcasted. NumPy advanced indexing is truly broadcasting because there are multiple arrays in the slicer; Awkward advanced indexing has only one array in the slicer, so it's not really broadcasting.\r\n\r\nTreating a length-1 dimension differently from any other length makes this rule hard to predict. The idea was that you'd get the length-1 dimension from a `np.newaxis`, but in your case, you got it from a reducer with `keepdims=True`. I'm thinking this was a bad rule to have introduced: it has unforeseen consequences.\r\n\r\nIn the test suite, the rule is only triggered in the tests that were added to check it. The rule was never advertised (after all, that tutorial was never presented), and it is unlikely to have made its way into any analyses other than yours, since it's rather easy to trigger the FIXME (which is much older, and may yet be unreachable without the new SliceVarNewAxis rule). Slices that are enabled by the rule can still be performed without it\u2014for instance, this test using the rule:\r\n\r\n```python\r\n    array = ak.Array(\r\n        [\r\n            [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]],\r\n            [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]],\r\n        ]\r\n    )\r\n    slicer = ak.Array([[3, 4], [0, 1, 2, 3]])\r\n    assert array[slicer[:, np.newaxis]].tolist() == [\r\n        [[3, 4], [8, 9], [13, 14]],\r\n        [[15, 16, 17, 18], [20, 21, 22, 23], [25, 26, 27, 28]],\r\n    ]\r\n```\r\n\r\ncan be replaced by the following, without the new rule:\r\n\r\n```python\r\n    assert array[[[[3, 4], [3, 4], [3, 4]], [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]]].tolist() == [\r\n        [[3, 4], [8, 9], [13, 14]],\r\n        [[15, 16, 17, 18], [20, 21, 22, 23], [25, 26, 27, 28]],\r\n    ]\r\n```\r\n\r\nYour use-case is also possible, but only if the slicer is all-variable (in keeping with the rule to avoid confusion between Awkward advanced indexing and NumPy advanced indexing):\r\n\r\n```python\r\n>>> y = ak.Array([[[1, 2, 3, 4], [5, 6, 7, 8]]])\r\n>>> t = ak.argmax(y, axis=-1, keepdims=True)\r\n>>> y[t]\r\n<Array [[[4], [8]]] type='1 * var * var * ?int64'>\r\n```\r\n\r\n(`y` is allowed to have regular dimensions, but `t` can't mix regular with irregular. You could keep `y` irregular or make `t` regular.)\r\n\r\nSo I think I'm going to remove it, which reverts only PR: https://github.com/scikit-hep/awkward-1.0/pull/694.",
  "created_at":"2021-07-21T14:32:04Z",
  "id":884237836,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40tGYM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-21T14:32:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I will be removing it: https://github.com/scikit-hep/awkward-1.0/discussions/1027",
  "created_at":"2021-07-21T16:01:37Z",
  "id":884304841,
  "issue":1022,
  "node_id":"IC_kwDODBCWws40tWvJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-21T16:01:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski are you happy to merge without testing this?",
  "created_at":"2021-07-20T15:25:14Z",
  "id":883483547,
  "issue":1023,
  "node_id":"IC_kwDODBCWws40qOOb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T15:25:14Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah, I can merge this without the full test. The Sphinx doctest formally tests docstrings, but that just finished successfully:\r\n\r\nhttps://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=7187&view=logs&j=d3c4c657-9510-5085-9998-cbed271467cb",
  "created_at":"2021-07-20T15:34:14Z",
  "id":883490120,
  "issue":1023,
  "node_id":"IC_kwDODBCWws40qP1I",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T15:34:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Fab, let's merge.",
  "created_at":"2021-07-20T21:18:18Z",
  "id":883715101,
  "issue":1024,
  "node_id":"IC_kwDODBCWws40rGwd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-20T21:18:18Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I had thought this was related to #1022, but it's not.\r\n\r\nYes, we can do `toRegularArray` for this (that's what PR #1029 does), as `toRegularArray` is pretty cheap. If the NumpyArray is not contiguous, it will need to be rewritten, but otherwise it will not. ListArrays have been converted `toListOffsetArray64` (more expensive) with less motivation!",
  "created_at":"2021-07-21T18:11:40Z",
  "id":884389210,
  "issue":1026,
  "node_id":"IC_kwDODBCWws40trVa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-21T18:11:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This PR is now an almost exact reversal of #694. The \"almost\" is for broadening the diagnostic output, e.g. [in Index.cpp](https://github.com/scikit-hep/awkward-1.0/pull/694/files#diff-29fc2e3641a024611be4349ab2c38d2399caca87c916ca2f1ce35ecbcc090192) and similar, as well as the effect this had on tests, which is good to keep.",
  "created_at":"2021-07-21T17:16:27Z",
  "id":884355171,
  "issue":1028,
  "node_id":"IC_kwDODBCWws40tjBj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-21T17:16:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I (hopefully) fixed the test for windows, but disabled auto-merge so that you'll get the final sign off.",
  "created_at":"2021-07-22T07:16:03Z",
  "id":884703683,
  "issue":1029,
  "node_id":"IC_kwDODBCWws40u4HD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-22T07:16:22Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"What do you think, @nsmith-? (@nsmith- is the author of `mixin_class`.) It looks good to me.\r\n\r\nI think most behavior names in applications would be the same as the Python class name. The built-in ones, `\"string\"`, `\"bytestring\"`, `\"char\"`, `\"byte\"`, `\"categorical\"`, and proposed `\"sorted_map\"` are short and not camel-case because they're \"core\" names for basic data types. Applications like Vector use traditional Python class naming conventions for the behavior names: `\"Vector2D\"`, `\"Vector3D\"`, `\"Vector4D\"`, `\"Momentum2D\"`, `\"Momentum3D\"`, `\"Momentum4D\"`, which match class names in the Vector hierarchy, though they're not first in the method resolution order. This disparity between the core built-in naming convention and application-level naming convention is reflected in Python itself: basic types are named `\"str\"`, `\"list\"`, `\"dict\"`, etc, but user-defined classes shouldn't be.\r\n\r\nEven though the naming conventions wouldn't _usually_ be in conflict, I agree that it's good to at least be able to override the name.",
  "created_at":"2021-08-09T13:29:19Z",
  "id":895223730,
  "issue":1030,
  "node_id":"IC_kwDODBCWws41XAey",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T13:29:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Looks fine to me. You might also consider defining a repr for the mixin if a name is provided a la\r\nhttps://github.com/CoffeaTeam/coffea/blob/ea73fc7cd8b27914845db54f296d9e55740d93cf/coffea/nanoevents/methods/nanoaod.py#L20-L25\r\nI think in the case the classname is used it could also work but perhaps too much magic?",
  "created_at":"2021-08-09T13:46:18Z",
  "id":895236560,
  "issue":1030,
  "node_id":"IC_kwDODBCWws41XDnQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T13:46:18Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, there's already magic to take `\"__record__\": XYZ` and make the type string be `XYZ[field1: x, field2: y, field3: z]` instead of `{field1: x, field2: y, field3: z}` (regardless of whether any behavior is defined). Overriding `\"__typestr__\"` with the classname only would reduce the type string to `XYZ`, which ought to be opt-in (maybe _another_ argument to `mixin_class`). For example, Vector does not do that, so that you can see which coordinate system is being used (through the names of the fields) and their types (e.g. `float32` vs `float64`).\r\n\r\nThis looks good to me, too, so I'll merge it. I think `mixin_class` has tests, right? I'll do a quick check before the merge.",
  "created_at":"2021-08-09T14:19:44Z",
  "id":895264617,
  "issue":1030,
  "node_id":"IC_kwDODBCWws41XKdp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T14:19:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(Yes, there are 11 hits in `tests/test_0355-mixins.py`, `tests/test_0815-broadcast-union-types-to-all-possibilities.py`, and `tests/test_0930-bug-in-unionarray-purelist_parameter.py`.)",
  "created_at":"2021-08-09T14:21:16Z",
  "id":895265863,
  "issue":1030,
  "node_id":"IC_kwDODBCWws41XKxH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T14:21:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif As a hint for how to go about translating and debugging, I showed some examples of my own debugging: 86377cc51b7ec42907a68297ee4d7749f4b55196 adds debugging code to compare the Python with the original C++ and 3878e10d3ecbbfa1ab8503f9a29f2473fbd2a170 is the subsequent commit that removes that debugging code (not sure which would be easier to read).\r\n\r\nOnce the C++ codebase is compiled, adding `std::cout` is fast to recompile (if you're using the `localbuild.py`), so you can add equivalent print statements on both sides when something is different, to try to figure out why something is different. The new Python code goes through all of the same steps as the C++ (in these \"non-weird\" cases), so you can check the intermediate Indexes to be sure that they're exactly the same at each step (or narrow in on where they're different).",
  "created_at":"2021-07-24T00:59:00Z",
  "id":885978329,
  "issue":1031,
  "node_id":"IC_kwDODBCWws40zvTZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-24T00:59:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I should have done this when you first started!\r\n\r\n@all-contributors please add @ioanaif  for code, test",
  "created_at":"2021-07-28T14:51:37Z",
  "id":888375219,
  "issue":1031,
  "node_id":"IC_kwDODBCWws4084ez",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-28T14:51:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/1035) to add @ioanaif! :tada:",
  "created_at":"2021-07-28T14:51:46Z",
  "id":888375334,
  "issue":1031,
  "node_id":"IC_kwDODBCWws4084gm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-28T14:51:46Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"Number of times each kernel is called in integration tests:\r\n\r\n| kernel name                                                            | count |\r\n|:---|---:|\r\n| awkward_BitMaskedArray_to_ByteMaskedArray                              | 275 |\r\n| awkward_BitMaskedArray_to_IndexedOptionArray                           | 13 |\r\n| awkward_ByteMaskedArray_getitem_carry                                  | 52 |\r\n| awkward_ByteMaskedArray_getitem_nextcarry                              | 71 |\r\n| awkward_ByteMaskedArray_getitem_nextcarry_outindex                     | 38 |\r\n| awkward_ByteMaskedArray_mask                                           | 1 |\r\n| awkward_ByteMaskedArray_numnull                                        | 194 |\r\n| awkward_ByteMaskedArray_overlay_mask                                   | 67 |\r\n| awkward_ByteMaskedArray_reduce_next_64                                 | 85 |\r\n| awkward_ByteMaskedArray_reduce_next_nonlocal_nextshifts_64             | 2 |\r\n| awkward_ByteMaskedArray_reduce_next_nonlocal_nextshifts_fromshifts_64  | 0 |\r\n| awkward_ByteMaskedArray_toIndexedOptionArray                           | 83 |\r\n| awkward_Content_getitem_next_missing_jagged_getmaskstartstop           | 34 |\r\n| awkward_Identities32_to_Identities64                                   | 134 |\r\n| awkward_Identities_extend                                              | 1 |\r\n| awkward_Identities_from_IndexedArray                                   | 7 |\r\n| awkward_Identities_from_ListArray                                      | 11 |\r\n| awkward_Identities_from_ListOffsetArray                                | 189 |\r\n| awkward_Identities_from_RegularArray                                   | 5 |\r\n| awkward_Identities_from_UnionArray                                     | 2 |\r\n| awkward_Identities_getitem_carry                                       | 6695 |\r\n| awkward_Index_iscontiguous                                             | 15470 |\r\n| awkward_Index_to_Index64                                               | 0 |\r\n| awkward_IndexedArray_fill                                              | 114 |\r\n| awkward_IndexedArray_fill_count                                        | 24 |\r\n| awkward_IndexedArray_flatten_nextcarry                                 | 454 |\r\n| awkward_IndexedArray_flatten_none2empty                                | 13 |\r\n| awkward_IndexedArray_getitem_adjust_outindex                           | 228 |\r\n| awkward_IndexedArray_getitem_carry                                     | 149 |\r\n| awkward_IndexedArray_getitem_nextcarry                                 | 184 |\r\n| awkward_IndexedArray_getitem_nextcarry_outindex                        | 63 |\r\n| awkward_IndexedArray_getitem_nextcarry_outindex_mask                   | 334 |\r\n| awkward_IndexedArray_local_preparenext_64                              | 39 |\r\n| awkward_IndexedArray_mask                                              | 403 |\r\n| awkward_IndexedArray_numnull                                           | 984 |\r\n| awkward_IndexedArray_index_of_nulls                                    | 20 |\r\n| awkward_IndexedArray_overlay_mask                                      | 381 |\r\n| awkward_IndexedArray_reduce_next_64                                    | 133 |\r\n| awkward_IndexedArray_reduce_next_fix_offsets_64                        | 66 |\r\n| awkward_IndexedArray_reduce_next_nonlocal_nextshifts_64                | 14 |\r\n| awkward_IndexedArray_reduce_next_nonlocal_nextshifts_fromshifts_64     | 8 |\r\n| awkward_IndexedArray_simplify                                          | 205 |\r\n| awkward_IndexedArray_validity                                          | 353 |\r\n| awkward_IndexedArray_ranges_next_64                                    | 6 |\r\n| awkward_IndexedArray_ranges_carry_next_64                              | 6 |\r\n| awkward_IndexedOptionArray_rpad_and_clip_mask_axis1                    | 17 |\r\n| awkward_ListArray_broadcast_tooffsets                                  | 747 |\r\n| awkward_ListArray_combinations                                         | 24 |\r\n| awkward_ListArray_combinations_length                                  | 24 |\r\n| awkward_ListArray_compact_offsets                                      | 492 |\r\n| awkward_ListArray_fill                                                 | 364 |\r\n| awkward_ListArray_getitem_carry                                        | 5135 |\r\n| awkward_ListArray_getitem_jagged_apply                                 | 135 |\r\n| awkward_ListArray_getitem_jagged_carrylen                              | 135 |\r\n| awkward_ListArray_getitem_jagged_descend                               | 24 |\r\n| awkward_ListArray_getitem_jagged_expand                                | 2 |\r\n| awkward_ListArray_getitem_jagged_numvalid                              | 35 |\r\n| awkward_ListArray_getitem_jagged_shrink                                | 35 |\r\n| awkward_ListArray_getitem_next_array                                   | 30 |\r\n| awkward_ListArray_getitem_next_array_advanced                          | 1097 |\r\n| awkward_ListArray_getitem_next_at                                      | 2804 |\r\n| awkward_ListArray_getitem_next_range                                   | 952 |\r\n| awkward_ListArray_getitem_next_range_carrylength                       | 952 |\r\n| awkward_ListArray_getitem_next_range_counts                            | 12 |\r\n| awkward_ListArray_getitem_next_range_spreadadvanced                    | 12 |\r\n| awkward_ListArray_localindex                                           | 23 |\r\n| awkward_ListArray_min_range                                            | 12 |\r\n| awkward_ListArray_num                                                  | 60 |\r\n| awkward_ListArray_rpad_and_clip_length_axis1                           | 12 |\r\n| awkward_ListArray_rpad_axis1                                           | 12 |\r\n| awkward_ListArray_validity                                             | 239 |\r\n| awkward_ListOffsetArray_compact_offsets                                | 44 |\r\n| awkward_ListOffsetArray_flatten_offsets                                | 58 |\r\n| awkward_ListOffsetArray_getitem_adjust_offsets                         | 29 |\r\n| awkward_ListOffsetArray_getitem_adjust_offsets_index                   | 11 |\r\n| awkward_ListOffsetArray_local_preparenext_64                           | 23 |\r\n| awkward_ListOffsetArray_reduce_global_startstop_64                     | 1204 |\r\n| awkward_ListOffsetArray_reduce_local_nextparents_64                    | 797 |\r\n| awkward_ListOffsetArray_reduce_local_outoffsets_64                     | 739 |\r\n| awkward_ListOffsetArray_reduce_nonlocal_findgaps_64                    | 384 |\r\n| awkward_ListOffsetArray_reduce_nonlocal_maxcount_offsetscopy_64        | 407 |\r\n| awkward_ListOffsetArray_reduce_nonlocal_nextshifts_64                  | 58 |\r\n| awkward_ListOffsetArray_reduce_nonlocal_nextstarts_64                  | 407 |\r\n| awkward_ListOffsetArray_reduce_nonlocal_outstartsstops_64              | 384 |\r\n| awkward_ListOffsetArray_reduce_nonlocal_preparenext_64                 | 407 |\r\n| awkward_ListOffsetArray_rpad_and_clip_axis1                            | 49 |\r\n| awkward_ListOffsetArray_rpad_axis1                                     | 34 |\r\n| awkward_ListOffsetArray_rpad_length_axis1                              | 34 |\r\n| awkward_ListOffsetArray_toRegularArray                                 | 138 |\r\n| awkward_MaskedArray_getitem_next_jagged_project                        | 34 |\r\n| awkward_NumpyArray_copy                                                | 87 |\r\n| awkward_NumpyArray_contiguous_copy                                     | 27 |\r\n| awkward_NumpyArray_contiguous_copy_from_many                           | 0 |\r\n| awkward_NumpyArray_contiguous_init                                     | 27 |\r\n| awkward_NumpyArray_contiguous_next                                     | 8 |\r\n| awkward_NumpyArray_fill                                                | 94486 |\r\n| awkward_NumpyArray_fill_tocomplex                                      | 1 |\r\n| awkward_NumpyArray_fill_fromcomplex                                    | 0 |\r\n| awkward_NumpyArray_fill_frombool                                       | 9384 |\r\n| awkward_NumpyArray_fill_tobool                                         | 9 |\r\n| awkward_NumpyArray_fill_scaled                                         | 46 |\r\n| awkward_NumpyArray_rearrange_shifted                                   | 19 |\r\n| awkward_NumpyArray_getitem_boolean_nonzero                             | 257 |\r\n| awkward_NumpyArray_getitem_boolean_numtrue                             | 257 |\r\n| awkward_NumpyArray_getitem_next_array                                  | 126 |\r\n| awkward_NumpyArray_getitem_next_array_advanced                         | 257 |\r\n| awkward_NumpyArray_getitem_next_at                                     | 1147 |\r\n| awkward_NumpyArray_getitem_next_null                                   | 9120 |\r\n| awkward_NumpyArray_getitem_next_range                                  | 429 |\r\n| awkward_NumpyArray_getitem_next_range_advanced                         | 7 |\r\n| awkward_NumpyArray_reduce_adjust_starts_64                             | 26 |\r\n| awkward_NumpyArray_reduce_adjust_starts_shifts_64                      | 49 |\r\n| awkward_NumpyArray_reduce_mask_ByteMaskedArray_64                      | 223 |\r\n| awkward_ListOffsetArray_argsort_strings                                | 0 |\r\n| awkward_NumpyArray_sort_asstrings_uint8                                | 5 |\r\n| awkward_NumpyArray_unique_strings                                      | 0 |\r\n| awkward_NumpyArray_subrange_equal                                      | 11 |\r\n| awkward_RegularArray_broadcast_tooffsets                               | 283 |\r\n| awkward_RegularArray_broadcast_tooffsets_size1                         | 256 |\r\n| awkward_RegularArray_combinations_64                                   | 15 |\r\n| awkward_RegularArray_compact_offsets                                   | 373 |\r\n| awkward_RegularArray_getitem_carry                                     | 1478 |\r\n| awkward_RegularArray_getitem_jagged_expand                             | 98 |\r\n| awkward_RegularArray_getitem_next_array                                | 1581 |\r\n| awkward_RegularArray_getitem_next_array_advanced                       | 268 |\r\n| awkward_RegularArray_getitem_next_array_regularize                     | 1852 |\r\n| awkward_RegularArray_getitem_next_at                                   | 2864 |\r\n| awkward_RegularArray_getitem_next_range                                | 1221 |\r\n| awkward_RegularArray_getitem_next_range_spreadadvanced                 | 2 |\r\n| awkward_RegularArray_localindex                                        | 3 |\r\n| awkward_RegularArray_num                                               | 19 |\r\n| awkward_RegularArray_rpad_and_clip_axis1                               | 30 |\r\n| awkward_UnionArray_fillindex                                           | 68 |\r\n| awkward_UnionArray_fillindex_count                                     | 113 |\r\n| awkward_UnionArray_fillna                                              | 29 |\r\n| awkward_UnionArray_filltags                                            | 68 |\r\n| awkward_UnionArray_filltags_const                                      | 113 |\r\n| awkward_UnionArray_flatten_combine                                     | 2 |\r\n| awkward_UnionArray_flatten_length                                      | 2 |\r\n| awkward_UnionArray_nestedfill_tags_index                               | 143 |\r\n| awkward_UnionArray_project                                             | 67 |\r\n| awkward_UnionArray_regular_index                                       | 14 |\r\n| awkward_UnionArray_regular_index_getsize                               | 11 |\r\n| awkward_UnionArray_simplify                                            | 8 |\r\n| awkward_UnionArray_simplify_one                                        | 632 |\r\n| awkward_UnionArray_validity                                            | 7 |\r\n| awkward_argsort                                                        | 47 |\r\n| awkward_quick_argsort                                                  | 0 |\r\n| awkward_carry_arange                                                   | 3565 |\r\n| awkward_combinations                                                   | 0 |\r\n| awkward_content_reduce_zeroparents_64                                  | 960 |\r\n| awkward_index_carry                                                    | 34 |\r\n| awkward_index_carry_nocheck                                            | 34 |\r\n| awkward_index_rpad_and_clip_axis0                                      | 80 |\r\n| awkward_index_rpad_and_clip_axis1                                      | 49 |\r\n| awkward_Index_nones_as_index                                           | 14 |\r\n| awkward_localindex                                                     | 9 |\r\n| awkward_missing_repeat                                                 | 262 |\r\n| awkward_new_Identities                                                 | 200 |\r\n| awkward_reduce_argmax                                                  | 14 |\r\n| awkward_reduce_argmax_complex                                          | 2 |\r\n| awkward_reduce_argmax_bool_64                                          | 0 |\r\n| awkward_reduce_argmin                                                  | 57 |\r\n| awkward_reduce_argmin_bool_64                                          | 0 |\r\n| awkward_reduce_argmin_complex                                          | 2 |\r\n| awkward_reduce_count_64                                                | 208 |\r\n| awkward_reduce_countnonzero                                            | 10 |\r\n| awkward_reduce_countnonzero_complex                                    | 5 |\r\n| awkward_reduce_max                                                     | 23 |\r\n| awkward_reduce_max_complex                                             | 6 |\r\n| awkward_reduce_min                                                     | 56 |\r\n| awkward_reduce_min_complex                                             | 8 |\r\n| awkward_reduce_prod                                                    | 127 |\r\n| awkward_reduce_prod_complex                                            | 8 |\r\n| awkward_reduce_prod_bool                                               | 94 |\r\n| awkward_reduce_prod_bool_complex                                       | 2 |\r\n| awkward_reduce_prod_int32_bool_64                                      | 0 |\r\n| awkward_reduce_prod_int64_bool_64                                      | 2 |\r\n| awkward_reduce_sum                                                     | 182 |\r\n| awkward_reduce_sum_complex                                             | 1 |\r\n| awkward_reduce_sum_bool                                                | 18 |\r\n| awkward_reduce_sum_bool_complex                                        | 2 |\r\n| awkward_reduce_sum_int32_bool_64                                       | 0 |\r\n| awkward_reduce_sum_int64_bool_64                                       | 2 |\r\n| awkward_regularize_arrayslice                                          | 387 |\r\n| awkward_slicearray_ravel                                               | 3398 |\r\n| awkward_slicemissing_check_same                                        | 0 |\r\n| awkward_quick_sort                                                     | 36 |\r\n| awkward_sort                                                           | 21 |\r\n| awkward_unique                                                         | 10 |\r\n| awkward_sorting_ranges                                                 | 93 |\r\n| awkward_sorting_ranges_length                                          | 93 |\r\n| awkward_one_mask                                                       | 0 |\r\n| awkward_zero_mask                                                      | 1 |\r\n",
  "created_at":"2021-07-23T15:14:17Z",
  "id":885709955,
  "issue":1032,
  "node_id":"IC_kwDODBCWws40ytyD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-23T15:14:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This was just to show how it can be done; the PR is no longer needed.",
  "created_at":"2021-08-09T13:23:18Z",
  "id":895219169,
  "issue":1032,
  "node_id":"IC_kwDODBCWws41W_Xh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T13:23:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Continuing work in #1044 ",
  "created_at":"2021-08-02T21:01:59Z",
  "id":891329850,
  "issue":1033,
  "node_id":"IC_kwDODBCWws41IJ06",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-02T21:01:59Z",
  "user":"MDQ6VXNlcjUyNjM1Nzcz"
 },
 {
  "author_association":"MEMBER",
  "body":"In fact, I have a browser tab open to https://citation-file-format.github.io/ The citation is in the Zenodo badge. A CITATION.cff file would bring more attention to it.",
  "created_at":"2021-07-28T14:09:29Z",
  "id":888340446,
  "issue":1034,
  "node_id":"IC_kwDODBCWws408v_e",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2021-07-28T14:09:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for the reminder, it didn't occur to me to check the badges for a DOI. It looks like we'd be duplicating some of our auto-generated content like contributor information etc. I wonder if there is an easy tool to integrate that, or whether we need to add something to the release process to generate a new `CITATION.cff`.",
  "created_at":"2021-07-28T14:18:38Z",
  "id":888347857,
  "issue":1034,
  "node_id":"IC_kwDODBCWws408xzR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-28T14:18:38Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Looking at the structure of a CITATION.cff, I think it could be hand-written. The DOI doesn't change: I have the badge linked to the versionless DOI, which always points to the latest version. I think it's actually a problem that Zenodo defines the authors to be the set of contributors to the repository, in part because some people contribute _a lot_ more than others (the variance is huge), but also because some people contributed to scikit-hep/awkward-0.x and not scikit-hep/awkward-1.0 (different repos). The contributors to scikit-hep/awkward-0.x are in the allcontributors list because I put them in there manually.\r\n\r\nBut if we want a shorter list for CITATION.cff, where do we draw the cut-off? I suppose this is why you were considering auto-generating the CITATION.cff, to include new authors as they appear, though the cut-off of people who made substantial contributions would be difficult to automate (number of lines changes?). Also, even though a lot of people made significant contributions, adding a big block of names in the references section of a paper only increases page count, and some journals include the references section in the page count that limits the lengths of articles.\r\n\r\nFor these practical reasons, perhaps it should be just \"Jim Pivarski _et al_\"? (With a strong emphasis on the \"_et al_\".) This way, it doesn't get too big in references sections and the link back here has pictures of everybody who contributed\u2014something people find if they click through the link, which is to say, if they're interested. For years, [ROOT was cited](https://github.com/root-project/root/#cite) with just two names (Rene and Fons), although the Zenodo link has 30 names (still much less than the number of people who have contributed to ROOT, 282). I think those 30 names are people who are now or have been on the [ROOT Team](https://root.cern/about/team/) page, but Awkward Array doesn't have a well-defined team.\r\n\r\nActually, let me put a bit more thought into this. Some people have been funded to work on Awkward Array. They are:\r\n\r\n   * @jpivarski / Jim Pivarski, Princeton\r\n   * @ianna / Ianna Osborne, Princeton\r\n   * @henryiii / Henry Schreiner, Princeton\r\n   * @ioanaif / Ioana Ifrim, Princeton\r\n   * @trickarcher / Anish Biswas, Manipal Institute of Technology (as GSoC and IRIS-HEP Fellow)\r\n   * @reikdas / Pratyush Das,, Institute Of Engineering and Management, Kolkata (at that time, now Purdue, as an IRIS-HEP Fellow)\r\n   * @EscottC / Charles Escott, Sacred Heart University (at that time, now Columbia, as a GSoC student)\r\n   * @Jayd-1234 / Jaydeep Nandi, National Institute of Technology, Silchar (as a GSoC student, though his involvement was at the very beginning of when it was called \"Awkward Array\")\r\n\r\nBut using that to define the list wouldn't count people who made substantial contributions, like @nsmith-, @drahnreb, @veprbl, @chrisburr, and you.\r\n\r\nNow that you're all linked into this thread, maybe you have opinions/preferences? Am I worrying too much about making the reference a big block of names by including everybody? ([27 people in .all-contributorsrc](https://github.com/scikit-hep/awkward-1.0/blob/main/.all-contributorsrc), which is less than ROOT's curated list of 30 names.)",
  "created_at":"2021-07-28T15:19:46Z",
  "id":888397855,
  "issue":1034,
  "node_id":"IC_kwDODBCWws408-Af",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-28T15:19:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"My personal take is favouring simplicity over adding infrastructure to keep track of a citation file, but I'm perhaps a bit blas\u00e9 when it comes to the premise in the first place. So, at least for my name, +0 on including in a CITATION.cff file, and `+1` on keeping it simple with Jim Pivarski *et al*.\r\n\r\n",
  "created_at":"2021-07-30T11:15:28Z",
  "id":889823189,
  "issue":1034,
  "node_id":"IC_kwDODBCWws41CZ_V",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-30T11:15:28Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">  +1 on keeping it simple with Jim Pivarski et al.\r\n\r\n+1",
  "created_at":"2021-07-30T11:20:39Z",
  "id":889825851,
  "issue":1034,
  "node_id":"IC_kwDODBCWws41Cao7",
  "performed_via_github_app":null,
  "reactions":{
   "+1":3,
   "total_count":3
  },
  "updated_at":"2021-07-30T11:20:39Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"One irritating thing is it seems the CITATION file has the version number in it - having another place (sometimes the only place) to change a version number is really a step backward for some projects. \"Last GitHub release\" would be really nice instead. :)\r\n\r\nFor names, I tend to prefer keeping a full author list then using a rule to truncate all of them (X names et. al. after a limit of Y names), but for software, this likely is hard to keep up (if all-contributors adds support, that might be really nice). Maybe the top X(=3?) contributors on GitHub (they are sorted in the GH UI), or anyone who has contributed more than X% (%10?) of the code, etc. I think the rule proposed above could be \"list of people who have creative control over the software, et al\". I'm mostly looking for a way to make this a suggestion that can apply to other projects. Some projects have more than one core developer; boost-histogram would likely need to be @HDembinski and me at the least. It would be nice to adopt a community guideline. (@danielskatz might have suggestions or input here?) - I'd be happy to propose a page to scikit-hep.org/developer .",
  "created_at":"2021-07-30T13:32:04Z",
  "id":889895768,
  "issue":1034,
  "node_id":"IC_kwDODBCWws41CrtY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T13:32:47Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"NONE",
  "body":"It would be more effective to open an issue in https://github.com/citation-file-format/citation-file-format for feature requests or format changes than to just comment about them here :)",
  "created_at":"2021-07-30T13:38:19Z",
  "id":889899420,
  "issue":1034,
  "node_id":"IC_kwDODBCWws41Csmc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T13:38:33Z",
  "user":"MDQ6VXNlcjI5MTM4NDU="
 },
 {
  "author_association":"NONE",
  "body":"perhaps a feature request to all-contributors too?",
  "created_at":"2021-07-30T13:39:06Z",
  "id":889899860,
  "issue":1034,
  "node_id":"IC_kwDODBCWws41CstU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T13:39:06Z",
  "user":"MDQ6VXNlcjI5MTM4NDU="
 },
 {
  "author_association":"NONE",
  "body":"For [Parsl](https://github.com/Parsl/parsl), we decided that all repo contributors with non-trivial/typo changes would be added as authors, in addition to other non-repo contributors ",
  "created_at":"2021-07-30T13:40:50Z",
  "id":889900858,
  "issue":1034,
  "node_id":"IC_kwDODBCWws41Cs86",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T13:40:50Z",
  "user":"MDQ6VXNlcjI5MTM4NDU="
 },
 {
  "author_association":"MEMBER",
  "body":"> It would be more effective to open an issue\r\n\r\nI haven't even set this up in a single repo yet, so probably not quite ready for that, but yes, I will if it's a problem. I first might just see what it does if you leave that blank, for example.",
  "created_at":"2021-07-30T13:47:29Z",
  "id":889904688,
  "issue":1034,
  "node_id":"IC_kwDODBCWws41Ct4w",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-30T13:47:29Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":":wave: Joining this Issue's discussion given thoughts / rough edges in https://github.com/scikit-hep/pyhf/issues/1541. I think one of the problems is that GitHub isn't being very clear on what parts of the CFF spec they support, as it clearly isn't all of it, but only GitHub can answer that so we'll need to open an Issue.",
  "created_at":"2021-07-31T23:43:04Z",
  "id":890417377,
  "issue":1034,
  "node_id":"IC_kwDODBCWws41ErDh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-31T23:43:04Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"During the release of `pyhf` `v0.6.3` we noticed some unexpected issues with `CITATION.cff` that we didn't have with `.zenodo.json` (c.f. https://github.com/scikit-hep/pyhf/issues/1541#issuecomment-913992537).",
  "created_at":"2021-09-07T05:06:35Z",
  "id":913993272,
  "issue":1034,
  "node_id":"IC_kwDODBCWws42em44",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-07T05:06:35Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Good to see that this is started. Keep in mind that \"refcount\" tests can simply be deleted (it's in the Google Doc). The C++ code was heavily tested for reference counts because reference counting between Python and C++ had to be carefully managed. Since the new implementation is all in Python, reference counting is completely done for us and we can ignore those tests.\r\n\r\nThat's why it's a good idea to copy the old tests, rather than changing them in place. We still want the old tests to be there in the interim, but there's a lot that we can drop for v2.",
  "created_at":"2021-07-29T09:23:44Z",
  "id":888956357,
  "issue":1036,
  "node_id":"IC_kwDODBCWws40_GXF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-29T09:23:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is being addressed in v2, as an application of a simplified `recursively_apply` interface.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/8981e2dd1ac813d62d46c3d98b5d38007803c954/src/awkward/_v2/operations/structure/from_regular.py#L38-L70",
  "created_at":"2021-11-03T19:25:18Z",
  "id":959850781,
  "issue":1037,
  "node_id":"IC_kwDODBCWws45Nikd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-03T19:25:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I took a brief look at the `arrayclass` function to gauge what would need to be done here. It looks like this might require a little bit of thought, because we currently do not seem to expose a means to determine the depth at which a `__record__` parameter was found. For this reason, it might be prudent to wait until v2 is done.",
  "created_at":"2021-07-30T11:36:25Z",
  "id":889833538,
  "issue":1038,
  "node_id":"IC_kwDODBCWws41CchC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T11:36:25Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I guess it's not quite like regular expressions.\r\n\r\nSince `\".\"` already means \"one level\" and `\"*\"` means \"any number of levels,\" how about\r\n\r\n   * `\".\", \".\"` for \"two levels\"\r\n   * `\".\", \".\", \".\"` for \"three levels\"\r\n   * `\"*\", \".\"` for \"two or more levels\"\r\n   * `\"*\", \".\", \".\"` for \"three or more levels\"\r\n\r\n? I don't think it would be necessary to introduce a number here, to simplify the problem of asking for \"57 levels,\" since that kind of thing is very unlikely. I can't imagine requirements needing to specify anything bigger than 3.",
  "created_at":"2021-07-30T14:01:22Z",
  "id":889913286,
  "issue":1038,
  "node_id":"IC_kwDODBCWws41Cv_G",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-30T14:01:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I guess it's not quite like regular expressions.\r\n\r\n@jpivarski yes, I don't mean that we should copy regex, rather that if we have similar syntax, it's going to confuse \"some\" people (myself included ;))\r\n\r\nI was originally in favour of using `.` to mean additional levels, so that suggestion here has a +1 from me. I thought you disproved of this syntax, but perhaps I misunderstood you. I'll add the suggestion to the issue description.\r\n\r\n>  I don't think it would be necessary to introduce a number here\r\n\r\nAnything with >5 dimensions is probably getting silly!\r\n\r\n",
  "created_at":"2021-07-30T14:27:00Z",
  "id":889929939,
  "issue":1038,
  "node_id":"IC_kwDODBCWws41C0DT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T14:27:31Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for the starting point Jim. If you're happy that we need to extend the content API in order to facilitate #1038, that's helpful!\r\n\r\nIt looks like this particular PR would be used in `arrayclass` like\r\n```python3\r\n\r\nfor i in range(layout.purelist_depth):\r\n    record = layout.nlist_parameter(i)\r\n    if record is not None:\r\n        break\r\nelse:\r\n    raise ValueError(\"no record found\")\r\n```\r\n\r\nWe would end up visiting the outer contents multiple times in this manner. Are you averse to having a `purelist_parameter_depth` which returns a `std::pair`with the parameter and the depth at which it was found? ",
  "created_at":"2021-07-30T14:38:41Z",
  "id":889937448,
  "issue":1039,
  "node_id":"IC_kwDODBCWws41C14o",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T14:39:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> We would end up visiting the outer contents multiple times in this manner. Are you averse to having a `purelist_parameter_depth` which returns a `std::pair`with the parameter and the depth at which it was found?\r\n\r\nI'm not averse to that. (pybind turns `std::pair` into a Python tuple.) But also, there's no reason to avoid walking over a tree structure multiple times. The one large order parameter that we preemptively optimize for is the length of the arrays\u2014that's why we have kernels separated out and rules against iterating over data in the middle layer, etc. But the size of a tree of layout nodes is not considered a parameter that can scale large, since it scales with the complexity of the data _type_, not the size of the dataset, so we don't need to optimize traversals over it. Walk over it 20 times if makes the code simpler.\r\n\r\n(That was a hard thing for me to get used to when I started programming in Lisp. Lisp was gleefully traversing and copying data structures that would have been more carefully examined in C or Pascal, and at first it seemed terribly wasteful. But then I saw how much easier it was to do things, not having to worry about minimizing the computer's work all the time, and a lot of data really is small. So really, it's more about identifying the few things you _do_ need to worry about as order parameters that can scale large, and everything else is written for clarity, rather than speed.)",
  "created_at":"2021-07-30T15:00:18Z",
  "id":889951089,
  "issue":1039,
  "node_id":"IC_kwDODBCWws41C5Nx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T15:00:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> But also, there's no reason to avoid walking over a tree structure multiple times.\r\n\r\nI don't disagree here, but I think in this case it might make implementation simpler \u2014 is it more common to want a parameter at a particular depth, or at any depth (but the depth needs to be known)? If the latter, then extending the `purelist_parameter` to give the depth might be helpful. ",
  "created_at":"2021-07-30T15:16:52Z",
  "id":889962233,
  "issue":1039,
  "node_id":"IC_kwDODBCWws41C775",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T15:16:52Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, as long as it's simpler for humans. My main point being that this particular place is not one to consider performance-optimizing.",
  "created_at":"2021-07-30T15:21:13Z",
  "id":889964951,
  "issue":1039,
  "node_id":"IC_kwDODBCWws41C8mX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T15:21:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Okay, as long as it's simpler for humans. My main point being that this particular place is not one to consider performance-optimizing.\r\n\r\nI think it was a point worth stating \u2014 it's easy to get out of the mindset of \"only the (important) loops matter\", and in this case, the dimensions are *not* the important loops!",
  "created_at":"2021-07-30T15:32:30Z",
  "id":889972057,
  "issue":1039,
  "node_id":"IC_kwDODBCWws41C-VZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T15:32:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"That was quick, thanks Jim. I had not realised that we had this broadcasting behaviour between record arrays and non-record arrays. Seeing this PR encouraged me to verify the behaviour, and initially it took me by surprise.\r\n\r\n> Maybe the rule should be \"any record\" rather than \"all records\"? That's why this PR is a draft.\r\n\r\nWell, I keep flip-flopping on this question. I think we should perhaps offer a `promote_scalars_to_records=True` argument to `broadcast_arrays`, so that if users *don't* want this, they can opt-out? \r\n\r\nI was at one point concerned about the existing promotion behaviour, but after considering nested record arrays, it is the only sensible way to perform promotion, so I'm in favour of it.",
  "created_at":"2021-07-30T14:58:39Z",
  "id":889949977,
  "issue":1040,
  "node_id":"IC_kwDODBCWws41C48Z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T15:14:12Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"This _was_ intentional. Loosening it from checking parameters to checking purelist_parameters seems to make sense in this case, since the IndexedArray should probably be invisible.\r\n\r\nBut in general, it shouldn't be _purelist_ parameters: the same parameters on different dimensions should count as distinct types. What would be ideal here would be to require all the parameters at this dimension of `self` to be equal to all the parameters at this dimension of `other`, and then that single set of parameters would be what is passed on to the merged output.\r\n\r\nSo in other words, a generalized `parameters_equal` method that\r\n\r\n   * considers index nodes and option-type nodes to be invisible (IndexedArray, IndexedOptionArray, BitMaskedArray, ByteMaskedArray, UnmaskedArray); merging their parameters with their content's (parent wins in any conflicts)\r\n   * considers UnionArrays to be invisible unless the different branches of the union have different parameters\r\n   * considers lists (ListArray, ListOffsetArray, RegularArray) and RecordArrays to be opaque\u2014does not descend.\r\n   * VirtualArrays would be invisible, though it should go through Forms to ensure that VirtualArrays are not materialized early.\r\n   * you can't descend below NumpyArrays or EmptyArrays\r\n\r\nand that's all the types.",
  "created_at":"2021-07-30T16:49:26Z",
  "id":890021075,
  "issue":1041,
  "node_id":"IC_kwDODBCWws41DKTT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T16:49:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ok, this marries with my understanding of things - purelist would otherwise consider inner dimensions too eagerly! I'll add this to the back burner! ",
  "created_at":"2021-07-30T20:12:49Z",
  "id":890129474,
  "issue":1041,
  "node_id":"IC_kwDODBCWws41DkxC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T20:12:49Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@SantamRC - are you still working on this PR? Please, close it if not. Thanks!",
  "created_at":"2021-09-03T14:07:07Z",
  "id":912567077,
  "issue":1044,
  "node_id":"IC_kwDODBCWws42ZKsl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-03T14:07:07Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ianna I am working on getting the data for the remaining kernel functions right now. Using this PR for it. Will close after it's done.\r\n\r\n",
  "created_at":"2021-09-03T14:13:34Z",
  "id":912571589,
  "issue":1044,
  "node_id":"IC_kwDODBCWws42ZLzF",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-09-03T14:13:34Z",
  "user":"MDQ6VXNlcjUyNjM1Nzcz"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - `LayoutBuilder` code is clean from dependencies on the `Content` classes, `Type`, and `Form` classes. It takes a JSON form. I will need to revisit `pybind11` part that still have some dependencies on the `Content` classes.",
  "created_at":"2021-08-17T21:43:04Z",
  "id":900651304,
  "issue":1045,
  "node_id":"IC_kwDODBCWws41rtko",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-17T21:43:04Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, you can do the honors and press \"squash and merge\" (so that I don't press it while you're adding one last commit or something).",
  "created_at":"2021-08-17T22:09:25Z",
  "id":900664360,
  "issue":1045,
  "node_id":"IC_kwDODBCWws41rwwo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-17T22:09:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Congrats on merging this, big PR!",
  "created_at":"2021-08-18T09:31:39Z",
  "id":900966035,
  "issue":1045,
  "node_id":"IC_kwDODBCWws41s6aT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-18T09:31:39Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"It's because `ak.Array([[1], [2]])` is an array of variable-length lists and `np.array([[1], [2]])` is an array of fixed-length lists:\r\n\r\n```python\r\n>>> ak.Array([[1], [2]])                   # note the \"var\"\r\n<Array [[1], [2]] type='2 * var * int64'>\r\n>>> \r\n>>> ak.Array(np.array([[1], [2]]))         # now it's regular\r\n<Array [[1], [2]] type='2 * 1 * int64'>\r\n```\r\n\r\nThe empty slice maintains var on the var-type array and the regular length of the regular array:\r\n\r\n```python\r\n>>> ak.Array([[1], [2]])[[]]\r\n<Array [] type='0 * var * int64'>\r\n>>> \r\n>>> ak.Array(np.array([[1], [2]]))[[]]\r\n<Array [] type='0 * 1 * int64'>\r\n```\r\n\r\nAnd this is preserved when converting to NumPy. Var-type arrays can only be converted to NumPy if they _happen to be regular in the instance you're trying to convert_. (Regular arrays can always be converted to NumPy.) The lengths of the inner dimensions of a var-type array are determined from what those list lengths happen to be in the particular instance. Here, they happen to be zero because they're empty.\r\n\r\n```python\r\n>>> ak.to_numpy(ak.Array([[1], [2]])[[]])\r\narray([], shape=(0, 0), dtype=int64)\r\n>>> \r\n>>> ak.to_numpy(ak.Array(np.array([[1], [2]]))[[]])\r\narray([], shape=(0, 1), dtype=int64)\r\n```\r\n\r\nIt's unfortunate that we have to have this subtlety\u2014that you, as user, have to be aware of the distinction between var-type and regular-type nested lists, but this is because only the regular-type can perfectly correspond to NumPy's behavior; var-type is new territory.\r\n\r\n~~Let me know if I should reopen this, but I think it answers your question.~~ Actually, no, I'm going to convert it to a Q&A discussion so that it remains visible. Others might encounter this, and I don't want to bury it by making it a closed issue.",
  "created_at":"2021-08-04T13:39:04Z",
  "id":892666781,
  "issue":1046,
  "node_id":"IC_kwDODBCWws41NQOd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-04T13:39:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I wanted to check if this is still the case in v2 and stumbled an another issue with `EmptyArray`s in v2 (fixed in https://github.com/scikit-hep/awkward-1.0/pull/1150/commits/54c55cd8ab2bf7890823527a7f15023463b1c3bf):\r\n```python\r\n>>> array = ak._v2.highlevel.Array([[], [], []])\r\n>>> ak.to_list(array.layout.toRegularArray())\r\n[[], [], [], []]\r\n>>> array = ak._v2.highlevel.Array([[1], [2], [3]])\r\n>>> ak.to_list(array.layout.toRegularArray())\r\n[[1], [2], [3]]\r\n>>> array = ak.Array([[], [], []])\r\n>>> ak.to_list(array.layout.toRegularArray())\r\n[[], [], []]\r\n```",
  "created_at":"2021-11-17T17:10:56Z",
  "id":971783518,
  "issue":1048,
  "node_id":"IC_kwDODBCWws457D1e",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-17T17:18:24Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"For more detail, here's where `ListOffsetArray.toRegularArray` is going wrong:\r\n\r\n```python\r\n>>> array = ak._v2.highlevel.Array([[], [], []])\r\n>>> array.layout\r\n<ListOffsetArray len='3'>\r\n    <offsets><Index dtype='int64' len='4'>[0 0 0 0]</Index></offsets>\r\n    <content><EmptyArray len='0'/></content>\r\n</ListOffsetArray>\r\n>>> array.layout.toRegularArray()\r\n<RegularArray size='0' len='4'>\r\n    <content><EmptyArray len='0'/></content>\r\n</RegularArray>\r\n```\r\n\r\nThe length of the ListOffsetArray is 3, but the length of the RegularArray is 4.\r\n\r\nHere's the new Python code with the error:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/7705900c44db2ced202ae61ce605f9d7ea536f4a/src/awkward/_v2/contents/listoffsetarray.py#L159-L165\r\n\r\nThe length of the RegularArray that gets returned should be `len(self)` or `len(self._offsets) - 1` (because offsets are one longer than the ListOffsetArray itself).\r\n\r\nHere's the original C++ code that it was translated from:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/7705900c44db2ced202ae61ce605f9d7ea536f4a/src/libawkward/array/ListOffsetArray.cpp#L383-L387\r\n\r\nThis uses `length()`, a function that doesn't exist in the Python version but means `len(self)`. That's how the error was made.\r\n\r\n@ianna Are you working on something where it would be easy to include this fix, or should I?",
  "created_at":"2021-11-17T17:26:54Z",
  "id":971797077,
  "issue":1048,
  "node_id":"IC_kwDODBCWws457HJV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-17T17:26:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna You already fixed it in #1150 (which I'm reviewing right now). Exactly this fix is included in the diff.\r\n\r\nWe should keep this issue here because it was for a different problem, the `to_numpy` shape issue. (Or is that handled by #1150? I'll check. Nope; the issue is still there in `ianna/to_numpy`, at the time of writing.)",
  "created_at":"2021-11-17T17:32:26Z",
  "id":971801725,
  "issue":1048,
  "node_id":"IC_kwDODBCWws457IR9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-17T17:32:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This seems sensible; I can understand why you'd want the timestamp in the source code to not vary with time. The documentation on reproducible builds talk about reproducing the application binary, and both of these timestamps go into comments, which cannot affect the binaries, but I can see it being good to make it regular, just on principle.\r\n\r\nI see that there's a fallback to `time.time()` if `SOURCE_DATE_EPOCH` is not defined, which is good. I don't know under what conditions the variable is defined (only during compilation?), but the fallback just reproduces the previous behavior, so I have no complaints.\r\n\r\nAnyway, I'm happy with this PR and will merge it if I'm sure you're done. If you don't plan to make any more changes, let me know in a comment and I'll merge it into the codebase. Thanks!",
  "created_at":"2021-08-09T13:21:51Z",
  "id":895218031,
  "issue":1050,
  "node_id":"IC_kwDODBCWws41W_Fv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T13:21:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Being able to trust binaries is the primary goal, but in the context of the openSUSE distribution, we also have python scripts and documentation all bundled into rpm packages and it is easier to reason for trust of our users if we are able to reproduce every bit in our packages and long term even the whole images.",
  "created_at":"2021-08-12T12:33:20Z",
  "id":897601460,
  "issue":1050,
  "node_id":"IC_kwDODBCWws41gE-0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-12T12:33:20Z",
  "user":"MDQ6VXNlcjYzNzk5MA=="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @bmwiedemann for code",
  "created_at":"2021-09-09T16:59:35Z",
  "id":916274798,
  "issue":1050,
  "node_id":"IC_kwDODBCWws42nT5u",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T16:59:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/1087) to add @bmwiedemann! :tada:",
  "created_at":"2021-09-09T16:59:43Z",
  "id":916274891,
  "issue":1050,
  "node_id":"IC_kwDODBCWws42nT7L",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T16:59:43Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"RE the lazy aspect to this question, I assume what's happening here is that `ak.with_field` is materialising the virtual array when broadcasting. ",
  "created_at":"2021-08-09T14:56:36Z",
  "id":895294229,
  "issue":1052,
  "node_id":"IC_kwDODBCWws41XRsV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T14:56:36Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> RE the lazy aspect to this question, I assume what's happening here is that `ak.with_field` is materialising the virtual array when broadcasting.\r\n\r\nThat's right.\r\n\r\nAssigning an array `x` as a new field of another array `y` necessarily evaluates any generators in `x` and `y` because it needs to broadcast them to see how to restructure `x` to fit `y`. That's so that something like\r\n\r\n```python\r\narray_of_3vectors[\"mass\"] = 125.0\r\n```\r\n\r\ncan set a `\"mass\"` field with the same value in all records in `array_of_3vectors`, regardless of whether `array_of_3vectors` is an array of records, an array of lists of records, or something deeper. Similarly,\r\n\r\n```python\r\narray_of_lists_of_lists[\"new_field\"] = array_of_lists\r\n```\r\n\r\nwill broadcast each element from the shallower `array_of_lists` into the corresponding `array_of_lists_of_lists`.\r\n\r\nThis is a fundamental problem with our design of lazy arrays: the idea is that it will be materialized when needed, but it can be hard for users to guess when it's needed. (Maybe you didn't know that field-assignment broadcasts, for instance.) There's a long history of bug-fixes for lazy arrays being materialized \"too early,\" but without a clear definition of when _is_ \"too early.\" In some cases, it was obvious: some code was just checking something that could be determined from type alone, such as the value of the `\"__record__\"` parameter. Materializing a VirtualArray node for that would be wrong, because the `\"__record__\"` parameter is expressed in the VirtualArray's Form, and we can search down that tree instead. Other cases have been more borderline: should a slice materialize a VirtualArray? It's a calculation on an array, but most people didn't expect it to materialize an array, especially when the slice was just picking out one field (e.g. `my_array.field`). Now a slice of a VirtualArray creates a new VirtualArray that would cascade through evaluating the slice, then evaluating the original array.\r\n\r\nBut for broadcasting, we can't delay that. To delay a calculation, we have to be able to say what its Form is going to be without actually performing the calculation. There are even some slices in which that's not possible: the Form of the output depends on the specific values of the array. (That is, if some lists are empty or well-aligned or something, the output would have a different Form than if they weren't, and in order to make a new VirtualArray, we have to predict the Form it would have upon evaluation.)\r\n\r\nDask solves the lazy array problem differently, and that's where we're directing new effort on this:\r\n\r\n![](https://raw.githubusercontent.com/jpivarski-talks/2021-05-21-dasksummit-awkward-collection/main/degrees-of-transparency-2.svg)\r\n\r\nOne difference is that a Dask collection has a `.compute()` method, so you get to say when it's the right time to start a calculation. As a consequence, the Dask DAG can be a different thing from an ordinary array (i.e. `.compute()` returns a different type, mapping DAG \u2192 in-memory array), so it doesn't need to support all the same operations that an ordinary array supports. Perhaps there would be no way to ask a DAG what the array type will be without computing it, for instance. But this would relax constraints on what can go into the DAG so that any operation can be lazy.\r\n\r\nDevelopment of an Awkward-Dask collection is starting next month.\r\n\r\n-----------------\r\n\r\nAs for solving your problem: are you trying to make a record array with virtual fields? Instead of assigning new fields into an existing object, you could create the array all at once by calling the `ak.Array` constructor:\r\n\r\n```python\r\narray = ak.Array({\"field1\": ak.virtual(...), \"field2\": ak.virtual(...)})\r\n```\r\n\r\nor `ak.zip`, limited to the non-virtual, list-like dimensions:\r\n\r\n```python\r\narray = ak.zip({\"field1\": ak.virtual(...), \"field2\": ak.virtual(...)}, depth_limit=1)\r\n```\r\n\r\nIf the fields were _lists of virtual_ data (i.e. the list offsets are not virtual, but the contents are), then you could use `depth_limit=2` to zip those lists together without materializing the virtual data. `depth_limit=1` is equivalent to the `ak.Array` constructor.\r\n\r\n",
  "created_at":"2021-08-09T15:25:31Z",
  "id":895318085,
  "issue":1052,
  "node_id":"IC_kwDODBCWws41XXhF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T15:25:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Zero-dimensional shapes were deliberately left out, since this is how we express a scalar result. That is, we don't make a distinction between what NumPy would call `float(0)`, `np.float64(0)`, and `np.array(0, np.float64)`. In Awkward 1.x, scalars come out as Python numbers (e.g. `float(0)`), but in Awkward 2.x development, we've started returning scalars as NumPy scalars (e.g. `np.float64(0)`), to allow results to keep their type.\r\n\r\nI don't think the distinction NumPy makes between `np.float64(0)` and `np.array(0, np.float64)` is a useful one to make: neither is an array in the sense of having a length or being able to access items with square brackets. It _is_ useful to be able to describe a scalar with types of different precision, though.\r\n\r\n`ak.fill_none` has changed since 1.4.0; it now uses the type hint. (I remember that being a long-standing wish-list item, but I can't find the closed issue.)\r\n\r\n```python\r\n>>> ak.fill_none(a, np.float32(0))\r\n<Array [1, 0] type='2 * float32'>\r\n>>> ak.fill_none(a, np.array(0, np.float32))\r\n<Array [1, 0] type='2 * float32'>\r\n```\r\n\r\nOn the other hand, it looks like we broke filling with a NumPy array in doing so:\r\n\r\n```python\r\n>>> ak.fill_none(a, np.array([0], np.float32), axis=-1)\r\n<Array [1, 0] type='2 * float32'>\r\n>>> ak.fill_none(a, np.array([[0]], np.float32), axis=-1)\r\n<Array [1, [0]] type='2 * union[float32, 1 * float32]'>\r\n```\r\n\r\nThat's off by one dimension. There's no such trouble when the fill value is a non-NumPy iterable.\r\n\r\n```python\r\n>>> ak.fill_none(a, 0, axis=-1)\r\n<Array [1, 0] type='2 * float64'>\r\n>>> ak.fill_none(a, [0], axis=-1)\r\n<Array [1, [0]] type='2 * union[float32, 1 * int64]'>\r\n>>> ak.fill_none(a, [[0]], axis=-1)\r\n<Array [1, [[0]]] type='2 * union[float32, 1 * var * int64]'>\r\n```\r\n\r\nI bet that will be an easy fix; I'll do it now.",
  "created_at":"2021-08-10T12:49:28Z",
  "id":895999667,
  "issue":1055,
  "node_id":"IC_kwDODBCWws41Z96z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-10T12:49:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is what's supposed to happen:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/24397d8360a6760ebc99e8122aa25e799b6c02f9/tests/test_1055-fill_none-numpy-dimension.py#L11-L38\r\n\r\n> #### In case that there is supposed to be support for shapeless arrays, this illustrates the inconsistency:\r\n> ```python\r\n> a = np.array(0) # value: array(0)\r\n> b = ak.Array(a) # value: <Array [0] type='1 * int64'>\r\n> c = ak.to_numpy(b) # value: array([0])\r\n> assert a.shape == c.shape # this fails\r\n> ```\r\n\r\nOn this point, I suppose we could make high-level functions (e.g. `ak.from_numpy`, `ak.Array` constructor) _refuse_ to convert zero-dimensional arrays. Then, at least, it wouldn't give the impression of recognizing zero-dimensional arrays?",
  "created_at":"2021-08-10T13:13:32Z",
  "id":896017089,
  "issue":1055,
  "node_id":"IC_kwDODBCWws41aCLB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-10T13:13:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the quick and helpful replies.\r\n\r\n> [...]\r\n> On this point, I suppose we could make high-level functions (e.g. `ak.from_numpy`, `ak.Array` constructor) _refuse_ to convert zero-dimensional arrays. Then, at least, it wouldn't give the impression of recognizing zero-dimensional arrays?\r\n\r\nThis seems like a good idea.",
  "created_at":"2021-08-10T13:25:42Z",
  "id":896027116,
  "issue":1055,
  "node_id":"IC_kwDODBCWws41aEns",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-10T13:25:42Z",
  "user":"MDQ6VXNlcjE1NjUxMTUw"
 },
 {
  "author_association":"MEMBER",
  "body":"That sounds good. The original idea was that if you wanted to know where in your code it's happening, you'd turn the warning into an exception (that's why instructions are given in the warning text), and the exception would give you a full stack trace.\r\n\r\nWill `stacklevel=3` always be exactly the right place? If I'm counting right, `stacklevel=1` is in `ak._util.deprecate` (the least useful place), `stacklevel=2` is in the `ak.*` function that called it, and `stacklevel=3` is in the user's code that called _that_. But now we'll have to be careful that `ak._util.deprecate` is always only called directly in an `ak.*` function, not inside a function called by an `ak.*` function.\r\n\r\nIt's unfortunate that Python doesn't know that we're drawing a dotted line between the Awkward library code and user code. Or is there an alternative that says, \"climb the stack trace until a function's `__module__` doesn't satisfy `__module__.startswith(\"awkward.\")`?\" It doesn't look like there is...\r\n\r\nWe use `ak._util.deprecate` infrequently, so maybe the thing to do is to just include a comment warning/reminding us about this constraint. Or maybe `stacklevel` should be passed into the `ak._util.deprecate` function as a required argument, so that we can make it `stacklevel=4` if we put it in something nested.",
  "created_at":"2021-08-11T17:36:23Z",
  "id":897018490,
  "issue":1058,
  "node_id":"IC_kwDODBCWws41d2p6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-11T17:36:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I've added a `stacklevel` kwarg to `ak._util.deprecate` that functions similar to `warnings.warn`: A `stacklevel` of 1 would report the position of the `ak._util.deprecate` call. However, I've set the default to 2 since we are mainly interested in reporting the position within the user's code. For cases an internal function (i.e. a function indirectly called by the user) is deprecated we can properly modify the `stacklevel` (e.g. set it to 3), in so far it is possible/deterministic. Other cases could be handled by a generic implementation that crawls the stack, as you have already outlined above - however, for now, this would be overkill i think.",
  "created_at":"2021-08-12T09:59:01Z",
  "id":897506625,
  "issue":1058,
  "node_id":"IC_kwDODBCWws41ft1B",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-12T09:59:01Z",
  "user":"MDQ6VXNlcjE1NjUxMTUw"
 },
 {
  "author_association":"MEMBER",
  "body":"> Other cases could be handled by a generic implementation that crawls the stack, as you have already outlined above - however, for now, this would be overkill i think.\r\n\r\nI agree. As an internal function, we're always the ones who will be calling it, so we'll just be careful to set the `stacklevel` appropriately.\r\n\r\nThis looks done; I'll be merging when the tests all pass.",
  "created_at":"2021-08-12T12:32:23Z",
  "id":897600770,
  "issue":1058,
  "node_id":"IC_kwDODBCWws41gE0C",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-12T12:32:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yup, you can merge it, I don't have any other additions. Thanks for the review!",
  "created_at":"2021-08-23T17:27:11Z",
  "id":903968776,
  "issue":1059,
  "node_id":"IC_kwDODBCWws414XgI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-23T17:27:11Z",
  "user":"MDQ6VXNlcjg4Mjg5MDg2"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Probably it would be more correct to compare:\r\n```python\r\nawkward.Array({\"record\": awkward.Record({\"column\": [1]})})\r\n```\r\nand\r\n```python\r\nawkward.Array([awkward.Record({\"column\": [1]})])\r\n```\r\nboth of which fail in the same way",
  "created_at":"2021-08-15T01:58:28Z",
  "id":898983177,
  "issue":1060,
  "node_id":"IC_kwDODBCWws41lWUJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-15T01:58:28Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"This one isn't valid because passing a dict to an `ak.Array` constructor takes each key as a column name, and therefore each value should be an iterable. A record is not iterable.\r\n\r\n```python\r\nawkward.Array({\"record\": awkward.Record({\"column\": [1]})})\r\n```\r\n\r\nPerhaps this one ought to work:\r\n\r\n```python\r\nawkward.Array([awkward.Record({\"column\": [1]})])\r\n```\r\n\r\nHowever, to get to the array you're trying to make, all your have to do is this:\r\n\r\n```python\r\nak.Array([{\"column\": [1]}])\r\n```\r\n\r\n`ak.Array` and `ak.Record` objects are only recognized in `ak.Array` and `ak.Record` constructors if they're the only object in the constructor, and then all it does is unwrap the old one and wrap it in a new `ak.Array` or `ak.Record`. if they're nested inside a list or dict, they're tested as generic iterables (`ak.Array`) or mappings (`ak.Record`).\r\n\r\nI'll look into these when I get to a computer. They might need better error messages (catching invalid cases earlier to provide better feedback), and your second example looks like it should work, though there's no advantage to writing it like that. (For instance, there's no performance advantage to iterating over a list of `ak.Record` than a list of dicts. In fact, the latter is significantly faster because it's a Python built-in.)\r\n",
  "created_at":"2021-08-15T13:55:02Z",
  "id":899054379,
  "issue":1060,
  "node_id":"IC_kwDODBCWws41lnsr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-15T13:55:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thank you for taking a look. The examples I provide are only MRE's, I'm not looking to define the data in my code, but to build an array from existing awkward arrays. Right now I had to settle for calling `.tolist()` to be able to build the combined array.\r\n\r\nIf I'm not mistaken this issue discusses behaviour of `awkward.from_iter`, and it already appears to have \"non-iter\" handling of dicts (remember that iterating over a dict doesn't give the full items: `list(dict(a=1)) == ['a']`). It would make sense that it could also properly treat `awkward.Record`'s. This just from the point of API, hopefully inside it could do something more optimal with the existing arrays.",
  "created_at":"2021-08-15T17:28:43Z",
  "id":899085159,
  "issue":1060,
  "node_id":"IC_kwDODBCWws41lvNn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-15T17:28:43Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"`ak.from_iter` is, roughly speaking, the opposite of `ak.to_list` in that it converts between Awkward Array data and Python data. Just the fact that it involves Python data puts both of these functions in the \"inefficient\" category; performance isn't going to be better if `ak.from_iter` encounters an `ak.Record` rather than a dict.\r\n\r\nOutside of the MRE we're discussing here, if you want to build arrays out of arrays, there are efficient ways to do that in compiled code, much faster than `ak.from_iter` or `ak.to_list`, and some of them are even _O(1)_\u2014independent of the size of the array. Like this, for instance:\r\n\r\n```python\r\n>>> a = ak.Array([1, 2, 3])\r\n>>> a\r\n<Array [1, 2, 3] type='3 * int64'>\r\n>>> b = ak.zip({\"a\": a}, depth_limit=1)\r\n>>> b\r\n<Array [{a: 1}, {a: 2}, {a: 3}] type='3 * {\"a\": int64}'>\r\n>>> c = ak.zip({\"b\": b}, depth_limit=1)\r\n>>> c\r\n<Array [{b: {a: 1}}, ... a: 2}}, {b: {a: 3}}] type='3 * {\"b\": {\"a\": int64}}'>\r\n```\r\n\r\nwhich nests every value of `a` inside a field of records in `b` inside a field of records in `c`, and that's just a rearrangement of metadata, it takes exactly as long whether the array has length 3 or 3 billion.\r\n\r\nAlthough I'll look into the apparent bug in your second example (right now, I think), building arrays up by putting existing arrays into the `ak.Array` constructor (which calls `ak.from_iter` if it's nested in any kind of Python structure) is disfavored.",
  "created_at":"2021-08-16T12:47:36Z",
  "id":899481688,
  "issue":1060,
  "node_id":"IC_kwDODBCWws41nQBY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-16T12:47:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've addressed the two MRE examples (in #1060) by\r\n\r\n   1. having `ak.Record.__iter__` explicitly iterate over its fields, like dict\r\n   2. explicitly checking for `ak.Array` and `ak.Record` in `ak.from_iter` to interpret both as their `ak.to_list` equivalents (i.e. `ak.Record` is interpreted as its equivalent dict)\r\n   3. not changing the \"Pandas-style\" `ak.Array` constructor, which takes the keys and values of a given (top-level) dict as column names and columns to zip together, iterating over the columns (even though iterating over a dict/Record's fields is probably not what you want)\r\n\r\nFollowing these rules, the first MRE becomes:\r\n\r\n```python\r\n>>> awkward.Array({\"record\": awkward.Record({\"column\": [1]})})\r\n<Array [{record: 'column'}] type='1 * {\"record\": string}'>\r\n```\r\n\r\nbecause the `ak.Array` constructor is being given a (top-level) dict, so `\"record\"` is interpreted as a column name and it iterates over the value, `awkward.Record({\"column\": [1]})`, which is an iteration over one string: `[\"column\"]`. This is exactly the same as though the `ak.Record` were a dict:\r\n\r\n```python\r\n>>> awkward.Array({\"record\": {\"column\": [1]}})\r\n<Array [{record: {column: 1}}] type='1 * {\"record\": {\"column\": int64}}'>\r\n```\r\n\r\nArguably, that's not what somebody would have intended if they wrote that, but putting in special cases to check for `ak.Record`, dict, Mapping, and maybe others for the sake of interpreting them differently or at least raising an error would be making the function too magical\u2014too hard to interpret or work around if it hits some other unexpected case.\r\n\r\nThe second MRE example is more meaningful:\r\n\r\n```python\r\n>>> awkward.Array([awkward.Record({\"column\": [1]})])\r\n<Array [{column: [1]}] type='1 * {\"column\": var * int64}'>\r\n```\r\n\r\nNow this is interpreting the `ak.Record` exactly as though it were a dict:\r\n\r\n```python\r\n>>> awkward.Array([{\"column\": [1]}])\r\n<Array [{column: [1]}] type='1 * {\"column\": var * int64}'>\r\n```\r\n\r\nTo reiterate my previous comment, this doesn't improve the performance. The `ak.Record` is actually being turned into a Python dict, which is then iterated over\u2014it could only be faster if it was originally a dict. In large-scale computations, you don't want to manipulate these things in a row-wise/record-oriented way, but in a columnar way with broadcasting, `ak.zip`, and other array-at-a-time functions.",
  "created_at":"2021-08-16T13:43:06Z",
  "id":899522074,
  "issue":1060,
  "node_id":"IC_kwDODBCWws41nZ4a",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-16T13:43:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Let's go through some of the operations present in numpy and their counterparts in Awkward\r\n\r\n## Array concatenation\r\n\r\nA simple operation of concatenating two arrays\r\n```python\r\n>>> np.concatenate([np.array([1, 2, 3]), np.array([4, 5, 6])])\r\narray([1, 2, 3, 4, 5, 6])\r\n```\r\nhas an obvious counterpart in Awkward\r\n```python\r\n>>> awkward.concatenate([awkward.Array([1, 2, 3]), awkward.Array([4, 5, 6])])\r\n<Array [1, 2, 3, 4, 5, 6] type='6 * int64'>\r\n```\r\nNow in Awkward we deal not only with numerical indices, but also with record key indices, so one might want to concatenate along that dimension as well. A naive approach:\r\n```python\r\n>>> awkward.concatenate([awkward.Array({\"a\": [1], \"b\": [2]}), awkward.Array({\"c\": [3], \"d\": [4]})])\r\n<Array [{a: 1, b: 2}, {c: 3, d: 4}] type='2 * union[{\"a\": int64, \"b\": int64}, {\"...'>\r\n>>> awkward.concatenate([awkward.Array({\"a\": [1], \"b\": [2]}), awkward.Array({\"c\": [3], \"d\": [4]})]).to_list()\r\n[{'a': 1, 'b': 2}, {'c': 3, 'd': 4}]\r\n```\r\ndoes concatenation along the numerical axis. The `awkward.zip` is not quite it, as it expectedly creates an extra dimension. It seems like there is an operation missing here, or I haven't found it.\r\n\r\n## Array stacking\r\n\r\nArray stacking is like concatenation, but creates a new axis. A simple use in Numpy:\r\n\r\n```python\r\n>>> np.stack([np.array([1, 2, 3]), np.array([4, 5, 6])])\r\narray([[1, 2, 3],\r\n       [4, 5, 6]])\r\n>>> np.array([[np.array([1]), np.array([2])], [np.array([3]), np.array([4])]])\r\narray([[[1],\r\n        [2]],\r\n\r\n       [[3],\r\n        [4]]])\r\n```\r\n\r\nThe well known `zip` operation from the Python standard library is a case of stacking:\r\n\r\n```python\r\nzip = lambda *arrays: np.stack(arrays, axis=1)\r\n```\r\n\r\nThere is no dedicated operation for stacking in Awkward, but the following trick apparently works:\r\n\r\n```python\r\n>>> awkward.concatenate([awkward.Array([1, 2, 3])[np.newaxis], awkward.Array([4, 5, 6])[np.newaxis]])\r\n<Array [[1, 2, 3], [4, 5, 6]] type='2 * 3 * int64'>\r\n```\r\n\r\nStacking along a record index requires specifying index values and is achievable with `awkward.zip`:\r\n\r\n```python\r\n>>> awkward.zip({\"a\": awkward.Array([1, 2, 3]), \"b\": awkward.Array([4, 5, 6])})\r\n<Array [{a: 1, b: 4}, ... b: 5}, {a: 3, b: 6}] type='3 * {\"a\": int64, \"b\": int64}'>\r\n>>> awkward.zip({\"a\": awkward.Array({\"i\": [1]}), \"b\": awkward.Array({\"k\": [2]})})\r\n<Array [{a: {i: 1}, b: {k: 2}}] type='1 * {\"a\": {\"i\": int64}, \"b\": {\"k\": int64}}'>\r\n```\r\n\r\nthe `Array` constructor works as well for this purpose\r\n```python\r\n>>> awkward.Array({\"a\": awkward.Array([1, 2, 3]), \"b\": awkward.Array([4, 5, 6])})\r\n<Array [{a: 1, b: 4}, ... b: 5}, {a: 3, b: 6}] type='3 * {\"a\": int64, \"b\": int64}'>\r\n```\r\n\r\nPresumably there would be an operation to do stacking at a deeper level, like `awkward.zip({\"a\": awkward.Array({\"i\": [1]}), \"b\": awkward.Array({\"k\": [2]})}, axis=1)` that would return `<Array [{i: {a: 1}, k: {b: 2}}] type='1 * {\"i\": {\"a\": int64}, \"k\": {\"b\": int64}}'>`\r\n\r\n# Array constructor\r\n\r\nIn Numpy, the array constructor allows to add arbitrary dimensions\r\n```python\r\n>>> np.array([[np.array([1, 2, 3])], [np.array([4, 5, 6])]])\r\narray([[[1, 2, 3]],\r\n\r\n       [[4, 5, 6]]])\r\n```\r\n\r\nSame works in Awkward for numerical dimensions:\r\n```python\r\n>>> awkward.Array([[awkward.Array([1, 2, 3])], [awkward.Array([4, 5, 6])]])\r\n<Array [[[1, 2, 3]], [[4, 5, 6]]] type='2 * var * var * int64'>\r\n```\r\nFor record dimensions:\r\n```python\r\n>>> awkward.Array({\"i\": awkward.Array({\"a\": [1]}), \"k\": awkward.Array({\"b\": [2]})})\r\n<Array [{i: {a: 1}, k: {b: 2}}] type='1 * {\"i\": {\"a\": int64}, \"k\": {\"b\": int64}}'>\r\n```\r\nFor mixed\r\n```python\r\n>>> awkward.Array({\"i\": awkward.Array([1, 2, 3]), \"j\": awkward.Array([4, 5, 6])})\r\n<Array [{i: 1, j: 4}, ... j: 5}, {i: 3, j: 6}] type='3 * {\"i\": int64, \"j\": int64}'>\r\n```\r\nand in the master version also the opposite way\r\n```python\r\n>>> awkward.Array([[awkward.Array({\"a\": [1]})], [awkward.Array({\"b\": [2]})]])\r\n<Array [[[{a: 1, b: None}, ... b: 2}]]] type='2 * var * var * {\"a\": ?int64, \"b\":...'>\r\n```\r\n(although the type appears to be a bit strange here, as it could have been a \"union\" instead)",
  "created_at":"2021-08-18T05:31:19Z",
  "id":900823635,
  "issue":1060,
  "node_id":"IC_kwDODBCWws41sXpT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-18T05:31:19Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"You presented a good overview. One point that should be emphasized, though, is that there's a conceptual distinction/asymmetry between \"columns\" and \"rows.\"\r\n\r\nBy \"columns,\" I mean data in an `ak.Array` that can be accessed by slicing with a string or a list of strings\u2014a field or fields of a record. By \"rows,\" I mean data that can be accessed with any other kind of slice, so that includes jagged dimensions. Awkward Array is optimized for having a large number of rows: anything involving row manipulation is done in compiled code, not Python iteration, and is therefore roughly as fast as a dedicated C program. (\"Roughly\" because I'm not considering the effects of multiple passes over the data and RAM \u2192 CPU cache throughput, which are relevant at higher speeds than could be achieved in a pure Python program.) Anything involving column manipulation is done in Python or dynamically typed C++ (which is currently [being converted to Python](https://github.com/scikit-hep/awkward-1.0/pulls?q=is%3Apr+%22C%2B%2B+refactoring%22)), so the scales are more limited. My standard recommendation is that the number of rows can scale to however much memory you have\u2014a computer with GBs of RAM can deal with billions of rows\u2014but the number of columns should not be much more than thousands.\r\n\r\nAs you've noticed, the available operations have an asymmetry between columns and rows, and that reflects the fact that the library was designed for this asymptote of scaling. Thus, `ak.concatenate` concatenates rows only, though it may do so at different dimensions/depths/`axis` numbers. The `axis` numbers are themselves a row-only concept.\r\n\r\nBut suppose you want to do something like [pd.concat](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) with `axis=1`. (Pandas _does_ treat rows and columns on a similar footing, using `axis=0` to apply a function to rows and `axis=1` to apply the same function to columns. Since Pandas only deals with 2-dimensional tables, `axis=0` and `axis=1` are the only options. Awkward Array's `axis` is a lot like NumPy's `axis` and nothing like Pandas's `axis`.) That would be to take these two arrays of records, each of which has different fields:\r\n\r\n```python\r\n>>> one = ak.Array([[{\"a\": 1, \"b\": 1.1}], [], [{\"a\": 2, \"b\": 2.2}]])\r\n>>> two = ak.Array([[{\"c\": 3, \"d\": 3.3}], [], [{\"c\": 4, \"d\": 4.4}]])\r\n```\r\n\r\nand turn them into an array of records with all four fields, `\"a\"`, `\"b\"`, `\"c\"`, `\"d\"`. The following does that in general:\r\n\r\n```python\r\n>>> onetwo = ak.zip(dict(zip(one.fields + two.fields, ak.unzip(one) + ak.unzip(two))))\r\n>>> onetwo.type\r\n3 * var * {\"a\": int64, \"b\": float64, \"c\": int64, \"d\": float64}\r\n>>> onetwo.tolist()\r\n[[{'a': 1, 'b': 1.1, 'c': 3, 'd': 3.3}], [], [{'a': 2, 'b': 2.2, 'c': 4, 'd': 4.4}]]\r\n```\r\n\r\nYou could imagine scaling the number of inputs from two (`one` and `two`) to an arbitrary number, since we're just adding lists of strings (`one.fields + two.fields`) and tuples (`ak.unzip(one) + ak.unzip(two)`). If the number of row dimensions vary, it might be necessary to limit the depth of zipping: `depth_limit=min(x.layout.purelist_depth for x in (one, two))`.\r\n\r\nIf we add such a function to the library, it would have to have a different name from \"concatenate,\" because I want to maintain this conceptual distinction between rows and columns.\r\n\r\nI like your stacking solution. If it's sufficiently general, we could overload `np.stack` with an `ak.stack` the same way that `ak.concatenate` overloads `np.concatenate`. (We use [NEP 18](https://numpy.org/neps/nep-0018-array-function-protocol.html) to overload NumPy functions if we can write an Awkward version of them that strictly generalizes the NumPy version.)\r\n\r\nOn your last point about the `ak.Array` constructor, it's not doing anything special with records or arrays (regardless of whether they're Awkward or NumPy) that are _nested within_ Python objects. If the top-level object passed to the `ak.Array` constructor is a list, then everything inside that list is iterated over (in Python) and it doesn't matter what kind of iterable it is. So\r\n\r\n```python\r\n>>> ak.Array([ak.Array([{\"a\": 1}]), ak.Array([{\"b\": 2}])])\r\n<Array [[{a: 1, b: None}], ... a: None, b: 2}]] type='2 * var * {\"a\": ?int64, \"b...'>\r\n```\r\n\r\nis not any different from\r\n\r\n```python\r\n>>> ak.Array([[{\"a\": 1}], [{\"b\": 2}]])\r\n<Array [[{a: 1, b: None}], ... a: None, b: 2}]] type='2 * var * {\"a\": ?int64, \"b...'>\r\n```\r\n\r\nOnce inside the outer list, the `ak.Array` constructor has concluded that this is a generic iterable and it iterates over `ak.Array([{\"a\": 1}])` in exactly the same way as it would iterate over `[{\"a\": 1}]`. In this \"iteration,\" dict-like objects are interpreted as records, so yes, we're iterating over both the keys and the values to do that, but it doesn't matter whether the objects are `ak.Record` or dict after PR #1061.\r\n\r\nWhy do you get a single record with nullable fields instead of a union-type? That was a deliberate choice: when iterating through the nested data, if we encounter dicts at the same level with different sets of keys, we could either have chosen to declare them as different record types and make a union or as a single record type in which the keys not seen in one dict are taken to have a value of \"None\" (i.e. ignoring the distinction between `key not in some_dict` and `some_dict[key] is None`, the way that `some_dict.get` works). This choice was made because option-type data have fewer limitations than union-type data, so it's a better default.\r\n\r\nIf you make the array manually with [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html), you can opt into or out of this choice. ([ak.from_iter](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_iter.html), which is what the [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html) constructor defers to in the generic iterable case, internally uses an ArrayBuilder to make its array.)\r\n\r\n```python\r\n>>> builder = ak.ArrayBuilder()\r\n>>> with builder.list():\r\n...     with builder.record():\r\n...         builder.field(\"a\").integer(1)\r\n... \r\n>>> with builder.list():\r\n...     with builder.record():\r\n...         builder.field(\"b\").integer(2)\r\n... \r\n>>> array = builder.snapshot()\r\n>>> array\r\n<Array [[{a: 1, b: None}], ... a: None, b: 2}]] type='2 * var * {\"a\": ?int64, \"b...'>\r\n>>> array.type\r\n2 * var * {\"a\": ?int64, \"b\": ?int64}\r\n>>> array.tolist()\r\n[[{'a': 1, 'b': None}], [{'a': None, 'b': 2}]]\r\n```\r\n\r\nNaming the records establishes that they are distinct, and that you want a union-type instead of two option-types.\r\n\r\n```python\r\n>>> builder = ak.ArrayBuilder()\r\n>>> with builder.list():\r\n...     with builder.record(\"A\"):\r\n...         builder.field(\"a\").integer(1)\r\n... \r\n>>> with builder.list():\r\n...     with builder.record(\"B\"):\r\n...         builder.field(\"b\").integer(2)\r\n... \r\n>>> array = builder.snapshot()\r\n>>> array\r\n<Array [[{a: 1}], [{b: 2}]] type='2 * var * union[A[\"a\": int64], B[\"b\": int64]]'>\r\n>>> array.type\r\n2 * var * union[A[\"a\": int64], B[\"b\": int64]]\r\n>>> array.tolist()\r\n[[{'a': 1}], [{'b': 2}]]\r\n```\r\n\r\nNaming records has [other uses](https://awkward-array.readthedocs.io/en/latest/ak.behavior.html) as well.",
  "created_at":"2021-08-18T12:34:42Z",
  "id":901075701,
  "issue":1060,
  "node_id":"IC_kwDODBCWws41tVL1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-18T12:34:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Is there a way I can see the windows build errors? It just says \"failed\" in the azure pipeline output..",
  "created_at":"2021-08-17T18:00:15Z",
  "id":900515278,
  "issue":1062,
  "node_id":"IC_kwDODBCWws41rMXO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-17T18:00:15Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Is there a way I can see the windows build errors? It just says \"failed\" in the azure pipeline output..\r\n\r\nReally? I didn't know that special permissions were needed to see them. I'm looking at them now, but in the meantime, here's a copy-paste:\r\n\r\n```\r\n2021-08-17T17:28:51.7690613Z   Microsoft (R) Build Engine version 15.9.21+g9802d43bc3 for .NET Framework\r\n2021-08-17T17:28:51.7714090Z   Copyright (C) Microsoft Corporation. All rights reserved.\r\n2021-08-17T17:28:51.7731713Z \r\n2021-08-17T17:28:55.2410730Z     Checking File Globs\r\n2021-08-17T17:28:55.6922340Z     Checking Build System\r\n2021-08-17T17:29:02.0417680Z     Building Custom Rule C:/Users/VssAdministrator/AppData/Local/Temp/pip-req-build-d2fcd6bg/CMakeLists.txt\r\n2021-08-17T17:29:02.9530613Z     allocators.cpp\r\n2021-08-17T17:29:03.4263409Z     Building Custom Rule C:/Users/VssAdministrator/AppData/Local/Temp/pip-req-build-d2fcd6bg/CMakeLists.txt\r\n2021-08-17T17:29:03.8799706Z     Content.cpp\r\n2021-08-17T17:29:04.3922280Z     awkward_BitMaskedArray_to_ByteMaskedArray.cpp\r\n2021-08-17T17:29:05.2819143Z     awkward_BitMaskedArray_to_IndexedOptionArray.cpp\r\n2021-08-17T17:29:06.2226213Z     awkward_ByteMaskedArray_getitem_carry.cpp\r\n2021-08-17T17:29:06.6930247Z     Identities.cpp\r\n2021-08-17T17:29:07.2611393Z     awkward_ByteMaskedArray_getitem_nextcarry.cpp\r\n2021-08-17T17:29:08.1943217Z     awkward_ByteMaskedArray_getitem_nextcarry_outindex.cpp\r\n2021-08-17T17:29:08.9549304Z     Index.cpp\r\n2021-08-17T17:29:09.1414889Z     awkward_ByteMaskedArray_mask.cpp\r\n2021-08-17T17:29:10.1513991Z     awkward_ByteMaskedArray_numnull.cpp\r\n2021-08-17T17:29:10.8302447Z     Iterator.cpp\r\n2021-08-17T17:29:11.7326355Z     awkward_ByteMaskedArray_overlay_mask.cpp\r\n2021-08-17T17:29:12.1184366Z     awkward_ByteMaskedArray_reduce_next_64.cpp\r\n2021-08-17T17:29:12.3559722Z     Reducer.cpp\r\n2021-08-17T17:29:13.0581593Z     awkward_ByteMaskedArray_reduce_next_nonlocal_nextshifts_64.cpp\r\n2021-08-17T17:29:13.9426639Z     Slice.cpp\r\n2021-08-17T17:29:14.0724709Z     awkward_ByteMaskedArray_reduce_next_nonlocal_nextshifts_fromshifts_64.cpp\r\n2021-08-17T17:29:15.0757110Z     awkward_ByteMaskedArray_toIndexedOptionArray.cpp\r\n2021-08-17T17:29:15.5021523Z     BitMaskedArray.cpp\r\n2021-08-17T17:29:16.1418243Z     awkward_Content_getitem_next_missing_jagged_getmaskstartstop.cpp\r\n2021-08-17T17:29:17.2118687Z     awkward_Identities32_to_Identities64.cpp\r\n2021-08-17T17:29:17.3779953Z     ByteMaskedArray.cpp\r\n2021-08-17T17:29:18.2332750Z     awkward_Identities_extend.cpp\r\n2021-08-17T17:29:19.2822864Z     awkward_Identities_from_IndexedArray.cpp\r\n2021-08-17T17:29:19.4563485Z     EmptyArray.cpp\r\n2021-08-17T17:29:20.2221264Z     awkward_Identities_from_ListArray.cpp\r\n2021-08-17T17:29:21.1929499Z     IndexedArray.cpp\r\n2021-08-17T17:29:21.3546850Z     awkward_Identities_from_ListOffsetArray.cpp\r\n2021-08-17T17:29:22.3219377Z     awkward_Identities_from_RegularArray.cpp\r\n2021-08-17T17:29:23.3401376Z     Generating Code...\r\n2021-08-17T17:29:24.3812800Z     Compiling...\r\n2021-08-17T17:29:24.3817053Z     awkward_Identities_from_UnionArray.cpp\r\n2021-08-17T17:29:24.5099993Z     ListArray.cpp\r\n2021-08-17T17:29:25.4433624Z     awkward_Identities_getitem_carry.cpp\r\n2021-08-17T17:29:26.4599581Z     awkward_Index_carry.cpp\r\n2021-08-17T17:29:27.1497748Z     ListOffsetArray.cpp\r\n2021-08-17T17:29:27.3307116Z     awkward_Index_iscontiguous.cpp\r\n2021-08-17T17:29:28.6572866Z     awkward_Index_nones_as_index.cpp\r\n2021-08-17T17:29:29.5880315Z     awkward_Index_to_Index64.cpp\r\n2021-08-17T17:29:30.0285208Z     None.cpp\r\n2021-08-17T17:29:30.4487360Z     awkward_IndexedArray_fill.cpp\r\n2021-08-17T17:29:31.4035758Z     awkward_IndexedArray_fill_count.cpp\r\n2021-08-17T17:29:31.5056138Z     NumpyArray.cpp\r\n2021-08-17T17:29:32.3111902Z     awkward_IndexedArray_flatten_nextcarry.cpp\r\n2021-08-17T17:29:33.0840942Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4708): error C2146: syntax error: missing ')' before identifier 'and' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0857084Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4708): error C2065: 'and': undeclared identifier [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0865722Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4708): error C2146: syntax error: missing ';' before identifier 'carry' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0871075Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4708): error C2146: syntax error: missing ';' before identifier 'and' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0914846Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4708): warning C4553: '==': result of expression not used; did you intend '='? [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0917702Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4708): error C2146: syntax error: missing ';' before identifier 'nexthead' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0922404Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4708): error C2059: syntax error: ')' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0925055Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4710): error C2059: syntax error: 'return' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0930733Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4718): error C2059: syntax error: 'else' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0936053Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4718): error C2143: syntax error: missing ';' before '{' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0939591Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4718): error C2447: '{': missing function header (old-style formal list?) [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0946266Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4740): error C2059: syntax error: '}' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0950659Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4740): error C2143: syntax error: missing ';' before '}' [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.0953392Z   C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\libawkward\\array\\NumpyArray.cpp(4706): fatal error C1075: '{': no matching token found [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-objects.vcxproj]\r\n2021-08-17T17:29:33.1001998Z     Record.cpp\r\n2021-08-17T17:29:33.3344153Z     awkward_IndexedArray_flatten_none2empty.cpp\r\n2021-08-17T17:29:34.3235483Z     awkward_IndexedArray_getitem_adjust_outindex.cpp\r\n2021-08-17T17:29:34.9626492Z     RecordArray.cpp\r\n2021-08-17T17:29:35.3361611Z     awkward_IndexedArray_getitem_carry.cpp\r\n2021-08-17T17:29:36.3486854Z     awkward_IndexedArray_getitem_nextcarry.cpp\r\n2021-08-17T17:29:37.1262442Z     RegularArray.cpp\r\n2021-08-17T17:29:37.2857634Z     awkward_IndexedArray_getitem_nextcarry_outindex.cpp\r\n2021-08-17T17:29:38.4204836Z     awkward_IndexedArray_getitem_nextcarry_outindex_mask.cpp\r\n2021-08-17T17:29:39.6683491Z     awkward_IndexedArray_index_of_nulls.cpp\r\n2021-08-17T17:29:39.8457308Z     UnionArray.cpp\r\n2021-08-17T17:29:40.6105912Z     awkward_IndexedArray_local_preparenext_64.cpp\r\n2021-08-17T17:29:41.6056741Z     awkward_IndexedArray_mask.cpp\r\n2021-08-17T17:29:42.2030860Z     UnmaskedArray.cpp\r\n2021-08-17T17:29:42.7749154Z     awkward_IndexedArray_numnull.cpp\r\n2021-08-17T17:29:43.7457480Z     awkward_IndexedArray_overlay_mask.cpp\r\n2021-08-17T17:29:44.0360036Z     VirtualArray.cpp\r\n2021-08-17T17:29:44.7106175Z     Generating Code...\r\n2021-08-17T17:29:45.6378137Z     Compiling...\r\n2021-08-17T17:29:45.6383975Z     awkward_IndexedArray_ranges_carry_next_64.cpp\r\n2021-08-17T17:29:46.2878462Z     Generating Code...\r\n2021-08-17T17:29:47.3075230Z     awkward_IndexedArray_ranges_next_64.cpp\r\n2021-08-17T17:29:49.5931218Z     awkward_IndexedArray_reduce_next_64.cpp\r\n2021-08-17T17:29:52.1254752Z     awkward_IndexedArray_reduce_next_fix_offsets_64.cpp\r\n2021-08-17T17:29:55.3896062Z     awkward_IndexedArray_reduce_next_nonlocal_nextshifts_64.cpp\r\n2021-08-17T17:29:57.6083320Z     awkward_IndexedArray_reduce_next_nonlocal_nextshifts_fromshifts_64.cpp\r\n2021-08-17T17:30:00.7146121Z     awkward_IndexedArray_simplify.cpp\r\n2021-08-17T17:30:03.8473107Z     awkward_IndexedArray_validity.cpp\r\n2021-08-17T17:30:06.1227593Z     awkward_IndexedOptionArray_rpad_and_clip_mask_axis1.cpp\r\n2021-08-17T17:30:08.6595517Z     awkward_ListArray_broadcast_tooffsets.cpp\r\n2021-08-17T17:30:11.3886698Z     awkward_ListArray_combinations.cpp\r\n2021-08-17T17:30:14.6268787Z     awkward_ListArray_combinations_length.cpp\r\n2021-08-17T17:30:17.0147466Z     awkward_ListArray_compact_offsets.cpp\r\n2021-08-17T17:30:20.2385339Z     awkward_ListArray_fill.cpp\r\n2021-08-17T17:30:23.0964666Z     awkward_ListArray_getitem_carry.cpp\r\n2021-08-17T17:30:25.6817185Z     awkward_ListArray_getitem_jagged_apply.cpp\r\n2021-08-17T17:30:28.1502568Z     awkward_ListArray_getitem_jagged_carrylen.cpp\r\n2021-08-17T17:30:31.0872683Z     awkward_ListArray_getitem_jagged_descend.cpp\r\n2021-08-17T17:30:33.4764550Z     awkward_ListArray_getitem_jagged_expand.cpp\r\n2021-08-17T17:30:36.2982518Z     awkward_ListArray_getitem_jagged_numvalid.cpp\r\n2021-08-17T17:30:39.0278603Z     Generating Code...\r\n2021-08-17T17:30:40.6603505Z     Compiling...\r\n2021-08-17T17:30:40.6605325Z     awkward_ListArray_getitem_jagged_shrink.cpp\r\n2021-08-17T17:30:41.6278083Z     awkward_ListArray_getitem_next_array.cpp\r\n2021-08-17T17:30:42.5605840Z     awkward_ListArray_getitem_next_array_advanced.cpp\r\n2021-08-17T17:30:43.2022105Z     Compiling...\r\n2021-08-17T17:30:43.2023658Z     ArrayBuilder.cpp\r\n2021-08-17T17:30:43.5308865Z     awkward_ListArray_getitem_next_at.cpp\r\n2021-08-17T17:30:44.4827181Z     awkward_ListArray_getitem_next_range.cpp\r\n2021-08-17T17:30:44.5656494Z     ArrayBuilderOptions.cpp\r\n2021-08-17T17:30:45.4164137Z     awkward_ListArray_getitem_next_range_carrylength.cpp\r\n2021-08-17T17:30:45.4961896Z     BoolBuilder.cpp\r\n2021-08-17T17:30:46.2724401Z     awkward_ListArray_getitem_next_range_counts.cpp\r\n2021-08-17T17:30:46.4905246Z     Builder.cpp\r\n2021-08-17T17:30:47.3272192Z     awkward_ListArray_getitem_next_range_spreadadvanced.cpp\r\n2021-08-17T17:30:47.6880605Z     Complex128Builder.cpp\r\n2021-08-17T17:30:48.1568257Z     awkward_ListArray_localindex.cpp\r\n2021-08-17T17:30:48.7594341Z     DatetimeBuilder.cpp\r\n2021-08-17T17:30:49.8001230Z     awkward_ListArray_min_range.cpp\r\n2021-08-17T17:30:49.9652431Z     Float64Builder.cpp\r\n2021-08-17T17:30:50.0445326Z     awkward_ListArray_num.cpp\r\n2021-08-17T17:30:51.0472007Z     awkward_ListArray_rpad_and_clip_length_axis1.cpp\r\n2021-08-17T17:30:51.0854694Z     GrowableBuffer.cpp\r\n2021-08-17T17:30:52.0630510Z     awkward_ListArray_rpad_axis1.cpp\r\n2021-08-17T17:30:52.7271974Z     IndexedBuilder.cpp\r\n2021-08-17T17:30:53.1339034Z     awkward_ListArray_validity.cpp\r\n2021-08-17T17:30:54.1729042Z     awkward_ListOffsetArray_argsort_strings.cpp\r\n2021-08-17T17:30:54.5423920Z     Int64Builder.cpp\r\n2021-08-17T17:30:55.1854729Z   C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\xutility(365): warning C4244: 'argument': conversion from '__int64' to 'int', possible loss of data [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-cpu-kernels-objects.vcxproj]\r\n2021-08-17T17:30:55.1884717Z     C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\algorithm(3832): note: see reference to function template instantiation 'decltype(auto) std::_Ref_fn<_Pr>::operator ()<__int64&,__int64&>(__int64 &,__int64 &)' being compiled\r\n2021-08-17T17:30:55.1980413Z             with\r\n2021-08-17T17:30:55.2004879Z             [\r\n2021-08-17T17:30:55.2006377Z                 _Pr=awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>\r\n2021-08-17T17:30:55.2007928Z             ]\r\n2021-08-17T17:30:55.2019709Z     C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\algorithm(4268): note: see reference to function template instantiation '_BidIt std::_Insertion_sort_unchecked<_BidIt,std::_Ref_fn<_Pr>>(_BidIt,const _BidIt,std::_Ref_fn<_Pr>)' being compiled\r\n2021-08-17T17:30:55.2023375Z             with\r\n2021-08-17T17:30:55.2024440Z             [\r\n2021-08-17T17:30:55.2026586Z                 _BidIt=std::_Vector_iterator<std::_Vector_val<std::_Simple_types<int64_t>>>,\r\n2021-08-17T17:30:55.2029681Z                 _Pr=awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>\r\n2021-08-17T17:30:55.2033356Z             ]\r\n2021-08-17T17:30:55.2037647Z     C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\cpu-kernels\\awkward_ListOffsetArray_argsort_strings.cpp(50): note: see reference to function template instantiation 'void std::stable_sort<std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>>,awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>>(_BidIt,_BidIt,_Pr)' being compiled\r\n2021-08-17T17:30:55.2042222Z             with\r\n2021-08-17T17:30:55.2043187Z             [\r\n2021-08-17T17:30:55.2045231Z                 _Ty=int64_t,\r\n2021-08-17T17:30:55.2046973Z                 _BidIt=std::_Vector_iterator<std::_Vector_val<std::_Simple_types<int64_t>>>,\r\n2021-08-17T17:30:55.2048495Z                 _Pr=awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>\r\n2021-08-17T17:30:55.2049130Z             ]\r\n2021-08-17T17:30:55.2050074Z     C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\src\\cpu-kernels\\awkward_ListOffsetArray_argsort_strings.cpp(92): note: see reference to function template instantiation 'Error awkward_ListOffsetArray_argsort_strings_impl<true,true,true>(int64_t *,const int64_t *,int64_t,const char *,const int64_t *,const int64_t *)' being compiled\r\n2021-08-17T17:30:55.3208790Z   C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\xutility(365): warning C4244: 'argument': conversion from 'const _Ty' to 'int', possible loss of data [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-cpu-kernels-objects.vcxproj]\r\n2021-08-17T17:30:55.3211354Z             with\r\n2021-08-17T17:30:55.3212393Z             [\r\n2021-08-17T17:30:55.3213813Z                 _Ty=int64_t\r\n2021-08-17T17:30:55.3214745Z             ]\r\n2021-08-17T17:30:55.3216586Z     C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\xutility(3988): note: see reference to function template instantiation 'decltype(auto) std::_Ref_fn<_Pr>::operator ()<__int64&,const _Ty&>(__int64 &,const _Ty &)' being compiled\r\n2021-08-17T17:30:55.3218522Z             with\r\n2021-08-17T17:30:55.3219559Z             [\r\n2021-08-17T17:30:55.3220851Z                 _Pr=awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>,\r\n2021-08-17T17:30:55.3224210Z                 _Ty=__int64\r\n2021-08-17T17:30:55.3225276Z             ]\r\n2021-08-17T17:30:55.3241906Z     C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\algorithm(3643): note: see reference to function template instantiation '_FwdIt std::lower_bound<_BidIt,__int64,_Pr>(_FwdIt,const _FwdIt,const _Ty &,_Pr)' being compiled\r\n2021-08-17T17:30:55.3243948Z             with\r\n2021-08-17T17:30:55.3244793Z             [\r\n2021-08-17T17:30:55.3246448Z                 _FwdIt=__int64 *,\r\n2021-08-17T17:30:55.3247862Z                 _BidIt=__int64 *,\r\n2021-08-17T17:30:55.3249176Z                 _Pr=std::_Ref_fn<awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>>,\r\n2021-08-17T17:30:55.3252267Z                 _Ty=__int64\r\n2021-08-17T17:30:55.3253781Z             ]\r\n2021-08-17T17:30:55.3257558Z     C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\algorithm(3676): note: see reference to function template instantiation 'void std::_Buffered_inplace_merge_divide_and_conquer<_BidIt,_Pr>(_BidIt,_BidIt,_BidIt,__int64,__int64,__int64 *const ,const ptrdiff_t,_Pr)' being compiled\r\n2021-08-17T17:30:55.3261895Z             with\r\n2021-08-17T17:30:55.3263273Z             [\r\n2021-08-17T17:30:55.3267196Z                 _BidIt=__int64 *,\r\n2021-08-17T17:30:55.3271704Z                 _Pr=std::_Ref_fn<awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>>\r\n2021-08-17T17:30:55.3275626Z             ]\r\n2021-08-17T17:30:55.3278130Z     C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\algorithm(3731): note: see reference to function template instantiation 'void std::_Buffered_inplace_merge_unchecked_impl<_BidIt,_Pr>(_BidIt,_BidIt,_BidIt,__int64,__int64,__int64 *const ,const ptrdiff_t,_Pr)' being compiled\r\n2021-08-17T17:30:55.3280141Z             with\r\n2021-08-17T17:30:55.3281406Z             [\r\n2021-08-17T17:30:55.3283216Z                 _BidIt=__int64 *,\r\n2021-08-17T17:30:55.3309843Z                 _Pr=std::_Ref_fn<awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>>\r\n2021-08-17T17:30:55.3312422Z             ]\r\n2021-08-17T17:30:55.3324229Z     C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\algorithm(4252): note: see reference to function template instantiation 'void std::_Buffered_inplace_merge_unchecked<_BidIt,_Pr>(_BidIt,_BidIt,_BidIt,__int64,__int64,__int64 *const ,const ptrdiff_t,_Pr)' being compiled\r\n2021-08-17T17:30:55.3333759Z             with\r\n2021-08-17T17:30:55.3335394Z             [\r\n2021-08-17T17:30:55.3336681Z                 _BidIt=__int64 *,\r\n2021-08-17T17:30:55.3339405Z                 _Pr=std::_Ref_fn<awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>>\r\n2021-08-17T17:30:55.3343946Z             ]\r\n2021-08-17T17:30:55.3349549Z     C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\algorithm(4275): note: see reference to function template instantiation 'void std::_Stable_sort_unchecked<__int64*,std::_Ref_fn<_Pr>>(const _BidIt,const _BidIt,const __int64,__int64 *const ,const ptrdiff_t,std::_Ref_fn<_Pr>)' being compiled\r\n2021-08-17T17:30:55.3357285Z             with\r\n2021-08-17T17:30:55.3362995Z             [\r\n2021-08-17T17:30:55.3365026Z                 _Pr=awkward_ListOffsetArray_argsort_strings_impl::<lambda_1f36786eccba502c48a687bbcb6c0be0>,\r\n2021-08-17T17:30:55.3369084Z                 _BidIt=__int64 *\r\n2021-08-17T17:30:55.3397199Z             ]\r\n2021-08-17T17:30:55.4261876Z     awkward_ListOffsetArray_compact_offsets.cpp\r\n2021-08-17T17:30:55.7207144Z     ListBuilder.cpp\r\n2021-08-17T17:30:56.3431405Z     awkward_ListOffsetArray_flatten_offsets.cpp\r\n2021-08-17T17:30:57.0131345Z     OptionBuilder.cpp\r\n2021-08-17T17:30:57.4072385Z     awkward_ListOffsetArray_getitem_adjust_offsets.cpp\r\n2021-08-17T17:30:58.3267335Z     RecordBuilder.cpp\r\n2021-08-17T17:30:58.5438939Z     awkward_ListOffsetArray_getitem_adjust_offsets_index.cpp\r\n2021-08-17T17:30:59.4716874Z     awkward_ListOffsetArray_local_preparenext_64.cpp\r\n2021-08-17T17:30:59.5504568Z     StringBuilder.cpp\r\n2021-08-17T17:31:00.4645047Z     Generating Code...\r\n2021-08-17T17:31:01.4161817Z     TupleBuilder.cpp\r\n2021-08-17T17:31:02.9029200Z     Compiling...\r\n2021-08-17T17:31:02.9032804Z     awkward_ListOffsetArray_reduce_global_startstop_64.cpp\r\n2021-08-17T17:31:03.3245022Z     UnionBuilder.cpp\r\n2021-08-17T17:31:03.9068956Z     awkward_ListOffsetArray_reduce_local_nextparents_64.cpp\r\n2021-08-17T17:31:04.8647931Z     awkward_ListOffsetArray_reduce_local_outoffsets_64.cpp\r\n2021-08-17T17:31:04.9286838Z     UnknownBuilder.cpp\r\n2021-08-17T17:31:05.9174665Z     awkward_ListOffsetArray_reduce_nonlocal_findgaps_64.cpp\r\n2021-08-17T17:31:06.4010669Z     ForthInputBuffer.cpp\r\n2021-08-17T17:31:06.8422700Z     awkward_ListOffsetArray_reduce_nonlocal_maxcount_offsetscopy_64.cpp\r\n2021-08-17T17:31:07.4585038Z     ForthMachine.cpp\r\n2021-08-17T17:31:07.9409910Z     awkward_ListOffsetArray_reduce_nonlocal_nextshifts_64.cpp\r\n2021-08-17T17:31:08.8738804Z     awkward_ListOffsetArray_reduce_nonlocal_nextstarts_64.cpp\r\n2021-08-17T17:31:09.8506540Z     awkward_ListOffsetArray_reduce_nonlocal_outstartsstops_64.cpp\r\n2021-08-17T17:31:10.8178646Z     awkward_ListOffsetArray_reduce_nonlocal_preparenext_64.cpp\r\n2021-08-17T17:31:10.8458113Z     ForthOutputBuffer.cpp\r\n2021-08-17T17:31:11.8674534Z     awkward_ListOffsetArray_rpad_and_clip_axis1.cpp\r\n2021-08-17T17:31:12.6131483Z     Generating Code...\r\n2021-08-17T17:31:13.1338325Z     awkward_ListOffsetArray_rpad_axis1.cpp\r\n2021-08-17T17:31:15.4527077Z     awkward_ListOffsetArray_rpad_length_axis1.cpp\r\n2021-08-17T17:31:19.2515196Z     awkward_ListOffsetArray_toRegularArray.cpp\r\n2021-08-17T17:31:21.9834392Z     awkward_MaskedArray_getitem_next_jagged_project.cpp\r\n2021-08-17T17:31:23.3554758Z     awkward_NumpyArray_contiguous_copy.cpp\r\n2021-08-17T17:31:24.4752319Z     awkward_NumpyArray_contiguous_copy_from_many.cpp\r\n2021-08-17T17:31:27.1899745Z     awkward_NumpyArray_contiguous_init.cpp\r\n2021-08-17T17:31:29.1152532Z     awkward_NumpyArray_contiguous_next.cpp\r\n2021-08-17T17:31:31.2073318Z     awkward_NumpyArray_copy.cpp\r\n2021-08-17T17:31:34.4126121Z     awkward_NumpyArray_fill.cpp\r\n2021-08-17T17:31:34.7661901Z     Compiling...\r\n2021-08-17T17:31:34.7668369Z     json.cpp\r\n2021-08-17T17:31:35.7597270Z     Generating Code...\r\n2021-08-17T17:31:37.2106484Z     Compiling...\r\n2021-08-17T17:31:37.2108566Z     awkward_NumpyArray_fill_frombool.cpp\r\n2021-08-17T17:31:38.1953833Z     awkward_NumpyArray_fill_fromcomplex.cpp\r\n2021-08-17T17:31:38.5154343Z     uproot.cpp\r\n2021-08-17T17:31:39.2867881Z     awkward_NumpyArray_fill_scaled.cpp\r\n2021-08-17T17:31:40.2826817Z     kernel-dispatch.cpp\r\n2021-08-17T17:31:40.5781139Z     awkward_NumpyArray_fill_tobool.cpp\r\n2021-08-17T17:31:42.0840034Z     awkward_NumpyArray_fill_tocomplex.cpp\r\n2021-08-17T17:31:43.3700856Z     awkward_NumpyArray_getitem_boolean_nonzero.cpp\r\n2021-08-17T17:31:43.4923404Z     BitMaskedArrayBuilder.cpp\r\n2021-08-17T17:31:44.6949658Z     awkward_NumpyArray_getitem_boolean_numtrue.cpp\r\n2021-08-17T17:31:45.9660720Z     ByteMaskedArrayBuilder.cpp\r\n2021-08-17T17:31:46.1045038Z     awkward_NumpyArray_getitem_next_array.cpp\r\n2021-08-17T17:31:47.4140893Z     awkward_NumpyArray_getitem_next_array_advanced.cpp\r\n2021-08-17T17:31:48.2793687Z     EmptyArrayBuilder.cpp\r\n2021-08-17T17:31:48.6845903Z     awkward_NumpyArray_getitem_next_at.cpp\r\n2021-08-17T17:31:50.0391234Z     awkward_NumpyArray_getitem_next_null.cpp\r\n2021-08-17T17:31:50.3993317Z     FormBuilder.cpp\r\n2021-08-17T17:31:51.3908465Z     awkward_NumpyArray_getitem_next_range.cpp\r\n2021-08-17T17:31:52.2087110Z     IndexedArrayBuilder.cpp\r\n2021-08-17T17:31:52.5920844Z     awkward_NumpyArray_getitem_next_range_advanced.cpp\r\n2021-08-17T17:31:53.8282816Z     awkward_NumpyArray_rearrange_shifted.cpp\r\n2021-08-17T17:31:54.8249184Z     IndexedOptionArrayBuilder.cpp\r\n2021-08-17T17:31:55.3416807Z     awkward_NumpyArray_reduce_adjust_starts_64.cpp\r\n2021-08-17T17:31:58.6792364Z     awkward_NumpyArray_reduce_adjust_starts_shifts_64.cpp\r\n2021-08-17T17:31:59.2461332Z     LayoutBuilder.cpp\r\n2021-08-17T17:31:59.9253557Z     awkward_NumpyArray_reduce_mask_ByteMaskedArray_64.cpp\r\n2021-08-17T17:32:00.9066804Z     awkward_NumpyArray_sort_asstrings_uint8.cpp\r\n2021-08-17T17:32:01.9854330Z     awkward_NumpyArray_subrange_equal.cpp\r\n2021-08-17T17:32:02.3323564Z     ListArrayBuilder.cpp\r\n2021-08-17T17:32:03.1226020Z     awkward_NumpyArray_unique_strings_uint8.cpp\r\n2021-08-17T17:32:04.2071092Z     ListOffsetArrayBuilder.cpp\r\n2021-08-17T17:32:04.2508952Z     Generating Code...\r\n2021-08-17T17:32:06.6507193Z     Compiling...\r\n2021-08-17T17:32:06.6513415Z     awkward_RegularArray_broadcast_tooffsets.cpp\r\n2021-08-17T17:32:07.1486590Z     NumpyArrayBuilder.cpp\r\n2021-08-17T17:32:08.1928502Z     awkward_RegularArray_broadcast_tooffsets_size1.cpp\r\n2021-08-17T17:32:09.2254961Z     awkward_RegularArray_combinations.cpp\r\n2021-08-17T17:32:09.3926696Z     RecordArrayBuilder.cpp\r\n2021-08-17T17:32:10.4260438Z     awkward_RegularArray_compact_offsets.cpp\r\n2021-08-17T17:32:11.4347528Z     awkward_RegularArray_getitem_carry.cpp\r\n2021-08-17T17:32:11.8668977Z     RegularArrayBuilder.cpp\r\n2021-08-17T17:32:12.4419295Z     awkward_RegularArray_getitem_jagged_expand.cpp\r\n2021-08-17T17:32:13.3841237Z     awkward_RegularArray_getitem_next_array.cpp\r\n2021-08-17T17:32:13.5563999Z     UnionArrayBuilder.cpp\r\n2021-08-17T17:32:14.4046231Z     awkward_RegularArray_getitem_next_array_advanced.cpp\r\n2021-08-17T17:32:15.3425510Z     UnmaskedArrayBuilder.cpp\r\n2021-08-17T17:32:15.3775382Z     awkward_RegularArray_getitem_next_array_regularize.cpp\r\n2021-08-17T17:32:16.6197176Z     awkward_RegularArray_getitem_next_at.cpp\r\n2021-08-17T17:32:17.3263499Z     IrregularlyPartitionedArray.cpp\r\n2021-08-17T17:32:17.7583785Z     awkward_RegularArray_getitem_next_range.cpp\r\n2021-08-17T17:32:19.0130215Z     awkward_RegularArray_getitem_next_range_spreadadvanced.cpp\r\n2021-08-17T17:32:19.1943166Z     PartitionedArray.cpp\r\n2021-08-17T17:32:20.0716491Z     awkward_RegularArray_localindex.cpp\r\n2021-08-17T17:32:20.6032002Z     ArrayType.cpp\r\n2021-08-17T17:32:21.0069845Z     awkward_RegularArray_num.cpp\r\n2021-08-17T17:32:22.1198619Z     Generating Code...\r\n2021-08-17T17:32:22.5405330Z     awkward_RegularArray_rpad_and_clip_axis1.cpp\r\n2021-08-17T17:32:25.2620565Z     awkward_UnionArray_fillindex.cpp\r\n2021-08-17T17:32:28.2063035Z     awkward_UnionArray_fillindex_count.cpp\r\n2021-08-17T17:32:30.8387702Z     awkward_UnionArray_fillna.cpp\r\n2021-08-17T17:32:33.5469414Z     awkward_UnionArray_filltags.cpp\r\n2021-08-17T17:32:37.0642528Z     awkward_UnionArray_filltags_const.cpp\r\n2021-08-17T17:32:41.3095706Z     Generating Code...\r\n2021-08-17T17:32:41.4501936Z     Compiling...\r\n2021-08-17T17:32:41.4509099Z     awkward_UnionArray_flatten_combine.cpp\r\n2021-08-17T17:32:44.7361168Z     awkward_UnionArray_flatten_length.cpp\r\n2021-08-17T17:32:48.9277331Z     awkward_UnionArray_nestedfill_tags_index.cpp\r\n2021-08-17T17:32:50.9807991Z     awkward_UnionArray_project.cpp\r\n2021-08-17T17:32:54.3707085Z     awkward_UnionArray_regular_index.cpp\r\n2021-08-17T17:32:55.5341204Z     Compiling...\r\n2021-08-17T17:32:55.5351548Z     ListType.cpp\r\n2021-08-17T17:32:55.7909709Z     awkward_UnionArray_regular_index_getsize.cpp\r\n2021-08-17T17:32:56.9913625Z     awkward_UnionArray_simplify.cpp\r\n2021-08-17T17:32:57.5673496Z     OptionType.cpp\r\n2021-08-17T17:32:58.1343007Z     awkward_UnionArray_simplify_one.cpp\r\n2021-08-17T17:32:59.4077813Z     PrimitiveType.cpp\r\n2021-08-17T17:32:59.4815004Z     awkward_UnionArray_validity.cpp\r\n2021-08-17T17:33:01.1090126Z     awkward_argsort.cpp\r\n2021-08-17T17:33:01.6296161Z     RecordType.cpp\r\n2021-08-17T17:33:03.2579633Z     awkward_carry_arange.cpp\r\n2021-08-17T17:33:03.6995407Z     RegularType.cpp\r\n2021-08-17T17:33:04.2809337Z     awkward_combinations.cpp\r\n2021-08-17T17:33:05.1981468Z     Type.cpp\r\n2021-08-17T17:33:05.2932826Z     awkward_content_reduce_zeroparents_64.cpp\r\n2021-08-17T17:33:06.3513428Z     awkward_index_carry_nocheck.cpp\r\n2021-08-17T17:33:06.9692002Z     UnionType.cpp\r\n2021-08-17T17:33:07.5748923Z     awkward_index_rpad_and_clip_axis0.cpp\r\n2021-08-17T17:33:08.8550436Z     UnknownType.cpp\r\n2021-08-17T17:33:08.9062923Z     awkward_index_rpad_and_clip_axis1.cpp\r\n2021-08-17T17:33:10.3308294Z     awkward_localindex.cpp\r\n2021-08-17T17:33:10.7852111Z     util.cpp\r\n2021-08-17T17:33:11.8065385Z     awkward_missing_repeat.cpp\r\n2021-08-17T17:33:13.8818427Z     ArrayCache.cpp\r\n2021-08-17T17:33:13.9538488Z     awkward_new_Identities.cpp\r\n2021-08-17T17:33:14.9877929Z     awkward_one_mask.cpp\r\n2021-08-17T17:33:15.4737085Z     ArrayGenerator.cpp\r\n2021-08-17T17:33:16.0230045Z     Generating Code...\r\n2021-08-17T17:33:18.7651792Z     Generating Code...\r\n2021-08-17T17:33:25.0862776Z     Compiling...\r\n2021-08-17T17:33:25.0866436Z     awkward_quick_argsort.cpp\r\n2021-08-17T17:33:27.6403989Z     awkward_quick_sort.cpp\r\n2021-08-17T17:33:31.0775788Z     awkward_reduce_argmax.cpp\r\n2021-08-17T17:33:32.3745562Z     awkward_reduce_argmax_bool_64.cpp\r\n2021-08-17T17:33:33.3365998Z     awkward_reduce_argmax_complex.cpp\r\n2021-08-17T17:33:34.4395609Z     awkward_reduce_argmin.cpp\r\n2021-08-17T17:33:35.3634673Z     awkward_reduce_argmin_bool_64.cpp\r\n2021-08-17T17:33:36.2520607Z     awkward_reduce_argmin_complex.cpp\r\n2021-08-17T17:33:37.1483141Z     awkward_reduce_count_64.cpp\r\n2021-08-17T17:33:38.1306348Z     awkward_reduce_countnonzero.cpp\r\n2021-08-17T17:33:39.1469734Z     awkward_reduce_countnonzero_complex.cpp\r\n2021-08-17T17:33:40.8930446Z     awkward_reduce_max.cpp\r\n2021-08-17T17:33:42.2628709Z     awkward_reduce_max_complex.cpp\r\n2021-08-17T17:33:43.3011258Z     awkward_reduce_min.cpp\r\n2021-08-17T17:33:44.3161624Z     awkward_reduce_min_complex.cpp\r\n2021-08-17T17:33:45.7222562Z     awkward_reduce_prod.cpp\r\n2021-08-17T17:33:46.8528626Z     awkward_reduce_prod_bool.cpp\r\n2021-08-17T17:33:47.8127977Z     awkward_reduce_prod_bool_complex.cpp\r\n2021-08-17T17:33:48.6900844Z     awkward_reduce_prod_complex.cpp\r\n2021-08-17T17:33:49.7179847Z     awkward_reduce_prod_int32_bool_64.cpp\r\n2021-08-17T17:33:50.5741102Z     Generating Code...\r\n2021-08-17T17:33:51.9275620Z     Compiling...\r\n2021-08-17T17:33:51.9276806Z     awkward_reduce_prod_int64_bool_64.cpp\r\n2021-08-17T17:33:52.8019683Z     awkward_reduce_sum.cpp\r\n2021-08-17T17:33:53.6761497Z     awkward_reduce_sum_bool.cpp\r\n2021-08-17T17:33:54.5147208Z     awkward_reduce_sum_bool_complex.cpp\r\n2021-08-17T17:33:55.4566212Z     awkward_reduce_sum_complex.cpp\r\n2021-08-17T17:33:56.3011876Z     awkward_reduce_sum_int32_bool_64.cpp\r\n2021-08-17T17:33:57.0643939Z     awkward_reduce_sum_int64_bool_64.cpp\r\n2021-08-17T17:33:57.8540459Z     awkward_regularize_arrayslice.cpp\r\n2021-08-17T17:33:58.6992371Z     awkward_slicearray_ravel.cpp\r\n2021-08-17T17:33:59.5248237Z     awkward_slicemissing_check_same.cpp\r\n2021-08-17T17:34:00.2670665Z     awkward_sort.cpp\r\n2021-08-17T17:34:01.7209592Z     awkward_sorting_ranges.cpp\r\n2021-08-17T17:34:02.5961829Z     awkward_sorting_ranges_length.cpp\r\n2021-08-17T17:34:03.5278603Z     awkward_unique.cpp\r\n2021-08-17T17:34:04.2680972Z     awkward_zero_mask.cpp\r\n2021-08-17T17:34:05.3049695Z     kernel-utils.cpp\r\n2021-08-17T17:34:06.2176047Z     Generating Code...\r\n2021-08-17T17:34:13.5169485Z     awkward-cpu-kernels-objects.vcxproj -> C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\awkward-cpu-kernels-objects.dir\\Release\\awkward-cpu-kernels-objects.lib\r\n2021-08-17T17:34:14.0765707Z     Building Custom Rule C:/Users/VssAdministrator/AppData/Local/Temp/pip-req-build-d2fcd6bg/CMakeLists.txt\r\n2021-08-17T17:34:14.8310093Z     awkward-cpu-kernels-static.vcxproj -> C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\pip-req-build-d2fcd6bg\\build\\temp.win-amd64-3.9\\Release\\Release\\awkward-cpu-kernels-static.lib\r\n```",
  "created_at":"2021-08-17T18:23:53Z",
  "id":900530628,
  "issue":1062,
  "node_id":"IC_kwDODBCWws41rQHE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-17T18:23:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The second batch of warnings is something that might have been there for a while. (I used to scour CI builds for warnings, but haven't recently.) Generally, we try to use `int64_t` for integers everywhere, and it looks like `awkward_ListOffsetArray_argsort_strings` broke that rule.\r\n\r\nThe syntax errors around NumpyArray.cpp line 4708 looks more relevant. There are some Windows-specific `#ifdefs`, so it's not impossible for a syntax error to show up only on Windows.",
  "created_at":"2021-08-17T18:27:26Z",
  "id":900532941,
  "issue":1062,
  "node_id":"IC_kwDODBCWws41rQrN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-17T18:27:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't know that `and` is a C++ keyword, and maybe Microsoft doesn't, either. That seems to be what the first syntax error is complaining about. At least, if `and` is legal, it hasn't been used anywhere else in the codebase, so `&&` should be used for homogeneity.\r\n\r\nWhy is that block of code a zero-argument lambda that just gets called? What's that for? Oh, I get it: because NumpyArray is a non-pointer, and it needs to be defined one of two different ways.\r\n\r\nIt's weird that I'm having trouble coming up with a solution to this. It ought to be possible to write this down without a lambda expression, whose support might be partial in the C++11 era. (Maybe it's the `[&]` to carry all closures into the lambda.)\r\n\r\nIt wouldn't be terrible to assign it as a `std::shared_ptr` and access it by dereferencing the raw pointer throughout the rest of the function (which goes to line 4804). That is, replacing `out.` \u2192 `out.get()->` or redefine `out` so that uses of it later in the function do not need to be changed. Calling `shallow_copy` to get a `std::shared_ptr` would not be expensive on the scale that you're considering, although having to reinterpret_cast or dynamic_cast the raw pointer (because `shallow_copy` forgets that it's a NumpyArray) is unpleasant.\r\n\r\nWhat I should say is that I won't complain about code style issues if you do it with reinterpret_cast or dynamic_cast (having just set the pointer, so you're 100% certain what class it is). Lambda functions are under-tested in this environment, which uses C++11 to avoid having to install custom compilers in the manylinux1 wheel-builders.",
  "created_at":"2021-08-17T18:44:27Z",
  "id":900544059,
  "issue":1062,
  "node_id":"IC_kwDODBCWws41rTY7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-17T18:44:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"My bad, I found the build log the second time I looked :)\r\nLooks like MSVC just doesn't like `and` instead of `&&`",
  "created_at":"2021-08-17T18:45:30Z",
  "id":900544765,
  "issue":1062,
  "node_id":"IC_kwDODBCWws41rTj9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-17T18:45:30Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> It's weird that I'm having trouble coming up with a solution to this.\r\n\r\nIsn't it? I was originally going to do a shared pointer, but then I thought the lambda trick is perfect for this case. It should be supported in C++11, though of course MSVC can be a bit unreliable.\r\nedit: ok the `and` issue was just tripping up the lambda parsing, seems ok now.",
  "created_at":"2021-08-17T18:49:36Z",
  "id":900547454,
  "issue":1062,
  "node_id":"IC_kwDODBCWws41rUN-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-17T18:59:06Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Absolutely, I'll take a look tomorrow!\n\nOn Tue, 24 Aug 2021, 17:01 Ianna Osborne, ***@***.***> wrote:\n\n> ***@***.**** commented on this pull request.\n> ------------------------------\n>\n> In docs-src/how-to-create-layoutbuilder.md\n> <https://github.com/scikit-hep/awkward-1.0/pull/1063#discussion_r694991980>\n> :\n>\n> > +builder.tag(1)\n> +builder.boolean(True)\n> +builder.tag(0)\n> +builder.float64(4.4)\n> +builder.tag(1)\n> +builder.boolean(False)\n> +builder.tag(1)\n> +builder.boolean(True)\n> +builder.tag(0)\n> +builder.float64(-2.2)\n> +```\n> +\n> +Snapshot\n> +--------\n> +\n> +To turn a #ak::LauoutBuilder into a layout, call `snapshot`. This is an inexpensive operation (may be done multiple times; the builder is unaffacted).\n>\n> @agoose77 <https://github.com/agoose77> - Can I take you up on your kind\n> offer of code review? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-hep/awkward-1.0/pull/1063#discussion_r694991980>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAJQZHIWEM6EMCB4O5KJWIDT6O66VANCNFSM5CLWECQA>\n> .\n>\n",
  "created_at":"2021-08-24T19:33:47Z",
  "id":904918368,
  "issue":1063,
  "node_id":"IC_kwDODBCWws417_Vg",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-08-24T19:33:47Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, I'm done with this PR. It introduces `LayoutBuilder32` and `LayoutBuilder64`, plus the error handling is improved, I think.",
  "created_at":"2021-08-24T20:46:55Z",
  "id":904962636,
  "issue":1063,
  "node_id":"IC_kwDODBCWws418KJM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-24T20:46:55Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and I'm holding off on merging until you decide what you want to do about @agoose77's comments, @ianna. Although when you've addressed everything, you can do the merge. That's what my sign-off above means.",
  "created_at":"2021-08-25T16:55:12Z",
  "id":905708537,
  "issue":1063,
  "node_id":"IC_kwDODBCWws41_AP5",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-08-25T16:55:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @SantamRC  for test",
  "created_at":"2021-09-09T17:05:39Z",
  "id":916279102,
  "issue":1065,
  "node_id":"IC_kwDODBCWws42nU8-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T17:05:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/awkward-1.0/pull/1088) to add @SantamRC! :tada:",
  "created_at":"2021-09-09T17:05:47Z",
  "id":916279181,
  "issue":1065,
  "node_id":"IC_kwDODBCWws42nU-N",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T17:05:47Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"The `np.ma.MaskedArray` and its mask were getting lost while constructing a structured array. I hadn't thought about the combination of these two things (actually, didn't know it was possible: the mask must be a structured array as well).\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/f9aab6f25e6afe6cce42b7b2590fa4bdff4cc205/tests/test_1066-to_numpy-masked-structured-array.py#L11-L16",
  "created_at":"2021-08-20T17:11:22Z",
  "id":902836433,
  "issue":1066,
  "node_id":"IC_kwDODBCWws410DDR",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-08-20T17:11:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I can reproduce this, and the `to_pydict` representation of these tables is valid. I assume we support empty field names (Jim may correct me), because the third test case works.",
  "created_at":"2021-08-20T20:40:08Z",
  "id":902945255,
  "issue":1068,
  "node_id":"IC_kwDODBCWws410dnn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-20T20:40:08Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The cause of this behaviour appears to be https://github.com/scikit-hep/awkward-1.0/blob/533f3852b5528543ad6b9c81d8f7d6fede69cb4c/src/awkward/operations/convert.py#L2902\r\n\r\nThis was introduced in https://github.com/scikit-hep/awkward-1.0/pull/606/commits/6dca8621e75c28b4bb36524ad90c7ad5c9299c46\r\n\r\nI don't know *why* we do this \u2014 it reminds me of the lazy column reading with Parquet, but a cursory glance doesn't ring any bells.",
  "created_at":"2021-08-20T20:51:47Z",
  "id":902950554,
  "issue":1068,
  "node_id":"IC_kwDODBCWws410e6a",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-20T20:51:47Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"What part of that commit is the problem?\r\n\r\nArrow Tables and Parquet files require named fields, but Awkward Arrays can be non-records. For non-records, we use the empty string as a field name, so the empty string is treated differently. I don't know if that's the problem here. I hadn't considered that empty string column names would be desirable in a context with real records (more than one field, more than a formality to fit the technology).",
  "created_at":"2021-08-21T15:08:35Z",
  "id":903130623,
  "issue":1068,
  "node_id":"IC_kwDODBCWws411K3_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-21T15:08:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> What part of that commit is the problem?\r\n> \r\n> Arrow Tables and Parquet files require named fields, but Awkward Arrays can be non-records. For non-records, we use the empty string as a field name, so the empty string is treated differently. I don't know if that's the problem here. I hadn't considered that empty string column names would be desirable in a context with real records (more than one field, more than a formality to fit the technology).\r\n\r\nIt's exactly this - we use the presence of only an empty key to implicitly mean \"this is an (Awkward) non-record array\". @rocurley in this case (I assume) wants to be able to load such a table and maintain the structure. \r\n\r\nRegardless, this cannot be \"solved\" - any convention that applies meaning to the key will ultimately collide with some value chosen by a particularly determined user. But, maybe an Awkward prefix would be better than an empty key?Perhaps this issue should become should be a discussion, to welcome other Arrow users' opinions.",
  "created_at":"2021-08-21T15:28:57Z",
  "id":903133265,
  "issue":1068,
  "node_id":"IC_kwDODBCWws411LhR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-21T15:30:13Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Then what it boils down to us that we wanted a space larger than the set of all possible records; we wanted records and non-records, so we took a corner out of the space of records to represent the non-records. Now those records are not representable. We could have picked a special-sounding key, like \"AwkwardFieldNotARecord,\" but it seemed like a better idea to use a special value (the only string of length zero) for this special meaning.\r\n\r\nChanging it now would touch a lot of code\u2014it wouldn't be clean and easy, though that's not the strongest agreement against it. The strongest argument is that some people have saved files with the one policy, but if new versions of Awkward Array use a different policy for interpreting single-key, empty key name records, the existing files will be interested differently\u2014it would not be backward compatible.\r\n\r\nThere is something that we _could_ do: we could introduce another convention to encode the single-key, empty key name case in yet another structure. That is, if you really wanted to save this Awkward Array:\r\n\r\n```\r\n{\"\": XYZ}\r\n```\r\n\r\nthen we would put it in a file containing\r\n\r\n```\r\n{\"\": {\"\": XYZ}}\r\n```\r\n\r\nOkay, so what if you wanted the latter? Well, we'd save it in s file that contained three nested, single-key, empty key name records. A single rule could shift everything down by one, and even though this breaks existing uses of these structures, I doubt anyone's using them seriously.\r\n\r\nDoing so, we'd get the whole space that we want: all records and non-records. The space of records is infinite, and that means that we can make a bijection to wither infinite space with strictly more things in it\u2014this is [Hilbert's hotel](https://en.wikipedia.org/wiki/Hilbert%27s_paradox_of_the_Grand_Hotel?wprov=sfla1).\r\n\r\nWe could have done this uniformly with _all_ structures, but if we did, then the Parquet view of all Awkward Arrays would be strange. This way, only strange structures get a strange encoding. (Strange enough, though, that I expect it would be raised as another bug report, when discovered!)\r\n\r\nI think we should make this a discussion, but let's hear back from @rocurley first.",
  "created_at":"2021-08-22T11:51:16Z",
  "id":903256809,
  "issue":1068,
  "node_id":"IC_kwDODBCWws411prp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-22T11:51:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Changing it now would touch a lot of code\u2014it wouldn't be clean and easy, though that's not the strongest agreement against it. The strongest argument is that some people have saved files with the one policy, but if new versions of Awkward Array use a different policy for interpreting single-key, empty key name records, the existing files will be interested differently\u2014it would not be backward compatible.\r\n\r\nThat's a really important point that I failed to mention yesterday \u2014 any change to the convention here breaks reading old files. As you say, we'd want a strong case to be made in order to do that. ",
  "created_at":"2021-08-22T12:08:48Z",
  "id":903259092,
  "issue":1068,
  "node_id":"IC_kwDODBCWws411qPU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-22T12:09:03Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"Hey folks!\r\n\r\nSo the context for how I encountered this is that I'm trying to read/write data in a very similar format to an Awkward Array from/to parquet, and I've got a [property test](https://hypothesis.readthedocs.io/en/latest/index.html) set up to generate random data and round-trip it. Hypothesis likes to generate \"smaller\" values, such as the empty string, and so it discovered this corner case.\r\n\r\nPractically speaking, it was pretty easy to solve this issue on my end: I just made it an error to make a column with an empty string as a name. Since my column names are directly human generated anyway, that wasn't much of a problem. Given that, I arguably no longer have a horse in this race. The reason I reported this as a bug anyway is that it felt like it made the library less predictable, which is something I value.\r\n\r\nI've been thinking about what circumstances could cause somebody to run into this outside the context of a test. I think the most likely case is if you're using it more as a dict than as a struct: your column names are generated at runtime. Maybe something like \"I'm recording the visitor counts per hour to each subdomain of my website (including the empty subdomain) and using awkward array to save it to parquet\". What does that person want? I think, mostly, they don't want the special case, but that's not on the table for backwards compatibility. Failing that, I think they want reliable round-tripping through parquet. The Hilbert hotel approach will get them that, so that's nice. On the other hand, you could imagine that they're writing the data to parquet so that some other system can consume it: in that case they want the current behavior.\r\n\r\nNot really sure what would be best here!",
  "created_at":"2021-08-22T16:24:11Z",
  "id":903294005,
  "issue":1068,
  "node_id":"IC_kwDODBCWws411yw1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-22T16:24:11Z",
  "user":"MDQ6VXNlcjE0OTA4MzI="
 },
 {
  "author_association":"MEMBER",
  "body":"The Awkward \u2192 Parquet \u2192 Awkward round trip can be made an identity function, which is more predictable, through the Hilbert Hotel encoding, but that makes the Awkward \u2192 Parquet step more complex and less predictable. The reason we're not wrapping everything in `{\"\":` is because we want the Parquet to be understandable in other (non-Awkward) programs. We can't have both Awkward \u2192 Parquet and Awkward \u2192 Parquet \u2192 Awkward both be simple because Parquet encodes a different space of data structures than Awkward.\r\n\r\nSince this was motivated by hypothesis testing the edge cases, not a real use-case, I think we shouldn't change it yet.\r\n\r\nAs for the possibility of a real use-case in which records are treated more like dicts than structs: that's an antipattern we should discourage. Field names in C structs are not generated because they get \"compiled in,\" there's a presumption that they're not dynamic (and can't be, unless you've got a JIT compiler). Trying to make them dynamic would make the performance worse, rather than better. Record names in Awkward Arrays are like that, too: each field creates its own memory buffer (and data type, and array infrastructure), so if they're dynamically created, the performance would be worse; it would make a lot more sense in that case to use a Python dict than an Awkward Array.\r\n\r\nOn the other hand, I should point out that a \"mapping\" data structure is a future feature (#780), but the structure is not a record with dynamically generated field names: it's two equal length arrays of keys and values (thus enforcing that the keys all have the same type as each other in one compact array, rather than an array and type for each key), in which the keys are sorted for binary searches. Uproot generates arrays of this type when it encounters a C++ `std::map` in a ROOT file, but there isn't anything in Awkward Array yet to use it for fast lookups.\r\n\r\nI'll make this a discussion.",
  "created_at":"2021-08-23T14:31:44Z",
  "id":903828716,
  "issue":1068,
  "node_id":"IC_kwDODBCWws4131Ts",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-23T14:31:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Whereas `mask_identity` is optional on most functions, it really doesn't make sense to have it on `argmin` and `argmax` (any `argX`). It's only there for completeness.\r\n\r\nIn general, the node is only added if `mask_identity` (just `mask` in C++) is true:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/b8c3187f48aab8618a728537d22a71b6a177c505/src/libawkward/array/NumpyArray.cpp#L3331-L3345\r\n\r\nBut you're getting an extra one because IndexedArray does a `simplify_optiontype`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/b8c3187f48aab8618a728537d22a71b6a177c505/src/libawkward/array/IndexedArray.cpp#L2265-L2272\r\n\r\nEven if `ISOPTION` is false, it should still do the `simplify_optiontype` because this also prevents an IndexedArray from being inside an IndexedArray (double indirection is bad, even without option-type). But the result of such a merger should be an IndexedArray, not an IndexedOptionArray. The above code does not allow for that possibility because it immediately constructs an IndexedOptionArray and calls `simplify_optiontype` on it.\r\n\r\nFortunately, [IndexedArray::simplify_optiontype](https://github.com/scikit-hep/awkward-1.0/blob/b8c3187f48aab8618a728537d22a71b6a177c505/src/libawkward/array/IndexedArray.cpp#L653-L956) knows that IndexedArray \u2218 IndexedArray = IndexedArray, so this bug could be fixed by only replacing the `IndexedOptionArray64(...).simplify_optiontype()` call in the above-quoted code with `IndexedArray64(...).simplify_optiontype()` if not `ISOPTION`: i.e. splitting the two cases of \"is option\" and \"is not option\" at this level of the tree, and `simplify_optiontype` will do the right thing if there are option or non-option types at the next level down.",
  "created_at":"2021-08-24T16:56:59Z",
  "id":904815244,
  "issue":1071,
  "node_id":"IC_kwDODBCWws417mKM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-24T16:56:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\nnparray = np.random.rand(1000000)\r\nv2_array = ak._v2.contents.NumpyArray(nparray)\r\n```\r\nHere is a one dimensional `v2_array` `NumpyArray` sorted by `numpy.sort()` compared with sorting the same `numpy` array:\r\n```python\r\nIn [9]: %timeit v2_array.sort()\r\n18.5 ms \u00b1 310 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [10]: %timeit nparray.sort()\r\n17.3 ms \u00b1 297 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\nThe same for a one dimensional `v2_array` `NumpyArray` sorted by `std::stable_sort`:\r\n```python\r\nIn [10]: %timeit v2_array.sort()\r\n1.26 s \u00b1 11.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [11]: %timeit nparray.sort()\r\n17.3 ms \u00b1 127 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\nUsing `std::sort` on non contiguous `NumpArray` compared with `numpy`:\r\n```python\r\nIn [3]: matrix = np.arange(64).reshape(8, -1)\r\n\r\nIn [4]: v2_array = ak._v2.contents.NumpyArray(matrix[:, 0])\r\n\r\nIn [5]: nparray = matrix[:, 0]\r\n\r\nIn [11]: %timeit v2_array.sort()\r\n1.05 ms \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\nIn [12]: %timeit nparray.sort()\r\n249 ns \u00b1 1.56 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\r\n```",
  "created_at":"2021-08-31T16:17:40Z",
  "id":909383450,
  "issue":1072,
  "node_id":"IC_kwDODBCWws42NBca",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-31T16:17:40Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"It's not a question of how fast NumPy can sort one large array; it's a question of how fast it can sort all the little arrays in a ListArray/ListOffsetArray. For instance, this would be a good measure of wall time:\r\n\r\n```python\r\nimport time\r\nimport numpy as np\r\n\r\ndef test(average_size):\r\n    counts = np.random.poisson(average_size, 1000000)\r\n    offsets = np.empty(1000000 + 1, np.int64)\r\n    offsets[0] = 0\r\n    offsets[1:] = np.cumsum(counts)\r\n    content = np.random.normal(0, 1, offsets[-1])\r\n    starts, stops = offsets[:-1], offsets[1:]\r\n    starttime = time.time()\r\n    for i in range(1000000):\r\n        content[starts[i]:stops[i]].sort()\r\n    return time.time() - starttime\r\n```\r\n\r\nThis sorts a ListOffsetArray of length 1000000 that contains irregular-length lists of `average_size`. Some physics problems are going to involve reasonably large lists:\r\n\r\n```python\r\n>>> test(10.0)\r\n0.5590884685516357\r\n```\r\n\r\nbut some are going to involve very short lists:\r\n\r\n```python\r\n>>> test(10.0)\r\n0.5590884685516357\r\n>>> test(3.0)\r\n0.43043017387390137\r\n>>> test(2.0)\r\n0.3896183967590332\r\n>>> test(1.5)\r\n0.3658483028411865\r\n>>> test(1.0)\r\n0.33797550201416016\r\n>>> test(0.5)\r\n0.30232715606689453\r\n>>> test(0.1)\r\n0.2877922058105469\r\n>>> test(0.01)\r\n0.2890009880065918\r\n```\r\n\r\nIn the last case, very few of the lists have 1 element, let alone the 2 elements at which sorting starts to be meaningful. That's why I suggested the `starts - stops >= 2` mask:\r\n\r\n```python\r\ndef test2(average_size):\r\n    counts = np.random.poisson(average_size, 1000000)\r\n    offsets = np.empty(1000000 + 1, np.int64)\r\n    offsets[0] = 0\r\n    offsets[1:] = np.cumsum(counts)\r\n    content = np.random.normal(0, 1, offsets[-1])\r\n    starts, stops = offsets[:-1], offsets[1:]\r\n    # computing the mask counts toward the total time\r\n    starttime = time.time()\r\n    mask = stops - starts >= 2\r\n    starts = starts[mask]\r\n    stops = stops[mask]\r\n    for i in range(len(starts)):\r\n        content[starts[i]:stops[i]].sort()\r\n    return time.time() - starttime\r\n```\r\n\r\nNow, rather than Python spinning its wheels over a bunch of nearly empty lists, it just skips them, and short lists become a good thing, rather than a bad thing:\r\n\r\n```python\r\n>>> test2(10.0)\r\n0.5271399021148682\r\n>>> test2(3.0)\r\n0.37287163734436035\r\n>>> test2(2.0)\r\n0.2766532897949219\r\n>>> test2(1.5)\r\n0.20734500885009766\r\n>>> test2(1.0)\r\n0.12771391868591309\r\n>>> test2(0.5)\r\n0.045095205307006836\r\n>>> test2(0.1)\r\n0.004815340042114258\r\n>>> test2(0.01)\r\n0.0021109580993652344\r\n```\r\n\r\nI was also thinking that hard-coding a special kernel for the `stops - starts == 2` cases could be a good thing (and maybe even `stops - starts == 3`, but eventually you hit diminishing returns):\r\n\r\n```python\r\nstarts, stops = offsets[:-1], offsets[1:]\r\nstarts_2 = starts[stops - starts == 2]\r\nmask = stops - starts > 2\r\nstarts = starts[mask]\r\nstops = stops[mask]\r\nnplike[\"awkward_sort_length_2_kernel\"](content, starts_2, len(starts_2))\r\nfor i in range(len(starts)):\r\n    content[starts[i]:stops[i]].sort()\r\n```\r\n\r\nwhere the `awkward_sort_length_2_kernel` does\r\n\r\n```c++\r\nfor (int64_t i = 0;  i < len_starts_2;  i++) {\r\n    T content_0 = content[starts_2[i]];\r\n    T content_1 = content[starts_2[i] + 1];\r\n    if (content_0 > content_1) {\r\n        content[starts_2[i]] = content_1;\r\n        content[starts_2[i] + 1] = content_0;\r\n    }\r\n}\r\n```\r\n\r\nA compiler would _love_ to optimize that kernel, and I think the optimization of the many-small-sortings problem hasn't been investigated\u2014certainly not as much as sorting one big array.\r\n\r\nThe closest thing NumPy has to this is sorting along an axis, which is not the same because those list sizes are regular, not variable. For this case, however, NumPy is 5\u201240\u00d7 faster than iteration:\r\n\r\n```python\r\ndef test3(exact_size):\r\n    content = np.random.normal(0, 1, 1000000 * exact_size)\r\n    offsets = np.arange(0, 1000000 * exact_size, exact_size)\r\n    starts, stops = offsets[:-1], offsets[1:]\r\n    starttime = time.time()\r\n    for i in range(len(starts)):\r\n        content[starts[i]:stops[i]].sort()\r\n    return time.time() - starttime\r\n\r\ndef test3_numpy(exact_size):\r\n    content = np.random.normal(0, 1, (1000000, exact_size))\r\n    starttime = time.time()\r\n    content.sort(axis=1)\r\n    return time.time() - starttime\r\n```\r\n\r\n```python\r\n>>> test3(10)\r\n0.5516564846038818\r\n>>> test3_numpy(10)\r\n0.09245777130126953\r\n\r\n>>> test3(4)\r\n0.49339795112609863\r\n>>> test3_numpy(4)\r\n0.02732539176940918\r\n\r\n>>> test3(3)\r\n0.4491863250732422\r\n>>> test3_numpy(3)\r\n0.018769264221191406\r\n\r\n>>> test3(2)\r\n0.4407010078430176\r\n>>> test3_numpy(2)\r\n0.011630535125732422\r\n```\r\n\r\nIt suggests that even though iteration in Python can be surprisingly fast sometimes, it's still dominating over the actual sorting of data by an order of magnitude. They don't start to get close until the list sizes get to be about 100 or so:\r\n\r\n```python\r\n>>> test3(100)\r\n2.723106622695923\r\n>>> test3_numpy(100)\r\n2.1994996070861816\r\n```\r\n\r\nincluding for irregularly sized lists:\r\n\r\n```python\r\n>>> test(100)\r\n2.717956304550171\r\n```\r\n\r\nSo any ListArray/ListOffsetArray sorting based on Python iteration will be spending much more time going through Python overhead than actually sorting if the user's lists are much shorter than 100, and that is typical for physics cases, which often enough have average sizes of 0.1, 1.0, 2.0, up to maybe 10.0. So Python iteration, using NumPy's built-in sorting, isn't a good idea.",
  "created_at":"2021-08-31T19:28:42Z",
  "id":909541429,
  "issue":1072,
  "node_id":"IC_kwDODBCWws42NoA1",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-08-31T19:28:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I get the following error on builds with Python 2.7:\r\n```python\r\nself = <ListArray len='3'>\r\n    <starts><Index dtype='int64' len='3'>[0 3 5]</Index></...]\r\n        </NumpyArray></content>\r\n    </ListOffsetArray></content>\r\n</ListArray>\r\noffsets = <Index dtype='int64' len='4'>[0 3 3 5]</Index>\r\n\r\n    def _broadcast_tooffsets64(self, offsets):\r\n>       return ListOffsetArray._broadcast_tooffsets64(self, offsets)\r\nE       TypeError: unbound method _broadcast_tooffsets64() must be called with ListOffsetArray instance as first argument (got ListArray instance instead)\r\n\r\nC:\\hostedtoolcache\\windows\\Python\\2.7.18\\x86\\lib\\site-packages\\awkward\\_v2\\contents\\listarray.py:184: TypeError\r\n\r\n```\r\n\r\nShall I ignore it or fix it? Thanks",
  "created_at":"2021-09-10T17:14:14Z",
  "id":917071285,
  "issue":1072,
  "node_id":"IC_kwDODBCWws42qWW1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-10T17:14:14Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - I get the following error on builds with Python 2.7:\r\n> Shall I ignore it or fix it?\r\n\r\nYou can copy the blanket `pytestmark = skip Python 2.7` that we have on the other v2 test files. We'll be dropping Python 2 support (https://github.com/scikit-hep/awkward-1.0/discussions/1010) on a similar timescale to releasing Awkward version 2, so there isn't much point in making v2 work for Python 2.\r\n\r\nOn the other hand, I know what it is: Python 3 methods are just plain functions, which we use to consolidate implementations so that ListOffsetArray can just use ListArray's implementation of some methods. (Also for methods common to Content and Form.) This _could_ be handled more carefully in Python 2 by constructing an instance of `types.MethodType` that rebinds a copy of a ListArray method for ListOffsetArray, and that would probably work just as well in Python 3. But if we're dropping Python 2 support anyway, why bother?",
  "created_at":"2021-09-10T17:57:33Z",
  "id":917099845,
  "issue":1072,
  "node_id":"IC_kwDODBCWws42qdVF",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-09-10T17:57:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"build on MacOS with Python 2.7 took longer then expected :-( ",
  "created_at":"2021-09-10T18:42:23Z",
  "id":917128508,
  "issue":1072,
  "node_id":"IC_kwDODBCWws42qkU8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-10T18:42:23Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> build on MacOS with Python 2.7 took longer then expected :-(\r\n\r\nI've just re-run it and it looks fine",
  "created_at":"2021-09-10T21:32:28Z",
  "id":917225196,
  "issue":1072,
  "node_id":"IC_kwDODBCWws42q77s",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-10T21:32:28Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I am done with all the changes, so you can merge this, after the tests pass. Thank You!",
  "created_at":"2021-09-03T15:11:02Z",
  "id":912610932,
  "issue":1074,
  "node_id":"IC_kwDODBCWws42ZVZ0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-03T15:11:02Z",
  "user":"MDQ6VXNlcjg4Mjg5MDg2"
 },
 {
  "author_association":"MEMBER",
  "body":"I can't imagine what that different thing might be. The whole reason we have this print-out is to diagnose internal stuff; if the print-out strings are identical, the data should be identical as well.\r\n\r\nI've reproduced the above. It doesn't survive a round-trip through v2:\r\n\r\n```python\r\n>>> from awkward._v2.tmp_for_testing import v1_to_v2, v2_to_v1\r\n\r\n>>> ak.max(ak.Array(v2_to_v1(v1_to_v2(X.layout))), axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * var * ?int64'>\r\n\r\n>>> ak.max(ak.Array(v2_to_v1(v1_to_v2(Y.layout))), axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * var * ?int64'>\r\n```\r\n\r\nIt doesn't seem to be related to the `ak.Array` high-level wrapper:\r\n\r\n```python\r\n>>> ak.max(ak.Array(X.layout), axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * 1 * ?int64'>\r\n>>> ak.max(ak.Array(Y.layout), axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * var * ?int64'>\r\n```\r\n\r\nIt (fortunately!) doesn't appear to be time/order-dependent:\r\n\r\n```python\r\n>>> ak.max(X, axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * 1 * ?int64'>\r\n>>> ak.max(Y, axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * var * ?int64'>\r\n>>> ak.max(Y, axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * var * ?int64'>\r\n>>> ak.max(X, axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * 1 * ?int64'>\r\n```\r\n\r\nBut it seems to be in the ListOffsetArray64 node, not in the NumpyArray node:\r\n\r\n```python\r\n>>> XX = ak.Array(ak.layout.ListOffsetArray64(X.layout.offsets, X.layout.content))\r\n>>> ak.max(X, axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * 1 * ?int64'>\r\n>>> ak.max(XX, axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * var * ?int64'>\r\n```\r\n\r\nFinding the point where the walk through `ak.max` diverges should reveal what, exactly, is different about them.",
  "created_at":"2021-08-25T22:14:21Z",
  "id":905912396,
  "issue":1075,
  "node_id":"IC_kwDODBCWws41_yBM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-25T22:14:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes I noticed that it doesn't survive any kind of dump-and-serialise, so it is very likely something that isn't supposed to be stateful is in fact stateful! ",
  "created_at":"2021-08-25T22:15:35Z",
  "id":905912994,
  "issue":1075,
  "node_id":"IC_kwDODBCWws41_yKi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-25T22:15:35Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Woah, it could be this suspicious looking bool:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/a8e8edf8e622b5b6ca550d901c5c161649e8530d/include/awkward/array/ListOffsetArray.h#L448-L453",
  "created_at":"2021-08-25T22:16:26Z",
  "id":905913370,
  "issue":1075,
  "node_id":"IC_kwDODBCWws41_yQa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-25T22:16:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, that's only used in reducers, only in ListOffsetArray (not even copied in `shallow_copy`, making it very volatile, since `shallow_copy` happens a lot).\r\n\r\nIt causes exactly the effect that you see:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/a8e8edf8e622b5b6ca550d901c5c161649e8530d/src/libawkward/array/ListOffsetArray.cpp#L1540-L1544\r\n\r\nI don't understand why it's there at all. It's good that we're combing through this in the v1 \u2192 v2 transition.",
  "created_at":"2021-08-25T22:19:01Z",
  "id":905914521,
  "issue":1075,
  "node_id":"IC_kwDODBCWws41_yiZ",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-08-25T22:19:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"These tests are an indicator of what was desired: https://github.com/scikit-hep/awkward-1.0/pull/447/files#diff-722c5c30935dd4b66b7d29d94a990b10cf0d1287630f788807c6f7bf09b03234\r\n\r\nI suspect that this hack was only intended to exist for the life of a reduce calculation, and was supposed to disappear thereafter. It's not disappearing in `X`/`ak.from_regular`.\r\n\r\nIt probably should have been a method argument, rather than a class member, to ensure its short life.",
  "created_at":"2021-08-25T22:23:19Z",
  "id":905916418,
  "issue":1075,
  "node_id":"IC_kwDODBCWws41_zAC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-25T22:23:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We've identified the source for this one: it's some subtle hackery in the C++ layer (I think intended for VirtualArrays, which are also going away). This aspect of v1 was not ported over to v2. Furthermore, enough of v2 has been implemented to demonstrate that the effect is gone:\r\n\r\n```python\r\n>>> X = ak._v2.operations.structure.from_regular(ak._v2.operations.convert.from_numpy(np.array([[0,1,2],[3,2,1]])))\r\n>>> X.layout\r\n<ListOffsetArray len='2'>\r\n    <offsets><Index dtype='int64' len='3'>[0 3 6]</Index></offsets>\r\n    <content><NumpyArray dtype='int64' len='6'>[0 1 2 3 2 1]</NumpyArray></content>\r\n</ListOffsetArray>\r\n```\r\n\r\nand\r\n\r\n```python\r\n>>> Y = ak._v2.highlevel.Array([[0,1,2],[3,2,1]])\r\n>>> Y.layout\r\n<ListOffsetArray len='2'>\r\n    <offsets><Index dtype='int64' len='3'>[0 3 6]</Index></offsets>\r\n    <content><NumpyArray dtype='int64' len='6'>[0 1 2 3 2 1]</NumpyArray></content>\r\n</ListOffsetArray>\r\n```\r\n\r\nlook the same and they are the same. When they get reduced, they return identical results.\r\n\r\n```python\r\n>>> ak._v2.operations.reducers.max(X, axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * 1 * ?int64'>\r\n>>> ak._v2.operations.reducers.max(Y, axis=-1, keepdims=True)\r\n<Array [[2], [3]] type='2 * 1 * ?int64'>\r\n```\r\n\r\nAlthough this isn't actually fixed in v1, it's a minor, subtle, hard-to-fix bug that will become irrelevant as soon as v2 is out. Efforts should be directed elsewhere. I'm going to close this issue so that it doesn't look like work to be done.",
  "created_at":"2021-12-07T22:34:31Z",
  "id":988310381,
  "issue":1075,
  "node_id":"IC_kwDODBCWws466Gtt",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-12-07T22:34:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif, I'm telling pytest to skip all _v2 related code in Python 2.7: see 623508f554e250e20a857b9a12dfd212d4c8a03a . You might want to include this one commit in your PR, too, if it passes tests. That way, you don't have to skip individual lines or functions that fail in Python 2.7.\r\n\r\nDiscussion #1010 says we'll be dropping Python 2.7 support by the end of this year, but maybe the more appropriate dividing line would be 1.x vs 2.x.",
  "created_at":"2021-09-01T12:40:09Z",
  "id":910244870,
  "issue":1079,
  "node_id":"IC_kwDODBCWws42QTwG",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2021-09-01T12:40:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Great! I'll add it to the PR.",
  "created_at":"2021-09-01T17:00:28Z",
  "id":910475113,
  "issue":1079,
  "node_id":"IC_kwDODBCWws42RL9p",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T17:00:28Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"Some of these are vague because they're \"any data that can be converted into an Array\" (with `ak.to_layout`). NumPy similarly has \"array-like,\" which also doesn't have a formal definition.\r\n\r\nSaying \"iterable\" is too precise because there can be iterables containing objects that can't be converted into arrays\u2014such as file objects, I guess. Maybe we could adopt the \"array-like\" terminology, though our \"array-like\" is broader than NumPy's.\r\n\r\nIf this is fixed, then the \"array\" or \"arrays\" argument of all of these functions should be changed to the same text (copy-paste).",
  "created_at":"2021-09-02T22:36:42Z",
  "id":912107074,
  "issue":1080,
  "node_id":"IC_kwDODBCWws42XaZC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-02T22:36:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yeah @jpivarski I am done working on this PR. Also the arguments are all placed in the same order ( Alphabetical ).\r\nThanks!!",
  "created_at":"2021-09-08T17:07:27Z",
  "id":915415039,
  "issue":1081,
  "node_id":"IC_kwDODBCWws42kB__",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-08T17:09:00Z",
  "user":"MDQ6VXNlcjUyNjM1Nzcz"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"You're passing in the data in the `counts` argument, which is causing the error. Try switching them around, that should fix it!",
  "created_at":"2021-09-07T10:03:09Z",
  "id":914171327,
  "issue":1083,
  "node_id":"IC_kwDODBCWws42fSW_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-07T10:03:09Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"Yeah - I just realized that when I started modifying the code. I'd just replaced the old `from_counts` with this one and didn't catch the arguments reversed. My bad, sorry for the noise!",
  "created_at":"2021-09-07T10:15:12Z",
  "id":914179829,
  "issue":1083,
  "node_id":"IC_kwDODBCWws42fUb1",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-09-07T10:15:12Z",
  "user":"MDQ6VXNlcjE3NzgzNjY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I don't have a Windows to hand to confirm this, but it looks very plausible \u2014 Windows has a 260 char path limit IIRC, and we check whether source is a path first.",
  "created_at":"2021-09-08T08:44:57Z",
  "id":915040813,
  "issue":1084,
  "node_id":"IC_kwDODBCWws42imot",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-08T08:44:57Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"If the path is too long, then it can't be a real file, so this should have an easy fix: we can catch the ValueError and go into the mode of assuming that the string is JSON.\r\n\r\nFor the bike routes example, `ak.from_iter(py_bikeroutes)` is an alternative to `ak.from_json(js_bikeroutes)`.",
  "created_at":"2021-09-08T12:44:12Z",
  "id":915205987,
  "issue":1084,
  "node_id":"IC_kwDODBCWws42jO9j",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-08T12:44:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This might fix #1084 (but we need to add a test)",
  "created_at":"2021-09-08T12:45:47Z",
  "id":915207066,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42jPOa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-08T12:46:01Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"There's an `ak._util.iswin` to check to see if you're on Windows and we know that the bikeroutes JSON is too long to be a Windows path. Such a test would require an HTTP connection, which should probably skip if the attempt to download with\r\n\r\n```python\r\nimport urllib.request\r\n\r\nurl = \"https://raw.githubusercontent.com/Chicago/osd-bike-routes/master/data/Bikeroutes.geojson\"\r\nbikeroutes_json = urllib.request.urlopen(url).read()\r\n```\r\n\r\nfails.",
  "created_at":"2021-09-08T13:46:12Z",
  "id":915253858,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42japi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-08T13:46:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> There's an `ak._util.iswin` to check to see if you're on Windows and we know that the bikeroutes JSON is too long to be a Windows path. Such a test would require an HTTP connection, which should probably skip if the attempt to download with\r\n> \r\n> ```python\r\n> import urllib.request\r\n> \r\n> url = \"https://raw.githubusercontent.com/Chicago/osd-bike-routes/master/data/Bikeroutes.geojson\"\r\n> bikeroutes_json = urllib.request.urlopen(url).read()\r\n> ```\r\n> \r\n> fails.\r\n\r\nLet's use that instead \u2014 it's a more direct test.",
  "created_at":"2021-09-08T13:51:07Z",
  "id":915257821,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42jbnd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-08T13:51:07Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't merge this because (as I understand it), it's waiting for a test. If I should add it to 1.5.0, let me know. Otherwise, there will probably be only one release candidate before the final release (but technically two \"rc\" releases in GitHub because it looks like the first one didn't trigger).",
  "created_at":"2021-09-09T17:47:39Z",
  "id":916309396,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42ncWU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T17:47:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nope; that's because Azure isn't configured to deploy anymore\u2014it's handled by GitHub Actions now, and I had forgotten.",
  "created_at":"2021-09-09T17:53:11Z",
  "id":916312977,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42ndOR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T17:53:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, I was thinking that this could just be a test that attempts to download through HTTP, and skips if it fails. I don't think it would be the only test that requires network access.\r\n\r\nThat way, we don't need to add a copyright message for Chicago's data. We're really just wanting it for its size (which could also be an auto-generated JSON, if we really want to avoid network access).",
  "created_at":"2021-09-09T18:42:06Z",
  "id":916346011,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42nlSb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T18:42:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I've added a test, but I can't test the previous behaviour against this test unless I poke around with the CI. Do you happen to have a Windows machine to hand?",
  "created_at":"2021-09-09T18:42:22Z",
  "id":916346166,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42nlU2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T18:42:22Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I've added a test, but I can't test the previous behaviour against this test unless I poke around with the CI. Do you happen to have a Windows machine to hand?\r\n\r\n\r\n\r\n> Oh, I was thinking that this could just be a test that attempts to download through HTTP, and skips if it fails. I don't think it would be the only test that requires network access.\r\n> \r\n> That way, we don't need to add a copyright message for Chicago's data. We're really just wanting it for its size (which could also be an auto-generated JSON, if we really want to avoid network access).\r\n\r\nRight, I forgot to motivate why I downloaded a sample; the data are quite large. It's not a *slow* download, but it's enough that I don't think we want to add it to the test suite unless required. Are you not comfortable with including the license file? It's not infectious, and it's only in the test suite.",
  "created_at":"2021-09-09T18:43:29Z",
  "id":916346849,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42nlfh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T18:43:29Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> Do you happen to have a Windows machine to hand?\r\n\r\nThe CI is our only access (both of us).\r\n\r\n> Right, I forgot to motivate why I downloaded a sample; the data are quite large. It's not a slow download, but it's enough that I don't think we want to add it to the test suite unless required.\r\n\r\nI considered it small enough to use in a classroom setting, and the downloads in CI are a lot faster than they are at home. (The CI machines are well-connected!) I just downloaded it in 0.3 seconds at home.\r\n\r\nGenerating simple JSON that is as large as the bikeroutes file would be sufficient. The bikeroutes sample is 2.3 MB, and a list of numbers up to 1 million is 7.8 MB:\r\n\r\n```python\r\njson.dumps(np.arange(1000000).tolist())\r\n```\r\n\r\nGenerating it and interpreting it as an Awkward Array takes 0.2 seconds:\r\n\r\n```python\r\nak.Array(json.dumps(np.arange(1000000).tolist()))\r\n```\r\n\r\nWithout your fix, the above line should fail in Windows (CI).",
  "created_at":"2021-09-09T19:00:33Z",
  "id":916357605,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42noHl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T19:00:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"If we want to test the Windows case, then we only need 260 chars of JSON, although I can see the merit in re-using a \"large\" dataset. I will update with your suggestions.",
  "created_at":"2021-09-09T19:02:53Z",
  "id":916359205,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42nogl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T19:02:53Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh yeah, and although the license is not infectious, simplifying things to the extent that no one has to evaluate whether that is the case is beneficial. I have a bias against including anything that requires a license, which can be overcome by a good enough reason: Lark has a good enough reason, as does pybind11, as well as RapidJSON... I just want to keep the list short, and this one test is doable without having to add the dataset.",
  "created_at":"2021-09-09T19:04:56Z",
  "id":916360629,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42no21",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T19:04:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> 260 chars of JSON\r\n\r\nI didn't know the threshold, but I did know that the bikeroutes exceeded it. That's the thinking behind the 7 MB of JSON.",
  "created_at":"2021-09-09T19:05:56Z",
  "id":916361411,
  "issue":1085,
  "node_id":"IC_kwDODBCWws42npDD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-09T19:05:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Oh, I ignored py2 here because I was thinking about v2. However, it makes sense to merge this now - thanks for backporting it ",
  "created_at":"2021-09-28T09:16:12Z",
  "id":929009272,
  "issue":1085,
  "node_id":"IC_kwDODBCWws43X454",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-28T09:16:12Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hey Jim, what's the motivation for this? Are many relying on these APIs? ",
  "created_at":"2021-09-12T18:54:03Z",
  "id":917690264,
  "issue":1089,
  "node_id":"IC_kwDODBCWws42steY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-12T18:54:03Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"As it turns out, there are. Technically, we shouldn't have changed this spelling without a deprecation cycle because it's not a hidden (underscored) interface, so the fact that Coffea was using it (see IRIS-HEP Slack #coffea) is legitimate.\r\n\r\nSince there haven't been little releases since 1.4.0, this change comes as a \"big bang\" to them. The motivation for the breaking change (\"change spelling to be the way the NumPy team capitalizes for `nplike` singletons but not NumpyArray, NumpyForm, and NumpyType\") was not strong enough to cause that pain: they'd have to update Coffea and put in a version dependence, and then we don't know how many data analyses also access `nplike` directly (I didn't _expect_ them to...).\r\n\r\nIf there's some pain point in the other direction: someone (you?) have code that relies on the \"NumPy\" spelling, then we can expose both spellings in this PR. But since the \"NumPy\" spelling was never in any non-RC release, it seems safe to me to not introduce such duplication.\r\n\r\nI've disabled auto-merge for now, but if there's no problem, I'll reenable it.",
  "created_at":"2021-09-12T19:03:23Z",
  "id":917691782,
  "issue":1089,
  "node_id":"IC_kwDODBCWws42st2G",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-12T19:03:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Not so much a big bang to coffea in particular - names can be fixed. The major problem is that if this change in capitalization were introduced it would break all previous versions of coffea currently in \"production\" (i.e. actively used in analysis). That would be Very Bad. \r\n\r\nIf you'd really like this spelling change I'm completely for doing it, we just have to be mindful of the analysts (including ourselves) when rolling it out.",
  "created_at":"2021-09-12T19:20:16Z",
  "id":917694692,
  "issue":1089,
  "node_id":"IC_kwDODBCWws42sujk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-12T20:28:32Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"The appropriate way to roll it out would be to have both spellings for a while and anything that uses the old spelling should get warnings with a deadline for when it would be removed.\r\n\r\nBut the main thing is that the motivation for changing it wasn't very strong\u2014for something even as painful as a deprecation cycle (better than a sudden breaking change!), someone would have to be asking for it. That wasn't the case: @henryiii added a spell-checker, I thought it was a good idea, and I saw that it made all the \"NumPy\" spellings consistent. Then I decided to extend that consistency to the `nplike` singletons, which I had in my mind as \"internal,\" even though they don't start with an underscore. Nobody asked for this change, and it's even arguable that it shouldn't happen because we never considered changing NumpyArray, NumpyForm, and NumpyType. It is especially true that NumpyArray is part of the public API (for developers of downstream libraries), even though it's not the recommended data analyst user interface.\r\n\r\nSo I'll merge this and make an RC2 release. We still have time before the final release to expose both spellings, if there's any reason for it, but I don't think there is.",
  "created_at":"2021-09-12T20:19:03Z",
  "id":917703628,
  "issue":1089,
  "node_id":"IC_kwDODBCWws42swvM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-12T20:19:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sounds like a good idea!",
  "created_at":"2021-09-13T14:17:07Z",
  "id":918239599,
  "issue":1091,
  "node_id":"IC_kwDODBCWws42uzlv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-13T14:17:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":":heart:",
  "created_at":"2021-09-13T14:25:29Z",
  "id":918247939,
  "issue":1091,
  "node_id":"IC_kwDODBCWws42u1oD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-13T14:25:29Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"NONE",
  "body":"This issue seems to already be handled properly in `to_parquet()`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/665a1870b0b05d1310881af583f4e05a3119bc15/src/awkward/operations/convert.py#L2947-L3067\r\n\r\nIt seems like most of this could be put into `to_arrow_table` and then `to_parquet` could just call `to_arrow_table` to get the table to write?",
  "created_at":"2021-09-14T09:26:34Z",
  "id":918977735,
  "issue":1093,
  "node_id":"IC_kwDODBCWws42xnzH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-14T09:26:34Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"PartitionedArrays are being dropped in v2, in favor of [Daskified Awkward Array](https://github.com/ContinuumIO/dask-awkward/).",
  "created_at":"2021-12-07T21:23:56Z",
  "id":988272722,
  "issue":1093,
  "node_id":"IC_kwDODBCWws4659hS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-07T21:23:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I used \"cherry-pick\" to select the commits from this PR that are relevant beyond the prototype. Since the \"kernel-level thing\" will not be a DAG (https://github.com/ContinuumIO/dask-awkward/discussions/2), we don't want all of these changes, but a few of them are general fix-ups, not specific to the prototype.",
  "created_at":"2021-09-28T18:08:09Z",
  "id":929500899,
  "issue":1095,
  "node_id":"IC_kwDODBCWws43Zw7j",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-28T18:08:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The mechanism for integrating Awkward Array `ak.Array`s with the NumPy API (which is used by `hist2d`) is the `__array_function__` method defined on `ak.Array`. This allows Awkward Array to propose Awkward-aware implementations of the high-level methods called on the array. This is described in detail in [the NEP](https://numpy.org/neps/nep-0018-array-function-protocol.html)\r\n\r\nBecause Awkward doesn't implement an overload for `numpy.histogram2d`, calling it used to fail in *old* versions of Awkward. \r\n\r\nIn the later versions (>=1.5.0), we now have a fallback to try and call the original NumPy function after converting any array arguments into nice rectilinear NumPy arrays: https://github.com/scikit-hep/awkward-1.0/blob/1.5.0/src/awkward/_connect/_numpy.py#L43-L55\r\n\r\nSo, the \"fix\" here is to update to the newest version of Awkward, or to explicitly convert your arguments to NumPy arrays before calling `hist2d` (if you are unable to upgrade).",
  "created_at":"2021-09-15T12:31:39Z",
  "id":919977043,
  "issue":1096,
  "node_id":"IC_kwDODBCWws421bxT",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2021-09-15T12:31:39Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"Great! Thanks a lot for the info and the fix. ",
  "created_at":"2021-09-15T13:02:49Z",
  "id":919998533,
  "issue":1096,
  "node_id":"IC_kwDODBCWws421hBF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-15T13:02:49Z",
  "user":"MDQ6VXNlcjEyODI4OTk1"
 },
 {
  "author_association":"MEMBER",
  "body":"That's right\u2014it should be fixed now (@agoose77 pointed to the update that did this). I did a quick test:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import matplotlib.pyplot as plt\r\n>>> plt.hist2d(ak.Array([1, 2, 3]), ak.Array([4, 5, 6]))\r\n(\r\n    array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n           [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\r\n           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n           [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\r\n    array([1. , 1.2, 1.4, 1.6, 1.8, 2. , 2.2, 2.4, 2.6, 2.8, 3. ]),\r\n    array([4. , 4.2, 4.4, 4.6, 4.8, 5. , 5.2, 5.4, 5.6, 5.8, 6. ]),\r\n    <matplotlib.collections.QuadMesh object at 0x7f85d1155280>\r\n)\r\n```\r\n\r\nFor reference, the arrays _do_ have to be flat, but they don't have to be explicitly converted from \"Awkward Brand\" arrays into \"NumPy Brand.\"",
  "created_at":"2021-09-15T14:22:02Z",
  "id":920065110,
  "issue":1096,
  "node_id":"IC_kwDODBCWws421xRW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-15T14:22:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - please, have a look. I've added a few `FIXME` notes to discuss. Thanks!",
  "created_at":"2021-09-23T12:16:51Z",
  "id":925756103,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43LerH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-23T12:16:51Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"`UnionArray` reducers need needs https://github.com/scikit-hep/awkward-1.0/pull/1082",
  "created_at":"2021-09-23T12:54:58Z",
  "id":925790156,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43Lm_M",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-23T12:54:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - as discussed: coverage for` src/awkward/_v2/_reducers.py` : **96%** \r\nShow keyboard shortcuts\r\n136 statements    131 run  5 missing  0 excluded\r\n\r\nNote, the missing, not covered code in this case is for Windows. Here is the rest:\r\n\r\n**Coverage report: 81%**\r\nModule | statements | missing | excluded | coverage\r\n-- | -- | -- | -- | --\r\nTotal | 19965 | 3844 | 0 | 81%\r\nsrc/awkward/_connect/_autograd.py | 39 | 30 | 0 | 23%\r\nsrc/awkward/_typeparser/generated_parser.py | 1597 | 829 | 0 | 48%\r\nsrc/awkward/_connect/_jax/__init__.py | 18 | 9 | 0 | 50%\r\nsrc/awkward/_connect/_numpy.py | 277 | 131 | 0 | 53%\r\nsrc/awkward/_v2/identifier.py | 78 | 36 | 0 | 54%\r\nsrc/awkward/_v2/contents/bitmaskedarray.py | 147 | 57 | 0 | 61%\r\nsrc/awkward/_v2/contents/virtualarray.py | 290 | 109 | 0 | 62%\r\nsrc/awkward/_v2/forms/virtualform.py | 80 | 29 | 0 | 64%\r\nsrc/awkward/_v2/forms/bytemaskedform.py | 66 | 22 | 0 | 67%\r\nsrc/awkward/_v2/forms/unionform.py | 116 | 38 | 0 | 67%\r\nsrc/awkward/_v2/forms/bitmaskedform.py | 73 | 23 | 0 | 68%\r\nsrc/awkward/operations/describe.py | 73 | 23 | 0 | 68%\r\nsrc/awkward/_v2/contents/unmaskedarray.py | 104 | 32 | 0 | 69%\r\nsrc/awkward/_v2/forms/regularform.py | 68 | 21 | 0 | 69%\r\nsrc/awkward/nplike.py | 332 | 104 | 0 | 69%\r\nsrc/awkward/_v2/contents/listoffsetarray.py | 332 | 99 | 0 | 70%\r\nsrc/awkward/_v2/forms/emptyform.py | 46 | 14 | 0 | 70%\r\nsrc/awkward/_v2/forms/indexedoptionform.py | 60 | 18 | 0 | 70%\r\nsrc/awkward/_v2/forms/recordform.py | 164 | 50 | 0 | 70%\r\nsrc/awkward/_v2/forms/unmaskedform.py | 53 | 16 | 0 | 70%\r\nsrc/awkward/_v2/forms/indexedform.py | 60 | 17 | 0 | 72%\r\nsrc/awkward/_v2/contents/emptyarray.py | 88 | 24 | 0 | 73%\r\nsrc/awkward/_v2/contents/indexedarray.py | 160 | 41 | 0 | 74%\r\nsrc/awkward/highlevel.py | 622 | 161 | 0 | 74%\r\nsrc/awkward/_v2/contents/indexedoptionarray.py | 231 | 57 | 0 | 75%\r\nsrc/awkward/_v2/contents/regulararray.py | 290 | 70 | 0 | 76%\r\nsrc/awkward/_v2/forms/listform.py | 74 | 18 | 0 | 76%\r\nsrc/awkward/_typeparser/parser.py | 225 | 52 | 0 | 77%\r\nsrc/awkward/_v2/contents/recordarray.py | 220 | 50 | 0 | 77%\r\nsrc/awkward/_v2/record.py | 84 | 19 | 0 | 77%\r\nsrc/awkward/operations/convert.py | 2094 | 474 | 0 | 77%\r\nsrc/awkward/_v2/contents/bytemaskedarray.py | 208 | 46 | 0 | 78%\r\nsrc/awkward/_v2/contents/content.py | 408 | 84 | 0 | 79%\r\nsrc/awkward/_v2/types/uniontype.py | 38 | 8 | 0 | 79%\r\nsrc/awkward/operations/reducers.py | 287 | 61 | 0 | 79%\r\nsrc/awkward/_v2/contents/unionarray.py | 191 | 36 | 0 | 81%\r\nsrc/awkward/behaviors/string.py | 149 | 28 | 0 | 81%\r\nsrc/awkward/_v2/contents/listarray.py | 250 | 44 | 0 | 82%\r\nsrc/awkward/_v2/tmp_for_testing.py | 232 | 41 | 0 | 82%\r\nsrc/awkward/partition.py | 441 | 79 | 0 | 82%\r\nsrc/awkward/forth.py | 6 | 1 | 0 | 83%\r\nsrc/awkward/_v2/forms/listoffsetform.py | 67 | 11 | 0 | 84%\r\nsrc/awkward/_v2/index.py | 106 | 17 | 0 | 84%\r\nsrc/awkward/_cpu_kernels.py | 13 | 2 | 0 | 85%\r\nsrc/awkward/_v2/forms/form.py | 131 | 20 | 0 | 85%\r\nsrc/awkward/_v2/types/recordtype.py | 65 | 10 | 0 | 85%\r\nsrc/awkward/operations/structure.py | 1161 | 172 | 0 | 85%\r\nsrc/awkward/_util.py | 1030 | 142 | 0 | 86%\r\nsrc/awkward/_v2/_slicing.py | 198 | 28 | 0 | 86%\r\nsrc/awkward/_v2/types/arraytype.py | 22 | 3 | 0 | 86%\r\nsrc/awkward/_v2/types/regulartype.py | 38 | 5 | 0 | 87%\r\nsrc/awkward/_connect/_numba/layout.py | 1458 | 179 | 0 | 88%\r\nsrc/awkward/_v2/forms/numpyform.py | 96 | 12 | 0 | 88%\r\nsrc/awkward/_v2/types/listtype.py | 32 | 4 | 0 | 88%\r\nsrc/awkward/_v2/types/optiontype.py | 32 | 4 | 0 | 88%\r\nsrc/awkward/_v2/contents/numpyarray.py | 221 | 25 | 0 | 89%\r\nsrc/awkward/behaviors/categorical.py | 145 | 16 | 0 | 89%\r\nsrc/awkward/_connect/_numba/__init__.py | 73 | 7 | 0 | 90%\r\nsrc/awkward/_connect/_numba/builder.py | 351 | 36 | 0 | 90%\r\nsrc/awkward/_connect/_numexpr.py | 75 | 7 | 0 | 91%\r\nsrc/awkward/_v2/types/unknowntype.py | 22 | 2 | 0 | 91%\r\nsrc/awkward/_v2/types/numpytype.py | 56 | 4 | 0 | 93%\r\nsrc/awkward/types.py | 14 | 1 | 0 | 93%\r\nsrc/awkward/__init__.py | 35 | 2 | 0 | 94%\r\nsrc/awkward/forms.py | 18 | 1 | 0 | 94%\r\nsrc/awkward/_v2/types/type.py | 40 | 2 | 0 | 95%\r\nsrc/awkward/behaviors/mixins.py | 44 | 2 | 0 | 95%\r\nsrc/awkward/_v2/_reducers.py | 136 | 5 | 0 | 96%\r\nsrc/awkward/_connect/_numba/arrayview.py | 894 | 21 | 0 | 98%\r\nsrc/awkward/_libawkward.py | 83 | 2 | 0 | 98%\r\nsrc/awkward/layout.py | 44 | 1 | 0 | 98%\r\nsrc/awkward/_connect/__init__.py | 1 | 0 | 0 | 100%\r\nsrc/awkward/_kernel_signatures.py | 2840 | 0 | 0 | 100%\r\nsrc/awkward/_typeparser/__init__.py | 1 | 0 | 0 | 100%\r\nsrc/awkward/_v2/__init__.py | 8 | 0 | 0 | 100%\r\nsrc/awkward/_v2/contents/__init__.py | 17 | 0 | 0 | 100%\r\nsrc/awkward/_v2/forms/__init__.py | 15 | 0 | 0 | 100%\r\nsrc/awkward/_v2/types/__init__.py | 10 | 0 | 0 | 100%\r\nsrc/awkward/behaviors/__init__.py | 1 | 0 | 0 | 100%\r\nsrc/awkward/operations/__init__.py | 1 | 0 | 0 | 100%\r\n\r\n\r\n```\r\nName                                                                                         Stmts   Miss  Cover   Missing\r\n--------------------------------------------------------------------------------------------------------------------------\r\nsrc/awkward/__init__.py                                                                         35      2    94%   15, 72\r\nsrc/awkward/_connect/__init__.py                                                                 1      0   100%\r\nsrc/awkward/_connect/_autograd.py                                                               39     30    23%   15-49, 56-85\r\nsrc/awkward/_connect/_jax/__init__.py                                                           18      9    50%   15-36, 40\r\nsrc/awkward/_connect/_numba/__init__.py                                                         73      7    90%   17-18, 31, 90, 92, 98, 107\r\nsrc/awkward/_connect/_numba/arrayview.py                                                       894     21    98%   20-21, 75, 192, 224, 265, 273, 337, 512, 519, 589, 680-681, 879, 1000, 105\r\n7-1058, 1513, 1568, 1744-1749\r\nsrc/awkward/_connect/_numba/builder.py                                                         351     36    90%   152, 159-162, 176, 190, 204, 214, 224, 238, 252, 262, 271-278, 292, 302, 3\r\n48-367, 381, 434, 438, 524-534, 672, 685-693\r\nsrc/awkward/_connect/_numba/layout.py                                                         1458    179    88%   44, 123, 137, 238, 243, 256-258, 266-268, 275, 286, 288, 292, 300, 312, 31\r\n6, 337, 475, 523, 539, 547, 551, 553, 555, 557, 559, 561, 563, 567, 604, 711, 751, 755, 759, 763, 782-783, 868-869, 895, 899, 983, 1059, 1061, 1065, 1080, 1144, 1148, 1152, 1156, 1232, 1236\r\n, 1251, 1336, 1432, 1510, 1514, 1518, 1522, 1620, 1716, 1720, 1724, 1728, 1793, 1852, 1856, 1860, 1864, 1970, 1989, 2020-2028, 2044-2052, 2066, 2252, 2256, 2360, 2362, 2366-2371, 2390, 2393\r\n-2394, 2400-2401, 2407-2408, 2426, 2444, 2451, 2459-2465, 2469-2470, 2474, 2478-2483, 2497, 2520, 2536, 2546, 2588, 2591-2608, 2644, 2653, 2784, 2788, 2792, 2796, 2800-2840, 2847-2882, 2889\r\n-2926\r\nsrc/awkward/_connect/_numexpr.py                                                                75      7    91%   19-20, 33, 60-61, 125-126\r\nsrc/awkward/_connect/_numpy.py                                                                 277    131    53%   17, 40, 68, 76, 109, 231-233, 246-249, 270-342, 393-478\r\nsrc/awkward/_cpu_kernels.py                                                                     13      2    85%   12, 16\r\nsrc/awkward/_kernel_signatures.py                                                             2840      0   100%\r\nsrc/awkward/_libawkward.py                                                                      83      2    98%   10, 14\r\nsrc/awkward/_typeparser/__init__.py                                                              1      0   100%\r\nsrc/awkward/_typeparser/generated_parser.py                                                   1597    829    48%   51-54, 63-73, 77-114, 119-139, 145-166, 172-176, 221-222, 225-234, 250-251\r\n, 264-265, 268, 271, 299-314, 319-320, 327-342, 350, 362-364, 367, 370, 373-383, 387, 390-393, 396, 399, 403-411, 415, 419, 438-457, 460, 474-489, 492-502, 505-514, 517-518, 522, 526, 530, \r\n534, 542-548, 553, 556-558, 561, 568, 571-574, 583-608, 614-615, 623, 627, 630, 638-640, 644-646, 654-659, 663-669, 674-678, 685-690, 693, 697, 700, 707-712, 716-726, 731, 735-750, 754, 756\r\n, 758, 761, 766-787, 793-795, 798-815, 818-835, 838-840, 846, 856, 859-860, 863, 869, 880-881, 885, 898-902, 905, 921-926, 932, 935, 943, 953-954, 957, 962, 965, 968, 975, 979-982, 1010, 10\r\n15, 1027-1030, 1033, 1043-1045, 1058, 1069, 1072, 1075, 1079, 1101-1102, 1110, 1113-1118, 1123-1125, 1128-1129, 1145-1147, 1149, 1167, 1170-1172, 1208-1217, 1237-1242, 1259-1261, 1268-1271,\r\n 1283-1285, 1290-1291, 1308, 1352-1358, 1378-1386, 1401, 1406, 1409-1447, 1452-1454, 1457-1470, 1477-1493, 1524-1527, 1543, 1553-1555, 1558-1582, 1586-1589, 1596-1597, 1600-1629, 1633-1636,\r\n 1640-1646, 1650-1656, 1696, 1699, 1701, 1709, 1719-1725, 1735, 1769, 1778, 1791-1793, 1814, 1847-1862, 1869, 1871, 1884-1893, 1915-1927, 1934, 1936, 1939-1976, 1984, 1994, 1999-2000, 2003,\r\n 2006-2007, 2020-2022, 2042, 2047, 2054, 2058-2062, 2065, 2069, 2155, 2207, 2212, 2219, 2223, 2228-2229, 2232-2233, 2236, 2256-2407, 2410, 2415, 2433-2435, 2439-2440, 2445-2446, 2452, 2460,\r\n 2486-2490, 2495-2500, 2503, 2508-2513, 2517, 2524-2551, 2555-2556, 2560, 2564-2565, 2569\r\nsrc/awkward/_typeparser/parser.py                                                              225     52    77%   16, 31, 34, 37, 62-66, 68-72, 76-77, 86, 99-100, 106, 114-115, 120-121, 12\r\n4, 130-131, 138-139, 148-149, 157-158, 168-169, 176-177, 186-187, 196-197, 205-206, 216-217, 225-226, 243-244, 273-274, 287\r\nsrc/awkward/_util.py                                                                          1030    142    86%   15-17, 30, 59, 69, 102, 174-175, 178-179, 182-186, 216-217, 220, 253, 286-\r\n288, 291-293, 306-308, 311-313, 420, 447, 482-487, 518, 520, 527-545, 553-557, 560, 563, 601, 624, 694, 726, 784, 943-949, 1014, 1018, 1026, 1030, 1038, 1060, 1070, 1119-1129, 1187, 1229-12\r\n90, 1315, 1327, 1336, 1362, 1386, 1402, 1460, 1469, 1490, 1526, 1583, 1593, 1632, 1651, 1660, 1737, 1739, 1760, 1766, 1786, 1788, 1792, 1801, 1803, 1805, 1874-1887\r\nsrc/awkward/_v2/__init__.py                                                                      8      0   100%\r\nsrc/awkward/_v2/_reducers.py                                                                   136      5    96%   22, 32, 164-180\r\nsrc/awkward/_v2/_slicing.py                                                                    198     28    86%   7-8, 64-65, 68-69, 74, 100, 103, 106, 126, 138-139, 150, 163, 218-219, 276-281, 318-319, 327, 380-381, 389, 423, 455\r\nsrc/awkward/_v2/contents/__init__.py                                                            17      0   100%\r\nsrc/awkward/_v2/contents/bitmaskedarray.py                                                     147     57    61%   29, 35, 41, 47, 53, 59, 65, 100, 120, 123-136, 163-178, 186-203, 206, 237, 252, 257-282, 317, 330, 332, 343\r\nsrc/awkward/_v2/contents/bytemaskedarray.py                                                    208     46    78%   19, 25, 31, 37, 66, 85, 88-99, 102-117, 123, 125, 130, 176-180, 225, 301-308, 366, 438-453, 472, 486, 489, 521, 529, 540\r\nsrc/awkward/_v2/contents/content.py                                                            408     84    79%   7-8, 23, 29, 64, 79, 88, 145, 230, 238, 245, 254, 266, 277, 296, 299, 344, 373, 391, 429-451, 457, 463, 469, 475, 489-493, 510, 517, 523, 535, 620, 625-632, 667, 671, 718, 724, 729-760, 763-794, 797, 802, 807-820, 827, 832\r\nsrc/awkward/_v2/contents/emptyarray.py                                                          88     24    73%   19, 22-30, 59, 84, 97, 102-118, 121\r\nsrc/awkward/_v2/contents/indexedarray.py                                                       160     41    74%   25, 30, 54, 72, 75-84, 87, 92, 132-136, 148, 217, 223, 260, 275-279, 336, 340, 376, 383-410\r\nsrc/awkward/_v2/contents/indexedoptionarray.py                                                 231     57    75%   24, 29, 53, 71, 74-83, 86-89, 97-100, 103, 146-150, 196, 263-270, 273-298, 309-315, 321, 421, 449-459, 533-547, 565, 582, 613, 620-627, 641\r\nsrc/awkward/_v2/contents/listarray.py                                                          250     44    82%   22, 27, 32, 67, 86, 89-99, 106, 155, 189, 285, 366-376, 562, 593-630, 638, 713, 723-730, 738\r\nsrc/awkward/_v2/contents/listoffsetarray.py                                                    332     99    70%   21, 26, 32, 82, 107, 112-113, 116, 130, 160-164, 189, 196, 203, 238-461, 497, 506, 541, 543, 683, 987, 998, 1008-1015, 1023\r\nsrc/awkward/_v2/contents/numpyarray.py                                                         221     25    89%   22, 32, 70, 97-98, 169-170, 195, 212-213, 234, 264-270, 277, 286, 303, 377, 466, 469, 472\r\nsrc/awkward/_v2/contents/recordarray.py                                                        220     50    77%   9-10, 24, 30, 33, 41, 48, 54, 62, 64, 70, 76, 94, 126-131, 161-164, 192, 208, 231-235, 247-253, 278-281, 317, 320, 323, 352, 357, 365, 384-395, 436, 442\r\nsrc/awkward/_v2/contents/regulararray.py                                                       290     70    76%   16, 22, 28, 56, 74, 77-87, 94, 145-146, 150-153, 156, 197, 204, 211, 253, 410, 413-416, 446, 453, 490, 528, 557, 563, 572, 574, 626-631, 658-702, 707-715\r\nsrc/awkward/_v2/contents/unionarray.py                                                         191     36    81%   7-8, 22, 32, 38, 44, 48, 55, 87, 127, 172-176, 256, 299-303, 313, 322-329, 340, 355-361, 375, 379, 396-403, 409\r\nsrc/awkward/_v2/contents/unmaskedarray.py                                                      104     32    69%   15, 34, 51, 54-62, 65, 74-75, 83-86, 89, 147-154, 182-188, 204, 223-227, 249\r\nsrc/awkward/_v2/contents/virtualarray.py                                                       290    109    62%   11-12, 25, 31, 48, 56, 62, 68-85, 98, 104, 110, 125, 129, 133, 136, 139-149, 159, 169, 173, 176, 179-185, 196-206, 227, 233, 239, 270, 282, 297, 302, 305-335, 356, 366, 370, 391, 409, 414, 432, 445, 448-452, 455-459, 474, 486\r\nsrc/awkward/_v2/forms/__init__.py                                                               15      0   100%\r\nsrc/awkward/_v2/forms/bitmaskedform.py                                                          73     23    68%   22, 28, 34, 40, 54, 58, 62, 66, 101, 104-119, 122, 132, 143, 154, 165, 169, 181, 185\r\nsrc/awkward/_v2/forms/bytemaskedform.py                                                         66     22    67%   20, 26, 32, 45, 49, 53, 75-85, 88-102, 105, 115, 125, 135, 146, 162, 166\r\nsrc/awkward/_v2/forms/emptyform.py                                                              46     14    70%   28-37, 40, 47, 50, 53, 61, 65, 77, 81\r\nsrc/awkward/_v2/forms/form.py                                                                  131     20    85%   166, 179-183, 185-189, 194, 201-202, 216, 222, 228, 249-252, 289, 292\r\nsrc/awkward/_v2/forms/indexedform.py                                                            60     17    72%   19, 25, 37, 41, 67, 70-83, 86, 95, 104, 113, 123, 139\r\nsrc/awkward/_v2/forms/indexedoptionform.py                                                      60     18    70%   19, 25, 37, 41, 67, 70-83, 86, 95, 104, 113, 123, 139, 143\r\nsrc/awkward/_v2/forms/listform.py                                                               74     18    76%   20, 26, 32, 45, 49, 53, 85, 91, 102, 115, 125, 135, 146, 151, 158, 166, 173, 177\r\nsrc/awkward/_v2/forms/listoffsetform.py                                                         67     11    84%   15, 27, 31, 60, 66, 76, 97, 117, 122, 129, 148\r\nsrc/awkward/_v2/forms/numpyform.py                                                              96     12    88%   7-8, 62, 72, 131, 137, 148, 160, 163, 166, 176, 197\r\nsrc/awkward/_v2/forms/recordform.py                                                            164     50    70%   7-8, 25, 32, 38, 51, 61, 70, 74, 84-85, 104-109, 119, 151, 156, 160, 164-166, 172, 186-191, 194, 207-211, 214-233, 242-251, 261, 270, 281, 291, 296\r\nsrc/awkward/_v2/forms/regularform.py                                                            68     21    69%   14, 20, 32, 36, 62, 65-78, 81, 90, 99, 108, 118, 123, 130, 138, 145, 149\r\nsrc/awkward/_v2/forms/unionform.py                                                             116     38    67%   7-8, 25, 31, 37, 44, 57, 61, 65, 68, 104, 108-110, 113-128, 131, 141, 151, 161, 172-176, 185, 192, 203, 211, 213, 225-228\r\nsrc/awkward/_v2/forms/unmaskedform.py                                                           53     16    70%   17, 28, 52, 55-66, 69, 77, 85, 93, 102, 118, 122\r\nsrc/awkward/_v2/forms/virtualform.py                                                            80     29    64%   18, 24, 40, 64, 68, 73, 79, 84, 93, 104, 115, 125-130, 134-139, 143-148, 152-157, 161-166, 170-175\r\nsrc/awkward/_v2/identifier.py                                                                   78     36    54%   15-17, 25, 30, 32, 40, 44, 48, 52, 56, 60, 64, 70, 73, 76, 79, 82, 85-104, 107, 110\r\nsrc/awkward/_v2/index.py                                                                       106     17    84%   27, 41, 48, 93, 105-119, 122, 142, 145, 148\r\nsrc/awkward/_v2/record.py                                                                       84     19    77%   7-8, 19, 23, 30, 50, 54, 81, 84, 90, 93, 96, 102, 105, 108, 111, 116-120\r\nsrc/awkward/_v2/tmp_for_testing.py                                                             232     41    82%   16, 23, 26, 29, 174-180, 187, 340-386, 393, 468-469, 482-483, 485-486, 529-530, 544, 553, 564, 580, 586, 595\r\nsrc/awkward/_v2/types/__init__.py                                                               10      0   100%\r\nsrc/awkward/_v2/types/arraytype.py                                                              22      3    86%   12, 28, 32\r\nsrc/awkward/_v2/types/listtype.py                                                               32      4    88%   12, 18, 24, 35\r\nsrc/awkward/_v2/types/numpytype.py                                                              56      4    93%   46, 54, 60, 71\r\nsrc/awkward/_v2/types/optiontype.py                                                             32      4    88%   14, 20, 26, 37\r\nsrc/awkward/_v2/types/recordtype.py                                                             65     10    85%   7-8, 19, 25, 28, 34, 40, 46, 58, 62\r\nsrc/awkward/_v2/types/regulartype.py                                                            38      5    87%   12, 24, 30, 42, 46\r\nsrc/awkward/_v2/types/type.py                                                                   40      2    95%   15, 25\r\nsrc/awkward/_v2/types/uniontype.py                                                              38      8    79%   7-8, 17, 23, 26, 32, 38, 49\r\nsrc/awkward/_v2/types/unknowntype.py                                                            22      2    91%   12, 18\r\nsrc/awkward/behaviors/__init__.py                                                                1      0   100%\r\nsrc/awkward/behaviors/categorical.py                                                           145     16    89%   19-21, 24, 27, 36-37, 40, 43, 48, 50, 52, 165, 169, 341, 350\r\nsrc/awkward/behaviors/mixins.py                                                                 44      2    95%   86, 110\r\nsrc/awkward/behaviors/string.py                                                                149     28    81%   18, 27-28, 31-34, 37, 40-43, 46-49, 60, 69-70, 73-76, 79, 82-85, 88-91, 165, 272\r\nsrc/awkward/forms.py                                                                            18      1    94%   40\r\nsrc/awkward/forth.py                                                                             6      1    83%   12\r\nsrc/awkward/highlevel.py                                                                       622    161    74%   11-13, 28, 211, 223, 247, 259, 272, 334, 363, 369-383, 395, 398, 401, 404-426, 520, 523, 998, 1063, 1072, 1116, 1121-1122, 1192, 1199, 1206, 1213, 1220, 1227, 1234, 1419, 1465, 1472-1473, 1494-1497, 1553, 1559, 1562, 1567-1574, 1577, 1590, 1596, 1598, 1648, 1677, 1683, 1697, 1717, 1729, 1732, 1735-1736, 1746, 1777, 1779, 1801-1812, 1848-1849, 1865, 1885, 1895, 1905, 1915, 1925, 1935, 1945, 1955, 1965, 1975, 2001, 2009-2025, 2066, 2073-2074, 2090, 2096, 2102-2105, 2281, 2296, 2302, 2314-2321, 2329-2330, 2358, 2366-2377, 2387, 2397, 2406, 2426-2429, 2685, 2701, 2711-2727, 2766-2767, 2770, 2773, 2796\r\nsrc/awkward/layout.py                                                                           44      1    98%   106\r\nsrc/awkward/nplike.py                                                                          332    104    69%   21, 28-31, 43-46, 144, 203, 221, 246, 266, 270, 310, 355, 364, 376, 412, 415, 418-430, 434, 441, 448, 451-466, 469-484, 487, 490-491, 495-498, 502-510, 514-530, 535-539, 542-546, 549-553, 556-560, 563-567, 570-574, 577-581, 584-588, 591-595, 601\r\nsrc/awkward/operations/__init__.py                                                               1      0   100%\r\nsrc/awkward/operations/convert.py                                                             2094    474    77%   18-20, 33, 36-39, 95-108, 131-141, 152-153, 160-163, 195, 198, 201, 207-208, 211, 214, 241, 246, 261-264, 271-272, 284-288, 303-305, 320, 336, 342, 370, 374-384, 415-432, 449-552, 583-600, 617-726, 775, 778, 784-787, 892, 930, 1003, 1009, 1103, 1127, 1192, 1195, 1198, 1201, 1207, 1210, 1216, 1225, 1310-1313, 1315-1318, 1320, 1322, 1364, 1394-1434, 1452-1462, 1473-1482, 1487, 1501, 1508-1509, 1519-1539, 1543-1559, 1563-1568, 1574-1575, 1580, 1589, 1608, 1619, 1625, 1635, 1640, 1674, 1680, 1685, 1694, 1702, 1718, 1724, 1751, 1766, 1774, 1790-1795, 1802-1803, 1808, 1814, 1824-1849, 1891, 1894, 1904, 1913, 1956, 1971-1972, 1988, 2165-2187, 2190-2203, 2360-2366, 2376, 2409, 2430-2436, 2500, 2508, 2722, 2733, 2780-2790, 2808-2818, 2858, 2890, 2914, 2918-2919, 2939, 3080, 3117, 3127-3128, 3209-3214, 3233, 3241, 3249, 3264, 3297, 3323-3332, 3402-3406, 3413, 3417, 3443, 3461, 3464, 3484, 3528, 3546, 3564, 3585, 3661, 3713, 3742, 3820, 3824-3829, 3893, 3930-3932, 3946, 4141, 4143, 4149, 4187, 4372-4391, 4410, 4439-4440, 4491, 4520, 4522, 4547, 4549, 4572, 4589, 4591, 4621, 4623, 4659, 4661, 4668, 4677, 4680, 4705, 4707, 4743, 4758, 4792, 4794, 4811, 4842, 4844, 4851, 4860, 4863, 4931, 5047, 5073, 5081, 5129, 5253-5254, 5275, 5281, 5289, 5329, 5357, 5366, 5418\r\nsrc/awkward/operations/describe.py                                                              73     23    68%   52, 64, 68-72, 125, 128, 131, 134, 155, 161, 170-182, 191, 279\r\nsrc/awkward/operations/reducers.py                                                             287     61    79%   88, 141, 338, 387, 438, 489, 543, 547-548, 604, 608-609, 674, 732-742, 802-812, 884-890, 966-972, 1040-1046, 1170-1176, 1245-1257, 1333-1351, 1420, 1422, 1424, 1426, 1429-1448, 1512\r\nsrc/awkward/operations/structure.py                                                           1161    172    85%   10-11, 157, 339, 371, 385, 400, 417, 433, 573, 606-608, 619, 679, 725, 886, 919, 950, 952, 1004, 1182, 1364, 1389, 1430, 1456, 1475-1482, 1504-1507, 1680, 1697, 1700, 1724, 1744, 1845, 1853, 1856, 1859, 1867, 1896, 1982-1986, 1990, 1994, 2004, 2020, 2060, 2082, 2097, 2150, 2170, 2232, 2269, 2293, 2310, 2331, 2685, 2698-2707, 2719, 2772, 2824, 2985, 2994, 2999, 3230, 3242, 3245-3249, 3258, 3263, 3274, 3276, 3287, 3297, 3329-3356, 3412, 3414, 3433, 3539, 3566-3570, 3733, 3790, 3792, 3795, 3824-3830, 3905, 3910, 3929, 3936, 4113, 4118, 4210, 4220-4221, 4229, 4240, 4268, 4276, 4278, 4280, 4282-4293, 4295, 4306-4317, 4324, 4330, 4351, 4420, 4448-4477, 4554, 4651\r\nsrc/awkward/partition.py                                                                       441     79    82%   9-10, 19-22, 33-36, 42-55, 75-82, 110, 131-136, 140, 158-159, 163, 175, 179, 185, 188, 195, 214, 244, 270, 295, 314, 388, 390, 413, 420, 423, 462, 465-474, 510, 518, 546, 561-563, 599-608, 617, 630, 683-684, 696, 702\r\nsrc/awkward/types.py  \r\n\r\n\r\n```",
  "created_at":"2021-09-28T16:19:25Z",
  "id":929380375,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43ZTgX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-28T16:19:25Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"No complaints about the 96%! (I don't think 100% is a goal because of things like the Windows cases. Twisting them to count as covered, for instance with one-line `X if P else Y` is an abuse in that it might make the code less readable.)\r\n\r\nI'm curious about the coverage for the whole `src/awkward/_v2` subdirectory, though it's not a part of this PR.",
  "created_at":"2021-09-28T16:36:58Z",
  "id":929393841,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43ZWyx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-28T16:36:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - apologies, I did not mean to delete your comment I just added a tick mark...",
  "created_at":"2021-09-29T14:36:13Z",
  "id":930238488,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43clAY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-29T14:36:13Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> The checklist of things remaining:\r\n> \r\n>  toRegularArray, which enables some implementations. This is not a huge project and would fit into the scope of this PR.\r\n>  UnionArray: not a part of this PR, as it requires merge.\r\n>  axis=None: not a part of this PR, as it requires ak._util.completely_flatten to recognize v2 classes.\r\n> On the last of these, instead of modifying ak._util to know about v2, I'm thinking of duplicating the high-level layer into ak._v2 with appropriate translations to make it v2-only. That would give us a chance to review algorithms that unnecessarily do if-elif-else over node types, rather than calling a method on the nodes. ak._util.completely_flatten, for example, could be a node method. That would make it easier to cut off v1 and go v2-only when the time comes. The disadvantage is that in the interim, any bug-fixes to v1 code would have to be also fixed in v2. We've seen one instance of that so far, and I think it's not too bad. The advantage of being able to cleanly cut over to v2 when it's ready may outweigh it. Before doing that, I'll have to have a clear schedule, so that we know how much time we're having to apply bug-fixes in both branches.",
  "created_at":"2021-09-29T14:37:21Z",
  "id":930239449,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43clPZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-29T14:37:21Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - pre-commit complains about B904: Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nI guess, it would be fixed by rebasing the branch? Thanks.",
  "created_at":"2021-09-29T14:48:39Z",
  "id":930249584,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43cntw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-29T14:48:39Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - pre-commit complains about B904: Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.\r\n\r\nThis seems to be a new rule, and it pops up in many places. We probably need to just ignore it globally (`setup.cfg`) for now, until we find out how to deal with it. The fix it's proposing uses some special syntax (the `from` keyword), which might not exist in Python 2. Although we will be dropping Python 2 support and the v2 parts are excluding Python 2 from the outset, we can't cross the boundary of having non-Python 2 _syntax_, because Python's initial .py \u2192 .pyc scan will fail.",
  "created_at":"2021-09-29T16:15:55Z",
  "id":930327050,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43c6oK",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-09-29T16:15:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, I've addressed all issues. Please, have a look. Thanks",
  "created_at":"2021-09-30T09:00:47Z",
  "id":931088827,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43f0m7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-30T09:00:47Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I agree. Looks great, thanks!",
  "created_at":"2021-09-30T12:22:39Z",
  "id":931272187,
  "issue":1099,
  "node_id":"IC_kwDODBCWws43ghX7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-30T12:22:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This was something I thought about recently too. I started hacking away at it, but ran out of time. Thanks for ear-marking this with an issue :)",
  "created_at":"2021-09-21T14:49:30Z",
  "id":924063800,
  "issue":1103,
  "node_id":"IC_kwDODBCWws43FBg4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-21T14:49:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"The XML blocks are for the layouts only, which are public API, but somewhat of a \"debugging view\" for developers. (Awkward v2 can't be wrapped with the high-level `ak.Array` yet, so right now, it's the only view for v2.) The choice of XML was because it's human readable, has a natural indentation, and kinda looks like standard Python reprs that are enclosed in angle brackets (`<`, `>`). The motivations weren't any deeper than that, and there is no program that I know of that expects them to be strict XML.\r\n\r\nHTML views would be cool, but that strikes me as a high-level thing, for `ak.Array`. (Though I suppose both high- and low-level views could get an HTML treatment.) I wonder what form it should take: nested boxes? a tree?\r\n\r\nIt would be important for the default rendering to not be large, even if the type contains records with many fields. These things will pop up in output, unasked for, and it can be annoying when such things make the window scroll (like Matplotlib's histograms when you forget to add a semicolon).\r\n\r\nMaybe if the representation starts small and only gets big if you click on parts to unpack them? (Like a GitHub diff, which provides more context if you click on \"...\".) Maybe type information could be integrated into values?",
  "created_at":"2021-09-21T15:24:49Z",
  "id":924097268,
  "issue":1103,
  "node_id":"IC_kwDODBCWws43FJr0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-21T15:24:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Oh certainly for both! I was *mainly* talking about the contents/layout XML, but it would make a lot of sense to include that in the higher-level objects too, as expandable parts. Note that the text repr, when not in the notebook, will remain unchanged.",
  "created_at":"2021-09-21T15:37:39Z",
  "id":924108810,
  "issue":1103,
  "node_id":"IC_kwDODBCWws43FMgK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-21T15:37:39Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"In fact, `np.unique` takes an `axis` parameter, so this would be entirely natural. `ak.unique` is [scheduled for translation to v2](https://github.com/scikit-hep/awkward-1.0/projects/4#card-68315087), so maybe a natural time for this feature to be supported is in Awkward 2.0.",
  "created_at":"2021-09-22T17:22:10Z",
  "id":925129069,
  "issue":1104,
  "node_id":"IC_kwDODBCWws43JFlt",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2021-09-22T17:22:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@yimuchen - NumPy unique returns the sorted unique elements of an array. There are three optional outputs in addition to the unique elements:\r\n\r\n- the indices of the input array that give the unique values\r\n- the indices of the unique array that reconstruct the input array\r\n- the number of times each unique value comes up in the input array\r\n\r\nDo you need any of these optional outputs or just the sorted array? Thanks!",
  "created_at":"2021-09-23T13:26:11Z",
  "id":925815297,
  "issue":1104,
  "node_id":"IC_kwDODBCWws43LtIB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-23T13:26:11Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"For our use-case, we only need the sorted array, though I would suppose it would be nice to have parity between numpy and awkward methods.",
  "created_at":"2021-09-23T17:48:19Z",
  "id":926026730,
  "issue":1104,
  "node_id":"IC_kwDODBCWws43Mgvq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-23T17:48:19Z",
  "user":"MDQ6VXNlcjExNzAzNjQ0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"In the mean-time, you can do this using Numba e.g. https://github.com/scikit-hep/awkward-1.0/discussions/902#discussioncomment-844323\r\n\r\n:)",
  "created_at":"2021-09-24T13:17:33Z",
  "id":926617455,
  "issue":1104,
  "node_id":"IC_kwDODBCWws43Ow9v",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-24T13:17:33Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"And one more level: layouts can differ even when the Form is known (a `content` can be longer than than it needs to be, but such \"hidden\" elements are not considered to impact the equality of two arrays).\r\n\r\nFor example, these are to be considered the same, even though they have different NumpyArrays (the first counts up to 5 and the second counts up to 10):\r\n\r\n```python\r\n>>> ak.Array(ak.layout.ListOffsetArray64(\r\n...     ak.layout.Index64(np.array([0, 3, 3, 5])),\r\n...     ak.layout.NumpyArray(np.arange(5) * 1.1)))\r\n<Array [[0, 1.1, 2.2], [], [3.3, 4.4]] type='3 * var * float64'>\r\n\r\n>>> ak.Array(ak.layout.ListOffsetArray64(\r\n...     ak.layout.Index64(np.array([0, 3, 3, 5])),\r\n...     ak.layout.NumpyArray(np.arange(10) * 1.1)))\r\n<Array [[0, 1.1, 2.2], [], [3.3, 4.4]] type='3 * var * float64'>\r\n```\r\n\r\nBoth of these arrays have the same Type (`3 * var * float64`) and the same Form (`{\"class\": \"ListOffsetArray64\", \"offsets\": \"i64\", \"content\": \"float64\"}`), but they have different buffers. If you ran them through [ak.to_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_buffers.html) and compared the results, those results would be different from each other.\r\n\r\nThe \"lists of rules\" of what is considered a valid array are given in the [documentation of each Content subclass](https://awkward-array.readthedocs.io/en/latest/ak.layout.Content.html), in the constructor of each Pythonic illustration. These rules are also checked for a given array in the implementation of [ak.is_valid](https://awkward-array.readthedocs.io/en/latest/_auto/ak.is_valid.html). While there are these explicit lists of what is valid, there aren't any explicit lists of what is equal, though two arrays that have the same `.tolist()` representation and the same type _and are both valid_ are equal to each other.",
  "created_at":"2021-09-23T18:27:06Z",
  "id":926053585,
  "issue":1105,
  "node_id":"IC_kwDODBCWws43MnTR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-23T18:27:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Right. This could be fixed by having argmin/argmax with `axis=None` first apply `ak.fill_none` with `axis=-1` and a fill value that is `-np.inf` for argmax and `np.inf` for argmin, before running it through `ak._util.completely_flatten`.\r\n\r\n```python\r\n>>> ak.argmax(ak.fill_none(x, -np.inf, axis=-1))\r\n4\r\n```\r\n\r\nThis converts the numerical data into floating point numbers, which doesn't break argmin/argmax (positions of minimum/maximum are unchanged), but it's an unnecessary computation. A more sophisticated version would use `np.iinfo(dtype).min` and `.max` instead of `-np.inf` and `np.inf`, but an array could contain multiple dtypes. It's maybe best for a first fix to just use `-np.inf` and `np.inf`.",
  "created_at":"2021-09-24T14:33:52Z",
  "id":926676502,
  "issue":1106,
  "node_id":"IC_kwDODBCWws43O_YW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-24T14:33:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"or by using `axis=-1`:\r\n```python\r\n>>> x = ak.Array([1, 2, 3, None, 4])\r\n>>> ak.argmax(x, axis=-1)\r\n4\r\n```",
  "created_at":"2021-11-17T15:01:49Z",
  "id":971665165,
  "issue":1106,
  "node_id":"IC_kwDODBCWws456m8N",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-17T15:01:49Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"I had forgotten about this, but it's an important bug to fix promptly. I applied the easy fix described above in both v1 and v2.",
  "created_at":"2021-11-17T15:42:39Z",
  "id":971703691,
  "issue":1106,
  "node_id":"IC_kwDODBCWws456wWL",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-17T15:42:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That's right: we don't accept Python objects as components within an Awkward Array. The fact that Pandas does accept Python objects forces the columns that use them to be accessed via Python iteration and not slicing. (Actually, Awkward 0.x did that, too, but it became a confusing mess that I have a list of issue numbers for.)\r\n\r\nThe data in this DataFrame could be a nested array type. Here's a general purpose converter:\r\n\r\n```python\r\narray = ak.Array({x: df[x].values.tolist() for x in df.columns})\r\n```\r\n\r\nNow if we look at `array.tolist()`, we see\r\n\r\n```python\r\n[{'col1': [('key', 'aaaa'), ('value', '1111')], 'col2': 'foo'}, {'col1': [('key', 'bbbb'), ('value', '2222')], 'col2': 'bar'}]\r\n```\r\n\r\nand if we look at `array.type`, we see\r\n\r\n```python\r\n2 * {\"col1\": var * (string, string), \"col2\": string}\r\n```\r\n\r\nIt knows about the internal structure of col1, and therefore it's possible to slice into it in ways that wouldn't be possible for generic Python objects (because each object could, in principle, have a different type):\r\n\r\n```python\r\n>>> array[\"col1\", \"0\"]\r\n<Array [['key', 'value'], ['key', 'value']] type='2 * var * string'>\r\n>>> array[\"col1\", \"1\"]\r\n<Array [['aaaa', '1111'], ['bbbb', '2222']] type='2 * var * string'>\r\n```\r\n\r\nParquet can also know about this structure:\r\n\r\n```python\r\n>>> ak.to_parquet(array, \"/tmp/map.parquet\")\r\n>>> array2 = ak.from_parquet(\"/tmp/map.parquet\")\r\n>>> array2\r\n<Array [{col1: [, ... col2: 'bar'}] type='2 * {\"col1\": var * {\"0\": string, \"1\": ...'>\r\n```\r\n\r\nThat's almost an exact round-trip: what has been lost is that `array` knows that the key-value pairs are tuples whose left and right fields can be addressed as (string) `\"0\"` and `\"1\"`, but `array2` thinks they're records whose field names are `\"0\"` and `\"1\"`. (That's metadata we ought to store in Parquet, but don't yet.)\r\n\r\nI don't know for certain what pyarrow is doing, but I think it's writing your Python objects as a pickled bytestream, with metadata instructing it to unpickle them when read back. That's how it gets to `ak.from_parquet`'s internals as an object array (`dtype=\"O\"`).\r\n\r\nAwkward Array won't be changed to accept Python objects, but we could consider some other mitigation: maybe a better error message, or maybe an attempt to run `ak.from_iter` on object arrays (though that would hide a performance bottleneck\u2014the Python objects are themselves a performance bottleneck). Let me know what you think and we'll either close this issue or change its goal.",
  "created_at":"2021-09-24T15:56:55Z",
  "id":926738907,
  "issue":1107,
  "node_id":"IC_kwDODBCWws43POnb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-24T15:56:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know if I'm missing something and should reopen this issue, but I think I've addressed it and will be closing it now. Thanks!",
  "created_at":"2021-09-27T22:00:55Z",
  "id":928335258,
  "issue":1107,
  "node_id":"IC_kwDODBCWws43VUWa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-27T22:00:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Same thing in v2: https://github.com/scikit-hep/awkward-1.0/pull/1099#discussion_r717090955.",
  "created_at":"2021-09-27T22:33:02Z",
  "id":928373965,
  "issue":1108,
  "node_id":"IC_kwDODBCWws43VdzN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-27T22:33:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is what the latest commit does. Given a concrete array,\r\n\r\n```python\r\n>>> typetracer = ak._typetracer.TypeTracer.instance()\r\n>>> concrete = ak._v2.contents.NumpyArray(np.arange(2*3*5).reshape(2, 3, 5) * 0.1)\r\n>>> concrete\r\n<NumpyArray dtype='float64' shape='(2, 3, 5)'>\r\n    [[[0.  0.1 0.2 0.3 0.4]\r\n      [0.5 0.6 0.7 0.8 0.9]\r\n     ...\r\n      [2.  2.1 2.2 2.3 2.4]\r\n      [2.5 2.6 2.7 2.8 2.9]]]\r\n</NumpyArray>\r\n```\r\n\r\nwe can now make abstract arrays that know their `dtype` and as much `shape` information as possible. The shape is in an Interval class that has string representations like \"`2...2`\" (possible values are 2 through 2 inclusive, syntax is like Ruby). It successfully passes through `NumpyArray._getitem_at`.\r\n\r\n```python\r\n>>> abstract = ak._v2.contents.NumpyArray(concrete.to(typetracer))\r\n>>> abstract\r\n<NumpyArray dtype='float64' shape='(2...2, 3, 5)'>[?? ... ??]</NumpyArray>\r\n>>> abstract[0]\r\n<NumpyArray dtype='float64' shape='(3...3, 5)'>[?? ... ??]</NumpyArray>\r\n>>> abstract[0][0]\r\n<NumpyArray dtype='float64' len='5...5'>[?? ... ??]</NumpyArray>\r\n>>> abstract[0][0][0]\r\n0.0\r\n```\r\n\r\nThe scalar output (`0.0`) is a completely made-up value; it can be configured with `fill`. Zero is a relatively safe choice because the most common reason for extracting a number from an Index is to determine the length of the next Index to allocate, and zero is the most extreme length. (Hopefully, all of the instances where a new `offsets` Index is created has the array value be the length _before_ adding one.)",
  "created_at":"2021-10-06T17:07:55Z",
  "id":936708927,
  "issue":1110,
  "node_id":"IC_kwDODBCWws431Qs_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-06T17:07:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif @ianna @stormiestsin I'll be merging this soon: it has a lot of little diffs throughout the tests to verify that every v2 test we've ever written preserves type information when passed through with TypeTracerArrays (arrays with no concrete values, going through the code just to see how the Form changes). There have also been a few changes to the code to make it work, and I noticed a few tests hadn't been enabled (commented out or still testing v1, rather than v2). I've turned the latter into `@pytest.mark.skip` so that we can see them and get back to them when merging and simplify are ready.\r\n\r\nLet me know if there are any issues with merging this PR (i.e. you're working on something that would have a substantial diff with this branch). Thanks!",
  "created_at":"2021-10-11T21:22:05Z",
  "id":940449847,
  "issue":1110,
  "node_id":"IC_kwDODBCWws44DiA3",
  "performed_via_github_app":null,
  "reactions":{
   "+1":3,
   "total_count":3
  },
  "updated_at":"2021-10-11T21:22:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi Jim, I went through the tests that I wrote earlier. \r\n\r\nFor the `validityerror` tests, (please correct me here if I am wrong) the `content` of the array is essentially a `typetracer`, so it should be fine for all the `validityerror` tests till now, since none of the implemented `validityerror` tests access the data. \r\n\r\nThere is one case, where checking for `parameters` calls a `is_unique` on the array buffer. That accesses data. I put a `TODO` in the `validityerror_parameters`, and it checks for unique data when the `__array__` is `categorical`. Will this pass the `array.typetracer.validityerror()` test, when we write that test?\r\n\r\nEverything else in the tests look great! I am not working on anything at the moment, so you can merge it if others don't have an issue.",
  "created_at":"2021-10-12T07:36:15Z",
  "id":940745726,
  "issue":1110,
  "node_id":"IC_kwDODBCWws44EqP-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-12T07:36:15Z",
  "user":"MDQ6VXNlcjg4Mjg5MDg2"
 },
 {
  "author_association":"MEMBER",
  "body":"It's not exactly the `content` that's a TypeTracerArray object, but each NumpyArray's `data` and each Index's `data` (what would normally be a NumPy or CuPy array). The `validityerror` method, as all methods should, only iterate over data in the arrays (which doesn't exist for TypeTracerArray) in kernels (which are no-ops for TypeTracerArray).\r\n\r\nAll of the implementations looked fine, but some of the tests hadn't been running. I got a few of them running, adding code where necessary, but others have been left as skipped tests.",
  "created_at":"2021-10-12T11:36:08Z",
  "id":940926316,
  "issue":1110,
  "node_id":"IC_kwDODBCWws44FWVs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-12T11:36:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In PR #1117 (which will auto-merge soon), all v2 tests are being moved into a new `tests/v2 directory`, and the \"v2\" qualifier is being removed from their filenames. Please do that for the tests in this PR, too!\r\n\r\nThe other directory restructuring that I'll be doing today will be to copy high-level code into `src/awkward/v2`. At first, it will be commented out, and then I'll slowly uncomment things as they're made to work with v2 only. But since that involves new files, it won't interfere with any of your ongoing work.",
  "created_at":"2021-10-18T15:32:19Z",
  "id":945898151,
  "issue":1111,
  "node_id":"IC_kwDODBCWws44YUKn",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-10-18T15:32:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1111?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> :exclamation: No coverage uploaded for pull request base (`main@b9a40c8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#section-missing-base-commit).\n> The diff coverage is `n/a`.\n\n",
  "created_at":"2021-10-22T15:07:40Z",
  "id":949717583,
  "issue":1111,
  "node_id":"IC_kwDODBCWws44m4pP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-09T15:03:22Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - there are a couple of places where for loops should be replaced by newly written kernel functions - they are marked with FIXME. Apart from that I think, it's done.",
  "created_at":"2021-11-08T16:33:49Z",
  "id":963343339,
  "issue":1111,
  "node_id":"IC_kwDODBCWws45a3Pr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-08T16:33:49Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The last comment was a strong hint about the cause: it remembers that the data's a float, but it looks (in the string representation) like an integer:\r\n\r\n```python\r\n>>> f = ak.forms.Form.fromjson('{\"class\": \"NumpyArray\", \"itemsize\": 8, \"format\": \"l\", \"primitive\": \"int64\", \"parameters\": {\"thing\": 1.23}}')\r\n>>> f\r\n{\r\n    \"class\": \"NumpyArray\",\r\n    \"itemsize\": 8,\r\n    \"format\": \"l\",\r\n    \"primitive\": \"int64\",\r\n    \"parameters\": {\r\n        \"thing\": 1\r\n    }\r\n}\r\n>>> f.parameters\r\n{'thing': 1.23}\r\n```\r\n\r\nProbably all that's affected here is the JSON print-out, but there are likely some spots in the code that take this JSON as the object itself (for simplicity). I'm looking now.",
  "created_at":"2021-10-14T18:17:04Z",
  "id":943604339,
  "issue":1113,
  "node_id":"IC_kwDODBCWws44PkJz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-14T18:17:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #1114. Thanks for catching it!",
  "created_at":"2021-10-14T18:36:03Z",
  "id":943618364,
  "issue":1113,
  "node_id":"IC_kwDODBCWws44Pnk8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-14T18:36:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In PR #1117 (which will auto-merge soon), all v2 tests are being moved into a new `tests/v2 directory`, and the \"v2\" qualifier is being removed from their filenames. Please do that for the tests in this PR, too!\r\n\r\nThe other directory restructuring that I'll be doing today will be to copy high-level code into `src/awkward/v2`. At first, it will be commented out, and then I'll slowly uncomment things as they're made to work with v2 only. But since that involves new files, it won't interfere with any of your ongoing work.",
  "created_at":"2021-10-18T15:32:16Z",
  "id":945898082,
  "issue":1116,
  "node_id":"IC_kwDODBCWws44YUJi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-18T15:32:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna @ioanaif @stormiestsin @agoose77 Coverage for v2 developments are being tracked here: https://app.codecov.io/gh/scikit-hep/awkward-1.0\r\n\r\nThere might be a way to integrate it into this page, though I don't know it just yet. Anyway, I'm hoping to keep the code coverage monitor at a \"minimum noise\" level: it's more often something we'll want to dive into as a diagnostic for specific functions, rather than an overall score to get concerned about if it decreases. (The baseline is starting out at 77% without having ever looked at it before. The v1 is higher, but we know we have some \"skips\" in v2.)\r\n\r\nTomorrow, I'll copy high-level files into the _v2 directory.",
  "created_at":"2021-10-18T22:33:07Z",
  "id":946220454,
  "issue":1120,
  "node_id":"IC_kwDODBCWws44Zi2m",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-18T22:33:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1121?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> :exclamation: No coverage uploaded for pull request base (`main@36a0bb6`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#section-missing-base-commit).\n> The diff coverage is `n/a`.\n\n",
  "created_at":"2021-10-19T16:50:19Z",
  "id":946912458,
  "issue":1121,
  "node_id":"IC_kwDODBCWws44cLzK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-19T19:47:02Z",
  "user":"MDQ6VXNlcjY1NTUzMDgw"
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1122](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (6eb1e63) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/a440328f8097d22c2ba053fd117fed543829afc0?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (a440328) will **increase** coverage by `0.99%`.\n> The diff coverage is `17.85%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_prettyprint.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19wcmV0dHlwcmludC5weQ==) | `0.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/highlevel.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2hpZ2hsZXZlbC5weQ==) | `35.10% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/\\_connect/numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L251bXB5LnB5) | `7.35% <7.35%> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `64.28% <11.11%> (-4.10%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `67.94% <25.00%> (-2.33%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `69.01% <25.00%> (-2.63%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `69.23% <25.00%> (-3.69%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `63.15% <25.00%> (-2.12%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `70.76% <25.00%> (-3.01%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `68.35% <25.00%> (-2.32%)` | :arrow_down: |\n| ... and [104 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1122/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-10-19T22:59:47Z",
  "id":947166946,
  "issue":1122,
  "node_id":"IC_kwDODBCWws44dJ7i",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-21T21:54:43Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1123?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> :exclamation: No coverage uploaded for pull request base (`main@b9a40c8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#section-missing-base-commit).\n> The diff coverage is `n/a`.\n\n",
  "created_at":"2021-10-22T17:42:02Z",
  "id":949838514,
  "issue":1123,
  "node_id":"IC_kwDODBCWws44nWKy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-22T17:55:38Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif @ianna @stormiestsin @agoose77: the names associated with each of a Record/RecordArray's children are now called \"fields\" instead of \"keys\", which were the last two conflicting names for this concept. (Previously, there was also \"recordlookup,\" in addition to \"keys\".) Some time ago, @nsmith- and I deliberated over the multiplicity of names and narrowed in on \"fields,\" which was applied to the high-level (in PR #464), but only now to the mid-level.\r\n\r\nThere was also some confusion among classes as to whether this should be a property or a zero-argument method (like a Mapping's `keys()`); it's a property, and all array nodes have this property. (It goes down to the first nested record, returning an empty list if there isn't any. The property is always a list, though a RecordArray/RecordForm/RecordType with `_fields is None` means that that record is a tuple. The non-underscored `fields` in that case is `[str(i) for i in range(len(self._contents))]`.\r\n\r\nForm keys (`form_key`) and parameter keys are separate concepts, and they're still named \"keys.\" But all of the method and function argument names like `index_or_key`, referring to record fields, have been named `index_or_field` (etc.).\r\n\r\nThis PR touches a lot of v2 code, so it could be hard to merge (sorry)! We'll be doing more of these \"name alignments,\" getting mid-level names in sync with the high-level names later, but this one is particularly pervasive and a common word in Python like \"key\" is hard to fix in only the places where it ought to be fixed, so I think this will be easier to do now than later.",
  "created_at":"2021-10-22T17:47:54Z",
  "id":949842264,
  "issue":1123,
  "node_id":"IC_kwDODBCWws44nXFY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-22T17:47:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1124?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> :exclamation: No coverage uploaded for pull request base (`main@c054c8d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#section-missing-base-commit).\n> The diff coverage is `n/a`.\n\n",
  "created_at":"2021-10-22T22:02:46Z",
  "id":949983752,
  "issue":1124,
  "node_id":"IC_kwDODBCWws44n5oI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-22T22:02:46Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and this will require Arrow 6.0.0 as a minimum (so that `ExtensionArray.to_pylist()` works). Hopefully, that will come out in the next few days.",
  "created_at":"2021-10-25T21:04:57Z",
  "id":951329792,
  "issue":1125,
  "node_id":"IC_kwDODBCWws44tCQA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-25T21:04:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1125](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (71892e4) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `1.50%`.\n> The diff coverage is `87.03%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `67.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `69.01% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `72.00% <\u00f8> (+8.84%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `69.49% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/to\\_arrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC90b19hcnJvdy5weQ==) | `80.00% <\u00f8> (+37.14%)` | :arrow_up: |\n| [src/awkward/\\_v2/types/numpytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL251bXB5dHlwZS5weQ==) | `91.80% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `69.56% <50.00%> (-1.21%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `75.00% <59.09%> (+2.61%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/bitmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2JpdG1hc2tlZGFycmF5LnB5) | `63.27% <80.00%> (+3.39%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `62.85% <80.00%> (+0.63%)` | :arrow_up: |\n| ... and [23 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1125/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-10-25T21:10:00Z",
  "id":951333776,
  "issue":1125,
  "node_id":"IC_kwDODBCWws44tDOQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-02T22:10:16Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna, @ioanaif, and @stormiestsin, I think you all have open PRs. Arrow 6.0.0 came out today, which introduced [a bug](https://issues.apache.org/jira/browse/ARROW-14485) that [broke our builds](https://dev.azure.com/jpivarski/Scikit-HEP/_build/results?buildId=8055&view=logs&jobId=4f922444-fdfe-5dcf-b824-02f86439ef14&j=4f922444-fdfe-5dcf-b824-02f86439ef14&t=982d46f3-a8a2-5060-e80f-0c9fd0589210).\r\n\r\nThe single commit 477581d9937776a9fa984a5c278d1eccf14f25d6 is a work-around that avoids it (not a proper fix, but the `from_parquet` code will be rewritten in the Dask context, anyway). You may want to apply this work-around to your branches if you're hitting this test failure.",
  "created_at":"2021-10-27T01:43:40Z",
  "id":952464621,
  "issue":1125,
  "node_id":"IC_kwDODBCWws44xXTt",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-10-27T01:43:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for catching this, ~~though it is disappointing. Quite a lot of unit tests are dedicated to checking the reference counting between Python and C++ to avoid leaks and double-frees.~~ **Actually, it's not: in the course of writing this response, I discovered that there's no problem here. You could skip to the end.**\r\n\r\nThe expression `ak.Array([1])` invokes the ArrayBuilder, which walks over the Python data to build a tree of Content nodes representing the data. In this case, that's just an iterable containing a Python `int`, so the only node in that tree is an [Int64Builder](https://github.com/scikit-hep/awkward-1.0/blob/main/src/libawkward/builder/Int64Builder.cpp). It allocates a [GrowableBuffer](https://github.com/scikit-hep/awkward-1.0/blob/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b/src/libawkward/builder/GrowableBuffer.cpp#L19-L29) of initial length 1024 \u00d7 8 bytes = 8 kB, which calls `kernel::malloc`, and since this isn't a GPU, that's this code:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b/include/awkward/kernel-dispatch.h#L174-L178\r\n\r\na shared pointer with an action to release the array when the reference count goes to zero:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b/include/awkward/kernel-dispatch.h#L73-L81\r\n\r\nWhen it gets attached to a Python object, the C++ `shared_ptr`'s reference count is controlled by the Python object's reference count. When the Python object is deleted, this whole chain gets invoked that goes back to the array.\r\n\r\nI tried testing it, too, though I added a counter to see how many times an array is created:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> counter = 0\r\n>>> while True:\r\n...   tmp = ak.Array([1])\r\n...   counter += 1\r\n... \r\n^CTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\nKeyboardInterrupt\r\n>>> counter\r\n1140231\r\n```\r\n\r\nIf it wasn't deleting that array, then we'd have 1140231 \u00d7 8 kB = 9 GB of memory leak, which isn't the case. Looking at `free`, it's more like 300 MB, so the leak is not in the (large) arrays; it might be in the (small) C++ or Python object instances.\r\n\r\nWhile running the above Python code, I watched the memory go down with\r\n\r\n```bash\r\n% while (true); do free -m | fgrep Mem: | awk '{print $4}' ; sleep 1; done\r\n```\r\n\r\nHere's a plot: the horizontal axis is seconds; the vertical axis is megabytes (according to `free -m`). The flat parts are when I started and stopped the loop.\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/139464371-272a6805-1758-4ac5-8afc-9c63c33313d6.png)\r\n\r\nWith garbage-collected languages, we have to be careful about interpreting this because the garbage collector will let the memory use grow up to a point and then invoke collection. Maybe we're just not seeing it reach that point\u2014after all, only 300 MB have been used in this minute. (On my Linux box, a little more than the rate you saw on MacOS.) However, CPython only uses the garbage collector for the data that it can't eliminate via reference counting, which I _had thought_ was anything with a cycle in it. I tried to construct some pure Python examples with cycles, but Python (3.9) is too smart: it figured out that the self-referential data was cyclic and deleting it without waiting for the garbage collector.\r\n\r\nSo I might as well ask, _does_ the garbage collector find this data, _if_ it gets invoked? In Python, we can manually invoke it:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import gc\r\n>>> while True:\r\n...   tmp = ak.Array([1])\r\n...   tmp2 = gc.collect()\r\n... \r\n^CTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\nKeyboardInterrupt\r\n```\r\n\r\nThe difference is striking: note the much smaller vertical axis:\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/139465447-b9b7b16c-a554-4158-8ed3-aa5166b65c06.png)\r\n\r\nThe memory usage is jumping around (not very high or low) because we're sampling memory usage at random times with respect to when the garbage collector is running, but there clearly isn't the 300 MB steady decline that we saw without the garbage collector. **So one conclusion is that there's no permanent leak: when the garbage collector runs, the objects do get cleaned up.**\r\n\r\nWhy, with our non-cyclic data, is Python not cleaning them up right away? I'm not sure, but I had a hunch that it might be because the reference is going into a compiled extension. We can do the same sort of thing with NumPy, creating a reference cycle through NumPy arrays, which is also a compiled extension:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> while True:\r\n...   tmp = np.array([None], dtype=object)\r\n...   tmp[0] = tmp\r\n... \r\n^CTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\nKeyboardInterrupt\r\n```\r\n\r\nHere, the memory loss is dramatic: whatever NumPy's doing is leaving a lot more data to be cleaned up only at garbage collection time. It uses up all the memory on my computer until it has only a few hundred MB left, then the garbage collector finally decides to get to work and the usage levels out. (The flat part at the bottom is _before_ I stopped the loop.)\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/139466830-44febab9-5249-4dad-a9e5-2ac475f26291.png)\r\n\r\nJust for completeness, let's do the NumPy example with an explicit garbage collector invocation:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import gc\r\n>>> while True:\r\n...   tmp = np.array([None], dtype=object)\r\n...   tmp[0] = tmp\r\n...   tmp2 = gc.collect()\r\n... \r\n^CTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\nKeyboardInterrupt\r\n```\r\n\r\nIndeed, it keeps it clean from the start:\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/139467230-c83e2c48-7ef7-48e0-adfd-cd2d4e6b1633.png)\r\n\r\nThe bottom line is that this isn't a problem with Awkward Array; it's just how Python garbage collection works with data in extension libraries. You shouldn't be making millions of little `ak.Arrays` anyway; they'll be horribly inefficient compared to a few big `ak.Arrays`, but I understand that this was raised as an in-principle issue.\r\n\r\nThere's another point that I could have raised at the beginning of all of this, though: we're working on Awkward 2.0, which removes the C++ layer in favor of Python that calls out to zero-memory-allocation C functions with ctypes, so even if there was a real memory leak here, it would be gone in the upcoming pure Python version. I could have led with that, though it feels like a cop-out: a memory leak in the present version would be bad news. Anyway, for these **two** reasons, this is not an issue that needs action.\r\n\r\nThanks for your diligence in reporting it, though!",
  "created_at":"2021-10-29T16:35:30Z",
  "id":954886192,
  "issue":1127,
  "node_id":"IC_kwDODBCWws446mgw",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-10-29T16:35:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the awesome analysis. I was using ak.Array together with pytorch so at first I thought maybe the problem was there. In the end I walked around it by converting the arrays to regular lists with `.to_list()`. But thanks for providing details why this happens - such devilish behavior from Python",
  "created_at":"2021-10-29T17:12:11Z",
  "id":954907984,
  "issue":1127,
  "node_id":"IC_kwDODBCWws446r1Q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-29T17:12:28Z",
  "user":"MDQ6VXNlcjI5MzU0OTE0"
 },
 {
  "author_association":"MEMBER",
  "body":"You can also work around it by calling `gc.collect()` at opportune times. Although I'd try to avoid that in a library, rather than an application, I've found it necessary in some very tight code:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/7fc47ac486b77983f80f65973490c16e7ebca589/src/uproot/interpretation/library.py#L798-L811\r\n\r\nBut since you're developing an application, you know the exact speed-to-memory tradeoffs and can decide where you want the garbage collector to collect (in addition to the times it would decide to do so on its own). PyTorch also has compiled extensions, so the above may apply to it, too.\r\n\r\nIf you're calling `.to_list()` and that's not a noticeable performance (speed) penalty, you might have been able to get away with working with Python lists from the start (unless you found the array-at-a-time interface convenient, the other reason one might want to use Awkward Array).\r\n\r\n> such devilish behavior from Python\r\n\r\nWell, all garbage collected languages. CPython's hybrid of reference counting and garbage collection makes the garbage collection penalties less significant than, say, PyPy or the Java Virtual Machine. Instead, the cost is an extra 8 bytes in every object...",
  "created_at":"2021-10-29T17:32:43Z",
  "id":954920094,
  "issue":1127,
  "node_id":"IC_kwDODBCWws446uye",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-29T17:32:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1128](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (8ad7973) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `1.75%`.\n> The diff coverage is `81.03%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `67.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `69.01% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `72.00% <\u00f8> (+8.84%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `69.49% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/from\\_arrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9mcm9tX2Fycm93LnB5) | `57.14% <0.00%> (-22.86%)` | :arrow_down: |\n| [src/awkward/\\_v2/operations/convert/to\\_arrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC90b19hcnJvdy5weQ==) | `66.66% <0.00%> (+23.80%)` | :arrow_up: |\n| [...c/awkward/\\_v2/operations/convert/to\\_arrow\\_table.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC90b19hcnJvd190YWJsZS5weQ==) | `22.22% <0.00%> (-57.78%)` | :arrow_down: |\n| [src/awkward/\\_v2/types/numpytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL251bXB5dHlwZS5weQ==) | `91.80% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/to\\_layout.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC90b19sYXlvdXQucHk=) | `21.87% <10.71%> (-58.13%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `69.56% <50.00%> (-1.21%)` | :arrow_down: |\n| ... and [26 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1128/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-01T19:17:58Z",
  "id":956517235,
  "issue":1128,
  "node_id":"IC_kwDODBCWws45A0tz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-03T18:12:44Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"Needs the fixes described in https://github.com/scikit-hep/awkward-1.0/pull/1129#issue-1042897843 to pass tests. They'll be merged with #1125 if that happens first. I should just wait for that to happen and then bring this up to date with `main`, then re-enable this auto-merge.",
  "created_at":"2021-11-02T22:17:39Z",
  "id":958263318,
  "issue":1128,
  "node_id":"IC_kwDODBCWws45HfAW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-02T22:17:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1129?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1129](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1129?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (635282e) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **not change** coverage.\n> The diff coverage is `n/a`.\n\n",
  "created_at":"2021-11-02T22:40:52Z",
  "id":958310586,
  "issue":1129,
  "node_id":"IC_kwDODBCWws45Hqi6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-02T22:40:52Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1130](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (6c19830) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `2.10%`.\n> The diff coverage is `86.97%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `67.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `69.01% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `72.00% <\u00f8> (+8.84%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `69.49% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/to\\_arrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC90b19hcnJvdy5weQ==) | `80.00% <\u00f8> (+37.14%)` | :arrow_up: |\n| [src/awkward/\\_v2/types/numpytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL251bXB5dHlwZS5weQ==) | `91.80% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `69.56% <50.00%> (-1.21%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `75.00% <59.09%> (+2.61%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/bitmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2JpdG1hc2tlZGFycmF5LnB5) | `63.27% <80.00%> (+3.39%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `62.85% <80.00%> (+0.63%)` | :arrow_up: |\n| ... and [23 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1130/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-03T06:46:56Z",
  "id":958689603,
  "issue":1130,
  "node_id":"IC_kwDODBCWws45JHFD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-03T06:46:56Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"So far, this _removes_ `simplify_uniontype`, but it doesn't move it into UnionArray.",
  "created_at":"2021-11-03T15:49:31Z",
  "id":959518672,
  "issue":1130,
  "node_id":"IC_kwDODBCWws45MRfQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-03T15:49:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> So far, this _removes_ `simplify_uniontype`, but it doesn't move it into UnionArray.\r\n\r\nIt was already in UnionArray, so I just removed it from `content`.",
  "created_at":"2021-11-03T15:51:58Z",
  "id":959527261,
  "issue":1130,
  "node_id":"IC_kwDODBCWws45MTld",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-03T15:51:58Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, so is this PR done?",
  "created_at":"2021-11-03T15:59:48Z",
  "id":959553489,
  "issue":1130,
  "node_id":"IC_kwDODBCWws45MZ_R",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-03T15:59:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes. All done with this PR. Will open new ones for the other tickets starting with `num`",
  "created_at":"2021-11-03T16:01:30Z",
  "id":959558918,
  "issue":1130,
  "node_id":"IC_kwDODBCWws45MbUG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-03T16:01:30Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1131](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (151508c) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `1.16%`.\n> The diff coverage is `81.12%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `67.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `69.01% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `72.00% <\u00f8> (+8.84%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `69.49% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/from\\_arrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9mcm9tX2Fycm93LnB5) | `57.14% <0.00%> (-22.86%)` | :arrow_down: |\n| [src/awkward/\\_v2/operations/convert/to\\_arrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC90b19hcnJvdy5weQ==) | `66.66% <0.00%> (+23.80%)` | :arrow_up: |\n| [...c/awkward/\\_v2/operations/convert/to\\_arrow\\_table.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC90b19hcnJvd190YWJsZS5weQ==) | `22.22% <0.00%> (-57.78%)` | :arrow_down: |\n| [src/awkward/\\_v2/types/numpytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL251bXB5dHlwZS5weQ==) | `91.80% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/to\\_layout.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC90b19sYXlvdXQucHk=) | `21.87% <10.71%> (-58.13%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `69.56% <50.00%> (-1.21%)` | :arrow_down: |\n| ... and [26 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1131/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-03T15:06:27Z",
  "id":959372177,
  "issue":1131,
  "node_id":"IC_kwDODBCWws45LtuR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-03T15:06:27Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1132](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (c2e03bb) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `0.56%`.\n> The diff coverage is `63.49%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/behaviors/string.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2JlaGF2aW9ycy9zdHJpbmcucHk=) | `100.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `67.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `69.01% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `72.00% <\u00f8> (+8.84%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `69.49% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/highlevel.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2hpZ2hsZXZlbC5weQ==) | `41.37% <\u00f8> (+6.26%)` | :arrow_up: |\n| [.../awkward/\\_v2/operations/convert/ak\\_from\\_buffers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2J1ZmZlcnMucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2N1cHkucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_iter.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2l0ZXIucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pheC5weQ==) | `80.00% <0.00%> (\u00f8)` | |\n| ... and [121 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1132/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-03T16:01:59Z",
  "id":959560576,
  "issue":1132,
  "node_id":"IC_kwDODBCWws45MbuA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T17:52:49Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"`recursively_apply` and `broadcast_and_apply` haven't been fully tested, but that will come when we reintroduce all the high-level functions. Whatever bugs are left are corner-cases. (It's translated from old code that worked and the basic cases work in the new code.)\r\n\r\nSo I'll merge this as soon as the tests pass, and then move on to to/from_buffers.",
  "created_at":"2021-11-04T16:52:52Z",
  "id":961230989,
  "issue":1132,
  "node_id":"IC_kwDODBCWws45SziN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T16:52:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If `f(ak.Array([1,2,3]))` didn't work at first (with \"non-precise type pyobject\" specifically) and then worked later, I suspect that the problem is that `ak._connect._numba.register()` is not getting called. This is what triggers the definition of all the Numba handlers for Awkward Arrays, so that it recognizes them as something other than vanilla Python objects.\r\n\r\nThis `ak._connect._numba.register()` is supposed to be called _by Numba_ when Numba starts up with `import numba`. This happens through a Python setuptools [entry point](https://setuptools.pypa.io/en/latest/pkg_resources.html#entry-points) that Numba has provided: when `import numba` is invoked, it runs all of the entry points that other packages, such as Awkward Array, have registered.\r\n\r\nI don't understand all the aspects of this mechanism, but it depends on the global state of the directories that pip and Python look at. There have been times when one version of `awkward` didn't uninstall cleanly on my development laptop and I've gotten this error. It's not a pure function of the Python library\u2014it depends on this extra information that pip sets up. If you're running `awkward` out of a local directory and haven't pip-installed it, for instance, it would always fail in this way. The way that I've fixed it in the past has been to `pip uninstall awkward` until there are no versions of it left and then do one clean `pip install` of the desired version.\r\n\r\nI don't know whether you're seeing this error in your development environment or in continuous testing\u2014I'd be more surprised about seeing it in continuous testing because that's usually a fresh install and can't be as easily broken. If it's in a development environment, it's more likely to affect intensive users than casual users because of all the updates that we do. I don't know if using venv strictly would make this 100% clean.\r\n\r\nIn any event, a much better work-around if you just want to work around it is\r\n\r\n```python\r\nak._connect._numba.register()\r\n```\r\n\r\nIf calling that before any Numba compilation of Awkward Arrays (i.e. concrete types, triggering an actual JIT) doesn't solve it, then something deeper is wrong.",
  "created_at":"2021-11-04T15:21:34Z",
  "id":961148902,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45Sffm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T15:21:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks Jim for the quick answer and detailed explanation.\r\n\r\nUnfortunately, calling `ak._connect._numba.register()` right after `import awkward as ak` does not help (`import numba` was done before importing `awkward`), it gives the same error and works after calling with a `numpy` array.\r\n\r\nOK, so the actual problem is then maybe related to `pip` since on my setup, Awkward is installed via Conda. I have not fully understood yet how the entry point hack works but I suspect that the Conda forge recipe is not picking it up (this is just a wild guess, I have really little idea about the mechanics). The reason I use `conda` is because on my current working machine (M1 MacBook), `numpy` and `numba` are not `pip install`able.\r\n\r\nI just checked in one of our batch farms (Lyon CC IN2P3) where everything is installed via `pip/venv` and the behaviour is similar, so maybe my `pip` theory is flawed `;)`\r\n\r\n```\r\n>>> import numba as nb\r\n... import numpy as np\r\n... import awkward as ak\r\n...\r\n... @nb.vectorize\r\n... def f(x):\r\n...     return x\r\n...\r\n\r\n>>> f(ak.Array([1,2,3]))\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-7e4d70666b5e> in <module>\r\n----> 1 f(ak.Array([1,2,3]))\r\n\r\n/pbs/throng/km3net/software/python/3.7.5/lib/python3.7/site-packages/numba/np/ufunc/dufunc.py in _compile_for_args(self, *args, **kws)\r\n    177         argtys = []\r\n    178         for arg in args[:nin]:\r\n--> 179             argty = typeof(arg)\r\n    180             if isinstance(argty, types.Array):\r\n    181                 argty = argty.dtype\r\n\r\n/pbs/throng/km3net/software/python/3.7.5/lib/python3.7/site-packages/numba/core/typing/typeof.py in typeof(val, purpose)\r\n     31         msg = _termcolor.errmsg(\r\n     32             \"cannot determine Numba type of %r\") % (type(val),)\r\n---> 33         raise ValueError(msg)\r\n     34     return ty\r\n     35\r\n\r\nValueError: cannot determine Numba type of <class 'awkward.highlevel.Array'>\r\n\r\n>>> f(np.array([1,2,3]))\r\narray([1, 2, 3])\r\n\r\n>>> f(ak.Array([1,2,3]))\r\n<Array [1, 2, 3] type='3 * int64'>\r\n\r\n>>> ak.__version__\r\n'1.5.1'\r\n\r\n>>> nb.__version__\r\n'0.50.1'\r\n```",
  "created_at":"2021-11-04T15:35:24Z",
  "id":961161807,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45SipP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T15:35:58Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, `ak._connect._numba.register()` does exactly what the entry point does, so if calling that registration function explicitly doesn't work as a work-around, then it's indicating that something else is the real problem.\r\n\r\nThere was some minimum Numba version that supported the entry point\u2014I submitted a bug report about it being called too late and that was fixed in some version. I'm using Numba version 0.54.1 right now (the latest from conda-forge). Can you try that? 0.50 sounds close to the threshold when some of these things were fixed.",
  "created_at":"2021-11-04T16:00:35Z",
  "id":961184270,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45SoIO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T16:00:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"On my M1 Mac, I have `0.54.1`:\r\n\r\n```\r\nIn [1]: import numba as nb\r\nn\r\nIn [2]: nb.__version__\r\nOut[2]: '0.54.1'\r\n```\r\n\r\nI updated Numba in our computing centre (CentOS 7) to `0.54.1` but still the same error.\r\n\r\nI noticed however that when I do not do the `ak._connect._numba.register()`, I get this:\r\n\r\n```\r\n>>> f(ak.Array([1,2,3]))\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-7e4d70666b5e> in <module>\r\n----> 1 f(ak.Array([1,2,3]))\r\n\r\n/pbs/throng/km3net/software/python/3.7.5/lib/python3.7/site-packages/numba/np/ufunc/dufunc.py in _compile_for_args(self, *args, **kws)\r\n    186         argtys = []\r\n    187         for arg in args[:nin]:\r\n--> 188             argty = typeof(arg)\r\n    189             if isinstance(argty, types.Array):\r\n    190                 argty = argty.dtype\r\n\r\n/pbs/throng/km3net/software/python/3.7.5/lib/python3.7/site-packages/numba/core/typing/typeof.py in typeof(val, purpose)\r\n     33         msg = _termcolor.errmsg(\r\n     34             f\"Cannot determine Numba type of {type(val)}\")\r\n---> 35         raise ValueError(msg)\r\n     36     return ty\r\n     37\r\n\r\nValueError: Cannot determine Numba type of <class 'awkward.highlevel.Array'>\r\n```\r\n\r\nand after calling the register function, I get this:\r\n\r\n```\r\n>>> ak._connect._numba.register()\r\n\r\n>>> f(ak.Array([1,2,3]))\r\n<ipython-input-4-a13cf829227f>:1: NumbaWarning:\r\nCompilation is falling back to object mode WITHOUT looplifting enabled because Function \"f\" failed type inference due to: non-precise type pyobject\r\nDuring: typing of argument at <ipython-input-4-a13cf829227f> (3)\r\n\r\nFile \"<ipython-input-4-a13cf829227f>\", line 3:\r\ndef f(x):\r\n    return x\r\n    ^\r\n\r\n  @nb.vectorize\r\n/pbs/throng/km3net/software/python/3.7.5/lib/python3.7/site-packages/numba/core/object_mode_passes.py:152: NumbaWarning: Function \"f\" was compiled in object mode without forceobj=True.\r\n\r\nFile \"<ipython-input-4-a13cf829227f>\", line 2:\r\n@nb.vectorize\r\ndef f(x):\r\n^\r\n\r\n  state.func_ir.loc))\r\n/pbs/throng/km3net/software/python/3.7.5/lib/python3.7/site-packages/numba/core/object_mode_passes.py:162: NumbaDeprecationWarning:\r\nFall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\r\n\r\nFor more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\r\n\r\nFile \"<ipython-input-4-a13cf829227f>\", line 2:\r\n@nb.vectorize\r\ndef f(x):\r\n^\r\n\r\n  state.func_ir.loc))\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-7e4d70666b5e> in <module>\r\n----> 1 f(ak.Array([1,2,3]))\r\n\r\n/pbs/throng/km3net/software/python/3.7.5/lib/python3.7/site-packages/numba/np/ufunc/dufunc.py in _compile_for_args(self, *args, **kws)\r\n    200                 argty = numpy_support.map_arrayscalar_type(arg)\r\n    201             argtys.append(argty)\r\n--> 202         return self._compile_for_argtys(tuple(argtys))\r\n    203\r\n    204     def _compile_for_argtys(self, argtys, return_type=None):\r\n\r\n/pbs/throng/km3net/software/python/3.7.5/lib/python3.7/site-packages/numba/np/ufunc/dufunc.py in _compile_for_argtys(self, argtys, return_type)\r\n    220             self._dispatcher, self.targetoptions, sig)\r\n    221         actual_sig = ufuncbuilder._finalize_ufunc_signature(\r\n--> 222             cres, argtys, return_type)\r\n    223         dtypenums, ptr, env = ufuncbuilder._build_element_wise_ufunc_wrapper(\r\n    224             cres, actual_sig)\r\n\r\n/pbs/throng/km3net/software/python/3.7.5/lib/python3.7/site-packages/numba/np/ufunc/ufuncbuilder.py in _finalize_ufunc_signature(cres, args, return_type)\r\n    183         if cres.objectmode:\r\n    184             # Object mode is used and return type is not specified\r\n--> 185             raise TypeError(\"return type must be specified for object mode\")\r\n    186         else:\r\n    187             return_type = cres.signature.return_type\r\n\r\nTypeError: return type must be specified for object mode\r\n\r\n>>> import numpy as np\r\n\r\n>>> f(np.array([1,2,3]))\r\narray([1, 2, 3])\r\n\r\n>>> f(ak.Array([1,2,3]))\r\n<Array [1, 2, 3] type='3 * int64'>\r\n```",
  "created_at":"2021-11-04T18:07:13Z",
  "id":961292757,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45TCnV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T18:07:13Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Huh, so `ak._connect._numba.register()` is doing what it's supposed to: Numba wasn't recognizing `ak.Array` before it was called, and has a different error afterward. (Mystery number 1: why aren't the entry points working?)\r\n\r\nFor mystery number 2: does `nb.njit` work but `nb.vectorize` not work? I haven't done much testing with `nb.vectorize`.",
  "created_at":"2021-11-04T18:41:29Z",
  "id":961317952,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45TIxA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T18:41:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes, I confirm that `@nb.njit` works. So it's related to `nb.vectorize`. My initial thought was that some introspection in the type inference chokes on something in Awkward. `nb.vectorize` will presumable inspect element types etc. but I am really not that comfortable with Numba intrinsics.",
  "created_at":"2021-11-04T18:46:41Z",
  "id":961321683,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45TJrT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T18:46:41Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, `@nb.vectorize` must be expecting the arguments to be arrays, so that it can compile in a loop over those arrays. Awkward Arrays are not recognized in Numba as ArrayLike, but only as Iterable. (ArrayLike would require us to produce a `shape` and `dtype`, which would only be possible for rectilinear ones, and that's value information, not type information.)\r\n\r\nI've used `@vectorize` to make ufuncs that Awkward Array has then caught and used like a NumPy ufunc, which is a very different code path. To do that, however, the types have to be given in the `@vectorize` decorator so that it can be compiled: only compiled, ready-to-run ufuncs satisfy NEP13 (call `ak.Array.__array_ufunc__`).\r\n\r\nDoes it work if you give `@vectorize` type info? It would be the data types of an element, so the types might be `[nb.float64, nb.int32]`, for instance.",
  "created_at":"2021-11-04T19:04:38Z",
  "id":961335230,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45TM--",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T19:04:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes! With e.g. `@nb.vectorize(\"int64(int64)\")` it works, but I have to specify all cases. I will have a look if this works for the actual use-case.",
  "created_at":"2021-11-04T19:25:25Z",
  "id":961349736,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45TQho",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T19:25:25Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"NONE",
  "body":"Alright, I can live with explicit type annotations, at least I managed to cover all the expected cases.\r\n\r\nDo you want to close this for now or are you going to look deeper?",
  "created_at":"2021-11-05T08:37:23Z",
  "id":961714140,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45Upfc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-05T08:37:23Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"We still don't know why registration didn't happen automatically, but that's an installation thing\u2014I don't think the problem is inside the codebase.\r\n\r\nAs for explicit annotations in vectorize, that's something I knew about but didn't connect to your case right away. This is labeled as a feature request\u2014I'll change the title to make it more explicit. It might require a change on Numba's side (e.g. have uncompelled vectorized functions check for a `__array_ufunc__` method and pass itself to that, optimistically).",
  "created_at":"2021-11-05T12:13:29Z",
  "id":961846093,
  "issue":1133,
  "node_id":"IC_kwDODBCWws45VJtN",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-05T12:13:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1134](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (c273c1b) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `2.39%`.\n> The diff coverage is `70.22%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/behaviors/string.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2JlaGF2aW9ycy9zdHJpbmcucHk=) | `100.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (+7.69%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (+7.04%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `77.33% <\u00f8> (+14.17%)` | :arrow_up: |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2N1cHkucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pheC5weQ==) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_json.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pzb24ucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [...rc/awkward/\\_v2/operations/convert/ak\\_from\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX251bXB5LnB5) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19jdXB5LnB5) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19qYXgucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| ... and [137 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1134/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-04T22:15:25Z",
  "id":961474279,
  "issue":1134,
  "node_id":"IC_kwDODBCWws45Tu7n",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T23:57:09Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"Next after this is `ak._v2.highlevel.__getstate__`/`__setstate__` (pickle). Then uncomment the tests (tests/v2/test_0348-form-keys.py, tests/v2/test_1007-from_buffers-empty-ndarray.py) and that should be it for this PR. It may need a pass-through `ak._v2.operations.structure.packed` (for pickle).",
  "created_at":"2021-11-10T01:05:26Z",
  "id":964686141,
  "issue":1134,
  "node_id":"IC_kwDODBCWws45f_E9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T01:05:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1135](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (d1a5815) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `0.75%`.\n> The diff coverage is `66.18%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/behaviors/string.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2JlaGF2aW9ycy9zdHJpbmcucHk=) | `100.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `67.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `69.01% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `72.00% <\u00f8> (+8.84%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `69.49% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/highlevel.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2hpZ2hsZXZlbC5weQ==) | `41.37% <\u00f8> (+6.26%)` | :arrow_up: |\n| [.../awkward/\\_v2/operations/convert/ak\\_from\\_buffers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2J1ZmZlcnMucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2N1cHkucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_iter.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2l0ZXIucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pheC5weQ==) | `80.00% <0.00%> (\u00f8)` | |\n| ... and [132 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1135/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-08T06:20:53Z",
  "id":962846633,
  "issue":1135,
  "node_id":"IC_kwDODBCWws45Y9-p",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-09T17:09:10Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"I agree; it's done now, so I'll enable auto-merge. Thanks again!",
  "created_at":"2021-11-09T17:03:27Z",
  "id":964349656,
  "issue":1135,
  "node_id":"IC_kwDODBCWws45es7Y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-09T17:03:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Instead of\r\n\r\n```python\r\n>>> ak.Array(\r\n...     ak.layout.RegularArray(\r\n...         ak.layout.RegularArray(\r\n...             ak.layout.NumpyArray(np.arange(24)),\r\n...             0,\r\n...         ),\r\n...         3,\r\n...     ),\r\n... )\r\n<Array [] type='0 * 3 * 0 * int64'>\r\n```\r\n\r\nit needs to make\r\n\r\n```python\r\n>>> ak.Array(\r\n...     ak.layout.RegularArray(\r\n...         ak.layout.RegularArray(\r\n...             ak.layout.NumpyArray(np.arange(24)),\r\n...             0,\r\n...             zeros_length=6,   #  <-----\r\n...         ),\r\n...         3,\r\n...     ),\r\n... )\r\n<Array [[[], [], []], [[], [], []]] type='2 * 3 * 0 * int64'>\r\n```",
  "created_at":"2021-11-08T19:44:05Z",
  "id":963510790,
  "issue":1136,
  "node_id":"IC_kwDODBCWws45bgIG",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-08T19:44:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It went deeper than just `ak.zip`; it was in `NumpyArray.toRegularArray()`, so that potentially fixes a lot of unseen bugs!",
  "created_at":"2021-12-07T23:53:17Z",
  "id":988352374,
  "issue":1136,
  "node_id":"IC_kwDODBCWws466Q92",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "rocket":1,
   "total_count":2
  },
  "updated_at":"2021-12-07T23:53:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1137](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f24eee1) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `2.58%`.\n> The diff coverage is `67.04%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/behaviors/string.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2JlaGF2aW9ycy9zdHJpbmcucHk=) | `100.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (+7.69%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (+7.04%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `77.33% <\u00f8> (+14.17%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (+5.08%)` | :arrow_up: |\n| [src/awkward/\\_v2/highlevel.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2hpZ2hsZXZlbC5weQ==) | `51.70% <0.00%> (+16.59%)` | :arrow_up: |\n| [.../awkward/\\_v2/operations/convert/ak\\_from\\_buffers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2J1ZmZlcnMucHk=) | `85.85% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2N1cHkucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_iter.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2l0ZXIucHk=) | `78.57% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pheC5weQ==) | `80.00% <0.00%> (\u00f8)` | |\n| ... and [147 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1137/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-09T10:32:12Z",
  "id":964019011,
  "issue":1137,
  "node_id":"IC_kwDODBCWws45dcND",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-11T16:04:02Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1138](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (bdf2ab2) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `3.69%`.\n> The diff coverage is `74.74%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/behaviors/string.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2JlaGF2aW9ycy9zdHJpbmcucHk=) | `100.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (+7.69%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (+7.04%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `78.66% <\u00f8> (+15.50%)` | :arrow_up: |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2N1cHkucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pheC5weQ==) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_json.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pzb24ucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [...rc/awkward/\\_v2/operations/convert/ak\\_from\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX251bXB5LnB5) | `36.53% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19jdXB5LnB5) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19qYXgucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| ... and [152 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1138/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-09T14:05:49Z",
  "id":964181957,
  "issue":1138,
  "node_id":"IC_kwDODBCWws45eD_F",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-12T15:38:30Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - perhaps, related to this discussion - here is a recursive version `sys.getsizeof()` that is extendable with custom handlers: https://code.activestate.com/recipes/577504/\r\n\r\nAn example of a `NumpyArrayHandler`:\r\n```python\r\ndef NumpyArrayHandler(obj):\r\n   yield obj._data\r\n   yield obj._data.ctypes.data\r\n   yield obj._data.shape\r\n```\r\na test case:\r\n```python\r\nnp_data = np.random.random(size=(4, 100 * 1024 * 1024 // 8 // 4))\r\narray = ak.from_numpy(np_data, regulararray=False)\r\narray = v1_to_v2(array.layout)\r\n```\r\nand a result:\r\n```python\r\nprint(total_size(array,{ak._v2.contents.NumpyArray:NumpyArrayHandler},verbose=True))\r\n48 <class 'awkward._v2.contents.numpyarray.NumpyArray'> <NumpyArray d...\r\n</NumpyArray>\r\n112 <class 'numpy.ndarray'> array([[0.798... 0.46882701]])\r\n32 <class 'int'> 140219093352448\r\n56 <class 'tuple'> (4, 3276800)\r\n28 <class 'int'> 4\r\n276\r\n```\r\nNumpy's `nbytes` returns:\r\n```python\r\narray.nbytes()\r\n104857600\r\n```\r\nThe `array` object graph produced with an [objgraph](https://mg.pov.lt/objgraph/) is:\r\n![array-graph](https://user-images.githubusercontent.com/1390682/141276847-3bf289e9-5ee7-43be-9ec3-f41146585e4a.png)\r\n",
  "created_at":"2021-11-11T09:15:01Z",
  "id":966125833,
  "issue":1138,
  "node_id":"IC_kwDODBCWws45lekJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-11T09:53:27Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The problem that recipe is solving is that `sys.getsizeof` is not deep, and most Python objects link to a lot of other objects. The situation is complicated because an object might link to some large data (or a cycle) that would not be deleted when the object gets deleted.\r\n\r\nFortunately, we don't need to solve the problem of figuring out how much memory is held by an ak.Array object graph. The array buffers are much larger than the Python objects, so just as NumPy's `nbytes` ignores the objects and only reports the data buffers (which aren't even counted in `sys.getsizeof`, recursive or not), we can ignore all the tuples and dicts in a Content hierarchy and just add up the sizes of the buffers.\r\n\r\nAs an `nbytes` property, it would just call NumPy's `nbytes` for all the Indexes and the NumpyArray `data` in the hierarchy, assuming no overlap. The function that takes overlap into account (not a method, as it will have to take multiple arrays to define what counts as overlap) will be more interesting.",
  "created_at":"2021-11-11T12:48:10Z",
  "id":966275222,
  "issue":1138,
  "node_id":"IC_kwDODBCWws45mDCW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-11T12:48:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In fact, in that overlaps function, two buffers can overlap without sharing the same pointer address. One might start at pointer address 0x1000000000000000 and have length 1024 while another starts at 0x1000000000000010 and have length 64. Overlaps are intervals in a one-dimensional space.",
  "created_at":"2021-11-11T12:50:42Z",
  "id":966276841,
  "issue":1138,
  "node_id":"IC_kwDODBCWws45mDbp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-11T12:50:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - an `nbytes` property is done and tested. The high-level `nbytes` property is enabled. Since I'll be starting on a function that takes overlaps into an account in the studies, I think, this PR can be merged.",
  "created_at":"2021-11-11T16:49:58Z",
  "id":966457770,
  "issue":1138,
  "node_id":"IC_kwDODBCWws45mvmq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-11T16:49:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, it's done. Please, check it when you have time. Thanks!",
  "created_at":"2021-11-12T13:18:35Z",
  "id":967112385,
  "issue":1138,
  "node_id":"IC_kwDODBCWws45pPbB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-12T13:18:35Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"This is understandably confusing: we did some work last summer and had some GPU demonstrators, but GPUs are not ready for use in analysis. The documentation stub is prominent because when this feature _is_ available, it should be highly featured.\r\n\r\nThe detailed status is that the demonstrators have shown that it's possible with the current architecture, but we wouldn't be able to remove synchronization points between kernel calls that _might_ kill performance if we continue the way we've been headed. Because of this and difficulties extending Awkward Array with JAX and Dask, we started an internal refactoring that replaces a lot of C++ objects with Python objects. (Ironically, that would help us fix the above-mentioned performance issue because we'll be able to delay evaluation of the kernels in such a way that the parallel process won't be interleaved with synchronization points.) We're in the middle of that refactoring.\r\n\r\nGPUs are definitely in the plans: [OAC-2103945](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2103945) has complete GPU support as one of its major goals, though that is a 3 year project.",
  "created_at":"2021-11-09T22:09:19Z",
  "id":964591157,
  "issue":1139,
  "node_id":"IC_kwDODBCWws45fn41",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-09T22:09:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks a lot for details Jim. I will continue the usual way in the mean time. ",
  "created_at":"2021-11-10T08:37:03Z",
  "id":964899006,
  "issue":1139,
  "node_id":"IC_kwDODBCWws45gzC-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T08:37:03Z",
  "user":"MDQ6VXNlcjQ5OTY2MDk="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1140](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (4a52773) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `0.75%`.\n> The diff coverage is `66.21%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/behaviors/string.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2JlaGF2aW9ycy9zdHJpbmcucHk=) | `100.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `67.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `69.01% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `72.00% <\u00f8> (+8.84%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `69.49% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/highlevel.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2hpZ2hsZXZlbC5weQ==) | `41.37% <\u00f8> (+6.26%)` | :arrow_up: |\n| [.../awkward/\\_v2/operations/convert/ak\\_from\\_buffers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2J1ZmZlcnMucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2N1cHkucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_iter.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2l0ZXIucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pheC5weQ==) | `80.00% <0.00%> (\u00f8)` | |\n| ... and [124 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1140/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-10T08:26:37Z",
  "id":964889802,
  "issue":1140,
  "node_id":"IC_kwDODBCWws45gwzK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T08:26:37Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - this PR is complete. Please, check when you have time. Thanks!",
  "created_at":"2021-11-10T16:21:14Z",
  "id":965506784,
  "issue":1140,
  "node_id":"IC_kwDODBCWws45jHbg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T16:21:14Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"It is tested. (I checked out your branch, ran all the tests\u2014success\u2014then added an exception on the line I wanted to see if it was covered\u2014it raised that exception\u2014so we're good.)",
  "created_at":"2021-11-10T18:53:37Z",
  "id":965642085,
  "issue":1140,
  "node_id":"IC_kwDODBCWws45jodl",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-10T18:53:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1141](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (b1bb82d) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `0.90%`.\n> The diff coverage is `66.21%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/behaviors/string.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2JlaGF2aW9ycy9zdHJpbmcucHk=) | `100.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `67.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `69.01% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `72.00% <\u00f8> (+8.84%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `69.49% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/highlevel.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2hpZ2hsZXZlbC5weQ==) | `41.37% <\u00f8> (+6.26%)` | :arrow_up: |\n| [.../awkward/\\_v2/operations/convert/ak\\_from\\_buffers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2J1ZmZlcnMucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2N1cHkucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_iter.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2l0ZXIucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pheC5weQ==) | `80.00% <0.00%> (\u00f8)` | |\n| ... and [126 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1141/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-10T09:11:47Z",
  "id":964924283,
  "issue":1141,
  "node_id":"IC_kwDODBCWws45g5N7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T09:11:47Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1142](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f59e490) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `0.07%`.\n> The diff coverage is `72.56%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `76.92% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [45 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1142/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-10T15:16:01Z",
  "id":965370709,
  "issue":1142,
  "node_id":"IC_kwDODBCWws45imNV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-12T19:46:43Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The following test fails with Numpy 1.3:\r\n```python\r\ndef test_ufunc_sum():\r\n    nparray = np.array(\r\n        [np.datetime64(\"2021-06-03T10:00\"), np.datetime64(\"2021-06-03T11:00\")]\r\n    )\r\n    akarray = ak._v2.highlevel.Array(nparray)\r\n\r\n    with pytest.raises(np.core._exceptions.UFuncTypeError):\r\n        akarray[1:] + akarray[:-1]\r\n\r\n```\r\ndue to an import error:\r\n```\r\ntests/v2/test_0835-datetime-type.py:12: in <module>\r\n    import numpy.core._exceptions as np_exception\r\nE   ImportError: No module named 'numpy.core._exceptions'\r\n```",
  "created_at":"2021-11-12T12:51:31Z",
  "id":967096825,
  "issue":1142,
  "node_id":"IC_kwDODBCWws45pLn5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-12T12:51:31Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"`completely_flatten` is now a `Content` method. (Since it only involves one array, turning it into a method lets us do OOP-style small functions instead of `if`-`elif` chains, which were only there because I was in a hurry and wanted to avoid writing C++ code.)",
  "created_at":"2021-11-12T13:29:03Z",
  "id":967119051,
  "issue":1142,
  "node_id":"IC_kwDODBCWws45pRDL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-12T13:29:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> ```python\r\n>     with pytest.raises(np.core._exceptions.UFuncTypeError):\r\n>         akarray[1:] + akarray[:-1]\r\n> ```\r\n\r\nRelying on a private module of another library is not a good idea, for exactly this reason. (It could disappear again in a future version, without warning.) Checking for TypeError (UFuncTypeError's immediately superclass) would work just as well. That's a public API.",
  "created_at":"2021-11-12T15:13:56Z",
  "id":967192930,
  "issue":1142,
  "node_id":"IC_kwDODBCWws45pjFi",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-12T15:13:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've changed the title of this PR to better reflect what has been updated. I'd also like to merge this as soon as it passes tests to keep PRs granular. (I've made some code cleanliness fixes, and if I'm working on something else in another PR, I might get confused that they're not already done. Thus, I want to start any PRs I do later today on this one.)\r\n\r\nNote that I haven't moved any new tests into tests/v2 and I haven't changed the \"skip\" status of any that are there. The things you were planning to do with this PR aren't done yet. You'll want to open a new PR, probably with this one's old name, to address those things.",
  "created_at":"2021-11-12T19:47:19Z",
  "id":967420217,
  "issue":1142,
  "node_id":"IC_kwDODBCWws45qak5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-12T19:47:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Awesome! Thanks, @jpivarski !",
  "created_at":"2021-11-15T09:33:12Z",
  "id":968701401,
  "issue":1142,
  "node_id":"IC_kwDODBCWws45vTXZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T09:33:12Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1143](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (b295ed4) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/0b0fb3f1c18b8acf5ab664ffefe513f67c82076b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0b0fb3f) will **increase** coverage by `3.53%`.\n> The diff coverage is `74.20%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/behaviors/string.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2JlaGF2aW9ycy9zdHJpbmcucHk=) | `100.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (+7.69%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (+7.04%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRmb3JtLnB5) | `78.66% <\u00f8> (+15.50%)` | :arrow_up: |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2N1cHkucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pheC5weQ==) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_from\\_json.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pzb24ucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_cupy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19jdXB5LnB5) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_jax.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19qYXgucHk=) | `80.00% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_json.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19qc29uLnB5) | `80.00% <0.00%> (\u00f8)` | |\n| ... and [145 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1143/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-11T23:13:57Z",
  "id":966691133,
  "issue":1143,
  "node_id":"IC_kwDODBCWws45nok9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-11T23:41:06Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"A note reminding me to write more of these documentation chapters is on the sidebar of my email everyday. `:)` At the moment, finishing up version 2.0 is on a high priority, and all of the documentation will need to be revisited when that's done, since there will be tiny differences in interface and we want to be sure that it's not misleading. As a global optimization, it will be a little better to write more of that documentation after the 2.0 release.\r\n\r\nIn the reference documentation (which is complete), under `ak.Array` \u2192 `__getitem__`, there's a detailed explanation of filtering by boolean arrays. I hope that helps you get started.",
  "created_at":"2021-11-12T13:23:20Z",
  "id":967115252,
  "issue":1144,
  "node_id":"IC_kwDODBCWws45pQH0",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-12T13:23:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1145](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (024404f) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **decrease** coverage by `0.00%`.\n> The diff coverage is `70.61%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `76.92% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [45 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1145/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-12T11:44:35Z",
  "id":967038834,
  "issue":1145,
  "node_id":"IC_kwDODBCWws45o9dy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-14T13:45:01Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1146](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (7dd2b12) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `0.17%`.\n> The diff coverage is `81.57%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [.../awkward/\\_v2/operations/convert/ak\\_from\\_buffers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2J1ZmZlcnMucHk=) | `85.85% <50.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/types/numpytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL251bXB5dHlwZS5weQ==) | `88.23% <81.81%> (-3.57%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/numpyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL251bXB5YXJyYXkucHk=) | `89.06% <100.00%> (+0.33%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/numpyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL251bXB5Zm9ybS5weQ==) | `87.12% <100.00%> (+0.46%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/bytemaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2J5dGVtYXNrZWRhcnJheS5weQ==) | `76.94% <0.00%> (+0.04%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/content.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2NvbnRlbnQucHk=) | `80.24% <0.00%> (+0.10%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `70.12% <0.00%> (+0.12%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedoptionarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRvcHRpb25hcnJheS5weQ==) | `83.04% <0.00%> (+0.16%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/listoffsetarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2xpc3RvZmZzZXRhcnJheS5weQ==) | `77.61% <0.00%> (+0.18%)` | :arrow_up: |\n| ... and [10 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1146/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-12T16:43:57Z",
  "id":967258096,
  "issue":1146,
  "node_id":"IC_kwDODBCWws45py_w",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-12T16:43:57Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1147](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9b9433c) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `0.11%`.\n> The diff coverage is `72.32%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `76.92% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [48 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1147/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-14T10:50:17Z",
  "id":968266707,
  "issue":1147,
  "node_id":"IC_kwDODBCWws45tpPT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-14T10:50:17Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1148](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (235476f) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `1.07%`.\n> The diff coverage is `76.80%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `78.84% <\u00f8> (+1.92%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [67 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1148/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-14T22:40:33Z",
  "id":968377161,
  "issue":1148,
  "node_id":"IC_kwDODBCWws45uENJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T16:16:49Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1149](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0fb8452) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `0.35%`.\n> The diff coverage is `77.61%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `76.92% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [50 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1149/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-15T10:56:54Z",
  "id":968772975,
  "issue":1149,
  "node_id":"IC_kwDODBCWws45vk1v",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T16:13:46Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I think, `datetime/timedelta` is done. Please, have a look. Thanks!\r\n\r\nUnrelated to this PR: I'm not sure about is the reducers default axis. Should it be `-1` or `None`?",
  "created_at":"2021-11-15T13:19:09Z",
  "id":968903028,
  "issue":1149,
  "node_id":"IC_kwDODBCWws45wEl0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T13:19:09Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Unrelated to this PR: I'm not sure about is the reducers default axis. Should it be `-1` or `None`?\r\n\r\nUnfortunately, it needs to be `None`. We'd like it to be `-1` because that's our most common use-case, but they override NumPy functions and those NumPy functions have a default of `None`:\r\n\r\n```python\r\n>>> np.sum([[1, 2], [3, 4]])\r\n10\r\n```",
  "created_at":"2021-11-15T13:38:41Z",
  "id":968919587,
  "issue":1149,
  "node_id":"IC_kwDODBCWws45wIoj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T13:38:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"When I get back in half an hour, I can point out which tests those are. This PR can be merged, if it's not going to cover them.",
  "created_at":"2021-11-15T13:43:35Z",
  "id":968923345,
  "issue":1149,
  "node_id":"IC_kwDODBCWws45wJjR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T13:43:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This PR looks fine\u2014removing `NumpyArray` wrappers from intermediate arrays is conceptually simpler.\r\n> \r\n> This doesn't add any tests, though. The PR I took over last Friday had some un-activated tests, and I thought you were planning on activating them (uncommenting, I think\u2014either that or removing `pytest.mark.skip`) and making them work. When I took over the PR, I did not do that. I don't want them to slip through the cracks!\r\n\r\nYes, three tests have been enabled and the other three need more work. \r\n\r\n1. Awkward 1.0 converted the following strings to `datetime`:\r\n\r\n```python\r\narray0 = ak._v2.contents.NumpyArray(\r\n            [\"2019-09-02T09:30:00\", \"2019-09-13T09:30:00\", \"2019-09-21T20:00:00\"]\r\n        )\r\n``` \r\nwhile Awkward 2.0 treats them as an unsupported dtype: `dtype('<U19')`\r\n\r\n2. The following Array's layout is a ListOffsetArray of a UnionArray and `_completely_flatten` does not handle it:\r\n```python\r\n    array = ak._v2.highlevel.Array(\r\n        [\r\n            [\r\n                np.datetime64(\"2020-03-27T10:41:11\"),\r\n                np.datetime64(\"2020-01-27T10:41:11\"),\r\n                np.datetime64(\"2020-05\"),\r\n                np.datetime64(\"2020-01-27T10:41:11\"),\r\n                np.datetime64(\"2020-04-27T10:41:11\"),\r\n            ],\r\n            [\r\n                np.datetime64(\"2020-04-27\"),\r\n                np.datetime64(\"2020-02-27T10:41:11\"),\r\n                np.datetime64(\"2020-01-27T10:41:11\"),\r\n                np.datetime64(\"2020-06-27T10:41:11\"),\r\n            ],\r\n            [\r\n                np.datetime64(\"2020-02-27T10:41:11\"),\r\n                np.datetime64(\"2020-03-27T10:41:11\"),\r\n                np.datetime64(\"2020-01-27T10:41:11\"),\r\n            ],\r\n        ]\r\n    )\r\n```\r\n```python\r\n>>> array.layout\r\n<ListOffsetArray len='3'>\r\n    <offsets><Index dtype='int64' len='4'>[ 0  5  9 12]</Index></offsets>\r\n    <content><UnionArray len='12'>\r\n        <tags><Index dtype='int8' len='12'>[0 0 1 0 0 2 0 0 0 0 0 0]</Index></tags>\r\n        <index><Index dtype='int64' len='12'>[0 1 0 2 3 0 4 5 6 7 8 9]</Index></index>\r\n        <content index='0'>\r\n            <NumpyArray dtype='datetime64[s]' len='10'>\r\n                ['2020-03-27T10:41:11' '2020-01-27T10:41:11'\r\n                 '2020-01-27T10:41:11' '2020-04-27T10:41:11'\r\n                 '2020-02-27T10:41:11' '2020-01-27T10:41:11'\r\n                 '2020-06-27T10:41:11' '2020-02-27T10:41:11'\r\n                 '2020-03-27T10:41:11' '2020-01-27T10:41:11']\r\n            </NumpyArray>\r\n        </content>\r\n        <content index='1'>\r\n            <NumpyArray dtype='datetime64[M]' len='1'>['2020-05']</NumpyArray>\r\n        </content>\r\n        <content index='2'>\r\n            <NumpyArray dtype='datetime64[D]' len='1'>['2020-04-27']</NumpyArray>\r\n        </content>\r\n    </UnionArray></content>\r\n</ListOffsetArray>\r\n>>> \r\n```\r\n\r\n3. a test that uses `ak.to_numpy`",
  "created_at":"2021-11-15T14:38:51Z",
  "id":968974736,
  "issue":1149,
  "node_id":"IC_kwDODBCWws45wWGQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T14:41:24Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> 1. Awkward 1.0 converted the following strings to `datetime`:\r\n> \r\n> ```python\r\n> array0 = ak._v2.contents.NumpyArray(\r\n>             [\"2019-09-02T09:30:00\", \"2019-09-13T09:30:00\", \"2019-09-21T20:00:00\"]\r\n>         )\r\n> ```\r\n> \r\n> while Awkward 2.0 treats them as an unsupported dtype: `dtype('<U19')`\r\n\r\nThe connection to NumPy is more direct now\u2014we no longer have to do the `.asint64().view(np.datetime64)` dance because NumpyArray's `data` does not go through the Python buffer protocol (which lacks datetimes/timedeltas). If satisfying old tests would mean putting in extra rules to modify NumPy's behavior, then we should change the old tests.\r\n\r\nIn this case, NumPy interprets the data as strings:\r\n\r\n```python\r\n>>> np.array([\"2019-09-02T09:30:00\", \"2019-09-13T09:30:00\", \"2019-09-21T20:00:00\"])\r\narray(['2019-09-02T09:30:00', '2019-09-13T09:30:00',\r\n       '2019-09-21T20:00:00'], dtype='<U19')\r\n\r\n>>> np.asarray([\"2019-09-02T09:30:00\", \"2019-09-13T09:30:00\", \"2019-09-21T20:00:00\"])\r\narray(['2019-09-02T09:30:00', '2019-09-13T09:30:00',\r\n       '2019-09-21T20:00:00'], dtype='<U19')\r\n```\r\n\r\nAnd it should, because making the output type depend on the formatting of the strings would make it less predictable. (Suppose someone didn't know about ISO time formatting and they just wanted strings that look like this.) Even Pandas won't interpret strings as dates unless a specific option is passed (`parse_dates`).\r\n\r\nSo this old test was wrong: it has to be given a NumPy array with datetime dtype.\r\n\r\nEven Python datetime objects don't work:\r\n\r\n```python\r\n>>> from datetime import datetime\r\n>>> np.array([datetime.now(), datetime.now(), datetime.now()])\r\narray([datetime.datetime(2021, 11, 15, 9, 0, 13, 584111),\r\n       datetime.datetime(2021, 11, 15, 9, 0, 13, 584115),\r\n       datetime.datetime(2021, 11, 15, 9, 0, 13, 584116)], dtype=object)\r\n```\r\n\r\nWe won't (ever) support `dtype=object`. The NumpyArray class is relatively low-level, it doesn't have to have a nice, convenient interface. It would be more reasonable for ak.Array to recognize datetime objects, but a nicety like that doesn't need to be implemented right away.\r\n\r\n> 1. The following Array's layout is a ListOffsetArray of a UnionArray and `_completely_flatten` does not handle it:\r\n> \r\n> ```python\r\n>     array = ak._v2.highlevel.Array(\r\n>         [\r\n>             [\r\n>                 np.datetime64(\"2020-03-27T10:41:11\"),\r\n>                 np.datetime64(\"2020-01-27T10:41:11\"),\r\n>                 np.datetime64(\"2020-05\"),\r\n>                 np.datetime64(\"2020-01-27T10:41:11\"),\r\n>                 np.datetime64(\"2020-04-27T10:41:11\"),\r\n>             ],\r\n>             [\r\n>                 np.datetime64(\"2020-04-27\"),\r\n>                 np.datetime64(\"2020-02-27T10:41:11\"),\r\n>                 np.datetime64(\"2020-01-27T10:41:11\"),\r\n>                 np.datetime64(\"2020-06-27T10:41:11\"),\r\n>             ],\r\n>             [\r\n>                 np.datetime64(\"2020-02-27T10:41:11\"),\r\n>                 np.datetime64(\"2020-03-27T10:41:11\"),\r\n>                 np.datetime64(\"2020-01-27T10:41:11\"),\r\n>             ],\r\n>         ]\r\n>     )\r\n> ```\r\n\r\nThis is the first test of `completely_flatten` for UnionArray. Here's what was missing:\r\n\r\n```diff\r\n--- a/src/awkward/_v2/contents/unionarray.py\r\n+++ b/src/awkward/_v2/contents/unionarray.py\r\n@@ -921,7 +921,7 @@ class UnionArray(Content):\r\n     def _completely_flatten(self, nplike, options):\r\n         out = []\r\n         for content in self._contents:\r\n-            out.extend(content[: self._length]._completely_flatten(nplike, options))\r\n+            out.extend(content[: len(self._tags)]._completely_flatten(nplike, options))\r\n         return out\r\n \r\n     def _recursively_apply(\r\n```\r\n\r\nThen you can\r\n\r\n```python\r\n>>> array.layout.completely_flatten()\r\narray(['2020-03-27T10:41:11', '2020-01-27T10:41:11',\r\n       '2020-01-27T10:41:11', '2020-04-27T10:41:11',\r\n       '2020-02-27T10:41:11', '2020-01-27T10:41:11',\r\n       '2020-06-27T10:41:11', '2020-02-27T10:41:11',\r\n       '2020-03-27T10:41:11', '2020-01-27T10:41:11',\r\n       '2020-05-01T00:00:00', '2020-04-27T00:00:00'],\r\n      dtype='datetime64[s]')\r\n```\r\n\r\nand work with the single NumPy array that results.\r\n\r\n> 1. a test that uses `ak.to_numpy`\r\n\r\n`ak._v2.operations.convert.to_numpy` hasn't been implemented (refactored) yet, but there have been a few places where I've wanted it. Getting this updated would be great if there's enough context to see how to do that.\r\n\r\nI don't think `to_numpy` should use `recursively_apply` because it does something unique at every node level. (It's not like these functions that descend until they get to an `axis` and then wrap up the result into the original structure: `to_numpy` turns every level into one NumPy array.) The current implementation might have a long `if`-`elif`-`elif` chain. That would be better as methods on all the Content subclasses. (Let Python's dispatch do the \"else if isinstance\".) This is similar to the `to_arrow` refactoring\u2014it used to be an `if`-`elif`-`elif` chain, but now it's a method on all the Content subclasses. `from_arrow` and `from_numpy` aren't, since they're going in the opposite direction.",
  "created_at":"2021-11-15T15:13:19Z",
  "id":969008961,
  "issue":1149,
  "node_id":"IC_kwDODBCWws45wedB",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-15T15:13:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - thanks! I've enabled all tests and started on `to_numpy` here - or should it be a separate PR?",
  "created_at":"2021-11-15T15:47:58Z",
  "id":969043617,
  "issue":1149,
  "node_id":"IC_kwDODBCWws45wm6h",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T15:47:58Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - thanks! I've enabled all tests and started on `to_numpy` here - or should it be a separate PR?\r\n\r\nIf it's easy enough to separate into another PR, then let's do it. (The only reason I sometimes don't is because there's so much to do and waiting for a prerequisite to finish testing and merge before starting the next would slow down development. This may be the end of the day for you; if you're planning to start working on `to_numpy` tomorrow, then a new PR makes a lot of sense.)\r\n\r\nIs this one ready to merge as-is?",
  "created_at":"2021-11-15T15:53:37Z",
  "id":969049164,
  "issue":1149,
  "node_id":"IC_kwDODBCWws45woRM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T15:53:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > @jpivarski - thanks! I've enabled all tests and started on `to_numpy` here - or should it be a separate PR?\r\n> \r\n> If it's easy enough to separate into another PR, then let's do it. (The only reason I sometimes don't is because there's so much to do and waiting for a prerequisite to finish testing and merge before starting the next would slow down development. This may be the end of the day for you; if you're planning to start working on `to_numpy` tomorrow, then a new PR makes a lot of sense.)\r\n> \r\n> Is this one ready to merge as-is?\r\n\r\nYes, I think so. I'll open a new PR tomorrow with `to_numpy` as I'd like to add more tests.",
  "created_at":"2021-11-15T15:57:09Z",
  "id":969052743,
  "issue":1149,
  "node_id":"IC_kwDODBCWws45wpJH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T15:57:09Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1150](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (c1d7c15) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `1.04%`.\n> The diff coverage is `78.66%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `78.84% <\u00f8> (+1.92%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [63 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1150/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-16T07:52:42Z",
  "id":969971889,
  "issue":1150,
  "node_id":"IC_kwDODBCWws450Jix",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-17T18:06:22Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - nearly done. There are two issues to fix and I'll work on them tomorrow.\r\n\r\nThe first is that validity check that expects `dtype == uint8`. This is in `tests/v2/test_0089-numpy-functions.py:345: ValueError: at layout (\"<class 'awkward._v2.contents.listoffsetarray.ListOffsetArray'>\"): __array__ = \"char\" requires dtype == uint8`\r\n\r\nThe second is a `ValueError: ak.size is ambiguous due to variable-length arrays at axis 2 (try ak.flatten to remove structure or ak.to_numpy to force regularity, if possible)` in `tests/v2/test_0089-numpy-functions.py:345`",
  "created_at":"2021-11-16T17:27:26Z",
  "id":970498098,
  "issue":1150,
  "node_id":"IC_kwDODBCWws452KAy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T17:27:26Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"We should drop `ak.size`. It was only added to support Pandas, and that's no longer a goal (#350). It makes no sense when arrays can be non-rectilinear; for that, you need all the information in `ak.num`. Such a test can probably be dropped as well.",
  "created_at":"2021-11-16T17:34:14Z",
  "id":970504059,
  "issue":1150,
  "node_id":"IC_kwDODBCWws452Ld7",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-16T17:34:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - please, review. I've enabled most of the tests. I've partially enabled the code for `Char` and `Byte` behaviour, however, these functions can be copied to `to_numpy` directly if nothing else is using it.",
  "created_at":"2021-11-17T11:10:41Z",
  "id":971473437,
  "issue":1150,
  "node_id":"IC_kwDODBCWws4554Id",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-17T11:10:41Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"`to_list` might also fix the failing tests:\r\n\r\n```\r\nE       AssertionError: assert False\r\nE        +  where False = <function array_equal at 0x7fcf985b1160>(array(['[111, 110, 101]', '[116, 119, 111]', '[116, 104, 114, 101, 101]'],\\n      dtype='<U25'), array(['one', 'two', 'three'], dtype='<U5'))\r\n```\r\n\r\nSomewhere, it's converting \"[111, 110, 101]\" (which is `[ord(x) for x in \"one\"]`) into a string, `'[111, 110, 101]'`, instead of just `\"one\"`. I don't know why that is because I don't see that error when I run the tests locally, but switching to `to_list` would change the code path, at least.",
  "created_at":"2021-11-17T18:03:55Z",
  "id":971826417,
  "issue":1150,
  "node_id":"IC_kwDODBCWws457OTx",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-17T18:03:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Looking at [the source code](https://github.com/scikit-hep/awkward-1.0/blob/06dfff9deed9d072b7353ab45357e99c4edd51e5/src/awkward/operations/structure.py#L1747) of `ak.where`, we are passing in layouts to `ak.behavior` (which [expects `Array` objects](https://github.com/scikit-hep/awkward-1.0/blob/06dfff9deed9d072b7353ab45357e99c4edd51e5/src/awkward/_util.py#L488)).",
  "created_at":"2021-11-16T17:58:36Z",
  "id":970523755,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452QRr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T18:03:06Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Yup, thanks! I'm fixing that in a branch, and I'm also seeing if I can make assignment to `ak.Array.behavior` change the array class, as that's what @lauridsj was expecting (and only got when he took a trivial slice, `arr[:]`, which went through the `ak.Array` constructor, which _does_ change the array class).",
  "created_at":"2021-11-16T18:02:26Z",
  "id":970526950,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452RDm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T18:02:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Ah, sorry \u2014 I posted this whilst moving between devices, and it was a partial reply!\r\n\r\nI also noticed that we do this in a couple of other routines in `structure.py` - would you like me to make a list of them?",
  "created_at":"2021-11-16T18:03:58Z",
  "id":970528058,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452RU6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T18:03:58Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> would you like me to make a list of them?\r\n\r\nYes, please! Getting these consistent would be a big deal. I'm also copying all of these corrections into v2 so that they're not lost in the upgrade.",
  "created_at":"2021-11-16T18:05:15Z",
  "id":970529024,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452RkA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T18:05:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #1153 fixes `ak.where`'s behavior-setting and also adds this:\r\n\r\n```python\r\n>>> v1 = ak.Array({\r\n...         \"x\": np.random.normal(size=N), \"y\": np.random.normal(size=N), \"z\": np.random.normal(size=N),\r\n...         \"t\": np.random.normal(size=N)}, with_name=\"LorentzVector\")\r\n>>> type(v1)\r\n<class 'awkward.highlevel.Array'>\r\n>>> v1.behavior is None\r\nTrue\r\n>>> v1.behavior = vector.behavior\r\n>>> type(v1)\r\n<class 'coffea.nanoevents.methods.vector.LorentzVectorArray'>\r\n>>> v1.behavior is None\r\nFalse\r\n```\r\n\r\nSetting the behavior reevaluates what the class ought to be, so that new methods (like `.mass`) become available immediately after that assignment.\r\n\r\nIf you give me a list of other functions to fix, I can do that\u2014or you can push directly to the PR branch. Just coordinate with me so that we know when to merge it.",
  "created_at":"2021-11-16T18:13:59Z",
  "id":970536001,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452TRB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T18:13:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"With the latest commit, 8528b25f9611d38912c4b64a1874813151a50e8d, you can also unset it:\r\n\r\n```python\r\n>>> v1 = ak.Array({\r\n...         \"x\": np.random.normal(size=N), \"y\": np.random.normal(size=N), \"z\": np.random.normal(size=N),\r\n...         \"t\": np.random.normal(size=N)}, with_name=\"LorentzVector\")\r\n>>> type(v1)\r\n<class 'awkward.highlevel.Array'>\r\n>>> v1.behavior = vector.behavior\r\n>>> type(v1)\r\n<class 'coffea.nanoevents.methods.vector.LorentzVectorArray'>\r\n>>> v1.behavior = None\r\n>>> type(v1)\r\n<class 'awkward.highlevel.Array'>\r\n```\r\n\r\n(and if different behaviors are set, each mapping to a different class, it won't get stuck on the first one).\r\n",
  "created_at":"2021-11-16T18:22:12Z",
  "id":970544082,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452VPS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T18:22:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"We also seem to pass layouts in `src/awkward/operations/structure.py`:\r\n* L1747 `concatenate`\r\n* L4420 `isclose`\r\n\r\nWIll check the other files.",
  "created_at":"2021-11-16T18:36:01Z",
  "id":970557683,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452Yjz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T18:36:24Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"`ak.concatenate` is already using the high-level `arrays` from the argument list, not the `contents` derived from `to_layout`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/06dfff9deed9d072b7353ab45357e99c4edd51e5/src/awkward/operations/structure.py#L1639-L1650\r\n\r\nBut for `ak.isclose`, you're right: `a` and `b` are the high-level arrays, while `one` and `two` are the layouts:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/06dfff9deed9d072b7353ab45357e99c4edd51e5/src/awkward/operations/structure.py#L4420\r\n\r\nFixed in daf4fbfc93ef75dc9484489b055524e8b734d5fd.",
  "created_at":"2021-11-16T18:59:20Z",
  "id":970576885,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452dP1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T18:59:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If you don't see any more, we can merge the PR.",
  "created_at":"2021-11-16T18:59:45Z",
  "id":970577194,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452dUq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T18:59:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I did another search and I couldn't see anything. Maybe we should add some tests to catch this for all the high level ops?\r\n",
  "created_at":"2021-11-16T20:29:12Z",
  "id":970653119,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452v2_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T20:29:50Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Perhaps, although I'd be satisfied by merging the PR now. A suite of tests would have to construct non-erroneous examples for each function, and that would take quite a bit of time.",
  "created_at":"2021-11-16T20:41:11Z",
  "id":970661379,
  "issue":1152,
  "node_id":"IC_kwDODBCWws452x4D",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2021-11-16T20:41:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks a lot for this fix - this was very quick!",
  "created_at":"2021-11-17T15:04:40Z",
  "id":971667843,
  "issue":1152,
  "node_id":"IC_kwDODBCWws456nmD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-17T15:04:40Z",
  "user":"MDQ6VXNlcjk5OTE2MzU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1153](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (daf4fbf) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `0.34%`.\n> The diff coverage is `77.83%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `76.92% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [52 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1153/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-16T18:17:13Z",
  "id":970538470,
  "issue":1153,
  "node_id":"IC_kwDODBCWws452T3m",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T19:43:57Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1154](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (52bdf9b) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `0.52%`.\n> The diff coverage is `78.15%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `76.92% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [55 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1154/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-16T19:05:45Z",
  "id":970581619,
  "issue":1154,
  "node_id":"IC_kwDODBCWws452eZz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T23:17:42Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1155](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (bce33ab) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `1.53%`.\n> The diff coverage is `83.23%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_reducers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19yZWR1Y2Vycy5weQ==) | `96.48% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `56.27% <51.72%> (+0.82%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `59.75% <63.23%> (+0.44%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.19% <63.58%> (-0.36%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `69.54% <74.19%> (+0.26%)` | :arrow_up: |\n| ... and [31 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1155/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-17T11:41:20Z",
  "id":971495580,
  "issue":1155,
  "node_id":"IC_kwDODBCWws4559ic",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-21T08:12:14Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Just for the record - as of the last commit: a reference C++ test is the same as in https://github.com/scikit-hep/awkward-1.0/pull/690 (without a snapshot call). It takes `235.00 ms `compared with `16.97 s` reported previously.\r\n<img width=\"1430\" alt=\"Screenshot 2021-12-02 at 17 45 33\" src=\"https://user-images.githubusercontent.com/1390682/144467546-803e25b5-162e-441d-b5d6-a06010987f70.png\">\r\n",
  "created_at":"2021-12-02T17:02:05Z",
  "id":984819045,
  "issue":1155,
  "node_id":"IC_kwDODBCWws46syVl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T17:02:05Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Just for the record - as of the last commit: a reference C++ test is the same as in #690 (without a snapshot call). It takes `235.00 ms `compared with `16.97 s` reported previously.\r\n\r\nWait, what? Replacing `std::shared_ptr` with `std::unique_ptr` is a 72\u00d7 speedup? On what test?\r\n\r\nI was remembering something much smaller, like a 50% improvement or something like that.",
  "created_at":"2021-12-02T19:56:36Z",
  "id":984954659,
  "issue":1155,
  "node_id":"IC_kwDODBCWws46tTcj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T19:56:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> > Just for the record - as of the last commit: a reference C++ test is the same as in #690 (without a snapshot call). It takes `235.00 ms `compared with `16.97 s` reported previously.\r\n> \r\n> Wait, what? Replacing `std::shared_ptr` with `std::unique_ptr` is a 72\u00d7 speedup? On what test?\r\n> \r\n> I was remembering something much smaller, like a 50% improvement or something like that.\r\n\r\nThere were two changes so far: removing temporary objects and moving shared pointers instead of copying them. The `std::shared_ptr` reference count is atomic, moving it is not and is hundred times faster than copying it. Besides, we were incrementing/decrementing the same pointer. I will need to run more tests and do further cleanup, but it looks like we may not need to go much further in eliminating the shared pointers :-)\r\n\r\nHere is the test I used:\r\n```c++\r\n  ak::ArrayBuilder myarray(ak::ArrayBuilderOptions(1024, 2.0));\r\n\r\n  for (int64_t i = 0; i < 10000000; i++) {\r\n\r\n    // populate builder with lists\r\n    myarray.beginrecord();\r\n    myarray.field_check(\"one\");\r\n    myarray.boolean(true);\r\n    myarray.field_check(\"two\");\r\n    myarray.integer(1);\r\n    myarray.field_check(\"three\");\r\n    myarray.real(1.1);\r\n    myarray.endrecord();\r\n\r\n    myarray.beginrecord();\r\n    myarray.field_check(\"one\");\r\n    myarray.boolean(false);\r\n    myarray.field_check(\"two\");\r\n    myarray.integer(2);\r\n    myarray.field_check(\"three\");\r\n    myarray.real(2.2);\r\n    myarray.endrecord();\r\n\r\n    myarray.beginrecord();\r\n    myarray.field_check(\"one\");\r\n    myarray.boolean(true);\r\n    myarray.field_check(\"two\");\r\n    myarray.integer(3);\r\n    myarray.field_check(\"three\");\r\n    myarray.real(3.3);\r\n    myarray.endrecord();\r\n  }\r\n```",
  "created_at":"2021-12-02T21:50:41Z",
  "id":985029163,
  "issue":1155,
  "node_id":"IC_kwDODBCWws46tlor",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T21:50:41Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"That's jaw-dropping.\r\n\r\nI knew there was a difference, but if I had known it was such a big difference, we would have done this much earlier. Fortunately, though, there will be a version 1.7.0 out soon that we can use as a baseline for studies. If that was the bottleneck (and it's so large that it must be), then my specialized JSON thing (#1165) is probably unnecessary\u2014discovering the type dynamically will probably be as fast as knowing the type ahead of time (because I wrongly thought that it was the type-querying that was slowing it down, rather than the `std::shared_ptr`). We'll see.\r\n\r\nFor reasons of keeping things module, the ForthOutputBuffer doesn't use ArrayBuilder's GrowableBuffer. It has its own `std::shared_ptr<void>` pointers:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ae58a75a9aa5cb7106d58a22ac7bd504657a74d2/include/awkward/forth/ForthOutputBuffer.h#L352\r\n\r\nand the resize update is here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/ae58a75a9aa5cb7106d58a22ac7bd504657a74d2/src/libawkward/forth/ForthOutputBuffer.cpp#L748-L749\r\n\r\nThe LayoutBuilder, AwkwardForth, and SpecializedJSON won't see speedups until ForthOutputBuffer is updated in the same way as GrowableBuffer.\r\n\r\nActually, in this PR's diff, I don't see any changes to GrowableBuffer's `std::shared_ptr<void>`. What you've updated is the `BuilderPtr`, which is definitely related to the type-discovery overhead (an unnecessary overhead, as it turns out), not the fact that snapshots share data with builders. Wasn't there also a gain to be had from replacing the pointers to the data themselves with `std::unique_ptr`? That's why we have to change the snapshot logic to copy, rather than view\u2014to allow the builder's buffers to be unshared pointers. Was I wrong in thinking that that was important? (That's what I thought was the ~50% gain, from a study you did about a year ago.)",
  "created_at":"2021-12-02T22:11:11Z",
  "id":985042644,
  "issue":1155,
  "node_id":"IC_kwDODBCWws46to7U",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T22:11:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think, it a bit premature to draw conclusions. The next commit drops shared pointer copies for the `field_check` and its relative execution drops from 27.2% to 7.3% ",
  "created_at":"2021-12-03T11:23:30Z",
  "id":985440340,
  "issue":1155,
  "node_id":"IC_kwDODBCWws46vKBU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-03T11:23:30Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - it looks like the failure is unrelated to this PR - the test fails in a clean area:\r\n```bash\r\n======================================================================== FAILURES ========================================================================\r\n_________________________________________________________________ test_numpyarray_grad_3 _________________________________________________________________\r\n\r\n    def test_numpyarray_grad_3():\r\n        def func_numpyarray_3(x):\r\n            return x[::-1]\r\n    \r\n        value_jvp, jvp_grad = jax.jvp(\r\n            func_numpyarray_3, (test_numpyarray_jax,), (test_numpyarray_tangent_jax,)\r\n        )\r\n        jit_value = jax.jit(func_numpyarray_3)(test_numpyarray)\r\n        value_vjp, vjp_func = jax.vjp(func_numpyarray_3, test_numpyarray_jax)\r\n    \r\n>       assert ak.to_list(value_jvp) == [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\r\n\r\ntests/test_0793-jax-element-wise-ops.py:73: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nawkward/operations/convert.py:1020: in to_list\r\n    return [to_list(x) for x in array]\r\nawkward/operations/convert.py:1020: in <listcomp>\r\n    return [to_list(x) for x in array]\r\nawkward/operations/convert.py:1020: in to_list\r\n    return [to_list(x) for x in array]\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = DeviceArray(9., dtype=float64)\r\n\r\n    def __iter__(self):\r\n      if self.ndim == 0:\r\n>       raise TypeError(\"iteration over a 0-d array\")  # same as numpy error\r\nE       TypeError: iteration over a 0-d array\r\n\r\n../../../anaconda3/lib/python3.8/site-packages/jax/_src/device_array.py:245: TypeError\r\n============================================================== 1 failed, 17 passed in 4.05s ==============================================================\r\n\r\n```",
  "created_at":"2021-12-09T14:05:37Z",
  "id":989883286,
  "issue":1155,
  "node_id":"IC_kwDODBCWws47AGuW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-09T14:05:37Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski - it looks like the failure is unrelated to this PR\r\n\r\nYes, I think something happened in a new JAX version. @ctrl-stormy will be replacing the JAX implementation next spring\u2014it will be based on a different strategy (bottom-up, rather than top-down)\u2014so I think we can disable these tests. I've already done that in #1183, though I didn't get it cleanly in any one commit.\r\n\r\nIf you add\r\n\r\n```python\r\npytestmark = pytest.mark.skip(\r\n    reason=\"Top-down JAX tests disabled; to be replaced by bottom-up.\"\r\n)\r\n```\r\n\r\nto tests/test_0645-from-jax.py, tests/test_0645-jax-refcount.py, tests/test_0645-to-jax.py, and tests/test_0793-jax-element-wise-ops.py, immediately after the imports and before the `jax = ...` lines (as in 5b0a273e807a8b25ba8b02d2651af28f267823d7), then it will merge cleanly with the modification I made to do the same.\r\n\r\nI don't know if these tests will be reinstated as-is when the JAX bottom-up implementation is done or if that implementation will have different tests, but this will at least turn them off for now without losing memory that they're a to-do item. Before releasing Awkward 2.0, we'll sweep through all the skipped tests, reinstating as many as we can, and we'll be reminded of this one at that time.",
  "created_at":"2021-12-09T14:37:12Z",
  "id":989911089,
  "issue":1155,
  "node_id":"IC_kwDODBCWws47ANgx",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-12-09T14:37:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"As it is now, `GrowableBuffer` uses `unique_ptr` instead of `shared_ptr`. `Builder`s return `nullptr` instead of `shared_from_this` in some cases - the overall time went down from `17.25 s` to <strike>`10.91 s`</strike>`10.46 s` (in the next commit), but the needed extra check did introduce some cost and I'm working on it.\r\n\r\nThe test run in a clean area on the left, the same test with this PR on the right.\r\n \r\n<img width=\"1775\" alt=\"Screenshot 2021-12-10 at 16 34 36\" src=\"https://user-images.githubusercontent.com/1390682/145599877-e60a86b5-ac01-4549-bb1d-7328aea80cb8.png\">\r\n",
  "created_at":"2021-12-10T15:24:36Z",
  "id":991062635,
  "issue":1155,
  "node_id":"IC_kwDODBCWws47Empr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-10T15:35:37Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, it's significant (factor of 2), but not gigantic (factor of 10). In that case, it's still valuable to write specialized readers for various formats (like #1165: in its sample task, this `shared_ptr` \u2192 `unique_ptr` improvement would take 40 seconds to 20 seconds (estimated), and the specialized JSON with `shared_ptr` takes it to 10 seconds\u2014removing `shared_ptr` there might not make as much of a difference because I don't think we're incrementing/decrementing those pointers (worth trying anyway), and RapidJSON parsing by itself can't get any faster than 6 seconds).\r\n\r\nMore importantly, we know why: the atomic increment/decrement (`shared_ptr`) or null check (`unique_ptr`) on every step is the speedbump, but it's a necessary speedbump because it has to check to see if the tree is changing. With specialized readers, the set of buffers/tree does not change. The specialized JSON reader has to check JSON format at every step to see if it needs to complain about wrong formats (despite the disclaimer, it does some checking), but binary formats don't even do that\u2014they just assume the spec and go. In that case, all we have as a bottleneck is Forth VM overhead, which is minimized by reducing the number of instructions (\"~5 ns/instruction\").",
  "created_at":"2021-12-10T15:46:33Z",
  "id":991080447,
  "issue":1155,
  "node_id":"IC_kwDODBCWws47Eq__",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-12-10T15:46:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hmm... this is strange (MacOS and py310):\r\n```bash\r\n================= 1770 passed, 68 skipped, 1 warning in 53.38s =================\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x000000011185edc0 (most recent call first):\r\n  Garbage-collecting\r\n  <no Python frame>\r\n/Users/runner/work/_temp/3e675f7f-b1be-4603-bf91-708afe92f1cf.sh: line 1: 11033 Segmentation fault: 11  python -m pytest -vv -rs tests\r\n##[error]Bash exited with code '139'.\r\nFinishing: Test\r\n```",
  "created_at":"2021-12-20T15:33:06Z",
  "id":998031045,
  "issue":1155,
  "node_id":"IC_kwDODBCWws47fL7F",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-20T15:33:06Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1156](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (72d8652) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `0.44%`.\n> The diff coverage is `76.66%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `76.92% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [57 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1156/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-17T15:55:14Z",
  "id":971715188,
  "issue":1156,
  "node_id":"IC_kwDODBCWws456zJ0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-17T15:55:14Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - and `kernel-specification.yml` will shrink as well.",
  "created_at":"2021-11-17T16:58:21Z",
  "id":971772458,
  "issue":1157,
  "node_id":"IC_kwDODBCWws457BIq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-17T16:58:21Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1159](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (c821111) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `0.98%`.\n> The diff coverage is `76.33%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `78.84% <\u00f8> (+1.92%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [67 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1159/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-18T23:46:18Z",
  "id":973466089,
  "issue":1159,
  "node_id":"IC_kwDODBCWws46Benp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-20T17:03:30Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"After some performance tests, too many to summarize, I've found that these new commands are usually a little faster (15% in most cases, a factor of 2 in some extremes) than their equivalents in `np.fromstring`, RapidJSON + ArrayBuilder (simple structure), and Python's `json` module.\r\n\r\nThey all have scaling dependencies that make sense: for instance Python's `json` module is a little faster than AwkwardForth (25%) for a non-nested list of booleans because it doesn't need to allocate Python objects (True and False are built-ins). `np.fromstring` is exactly as fast as `textint->`, but several times slower for floating-point numbers (`textfloat->` is pay-as-you-go: decimals and exponents cost more, but not as much as in NumPy). RapidJSON also has pay-as-you-go floating point handling, and it might only be slower than the AwkwardForth commands because the test had it coupled in with ArrayBuilder (through `ak._ext.fromjson`).\r\n\r\nAs expected, the cost of `enum` scales with the number of strings to check\u2014specifically, the number of strings before the ones that match. That's because the current algorithm just cycles through all the strings, doing a `strncmp` for each. There were some hints that it depended on the lengths of those failing strings\u2014it shouldn't, because `strncmp` ought to give up on the first non-matching character\u2014but those hints didn't hold up to more extreme cases. `strncmp` behaves as expected. If this is ever used for JSON objects with many fields (I _expect_ it to), then the cycle-through-strings algorithm needs to be replaced with a trie. It might even make a difference to ensure that the string contents are allocated near each other in memory: the hint that went away might be pointer-chasing. But if we go to that much trouble, we might as well implement a trie (compact in memory after the set of strings is fully known).\r\n\r\nThe regular `case` statement (jump-table) costs 11 to 18 ns per invocation (2 cases vs 12 cases). I'd have to think about it, but it might actually be 2 instructions, which fits with the \"5 ns per instruction\" rule of thumb. Irregular `case` statements are as slow as would be expected: 12 cases is 24.6 times slower. (That probably corresponds to an instruction count, too.)\r\n\r\nOn the whole, the value of this is not that it's a faster JSON parser\u2014it is a little bit, but not enough to get excited about\u2014but that we can bypass ArrayBuilder's type agnosticism. For instance, an array of booleans one level deep (ArrayBuilder only needs to step down one level of a tree for each entry), AwkwardForth is only about 2\u00d7 faster than RapidJSON + ArrayBuilder:\r\n\r\n```python\r\n>>> import time\r\n>>> import numpy as np\r\n>>> import awkward as ak\r\n>>> from awkward.forth import ForthMachine32\r\n>>> data = b\"[\" + (br'true,false,' * 100000000)[:-1] + b\"]\"   # shallow!\r\n>>> data2 = {\"x\": np.array([data])}\r\n>>> starttime = time.time(); tmp = ak._ext.fromjson(data); time.time() - starttime\r\n7.373785972595215\r\n>>> starttime = time.time(); tmp = ak._ext.fromjson(data); time.time() - starttime\r\n7.3505895137786865\r\n>>> starttime = time.time(); tmp = ak._ext.fromjson(data); time.time() - starttime\r\n7.420138835906982\r\n>>> vm = ForthMachine32(r'input x output y uint8 100000000 0 do 1 x skip x enum s\" false\" s\" true\" y <- stack loop')\r\n>>> starttime = time.time(); vm.run(data2); time.time() - starttime\r\n3.983327627182007\r\n>>> starttime = time.time(); vm.run(data2); time.time() - starttime\r\n4.056304454803467\r\n>>> starttime = time.time(); vm.run(data2); time.time() - starttime\r\n4.007813453674316\r\n```\r\n\r\nBut just put these same booleans inside 10 levels of list nesting, and now ArrayBuilder has to walk down 10 levels of a tree with each boolean, checking the types of the nodes all the way down. Now AwkwardForth is about 7\u00d7 faster.\r\n\r\n```python\r\n>>> import time\r\n>>> import numpy as np\r\n>>> import awkward as ak\r\n>>> from awkward.forth import ForthMachine32\r\n>>> data = b\"[[[[[[[[[[\" + (br'true,false,' * 100000000)[:-1] + b\"]]]]]]]]]]\"   # deep!\r\n>>> data2 = {\"x\": np.array([data])}\r\n>>> starttime = time.time(); tmp = ak._ext.fromjson(data); time.time() - starttime\r\n30.746235132217407\r\n>>> starttime = time.time(); tmp = ak._ext.fromjson(data); time.time() - starttime\r\n31.118453979492188\r\n>>> starttime = time.time(); tmp = ak._ext.fromjson(data); time.time() - starttime\r\n31.27355456352234\r\n>>> vm = ForthMachine32(r'input x output y uint8 9 x skip 100000000 0 do 1 x skip x enum s\" false\" s\" true\" y <- stack loop')\r\n>>> starttime = time.time(); vm.run(data2); time.time() - starttime\r\n4.135768890380859\r\n>>> starttime = time.time(); vm.run(data2); time.time() - starttime\r\n4.166326284408569\r\n>>> starttime = time.time(); vm.run(data2); time.time() - starttime\r\n4.207798480987549\r\n```\r\n\r\nList-descent is a relatively inexpensive part of ArrayBuilder\u2014finding the right field in a record that can be out of order (JSON object keys can easily be out of order!) is a lot more costly, but harder to set up the test. A JSON-to-Awkward reader leveraging JSON schemas will benefit for reasons of avoiding ArrayBuilder more than just being a slightly faster JSON parser.",
  "created_at":"2021-11-20T22:28:48Z",
  "id":974720473,
  "issue":1159,
  "node_id":"IC_kwDODBCWws46GQ3Z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-20T22:28:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1160](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (b76fed0) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `0.98%`.\n> The diff coverage is `76.33%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `78.84% <\u00f8> (+1.92%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [67 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1160/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-20T22:49:48Z",
  "id":974722469,
  "issue":1160,
  "node_id":"IC_kwDODBCWws46GRWl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-20T22:49:48Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1161](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (3a3c91c) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `1.56%`.\n> The diff coverage is `76.45%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `78.84% <\u00f8> (+1.92%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `81.94% <\u00f8> (+1.38%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [78 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1161/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-22T11:13:08Z",
  "id":975413668,
  "issue":1161,
  "node_id":"IC_kwDODBCWws46I6Gk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T16:36:26Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Thank you very much for fixing that (and finding a few untested parts of _broadcasting.py). The commit I just made was to remove some commented-out code, that's all. It's set to auto-merge when the tests pass again.\r\n\r\nThank you! Great! Happy to have this merged! \r\n",
  "created_at":"2021-11-30T16:25:38Z",
  "id":982799110,
  "issue":1161,
  "node_id":"IC_kwDODBCWws46lFMG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T16:25:38Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Woohoo! Thanks for this \ud83d\ude04 ",
  "created_at":"2021-11-30T17:32:43Z",
  "id":982860888,
  "issue":1161,
  "node_id":"IC_kwDODBCWws46lURY",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-30T17:32:43Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1162](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (609f1f6) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `1.24%`.\n> The diff coverage is `79.90%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `78.84% <\u00f8> (+1.92%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [75 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1162/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-22T17:07:06Z",
  "id":975737389,
  "issue":1162,
  "node_id":"IC_kwDODBCWws46KJIt",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-23T20:11:17Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"Code generated for `test_number`:\r\n\r\n```forth\r\ninput source\r\noutput node0-data float64\r\n\r\nsource skipws\r\nsource enumonly s\" [\" drop\r\nsource skipws\r\n0\r\nsource enum s\" ]\"\r\nbegin\r\nwhile\r\n  source textfloat-> node0-data\r\n  1+\r\n  source skipws\r\n  source enumonly s\" ]\" s\" ,\"\r\n  source skipws\r\nrepeat\r\nsource skipws\r\n```\r\n\r\nCode generator for `test_option_integer`:\r\n\r\n```forth\r\ninput source\r\noutput node0-mask int8\r\noutput node1-data int64\r\n\r\nsource skipws\r\nsource enumonly s\" [\" drop\r\nsource skipws\r\n0\r\nsource enum s\" ]\"\r\nbegin\r\nwhile\r\n  source enum s\" null\" dup if\r\n    source textfloat-> node1-data\r\n  else\r\n    0 node1-data <- stack\r\n  then\r\n  node0-mask <- stack\r\n  1+\r\n  source skipws\r\n  source enumonly s\" ]\" s\" ,\"\r\n  source skipws\r\nrepeat\r\nsource skipws\r\n```\r\n\r\nCode generated for `test_string`:\r\n\r\n```forth\r\ninput source\r\noutput node0-offsets int64\r\noutput node1-data uint8\r\n0 node0-offsets <- stack\r\n\r\nsource skipws\r\nsource enumonly s\" [\" drop\r\nsource skipws\r\n0\r\nsource enum s\" ]\"\r\nbegin\r\nwhile\r\n  source quotedstr-> node1-data\r\n  node0-offsets +<- stack\r\n  1+\r\n  source skipws\r\n  source enumonly s\" ]\" s\" ,\"\r\n  source skipws\r\nrepeat\r\nsource skipws\r\n```\r\n\r\nCode generated for `test_array_integer`:\r\n\r\n```forth\r\ninput source\r\noutput node0-offsets int64\r\noutput node1-data int64\r\n0 node0-offsets <- stack\r\n\r\nsource skipws\r\nsource enumonly s\" [\" drop\r\nsource skipws\r\n0\r\nsource enum s\" ]\"\r\nbegin\r\nwhile\r\n  source enumonly s\" [\" drop\r\n  source skipws\r\n  0\r\n  source enum s\" ]\" \r\n  begin\r\n  while\r\n    source textfloat-> node1-data\r\n    1+\r\n    source skipws\r\n    source enumonly s\" ]\" s\" ,\" \r\n    source skipws\r\n  repeat\r\n  node0-offsets +<- stack\r\n  1+\r\n  source skipws\r\n  source enumonly s\" ]\" s\" ,\"\r\n  source skipws\r\nrepeat\r\nsource skipws\r\n```\r\n\r\nCode generated for `test_array_array_integer`:\r\n\r\n```forth\r\ninput source\r\noutput node0-offsets int64\r\noutput node1-offsets int64\r\noutput node2-data int64\r\n0 node0-offsets <- stack\r\n0 node1-offsets <- stack\r\n\r\nsource skipws\r\nsource enumonly s\" [\" drop\r\nsource skipws\r\n0\r\nsource enum s\" ]\"\r\nbegin\r\nwhile\r\n  source enumonly s\" [\" drop\r\n  source skipws\r\n  0\r\n  source enum s\" ]\" \r\n  begin\r\n  while\r\n    source enumonly s\" [\" drop\r\n    source skipws\r\n    0\r\n    source enum s\" ]\" \r\n    begin\r\n    while\r\n      source textfloat-> node2-data\r\n      1+\r\n      source skipws\r\n      source enumonly s\" ]\" s\" ,\" \r\n      source skipws\r\n    repeat\r\n    node1-offsets +<- stack\r\n    1+\r\n    source skipws\r\n    source enumonly s\" ]\" s\" ,\" \r\n    source skipws\r\n  repeat\r\n  node0-offsets +<- stack\r\n  1+\r\n  source skipws\r\n  source enumonly s\" ]\" s\" ,\"\r\n  source skipws\r\nrepeat\r\nsource skipws\r\n```\r\n\r\nCode generated for `test_option_record`:\r\n\r\n```forth\r\ninput source\r\noutput node0-index int64\r\noutput node1-data int64\r\noutput node2-data float64\r\nvariable node0-count\r\n\r\nsource skipws\r\nsource enumonly s\" [\" drop\r\nsource skipws\r\n0\r\nsource enum s\" ]\"\r\nbegin\r\nwhile\r\n  source enumonly s\" null\" s\" {\" if\r\n    node0-count @ node0-index <- stack\r\n    1 node0-count +!\r\n    source skipws\r\n    source enum s\" }\" \r\n    begin\r\n    while\r\n      source enumonly s\" \\\"x\\\"\" s\" \\\"y\\\"\"\r\n      source skipws\r\n      source enumonly s\" :\" drop \r\n      source skipws\r\n      case\r\n        0 of\r\n          source textfloat-> node1-data\r\n        endof\r\n        1 of\r\n          source textfloat-> node2-data\r\n        endof\r\n      endcase\r\n      source skipws\r\n      source enumonly s\" }\" s\" ,\" \r\n      source skipws\r\n    repeat\r\n  else\r\n    -1 node0-index <- stack\r\n  then\r\n  1+\r\n  source skipws\r\n  source enumonly s\" ]\" s\" ,\"\r\n  source skipws\r\nrepeat\r\nsource skipws\r\n```\r\n\r\nUnlike ArrayBuilder, which has to anticipate that any level of nesting might become a record, all of these option-types are implemented with ByteMaskedArray except for optional records, which are IndexedOptionArray. That's because making a dummy-value record to hide behind a ByteMaskedArray mask would be complicated to do and could be arbitrarily large. Only the IndexedOptionArray needs a variable to keep a global count: everything else is stacky.\r\n\r\nNote that this does not _validate_ the JSON against its schema; it _assumes_ the schema to parse the data more quickly.",
  "created_at":"2021-11-23T01:33:39Z",
  "id":976091069,
  "issue":1162,
  "node_id":"IC_kwDODBCWws46Lfe9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-23T01:33:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Tomorrow I should write a docstring and do some performance tests.\r\n\r\nAfter that, plain old JSON (no schema).",
  "created_at":"2021-11-23T01:37:27Z",
  "id":976092216,
  "issue":1162,
  "node_id":"IC_kwDODBCWws46Lfw4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-23T01:37:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, using a JSONSchema to build a Forth machine often helps a little with performance, maybe 20%, over the untyped case, but not a lot. It depends on the data structure, of course, but there isn't a clear pattern where the specialized code approach shines.\r\n\r\nTaking this to be a standard example:\r\n\r\n```python\r\nMULTIPLIER = int(10e6)\r\njson_string = b\"[\" + b\", \".join([\r\n    b'[{\"x\": 1.1, \"y\": [1]}, {\"x\": 2.2, \"y\": [1, 2]}, {\"x\": 3.3, \"y\": [1, 2, 3]}],' +\r\n    b'[],' +\r\n    b'[{\"x\": 4.4, \"y\": [1, 2, 3, 4]}, {\"x\": 5.5, \"y\": [1, 2, 3, 4, 5]}]'\r\n] * MULTIPLIER) + b\"]\"\r\n```\r\n\r\n(from the one-slide-summary), the schema-specialized Forth code takes 31.5 seconds and the untyped ArrayBuilder takes 38.6 seconds (average of 3 runs; standard deviations of 0.5 seconds). For very simple types, the ArrayBuilder wins, and for complex types, Forth's gains are only at this 20% level. It might not justify maintaining it as a separate code path.\r\n\r\nPulling it apart is revealing: with all ArrayBuilder code removed, just the RapidJSON parsing is 6.0 seconds while the Forth is still 30.8 seconds. RapidJSON is 5\u00d7 faster at parsing JSON than the schema-specialized Forth, and I should have checked the RapidJSON-only figure first because this is consistent with Forth's ~5 ns/instruction rule: in these runs, it was 7.69 ns/instruction.\r\n\r\nAbsorbing the `skipws` (skip whitespace) commands into the subsequent command could shave off 20%, but it's not clear that that's worthwhile.\r\n\r\nWhile AwkwardForth may be a good idea for general binary formats, especially ROOT, it's not great for JSON. Ideally, we'd want to attach the dynamic execution to RapidJSON's own parsing (and therefore not have to compete in that department). Just something to think about.",
  "created_at":"2021-11-23T23:50:33Z",
  "id":977293143,
  "issue":1162,
  "node_id":"IC_kwDODBCWws46QE9X",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-23T23:50:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1163](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (be8ae30) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/f795658559d61f1fd394c828e95608c9de8ee027?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (f795658) will **increase** coverage by `1.07%`.\n> The diff coverage is `76.80%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/forms/bitmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2JpdG1hc2tlZGZvcm0ucHk=) | `75.64% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/bytemaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2J5dGVtYXNrZWRmb3JtLnB5) | `76.05% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/emptyform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2VtcHR5Zm9ybS5weQ==) | `78.84% <\u00f8> (+1.92%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/indexedoptionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2luZGV4ZWRvcHRpb25mb3JtLnB5) | `76.81% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3Rmb3JtLnB5) | `75.94% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/listoffsetform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL2xpc3RvZmZzZXRmb3JtLnB5) | `80.55% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/regularform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlZ3VsYXJmb3JtLnB5) | `75.34% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unionform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VuaW9uZm9ybS5weQ==) | `76.19% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/unmaskedform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3VubWFza2VkZm9ybS5weQ==) | `74.57% <\u00f8> (\u00f8)` | |\n| ... and [67 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1163/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-11-22T19:54:03Z",
  "id":975866966,
  "issue":1163,
  "node_id":"IC_kwDODBCWws46KoxW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T19:54:03Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1164?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1164](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1164?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (279770c) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/e4b664096aa4693f5734fb252598cd9c00b5b671?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (e4b6640) will **decrease** coverage by `0.03%`.\n> The diff coverage is `87.85%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1164?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [...ward/\\_v2/operations/convert/ak\\_from\\_json\\_schema.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1164/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pzb25fc2NoZW1hLnB5) | `85.54% <87.85%> (-0.51%)` | :arrow_down: |\n",
  "created_at":"2021-11-23T14:43:09Z",
  "id":976648713,
  "issue":1164,
  "node_id":"IC_kwDODBCWws46NnoJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T08:53:04Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - I've moved the snapshot() generation out of the json functions. The changes are minimal, however this is causing a problem. Please, have a look at a failing `tests/test_0384-lazy-arrayset.py`.\r\n\r\nIt looks like that the contents order is not preserved thus a generated `form_key` is different.\r\n<img width=\"1228\" alt=\"Screenshot 2021-11-23 at 18 11 36\" src=\"https://user-images.githubusercontent.com/1390682/143071775-d9325b6d-d6ba-4d63-89a1-d4f192daff62.png\">\r\n\r\nnew vs old:\r\n```diff\r\n< [('get', 'kitty-node13-tags'), ('get', 'kitty-node13-index'), ('get', 'kitty-node16-offsets'), ('get', 'kitty-node15-data'), ('get', 'kitty-node2-data'), ('get', 'kitty-node3-data')]\r\n---\r\n> [('get', 'kitty-node11-tags'), ('get', 'kitty-node11-index'), ('get', 'kitty-node14-offsets'), ('get', 'kitty-node13-data'), ('get', 'kitty-node6-data'), ('get', 'kitty-node7-data')]\r\n```",
  "created_at":"2021-11-23T17:22:46Z",
  "id":976882489,
  "issue":1164,
  "node_id":"IC_kwDODBCWws46Ogs5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-23T17:22:46Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"The JSON representation of RecordForms is an object (curly brackets); I wish I had chosen two JSON arrays, but it has to remain backward-compatible. When that passes through Python dicts in Python < 3.6, the order may be lost. However, you're seeing errors in all the Python versions and the stage at which `form_keys` are associated with these Forms is much earlier. This one way in which the order might not be preserved is not what's going on in the latest round of failing tests.\r\n\r\nIn `test_0384-lazy-arrayset.py:test_lazy_buffers`, the JSON has `\"listcollection\"` before `\"collection\"`, so the ArrayBuilder must parse it first and it must be the first key in the RecordBuilder. RecordBuilder, RecordForm (when not serialized), and RecordArray all have a well-defined order for their keys and contents. Of your two screenshots, the one on the left is wrong.\r\n\r\nI can't see what's going wrong here, but there must be an exact point in the process when it goes wrong. Are the records already out of order in the `array` defined on lines 25-65 (e.g. `\"collection\"` is before `\"listcollection\"` in the `array.layout.recordlookup`)? If so, at what point do they get out of order? They ought to all be in order in the RecordBuilder; the only code that has changed is code between the RecordBuilder and the RecordArray. What about the RecordForm that comes between those two? (As opposed to the various Forms that the later `ak.from_buffers` call makes on line 76.)",
  "created_at":"2021-11-23T18:05:40Z",
  "id":976952436,
  "issue":1164,
  "node_id":"IC_kwDODBCWws46Oxx0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-23T18:05:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, mode debugging is needed... I'm on it.",
  "created_at":"2021-11-23T18:14:43Z",
  "id":976964033,
  "issue":1164,
  "node_id":"IC_kwDODBCWws46O0nB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-23T18:14:43Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1165?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1165](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1165?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (036e9a3) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/e4b664096aa4693f5734fb252598cd9c00b5b671?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (e4b6640) will **decrease** coverage by `0.03%`.\n> The diff coverage is `87.85%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1165?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [...ward/\\_v2/operations/convert/ak\\_from\\_json\\_schema.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1165/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pzb25fc2NoZW1hLnB5) | `85.54% <87.85%> (-0.51%)` | :arrow_down: |\n",
  "created_at":"2021-11-24T19:59:50Z",
  "id":978180646,
  "issue":1165,
  "node_id":"IC_kwDODBCWws46Tdom",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-25T03:38:41Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"This replaces #1162.\r\n\r\nForth may be good for binary formats (especially unruly ones like ROOT, where you really need the Turing completeness!), but parsing JSON just required too many instructions (at 7 ns per instruction). This PR implements a very simple language for encoding the JSON \u2192 Awkward rules, given a JSONSchema, and lets RapidJSON do the JSON parsing. These instructions are invoked by RapidJSON's SAX interface. It's called \"JSON assembly\" because it's basically a virtual assembly language.\r\n\r\nTesting again with this example:\r\n\r\n```python\r\nMULTIPLIER = int(10e6)\r\njson_string = b\"[\" + b\", \".join([\r\n    b'[{\"x\": 1.1, \"y\": [1]}, {\"x\": 2.2, \"y\": [1, 2]}, {\"x\": 3.3, \"y\": [1, 2, 3]}],' +\r\n    b'[],' +\r\n    b'[{\"x\": 4.4, \"y\": [1, 2, 3, 4]}, {\"x\": 5.5, \"y\": [1, 2, 3, 4, 5]}]'\r\n] * MULTIPLIER) + b\"]\"\r\n```\r\n\r\nwe have\r\n\r\n   * **70.0 seconds** in Python's `json` module (projected from a 1/10th dataset, since the full dataset would produce 20 GB of Python lists and dicts)\r\n   * **38.6 seconds** with untyped ArrayBuilder\u2014the state of the art if you have no schema\r\n   * **31.5 seconds** with schema-specialized Forth code (trading ArrayBuilder navigation costs with Forth navigation costs)\r\n   * **10.4 seconds** with RapidJSON + this simple language \u2192 Awkward Arrays\r\n   * **6.0 seconds** for RapidJSON with no output\r\n\r\nSo getting just iterating over this simple language and writing out output costs 4.4 seconds, which is a little less than the time RapidJSON spends parsing it. Good enough.\r\n\r\nFor non-Awkward but JSON-schema users (who might be using Dask's \"bag\" API now), the value proposition is that using Awkward Array means they can load their data 7\u00d7 faster, use 10\u00d7 less memory, and then do fast vectorized operations on what has been loaded, rather than Python iteration.\r\n\r\nNow I need to find out why Windows is failing.",
  "created_at":"2021-11-25T01:10:41Z",
  "id":978691819,
  "issue":1165,
  "node_id":"IC_kwDODBCWws46Vabr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-25T01:10:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Fixed the \"Windows bug\" (it was revealing a bug that was merely hidden on the other platforms) and tried a few things to get a bit more speed, such as merging identical `switch` cases and merging the two classes into a single class (that mattered for the Forth implementation...). No changes in speed. I'm leaving the `switch` cases merged (it's easier to read), but keeping the classes separate (I want to hide the RapidJSON dependency from the header files).",
  "created_at":"2021-11-25T03:31:43Z",
  "id":978794604,
  "issue":1165,
  "node_id":"IC_kwDODBCWws46Vzhs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-25T03:31:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Another thing to try: each type of instruction should only be writing to one type of output buffer. Replace the single `std::vector` of abstract `ForthOutputBuffets` with a `std::vector` for each specialized subclass, which ought to avoid vtable indirection.\r\n\r\nThe output dtype, currently an argument for each assembly instruction, would have to become fixed. (Easy way to do that would be to just add assertions in the constructor. That way, the Python code can be unchanged.)\r\n\r\nAlso, replace `std::shared_ptr` with `std::unique_ptr`, as we already know that makes a difference.",
  "created_at":"2021-11-25T03:57:45Z",
  "id":978805471,
  "issue":1165,
  "node_id":"IC_kwDODBCWws46V2Lf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-25T03:57:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1166?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1166](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1166?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (3939ed1) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/e4b664096aa4693f5734fb252598cd9c00b5b671?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (e4b6640) will **decrease** coverage by `0.03%`.\n> The diff coverage is `87.85%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1166?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [...ward/\\_v2/operations/convert/ak\\_from\\_json\\_schema.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1166/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pzb25fc2NoZW1hLnB5) | `85.54% <87.85%> (-0.51%)` | :arrow_down: |\n",
  "created_at":"2021-11-29T08:53:59Z",
  "id":981412532,
  "issue":1166,
  "node_id":"IC_kwDODBCWws46fyq0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-29T08:53:59Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I ran into this again the other day, and wondered if we'd placed it on the v2 roadmap. Nice!",
  "created_at":"2021-11-29T09:21:30Z",
  "id":981435630,
  "issue":1166,
  "node_id":"IC_kwDODBCWws46f4Tu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-29T09:21:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"For context, this is the conversation where it came up: https://github.com/scikit-hep/awkward-1.0/pull/1164#issuecomment-976964033.",
  "created_at":"2021-11-29T09:23:34Z",
  "id":981440061,
  "issue":1166,
  "node_id":"IC_kwDODBCWws46f5Y9",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-11-29T09:23:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> For context, this is the conversation where it came up: [#1164 (comment)](https://github.com/scikit-hep/awkward-1.0/pull/1164#issuecomment-976964033).\r\n\r\nAh, looks like I should have filed an issue at the time! Nice catch.",
  "created_at":"2021-11-29T09:46:30Z",
  "id":981461460,
  "issue":1166,
  "node_id":"IC_kwDODBCWws46f-nU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-29T09:46:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jrueb - thanks for spotting it! Indeed, I can reproduce this crash with other reducers.  ",
  "created_at":"2021-11-30T09:56:03Z",
  "id":982471545,
  "issue":1167,
  "node_id":"IC_kwDODBCWws46j1N5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T09:56:03Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Also in v2, which is a clue that it's in the kernels, rather than the Content infrastructure:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import numpy as np\r\n>>> a = ak._v2.contents.NumpyArray([])\r\n>>> b = ak._v2.contents.ListOffsetArray(ak._v2.index.Index64([0]), a)\r\n>>> c = ak._v2.contents.ListOffsetArray(ak._v2.index.Index64([0]), b)\r\n>>> ak._v2.operations.reducers.any(c, axis=1)\r\nFloating point exception (core dumped)\r\n```\r\n\r\nThere are no crashes for `axis=0` or `axis=2`, or any on `b` or `a`, so I think that means that it's getting into this if statement:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/40837bfb0c066d954c0858e5e48d02706252af99/src/awkward/_v2/contents/listoffsetarray.py#L1419\r\n\r\nand not the else branch. It's what I called the \"nonlocal\" case, with kernels\r\n\r\n   * `awkward_ListOffsetArray_reduce_nonlocal_maxcount_offsetscopy_64`\r\n   * `awkward_ListOffsetArray_reduce_nonlocal_preparenext_64`\r\n   * `awkward_ListOffsetArray_reduce_nonlocal_nextstarts_64`\r\n   * `awkward_ListOffsetArray_reduce_nonlocal_findgaps_64`\r\n   * `awkward_ListOffsetArray_reduce_nonlocal_outstartsstops_64`, and\r\n   * `awkward_ListOffsetArray_reduce_nonlocal_nextshifts_64`.\r\n\r\n(Not all of them run; some are for argmin/max only.)\r\n\r\nIt would probably be easier to debug in v2, since the turn-around time of inserting debugging code in Python is faster than the v1 C++.",
  "created_at":"2021-11-30T10:04:35Z",
  "id":982479100,
  "issue":1167,
  "node_id":"IC_kwDODBCWws46j3D8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T10:04:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski - it's because of a division by 0 in a kernel function. I'm submitting a bug-fix PR - just checking if there are other cases as that.",
  "created_at":"2021-11-30T10:26:27Z",
  "id":982499233,
  "issue":1167,
  "node_id":"IC_kwDODBCWws46j7-h",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T10:26:27Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! I had been wondering what \"Floating point exception\" meant\u2014it's a process-level error message I haven't seen before. Also, I'd expect integer division by zero to halt a process, but I would have thought that floating point division by zero would just return `nan` or `inf`. That's the usual behavior for floating point arithmetic:\r\n\r\n```c++\r\nroot [0] double x = 3.14\r\n(double) 3.1400000\r\nroot [1] x / 0.0\r\n(double) inf\r\nroot [2] 0.0 / 0.0\r\n(double) nan\r\n```",
  "created_at":"2021-11-30T10:35:16Z",
  "id":982506411,
  "issue":1167,
  "node_id":"IC_kwDODBCWws46j9ur",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-30T10:35:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"this PR https://github.com/scikit-hep/awkward-1.0/pull/1168 fixes it",
  "created_at":"2021-11-30T10:44:11Z",
  "id":982513150,
  "issue":1167,
  "node_id":"IC_kwDODBCWws46j_X-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T10:44:11Z",
  "user":"MDQ6VXNlcjEzOTA2ODI="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1168?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1168](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1168?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (6453e9e) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/e4b664096aa4693f5734fb252598cd9c00b5b671?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (e4b6640) will **decrease** coverage by `0.03%`.\n> The diff coverage is `87.85%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1168?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [...ward/\\_v2/operations/convert/ak\\_from\\_json\\_schema.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1168/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha19mcm9tX2pzb25fc2NoZW1hLnB5) | `85.54% <87.85%> (-0.51%)` | :arrow_down: |\n",
  "created_at":"2021-11-30T10:51:31Z",
  "id":982518702,
  "issue":1168,
  "node_id":"IC_kwDODBCWws46kAuu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T10:51:31Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1169?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1169](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1169?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (b61dbed) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **not change** coverage.\n> The diff coverage is `n/a`.\n\n",
  "created_at":"2021-12-02T09:30:42Z",
  "id":984449166,
  "issue":1169,
  "node_id":"IC_kwDODBCWws46rYCO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T09:30:42Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1170?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1170](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1170?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (66e8c98) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **decrease** coverage by `0.03%`.\n> The diff coverage is `33.33%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1170?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1170/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1170/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/recordarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1170/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlY29yZGFycmF5LnB5) | `76.80% <50.00%> (-0.46%)` | :arrow_down: |\n",
  "created_at":"2021-12-02T17:58:20Z",
  "id":984864632,
  "issue":1170,
  "node_id":"IC_kwDODBCWws46s9d4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T19:12:37Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1172?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1172](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1172?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (058edde) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **decrease** coverage by `0.03%`.\n> The diff coverage is `33.33%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1172?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1172/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1172/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `55.45% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1172/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/recordarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1172/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlY29yZGFycmF5LnB5) | `76.80% <50.00%> (-0.46%)` | :arrow_down: |\n",
  "created_at":"2021-12-03T21:58:37Z",
  "id":985870808,
  "issue":1172,
  "node_id":"IC_kwDODBCWws46wzHY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-03T21:58:37Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, the \"False, False\" in the second fixed-length array is random memory because the copy only went over 2 items, rather than 4. The PR fixes it and will be auto-merged.",
  "created_at":"2021-12-06T20:18:20Z",
  "id":987164529,
  "issue":1173,
  "node_id":"IC_kwDODBCWws461u9x",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-06T20:18:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Fab, that's what I was thinking - I feel as though we fixed a similar bug this past year!\r\n\r\nThanks for the _rapid_ PR.",
  "created_at":"2021-12-06T21:11:33Z",
  "id":987218167,
  "issue":1173,
  "node_id":"IC_kwDODBCWws4618D3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-06T21:11:33Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1174?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1174](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1174?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (c2420f6) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **decrease** coverage by `0.03%`.\n> The diff coverage is `33.33%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1174?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1174/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1174/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/recordarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1174/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlY29yZGFycmF5LnB5) | `76.80% <50.00%> (-0.46%)` | :arrow_down: |\n",
  "created_at":"2021-12-06T19:56:49Z",
  "id":987135878,
  "issue":1174,
  "node_id":"IC_kwDODBCWws461n-G",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-06T19:56:49Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1175?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1175](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1175?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (6c8db14) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **decrease** coverage by `0.03%`.\n> The diff coverage is `37.50%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1175?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1175/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1175/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/recordarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1175/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlY29yZGFycmF5LnB5) | `76.80% <50.00%> (-0.46%)` | :arrow_down: |\n",
  "created_at":"2021-12-06T20:27:35Z",
  "id":987180765,
  "issue":1175,
  "node_id":"IC_kwDODBCWws461y7d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-06T20:27:35Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"See more detail in my response to ssl-hep/ServiceX_Uproot_Transformer#29, but `explode_fields` is being deprecated in Awkward version 2. There's an example workaround there.",
  "created_at":"2021-12-07T13:45:20Z",
  "id":987939727,
  "issue":1176,
  "node_id":"IC_kwDODBCWws464sOP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-07T13:45:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(This wasn't really a bug; it was intended behavior, though I understand how it would be confusing. You can't use `explode_records` indiscriminately on all data, but then again, it's restructuring the data so it's something you'd have to be paying attention to, anyway. In retrospect, I probably should have never added this option, but instead I should have written documentation on how to do the exploding manually when an Arrow error message comes up.)",
  "created_at":"2021-12-07T13:47:46Z",
  "id":987941665,
  "issue":1176,
  "node_id":"IC_kwDODBCWws464ssh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-07T13:47:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1178?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1178](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1178?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (ff5bba1) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **decrease** coverage by `0.03%`.\n> The diff coverage is `33.33%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1178?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1178/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1178/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1178/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/recordarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1178/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlY29yZGFycmF5LnB5) | `76.80% <50.00%> (-0.46%)` | :arrow_down: |\n",
  "created_at":"2021-12-07T21:53:07Z",
  "id":988288437,
  "issue":1178,
  "node_id":"IC_kwDODBCWws466BW1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-07T21:53:07Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1179?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1179](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1179?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (fe4f307) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `0.13%`.\n> The diff coverage is `40.00%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1179?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1179/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_util.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1179/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL191dGlsLnB5) | `81.90% <\u00f8> (+8.90%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1179/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/recordarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1179/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlY29yZGFycmF5LnB5) | `76.80% <50.00%> (-0.46%)` | :arrow_down: |\n| [src/awkward/\\_v2/operations/structure/ak\\_unzip.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1179/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuemlwLnB5) | `80.00% <100.00%> (\u00f8)` | |\n",
  "created_at":"2021-12-07T22:27:28Z",
  "id":988306587,
  "issue":1179,
  "node_id":"IC_kwDODBCWws466Fyb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-07T22:27:28Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1180?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1180](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1180?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (c931f2c) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **decrease** coverage by `0.03%`.\n> The diff coverage is `33.33%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1180?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1180/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1180/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1180/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/recordarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1180/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlY29yZGFycmF5LnB5) | `76.80% <50.00%> (-0.46%)` | :arrow_down: |\n",
  "created_at":"2021-12-07T22:59:56Z",
  "id":988322637,
  "issue":1180,
  "node_id":"IC_kwDODBCWws466JtN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-07T22:59:56Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1181](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (16850d1) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `0.14%`.\n> The diff coverage is `68.42%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/recordarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlY29yZGFycmF5LnB5) | `76.80% <50.00%> (-0.46%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/numpyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL251bXB5YXJyYXkucHk=) | `89.50% <100.00%> (+0.15%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.54% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/\\_util.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1181/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL191dGlsLnB5) | `81.90% <0.00%> (+8.90%)` | :arrow_up: |\n",
  "created_at":"2021-12-08T00:02:35Z",
  "id":988358011,
  "issue":1181,
  "node_id":"IC_kwDODBCWws466SV7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-08T17:34:30Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1182](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (96be554) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `0.14%`.\n> The diff coverage is `40.00%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_util.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL191dGlsLnB5) | `81.90% <\u00f8> (+8.90%)` | :arrow_up: |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/recordarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlY29yZGFycmF5LnB5) | `76.80% <50.00%> (-0.46%)` | :arrow_down: |\n| [src/awkward/\\_v2/operations/structure/ak\\_unzip.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuemlwLnB5) | `80.00% <100.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.54% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/numpyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1182/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL251bXB5YXJyYXkucHk=) | `89.50% <0.00%> (+0.15%)` | :arrow_up: |\n",
  "created_at":"2021-12-08T18:24:44Z",
  "id":989069070,
  "issue":1182,
  "node_id":"IC_kwDODBCWws468_8O",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-08T19:07:08Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1183](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (8d8b01e) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `0.43%`.\n> The diff coverage is `89.73%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_util.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL191dGlsLnB5) | `81.90% <\u00f8> (+8.90%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/bitmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2JpdG1hc2tlZGFycmF5LnB5) | `65.74% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `60.54% <0.00%> (+1.24%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/regulararray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3JlZ3VsYXJhcnJheS5weQ==) | `81.39% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `55.45% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| ... and [21 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1183/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-12-08T21:53:50Z",
  "id":989240488,
  "issue":1183,
  "node_id":"IC_kwDODBCWws469pyo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-09T19:10:05Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1184](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (e04c358) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `1.50%`.\n> The diff coverage is `83.23%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_reducers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19yZWR1Y2Vycy5weQ==) | `96.48% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `56.27% <51.72%> (+0.82%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `59.75% <63.23%> (+0.44%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.19% <63.58%> (-0.36%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `69.54% <74.19%> (+0.26%)` | :arrow_up: |\n| ... and [31 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1184/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-12-10T02:40:04Z",
  "id":990553958,
  "issue":1184,
  "node_id":"IC_kwDODBCWws47Cqdm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-10T23:11:14Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1186](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (51fcb91) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `1.50%`.\n> The diff coverage is `83.23%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_reducers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19yZWR1Y2Vycy5weQ==) | `96.48% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `56.27% <51.72%> (+0.82%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `59.75% <63.23%> (+0.44%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.19% <63.58%> (-0.36%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `69.54% <74.19%> (+0.26%)` | :arrow_up: |\n| ... and [31 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1186/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-12-15T19:46:05Z",
  "id":995160212,
  "issue":1186,
  "node_id":"IC_kwDODBCWws47UPCU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T18:10:41Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"IIRC, llvmlite supports 3.10, but numba only supports 3.10 in master (and only in the last few days). The wheels build, it's just not possible to run the normal tests.",
  "created_at":"2021-12-15T20:35:27Z",
  "id":995193490,
  "issue":1186,
  "node_id":"IC_kwDODBCWws47UXKS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-15T20:35:27Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"On the calls, they've still been having troubles with 3.10's bytecode optimization. It's harder to tell where the syntax boundaries are.",
  "created_at":"2021-12-15T22:15:22Z",
  "id":995256214,
  "issue":1186,
  "node_id":"IC_kwDODBCWws47UmeW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-15T22:15:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`buildtest-awkward (Windows py39-32bit)` needs to be removed or replaced with `buildtest-awkward (Windows py310-32bit)` in the branch protection settings, otherwise this is ready. (there are also a few new 310 checks) :)",
  "created_at":"2021-12-16T19:19:07Z",
  "id":996108051,
  "issue":1186,
  "node_id":"IC_kwDODBCWws47X2cT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T19:19:07Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Got it, thanks! I added all the Python 3.10 tests to the requirements.\r\n\r\nGreat!",
  "created_at":"2021-12-16T20:44:22Z",
  "id":996180518,
  "issue":1186,
  "node_id":"IC_kwDODBCWws47YIIm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T20:44:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This was hard to do before due to Python 2.7 requiring cibuildwheel 1. The newer toml config is nice too. :)",
  "created_at":"2021-12-16T21:45:28Z",
  "id":996217747,
  "issue":1186,
  "node_id":"IC_kwDODBCWws47YROT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T21:45:28Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1187](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (e54f0a8) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `1.50%`.\n> The diff coverage is `83.23%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_reducers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19yZWR1Y2Vycy5weQ==) | `96.48% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `56.27% <51.72%> (+0.82%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `59.75% <63.23%> (+0.44%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.19% <63.58%> (-0.36%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `69.54% <74.19%> (+0.26%)` | :arrow_up: |\n| ... and [31 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1187/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-12-16T10:06:10Z",
  "id":995622578,
  "issue":1187,
  "node_id":"IC_kwDODBCWws47V_6y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T10:06:10Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2021-12-16T12:31:42Z",
  "id":995773741,
  "issue":1187,
  "node_id":"IC_kwDODBCWws47Wk0t",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-16T12:31:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1188](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (cf5921c) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `1.50%`.\n> The diff coverage is `83.23%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_reducers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19yZWR1Y2Vycy5weQ==) | `96.48% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `56.27% <51.72%> (+0.82%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `59.75% <63.23%> (+0.44%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.19% <63.58%> (-0.36%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `69.54% <74.19%> (+0.26%)` | :arrow_up: |\n| ... and [31 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1188/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-12-17T05:23:42Z",
  "id":996447309,
  "issue":1188,
  "node_id":"IC_kwDODBCWws47ZJRN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-17T05:23:42Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"@fleble check out https://github.com/scikit-hep/awkward-1.0/discussions/710 . Maybe it's worth revisiting the proposal there for `ak.singletons()`",
  "created_at":"2021-12-19T00:37:16Z",
  "id":997306271,
  "issue":1189,
  "node_id":"IC_kwDODBCWws47ca-f",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-19T00:37:16Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1194](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (0100b84) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `1.53%`.\n> The diff coverage is `83.23%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_reducers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19yZWR1Y2Vycy5weQ==) | `96.48% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `56.27% <51.72%> (+0.82%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `59.75% <63.23%> (+0.44%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.19% <63.58%> (-0.36%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `69.54% <74.19%> (+0.26%)` | :arrow_up: |\n| ... and [31 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1194/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-12-27T19:36:47Z",
  "id":1001714625,
  "issue":1194,
  "node_id":"IC_kwDODBCWws47tPPB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-27T19:36:47Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"NONE",
  "body":"# [Codecov](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) Report\n> Merging [#1195](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (02c64e3) into [main](https://codecov.io/gh/scikit-hep/awkward-1.0/commit/995540142863ed09bde57e13957699fdf4224507?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) (9955401) will **increase** coverage by `1.53%`.\n> The diff coverage is `83.23%`.\n\n| [Impacted Files](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | Coverage \u0394 | |\n|---|---|---|\n| [src/awkward/\\_v2/\\_connect/pyarrow.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19jb25uZWN0L3B5YXJyb3cucHk=) | `83.33% <0.00%> (-0.48%)` | :arrow_down: |\n| [src/awkward/\\_v2/\\_reducers.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL19yZWR1Y2Vycy5weQ==) | `96.48% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/forms/recordform.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2Zvcm1zL3JlY29yZGZvcm0ucHk=) | `66.46% <0.00%> (\u00f8)` | |\n| [src/awkward/\\_v2/operations/convert/ak\\_to\\_numpy.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvY29udmVydC9ha190b19udW1weS5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [...c/awkward/\\_v2/operations/structure/ak\\_unflatten.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL29wZXJhdGlvbnMvc3RydWN0dXJlL2FrX3VuZmxhdHRlbi5weQ==) | `80.00% <\u00f8> (\u00f8)` | |\n| [src/awkward/\\_v2/types/arraytype.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL3R5cGVzL2FycmF5dHlwZS5weQ==) | `80.76% <0.00%> (-0.72%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/unmaskedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL3VubWFza2VkYXJyYXkucHk=) | `56.27% <51.72%> (+0.82%)` | :arrow_up: |\n| [src/awkward/\\_v2/contents/indexedarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2luZGV4ZWRhcnJheS5weQ==) | `59.75% <63.23%> (+0.44%)` | :arrow_up: |\n| [src/awkward/\\_v2/\\_typetracer.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL190eXBldHJhY2VyLnB5) | `69.19% <63.58%> (-0.36%)` | :arrow_down: |\n| [src/awkward/\\_v2/contents/emptyarray.py](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep#diff-c3JjL2F3a3dhcmQvX3YyL2NvbnRlbnRzL2VtcHR5YXJyYXkucHk=) | `69.54% <74.19%> (+0.26%)` | :arrow_up: |\n| ... and [31 more](https://codecov.io/gh/scikit-hep/awkward-1.0/pull/1195/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scikit-hep) | |\n",
  "created_at":"2021-12-28T14:34:04Z",
  "id":1002134177,
  "issue":1195,
  "node_id":"IC_kwDODBCWws47u1qh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-28T14:34:04Z",
  "user":"MDM6Qm90MjI0Mjk2OTU="
 },
 {
  "author_association":"MEMBER",
  "body":"ak.packed has already been translated to v2; is this issue present in the translated function? We don't want the bug to regress when we update. Thanks!",
  "created_at":"2021-12-28T14:35:24Z",
  "id":1002134735,
  "issue":1195,
  "node_id":"IC_kwDODBCWws47u1zP",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-12-28T14:35:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This should already be fixed in v2 after looking at the source: we immediately call `packed()` on the `project()`ed result",
  "created_at":"2021-12-28T14:38:07Z",
  "id":1002135861,
  "issue":1195,
  "node_id":"IC_kwDODBCWws47u2E1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-28T15:22:32Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 }
]
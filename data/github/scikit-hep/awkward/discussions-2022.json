[
 {
  "author":{
   "login":"kreisl"
  },
  "body":"Hi,\r\n\r\nI am analyzing a nested record structure shaped similar to:\r\n```\r\narray = ak.Array([\r\n    {\r\n    \"a\": {\"b\": [1,1], \"c\":[2,2]},\r\n    \"d\": {\"b\": [3,3], \"c\":[4,4]}\r\n    },\r\n    {\r\n    \"a\": {\"b\": [5,5], \"c\":[6,6]},\r\n    \"d\": {\"b\": [7,7], \"c\":[8,8]}\r\n    }\r\n])\r\n```\r\nWhat is the recommended way of unraveling the array such that all the record are flattened like so:\r\n`ak.Array([[1,1,2,2,3,3,4,4], [5,5,6,6,7,7,8,8]])` or `ak.Array([[1,1,3,3,2,2,4,4], [5,5,7,7,6,6,8,8]])` which I need for processing inside pytorch.\r\nRight now I do the following:\r\n\r\n```\r\ndef unzip_and_concatenate_test(array, labels):\r\n    array = ak.concatenate(ak.unzip(array[labels.pop(0)]), axis=0)\r\n    if labels:\r\n        return unzip_and_concatenate(array, labels)\r\n    return array\r\n```\r\nwhere the labels are given by for example `[['a','d'],['b','c']]`.\r\nIs there a simpler approach to flattening nested records?\r\n\r\nI also have another related question. After I flattened the array is it possible to \"view\" it as records again, or would I need to copy and create a new Array using the Arraybuilder? I may have to mask certain entries (for example masking all entries from \"c\") of the flattened array after processing, which would be easier when the array would be viewable as a record rather than computing the positions by hand.\r\nBelow I have sketched the workflow:\r\nRecordArray -> flattened Array -> processing in pytorch -> masking the Array -> postprocessing\r\n\r\nThank you and best regards,\r\nLukas",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"On the face of it, doing things by hand seems like the easiest approach. We do have tools to generalise this for arrays, but it involves constructing layouts yourself. Do you have any more information about the structure, e.g. are your arrays regular?\r\n",
     "createdAt":"2022-01-06T14:13:35Z",
     "number":1918781,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"kreisl"
        },
        "body":"My arrays are regular. Every entry in the array has the same shape: for example \r\n```  \r\n[{\r\n  \"class_1\": {\"property_1\": [1,2], \"property_2\":[1,2,3]},\r\n  \"class_2\": {\"property_1\": [1,2], \"property_2\":[1,2,3]}\r\n},{\r\n  \"class_1\": {\"property_1\": [4,3], \"property_2\":[1,2,6]},\r\n  \"class_2\": {\"property_1\": [0,1], \"property_2\":[1,4,3]}\r\n}]\r\n```\r\nflattened array: \r\n```\r\n[ [1,2,1,2,3,1,2,1,2,3], [4,3,1,2,6,0,1,1,4,3] ]\r\n```\r\nWould it be possible to view this flattened array again with the \"records\" layout? This would make post processing easier (for example selecting only the `property_2` of `class_1`).",
        "createdAt":"2022-01-06T15:13:43Z",
        "number":1919201
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"For some context, Awkward is not really designed to be able to dissolve fields - they're an important part of the structure. For this reason, we don't have any built-in routines to do this. \r\n\r\nIf you have a small number of fields (e.g. 2 properties, 2 classes), then manually concatenating is fine. To rebuild the structure, you can use `ak.zip` with strided views over the ravelled array, e.g.\r\n```python3\r\nres = ...\r\n\r\nb = res[:, :2]\r\nc = res[:, 2:4]\r\nb2 = res[:, 4:6]\r\nc2 = res[:, 6:8]\r\n\r\narray_res = ak.zip(\r\n    {\"a\": ak.zip({\"b\": b, \"c\": c}, depth_limit=1), \"d\": ak.zip({\"b\": b2, \"c\": c2}, depth_limit=1)}\r\n)\r\n```",
        "createdAt":"2022-01-06T15:22:21Z",
        "number":1919255
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"The arrays are always fixed-length and you want to go between views of them as a single contiguous array and as records with fields, that's sounding more like [NumPy structured arrays](https://numpy.org/doc/stable/user/basics.rec.html) than Awkward record arrays.\r\n\r\n**In a NumPy structured array,** the memory layout is a single buffer in which fields must be contiguous, equivalent to an array of\r\n\r\n```c\r\nstruct {\r\n    struct {\r\n        int b[2];\r\n        int c[2];\r\n    } a;\r\n    struct {\r\n        int b[3];\r\n        int c[3];\r\n    } d;\r\n};\r\n```\r\n\r\nin C (I don't remember the exact syntax, but I think you understand what I mean). This data can be viewed (\"reinterpret cast\") as a single array of integers, or as an array of one-level structs. Views are fast, just interpreting the in-memory buffer in different ways.\r\n\r\n**In an Awkward record array,** the memory layout is separate for each field. `array.a.b` may be in a completely different memory address from `array.a.c`, and each of them are one-dimensional arrays of integers. That makes other types of rearrangement fast: incorporating `array.a.b` in some `other_array`, splitting all fields out ([ak.unzip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.unzip.html)), combining fields into new records ([ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html)), etc. Those are just interpreting the in-memory buffers (plural) in different ways.\r\n\r\nHowever, turning an Awkward record array into a single flat array without fields is not a trivial operation. You'd do it by picking fields out and calling [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) on them, possibly with an explicit `axis`. Pulling the pieces back into fields would involve the slices @agoose77 specified. Slices are just views (fast), but you have to be very aware of the regular lengths to slice them correctly.\r\n\r\nGiven that you want to do the records-to-single-array and back frequently, NumPy's data structure sounds more appropriate to your problem. It _is_ possible to make nested fields in a NumPy structured array; I've recently learned about that, though I haven't tried it.\r\n\r\nLet's see...\r\n\r\n```python\r\n>>> import numpy as np\r\n\r\n>>> flat = np.array([1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8])\r\n\r\n>>> structured = flat.view([(\"a\", [(\"b\", int, 2), (\"c\", int, 2)]), (\"d\", [(\"b\", int, 2), (\"c\", int, 2)])])\r\n\r\n>>> structured\r\narray([(([1, 1], [2, 2]), ([3, 3], [4, 4])),\r\n       (([5, 5], [6, 6]), ([7, 7], [8, 8]))],\r\n      dtype=[('a', [('b', '<i8', (2,)), ('c', '<i8', (2,))]), ('d', [('b', '<i8', (2,)), ('c', '<i8', (2,))])])\r\n\r\n>>> structured[\"d\"][\"b\"]\r\narray([[3, 3],\r\n       [7, 7]])\r\n\r\n>>> q = structured[\"d\"][\"b\"]\r\n>>> q[0, 0] = 999    # mutable and still linked to the original\r\n>>> q\r\narray([[999,   3],\r\n       [  7,   7]])\r\n>>> structured       # yup, still linked\r\narray([(([1, 1], [2, 2]), ([999,   3], [4, 4])),\r\n       (([5, 5], [6, 6]), ([  7,   7], [8, 8]))],\r\n      dtype=[('a', [('b', '<i8', (2,)), ('c', '<i8', (2,))]), ('d', [('b', '<i8', (2,)), ('c', '<i8', (2,))])])\r\n```\r\n\r\nThat works, and this `structured` array is re-viewable into a `flat` one and back.\r\n\r\nHaving this single buffer is an advantage and a disadvantage. If you really want the fields to be \"dissolvable\" into a single array (of a specific, particular order), then it's just what you want. Awkward records weren't designed like this because the idea is that record fields describe semantically distinct data, like columns with different names, data types, and probably different units of measurement. Those are values that you'd never want to mix indiscriminately, but you likely want to pull a column out and deal with it separately from the rest of the structure.",
        "createdAt":"2022-01-06T18:05:23Z",
        "number":1920337
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-06T12:51:30Z",
  "number":1205,
  "title":"Flattening nested RecordArrays and restoring records from flattened Array",
  "url":"https://github.com/scikit-hep/awkward/discussions/1205"
 },
 {
  "author":{
   "login":"maxgalli"
  },
  "body":"Hi everyone,\r\n\r\nI was wondering if there is a function that performs permutations without (or with, but it wouldn't be my use case) repetitions of items inside an array. Taking as example the one from ```ak.combinations``` it would be something like:\r\n```python\r\n>>> array = ak.Array([\"a\", \"b\", \"c\"])\r\n>>> ak.to_list(ak.permutations(array=array, n=2, replacement=False, axis=0))\r\n[('a', 'b'), ('a', 'c'), ('b', 'c'), ('b', 'a'), ('c', 'a'), ('c', 'b')]\r\n```\r\nMy use case would be a tag and probe-like workflow, in which we select pairs of photons but each photon within the pair can be both tag and probe, hence we have to treat both cases.\r\n\r\nIf there is no such thing, are there any ideas about how it could be nicely and efficiently implemented (maybe not in awkward since outside HEP could be not so needed, but maybe inside something like coffea @lgray)?\r\n\r\nThanks a lot,\r\n\r\nMassimiliano",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Are you sure what you're looking for should be called \"permutations\"? It looks like you want the Cartesian product with the diagonal elements removed (see [ak.cartesian](https://awkward-array.readthedocs.io/en/latest/_auto/ak.cartesian.html)).\r\n\r\nHere's a way to get your desired output (up to the order, but I think the middle two are swapped):\r\n\r\n```python\r\n>>> array = ak.Array([\"a\", \"b\", \"c\"])\r\n>>> left, right = ak.unzip(ak.argcartesian([array, array], axis=0))\r\n>>> left\r\n<Array [0, 0, 0, 1, 1, 1, 2, 2, 2] type='9 * int64'>\r\n>>> right\r\n<Array [0, 1, 2, 0, 1, 2, 0, 1, 2] type='9 * int64'>\r\n>>> different = (left != right)\r\n>>> left[different]\r\n<Array [0, 0, 1, 1, 2, 2] type='6 * int64'>\r\n>>> right[different]\r\n<Array [1, 2, 0, 2, 0, 1] type='6 * int64'>\r\n>>> array[left[different]]\r\n<Array ['a', 'a', 'b', 'b', 'c', 'c'] type='6 * string'>\r\n>>> array[right[different]]\r\n<Array ['b', 'c', 'a', 'c', 'a', 'b'] type='6 * string'>\r\n>>> ak.zip([array[left[different]], array[right[different]]], depth_limit=1).tolist()\r\n[('a', 'b'), ('a', 'c'), ('b', 'a'), ('b', 'c'), ('c', 'a'), ('c', 'b')]\r\n```\r\n\r\nThis could be the implementation of what you're looking for, with some care to apply the cut (`left[different]` and `right[different]`) at the appropriate `axis`. But what should such a function be named?\r\n\r\n-------------\r\n\r\nRegarding permutations, all the ways that a multiset of items can be ordered as a sequence, we could use NumPy's function at `axis=0`... _Or not!_ It turns out that NumPy doesn't have a function for listing all permutations. They have [np.random.permutation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.permutation.html), but not one that enumerates. There are various recipes online for doing it with [np.repeat](https://numpy.org/doc/stable/reference/generated/numpy.repeat.html) and [np.tile](https://numpy.org/doc/stable/reference/generated/numpy.tile.html), but by that point and because you'd want something that works at `axis != 0`, it would have to be a new, specialized Awkward Array function.\r\n\r\n(A NumPy function that takes an array of length `n` and makes an array of length `n!` would probably be a bad idea. But for jagged arrays with typically small inner dimension, the output array would have total size `sum_i (n_i!)`, which wouldn't be so bad.)",
     "createdAt":"2022-01-19T20:42:41Z",
     "number":2000767,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"maxgalli"
        },
        "body":"I wasn't indeed sure about how to call what I needed :)\r\nThank you for the very detailed answer, I will try it out!",
        "createdAt":"2022-01-19T21:02:22Z",
        "number":2000881
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-19T17:15:40Z",
  "number":1236,
  "title":"Permutations without repetition",
  "url":"https://github.com/scikit-hep/awkward/discussions/1236"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"Currently, the minimum NumPy version that Awkward Array can use is 1.13.1 (Jul 6, 2017, almost 5 years old). This minimum was set at the beginning of the project because it is the first NumPy version that supports [NEP 13](https://numpy.org/neps/nep-0013-ufunc-overrides.html), without which Awkward Arrays couldn't use any ufuncs. Worse still, if someone _tried_ to use a ufunc, it would give wrong answers, rather than raise exceptions.\r\n\r\nAnother meaningful cut-off would be to require NumPy >= 1.17.0 (Jul 26, 2019, almost 3 years old), because this is the first version that supports [NEP 18](https://numpy.org/neps/nep-0018-array-function-protocol.html), without which other NumPy functions, like `np.concatenate`, wouldn't work, though you could substitute `ak.concatenate` and it would work fine. This was never a limiting constraint because if you had an old NumPy and tried to use `np.concatenate`, it would just be slow and/or raise exceptions.\r\n\r\nIn #1230, @agoose77 brought up the idea of implementing NumPy's random number generators (also required much earlier, in #489). NumPy has a legacy API that was superseded in NumPy 1.17.0. If we restrict to that NumPy version, we only need to implement the newer API, which is, of course, half the work! There were some bug-fixes in the API, so the minimum NumPy version might actually be 1.17.x for some x to be determined.\r\n\r\n**So the question is, would anyone's life be made more difficult if Awkward Array's minimum NumPy version became 1.17.x? Are there any systems that just have to use old NumPys?**",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Worth noting here before I forget it: we have some work-arounds for NumPy arrays that lack a `tobytes` method (switching to the older `tostring` instead). If the new minimal version of NumPy has `tobytes`, then that simplifies some code and we should remember to update those cases.",
     "createdAt":"2022-01-21T16:50:00Z",
     "number":2017038,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-19T19:34:56Z",
  "number":1238,
  "title":"Requiring NumPy >= 1.17.x",
  "url":"https://github.com/scikit-hep/awkward/discussions/1238"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi, \r\n\r\nIf I have a list of awkward arrays, for example:\r\n```\r\n[{'x': 20, y: [1, 2, 3, 4, 5]}, {'x':30, 'y':[6, 7, 8, 9, 10]}] \r\n```\r\nAnd I would like to combine the elements of the list, such that I have \r\n```\r\n{'x': [20,30], 'y': [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]] }\r\n```\r\n\r\nIs there a function which does that?  I tried to use `ak.concatenate`but I get back something like:\r\n```\r\n['x', 'y', 'x', 'y']\r\n```\r\n\r\nThanks,\r\nMo",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Now I wish the string representation of Awkward Arrays and Records didn't look so much like Python lists and dicts, because Python data type is important for this question!\r\n\r\n[ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) is the right function to use, though you can only use it without qualification if you really do have two Awkward Arrays:\r\n\r\n```python\r\n>>> one = ak.Array([{\"x\": 20, \"y\": [1, 2, 3, 4, 5]}])\r\n>>> two = ak.Array([{\"x\": 30, \"y\": [6, 7, 8, 9, 10]}])\r\n>>> onetwo = ak.concatenate([one, two])\r\n>>> onetwo.tolist()\r\n[{'x': 20, 'y': [1, 2, 3, 4, 5]}, {'x': 30, 'y': [6, 7, 8, 9, 10]}]\r\n```\r\n\r\nYou wanted the x data and the y data in separate arrays, so\r\n\r\n```python\r\n>>> onetwo.x\r\n<Array [20, 30] type='2 * int64'>\r\n>>> onetwo.y\r\n<Array [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]] type='2 * var * int64'>\r\n```\r\n\r\nor maybe\r\n\r\n```python\r\n>>> ak.Record({\"x\": onetwo.x, \"y\": onetwo.y}).tolist()\r\n{'x': [20, 30], 'y': [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]}\r\n```\r\n\r\nTo end up with lists of field names, I think you might be iterating over Python dicts or Awkward Records, rather than concatenating Arrays. Maybe this?\r\n\r\n```python\r\n>>> one = ak.Record({\"x\": 20, \"y\": [1, 2, 3, 4, 5]})\r\n>>> two = ak.Record({\"x\": 30, \"y\": [6, 7, 8, 9, 10]})\r\n>>> ak.concatenate([one, two])\r\n<Array ['x', 'y', 'x', 'y'] type='4 * string'>\r\n```\r\n\r\nLike all functions that take [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html) input, if [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) gets an argument that is not an [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html), it will try to coerce it into being an [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html). A NumPy array, for instance, would be converted with [ak.from_numpy](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_numpy.html). Anything with no recognizable type other than being a Python iterable will be iterated over, using [ak.from_iter](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_iter.html).\r\n\r\n[ak.Record](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Record.html), like a [Python dict](https://docs.python.org/3/tutorial/datastructures.html#dictionaries), iterates over its field names:\r\n\r\n```python\r\n>>> list({\"x\": 20, \"y\": [1, 2, 3, 4, 5]})\r\n['x', 'y']\r\n>>> list(ak.Record({\"x\": 20, \"y\": [1, 2, 3, 4, 5]}))\r\n['x', 'y']\r\n```",
     "createdAt":"2022-01-25T21:26:18Z",
     "number":2046401,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"MoAly98"
     },
     "body":"Ha! That's exactly what was happening. I'm a bit confused as to why I ended up with a `Record` object though. I had started with data from `data = uproot.concatenate(..)` then I was testing my code on just one event by using `data[0]` where data should be similar to the example I gave above -- should this not return an `Array` object? Is there a general rule as to when a slicing operation on an awkward array return a `Record` object ? \r\n\r\nThank you so much, that's been extremely helpful!",
     "createdAt":"2022-01-25T21:46:04Z",
     "number":2046530,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"If `data` is an [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html), `data[0]` might be an [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html), an [ak.Record](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Record.html), or a numerical/boolean/datetime/string/bytes scalar:\r\n\r\n```python\r\n>>> array = ak.Array([[{\"x\": 1, \"y\": 1.1}], [], [{\"x\": 2, \"y\": 2.2}]])\r\n>>> array\r\n<Array [[{x: 1, y: 1.1}], ... [{x: 2, y: 2.2}]] type='3 * var * {\"x\": int64, \"y\"...'>\r\n>>> array[2]\r\n<Array [{x: 2, y: 2.2}] type='1 * {\"x\": int64, \"y\": float64}'>\r\n>>> array[2, 0]\r\n<Record {x: 2, y: 2.2} type='{\"x\": int64, \"y\": float64}'>\r\n>>> array[2, 0, \"y\"]\r\n2.2\r\n```\r\n\r\nIt depends on the `array.type`:\r\n\r\n```python\r\n>>> array.type\r\n3 * var * {\"x\": int64, \"y\": float64}\r\n```\r\n\r\nSuccessive unpacking will get simpler objects until they can't be unpacked anymore.\r\n\r\nI also answered your other question on scikit-hep/uproot4/discussions/550 (for cross-referencing, since they're related).",
        "createdAt":"2022-01-25T22:30:48Z",
        "number":2046785
       },
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"I see, that makes sense. Is there a scenario where we use `uproot.concatenate` or `uproot.iterate` where the return type ends up being for example:\r\n```\r\nN * var * {\"branch1\": int64, \"branch2\": float64}\r\n````\r\nrahter than \r\n```\r\nN  * {\"branch1\": int64, \"branch2\": float64}\r\n````\r\nwhere `N` is the number of events in the file?\r\n\r\nI also wonder if there is any documentation on how to drop fields from an awkward arrays and how to change field names, if either operations are possible?\r\n\r\nThank you very much.",
        "createdAt":"2022-01-26T13:36:05Z",
        "number":2050939
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I'm hesitating before making such a categorical statement, but I think there is _no_ way that [uproot.concatenate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.concatenate.html), [uproot.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html), [uproot.lazy](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.lazy.html), [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays), or [uproot.TTree.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#iterate) can make a data type like\r\n\r\n```\r\nN * var * XYZ\r\n```\r\n\r\nAll of those 5 functions (all array-fetching functions except for [uproot.TBranch.array](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#array)) get data from multiple TBranches, so there would always be a top-level record structure for the separate branches, even if there's only one of them, at least to specify their names (or its name). The question is whether any of those are ever made with [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html) without restricting `depth_limit`, such that a structure that's originally\r\n\r\n```\r\nN * {field1: var * X, field2: var * Y, ...}\r\n```\r\n\r\nmight be returned as\r\n\r\n```\r\nN * var * {field1: X, field2: Y, ...}\r\n```\r\n\r\nand I think the answer is \"no.\" Whether or not it can do that, even in principle, depends on whether or not the \"var\" before \"X\" has the same number of subentries for each entry as the \"var\" before \"Y\", and that's something that we can't know about two unconnected branches. Returning a different _type_ depending on the _values_ of data in a file would be bad\u2014the type must be fully determined by what's in the TTree metadata, before reading any TBaskets.\r\n\r\nActually, now that I think about it, there is type-level information (in the TTree metadata) that would say whether one branch's \"var\" is the same as another branch's \"var\": if the two TBranches' TLeaves have the same [TLeaf::GetLeafCount](https://root.cern.ch/doc/master/classTLeaf.html#a6fb5b55f85ae4b64447e48d247fc5ddf). _In principle_, the array type could depend on that and not violate the rule against types depending on values. However, I don't think I implemented anything like that\u2014such a rule would be more subtle than Uproot ought to be.\r\n\r\nI found a good test case:\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> tree = uproot.open(skhep_testdata.data_path(\"uproot-small-evnt-tree-fullsplit.root\"))[\"tree\"]\r\n```\r\n\r\nThe \"SliceF32\" and \"SliceF64\" branches have the same [TLeaf::GetLeafCount](https://root.cern.ch/doc/master/classTLeaf.html#a6fb5b55f85ae4b64447e48d247fc5ddf). In C++, the function returns the same pointer; in Python, it returns references to the same object, which can be identified by the \"at 0x...\" in its repr:\r\n\r\n```python\r\n>>> tree[\"evt/SliceF32\"].member(\"fLeaves\")[0].member(\"fLeafCount\")\r\n<TLeafElement (version 1) at 0x7f8803f3a580>\r\n>>> tree[\"evt/SliceF64\"].member(\"fLeaves\")[0].member(\"fLeafCount\")\r\n<TLeafElement (version 1) at 0x7f8803f3a580>\r\n```\r\n\r\nEven though these have the same leaf count and _could in principle_ be `100 * var * {\"evt/SliceF32\": float32, \"evt/SliceF64\": float64}`, they aren't:\r\n\r\n```python\r\n>>> tree.arrays([\"evt/SliceF32\", \"evt/SliceF64\"]).type\r\n100 * {\"evt/SliceF32\": var * float32, \"evt/SliceF64\": var * float64}\r\n```\r\n\r\nSo no: those 5 functions will always return arrays with a record structure at top-level, even if they _could_ have been merged into an array of lists of records.\r\n\r\n-----------------\r\n\r\nDropping fields is done by slicing. One of the allowed slice types is a list of strings, which would pull out more than one field. You can use this to pull out just the fields you want.\r\n\r\n```python\r\n>>> array = ak.Array([{\"x\": 1, \"y\": 1.1, \"z\": \"one\"}, {\"x\": 2, \"y\": 2.2, \"z\": \"two\"}])\r\n>>> array[\"x\"]\r\n<Array [1, 2] type='2 * int64'>\r\n>>> array[[\"x\", \"y\"]]\r\n<Array [{x: 1, y: 1.1}, {x: 2, y: 2.2}] type='2 * {\"x\": int64, \"y\": float64}'>\r\n>>> array[[\"z\", \"x\"]]\r\n<Array [{z: 'one', x: 1}, {z: 'two', x: 2}] type='2 * {\"z\": string, \"x\": int64}'>\r\n```\r\n\r\nIt's documented under [ak.Array.getitem](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#nested-projection) as \"nested projection.\"\r\n\r\nThe list given to a projection is an inclusive list, so you have to write down which ones you want to keep, rather than the ones you want to drop. You can get a full list of fields from [ak.fields](https://awkward-array.readthedocs.io/en/latest/_auto/ak.fields.html).\r\n\r\n~~You can also drop fields, changing the [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html) or [ak.Record](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Record.html) object in place.~~ Nope, apparently not: you can assign in-place with `__setitem__` (possibly replacing and old field), but the corresponding `__delitem__` (which would be quite similar) apparently hasn't been implemented. That would be a good feature request, though it would likely only go into v2 at this point.\r\n\r\nThe assignment with `__setitem__` is implemented through [ak.with_field](https://awkward-array.readthedocs.io/en/latest/_auto/ak.with_field.html#ak.with_field) if you want a non-in-place version. Internally, Awkward Arrays are implemented with immutable but structurally shared pieces (only the high-level [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html) and [ak.Record](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Record.html) can actually be changed in-place), so making a new array with a different field uses the same memory as replacing a field in an old array. It's just syntactic sugar for your convenience.",
        "createdAt":"2022-01-26T15:49:12Z",
        "number":2051904
       },
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"Brilliant, thanks a lot for the comprehensive response and advice. \r\n\r\nI will raise a feature request for  `__delitem__` as i think it would be useful. I delete the fields now via a list comprehension where I drop I don't process the fields I don't want.\r\n\r\nThanks a lot.\r\nMo",
        "createdAt":"2022-01-26T19:08:15Z",
        "number":2053343
       }
      ],
      "totalCount":4
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-01-25T13:46:46Z",
  "number":1251,
  "title":"How to combine a list of awkward arrays ?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1251"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi, \r\nI am trying to assign a new field in an `awkward` array to a `float`. To do that, I am simply using an equivalent call to \r\n```\r\nsum = ak.sum(array['y'])\r\narray[\"x\"] = sum\r\n```\r\nHowever, I am running into the following error from the `with_field` method:\r\n```\r\n  File \"/afs/cern.ch/user/m/maly/.conda/envs/thbb_env/lib/python3.9/site-packages/awkward/highlevel.py\", line 1068, in __setitem__\r\n    array = ak.operations.structure.with_field(self.layout, what, where)\r\n  File \"/afs/cern.ch/user/m/maly/.conda/envs/thbb_env/lib/python3.9/site-packages/awkward/operations/structure.py\", line 936, in with_field\r\n    out = (ak.layout.RecordArray([what], [where], parameters=base.parameters),)\r\nValueError: content argument must be a Content subtype\r\n```\r\nI can get around the error by simply setting the field to be a numpy array with identical elements using `np.full` with a shape identical to `array['y']`. Is it a rule that I have to set the field to be an awkward array or a numpy array? ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"This has been working for me:\r\n\r\n```python\r\n>>> array = ak.Array([{\"y\": 1}, {\"y\": 2}, {\"y\": 3}])\r\n>>> sum = ak.sum(array['y'])\r\n>>> array[\"x\"] = sum\r\n>>> array\r\n<Array [{y: 1, x: 6}, ... x: 6}, {y: 3, x: 6}] type='3 * {\"y\": int64, \"x\": int64}'>\r\n```\r\n\r\nConsidering that `sum` is already a defined symbol in Python, and it's a thing that can't be assigned into an [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html) (`<built-in function sum>`), maybe your script is seeing the original `sum`, rather than the output of `ak.sum(array['y'])`?\r\n\r\nI just did a quick check and see that the right-hand-side can be Python numbers (`int`, `float`) or NumPy numbers (`np.int32`, `np.float32`, etc.). That's not the problem.",
     "createdAt":"2022-01-26T20:06:19Z",
     "number":2053678,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"It turns out I was assigning to an already exising field name by mistake, which had a different type than float. I think this is what caused the error.",
        "createdAt":"2022-01-30T13:20:26Z",
        "number":2076018
       },
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"Actually,  a follow up on that.. what if I want to assign the field to an array, is there a way to do that without getting Broadcast errors? For example I want to end up with \r\n```\r\n<Array [{y: 1, x: [5,6,7]}, ... x: [5,6,7]}, {y: 3, x: [5,6,7]}] type='3 * {\"y\": int64, \"x\": var*int64}'>\r\n```",
        "createdAt":"2022-02-03T13:48:35Z",
        "number":2103875
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Well, facetiously, it's possible if it broadcasts correctly!\r\n\r\nIn this case, it looks like you want the \"x\" that you assign to have the same length as \"y\", meaning the same number of `[5, 6, 7]` lists as \"y\" has numbers, since \"x\" is an array of lists and \"y\" is an array of numbers.\r\n\r\nNow I'm a little worried that broadcasting might have been too strong a constraint\u2014it might always try to make them have the same structure, the way it would have to if you were doing a numerical operation on the two arrays (such as `+`). In that case, my snide comment aside, it might actually be impossible to do that assignment, and we'd have to rethink how assignments are defined.\r\n\r\nOne thing that would always be possible is to use an Array constrictor,\r\n\r\n```python\r\nak.Array({\"y\": y, \"x\": x})\r\n```\r\n\r\nor `ak.zip`,\r\n\r\n```python\r\nak.zip({\"y\": y, \"x\": x}, depth_limit=1)\r\n```\r\n\r\nThe `depth_limit` gives you control over how deeply it attempts to zip the arrays together, which is the control that might be lost if broadcasting is too strong a rule for assignment.\r\n\r\n(In any case, don't worry about performance of making a new array vs attaching to an old array\u2014they do the same thing internally, which is to share memory as much as possible.)",
        "createdAt":"2022-02-03T14:05:32Z",
        "number":2104012
       },
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"Hmmm i see. In my case, I have a very large array (~O(10^6) events) which already has fields that are arrays. I just want to add a field to it that is an array of some length such that each event gets an instance of that array. If I try to break down my original array, rebuild it as a dict then pass it to `ak.Array` I:\r\n\r\nA) still get errors \r\n```\r\ndict of arrays in ak.Array constructor must have arrays of equal length\r\n```\r\nB) Code becomes very slow \r\n\r\nMy code:\r\n```\r\nnew_data = {k: data[k] for k in data.fields }\r\nnew_data[new_key] = new_arr \r\ndata = ak.Array(new_data)\r\n```\r\n\r\nusing `ak.zip` solves the error, but does performance is still very bad due to the dict comprehension. ",
        "createdAt":"2022-02-04T11:05:57Z",
        "number":2110082
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"How many fields does your array have?",
        "createdAt":"2022-02-04T11:18:09Z",
        "number":2110148
       },
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"This will vary. I'm writing a dumper which grabs any number of branches (specified by useR) out of root files, as well as create new ones. I'm currently testing with roughly 10 fields. ",
        "createdAt":"2022-02-04T11:21:41Z",
        "number":2110171
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Just a quick demo that gluing arrays together with [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html) (with `depth_limit=1`) is _O(1)_ in time and memory. (**Edit:** [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html) with `depth_limit=1` should be equivalent to the [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html) constructor, since the latter doesn't do any zipping.)\r\n\r\n```python\r\nIn [1]: import awkward as ak, numpy as np, psutil\r\n\r\nIn [2]: psutil.virtual_memory().percent\r\nOut[2]: 24.9\r\n\r\nIn [3]: fields_1e6 = {f\"f{i}\": ak.Array(np.random.normal(0, 1, int(1e6))) for i\r\n   ...: in range(10)}\r\n\r\nIn [4]: fields_1e8 = {f\"f{i}\": ak.Array(np.random.normal(0, 1, int(1e8))) for i\r\n   ...: in range(10)}\r\n\r\nIn [5]: psutil.virtual_memory().percent\r\nOut[5]: 73.6\r\n\r\nIn [6]: tmp = []\r\n\r\nIn [7]: %timeit tmp.append(ak.zip(fields_1e6, depth_limit=1))\r\n353 \u00b5s \u00b1 9.62 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n\r\nIn [8]: len(tmp)\r\nOut[8]: 8111\r\n\r\nIn [9]: psutil.virtual_memory().percent\r\nOut[9]: 73.9\r\n\r\nIn [10]: %timeit tmp.append(ak.zip(fields_1e8, depth_limit=1))\r\n354 \u00b5s \u00b1 8.98 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n\r\nIn [11]: len(tmp)\r\nOut[11]: 16222\r\n\r\nIn [12]: psutil.virtual_memory().percent\r\nOut[12]: 73.9\r\n```\r\n\r\nThe `fields_1e6` and especially `fields_1e8` are using a lot of memory, collectively 50% of what I have on my computer (because `psutil`'s memory reporting goes from 24.9 up to 73.6; that's real RAM, not swap). However, creating a new array by zipping them together is 350 \u00b5s regardless of whether I use the million-element arrays or the 100 million-element arrays. Also, they're persistently added to `tmp` and the memory usage doesn't appreciably increase. (The 0.3% increase could be web browser stuff in the background.)\r\n\r\nIf this had been assigning a field to an existing array or `depth_limit != 1` (with arrays that _have_ depth), then there would be some additional cost from checking to see if they broadcast, or creating a broadcasted array if it needs modification to do so, and those can be _O(n)_.\r\n\r\nThe rule is that the output of an operation reuses as much of the input's buffers as possible, and the columnar layout of the arrays in memory allows for more opportunities to apply this technique. (It's called \"structural sharing,\" a [technique made popular by Rich Hickey](https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/PersistentDataStructure.md) in the design of Clojure, but it works even better for [columnar data structures](https://dl.acm.org/citation.cfm?id=2814228.2814230).)\r\n\r\n-----------------\r\n\r\nOh wait! Your 10 fields is just a small sample. You might have as many fields as a TTree, which can be up to thousands (some CMS and ATLAS files I've seen have 1000\u20123000 TBranches).\r\n\r\nTime and memory costs _do_ scale with the number of fields, but _O(f)_, not _O(f \u00d7 n)_ for _f_ fields and arrays of length _n_. I'll do another run of that:\r\n\r\n```python\r\nIn [1]: import awkward as ak, numpy as np, psutil\r\n\r\nIn [2]: psutil.virtual_memory().percent\r\nOut[2]: 25.2\r\n\r\nIn [3]: fields_10 = {f\"f{i}\": ak.Array(np.random.normal(0, 1, int(1e6))) for i i\r\n   ...: n range(10)}\r\n\r\nIn [4]: fields_1000 = {f\"f{i}\": ak.Array(np.random.normal(0, 1, int(1e6))) for i\r\n   ...:  in range(1000)}\r\n\r\nIn [5]: psutil.virtual_memory().percent\r\nOut[5]: 73.7\r\n\r\nIn [6]: tmp = []\r\n\r\nIn [7]: %timeit tmp.append(ak.zip(fields_10, depth_limit=1))\r\n354 \u00b5s \u00b1 9.95 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n\r\nIn [8]: len(tmp)\r\nOut[8]: 8111\r\n\r\nIn [9]: psutil.virtual_memory().percent\r\nOut[9]: 73.9\r\n\r\nIn [10]: %timeit tmp.append(ak.zip(fields_1000, depth_limit=1))\r\n18.2 ms \u00b1 130 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [11]: len(tmp)\r\nOut[11]: 8922\r\n\r\nIn [12]: psutil.virtual_memory().percent\r\nOut[12]: 75.6\r\n\r\nIn [13]: 18.2 / 0.354\r\nOut[13]: 51.41242937853107\r\n```\r\n\r\nWe can see the time cost: [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html) on 1000 fields takes 18.2 ms, which is 50\u00d7 slower than an array with 100\u00d7 fewer fields (_almost_ proportional). The memory cost should be a little bigger, too, because a `std::map` of 1000 items is larger than a `std::map` of 10 items. (In Awkward 2.0, these will be Python dicts, not C++ `std::maps`, and there would be more referencing and less copying.) However, `%time` created a lot fewer of the big ones, so they're hard to compare.\r\n\r\nBut the objects that are larger and slower are the \"handles\" to the big data. If your numeric data in arrays outweighs your administrative metadata (as it should be), then all of that dict manipulation for 1000 fields won't be a significant cost.\r\n\r\n**So\u2014what's \"slow\" and how do you measure it?** Sometimes, people use small arrays like `[[1, 2, 3], [], [4, 5]]` for performance tests and compare \u00b5s times to conclude that an operation is too slow, but this isn't the right metric if you're eventually going to be working on large-_n_ datasets.",
        "createdAt":"2022-02-04T14:54:41Z",
        "number":2111713
       }
      ],
      "totalCount":7
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-26T19:26:08Z",
  "number":1252,
  "title":"Asigning a field to a float",
  "url":"https://github.com/scikit-hep/awkward/discussions/1252"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi,\r\nI ran into this issue when I simply wanted to check if an awkward array is empty or not after I filtered data from it. My array had a type that looks like:\r\n```\r\nN  * {\"field1\": X, \"field2\": var*Y}\r\n```\r\nwhich means that `ak.size` fails to compute the array size. I am wondering if there is a method to extract the  value `N` from the array without turning it into a list of lists? \r\n\r\nThanks,\r\nMo",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"`ak.size` is a function that implements `np.size` when the arrays are rectilinear. In your case, you do not have a rectilinear array, so this fails. The length `N` is just `len(array)`. For more complex length calculations there are `ak.count` and `ak.num`, which return slightly different length information that you can use.",
     "createdAt":"2022-01-30T13:30:24Z",
     "number":2076043,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"That's right, [ak.count](https://awkward-array.readthedocs.io/en/latest/_auto/ak.count.html) and [ak.num](https://awkward-array.readthedocs.io/en/latest/_auto/ak.num.html) are the counting functions. You likely want [ak.num](https://awkward-array.readthedocs.io/en/latest/_auto/ak.num.html) for any \"how many XYZs do I have?\" questions, since this function treats records (`{\"field1\": X, \"field2\": var*Y}` objects) as units, whereas [ak.count](https://awkward-array.readthedocs.io/en/latest/_auto/ak.count.html) is a reducer that cuts across objects, as described more fully in the [ak.sum](https://awkward-array.readthedocs.io/en/latest/_auto/ak.sum.html) documentation.\r\n\r\nSince your type is just\r\n\r\n```\r\nN  * {\"field1\": X, \"field2\": var*Y}\r\n```\r\n\r\nand I take it you want to know \"`N`\", just do `len(array)`. It happens to be the case that [ak.num](https://awkward-array.readthedocs.io/en/latest/_auto/ak.num.html) with `axis=0` is the same as `len(array)`.\r\n\r\n----------------\r\n\r\nAlso, if you made the `array` with cuts by\r\n\r\n```python\r\narray = array_without_cuts[cuts]\r\n```\r\n\r\nthen you can also do\r\n\r\n```python\r\nnp.count_nonzero(cuts)\r\n```\r\n\r\nto get the number of entries that would pass without even applying the cut.",
        "createdAt":"2022-01-31T20:50:06Z",
        "number":2084594
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-30T13:26:28Z",
  "number":1268,
  "title":"Extracting outermost size of an array?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1268"
 },
 {
  "author":{
   "login":"cesarecazzaniga"
  },
  "body":"Dear experts,\r\n\r\nI cannot find a straightforward way to make a calculation using ak array operations for a particular task. \r\nI post here the formula I would like to compute.\r\n\r\n![Screenshot 2022-01-07 at 14 51 22 png](https://user-images.githubusercontent.com/75684123/151842233-922dde81-9a00-4319-9754-fd0851eec0ff.jpeg)\r\n\r\n\r\nAt the moment I tried with for loops, but it is very inefficient, and also with Numba some operations are not supported. Can you suggest some fast way to implement this please?\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Could you share some example arrays so that we can work out how your problem maps to this formula?",
     "createdAt":"2022-01-31T18:23:27Z",
     "number":2083654,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"cesarecazzaniga"
        },
        "body":"Hi, so the array has the following structure for example:\r\n\r\n`[[{pt: 23.8, eta: -1.27, phi: 1.35, mass: 0, pdgId: 11, rapidity: -1.27}, {....}], [...]]`\r\n\r\nSo, in each event we have more particles, and we have the information you see above for each particle. Let's say we want to calculate the isolation of the particles \"P\" with a certain feature (eg pdgId = 11, not all the particles \"i\" in the list have this pdgId). \r\n\r\n\r\n",
        "createdAt":"2022-01-31T19:16:02Z",
        "number":2084051
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I'm still not fully sure what all of your quantities mean. Perhaps you could share a loop version?\r\n\r\nIn general, you can use boolean slicing or array masking to cut out particles that do not meet the relevant criteria, e.g.\r\nLet's assume your array is `N * var`, where `N` is the number of events, and `var` is the multiplicity of each event.\r\n\r\nTo get a boolean slice that takes only the particles with pdgId `i`, we can write\r\n```python\r\nis_pdgid_i = event.pdgId == i\r\n```\r\n",
        "createdAt":"2022-01-31T19:34:27Z",
        "number":2084184
       },
       {
        "author":{
         "login":"cesarecazzaniga"
        },
        "body":"So, when I tried to construct the `ak.ArrayBuilder()`, I did something like this:\r\n\r\n```\r\ndef compute_builder_isolation(builder,jet_pf_cands_plus_leptons,leptons_pdgid,pt_min,R):                                                                                                                                                                                                                   \r\n   for event_particles_list in jet_pf_cands_plus_leptons:                                                                                                                                                                          \r\n     builder.begin_list()\r\n     for P in range(len(event_particles_list)):\r\n      isolation = -99999.\r\n      if (event_particles_list[P].pdgId == leptons_pdgid):\r\n       for i in range(len(event_particles_list)):\r\n           sum = 0\r\n           if i != P:\r\n             delta_r_iP = np.sqrt((event_particles_list[i].phi - event_particles_list[P].phi)**2 + (event_particles_list[i].eta - event_particles_list[P].eta)**2)\r\n             if event_particles_list[i].pt > pt_min and delta_r_iP < R:\r\n               sum += event_particles_list[i].pt\r\n       isolation = sum/event_particles_list[P].pt\r\n      builder.append(isolation)\r\n     builder.end_list()\r\n\r\n   return builder \r\n```\r\n\r\n",
        "createdAt":"2022-01-31T19:48:54Z",
        "number":2084267
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"The purpose of [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html) is to convert non-columnar data into columnar data. If `jet_pf_cands_plus_leptons` is already an Awkward Array, there is no reason to bring ArrayBuilder into it. (For instance, even if you are unconcerned about the time the computation will take and will do it with nested for loops, you could have those loops create Python lists of dicts and pass them to [ak.from_iter](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_iter.html).) Oh! Maybe you're setting this up to run in Numba, and so ArrayBuilder is the only way to get it out of the Numba-compiled function?\r\n\r\nAnother thing to know is that you can avoid having to calculate \u0394R yourself by installing and using [Vector](https://github.com/scikit-hep/vector):\r\n\r\n```python\r\nimport vector\r\nvector.register_awkward()\r\n\r\njet_pf_cands_plus_leptons = ak.with_name(jet_pf_cands_plus_leptons, \"Momentum4D\")\r\n\r\njet_pf_cands_plus_leptons.deltaR(...)   # put another array of vectors in the \"...\"\r\n```\r\n\r\n-------\r\n\r\nAnyway, it looks to me like what you want to do is\r\n\r\n```python\r\npdgid_cut = (jet_pf_cands_plus_leptons.pdgId == leptons_pdgid)\r\npt_cut = (jet_pf_cands_plus_leptons.pt > pt_min)\r\n\r\ngood_leptons = jet_pf_cands_plus_leptons[pdgid_cut & pt_cut]\r\n```\r\n\r\nto get `good_leptons` that have the right `pdgid` (note: do you want both `pdgid` and `-pdgid`? if so, use `abs`) and high enough `pt`.\r\n\r\nNow you're going to want to consider all pairs, indexed with `P` and `i`, of these `good_leptons` per event. I would normally point you to [ak.combinations](https://awkward-array.readthedocs.io/en/latest/_auto/ak.combinations.html), which makes exactly the \"N choose 2\" pairs without repeating that you want, except that you're going to want to sum over all leptons `i` _for a given_ `P`, so they're going to need to be grouped by `P`. I think the clearest way of doing that is to use [ak.cartesian](https://awkward-array.readthedocs.io/en/latest/_auto/ak.cartesian.html) with `nested=True` and we'll exclude the cases in which `i == P` (they have \u0394R == 0, after all).\r\n\r\nSince I don't have your dataset, I'm going to make a simplified one with the relevant features:\r\n\r\n```python\r\n>>> good_leptons = ak.Array([\r\n...     [{\"x\": 0, \"y\": 1, \"z\": 2}],\r\n...     [{\"x\": 0, \"y\": 1, \"z\": 2}, {\"x\": 1, \"y\": 2, \"z\": 3}],\r\n...     [{\"x\": 0, \"y\": 1, \"z\": 2}, {\"x\": 1, \"y\": 2, \"z\": 3}, {\"x\": 2, \"y\": 3, \"z\": 4}],\r\n...     [{\"x\": 0, \"y\": 1, \"z\": 2}, {\"x\": 1, \"y\": 2, \"z\": 3}, {\"x\": 2, \"y\": 3, \"z\": 4}, {\"x\": 3, \"y\": 4, \"z\": 5}],\r\n... ], with_name=\"Momentum3D\")\r\n>>> good_leptons\r\n<MomentumArray3D [[{x: 0, y: 1, z: 2}, ... y: 4, z: 5}]] type='4 * var * Momentu...'>\r\n```\r\n\r\n[ak.cartesian](https://awkward-array.readthedocs.io/en/latest/_auto/ak.cartesian.html) takes a two lists, like \"A, B, C\" and \"1, 2\", and returns their Cartesian product, like \"A-1, A-2, B-1, B-2, C-1, C-2\". Setting `nested=True` turns the input array of lists of objects into an output array of lists of lists of objects\u2014one level deeper\u2014so that the output can be grouped by one of the input values: \"[A-1, A-2], [B-1, B-2], [C-1, C-2]\". We'll want that so that we can ask for the sum over each inner list and get the sum over A, followed by the sum over B, followed by the sum over C. [ak.unzip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.unzip.html) splits the left and right halves of each pair into separate arrays so that we can use them both in calculations (such as the \u0394R calcuation).\r\n\r\n```python\r\n>>> leptons_P, leptons_i = ak.unzip(ak.cartesian([good_leptons, good_leptons], nested=True))\r\n\r\n>>> leptons_P.x.tolist()\r\n[\r\n    [[0]],\r\n    [[0, 0], [1, 1]],\r\n    [[0, 0, 0], [1, 1, 1], [2, 2, 2]],\r\n    [[0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]\r\n]\r\n>>> leptons_i.x.tolist()\r\n[\r\n    [[0]],\r\n    [[0, 1], [0, 1]],\r\n    [[0, 1, 2], [0, 1, 2], [0, 1, 2]],\r\n    [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]\r\n]\r\n```\r\n\r\nSee how the innermost numbers are in same-valued groups for `leptons_P` and in different-valued groups for `leptons_i`? When we do any calculations involving elements from `leptons_P` and `leptons_i`, we'll be doing it on all combinations as though they're in two nested `for` loops.\r\n\r\nNow that we have these two collections, we can calculate the \u0394R between them:\r\n\r\n```python\r\n>>> deltaR = leptons_P.deltaR(leptons_i)\r\n\r\n>>> deltaR.tolist()\r\n[\r\n    [[0.0]],\r\n    [[0.0, 0.5749801549254422], [0.5749801549254422, 0.0]],\r\n    [[0.0, 0.5749801549254422, 0.763476592636774], [0.5749801549254422, 0.0, 0.1924914766026644], [0.763476592636774, 0.1924914766026644, 0.0]],\r\n    [[0.0, 0.5749801549254422, 0.763476592636774, 0.8545361946077011], [0.5749801549254422, 0.0, 0.1924914766026644, 0.28587761696419933], [0.763476592636774, 0.1924914766026644, 0.0, 0.09352784779524788], [0.8545361946077011, 0.28587761696419933, 0.09352784779524788, 0.0]]\r\n]\r\n```\r\n\r\nWe don't want the \u0394R that are zero, because they're the same particle, and we don't want values larger than `R`. That's another cut:\r\n\r\n```python\r\n>>> pairs_cut = (0 < deltaR) & (deltaR < 0.5)\r\n>>> pairs_cut\r\n<Array [[[False]], ... True, True, False]]] type='4 * var * var * bool'>\r\n```\r\n\r\nand it can be applied to these deeply nested leptons:\r\n\r\n```python\r\n>>> isolated_i = leptons_i[pairs_cut]\r\n\r\n>>> isolated_i.tolist()\r\n[\r\n    [[]],\r\n    [[], []],\r\n    [[], [{'x': 2, 'y': 3, 'z': 4}], [{'x': 1, 'y': 2, 'z': 3}]],\r\n    [[], [{'x': 2, 'y': 3, 'z': 4}, {'x': 3, 'y': 4, 'z': 5}], [{'x': 1, 'y': 2, 'z': 3}, {'x': 3, 'y': 4, 'z': 5}], [{'x': 1, 'y': 2, 'z': 3}, {'x': 2, 'y': 3, 'z': 4}]]\r\n]\r\n```\r\n\r\nBecause we wanted to sum over the `pt` of those leptons in `i` with these cuts:\r\n\r\n```python\r\n>>> sums_over_i = ak.sum(isolated_i.pt, axis=-1)\r\n\r\n>>> sums_over_i.tolist()\r\n[\r\n    [0.0],\r\n    [0.0, 0.0],\r\n    [0.0, 3.605551275463989, 2.23606797749979],\r\n    [0.0, 8.60555127546399, 7.23606797749979, 5.841619252963779]\r\n]\r\n```\r\n\r\nNotice something about this sum, though: it's an array of lists of numbers\u2014no more \"lists of lists\". That's because summation turns each of the inner lists into a number, and now we're back to the same multiplicity as `good_leptons`. Your `isolation` variable is therefore\r\n\r\n```python\r\n>>> isolation = sums_over_i / good_leptons.pt\r\n\r\n>>> isolation.tolist()\r\n[\r\n    [0.0],\r\n    [0.0, 0.0],\r\n    [0.0, 1.6124515496597098, 0.6201736729460423],\r\n    [0.0, 3.8485195271594996, 2.0069241635091153, 1.1683238505927558]\r\n]\r\n```\r\n\r\nThe `good_leptons` that I constructed was fake, intended to make the step-by-step logic of this more clear. When you try to apply this with your leptons, _do it interactively in a terminal, IPython, or Jupyter notebook!_ It will be absolutely necessary to be able to interact with the data to get this right. If you put it all in a script and run it, you'll very likely run into some minor indexing problem that would be resolvable if you could just look at the data. It would be sufficient to take a sample of 10 events or less\u2014that would be enough to figure out if you're aligning the indexes correctly.\r\n\r\n-------\r\n\r\nI'm just realizing, at the end of all this, that you might have the whole thing working in Numba. Is that true? If so, it isn't necessary to convert it to a columnar form\u2014usually, you do the columnar analysis (with [ak.cartesian](https://awkward-array.readthedocs.io/en/latest/_auto/ak.cartesian.html) and all that) because you get to see your intermediate steps more readily than when it's all hidden inside a `for` loop, especially if it's compiled by Numba.\r\n\r\nAnyway, now you can compare the `for` loop form and the columnar form, to see how the two approaches differ.",
        "createdAt":"2022-02-01T01:18:31Z",
        "number":2085723
       },
       {
        "author":{
         "login":"cesarecazzaniga"
        },
        "body":"thanks a lot for the help !",
        "createdAt":"2022-02-01T13:21:47Z",
        "number":2088845
       },
       {
        "author":{
         "login":"cesarecazzaniga"
        },
        "body":"I think this is a very nice solution, and works.",
        "createdAt":"2022-02-01T13:22:35Z",
        "number":2088851
       }
      ],
      "totalCount":6
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-31T17:24:00Z",
  "number":1272,
  "title":"Calculating isolation",
  "url":"https://github.com/scikit-hep/awkward/discussions/1272"
 },
 {
  "author":{
   "login":"cesarecazzaniga"
  },
  "body":"Dear experts,\r\n\r\nI have a question regarding the implementation of the following calculation with awkward arrays. \r\nLet's say I have an array with electrons for each event (with pt, phi, eta, pdgid, charge, etc...), and I want to calculate the invariant mass of all the di-electron pairs with the correct charge configurations. Since the choice of the correct charge configuration is not unique, then I would like to pick up the configuration which minimise a metric f. Let's say the metric f could be the sum of the distances delta R ^2 for each given couple in a given charge configuration (not sure about this metric choice, but I had a try on a c++ code and seems to reasonably work .... but maybe there can be better possibilities).\r\n\r\nSo, let' say I have 4 electrons: e1+, e2-, e3+, e4- , first I would like to have all the allowed couples (+,-): {(1+,2-) , (3+,4-)} or {(1+,4-),(2-,3+)}. \r\nAfter that I want to calculate for each of charge configuration the metric f, eg. : f({(1+,2-) , (3+,4-)}) = deltaR(1,2)^2 + deltaR(3,4)^2.\r\nFinally, I want to find the unique charge configuration that minimise the given metric.\r\n\r\nCould you please guide me a bit on how to implement this ? \r\n\r\nLet me know, and thanks in advance for the help !\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Hi @cesarecazzaniga My understanding of your problem is:\r\n- You have an array of type `N * var * {...}`, where `N` is the number of events, and `var` is the number of electrons per-event.\r\n- You want to find _all_ possible electron-electron pairings per-event that satisfy the charge configuration requirement. \r\n- You want to choose the best of these possible configurations by minimizing a metric.\r\n\r\nIf I've correctly described your problem, then at a high-level it sounds like you want to generate an array of `N * var * var * {...}` proposals, evaluate your metric, and then extract the minimum. This last step is quite straightforward, especially with the `vector` library that provides an implementation of `deltaR` (and `deltaR2`) for `Momentum4D` vectors, e.g.\r\n\r\n```python\r\nproposal = ...\r\n\r\nmetric = np.sum(proposal.slot0.deltaR2(proposal.slot1), axis=-1)\r\ni_best = ak.argmin(metric, keepdims=True, axis=-1)\r\nbest = proposal[i_best]\r\n```\r\n\r\nThe difficulty here is, I imagine, generating your proposals. \r\n\r\nOne option is to take all of the permutations over the sequence, and then the valid pairs within that sequence. We don't have a built-in method for generating random permutations, but you could implement Heap's algorithm to compute them with Numba. Still, this is likely *not* a good solution - for a start, it is explosively combinatorial, so I'd like to clarify that this is what you're trying to do first!\r\n\r\nHere's an example of generating the permutations. I probably would not do this in actual code, because of how quick it is to consume all of your RAM\r\n\r\nhttps://gist.github.com/agoose77/0c4ef3ee7582aebf75871e5974f4af72\r\n\r\nNote that generating the permutations this way includes repeats - this doesn't account for the fact that the order between pairs does not matter, or that (a, b) is the same pair as (b, a)\r\n",
     "createdAt":"2022-02-14T14:56:31Z",
     "number":2173858,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"cesarecazzaniga"
        },
        "body":"Thanks a lot for your answer !\r\n\r\nyes, I think you described correctly the problem, and also spotted that the real problem could be generating the proposal.\r\nIn principle I would like to find have all possible pairings without repetitions per each event, and then select the ones with the correct charge configuration. So only (+,-) pairs would be allowed while (+,+) pairs would not. Do you think would be possible to do that in an efficient way ?",
        "createdAt":"2022-02-14T15:53:59Z",
        "number":2174271
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"OK, I can see that we can use a much simpler solution in this case!\r\n\r\nThe problem with taking naive permutations of the sequence is that it treats `(x, y)` as distinct from `(y, x)`, and similarly `[(x, y), (p, q)]` as distinct from `[(p, q), (x, y)]`. We can instead take permutations of either the positive or negative arrays, and zip those permutations with the other array to build pairs:\r\n\r\nIn this notebook, I've implemented a different `argpermutations` that computes `nPk` between the largest and smallest sublist. Then it builds a record array of tuples that contain the appropriate indices for each charge type.\r\n\r\nUnlike the existing `argXXX` functions, this is not so convenient to use, because it requires that you add a dimension to your arrays before indexing them with these permuted indices. The second array that is passed to `broadcast_arrays` is just there to provide the correct shape such that the `np.newaxis` in `pos[..., np.newaxis, :]` is broadcasted to the number of configurations. \r\n\r\nhttps://gist.github.com/agoose77/fd6b6a0cdc41fb361e4b33b950cb8e80\r\n\r\nI'm not sure if this is the *best* API for this solution. I had to implement `_permutations` (which is fairly essential), but perhaps we could find a nicer way to write `argpermutations` in order to avoid the `ak.broadcast_arrays` confusion.",
        "createdAt":"2022-02-14T19:50:43Z",
        "number":2175868
       },
       {
        "author":{
         "login":"cesarecazzaniga"
        },
        "body":"thanks a lot, I will try to implement with your method and come back to you !",
        "createdAt":"2022-02-14T20:32:17Z",
        "number":2176123
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"These are all good ideas.\r\n\r\nHere's one more: since there's an upper limit on the number of leptons you're considering (4), you could put each of the combinatorial possibilities into its own array and use [ak.where](https://awkward-array.readthedocs.io/en/latest/_auto/ak.where.html) to pick the best elements from each pair of arrays. (Although it would be nice if there was a version of this function that took more than 2 arguments, you can pick the best of `2**n` arrays with only `n` calls to [ak.where](https://awkward-array.readthedocs.io/en/latest/_auto/ak.where.html).)\r\n\r\n@936-BCruz's solution to a very similar (the same?) problem is here: https://github.com/936-BCruz/Translating-Analyses-Into-Prototype-Analysis-Systems/blob/main/Higgs%20to%204%20Leptons%20Analysis.ipynb",
        "createdAt":"2022-02-14T21:30:52Z",
        "number":2176477
       },
       {
        "author":{
         "login":"cesarecazzaniga"
        },
        "body":"Hello, so when running the code with your implementation (@agoose77 ), I run into troubles with some initial charge configurations like \r\n```\r\nx = ak.Array(\r\n    [\r\n        [\r\n         {\"id\": 0, \"charge\": +1}, \r\n         {\"id\": 1, \"charge\": +1},   \r\n        ],\r\n        \r\n        [\r\n            {\"id\": 0, \"charge\": -1},\r\n            {\"id\": 1, \"charge\": 1},\r\n            {\"id\": 2, \"charge\": 1},\r\n            {\"id\": 3, \"charge\": -1},\r\n        ],\r\n        \r\n    ]\r\n) \r\n```\r\n\r\n\r\nSo, for this case (but there are also others where you have for example in one event only one lepton) I get the following error message at this stage here of your code:\r\n\r\n`pos_, _ = ak.broadcast_arrays(pos[..., np.newaxis, :], ix.slot0[..., [0]])\r\nneg_, _ = ak.broadcast_arrays(neg[..., np.newaxis, :], ix.slot1[..., [0]])`\r\n\r\nthe error is like: \r\n\r\n`ValueError: in ListOffsetArray64 attempting to get 0, index out of range`\r\n\r\nDo you know how can I deal with this ?\r\n\r\n",
        "createdAt":"2022-02-15T23:44:29Z",
        "number":2184746
       },
       {
        "author":{
         "login":"cesarecazzaniga"
        },
        "body":"By the way ... I have not tried yet the solution proposed by @jpivarski (thanks for also the nice reference code) , maybe I can have a try also with that one ...",
        "createdAt":"2022-02-15T23:48:44Z",
        "number":2184759
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Could you clarify whether there is an upper bound on the number of pairs? If so, the explicit method suggested by @jpivarski may be a clearer option.\r\n\r\nIn any case, to answer your question:\r\n\r\nThe purpose of `ak.broadcast_arrays` here is to \"grow\" the newly added (via `np.newaxis`) `1` dimension in `pos` so that the resulting array can be indexed by `ix`. Unlike other operations, indexing requires that the structure matches (rather than just be broadcastable), so we have to do this explicitly.\r\n\r\nA clearer solution that I didn't think about at the time is to use `ak.num` instead of a single-value slice, e.g.\r\n```python3\r\npos_, _ = ak.broadcast_arrays(pos[..., np.newaxis, :], ak.num(ix.slot0, axis=-1))\r\nneg_, _ = ak.broadcast_arrays(neg[..., np.newaxis, :], ak.num(ix.slot1, axis=-1))\r\n```\r\nThe result of `ak.num(..., axis=-1)` actually has one-fewer dimensions than `pos[..., np.newaxis, :]`. For your example:\r\n```pycon\r\n>>> pos[..., np.newaxis, :].type\r\n2 * 1 * var * {\"id\": int64, \"charge\": int64}\r\n>>> ak.num(ix.slot0, axis=-1)\r\n2 * var * int64\r\n```\r\nThis is fine, though, because `ak.broadcast_arrays` will _left_ broadcast these so that the `2`s line up, i.e. it will add a 1-sized dimension to the end of the `ak.num(...)` argument:\r\n```python\r\n2 * var * 1 * int64\r\n```\r\nso that we have:\r\n```pycon\r\n>>> pos_.type\r\n2 *  var  * var * {\"id\": int64, \"charge\": int64}\r\n#     ^ the 1 is grown to var\r\n```\r\n",
        "createdAt":"2022-02-16T11:44:33Z",
        "number":2187971
       },
       {
        "author":{
         "login":"cesarecazzaniga"
        },
        "body":"sorry, I have not specified an upper bound on the number of pairs .... because in my specific case it would be better not to assume it",
        "createdAt":"2022-02-16T13:43:21Z",
        "number":2188695
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Not having an upper bound on combinations to check means you can't do the explicit combinatorics in my suggestion. Brian's notebooks do not use explicit combinatorics, but they do put an upper limit on the number of particles, so it's more general than the problem it's solving. That's why I've had this \"explicit combinatorics\" idea in the back of my head for a while.",
        "createdAt":"2022-02-16T17:46:55Z",
        "number":2190538
       }
      ],
      "totalCount":9
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-02-14T10:33:29Z",
  "number":1285,
  "title":"Calculate all configurations of leptons with opposite charge",
  "url":"https://github.com/scikit-hep/awkward/discussions/1285"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi,\r\n\r\nThis might be a stupid question, but I have an awkward array that looks like:\r\n```\r\narr = [ [1,2,3], [4,5], ...] \r\n```\r\nand I would like to filter/mask the inner values of the array (e.g. mask elements <= 1 such that `arr -> [[None,2,3],[4,5] `). I know how to do this if I have a field, since it's a simple `arr.field > X', but in this case the array has no fields. \r\n\r\nIs this possible? \r\n\r\nThank you very much.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"The masking and filtering features in NumPy / Awkward are actually orthogonal (unrelated) to the fields feature. \r\n\r\nIf you have an array e.g.\r\n```python\r\nx = ak.Array([\r\n    [1,2,3],\r\n    [4, 5],   \r\n    [6, 7, 8, 9]\r\n])\r\n```\r\nthen to mask _out_ the elements `<=1`, you can just use `.mask`, e.g.\r\n```python\r\ny = x.mask[x > 1]\r\n```\r\n\r\nThe [top-level function `ak.mask`](https://awkward-array.readthedocs.io/en/latest/_auto/ak.mask.html?highlight=mask#ak.mask), unlike `ak.Array.mask` actually accepts a `valid_when` argument that you can use to invert the meaning of the boolean mask array (i.e. if you want `True` to mean \"set this value to `None`\").\r\n\r\nFor more information on filtering and masking, I recommend reading [the docs](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html?highlight=mask#filtering) on the topic :)",
     "createdAt":"2022-02-14T18:25:38Z",
     "number":2175290,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-02-14T17:57:36Z",
  "number":1287,
  "title":"Filtering a Jagged Array with no fields",
  "url":"https://github.com/scikit-hep/awkward/discussions/1287"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@swishdiff has started developing infrastructure to perform Awkward Array calculations on GPUs. In doing this, we're facing some questions that would have implications for users. One of these deals with concurrency.\r\n\r\nWhen you launch a CUDA kernel in C++, the kernel runs asynchronously, returning control to the C++ driving program before the calculation is complete (potentially). But Awkward Array operations are eager: they finish calculating before returning control to Python. ([dask-awkward](https://github.com/ContinuumIO/dask-awkward/) is another story.) Since the whole point of a GPU backend is for better utilization of resources, we may want to adopt that model.\r\n\r\nEach Awkward operation (`ak.*` functions, NumPy ufuncs, slicing, etc.) assumes that the values in an array are valid, so at minimum there needs to be a `cudaSynchronize` between each operation (and also between the intermediate steps that comprise a high-level operation). But if we put the `cudaSynchronize` at the _beginning_ each Awkward operation,\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/155616901-522408ae-b273-4c71-9011-bfaf1b177776.png)\r\n\r\nwe can get strictly greater utilization than if we put the `cudaSynchronize` at the _end_ of each operation,\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/155616960-3d2c1a15-8782-4bc8-a761-bcbd67141af0.png)\r\n\r\nwhere \"CPU\" is returning to Python and doing Python stuff, while \"GPU\" is doing numerical calculations.\r\n\r\nHowever, there's a consequence: if control is returned to Python before the numerical calculation completes, then any errors deriving from data in the arrays can't be raised as Python exceptions. If, for instance, you're trying to broadcast together two arrays that have different lengths, you'll get an error about that (the length is represented in Python, on the CPU), but if they have the same outer length and incompatible internal lengths, you won't get an error. That includes things like\r\n\r\n```python\r\nfirst_muon_in_each_event = events.muons[:, 0]\r\n```\r\n\r\nwhen some events might have zero muons.\r\n\r\nCuPy also returns control to Python before the GPU calculation completes, so I wondered what CuPy does. Here's an example of an operation that can only raise an error if you look at the data in the array: slicing by an array of integer indexes, some of which are out of bounds.\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> array = np.arange(1000000) * 1.1\r\n>>> array\r\narray([0.0000000e+00, 1.1000000e+00, 2.2000000e+00, ..., 1.0999967e+06,\r\n       1.0999978e+06, 1.0999989e+06])\r\n>>> indexes = np.arange(0, 1000000, 10)\r\n>>> indexes\r\narray([     0,     10,     20, ..., 999970, 999980, 999990])\r\n>>> array[indexes]\r\narray([0.000000e+00, 1.100000e+01, 2.200000e+01, ..., 1.099967e+06,\r\n       1.099978e+06, 1.099989e+06])\r\n>>> array[indexes + 5]\r\narray([5.5000000e+00, 1.6500000e+01, 2.7500000e+01, ..., 1.0999725e+06,\r\n       1.0999835e+06, 1.0999945e+06])\r\n>>> array[indexes + 15]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nIndexError: index 1000005 is out of bounds for axis 0 with size 1000000\r\n```\r\n\r\nOkay, NumPy raises an error if the slice is wrong. What does CuPy do?\r\n\r\n```python\r\n>>> import cupy as cp\r\n>>> array = cp.arange(1000000) * 1.1\r\n>>> array\r\narray([0.0000000e+00, 1.1000000e+00, 2.2000000e+00, ..., 1.0999967e+06,\r\n       1.0999978e+06, 1.0999989e+06])\r\n>>> indexes = cp.arange(0, 1000000, 10)\r\n>>> indexes\r\narray([     0,     10,     20, ..., 999970, 999980, 999990])\r\n>>> array[indexes]\r\narray([0.000000e+00, 1.100000e+01, 2.200000e+01, ..., 1.099967e+06,\r\n       1.099978e+06, 1.099989e+06])\r\n>>> array[indexes + 5]\r\narray([5.5000000e+00, 1.6500000e+01, 2.7500000e+01, ..., 1.0999725e+06,\r\n       1.0999835e+06, 1.0999945e+06])\r\n>>> array[indexes + 15]\r\narray([1.6500000e+01, 2.7500000e+01, 3.8500000e+01, ..., 1.0999835e+06,\r\n       1.0999945e+06, 5.5000000e+00])\r\n```\r\n\r\nIt does not raise an error! The last element should be beyond the bounds of the array, but they evidently \"wrapped around,\" returning 5.5, a value near the beginning. This is on purpose [documentation](https://docs.cupy.dev/en/latest/user_guide/difference.html), presumably because of this issue\u2014it can either return control to Python before it finishes processing or it can detect the error, but not both.\r\n\r\nWe could take a similar policy, but there are _more_ ways that Awkward operations can encounter errors in the midst of processing. How do people feel about the possibility of running calculations that suppress errors\u2014doing some random thing like CuPy's wrap-around when the CPU-based calculation would raise an error?\r\n\r\nAnother possibility that @swishdiff and I discussed is to set a flag and raise a Python exception when you try to do the _next_ operation. That is, if you compute A, B, and C, and B is invalid, you get the error message when C begins. It sounds like that could make debugging difficult, but we could put the name of operation B (e.g. `ak.this` or `ak.that`) in the error message.\r\n\r\nWhat does everyone think?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"kpedro88"
     },
     "body":"> It sounds like that could make debugging difficult, but we could put the name of operation B (e.g. `ak.this` or `ak.that`) in the error message.\r\n\r\nError messages are often incomprehensible throughout scientific Python (not highlighting or singling out Awkward by any means: numpy, pandas, matplotlib all suffer from this at least as much), so I tend to doubt this would be much worse (without having tried it, of course).",
     "createdAt":"2022-02-24T22:43:22Z",
     "number":2247360,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"NJManganelli"
        },
        "body":"I think given how incomprehensible in general error messages can be, if the error is raised at C with a warning that it could be operation B that's actually faulting, that will make things orders of magnitude better. I would happily live with that, at least... ",
        "createdAt":"2022-02-24T23:03:31Z",
        "number":2247429
       },
       {
        "author":{
         "login":"matthewfeickert"
        },
        "body":"I haven't thought about the technical limitations that can be hit here, though @jpivarski and @swishdiff have, so my thought in general is that if error messages can get improved this is a huge bonus all around.\r\n\r\nI'm primarily a consumer of GPU library APIs, not a designer or developer of them, so I think I would agree with @Moelf in https://github.com/scikit-hep/awkward-1.0/discussions/1321#discussioncomment-2247433 that if the CuPy behavior can be avoided (for basically anything else) that is probably a benefit.",
        "createdAt":"2022-02-25T06:50:53Z",
        "number":2249434
       }
      ],
      "totalCount":2
     }
    },
    {
     "author":{
      "login":"Moelf"
     },
     "body":">It sounds like that could make debugging difficult, but we could put the name of operation B (e.g. ak.this or ak.that) in the error message.\r\n\r\nmy 2c is this still better than CuPy's behavior. \r\n\r\nIf we can reach deeper, (idk how deep is awkward plan to interface with CUDA), you can certainly check for exception during a non-blocking synchronization. ([ref](https://github.com/JuliaGPU/CUDA.jl/blob/95eec21737c5058817fbb979fb879301a859553e/src/compiler/exceptions.jl#L26))",
     "createdAt":"2022-02-24T23:04:56Z",
     "number":2247433,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"It sounds like people are in favor of error messages, against following CuPy's lead in pretending they don't exist. I can get behind that.\r\n\r\nOur plans for CUDA are to have Awkward Arrays be manually copyable between main memory and GPU global memory, and all the high-level operations (slicing, `ak.*`, NumPy ufuncs and overloads) would be possible for the GPU-bound arrays just as they are for the CPU-bound arrays. The original plan was to do this without any difference in semantics: take a script using Awkward Array on the CPU, add\r\n\r\n```python\r\nsource = ak.to_backend(source, \"cuda\")\r\n```\r\n\r\nat the beginning of the script and\r\n\r\n```python\r\nsink = ak.to_backend(sink, \"cpu\")\r\n```\r\n\r\nat the end, and it would work the same.\r\n\r\nIf we put `cudaSynchronize` at the end of each operation, it can be _exactly_ the same, at a cost of never being able to use both CPU and GPU at the same time. (How big of a cost would that be in practice? We don't know. It depends on how fast the GPU numerical math is compared to Python \"administrative work\" on the CPU.)\r\n\r\nIf we put `cudaSynchronize` at the beginning of each operation, it would be exactly the same for process that do not raise an error. That's something. Also, we gain the ability to utilize both CPU and GPU at the same time. The prospect of raising an error in step \"C\" when the actual mistake was made in step \"B\" is a user-visible, semantic difference, but it sounds like you all are saying that that level of confusion is \"in the noise\" for day-to-day confusion with error messages.\r\n\r\n> you can certainly check for exception during a non-blocking synchronization\r\n\r\nIn addition to any CUDA errors (\"GPU is unplugged! Plug it back in!\"), the errors we're interested in are Awkward indexing errors, like `events.muons[:, 0]` when some events have no muons. We only know if the slice succeeded when all kernels that implement it have completed. Also, the output arrays of step \"B\" must be populated with valid output before \"C\" can begin. Kernels are purely side-effect driven, so step \"C\" has to be temporally after step \"B\", and there must be a `cudaSynchronize` between them.\r\n\r\nOh! I just got what you're saying: there's a CUDA call (different from `cudaSynchronize`?) that's sent from the CPU saying, \"Launch this new device kernel as soon as you're done with the previous device kernels.\" That would allow us to get even better occupancy:\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/155773944-1e99c42c-fd42-47b6-8507-a7f9b4a81197.png)\r\n\r\nThat certainly looks desirable, though it means that the error feedback could come even later. When step \"C\" is requested from the CPU and returns control to Python, step \"B\" (which has an error) might not have finished yet. The error flag could prevent \"C\" from doing any work: every device thread could start by checking the error flag and refusing to work if it's set, and then steps \"D\", \"E\", and \"F\" all quickly skip their work to report the error only at the end.\r\n\r\n**Revised question to everybody:** what if errors are only raised when\r\n\r\n```python\r\nsink = ak.to_backend(sink, \"cpu\")\r\n```\r\n\r\nis called? We can insert enough forensic information into the error state structure to say which high-level operation (slicing, `ak.*`, NumPy ufuncs and overloads) it was, possibly with a `repr` of its arguments, like\r\n\r\n```\r\nError occurred in\r\n\r\n    ak.whatever(\r\n        array=<Array:cuda type=\"1000000 * var * {pt: float32, eta: float32, phi: flo...\">,\r\n        some_weird_option=True,\r\n    )\r\n\r\nSpecific error message from the kernel. (A constant string with no specific-index information.)\r\n```\r\n\r\nand possibly include a line number in the source code calling `ak.whatever` using Python's inspect module. (That wouldn't help on the commandline or in Jupyter, and the source code calling `ak.whatever` might be Coffea internals.) However, as far as Python is concerned, this error was raised by `ak.to_backend`, not `ak.whatever`.\r\n\r\nIn this model, error handling is strictly global, but it looks like CuPy has only one CUDA context, so we may be limited to that from external constraints (we use CuPy). It may be that multi-GPU handling is off the table, too: that would have to be implemented by multiple Python processes (such as through Dask).\r\n\r\nOne thing that I like about raising GPU data-dependent errors in `ak.to_backend(sink, \"cpu\")` and nowhere else is that it avoids giving the misleading impression that the error happened in step \"C\". The nature of the error\u2014some indexing thing\u2014is not going to look like it might have come from trying to copy the array back.",
     "createdAt":"2022-02-25T19:34:23Z",
     "number":2253648,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"A follow-up to this story can be found here: https://github.com/scikit-hep/awkward-1.0/pull/1327#issuecomment-1058409361.",
        "createdAt":"2022-03-03T23:45:03Z",
        "number":2291720
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"> how deep is awkward plan to interface with CUDA\r\n\r\nI meant to answer this question, too, but got distracted by other things. There are three plans for Awkward-CUDA integration:\r\n\r\n  1. Make all operations (slicing, `ak.*`, NumPy ufuncs and overloads) work for GPU-resident arrays just as they do for arrays in main memory, as described above.\r\n  2. _Possibly, if (1) is not good enough:_ collect a DAG of operations to all get dispatched to the GPU at once when you call `.compute()`, just like Dask. Unfortunately, this this requires a different DAG from the one being developed in [dask-awkward](https://github.com/ContinuumIO/dask-awkward/), since the instructions that go to the GPU have to be \"run this kernel, then run that kernel,\" so the DAG nodes have to be kernels, but dask-awkward's is a DAG of high-level instructions, such as `ak.this` and `ak.that`. Of course we considered this early on, but concluded that the same DAG can't satisfy both goals\u2014batching kernels for a GPU and parallelizing or distributing general tasks\u2014at least not comfortably. [In a prototype](https://github.com/ContinuumIO/dask-awkward/pull/1), we discovered that the kernel-level DAG would be huge for typical workflows, not a compact message for communicating between distributed computers. Since this would be a major project, we're keeping an eye on it but not planning it at the moment. Hopefully, \"non-(CPU)-blocking synchronization\" can get us all the occupancy we need in plan (1).\r\n  3. Just as Awkward Arrays can be sent to host/CPU functions JIT-compiled by Numba, we want to be able to send Awkward Arrays to device/CUDA functions JIT-compiled by `numba.cuda`. This isn't currently possible, but it is something we plan on doing. Some work was needed on the Numba side to make this possible ([see original conversation](https://numba.discourse.group/t/making-awkward-arrays-work-in-the-cuda-target/63/3)), but some or all of that ought to be ready by now and I'm the foot-dragging one at the moment. This is independent of (1) and (2), but it might be a better user interface in some circumstances. (E.g. you get to fuse operations into a single kernel, though it's more work to set up such a function, just as it is for Numba on the CPU.)",
     "createdAt":"2022-02-26T19:45:52Z",
     "number":2257866,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":4
  },
  "createdAt":"2022-02-24T22:34:26Z",
  "number":1321,
  "title":"GPU backend, eagerness, and errors",
  "url":"https://github.com/scikit-hep/awkward/discussions/1321"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"Just like 1.8.0rc7, but this is not a release candidate. See the release candidates for all the changes with respect to 1.7.0.\r\n\r\nThe first non-release candidate to drop Python 2.7 and 3.5, and the first to support Python 3.10.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward-1.0/releases/tag/1.8.0'>1.8.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-03-01T22:14:25Z",
  "number":1330,
  "title":"1.8.0",
  "url":"https://github.com/scikit-hep/awkward/discussions/1330"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi,\r\n\r\nI have the following awkward array:\r\n```\r\nakarr = {\"px\": [1,2,3], \"py\": [4,5,6], \"pz\": [7,8,9], \"E\": [10,11,12]}\r\n\r\n```\r\nAnd I would like to add to this array another field which holds the four momenta  from these values in `akarr`. Does `awkward` support any of the LorentzVector object from other sci-hep packages? if so, how would I add this `lorentz_vec` field?\r\n\r\nThank you very much.\r\nMo",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"The [`vector` package](https://vector.readthedocs.io/en/latest/) provides a set of `LorentzXXX` classes, which exposes a suite of methods and properties relating specifically to Lorentz vectors.\r\n\r\n`vector` can take existing arrays, and figure out which kind of array to return to you according to the fields that it has. For example, given your `akarr`:\r\n```pycon\r\n>>> import awkward as ak\r\n>>> import vector\r\n>>> akarr = vector.zip({\"px\": [1,2,3], \"py\": [4,5,6], \"pz\": [7,8,9], \"E\": [10,11,12]})\r\n>>> akarr\r\n<MomentumArray4D [{x: 1, y: 4, z: 7, ... z: 9, t: 12}] type='3 * Momentum4D[\"x\":...'>\r\n```\r\n\r\nNote that the array has more than _just_ the Lorentz methods: it is a `MomentumArray4D` which pulls in lots of other classes:\r\n```pycon\r\n>>> akarr.__class__.__mro__\r\n(vector._backends.awkward_.MomentumArray4D,\r\n vector._backends.awkward_.MomentumAwkward4D,\r\n vector._methods.LorentzMomentum,\r\n vector._methods.SpatialMomentum,\r\n vector._methods.PlanarMomentum,\r\n vector._methods.Momentum,\r\n vector._methods.MomentumProtocolLorentz,\r\n vector._backends.awkward_.VectorAwkward4D,\r\n vector._backends.awkward_.VectorAwkward,\r\n vector._methods.Lorentz,\r\n vector._methods.Spatial,\r\n vector._methods.Planar,\r\n vector._methods.Vector4D,\r\n vector._methods.Vector,\r\n vector._methods.VectorProtocolLorentz,\r\n vector._methods.MomentumProtocolSpatial,\r\n vector._methods.VectorProtocolSpatial,\r\n vector._methods.MomentumProtocolPlanar,\r\n vector._methods.VectorProtocolPlanar,\r\n vector._methods.VectorProtocol,\r\n awkward.highlevel.Array,\r\n numpy.lib.mixins.NDArrayOperatorsMixin,\r\n collections.abc.Iterable,\r\n collections.abc.Sized,\r\n object)\r\n```\r\n\r\nThe `vector.zip` function is just a convenience function. You can do this by hand by first registering the Vector behaviours, so that Awkward can find them:\r\n```pycon\r\n>>> vector.register_awkward()\r\n```\r\n\r\nThen, using the `with_name` field in `ak.zip` to inform Awkward that the array should have the `Momentum4D` behavior:\r\n```pycon\r\n>>> ak.zip({\"px\": [1,2,3], \"py\": [4,5,6], \"pz\": [7,8,9], \"E\": [10,11,12]}, with_name=\"Momentum4D\")\r\n<MomentumArray4D [{px: 1, py: 4, pz: 7, ... E: 12}] type='3 * Momentum4D[\"px\": i...'>\r\n```\r\n\r\nIf you already have an array, and you just want to make it behave like `Momentum4D`, then you can use `vector.awk`:\r\n```pycon\r\n>>> arr = ak.zip({\"px\": [1,2,3], \"py\": [4,5,6], \"pz\": [7,8,9], \"E\": [10,11,12]})\r\n>>> vec = vector.awk(arr)\r\n>>> vec\r\n<MomentumArray4D [{x: 1, y: 4, z: 7, ... z: 9, t: 12}] type='3 * Momentum4D[\"x\":...'>\r\n```\r\n\r\nLike with `vector.zip`, `vector.awk` just figures out the name `Momentum4D` to use (from the fields in your array). You can also do this by hand:\r\n```pycon\r\n>>> arr = ak.zip({\"px\": [1,2,3], \"py\": [4,5,6], \"pz\": [7,8,9], \"E\": [10,11,12]})\r\n>>> vec = ak.with_name(arr, \"Momentum4D\")\r\n>>> vec\r\n```\r\n\r\nI'm not entirely clear on what field you're trying to add (your vector is _already_ a four-vector), but I imagine that you're most interested in using Vector, so that's the question I've answered :)",
     "createdAt":"2022-03-04T10:44:03Z",
     "number":2294393,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-03-04T10:00:36Z",
  "number":1342,
  "title":"Awkward array of Lorentz Vectors from Awkward arrays of components",
  "url":"https://github.com/scikit-hep/awkward/discussions/1342"
 },
 {
  "author":{
   "login":"masonproffitt"
  },
  "body":"Is there a short and easy top-level way to check if an `Array` has a tuple structure? That is, if the fields refer to elements of tuples. For a plain `RecordArray`, you can do:\r\n\r\n```python\r\n>>> a = ak.Array([(1, 2)])\r\n>>> a.layout\r\n<RecordArray length=\"1\">\r\n    <field index=\"0\">\r\n        <NumpyArray format=\"l\" shape=\"1\" data=\"1\" at=\"0x55e8e613deb0\"/>\r\n    </field>\r\n    <field index=\"1\">\r\n        <NumpyArray format=\"l\" shape=\"1\" data=\"2\" at=\"0x55e8e613ded0\"/>\r\n    </field>\r\n</RecordArray>\r\n>>> a.layout.istuple\r\nTrue\r\n```\r\n\r\nbut this doesn't work at higher depths:\r\n\r\n```python\r\n>>> b = ak.Array([[(1, 2)]])\r\n>>> b.layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 1]\" offset=\"0\" length=\"2\" at=\"0x55e8e613d600\"/></offsets>\r\n    <content><RecordArray length=\"1\">\r\n        <field index=\"0\">\r\n            <NumpyArray format=\"l\" shape=\"1\" data=\"1\" at=\"0x55e8e66aa9c0\"/>\r\n        </field>\r\n        <field index=\"1\">\r\n            <NumpyArray format=\"l\" shape=\"1\" data=\"2\" at=\"0x55e8e667b330\"/>\r\n        </field>\r\n    </RecordArray></content>\r\n</ListOffsetArray64>\r\n>>> b.layout.istuple\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'awkward._ext.ListOffsetArray64' object has no attribute 'istuple'\r\n```\r\n\r\nIn this case, you could do `b.layout.content.istuple`, but the point is that this is not generic to all `ak.Array`s. Another option is `b.fields == [str(i) for i in range(len(b.fields))]`, but this would lead to false positives for `RecordArray`s that have fields explicitly named `'0'`, `'1'`, etc.\r\n\r\nAm I missing a simple way to check this or does this feature not exist?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Right now, I we don't expose this information at the top-level. `ak.Array` has the `fields` attribute which returns `ak.operations.describe.fields(self)`, but we don't yet have an equivalent for `istuple`.\r\n\r\nIn `v2`, we are a bit better at making tuples and records behave predictably with respect to numeric (albeit in string form) fields, but there is still no function to test the entire layout for any tuple. #1351 is one solution for this.\r\n\r\nFor now, you can do this with a layout visitor:\r\n\r\n```python\r\ndef is_tuple(array):\r\n    layout = ak.to_layout(array, allow_record=True)\r\n\r\n    def visitor(layout):\r\n        if isinstance(layout, (ak.layout.Record, ak.layout.RecordArray)):\r\n            return layout.istuple\r\n        elif isinstance(\r\n            layout, ak._util.listtypes + ak._util.optiontypes + ak._util.indexedtypes\r\n        ):\r\n            return visitor(layout.content)\r\n        elif isinstance(layout, ak.layout.VirtualArray):\r\n            return visitor(layout.array)\r\n        elif isinstance(layout, ak._util.uniontypes):\r\n            return all(visitor(x) for x in layout.contents)\r\n        elif isinstance(layout, ak.layout.NumpyArray):\r\n            return False\r\n        else:\r\n            raise ValueError\r\n\r\n    return visitor(layout)\r\n```\r\n\r\nNote that unions over records as non-tuples are a less well defined concept (are they tuples if they have different fields?). Here I'm considering a union of tuples as a tuple, even if they have differing numbers of fields.\r\n",
     "createdAt":"2022-03-08T11:20:29Z",
     "number":2315779,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"> Am I missing a simple way to check this or does this feature not exist?\r\n\r\nThe feature doesn't exist. @agoose77's sample code would be a good implementation of it, but the `ak._util.recursively_apply` interface changes between version 1 and 2, so it would be better to not use that in downstream code. (The underscore in `_util` is supposed to hide it.) #516 is a request that the interface be made public, and we could do that in version 2, since the new interface is better thought-through.\r\n\r\n(We'd do it by adding a high-level function that calls `Content._recursively_apply` or `ak._v2._broadcasting.broadcast_and_apply`, depending on whether there's one array or multiple. The interfaces for these two are now the same. Adding a new public function to call into this interface is better than just exposing the existing one because that gives us wiggle room to change our internal function without making those changes public.)\r\n\r\nGetting back to this question, an `ak.is_tuple` could live alongside `ak.fields`. It would be a reasonable high-level interface to have. (Do Forms/Types already do it? That kind of thing can be known from the Form or Type alone.)",
        "createdAt":"2022-03-08T15:40:55Z",
        "number":2317545
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"It looks like you've already gotten started on this: #1351. Great!",
        "createdAt":"2022-03-08T15:44:26Z",
        "number":2317575
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-03-08T10:52:04Z",
  "number":1349,
  "title":"Generic way to detect a tuple structure?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1349"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@LunarLanding, your https://github.com/pydata/xarray/issues/4118#issuecomment-1059382908 looks pretty similar to Awkward Array, as @tacaswell has pointed out. Instead of the arbitrary graph of n-dimensional arrays, we have a tree structure of (mostly) 1-dimensional arrays, though the [nodes in that tree are specialized](https://awkward-array.readthedocs.io/en/latest/ak.layout.Content.html) to emulate generic data structures, like variable-length lists, records/structs, missing data, and heterogeneous unions.\r\n\r\nIt's different from the [xarray Datatree](https://xarray-datatree.readthedocs.io/en/latest/quick-overview.html) proposal that that thread was about in that Awkward Arrays allow for the number of datasets with different lengths to scale. For instance, this array contains 134 million lists of different lengths and uses 4.5 GB of RAM to do so:\r\n\r\n```python\r\n>>> # version 2.0 is hidden as an experimental submodule within version 1.8.0\r\n>>> import awkward._v2 as ak\r\n>>>\r\n>>> # caution: 3.8 GB download\r\n>>> array = ak.from_parquet(\"s3://pivarski-princeton/chep-2021-jagged-jagged-jagged/zlib9-jagged1.parquet\")\r\n>>> array\r\n<Array [[-0.423, 2.34, ..., -0.298], ...] type='134217728 * var * float32'>\r\n>>> array.show()\r\n[[-0.423, 2.34, -0.757, 0.732, -2.63, ..., -0.129, -0.297, 0.597, -0.298],\r\n [-1.94, 0.835, -0.14, -0.742, -0.369, 1.8],\r\n [-0.467, 0.315, 0.472, 0.592, -1.14, ..., 0.421, -0.689, 0.875, -0.631, 0.505],\r\n [0.136, 0.375, 1.41],\r\n [-0.132, 0.113, -2.2, 0.943, -0.466, -1.16, -0.351, -0.866, 0.494, 0.159],\r\n [-0.563, 0.43, 0.843, -1.23, -0.305, 0.528, -0.0884, -0.77, 1.31, -0.653],\r\n [0.279, 2.25, 0.599, 0.857, 1.58, 0.557, -0.8, -0.459],\r\n [-0.0717, -0.776, -1.22, 1.07],\r\n [-2.27, 0.365, 0.977, 1.17, -0.141, 0.731, 0.171, -0.565, 1.94, 2.48],\r\n [-2.12, -0.0187, 1.19, -1.56, 0.165, -1.32, -0.19],\r\n ...,\r\n [0.637, 0.974, 0.338, -0.313, -0.239, 1.57, -0.0724],\r\n [0.632, 0.838, 0.542, -0.342, 0.43, -1.13, -1.53, 1, 0.398, -0.438],\r\n [1.17, -0.288, 0.0477, -0.656, -0.61, ..., 0.441, 0.142, -0.0544, -0.697],\r\n [0.661, 1.21, -0.111, -0.645],\r\n [0.455, 0.0988, 0.826, 0.196, 1.51, ..., 0.561, -0.456, -1.58, 0.608, 0.537],\r\n [-0.993, 0.708, 1.76, 0.186, -0.413, -0.538, 0.13, 0.0459],\r\n [0.417, 1.01, -1.29, -0.397],\r\n [-1.2, -0.913, 2.6, 1.47, -0.855, ..., 0.186, 1.14, -0.131, 1.09, -1.11],\r\n [-1.17, 1.41, 0.72]]\r\n>>> len(array)\r\n134217728\r\n>>> ak.num(array)\r\n<Array [11, 6, 13, 3, 10, 10, ..., 4, 12, 8, 4, 11, 3] type='134217728 * int64'>\r\n>>> ak.mean(ak.num(array))\r\n8.0\r\n>>> ak.std(ak.num(array))\r\n3.2404736136792627\r\n>>> array.layout\r\n<ListOffsetArray len='134217728'>\r\n    <offsets><Index dtype='int32' len='134217729'>\r\n        [         0         11         17 ... 1073741810 1073741821 1073741824]\r\n    </Index></offsets>\r\n    <content><NumpyArray dtype='float32' len='1073741824'>\r\n        [-0.422626    2.344078   -0.7568038  ... -1.1683336   1.4103566\r\n          0.71982837]\r\n    </NumpyArray></content>\r\n</ListOffsetArray>\r\n>>> array.layout.offsets.data.nbytes\r\n536870916\r\n>>> array.layout.content.data.nbytes\r\n4294967296\r\n```\r\n\r\nIt's also different from a sparse matrix in that a sparse matrix is a pair of different-length buffers that are used to represent a logically rectilinear array; an Awkward Array is a set of different-length buffers that are used to represent a logically irregular array. But if you had a sparse matrix implementation on hand, you could probably use it to implement an Awkward Array.\r\n\r\nI'd be interested to know if your use-case fits into this model, or if you're thinking of something still more general or just different from what we have here.",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-04-04T20:06:17Z",
  "number":1396,
  "title":"Scalable variable-length arrays",
  "url":"https://github.com/scikit-hep/awkward/discussions/1396"
 },
 {
  "author":{
   "login":"ioanaif"
  },
  "body":"The motivation behind removing the sorting facility for Records and RecordArray lays in the fact that we do not have any order defined for records. Thus, sorting across record boundaries destroys the identities of those records. Consider the following example:\r\n\r\n```python\r\n>>> array = ak.Array([{\"x\": 3, \"y\": 1.1}, {\"x\": 1, \"y\": 3.3}, {\"x\": 2, \"y\": 2.2}])\r\n>>> sorted_array = ak.sort(array)\r\n>>> array.tolist()\r\n[{'x': 3, 'y': 1.1}, {'x': 1, 'y': 3.3}, {'x': 2, 'y': 2.2}]\r\n>>> sorted_array.tolist()\r\n{'x': [1, 2, 3], 'y': [1.1, 2.2, 3.3]}\r\n```\r\nDo we want to keep this in v2? I'll vote no.\r\n\r\n@jpivarski ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Including everyone who was involved on an issue or PR about sorting:\r\n\r\n- @ianna\r\n- @agoose77\r\n- @nsmith-\r\n- @yimuchen (#1104 and #506)\r\n- @sterbini (#835)\r\n- @drahnreb (also #835)\r\n- @jrueb (#763 and #686)\r\n- @gordonwatts (#523)\r\n\r\n**Anyone who wants to keep the v1 record-sorting rule in v2, please speak up!**\r\n\r\nIt's a bit non-intuitive, as you can see from @ioanaif's example (above). By making `ak.sort(record_array)` and `ak.argsort(record_array)` be TypeErrors, we'd have the freedom thereafter to define a more logical sorting rule. (It's always easier to add a behavior to what is an error-state than to change a behavior.)\r\n\r\nThis affects v2 and not v1\u2014it would make the refusal to sort records be an API-changing change between v1 and v2.",
     "createdAt":"2022-04-28T18:38:19Z",
     "number":2656935,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"@jpivarski this is maybe offtopic, but would it be in future scope to add a `__key__` behaviour to records, so that they have a defined ordering?",
     "createdAt":"2022-04-28T18:56:06Z",
     "number":2657011,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"In Python, `key` is a callable; Awkward parameters have to be JSON-serializable. So if the `__record_sort_fields__` is an ordered list of field names to use for sorting, then maybe.\r\n\r\nIt wouldn't be super-necessary, though. You can change a record's field order with a slice, sort it lexicographically by the field order, and then restore the original field order on the sorted output.\r\n\r\n```python\r\n>>> original = ak._v2.Array([\r\n...     [{\"x\": 1, \"y\": 1.1, \"z\": \"one\"}, {\"x\": 2, \"y\": 2.2, \"z\": \"two\"}],\r\n...     [],\r\n...     [{\"x\": 3, \"y\": 3.3, \"z\": \"three\"}],\r\n... ])\r\n\r\n>>> original.show(type=True)\r\ntype: 3 * var * {\r\n    x: int64,\r\n    y: float64,\r\n    z: string\r\n}\r\n[[{x: 1, y: 1.1, z: 'one'}, {x: 2, y: 2.2, z: 'two'}],\r\n [],\r\n [{x: 3, y: 3.3, z: 'three'}]]\r\n\r\n>>> reordered = original[[\"z\", \"x\", \"y\"]]\r\n>>> reordered.show(type=True)\r\ntype: 3 * var * {\r\n    z: string,\r\n    x: int64,\r\n    y: float64\r\n}\r\n[[{z: 'one', x: 1, y: 1.1}, {z: 'two', x: 2, y: 2.2}],\r\n [],\r\n [{z: 'three', x: 3, y: 3.3}]]\r\n```\r\n\r\nSo one would just need a lexicographical sort to be defined to get many others.\r\n\r\nIf you meant `__key__` to provide an arbitrary callable (like Python), a better fit to the way Awkward Array does things would be to use `argsort` on computed items and then apply the resulting integer array to the record array as a slice.",
        "createdAt":"2022-04-28T19:13:55Z",
        "number":2657098
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Your suggestion to re-order the fields raises an interesting question for me \u2014 is the field ordering in a `RecordArray` a meaningful part of its type? I believe the answer is currently \"no\": the JSON representation of the form is unordered (by definition), and the type object performs unordered comparison. As such, it feels slightly awkward if we rely on field ordering for a particular feature (sorting), but do not preserve it elsewhere (well, we might preserve it, but any reasonable JSON-JSON transform could destroy the ordering).\r\n\r\nAs such, my original thought process was to have `__key__` be a sequence of strings that denotes the sorting order, mapping each record to a key tuple. Unlike your proposal, this would enable sorting on an **opt-in** basis. One absolutely could do this already with argsort, e.g.\r\n\r\n```python\r\ndef sort_records(array, keys, axis=-1):\r\n    ordered = array\r\n    for key in reversed(keys):\r\n        ordered = ordered[ak.argsort(ordered[key], axis=axis, stable=True)]\r\n    return ordered\r\n```\r\n\r\nbut it's a bit clunky. I don't know enough about the wider use cases for ordering a record, but I imagine there's a fairly strong case to be made for it.\r\n\r\nOne thing that I don't like about `__key__` being an array parameter is that it adds an extra step for users to construct valid arrays. At the moment, users can construct `vector` arrays using `ak.with_name` or the vector constructors. If it made sense to define a sorting order for vectors (it doesn't), then users would need to use the vector constructors to ensure that they don't forget to call `ak.with_ordering` (terrible name).\r\n\r\nEqually, we don't want it to be a behavior class attribute, because we want this ordering to be found even when the record is contained within another record. \r\n\r\nSo, if not a `__key__`  parameter, maybe a new `ak.behavior` overload? But, I'm getting ahead of myself - I don't have a demonstrable need for this myself.",
        "createdAt":"2022-04-28T21:03:26Z",
        "number":2657576
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Thinking about this again, I think we would benefit from implementing an Awkward equivalent to https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html. Perhaps we could even overload it, as we could detect tuple-arrays and use their ordering.\r\n\r\nI'm still thinking we probably need to handle non-tuple records with a specified order. The form that we translate to/from JSON does not allow us to order the parameters, so we cannot rely on the ordering as it might not round-trip. I don't know whether we need to \"fix\" that with a schema evolution, or whether we specify that the order is _not_ guaranteed. I quite like the idea that the order is not given for a record.\r\n\r\nI propose we *change* the sorting behaviour to be lexicographic, rather than make it an error. I think _most_ users would expect the sorting algorithm to preserve the atomicity of records. We've already established that the existing behaviour is wrong (unexpected), so I think the next step is to make it do what we expect. I realise this is a silent breaking change for some. *Maybe* we add a short-lived `allow_records` to allow users to opt-in to the correct behaviour? In future, that parameter will be required to be `True` if passed at all, before finally being removed as redundant. We can specify in the roadmap the version / date at which this flag must be passed as `True` or ignored.\r\n\r\nAs an aside @jpivarski have there been any thoughts as to versioning our form schema (probably just a monotonic increasing number) for backwards compatibility?",
        "createdAt":"2022-07-08T08:04:37Z",
        "number":3105892
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I think specifying a sort order for all of the data types is a good idea, and we can make it exactly the same as Python's sort order for lists and dicts of equal type. (Except\u2014Awkward record fields have a prescribed order that is not necessarily alphabetical for the fields. We want the sort order to depend on Awkward field order because Awkward field order can easily be changed, and that would give someone a lot of flexibility in sorting something they way they want it, even before bringing in `argsort`.)\r\n\r\nHowever, I think the process of introducing a sort order is a big project, since record fields can have any type and we therefore have to have an ordering on all types, not just records. (Maybe a subproject could be to define a sort order for records of only numeric types, and while that's a major use-case, it feels arbitrary. Some objects representing particles would be sortable, but give them a list of links to associated jets and then they become non-sortable?)\r\n\r\nAlso, the list sorting is going to mix in strange ways with NumPy-style sorting:\r\n\r\n```python\r\n>>> sorted([[4, 2, 6], [1, 5, 3]])\r\n[[1, 5, 3], [4, 2, 6]]\r\n>>> np.sort([[4, 2, 6], [1, 5, 3]], axis=0)\r\narray([[1, 2, 3],\r\n       [4, 5, 6]])\r\n```\r\n\r\nLexicographic sorting treats objects like lists and records as unbreakable objects and rearranges them according to an order defined by `__lt__`. NumPy sorting moves numbers between objects. It makes sense if you're thinking of the data as a table.\r\n\r\nSo if lexicographic sorting of records needs some kind of order defined for lists because the fields may be lists, how do we harmonize that list-ordering with NumPy-style sorting? This is similar to the problem we have with Awkward-style (left-to-right) broadcasting and NumPy-style (right-to-left) broadcasting: agreement with NumPy is in conflict with intuition about objects.\r\n\r\nWhatever we do to solve these problems in the long-term, I think we should prevent sorting of records in the short-term. Currently, something like NumPy-style sorting is applied to records, which is wildly at odds with intuition about objects, and it's causing trouble. We do eventually want to change the behavior, but I don't think we can do that on a short enough timescale (the v2 release in December). Excluding the non-intuitive behavior at a major release boundary opens the possibility of adding better behavior at any time, not necessarily a major release boundary, since error state \u2192 new behavior is not considered an API breaking change, whereas old behavior \u2192 new behavior is.\r\n\r\n> As an aside @jpivarski have there been any thoughts as to versioning our form schema (probably just a monotonic increasing number) for backwards compatibility?\r\n\r\nBy \"form schema,\" do you mean the JSON representation of Forms? These aren't versioned; that JSON representation is fixed since 1.1.0 with no updates planned, since it's necessary for reading old data (e.g. in pickle, but in [other formats](https://awkward-array.org/how-to-convert-buffers.html), too). I don't see a reason why it would have to evolve, since it's only concerned with the physical layer (how to turn bytes into ak.Arrays); new behaviors and functionality go into the parameters, which are free-form. How arrays _behave_ (as opposed to what data they contain) are determined by the version of the `awkward` package and what classes you have loaded into `ak.behaviors` (and therefore the versions of `vector` or `coffea`, or whatever is defining those behaviors).\r\n\r\n(Arrow did a similar thing: they fixed the physical layer at 1.0 and won't be changing it. All new features are in metadata, which is like our `parameters`. Similarly, [JSON was specified in 2002](https://web.archive.org/web/20030228034147/http://www.crockford.com/JSON/index.html) and hasn't been modified since\u2014you can't express new physical types with it, but you can interpret those structures in various ways.)",
        "createdAt":"2022-07-08T14:16:10Z",
        "number":3108266
       }
      ],
      "totalCount":4
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Note: sorting of records is also being discussed in #1361.",
     "createdAt":"2022-07-08T13:26:42Z",
     "number":3107926,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":3
  },
  "createdAt":"2022-04-28T18:23:14Z",
  "number":1451,
  "title":"Removing `ak.sort` for Records and RecordArray in v2",
  "url":"https://github.com/scikit-hep/awkward/discussions/1451"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi,\r\n\r\nI have some data that I store in the form of a dict of awkward arrays:\r\n\r\n```\r\nakarr = ak.Array({\"x\": [[1,2], [3,4,5]], \"y\": [[6,7,8,9], [10]] })\r\nakarr2 = ak.Array({\"x\": [[10,20], [30,40,50], [110] ], \"y\": [[60,70,80,90], [100], [120]] }) # different length to akarr\r\nmydict = {\"sample1\": {\"tree1\":  akarr, \"tree2\": akarr2} , \"sample2\": ...}\r\n```\r\n\r\nThese awkward arrays will be fairly large and can be deeply jagged. I am looking to find the most efficienct way of saving the information and re-loading it in another part of my framework. I wanted to ask the experts if you have advice about the best way to do this given the current supported tools?\r\n\r\nThanks, \r\nMo",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"The right answer _should be_ to make your dict of arrays into a Record:\r\n\r\n```python\r\n>>> akarr = ak.Array({\"x\": [[1,2], [3,4,5]], \"y\": [[6,7,8,9], [10]] })\r\n>>> akarr2 = ak.Array({\"x\": [[10,20], [30,40,50], [110] ], \"y\": [[60,70,80,90], [100], [120]] })\r\n>>> mydict = {\"sample1\": {\"tree1\":  akarr, \"tree2\": akarr2} , \"sample2\": 123}\r\n>>> record = ak.Record(mydict)\r\n>>> record\r\n<Record ... 110], y: [120]}]}, sample2: 123} type='{\"sample1\": {\"tree1\": var * {...'>\r\n```\r\n\r\nSure, the trees have different lengths and the samples can have all sorts of different shapes, but that's allowed because it's just a more complex data type.\r\n\r\n```python\r\n>>> print(record.type)\r\n{\"sample1\": {\"tree1\": var * {\"x\": var * int64, \"y\": var * int64}, \"tree2\": var * {\"x\": var * int64, \"y\": var * int64}}, \"sample2\": int64}\r\n```\r\n\r\nYou can pull each of these items out, and none of these manipulations are limited by the size of any datasets.\r\n\r\n```python\r\n>>> record.sample1.tree1\r\n<Array [{x: [1, 2], y: [6, 7, ... 5], y: [10]}] type='2 * {\"x\": var * int64, \"y\"...'>\r\n```\r\n\r\nHere's the \"_should be_\" part. We should be able to save the data as Parquet and read it back again with no trouble. Parquet was made for these sorts of data structures, and our use of Parquet in [Awkward v2](https://github.com/scikit-hep/awkward-1.0/discussions/1151) is expanding to include all the metadata so that nothing gets lost when you write to a file and read it back. (Parquet would then be a \"first class\" serialization format for Awkward Array.) Your examples don't have any metadata, though. Be sure to install a recent pyarrow, such as 6.0 or 7.0. (That will be required in v2, but v1 only requires 2.0.)\r\n\r\nSo we _should be_ able to just write it out and read it back:\r\n\r\n```python\r\n>>> ak.to_parquet(record, \"output.parquet\")\r\n>>> ak.from_parquet(\"output.parquet\")\r\n<Array ['sample1', 'sample2'] type='2 * string'>\r\n```\r\n\r\nOkay, a bad thing happened there: we only got the names of the record field back. That's because of this bug: #1453. If I have time, I'll fix it today. What's happening is that Parquet only stores array-like data, so our `to_parquet` rejects records, but an internal function doesn't get the message and instead iterates over the record, which returns field names, just like iterating over a dict:\r\n\r\n```python\r\n>>> list(record)\r\n['sample1', 'sample2']\r\n>>> list(mydict)\r\n['sample1', 'sample2']\r\n```\r\n\r\nWhat `to_parquet` ought to do is turn a record into a length-1 array of records, save it, and then have `from_parquet` unpack it so that you get a record back, rather than a length-1 array. That message will have to be in metadata, so the round-trip would only be possible in v2.\r\n\r\nNevertheless, you can make a length-1 array for this record with the following idiom:\r\n\r\n```python\r\n>>> singleton = ak.Array(record.layout.array)\r\n```\r\n\r\n(For a general record, if you don't know where it came from, you'd have to say `ak.Array(record.layout.array[record.layout.at : record.layout.at + 1])`, which is more of a mouthful.) See how the type of the length-1 array differs from the record itself:\r\n\r\n```python\r\n>>> print(singleton.type)\r\n1 * {\"sample1\": {\"tree1\": var * {\"x\": var * int64, \"y\": var * int64}, \"tree2\": var * {\"x\": var * int64, \"y\": var * int64}}, \"sample2\": int64}\r\n>>> print(record.type)\r\n{\"sample1\": {\"tree1\": var * {\"x\": var * int64, \"y\": var * int64}, \"tree2\": var * {\"x\": var * int64, \"y\": var * int64}}, \"sample2\": int64}\r\n```\r\n\r\nIt's just one of those records. Anyway, now you can write it to Parquet and read it (the `singleton`) back:\r\n\r\n```python\r\n>>> ak.to_parquet(singleton, \"output.parquet\")\r\n\r\n>>> ak.from_parquet(\"output.parquet\")\r\n<Array [... 110], y: [120]}]}, sample2: 123}] type='1 * {\"sample1\": {\"tree1\": va...'>\r\n\r\n>>> ak.from_parquet(\"output.parquet\").tolist() == [\r\n...     {\"sample1\": {\"tree1\": akarr.tolist(), \"tree2\": akarr2.tolist()},\r\n...      \"sample2\": 123}]\r\nTrue\r\n```",
     "createdAt":"2022-04-29T18:31:01Z",
     "number":2663390,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"#1456 fixes the above problem, so after it merges and if you're running from the main branch, \r\n\r\n```python\r\nak.to_parquet(record, \"output.parquet\")\r\nak.from_parquet(\"output.parquet\")\r\n```\r\n\r\nwill write and read back the `record` to and from Parquet.\r\n\r\nIn v1 (pictured above), it will come back as a length-1 array, but in v2, it will come back as a record because I added a \"is this a single record?\" flag to the metadata. (The metadata round-trip requires pyarrow 6.0+ and is only implemented in Awkward v2.)",
        "createdAt":"2022-04-29T20:24:18Z",
        "number":2663894
       },
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"Thanks Jim as always for the detailed answer. I have tried to implement the method you suggested on one of the \"smaller\" samples I have (143,138 events) and the call to `from_iter()` function seems to be very slow (~88s) even on my own computer. I see from the source code the `from_iter` just calls the array builder, but i am not sure which part is causing the slow performance and if I can do anything to get around it. Any suggestions there?",
        "createdAt":"2022-05-01T12:50:17Z",
        "number":2668988
       },
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"I should add that this would be more signficant for me since I process the data in pieces to avoid filling memory, so I write an arbitrary number of files -- for one particular dataset (ttbar events) for example I dump 56 files. Spending 90s just writing data on my personal computer ( probably 5-10 times that on a shared machine) will be quite bad.",
        "createdAt":"2022-05-01T12:54:46Z",
        "number":2669001
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"@MoAly98 I feel like I've missed something, so bear with me!\r\n`from_iter` will be slow because it doesn't know the type of data up-front. Why are you using it? Jim's solution (or my cheat variant: `ak.Array(ak.packed(record).layout.array)`) should be readable and writable with Parquet.\r\n",
        "createdAt":"2022-05-01T18:55:09Z",
        "number":2670131
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"`from_iter` is slow for two reasons: it has to discover data type as it iterates, as @agoose77 said, and because it's iterating in Python, rather than compiled code. (If the thing that you're iterating over is an Awkward Array, then there's three reasons: iteration over Awkward Arrays in Python is slowed down by complex `__getitem__` logic.)\r\n\r\nBut why are you calling `from_iter`? Oh! Record's constructor from a dict does:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/edb2ff2834ef6d7d56adeefa8628329747318446/src/awkward/highlevel.py#L1556-L1557\r\n\r\nThat's my mistake; I thought Record had a constructor from a dict like Array's, which does not call `from_iter`:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/edb2ff2834ef6d7d56adeefa8628329747318446/src/awkward/highlevel.py#L226-L244\r\n\r\n`from_iter` should be a last resort, never used on arrays that are already columnar. Either we should add a smarter constructor to Record (to check to see if what it's looking at are already arrays and not iterate over them if they are) or I should recommend something like this:\r\n\r\n```python\r\n>>> ak.Array({\"sample1\": ak.Array({\"tree1\": akarr[np.newaxis], \"tree2\": akarr2[np.newaxis]})})\r\n<Array [... 100]}, {x: [110], y: [120]}]}}] type='1 * {\"sample1\": {\"tree1\": 2 * ...'>\r\n```\r\n\r\nThis makes a length-1 array (which also evades #1453) by adding a dimension with `np.newaxis`. That resolves the problem of `akarr` and `akarr2` having different lengths.\r\n\r\nThe constructor logic in Array doesn't check for arrays two levels deep, so this calls `ak.Array` twice. It's hard to say where the cut-off should be; how deeply we should search for arrays (Awkward, NumPy, or other) to avoid iterating over them.",
        "createdAt":"2022-05-02T16:48:08Z",
        "number":2675078
       },
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"Hi @agoose77 and @jpivarski !\r\n\r\nI should have probably made it clearer in my reply. I only mentioned `from_iter` because it is a internally called in the constructor for `ak.Record` when a `dict` is passed, as Jim pointed out. \r\n\r\nNow regarding the solution with using `np.newaxis`: This is what I have been doing so far actually, but the drawback I come across is when I try to parallelise the reading of the files by using partitions. The fact that all arrays are length-1 means there is only 1 partition in the file. Maybe this is naive thinking from me, but I am trying to get around long read times when reading the files I dump (especially on `lxplus`).  At the moment, I mitigate for this a bit by only processing < 2GBs chunks of data (where possible, or just one file at a time if file size > 2GB) from ROOT and dumping them into parquet. I wanted to see if reading the individual parquet files in pieces with `dask` can help the read time of these parquet files. \r\n\r\nAm I thinking about this in a wrong way? I really appreciate your input on this :)",
        "createdAt":"2022-05-03T10:02:12Z",
        "number":2679214
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"There's a tension between wanting objects with different lengths packaged together and wanting to use their lengths to divide them up into chunks for parallelization. I don't think there's a way around just packing and unpacking them. The opposite of\r\n\r\n```python\r\npacked = ak.Array({\"tree1\": array1[np.newaxis], \"tree2\": array2[np.newaxis]})\r\n```\r\n\r\nis\r\n\r\n```python\r\narray1 = packed.tree1[0]\r\narray2 = packed.tree2[0]\r\n```\r\n\r\nYou'd only be able to parallelize `array1` and `array2` separately, anyway, because they have different lengths. (They couldn't be in the same parallelization job because how would you align the elements of `array1` with `array2`, unless you had some criterion I don't know about for mapping `array1` elements with `array2` elements. And if so, then that criterion could be used to make a better `packed`.)\r\n\r\nActually, if these `array1` and `array2` don't actually have an index-to-index relationship to each other, why put them in the same file?",
        "createdAt":"2022-05-03T13:08:21Z",
        "number":2680144
       }
      ],
      "totalCount":7
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-04-29T16:38:12Z",
  "number":1454,
  "title":"Saving dict of awkward arrays  and re-reading it efficiently",
  "url":"https://github.com/scikit-hep/awkward/discussions/1454"
 },
 {
  "author":{
   "login":"ZhenxuanZhang-Jensen"
  },
  "body":"Hi, \r\n   I'm trying to understand what awkward array can do and use it in my specific particle physics analysis. I'm quite confused about how many exactly things can save in one awkward array. For instance, I want to zip couples of events and each event has couples of particles and each particle has couples of values, then I would code like this: `array=ak.zip(\r\n{\r\n'muon_pt':[[1,2,3],[4,5],[6]], 'muon_eta':[[1,2,3],[4,5],[6]] , 'muon_phi':[[1,2,3],[4,5],[6]],'muon_E':[[1,2,3],[4,5],[6]],'electron_pt':[[1,2],[3],[4]], 'electron_eta':[[1,2],[3],[4]] , 'electron_phi':[[1,2],[3],[4]],'electron_E':[[1,2],[3],[4]]\r\n}\r\n)` I want to zip three events to an awaward array with two objects muon & electron 4D infos with different numbers of particle in each event. But if I zip like this, then it will show the very common error as left boardcasting: `ValueError: in ListOffsetArray64, cannot broadcast nested list `. As far as I'm concerned, I think awkward array is very compatible to save jagged array, this issue should be sovled with some different math magic that I don't know, right? \r\n  Please help me if you have any comments or suggection to solve this problem, many thanks.\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"When you're posting on GitHub (which uses GitHub Flavoured Markdown), you can use triple backticks \\`\\`\\` to embed entire blocks of code!\r\n\r\nThe error that your seeing when executing\r\n```python\r\narray = ak.zip(\r\n    {\r\n        \"muon_pt\": [[1, 2, 3], [4, 5], [6]],\r\n        \"muon_eta\": [[1, 2, 3], [4, 5], [6]],\r\n        \"muon_phi\": [[1, 2, 3], [4, 5], [6]],\r\n        \"muon_E\": [[1, 2, 3], [4, 5], [6]],\r\n        \"electron_pt\": [[1, 2], [3], [4]],\r\n        \"electron_eta\": [[1, 2], [3], [4]],\r\n        \"electron_phi\": [[1, 2], [3], [4]],\r\n        \"electron_E\": [[1, 2], [3], [4]],\r\n    }\r\n)\r\n```\r\noccurs when Awkward tries to form a record of `\"muon_pt\", \"muon_eta\", ..., \"electron_E\"` at the deepest level of nesting. This is the default behaviour. However, it looks like you want each record to contain lists of values. This corresponds to setting a `depth_limit` in `ak.zip`:\r\n\r\n```python\r\narray = ak.zip(\r\n    {\r\n        \"muon_pt\": [[1, 2, 3], [4, 5], [6]],\r\n        \"muon_eta\": [[1, 2, 3], [4, 5], [6]],\r\n        \"muon_phi\": [[1, 2, 3], [4, 5], [6]],\r\n        \"muon_E\": [[1, 2, 3], [4, 5], [6]],\r\n        \"electron_pt\": [[1, 2], [3], [4]],\r\n        \"electron_eta\": [[1, 2], [3], [4]],\r\n        \"electron_phi\": [[1, 2], [3], [4]],\r\n        \"electron_E\": [[1, 2], [3], [4]],\r\n    },\r\n    depth_limit=1\r\n)\r\n```\r\n\r\nThis gives you `array.type` of \r\n``` python\r\n3 * {\r\n    \"muon_pt\": var * int64,\r\n    \"muon_eta\": var * int64,\r\n    \"muon_phi\": var * int64,\r\n    \"muon_E\": var * int64,\r\n    \"electron_pt\": var * int64,\r\n    \"electron_eta\": var * int64,\r\n    \"electron_phi\": var * int64,\r\n    \"electron_E\": var * int64,\r\n}\r\n```\r\nI.e. each event has a list of `muon_pt`, a list of `muon_eta`, etc.\r\n\r\nSome additional notes:\r\n\r\nAwkward Array has support for four vectors using the [`vector` library](https://github.com/scikit-hep/vector). This library leverages the Awkward Array's [behaviour](https://awkward-array.readthedocs.io/en/latest/ak.behavior.html) mechanism for adding methods to an Array.\r\n\r\nTo be able to use these behaviours, you need to have properly named fields, i.e. only `E`, `pt`, `eta`, etc. instead of `muon_eta`, `muon_pt`. You can, of course, create these later by zipping the vector fields into a new object, but if you get to choose how to build this array, I'd do it directly:\r\n```python\r\nmuon = ak.zip(\r\n    {\r\n        \"pt\": [[1, 2, 3], [4, 5], [6]],\r\n        \"eta\": [[1, 2, 3], [4, 5], [6]],\r\n        \"phi\": [[1, 2, 3], [4, 5], [6]],\r\n        \"E\": [[1, 2, 3], [4, 5], [6]],\r\n    },\r\n    depth_limit=1,\r\n    with_name=\"Momentum4D\"\r\n)\r\nelectron = ak.zip(\r\n    {\r\n        \"pt\": [[1, 2], [3], [4]],\r\n        \"eta\": [[1, 2], [3], [4]],\r\n        \"phi\": [[1, 2], [3], [4]],\r\n        \"E\": [[1, 2], [3], [4]],\r\n    },\r\n    depth_limit=1,\r\n    with_name=\"Momentum4D\"\r\n)\r\narray = ak.zip(\r\n    {\r\n        \"muon\": muon,\r\n        \"electron\": electron\r\n    },\r\n    depth_limit=1\r\n)\r\n```\r\nIn the above code sample, we create two arrays `electron` and `muon` that contain your different particle information, and then zip _them_ together into a single array.\r\n\r\nSee these talks for some examples:\r\nhttps://github.com/jpivarski-talks/2021-07-06-pyhep-uproot-awkward-tutorial/blob/main/evaluated/uproot-awkward-tutorial.ipynb\r\nhttps://github.com/jpivarski-talks/2021-09-13-star-uproot-awkward-tutorial/blob/main/tutorial-EVALUATED.ipynb\r\n\r\n\r\nHowever, when you place arrays into fields (`muon`, `electron`), you restrict the kinds of operations that you can perform with them. Sometimes it's preferable to have a single array, and add a new field that identifies the particle type e.g. `pdgid`. ",
     "createdAt":"2022-05-06T08:36:05Z",
     "number":2699109,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Thank you so much for the answer and also the suggestion.",
        "createdAt":"2022-05-06T08:41:26Z",
        "number":2699136
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I've updated my answer with some more information :)",
        "createdAt":"2022-05-06T08:56:40Z",
        "number":2699232
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"yes, this should be very helpful, thank you again.",
        "createdAt":"2022-05-06T09:00:06Z",
        "number":2699254
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-05-06T08:25:38Z",
  "number":1463,
  "title":"How to understand events, objects, and numbers in an awkward array?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1463"
 },
 {
  "author":{
   "login":"ZhenxuanZhang-Jensen"
  },
  "body":"Hi, experts\r\n  I meet a code like this `name = (\"a\",\"b\")`, `events[name] = ak.Array([*])`, I don't understand what this kind of expression means, how to save a tuple as a name in an awkward array? what is this mean? thanks for the help.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"`tuple` keys in NumPy and Awkward usually refer to dimensions, e.g.\r\n```python\r\n>>> import numpy as np\r\n>>> matrix = np.identity(3)\r\n>>> matrix[(0, 1)]\r\n0\r\n```\r\n\r\nAwkward Array extends this to support another kind of structural feature: fields. Awkward Array let's you nest fields, e.g.\r\n\r\n```python\r\nak.count(x.y.z)\r\n```\r\n\r\nand this is the same as writing\r\n```python\r\nak.count(x[('y', 'z')])\r\n```\r\n\r\nDue to how indexing works, it's also the same if you didn't write any parentheses \r\n```python\r\nak.count(x['y', 'z'])\r\n```\r\nas Python constructs a tuple for the `__getitem__` call.\r\n\r\nSo, to answer your question, `events[(\"a\", \"b\")] = ...` is setting the field `b` of `events.a` (the `RecordArray` at field `a`) to a given value. You might think that you can just write\r\n```python\r\nevents.a.b = ...\r\n```\r\n\r\nBut due to how Awkward Array works, this won't actually update `events`, because `events.a` is not a reference to the field `a` of `events`, but rather a shallow copy.",
     "createdAt":"2022-05-16T12:16:49Z",
     "number":2759167,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"okay, cool, this is very clear, thanks a lot",
        "createdAt":"2022-05-16T12:26:40Z",
        "number":2759231
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I should add - the order of fields and dimensions in the index is interchangeable, see https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#projection",
        "createdAt":"2022-05-16T12:31:12Z",
        "number":2759266
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-05-16T11:55:03Z",
  "number":1473,
  "title":"how to understand name a record directly with name of a tuple?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1473"
 },
 {
  "author":{
   "login":"ananis25"
  },
  "body":"Hi - awkward arrays have made processing arrow datasets with numba really nice! However, I can't quite figure how to use the combo for more dynamic use cases. \r\n\r\nI have a dataset of the type - `N * {\"a\": int64, \"b\": int64, \"events\": var * {\"c\": int64, \"d\": int64}}`. Essentially, a list of sequences where each sequence contains some attributes and a list of events, which themselves have event level attributes. \r\nI am trying to write a regex style pattern matcher, where we match against a stream of events rather than characters. The numba routine goes over each row of the dataset and checks if the events array has a consecutive set satisfying the pattern condition. \r\n\r\nPattern matching is recursive, so it is hard to statically determine when a specific event attribute is accessed. And I get a lot of errors of the kind,`TypeError(\"only a *constant* field name string may be used as a record slice in compiled code\")`.Issue #1420 makes it clear that non literal strings can't be used to index an awkward array since the output could be a scalar/array of any type. \r\n\r\nHowever, given the `events` attribute is a _homogenous_ record array, with only integer values, is it possible to override how awkward types the `getitem` call for it? All attribute access code is of the form - `dataset[i][\"events\"][j][prop]`, where `prop` is a variable string. ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"How many fields (columns) do you have? One easy solution would be to explode the records into a list of arrays, which can then be indexed, e.g.\r\n```python\r\nevent_fields = ak.unzip(events)\r\n\r\ndef process_events(event_fields):\r\n    j = choose_column_index()\r\n    do_something_with(\r\n        event_fields[j]\r\n    )\r\n```\r\n\r\nAs @jpivarski notes in #1420, supporting non-literal strings is an intended feature, but it's not the highest priority right now with all of the other things that are being worked on!",
     "createdAt":"2022-05-26T18:27:32Z",
     "number":2829524,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ananis25"
        },
        "body":"Thanks, thats a neat idea! The num of fields in the `events` array is upwards of 10, though mostly nullable. \r\n\r\nThe `unzip` routine doesn't seem to work inside numba jit, though instead I could do something like:\r\n```py\r\nawk_data[\"events\"] = ak.zip(ak.unzip(awk_data[\"events\"]))\r\n```\r\nAnd replace all string indexing into a record with integer indexing into a tuple. I'll try this. \r\n\r\n\r\n",
        "createdAt":"2022-05-26T19:18:12Z",
        "number":2829796
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I don't think a tuple would help much here either: you still need (literal) strings. \r\n\r\nAs I see it, you have two options with the current features in Awkward: \r\n1. pass in the unzipped array to the jitted function (as in my example)\r\n2. generate a function that is then jitted. You can do this with AST generation or string exec, whether you prefer! \r\n\r\nNote that option (1) is the simplest.",
        "createdAt":"2022-05-26T19:59:15Z",
        "number":2830036
       },
       {
        "author":{
         "login":"ananis25"
        },
        "body":"Got it, tuples can still be heterogenous so integer indexing can't guarantee that output type will always be the same. I took your suggestion to construct a 2D array instead! Hopefully, this is sufficient for what I need. \r\n\r\n```py\r\nimport awkward as ak\r\n\r\ndata = {\r\n    \"a\": np.arange(100),\r\n    \"b\": np.arange(100, 200),\r\n    \"events\": [\r\n        [{\"c\": j if j == 0 else None, \"d\": j + 1} for j in range(i)]\r\n        for i in range(200, 300)\r\n    ],\r\n}\r\nawk_data = ak.Array(data)\r\n\r\nevents = awk_data[\"events\"]\r\nlist_arrays = [events[f] for f in events.fields]\r\nlist_counts = ak.num(list_arrays[0])\r\n\r\n# to stack the arrays for event-level attributes, we need them to be 2d\r\nlist_arrays_2d = [ak.unflatten(ak.flatten(arr), 1) for arr in list_arrays]\r\nlist_array_flat = ak.to_regular(ak.concatenate(list_arrays_2d, axis=1))\r\nout_array = ak.unflatten(list_array_flat, list_counts)\r\n\r\nprint(events.type)\r\n# 100 * var * {\"c\": ?int64, \"d\": int64}\r\n\r\nprint(out_array.type)\r\n# 100 * var * 2 * ?int64\r\n```\r\n\r\n> generate a function that is then jitted. You can do this with AST generation or string exec, whether you prefer!\r\n\r\nI was indeed dreading having to create functions by composing strings! Could you please link to any examples of AST generation for numba code? Thank you. \r\n",
        "createdAt":"2022-05-27T13:07:51Z",
        "number":2834176
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"The reason that I don't think (Awkward) tuples will work is that they are still implemented using string indices, so the same restrictions apply as with records. Furthermore, the `.slot<N>` attributes don't work in Numba (IIRC), so they're no use here either. \r\n\r\nYou might find it slightly cleaner to write `list_arrays = ak.unzip(events)`.\r\n\r\nIf you pass in `list_arrays` directly to your Numba function, it is likely to be more performant than stacking them into a regular 2D array, because if they're contiguous (which they sometimes may be if you've not transformed them) the Numba conversion routines will not need to copy them. That said, I can't remember whether we check this or just make a copy regardless, though I doubt it.\r\n\r\nYou can pass in a separate 2D empty array (or construct it in the function), because you'll need to allocate memory for the result.\r\n\r\nHere's some pseudo code that illustrates what I mean:\r\n\r\n```python3\r\n\r\nlist_arrays = ak.unzip(events)\r\nresult = np.empty((len(events), len(events.fields)), dtype=np.float64)\r\n\r\n@njit\r\ndef transform(list_arrays, result):\r\n    first = list_arrays[0]\r\n    \r\n    for i in range(len(first)):\r\n        j = select_some_column(list_arrays, i)\r\n        result[i, j] = ...\r\n        \r\ntransform(list_arrays, result)\r\n```",
        "createdAt":"2022-05-27T15:22:47Z",
        "number":2835090
       },
       {
        "author":{
         "login":"ananis25"
        },
        "body":"> If you pass in list_arrays directly to your Numba function, it is likely to be more performant than stacking them into a regular 2D array, because if they're contiguous (which they sometimes may be if you've not transformed them) the Numba conversion routines will not need to copy them. That said, I can't remember whether we check this or just make a copy regardless, though I doubt it.\r\n\r\nYeah, avoiding a copy would have been better, though I need to multiindex to keep the logic simple. So, a single array of type `N * var * M * int64` is preferable to a list of M arrays of the type `N * var * int64`. Numba gives me grief with reflected lists too. \r\nI will explore writing a wrapper class akin to awkward for the list of arrays to get around copying.  ",
        "createdAt":"2022-05-27T16:02:36Z",
        "number":2835348
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"@ananis25 here's an example of AST generation:\r\n\r\nhttps://gist.github.com/agoose77/ec37316c72214b47ebf06b1956a72072\r\n\r\nI'm fairly confident that there is a way to do this using Numba's API, but I'm not yet familiar enough with Numba's internals to be able to suggest a solution in the time I have available to look at this problem :) AST generation is an \"obvious\" way to solve this, even if it's a bit clunky.",
        "createdAt":"2022-05-27T16:08:53Z",
        "number":2835377
       },
       {
        "author":{
         "login":"ananis25"
        },
        "body":"No worries, thank you for taking the time! I guessed you were hinting at something like numba rewrite passes, though using the python ast module also gave me an idea about how to write filter functions for events. \r\n",
        "createdAt":"2022-05-27T16:27:51Z",
        "number":2835471
       }
      ],
      "totalCount":7
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-05-26T18:08:02Z",
  "number":1481,
  "title":"getting around the literal string requirement for indexing",
  "url":"https://github.com/scikit-hep/awkward/discussions/1481"
 },
 {
  "author":{
   "login":"ZhenxuanZhang-Jensen"
  },
  "body":"Hi experts,\r\n   Currently, I am trying to use uproot to load a root file, and change it as a numpy array and do the cut, and calculate the cut efficiency. I keep this in mind for a long time, and I'm always thinking about this kind of situation which I met a lots of times. Can we don't use for loop, but use awkward array to do the exact same thing? such as creating an awkward array to save all the efficiency and then using slice to do the cut?\r\nBest regards,\r\nZhenxuan\r\n```sig_xgb_roc = []\r\nbkg_xgb_roc = []\r\nfor i in np.linspace(-1,1,1000):\r\n    sig_xgb_roc.append(event_signal_xgb_weights[event_signal_xgb['DiphotonMVA_self'].array(library = 'np') > i].sum() / event_signal_xgb_weights.sum())\r\n    bkg_xgb_roc.append((event_bkg_pp_xgb_weights[event_bkg_pp_xgb['DiphotonMVA_self'].array(library = 'np') > i].sum() +  event_bkg_pfff_xgb_weights[event_bkg_pfff_xgb['DiphotonMVA_self'].array(library = 'np') > i].sum()) / ( event_bkg_pp_xgb_weights.sum() + event_bkg_pfff_xgb_weights.sum())) ```",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Well, first thing: the [uproot.TBranch.array](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#array) method reads data and you don't want to read and re-read the same data over and over. You could pass a `cache` to the `array` method so that it would check the `cache` for an already-read value instead of getting it from the file, but it's probably easier to just move the\r\n\r\n```python\r\nsignal_xgb = event_signal_xgb['DiphotonMVA_self'].array(library = 'np')\r\nbkg_pp_xgb = event_bkg_pp_xgb['DiphotonMVA_self'].array(library = 'np')\r\nbkg_pfff_xgb = event_bkg_pfff_xgb['DiphotonMVA_self'].array(library = 'np')\r\n```\r\n\r\nread commands out of the loop and just use those arrays, rather than reading them, in the loop.\r\n\r\nSecondly, you can remove the sums that don't depend on `i` outside the loop on `i`, so that they don't get recomputed each time. That is,\r\n\r\n```python\r\nsignal_xgb_weights_sum = event_signal_xgb_weights.sum()\r\nbkg_pp_xgb_weights_sum = event_bkg_pp_xgb_weights.sum()\r\nbkg_pfff_xgb_weights_sum = event_bkg_pfff_xgb_weights.sum()\r\n```\r\n\r\nWhat remains is a for loop over 1000 values of `i`, and generally pure Python is acceptable for loops of only 1000. That is, the fact that you have to step over Python statements 1000 times is not going to be a pain point: the Python overhead will be milliseconds. _But_, what you have in that loop is a few sums, and if those sums are over a large amount of data, then doing 1000 of those might be painful.\r\n\r\nSo first, try the fixes described above and if it's still a problem, you might want to try using [np.sort](https://numpy.org/doc/stable/reference/generated/numpy.sort.html), [np.cumsum](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html) and/or [np.searchsorted](https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html). You want to add up everything in an array that's greater than a given `i`. Instead of repeatedly cutting and summing, sort the data so that all the elements greater than `i` would be to the right of `i`, do one cumulative sum, so that the sum of everything greater than `i` is the value at `i`, and that might be fast enough already because 1000 tests would just be 1000 array-lookups (reading values from the cumulative sum 1000 times). If it _really, really_ matters after that, you can even vectorize the lookups with `searchsorted`, but I doubt it will matter\u2014unless this example with 1000 was a small-scale test and you'll actually be doing it with millions or billions of `i` values.\r\n\r\nAll while you are making these incremental improvements, check the results of the optimized code against the results you already have with the slow code. Don't try to do all the improvements at once\u2014small steps with correctness-checking at each step (i.e. \"test driven development\")\u2014will likely involve less debugging.",
     "createdAt":"2022-06-13T16:19:05Z",
     "number":2940115,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Thanks a lot, Jim. The advices and suggestions for you are always very helpful, I do learn a lot, and indeed, it helps me to reduce the time consumption. Actually, I am finding a universal way to avoid using loop. Maybe the example I gave is a too easy one, let me give you another problem I met before, and maybe I can touch on the essence of the array usages this time.\r\n\r\nI used to train a DNN model to choose the correct 4 signal jets over bunches of jets(different events have different numbers, maybe like 7jets) of each event. To mark the signal and bkgs in training, I choose to use arrays to save 7 jets in each event, and the arrays in which the first 4 jets are signal jets will be marked as signals.like:[1,2,3,4,5,6,7],[2,1,3,4,5,6,7] -> mark as signal,(equal to 1 in training),[5,1,2,3,4,6,7],[6,1,2,3,4,5,7]-> mark as bkg,(equal to 0 in training)\r\n\r\n So the point is, we will have 7! combinations of each event, and 4! of them will be marked as signals and other will be marked as bkgs. And it's all good by now, we can get the model. And the difficult part is in the evaluation(application), as you know, for each event we have 7! combination, and I need to give them all a score then choose the best score combination, and the first four jets in that best score combination of each event is are the signal jets we are looking for. So if we use for loop to do these, we need to loop over all the events, and in each event, we need to loop over 7! combination to find the best score array, and get the information of the first four indexes of that array and save that in the tree. And it seems that we can't save it all to a big array to do the model prediction since it only can do it one by one. So for me, it seems very difficult to decrease the time consumption or use a more advanced way to improve these. And I think it's very common for everyone if we need to use DNN and do the evaluation, I don't know if there are any better ways to cut time or maybe awkward array can better handle this situation. \r\nBest,\r\nZhenxuan",
        "createdAt":"2022-06-14T15:52:42Z",
        "number":2948630
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"> And it seems that we can't save it all to a big array to do the model prediction since it only can do it one by one.\r\n\r\n7! = 5040 is a lot of combinations to keep in memory as an intermediate array. When you do `ak.combinations(jets, 7)` in Awkward, it makes an IndexedArray of the `jets`, so it's not making full copies of each jet, but still that's a lot of integers to have in memory. You'll want to iterate over them, not store them in memory.\r\n\r\nThat leads me to think you'll want to use Numba for this problem, with `for` loops, but compiled. Or, actually, RDataFrame.\r\n\r\n```python\r\n>>> import awkward as ak    # or import awkward._v2 as ak\r\n>>> import numba as nb\r\n>>> jets = ak.Array([[{\"px\": 1}, {\"px\": 2}], [], [{\"px\": 3}, {\"px\": 4}, {\"px\": 5}]])\r\n>>> @nb.njit\r\n... def walk_over_combinations(jets):\r\n...     for event_jets in jets:\r\n...         for i in range(len(event_jets)):\r\n...             for j in range(i + 1, len(event_jets)):\r\n...                 print(event_jets[i].px, event_jets[j].px)\r\n... \r\n>>> walk_over_combinations(jets)\r\n1 2\r\n3 4\r\n3 5\r\n4 5\r\n```\r\n\r\nOr, with very recent versions of Awkward Array (from GitHub's `main` branch),\r\n\r\n```python\r\n>>> import awkward._v2 as ak   # has to be version 2\r\n>>> jets = ak.Array([[{\"px\": 1}, {\"px\": 2}], [], [{\"px\": 3}, {\"px\": 4}, {\"px\": 5}]])\r\n>>> rdf = ak.to_rdataframe({\"jets\": jets})\r\n>>> rdf2 = rdf.Define(\"total\", \"double total = 0; for (auto jet : jets) { total += jet.px(); } return total\")\r\n>>> rdf2.AsNumpy([\"total\"])\r\n{'total': ndarray([ 3.,  0., 12.])}\r\n```\r\n\r\nand presumably you can put more exciting C++ code in the `Define`. Record objects like individual jets are on-demand classes with their fields as methods, and lists are iterable things (use `auto` for an instance type\u2014it's not a guessable type name). If the list only contains numbers, they're [ROOT::VecOps::RVecs](https://root.cern/doc/master/classROOT_1_1VecOps_1_1RVec.html). We don't have a working `ak.from_rdataframe` yet, though.\r\n\r\nBoth of these methods will allow you to compile loops over your very large number of combinations. At the moment, I can't think of any other way to approach this problem.\r\n\r\nPerhaps [this would have been a good tool](https://github.com/jpivarski/PartiQL), but I never finished it.",
        "createdAt":"2022-06-14T20:26:22Z",
        "number":2950558
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"I have tried numba before and it seems I can't use TensorFlow in numba, so I can't do the prediction in numba, so as the Rdataframe.",
        "createdAt":"2022-06-15T06:19:54Z",
        "number":2953012
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I don't fully understand what you're doing, but at a high level, you wouldn't use Numba / RDataFrame to perform your model fitting. Instead, you'd use it to manipulate/transform your arrays, treating it like a black box:\r\n\r\n```mermaid\r\nflowchart LR\r\ndata[Data Source] --> select[Jagged Selection] --> tensor[TensorFlow]\r\n```\r\n\r\n",
        "createdAt":"2022-06-15T08:57:22Z",
        "number":2954142
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"yes, What I want to do is to use Numba or RDataFrame to accelerate the DNN model evaluation and get the score of each given dataset that needs to be produced in the for loop. So I need to predict the score one by one in the for loop inside Numba/RDataFrame which means I need to use tensorflow or more specific `model.predict(array)` inside Numba/RDataFrame. But I tried, and it seems that doesn't work, so it just gets me back to the very beginning of my problem, how to accelerate the whole running process...",
        "createdAt":"2022-06-15T13:41:08Z",
        "number":2956112
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"The trick is to use these tools to build up what you're calling \"`array`\" here. That's what would use Numba or RDataFrame, the formatting of that data. Numba and RDataFrame don't recognize TensorFlow objects or functions as types (it would have to have an explicit Numba or C++ interface; it's possible there's a C++ interface, but I doubt you want to use it).\r\n\r\nThe `model.predict` function surely takes large, multi-event datasets as possible inputs, perhaps as a two-dimensional NumPy array.\r\n\r\nIf your TensorFlow model takes jagged data, the interface it probably wants is [RaggedTensor](https://www.tensorflow.org/guide/ragged_tensor). We don't have an explicit function to convert an Awkward Array into a RaggedTensor (issue #1466), but it wouldn't be hard to do that conversion, zero-copy. [RaggedTensor.from_row_lengths](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#from_row_lengths) needs the values that [ak.num](https://awkward-array.readthedocs.io/en/latest/_auto/ak.num.html) provides (high-level route), and [RaggedTensor.from_row_splits](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#from_row_splits) takes the `offsets` of a `ListOffsetArray` (low-level route, fewer computations). If you go the low-level route, you can find the `ListOffsetArray` inside an array's `layout`, and if your jagged array is based on a `ListArray` or if the `offsets` doesn't start with `0` (which TensorFlow needs), there's a `toListOffsetArray64(start_at_zero=True)` method that you can call on it.",
        "createdAt":"2022-06-15T14:34:36Z",
        "number":2956583
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Okay. Thanks for all the answers, I think I know where I need to take a look now.",
        "createdAt":"2022-06-15T14:53:32Z",
        "number":2956718
       }
      ],
      "totalCount":7
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-06-13T15:44:37Z",
  "number":1500,
  "title":"How to avoid using for loop in these kind of  situations",
  "url":"https://github.com/scikit-hep/awkward/discussions/1500"
 },
 {
  "author":{
   "login":"ZhenxuanZhang-Jensen"
  },
  "body":"I try to use `vector` in SWAN, and it need to have the at least 1.2.0 version `awkward`. So I try to upgrade the awkward with pip, and it shows the upgrade is successful, but the awkward version doesn't change at all. And also, I check the [LCG release](https://lcginfo.cern.ch/pkg/awkward/), and it shows that the 101swan has the awkward version with 1.7.0, why does my 101swan only have a 1.0.2 version awkward? ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"This sounds like an environment problem. The causes for this can be numerous. Can you run the following diagnostics?\r\n\r\n```bash\r\nwhich pip\r\nwhich python3\r\npython3 -m site\r\npython3 -m pip list -v\r\nenv | grep PYTHON\r\n```\r\n\r\nand report the results?",
     "createdAt":"2022-06-15T15:57:04Z",
     "number":2957261,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"```\r\n>>> which pip\r\n... /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/bin/pip\r\n>>> which python3\r\n... /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/bin/python3\r\n>>> python3 -m site\r\n... sys.path = [\r\n    '/eos/home-i00/z/zhenxuan',\r\n    '/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/python',\r\n    '/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib',\r\n    '/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages',\r\n    '/cvmfs/sft.cern.ch/lcg/releases/Python/3.8.6-3199b/x86_64-centos7-gcc8-opt/lib/python38.zip',\r\n    '/cvmfs/sft.cern.ch/lcg/releases/Python/3.8.6-3199b/x86_64-centos7-gcc8-opt/lib/python3.8',\r\n    '/cvmfs/sft.cern.ch/lcg/releases/Python/3.8.6-3199b/x86_64-centos7-gcc8-opt/lib/python3.8/lib-dynload',\r\n    '/eos/user/z/zhenxuan/.local/lib/python3.8/site-packages',\r\n    '/cvmfs/sft.cern.ch/lcg/releases/Python/3.8.6-3199b/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages',\r\n]\r\nUSER_BASE: '/eos/user/z/zhenxuan/.local' (exists)\r\nUSER_SITE: '/eos/user/z/zhenxuan/.local/lib/python3.8/site-packages' (exists)\r\nENABLE_USER_SITE: True\r\n>>> python3 -m pip list -v\r\n... Package                            Version       Location               Installer\r\n---------------------------------- ------------- ---------------------------------------------------------------------------------------- ---------\r\nabsl-py                            0.11.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nAGILe                              1.5.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nalabaster                          0.7.12        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nAPFEL                              3.0.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nappdirs                            1.4.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\narchspec                           0.1.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nasn1crypto                         0.24.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nastor                              0.8.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nastroid                            2.3.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nastunparse                         1.6.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\natomicwrites                       1.3.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nattrs                              19.3.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nautopep8                           1.4.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nawkward                            1.0.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nawkward0                           0.15.3        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nBabel                              2.9.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nbackcall                           0.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nbackports-abc                      0.5           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nbackports.functools-lru-cache      1.5           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nbackports.lzma                     0.0.13        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nbackports.shutil-get-terminal-size 1.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nbackports.ssl-match-hostname       3.7.0.1       /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nbcrypt                             3.1.6         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nbleach                             3.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nboost-histogram                    0.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncachetools                         3.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncaniusepython3                     7.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nCartopy                            0.17.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncertifi                            2019.3.9      /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncffi                               1.12.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nchardet                            3.0.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nClick                              7.0           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncloudpickle                        1.2.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncolorcet                           2.0.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nconfigparser                       3.7.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\ncontrol                            dev           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nCouchDB                            1.2           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncoverage                           4.5.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncpymad                             1.6.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncryptography                       3.3.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\ncx-Oracle                          7.1.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ncycler                             0.10.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nCython                             0.29.21       /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ndask                               0+unknown     /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ndecorator                          4.3.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ndefusedxml                         0.6.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ndill                               0.3.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ndistlib                            0.2.9         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ndistributed                        1.28.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ndocopt                             0.6.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ndocutils                           0.16          /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nelasticsearch                      6.3.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nentrypoints                        0.3           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nfilelock                           3.0.12        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nflake8                             3.7.8         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nflatbuffers                        1.12.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nfonttools                          4.33.3        /eos/home-i00/z/zhenxuan/.local/lib/python3.8/site-packages               pip\r\nfuncsigs                           1.0.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nfuture                             0.17.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ngast                               0.3.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nGDAL                               2.4.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nGenshi                             0.7           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nglobus-sdk                         2.0.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ngoogle-auth                        1.6.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ngoogle-auth-oauthlib               0.4.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ngoogle-pasta                       0.2.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ngosam                              2.0.4-12f4de9 /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ngraphviz                           0.11.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ngrpcio                             1.28.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nh5py                               2.10.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nHeapDict                           1.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nhepdata-converter                  0.2.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nhepdata-validator                  0.2.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nhtml5lib                           1.0.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nhypothesis                         5.3.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nidna                               2.8           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nimageio                            2.5.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nimagesize                          1.2.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\niminuit                            1.5.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nimportlib-metadata                 2.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nimportlib-resources                1.0.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\niniconfig                          0.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nipaddress                          1.0.22        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nipydatawidgets                     4.0.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nipykernel                          5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nipympl                             0.4.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nipyparallel                        6.2.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nipython                            7.5.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nipython-genutils                   0.2.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nipywidgets                         7.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nisort                              4.3.20        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nitk                                5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nitk-core                           5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nitk-filtering                      5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nitk-io                             5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nitk-meshtopolydata                 0.6.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nitk-numerics                       5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nitk-registration                   5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nitk-segmentation                   5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nitkwidgets                         0.32.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njax                                0.2.9         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njaxlib                             0.1.59        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\njedi                               0.13.3        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nJinja2                             2.11.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njoblib                             0.14.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njson5                              0.9.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njsonpatch                          1.28          /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njsonpointer                        2.0           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njsonschema                         3.2.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyter                            1.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyter-client                     5.3.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyter-console                    6.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyter-contrib-core               0.3.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyter-contrib-nbextensions       0.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyter-core                       4.6.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyter-highlight-selected-word    0.2.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyter-latex-envs                 1.4.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyter-nbextensions-configurator  0.4.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyterlab                         1.2.6         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\njupyterlab-server                  1.0.6         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nKeras                              2.2.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nKeras-Applications                 1.0.8         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nKeras-Preprocessing                1.1.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nkiwisolver                         1.0.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nkubernetes                         9.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nlazy-object-proxy                  1.4.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nLHAPDF                             6.3.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nllvmlite                           0.34.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nlogilab-common                     1.4.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nlxml                               4.6.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nM2Crypto                           0.34.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nMarkdown                           3.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nMarkupSafe                         1.0           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmatplotlib                         3.3.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmccabe                             0.6.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmessaging                          1.1           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmetakernel                         0.24.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nminrpc                             0.0.11        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmistune                            0.8.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmock                               3.0.5         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmore-itertools                     7.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmplhep                             0.3.24        /eos/home-i00/z/zhenxuan/.local/lib/python3.8/site-packages               pip\r\nmplhep-data                        0.0.3         /eos/home-i00/z/zhenxuan/.local/lib/python3.8/site-packages               pip\r\nmpmath                             1.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmsgpack                            0.6.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmultiprocess                       0.70.10       /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nmysql-connector-python             8.0.18        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nnbconvert                          5.5.0.dev0    /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nnbformat                           4.4.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nnetworkx                           2.3           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nnose                               1.3.7         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nnotebook                           5.7.8         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nnumba                              0.51.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nnumexpr                            2.6.9         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nnumpy                              1.18.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\noauthlib                           3.0.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\noctave-kernel                      0.31.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nonnxruntime                        1.6.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nopt-einsum                         3.2.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nOWSLib                             0.17.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npackaging                          19.0          /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npandas                             1.2.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npandocfilters                      1.4.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nparam                              1.9.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nparamiko                           2.4.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nparsl                              0.9.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nparso                              0.4.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npathlib2                           2.3.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npathos                             0.2.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npatsy                              0.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npbr                                5.2.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npexpect                            4.7.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npickleshare                        0.7.5         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nPillow                             6.2.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npip                                21.0.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npipenv                             2020.11.15    /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npkgconfig                          1.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nplotly                             0+unknown     /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npluggy                             0.13.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npox                                0.2.5         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nppft                               1.6.4.9       /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nprettytable                        0.7.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nprofessor2                         X.Y.Z         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nprometheus-client                  0.7.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nprompt-toolkit                     2.0.9         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nprotobuf                           3.15.8        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npsutil                             5.6.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nptyprocess                         0.6.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npy                                 1.9.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npy4j                               0.10.9.1      /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyarrow                            3.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyasn1                             0.4.5         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyasn1-modules                     0.2.5         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npycairo                            1.18.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npycodestyle                        2.5.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npycparser                          2.19          /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyct                               0.4.6         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npydot                              1.4.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npydot-ng                           2.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyflakes                           2.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nPygments                           2.4.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npygraphviz                         1.5           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nPyHEADTAIL                         1.14.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyHepMC3                           3.2.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyHepMC3.rootIO                    3.2.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyHepMC3.search                    3.2.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyhf                               0.5.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nPyJWT                              2.0.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npylint                             2.3.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nPyNaCl                             1.3.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyparsing                          2.4.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyproj                             2.1.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nPyQt5                              5.12.3        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nPyQt5-sip                          4.19.15       /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\nPyRDF                              0.2.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyrsistent                         0.15.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyserial                           3.4           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyshp                              2.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyspark                            3.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/python\r\npystan                             2.19.1.1      /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\npytest                             0.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npytest-runner                      5.1           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npython-dateutil                    2.8.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npython-gitlab                      1.8.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npython-prctl                       1.7           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npython-vxi11                       0.9           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npytz                               2019.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nPyWavelets                         1.0.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nPyYAML                             5.3.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\npyzmq                              18.0.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nqmtest                             2.4.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nqtconsole                          4.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nQtPy                               1.7.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nredis                              3.2.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nrequests                           2.22.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nrequests-oauthlib                  1.2.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nretrying                           1.3.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nrise                               5.5.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nrivet                              3.1.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nroot-numpy                         4.8.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nrsa                                4.0           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nscandir                            1.10.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nscikit-image                       0.14.3        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nscikit-learn                       0.21.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nscipy                              1.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nseaborn                            0.9.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nSend2Trash                         1.5.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsetuptools                         44.1.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsetuptools-scm                     3.3.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nShapely                            1.6.4.post2   /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsimplegeneric                      0.8.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsimplejson                         3.16.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsingledispatch                     3.4.0.3       /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsix                                1.12.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsnowballstemmer                    2.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsollya                             0.3           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsortedcontainers                   2.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nSphinx                             3.4.3         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsphinx-rtd-theme                   0.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nSQLAlchemy                         1.2.10        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nstatsmodels                        0.10.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nstevedore                          3.3.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nstomp.py                           6.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nstorm                              0.23          /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nsympy                              1.4           /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntables                             3.6.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntabulate                           0.8.9         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntblib                              1.4.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntensorboard                        2.3.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\ntensorboard-plugin-wit             1.7.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\ntensorflow-cpu                     2.3.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\ntensorflow-estimator               2.3.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages pip\r\ntermcolor                          1.1.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nterminado                          0.8.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntestpath                           0.4.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nTheano                             1.0.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntoml                               0.10.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntoolz                              0.9.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntorch                              1.7.0a0       /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntorchvision                        0.8.0a0       /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntornado                            5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntqdm                               4.56.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntraitlets                          4.3.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntraittypes                         0.2.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntyped-ast                          1.4.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntypeguard                          2.7.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntyping-extensions                  3.7.4.3       /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\ntzlocal                            2.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nuhi                                0.3.1         /eos/home-i00/z/zhenxuan/.local/lib/python3.8/site-packages               pip\r\nuncertainties                      3.1.2         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nupandas                            0.2.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nuproot                             4.0.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nuproot3                            3.14.2        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nuproot3-methods                    0.10.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nurllib3                            1.25.3        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nvcversioner                        2.16.0.0      /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nvirtualenv                         20.4.3        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nvirtualenv-clone                   0.5.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nvirtualenvwrapper                  4.8.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nwcwidth                            0.1.7         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nwebencodings                       0.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nwebsocket-client                   0.56.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nWerkzeug                           0.15.4        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nwheel                              0.33.4        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nwidgetsnbextension                 3.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nwrapt                              1.11.1        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nxenv                               1.0.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nxgboost                            0.90          /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nxrootd                             5.1.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nyoda                               1.9.0         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nzict                               0.1.4         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nzipp                               0.5.1         /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\nzstandard                          0.14.0        /cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\n>>>  env | grep PYTHON\r\n... PYTHONIOENCODING=UTF-8\r\nPYSPARK_PYTHON=/cvmfs/sft.cern.ch/lcg/releases/Python/3.8.6-3199b/x86_64-centos7-gcc8-opt/bin/python3\r\nIPYTHONDIR=/scratch/zhenxuan/.ipython\r\nPYTHONHOME=/cvmfs/sft.cern.ch/lcg/releases/Python/3.8.6-3199b/x86_64-centos7-gcc8-opt\r\nPYTHONPATH=/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/python:/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages\r\n```",
        "createdAt":"2022-06-15T16:12:01Z",
        "number":2957400
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"here is the upgrade command and the way I import awkward:\r\n![image](https://user-images.githubusercontent.com/65934516/173877668-c1b8851b-8706-484b-8897-eb88665b9378.png)\r\n",
        "createdAt":"2022-06-15T16:24:57Z",
        "number":2957494
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"What is happening here is that the version of Awkward on CVMFS is old, and in a non-writable directory. Pip falls back to a user installation, which means `$HOME/.local/lib/...`. \r\n\r\nNow, for loading a module, Python follows the order shown in `python3 -m site`. If you look at the output of `python3 -m site`, the CVMFS path entry is placed before the `$HOME/.local` entry.\r\n\r\nIn simple terms:\r\n- `pip` installs `awkward` into `$HOME/.local/...`\r\n- Python finds `awkward` in `/cvmfs/sft.cern.ch/...`\r\n\r\nThe reason that Python is finding `/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages` first, and not your user site directory, is because `PYTHONPATH` takes precedence over the user site directory, and you have `PYTHONPATH` set. I think this is for the LCG view mechanism.\r\n\r\nNow, to fix this, you probably need to update the version of `awkward` from CVMFS. I am not familiar with CVMFS, but I believe you need to switch to a newer \"view\". You should consult the documentation on how to do this.\r\n\r\nThe other option, which is perfectly valid, is to move your user site directory to the front of Python's module search path. You can do this with:\r\n`export PYTHONPATH=\"$(python3 -m site --user-site):$PYTHONPATH\"` or something like that. ",
        "createdAt":"2022-06-15T18:06:06Z",
        "number":2958174
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Right!! thank you so much. it works after I use : `export PYTHONPATH=\"$(python3 -m site --user-site):$PYTHONPATH\"`",
        "createdAt":"2022-06-16T04:07:23Z",
        "number":2960873
       }
      ],
      "totalCount":4
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-06-15T15:54:27Z",
  "number":1506,
  "title":"Can't use latest version of awkward in SWAN",
  "url":"https://github.com/scikit-hep/awkward/discussions/1506"
 },
 {
  "author":{
   "login":"ZhenxuanZhang-Jensen"
  },
  "body":"Hi experts,\r\n   I read the jets kinematic from a root file, and save it as an awkward array. And it's a jagged array, every event has different numbers of jets. How can I combine two of those jets in each event to be a new object(W boson), and then choose the one closest to the W inv mass in each event and save it in the event array as a new object called \"W\"?\r\n```python\r\ncat_4jets_jets_pt = events['SelectedJet_pt'][ak.num(events['SelectedJet_pt']) >= 4]\r\ncat_4jets_jets_eta = events['SelectedJet_eta'][ak.num(events['SelectedJet_eta']) >= 4]\r\ncat_4jets_jets_phi = events['SelectedJet_phi'][ak.num(events['SelectedJet_phi']) >= 4]\r\ncat_4jets_jets_mass = events['SelectedJet_mass'][ak.num(events['SelectedJet_mass']) >= 4]\r\n\r\ncat_4jets_jets_LV =  vector.zip({\r\n    \"pt\": cat_4jets_jets_pt,\r\n    \"phi\": cat_4jets_jets_eta,\r\n    \"eta\": cat_4jets_jets_phi,\r\n    \"M\": cat_4jets_jets_mass,\r\n})\r\n```",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"First things's first, I'd `ak.zip` the fields of the jet (`pt`, `eta`, ...) into their own record:\r\n```python3\r\nimport awkward as ak\r\n\r\njet = ak.zip({\r\n    'pt': events.SelectedJet_pt,\r\n    'eta': events.SelectedJet_eta,\r\n    'phi': events.SelectedJet_phi,\r\n    'mass': events.SelectedJet_mass,\r\n})\r\n```\r\n\r\nThe `vector` library gives useful helper classes that add special _behaviours_ to Awkward Arrays. We can use the `Momentum4D` class to get a four-vector behaviour:\r\n```python3\r\nimport vector\r\n\r\njet = vector.zip({\r\n    'pt': events.SelectedJet_pt,\r\n    'eta': events.SelectedJet_eta,\r\n    'phi': events.SelectedJet_phi,\r\n    'mass': events.SelectedJet_mass,\r\n})\r\n```\r\n\r\nNote that, `vector.zip` is just shorthand for `ak.zip` + some logic that guesses which behaviour you want. You can also use\r\n```python3\r\nvector.register_awkward()\r\n\r\njet = ak.zip({\r\n    'pt': events.SelectedJet_pt,\r\n    'eta': events.SelectedJet_eta,\r\n    'phi': events.SelectedJet_phi,\r\n    'mass': events.SelectedJet_mass,\r\n}, with_name=\"Momentum4D\")\r\n```\r\n\r\nTo solve your problem, we don't _need_ this behaviour, because you just want the invariant mass. But, it's helpful to use it in case you later decide to do something more complex. The subsequent solutions will make use of `Momentum4D` in order to add jets (four vectors) together.\r\n```python3\r\nfour_jet = jet[ak.num(jet, axis=-1) >= 4]\r\n```\r\n\r\nTo explore all of the pairwise combinations of the jets, you can use`ak.combinations`:\r\n```python3\r\npair = ak.combinations(four_jet, axis=1,  n=2)\r\n```\r\nThen you can add together the four vectors to compute the boson vector\r\n```python3\r\nboson = pair.slot0 + pair.slot1\r\n```\r\nWith the `boson` vector we can compute the absolute mass difference between each boson candidate and the known W boson mass:\r\n```python3\r\nw_boson_mass = ...\r\ndelta_mass = np.abs(boson.mass - w_boson_mass)\r\n```\r\nThen, finally, we can pull out the boson with the shortest mass.\r\n```python3\r\ni_best = ak.argmin(\r\n    delta_mass, \r\n    axis=-1, \r\n    keepdims=True\r\n)\r\nw_boson = boson[i_best]\r\n```",
     "createdAt":"2022-06-16T11:01:41Z",
     "number":2963246,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"First of all, thanks for the answers and the clues, it helps me to get this straight in my mind now. But in this way, indeed we can have a W boson, but as you know, we get 4 jets as an array to finally form the Higgs boson candidate, so after finding the closest to 80GeV W boson, we need to find two jets out of the remained jets to form a Higgs boson. I'm thinking maybe change the ak.combinations numbers to 4 or so? Do you have any ideas? thanks.",
        "createdAt":"2022-06-16T13:08:23Z",
        "number":2964155
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"`ak.combinations(jets, 4)` is what you want, though I hope you have enough memory for all the combinations. There was another question recently in which `ak.combinations(jets, 7)` was needed, and for that I suggested Numba, since it might be hard to keep all of those combinations in memory at once.\r\n\r\nAha\u2014that was you! https://github.com/scikit-hep/awkward/discussions/1500#discussioncomment-2948630 Sorry for not recognizing at first.",
        "createdAt":"2022-06-16T13:25:08Z",
        "number":2964278
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Yeah, that is also the reason I try to use vector in Numba and also face some issues on it.  About the ak.combinations(jets,4), I still get some confusion. Can we choose the two jets of the four in the combination which is closest to the W boson mass just like using ak.combination(jets,2) and slot0,slot1 then form choose the rest of them closest to higgs invM? I mean first, we need to choose the closest to 80 Gev two jets and then we need to choose two jets in the rest of all jets to form as the closest to 125 GeV Higgs candidate.",
        "createdAt":"2022-06-16T13:36:31Z",
        "number":2964352
       }
      ],
      "totalCount":3
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Just linking everything together, so that we can see what you've tried and what's working/not working:\r\n\r\n   * https://github.com/scikit-hep/vector/discussions/205\r\n   * https://github.com/scikit-hep/vector/discussions/206",
     "createdAt":"2022-06-16T13:07:03Z",
     "number":2964148,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"yes, thanks Jim, it basically the problems I met when I do the same analysis with `uproot`,`awkward`,`vector` and `numba`",
        "createdAt":"2022-06-16T13:09:46Z",
        "number":2964168
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-06-16T09:18:25Z",
  "number":1507,
  "title":"How to create a new object with awkward?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1507"
 },
 {
  "author":{
   "login":"d-leroy"
  },
  "body":"Hello,\r\n\r\nIs it possible to build awkward arrays (more specifically record arrays) in C++ from various pointers to C++ arrays, to obtain what would basically be a awkward array \"view\" aggregated from several arrays that could then be exposed through an embedded Python interpreter?\r\n\r\nLet's say I have arrays `A`, `B`, and `C`. I would like to aggregate them into an awkward record array of the form `{\"A\": A, \"B\": B, \"C\": C}`, without copying data, to pass to the numba-jitted version of a Python function provided by the user.\r\nNote that those C++ arrays can be wrapped as numpy arrays if that helps.\r\n\r\nIs this possible at all, and if it is, how I should go about doing this effectively? \r\n\r\nThanks in advance for the help!\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"With v2, Awkward Array is removing the mid-level C++ such that predominantly the kernels are left in C++. Whilst, for v1, there _is_ a C++ layer, it will disappear.\r\n\r\nSo, if you want to create Awkward Arrays from C++, your best bet is probably using e.g. pybind11 to invoke Python code: https://pybind11.readthedocs.io/en/stable/advanced/embedding.html\r\nThis is what the new fastjet bindings do, e.g. https://github.com/scikit-hep/fastjet/blob/80c6ed177cc09667cb3f997a632edd21d342e906/src/fastjet/_singleevent.py#L69-L83 \r\n\r\nIn principle you can just return Python buffers and pass them to the zip function, but in practice I think we probably don't generalise all the way down to non-NumPy arrays, so you'd want to [wrap each pointer in a NumPy array](https://pybind11.readthedocs.io/en/stable/advanced/pycpp/numpy.html#arrays)",
     "createdAt":"2022-06-17T09:06:50Z",
     "number":2970154,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Well, actually, there are exceptions to exceptions and although we're moving the implementation of Awkward Arrays from C++ to Python, we're adding new ways of creating Awkward Arrays in C++.\r\n\r\nThis is from @ManasviGoyal's draft PR #1494: https://github.com/scikit-hep/awkward/blob/1ada7670af7c386478a5b818dc967a8c9e44f604/src/awkward/cpp-headers/GrowableBuffer.h\r\n\r\nThe difference here is that the new array-creating code is in a file of header-only template definitions, obliviating all the issues with linking against compiled shared libraries. Also, it's not the main codebase, the implementation of the arrays themselves (which was a maintenance burden for us, since it's so much easier to develop in Python). This just constructs arrays.\r\n\r\nThinking about @d-leroy's question, I guess he doesn't actually need LayoutBuilder to exist, he just needs to do the same sort of thing that LayoutBuilder does when jumping the border from C++ to Python. He needs to present his data to Python as a [Py_buffer object](https://docs.python.org/3/c-api/buffer.html), which can be done in pybind11 (for example) using [py::buffer_info](https://pybind11.readthedocs.io/en/stable/advanced/pycpp/numpy.html). I'm assuming the data will be owned by a C++ object\u2014either the ownership will be managed by putting a [Py_DECREF](https://docs.python.org/3/c-api/refcounting.html) in the C++ object's destructor or through pybind11's [return value policies](https://pybind11.readthedocs.io/en/stable/advanced/functions.html?highlight=ownership#return-value-policies). Or maybe they're long-lived objects and he'll just keep them around forever or handle ownership manually. Anyway, if it's not done right, Python can segfault as easily as C++ can.\r\n\r\nActually, if you're not worried about ownership safety at all, you can make a buffer entirely on the Python side with only two integers: the pointer value and length of the buffer. [This SO question](https://stackoverflow.com/a/23934417/1623645) explains how to do it; a NumPy array is a buffer and can be used in [ak.from_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_buffers.html) (though the NumPy dtype will be ignored).\r\n\r\nSo assuming that he has Python buffer objects for each of the arrays that goes toward building an Awkward Array (the `data` of a NumpyArray, the `offsets` of a ListOffsetArray, etc.), then he just has to describe how they fit together with a [Form](https://awkward-array.readthedocs.io/en/latest/ak.forms.Form.html) (which can be JSON) and he has to know the length of the final array. That's the interface of [ak.from_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_buffers.html).\r\n\r\nIt may be useful to experiment with [ak.to_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_buffers.html) to see what these things look like for a given array of a desired type. Also there's a [tutorial on using them here](https://awkward-array.org/how-to-convert-buffers.html). The `from_buffers`/`to_buffers` pair is _slightly_ different between version 1 and the [impending version 2](https://github.com/scikit-hep/awkward/wiki#current-status), as are the Form constructors, so it's probably a good idea to learn and build on the version 2 interface. (The differences I'm talking about is where the functions are located, whether they have underscores between words of their names, the spelling and defaults of some arguments, etc.)",
        "createdAt":"2022-06-17T12:42:43Z",
        "number":2971359
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Right - to add some context, one of the motivating factors for replacing the C++ layer with Python was that whilst it didn't offer performance gains (in fact, harmed performance), it did prove tricky to link against, and limited integration with other libraries like Jax (see https://indico.cern.ch/event/855454/contributions/4605044/ for more). That's why Jim refers to the new header-only library that is being built in `cpp-templates`.\r\n\r\nAs Jim and Ianna elaborate, @d-leroy just needs to pass their data to the Python constructors, whether that's `ak.zip`, `ak.from_buffers`, or even the actual layout objects themselves. If they want to provide a nice API that the user calls, they must either write a thin Python shim that collects these buffers and builds the layout object, or directly call awkward from C++ using the pybind11 Python API: https://pybind11.readthedocs.io/en/stable/advanced/embedding.html#adding-embedded-modules\r\n\r\nNote that, as well as pybind11, there's also nanobind.",
        "createdAt":"2022-06-17T18:12:53Z",
        "number":2973659
       }
      ],
      "totalCount":2
     }
    },
    {
     "author":{
      "login":"ianna"
     },
     "body":"@d-leroy - I think, you can try the following:\r\n\r\n1. Wrap C++ arrays as numpy arrays. This is just an example:\r\n```python\r\n>>> A = np.array([1,2,3,4])\r\n>>> B = np.array([1.1, 2.2, 3.3, 4.4, 5.5])\r\n>>> C = np.array([1+0j, 2+2j, 3+3j])\r\n```\r\n\r\n2. Describe the `Form` of the future `ak._v2.Array` you want to get. Note, you need to provide a unique `\"form_key\"` for each content:\r\n```\r\n>>> form_str = \"\"\"\r\n...      {\r\n...          \"form_key\": \"node0\",\r\n...          \"class\": \"RecordArray\",\r\n...          \"contents\": {\r\n...              \"A\": {\r\n...                  \"class\": \"NumpyArray\",\r\n...                  \"primitive\": \"int64\",\r\n...                  \"form_key\": \"node1\"\r\n...              },\r\n...              \"B\": {\r\n...                  \"class\": \"NumpyArray\",\r\n...                  \"primitive\": \"float64\",\r\n...                  \"form_key\": \"node2\"\r\n...              },\r\n...              \"C\": {\r\n...                  \"class\": \"NumpyArray\",\r\n...                  \"primitive\": \"complex128\",\r\n...                  \"form_key\": \"node3\"\r\n...              }\r\n...          }\r\n...      }\r\n...      \"\"\"\r\n>>> \r\n>>> form = ak._v2.forms.from_json(form_str)\r\n```\r\n3. Define a dictionary to associate the `\"form_key\"` with each numpy array:\r\n```python\r\n>>> buffers = {\"node1-data\": A, \"node2-data\": B, \"node3-data\": C}\r\n```\r\n4. Pass it to `ak._v2.from_buffers`:\r\n```python\r\n>>> array = ak._v2.from_buffers(form, 3, buffers)\r\n>>> array\r\n<Array [{A: 1, B: 1.1, C: 1+0j}, ..., {...}] type='3 * {A: int64, B: float6...'>\r\n``` \r\n",
     "createdAt":"2022-06-17T13:59:50Z",
     "number":2971959,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-06-17T07:57:03Z",
  "number":1509,
  "title":"Awkward array as view over collection of C++ pointers?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1509"
 },
 {
  "author":{
   "login":"ZhenxuanZhang-Jensen"
  },
  "body":"Hi experts,\r\n   Recently I used a preselection framework to output a parquet file for further analysis, and in the framework, I save a jagged array in the output parquet file to save the different jets in each event:\r\n``` python\r\nevents['SelectedJet_mass']\r\norigin print: \r\n [[32.3, 18.5, 7.44, 6.9, 8.02, 9.36, 8.24], ... 6.08], [24.7, 8.09, 9.2, 12.8, 4.04]]\r\ntype: <class 'awkward.highlevel.Array'>\r\nthe list of this info: \r\n [[32.34375, 18.484375, 7.4375, 6.8984375, 8.0234375, 9.359375, 8.2421875], [18.625, 18.984375, 9.40625, 10.0, 6.58203125, 6.27734375], [12.921875, 12.28125, 6.58203125, 6.27734375], [40.375, 14.5703125, 6.48046875, 7.921875, 6.8359375, 6.58203125], [12.84375, 12.765625, 8.390625, 8.78125, 8.0859375], [16.09375, 8.109375, 11.2578125, 4.9140625], [16.625, 3.734375, 4.296875, 5.20703125], [11.03125, 15.8359375, 12.1875, 7.015625], [8.359375, 6.12890625, 4.375, 3.984375, 5.30859375], [10.1953125, 7.24609375, 10.2734375, 8.1953125], [27.578125, 9.9609375, 7.01171875, 6.81640625, 6.23828125], [10.296875, 10.0703125, 7.7265625, 5.24609375], [16.578125, 10.1875, 8.9453125, 5.19140625],...]\r\n```\r\nAnd for now, I need to change it to the root file to compute the limit. I use this package to do that:\r\n``` python\r\nfrom parquet_to_root import parquet_to_root\r\nparquet_to_root(\"merged_nominal_addVars.parquet\", \"data.root\", treename='Data_13TeV_cat0', verbose=False)\r\n ```\r\nAnd it shows me a bug like this:\r\n```shell\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/tmp/ipykernel_526/1388603186.py in <module>\r\n      2 from parquet_to_root import parquet_to_root\r\n      3 # parquet_to_root(\"/eos/user/z/zhenxuan/hhwwgg_data/merged_nominal_addVars.parquet\", \"/eos/user/z/zhenxuan/hhwwgg_root/hhwwgg_data.root\", treename='Data_13TeV_cat0', verbose=False)\r\n----> 4 parquet_to_root(\"/eos/user/z/zhenxuan/hhwwgg_M300_test/merged_nominal.parquet\", \"/eos/user/z/zhenxuan/hhwwgg_root/hhwwgg_check_jaggedArray.root\", treename='Data_13TeV_cat0', verbose=False)\r\n\r\n~/.local/lib/python3.9/site-packages/parquet_to_root/parquet_to_root_pyroot.py in parquet_to_root_pyroot(infiles, outfile, treename, verbose)\r\n    150             _setup_branch_list(field, tree, vectorlens, stringarrs)\r\n    151         else:\r\n--> 152             raise ValueError(f'Cannot translate field \"{branch}\" of input Parquet schema. Field is described as {field.type}')\r\n    153 \r\n    154     # Fill loop\r\n\r\nValueError: Cannot translate field \"SelectedJet_mass\" of input Parquet schema. Field is described as large_list<item: float not null>\r\n```\r\nAs far as I know, if we change the awkward array to root file, we actually change the array field to the root's tree branches. If so, it seems no way we can save different numbers of variable in different branches so we can't convert the jagged array to root file right?\r\nIf so, the only method is to pad it all to the same length and then convert it right?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"I think, from a cursory glance of the source code, `parquet_to_root` only supports writing single-field lists to a TTree. I suspect that your list has more than one field in order for you to see that error.\r\n\r\nI'd suggest using [uproot](https://uproot.readthedocs.io/en/stable/basic.html#writing-ttrees-to-a-file) to write to a TTree.",
     "createdAt":"2022-06-22T09:44:28Z",
     "number":3000830,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Hi Angus,\r\n   I do some check on the uproot to write the TTree. I find that if we use uproot to write the jagged array in the tree and use uproot to open it, then it's perfectly fine. But if we want to open it with ROOT,then the result looks strange to me and I can't understand. As I said above, I try to convert my jagged array file to root file so that I can do some further moves on a C++ framework. So do you know how to use ROOT(you know, like TFile TTree GetEntry) to open the tree and branch and access the right numbers?\r\nBy the way, Do you know how to write the code and the output with like \">>>\" or \"...\" in GitHub so that I can show my code and result more beautifully and clearly? thanks!\r\n```python\r\nimport uproot\r\nfile = uproot.recreate(\"example.root\")\r\nimport awkward as ak\r\nfile[\"tree3\"] = {\"branch\": ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]])}\r\nfile[\"tree3\"].show()\r\n```\r\n```shell\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nnbranch              | int32_t                  | AsDtype('>i4')\r\nbranch               | double[]                 | AsJagged(AsDtype('>f8'))\r\n```\r\n```python\r\nimport ROOT\r\n\r\ntfile = ROOT.TFile(\"example.root\")\r\ntree = tfile.Get('tree3')\r\nnentries = tree.GetEntries()\r\nfor i in range(0,nentries):\r\n    print(tree.GetEntry(i))\r\n```\r\n```shell\r\n28\r\n4\r\n20\r\n```",
        "createdAt":"2022-06-22T13:06:51Z",
        "number":3002120
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"[`TTree::GetEntry` loads the given entry and returns the number of bytes read](https://root.cern.ch/doc/master/classTTree.html#a14c88179bd5fd2116228707d6addea9f). In PyROOT, to read the branch, you can write \r\n\r\n```python3\r\n>>> import ROOT\r\n>>> import numpy as np\r\n>>> tfile = ROOT.TFile(\"example.root\")\r\n>>> tree = tfile.Get('tree3')\r\n>>> for entry in tree:\r\n>>>     branch = np.asarray(entry.branch)\r\n>>>     print(branch)    \r\n[1.1 2.2 3.3]\r\n[]\r\n[4.4 5.5]\r\n```\r\n\r\nthe loop over `tree` is syntactic sugar for your for loop. The `entry` object is actually just `tree`.",
        "createdAt":"2022-06-22T13:23:48Z",
        "number":3002245
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Okay, thanks a lot",
        "createdAt":"2022-06-23T03:43:02Z",
        "number":3006921
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-06-22T09:40:16Z",
  "number":1512,
  "title":"How to change the output parquet file with jagged array to root file?",
  "url":"https://github.com/scikit-hep/awkward/discussions/1512"
 },
 {
  "author":{
   "login":"pkausw"
  },
  "body":"Dear experts,\r\n\r\n(apologies if this is a duplicate question/issue -- I looked around but found nothing, so I decided to open a new discussion)\r\n\r\nWe are developing a new setup for analyses in our group which heavily relies on awkward and its nice features! My question particularly revolves around the read/write capabilities of awkward to parquet. We'd like to load only specific columns from nested parquet file structures that were created used `awkward.to_parquet`. Right now, the `awkward.from_parquet` function only supports names of \"top-level\" fields (such as `Jet`) but not something more specific, e.g. `Jet.list.item.pt` as it is supported by the parquet format.\r\n\r\nWhile poking around, I found [this issue](https://issues.apache.org/jira/browse/ARROW-14485) from last October where Jim already tried exactly the syntax from above and noticed that the `nullable` argument in the underlying pyarrow struct is not correctly preserved by the `pyarrow.parquet.ParequetFile.read_row_group` function.\r\n\r\nNow here is my question: Are there ongoing efforts to include loading specific column from nested parquet files? Or is the plan for `awkward` to wait for a fix of this issue on the pyarrow side? My naive assumption would be that the loss of the `nullable` information doesn't break anything, right? Just trying it out myself, the resulting awkward array is usable the same way, though the underlying layout changed of course. (Again, this is very naive, no idea about the effects this might have downstream)\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"I'll wait for another developer who's worked on the V2 Parquet interface to give a more concrete answer, but from glancing at [the source](https://github.com/scikit-hep/awkward/blob/main/src/awkward/_v2/operations/ak_from_parquet.py), I'd wager that we will support nested fields in the manner that you describe.\r\n\r\nAs to the PyArrow bug, assuming that it hasn't been fixed, that would be a show-stopper. However, it looks like we have a work-around already: https://github.com/scikit-hep/awkward/commit/a719f8ccac3af84e436ecc366cc0690e53bda6fa, which I assume is covered by this test: https://github.com/scikit-hep/awkward/blob/a719f8ccac3af84e436ecc366cc0690e53bda6fa/tests/v2/test_0593-preserve-nullability-in-arrow-and-parquet.py\r\n\r\nI haven't tested any of this, but you can! If you install the [latest RC](https://pypi.org/project/awkward/1.9.0rc7/), we have the V2 API under `awkward._v2`. If we've implemented this support, you could try it out.",
     "createdAt":"2022-06-30T13:50:23Z",
     "number":3056639,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"You're right, @agoose77, that the version 2 implementation has a lot more options for limiting the read\u2014motivated by the fact that v1 would often be used with lazy arrays, but now we're separating the laziness from Awkward into [dask-awkward](https://github.com/ContinuumIO/dask-awkward/) and the non-Dask Awkward function will need to be more tuneable.\r\n\r\nHere's how to do a projected read. I'm importing Awkward as\r\n\r\n```python\r\n>>> import awkward as ak\r\n```\r\n\r\nso that all of my \"`._v2`\"s are explicit, but you could import `awkward._v2 as ak`.\r\n\r\nIn order to know what columns there are, to know what to ask for, we can get a metadata object:\r\n\r\n```python\r\n>>> filename = \"https://pivarski-princeton.s3.amazonaws.com/chicago-taxi.parquet\"\r\n>>> metadata = ak._v2.metadata_from_parquet(filename)\r\n```\r\n\r\nThis has a number of things, including Parquet's own metadata, with useful fields, such as `metadata.metadata.num_rows` (number of entries) and `metadata.metadata.num_row_groups` (number of individually readable chunks). But for now, let's look at the type, which is a property of `metadata.form`.\r\n\r\n```python\r\n>>> metadata.form.type.show()\r\nvar * {\r\n    trip: {\r\n        sec: ?float32,\r\n        km: ?float32,\r\n        begin: {\r\n            lon: ?float64,\r\n            lat: ?float64,\r\n            time: ?datetime64[ms]\r\n        },\r\n        end: {\r\n            lon: ?float64,\r\n            lat: ?float64,\r\n            time: ?datetime64[ms]\r\n        },\r\n        path: var * {\r\n            londiff: float32,\r\n            latdiff: float32\r\n        }\r\n    },\r\n    payment: {\r\n        fare: ?float32,\r\n        tips: ?float32,\r\n        total: ?float32,\r\n        type: var * char\r\n    },\r\n    company: var * char\r\n}\r\n```\r\n\r\nThis is a deeply nested view of the type. Parquet likes to think of data as being members of columns, which are the leaves of this tree. They have names with dots:\r\n\r\n```python\r\n>>> metadata.form.columns()\r\n['trip.sec',\r\n 'trip.km',\r\n 'trip.begin.lon',\r\n 'trip.begin.lat',\r\n 'trip.begin.time',\r\n 'trip.end.lon',\r\n 'trip.end.lat',\r\n 'trip.end.time',\r\n 'trip.path.londiff',\r\n 'trip.path.latdiff',\r\n 'payment.fare',\r\n 'payment.tips',\r\n 'payment.total',\r\n 'payment.type',\r\n 'company']\r\n```\r\n\r\nYou can select a few of these (single string/list of strings/possibly with wildcards) from the Form, with projects the type to have only the columns you pick:\r\n\r\n```python\r\n>>> metadata.form.select_columns([\"trip.path.*diff\", \"payment.type\"]).type.show()\r\nvar * {\r\n    trip: {\r\n        path: var * {\r\n            londiff: float32,\r\n            latdiff: float32\r\n        }\r\n    },\r\n    payment: {\r\n        type: var * char\r\n    }\r\n}\r\n```\r\n\r\nPassing this to the `from_parquet` function reduces the amount of data you need to read. So does limiting the set of row groups. (This is a half-GB file, and we're pulling the largest columns.)\r\n\r\n```python\r\n>>> array = ak._v2.from_parquet(filename, columns=[\"trip.path.*diff\", \"payment.type\"], row_groups=[0])\r\n```\r\n\r\nThe array we pulled has only the columns we asked for.\r\n\r\n```python\r\n>>> array.show()\r\n[[{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n ...,\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: []}, payment: {type: ..., ...}}, ..., {trip: {...}, ...}],\r\n [{trip: {path: []}, payment: {type: ..., ...}}, ..., {trip: {...}, ...}],\r\n [{trip: {path: []}, payment: {type: ..., ...}}, ..., {trip: {...}, ...}],\r\n [{trip: {path: []}, payment: {type: ..., ...}}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: []}, payment: {type: 'Cash'}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}],\r\n [{trip: {path: [{...}, ...]}, payment: {...}}, {...}, ..., {trip: {...}, ...}]]\r\n>>> array.type.show()\r\n353 * var * ?{\r\n    trip: {\r\n        path: var * {\r\n            londiff: float32,\r\n            latdiff: float32\r\n        }\r\n    },\r\n    payment: {\r\n        type: string\r\n    }\r\n}\r\n```\r\n\r\nI think the effect of [ARROW-14485](https://issues.apache.org/jira/browse/ARROW-14485) is to make the taxi records nullable, when they weren't nullable in the original file. I also think there's something wrong with the metadata form presenting the `payment.type` as `var * char`, rather than `string`, as it's supposed to be.\r\n\r\nBut these issues won't stop data analysis:\r\n\r\n```python\r\n>>> array.trip.path.londiff * 100\r\n<Array [[[-0.00241, -0.00241], ...], ...] type='353 * var * option[var * fl...'>\r\n```\r\n\r\n(See [upcoming tutorial](https://github.com/jpivarski-talks/2022-07-11-scipy-loopy-tutorial/blob/main/exercises/exercise-3-EVALUATED.ipynb) for more on this file.)",
     "createdAt":"2022-06-30T15:23:45Z",
     "number":3057439,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"pkausw"
     },
     "body":"Sorry for the late reply! Thanks for the explanation and pointing me to `awkward._v2` and the tutorial! This answers my question -- I think we'll either life with loading 'to many' column for now or implement something in the current version of awkward until version 2.0 goes live :+1:",
     "createdAt":"2022-07-06T12:27:57Z",
     "number":3092101,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":3
  },
  "createdAt":"2022-06-30T13:39:17Z",
  "number":1522,
  "title":"Load specific columns from parquet files",
  "url":"https://github.com/scikit-hep/awkward/discussions/1522"
 },
 {
  "author":{
   "login":"fstrug"
  },
  "body":"Link to page: https://awkward-array.org/how-to-restructure-add-fields.html\r\n\r\nI need some further documentation so that I can understand how to add a new \"branch\" to an awkward array that comes from an opened TTree using uproot. It seems that TTrees opened in uproot follow a similar structure to this example:\r\n\r\n```\r\narray1 = ak.zip({\"x\": [1, 2, 3, 4, 5], \"y\": [1.1, 2.2, 3.3, 4.4, 5.5]})\r\n```\r\nHow would I go about adding a new field to this array such that it is properly indexed/formatted with the original array. If we try:\r\n```\r\narray1 = ak.zip({\"x\": [1, 2, 3, 4, 5], \"y\": [1.1, 2.2, 3.3, 4.4, 5.5]})\r\narray2 = ak.zip({\"z\" : [6,7,8,9,10]})\r\nprint(repr(array1))\r\nprint(repr(array2))\r\n**Output:**\r\n<Array [{x: 1, y: 1.1}, ... {x: 5, y: 5.5}] type='5 * {\"x\": int64, \"y\": float64}'>\r\n<Array [{z: 6}, {z: 7}, ... {z: 9}, {z: 10}] type='5 * {\"z\": int64}'>\r\n```\r\nNow if we try combine array1 and array2 together.\r\n```\r\nnew_array = ak.zip([array1, array2])\r\nprint(repr(new_array))\r\n```\r\nWe get this output\r\n`<Array [({x: 1, y: 1.1}, ... {z: 10})] type='5 * ({\"x\": int64, \"y\": float64}, {\"...'>`\r\nInstead of something like:\r\n`<Array [{x: 1, y: 1.1,z:6}, . . ., {x:5, y:5.5, z:10} ] type='5 * ({\"x\": int64, \"y\": float6...'>`\r\nHow can we add the field \"z\" and get the desired format?\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"What you're getting is nested: \"tuple of (record of x and y) and (record of z)\". A \"tuple\" is a record without field names (these two fields are `\"0\"` and `\"1\"`), so this is records-within-records because you constructed a record array out of two record arrays.\r\n\r\nYou can flatten the field structure like this:\r\n\r\n```python\r\nnew_array = ak.zip({\"x\": array1.x, \"y\": array1.y, \"z\": array2.z})\r\n```\r\n\r\nor more automatically like this:\r\n\r\n```python\r\nnew_array = ak.zip(dict(zip(ak.fields(array1), ak.unzip(array1))) | dict(zip(ak.fields(array2), ak.unzip(array2))))\r\n```\r\n\r\nusing the `dict1 | dict2` [syntax from Python 3.9+](https://peps.python.org/pep-0584/), or if you have an older Python, there's `{**dict1, **dict2}`.",
     "createdAt":"2022-07-14T17:49:11Z",
     "number":3149257,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"fstrug"
        },
        "body":"I am able to get this working, but I'm not having the same luck when applying this to my data. I get an error when trying to reconstruct the array from itself (I haven't been able to add a 'field' yet because of this). With the following:\r\n```\r\nfilename = \"root://kodiak-se.baylor.edu//store/user/lpcsusyhad/Stop_production/Summer16_94X_v3/PostProcessed_11Apr2019_fastsimv5_v6p1_v6p5/SMS_T2tt_mStop_150to250_fastsim_\\\r\n2016/SMS_T2tt_mStop150_mLSP0_fastsim_2016_Skim_070621_1_082136_0.root:Events\"\r\nevents = uproot.open(filename)\r\nbatch_size = 1000\r\nfor batch in events.iterate(step_size=batch_size, library='ak'):\r\n    recreated_batch = ak.zip(dict(zip(ak.fields(batch), ak.unzip(batch))))\r\n```\r\nGives me the following.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/uscms_data/d3/fstrug/SusyAnalysis_CMSSW_12_0_4/CMSSW_12_0_4/src/Studies/acc_study/SB_acc_study.py\", line 81, in <module>\r\n    recreated_batch = ak.zip(dict(zip(ak.fields(batch), ak.unzip(batch))))\r\n  File \"/uscms_data/d3/fstrug/SusyAnalysis_CMSSW_12_0_4/CMSSW_12_0_4/src/python3_9/lib/python3.9/site-packages/awkward/operations/structure.py\", line 664, in zip\r\n    out = ak._util.broadcast_and_apply(\r\n  File \"/uscms_data/d3/fstrug/SusyAnalysis_CMSSW_12_0_4/CMSSW_12_0_4/src/python3_9/lib/python3.9/site-packages/awkward/_util.py\", line 1166, in broadcast_and_apply\r\n    out = apply(broadcast_pack(inputs, isscalar), 0, user)\r\n  File \"/uscms_data/d3/fstrug/SusyAnalysis_CMSSW_12_0_4/CMSSW_12_0_4/src/python3_9/lib/python3.9/site-packages/awkward/_util.py\", line 919, in apply\r\n    outcontent = apply(nextinputs, depth + 1, user)\r\n  File \"/uscms_data/d3/fstrug/SusyAnalysis_CMSSW_12_0_4/CMSSW_12_0_4/src/python3_9/lib/python3.9/site-packages/awkward/_util.py\", line 969, in apply\r\n    nextinputs.append(x.broadcast_tooffsets64(offsets).content)\r\nValueError: in ListOffsetArray64, cannot broadcast nested list\r\n```\r\nThis tree does have branches that contain vectors such as jet_pts for an event, but I am not sure if this is what the error is referring to.",
        "createdAt":"2022-07-14T19:23:21Z",
        "number":3149795
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"What it's referring to is that the lists you're trying to zip together have a different size. For instance, you could zip and array of \"muon pt\" with \"muon eta\" because even though every event has a different number of muons, it has the same number of muon pts as it has muon etas. But it wouldn't be able to zip \"muon pt\" with \"jet pt\" because there can be a different number of muons as jets.\r\n\r\n(The v2 error message is better than this one... But what's in production is still version 1.)\r\n\r\nAnyway, you can set a `depth_limit` for the `ak.zip` ([see documentation](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html)), which keeps it from zipping through all dimensions. You can definitely put \"muon pt\" in the same record array as \"jet pt\", as long as they're like this:\r\n\r\n```python\r\n{\"muon_pt\": [#, #, #], \"jet_pt\": [#, #, #, #, #]}\r\n```\r\n\r\nThe down-side is that this also prevents the muon fields from being zipped. That is, you get this:\r\n\r\n```python\r\n{\"muon_pt\": [#, #, #], \"muon_eta\": [#, #, #]}\r\n```\r\n\r\nrather than this:\r\n\r\n```python\r\n{\"muon\": {\"pt\": #, \"eta\": #}}\r\n```\r\n\r\nwhich is what _I'd_ want if I were you. (You get to think of muons as objects, rather than collections of unconnected attributes.)\r\n\r\nYou can get what you really want if you do it manually:\r\n\r\n```python\r\nak.zip({\r\n    \"muon\": ak.zip({\r\n        \"pt\": batch[\"Muon_pt\"],\r\n        \"eta\": batch[\"Muon_eta\"],\r\n        \"phi\": batch[\"Muon_phi\"],\r\n        \"mass\": batch[\"Muon_mass\"],\r\n    }),\r\n    \"jet\": ak.zip({\r\n        \"pt\": batch[\"Jet_pt\"],\r\n        \"eta\": batch[\"Jet_eta\"],\r\n        \"phi\": batch[\"Jet_phi\"],\r\n        \"mass\": batch[\"Jet_mass\"],\r\n    }),\r\n})\r\n```\r\n\r\nThe reason this has to be done manually is because it needs input from you about which branches have the same number of items in each event and which don't. If there's a naming convention that guarantees this, then you can script that.",
        "createdAt":"2022-07-14T21:18:54Z",
        "number":3150391
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-07-14T14:59:51Z",
  "number":1549,
  "title":"How to restructure arrays by adding fields",
  "url":"https://github.com/scikit-hep/awkward/discussions/1549"
 },
 {
  "author":{
   "login":"lnoehte"
  },
  "body":"Hi,\r\nI just ran into something unexpected when masking a RecordArray. If I have to mask RecordArrays differently compared to normal Arrays, let me know. I didn't find anything about that, yet.\r\nI am using awkward version 1.8.0.\r\n\r\nTo give an example which behavior I would expect, I will mask an Array first:\r\n\r\n```python\r\nimport awkward as ak\r\narray1 = ak.Array(\r\n    [\r\n        [53.1, 30.2],\r\n        [],\r\n        [91.7, 88.6, 20.3],\r\n    ]\r\n)\r\nprint(\"array1:\")\r\nprint(array1)\r\n\r\nmask1 = array1 > 40\r\nprint(\"mask1:\")\r\nprint(mask1)\r\n\r\nprint()\r\nprint(\"slicing array1 with mask1:\")\r\nprint(array1[mask1])\r\nprint(\"masking array1 with mask1:\")\r\nprint(array1.mask[mask1])\r\nprint(ak.mask(array1, mask1))\r\n```\r\n\r\n\r\nThe output is the following:\r\n\r\n```python\r\narray1:\r\n[[53.1, 30.2], [], [91.7, 88.6, 20.3]]\r\nmask1:\r\n[[True, False], [], [True, True, False]]\r\n\r\nslicing array1 with mask1:\r\n[[53.1], [], [91.7, 88.6]]\r\nmasking array1 with mask1:\r\n[[53.1, None], [], [91.7, 88.6, None]]\r\n[[53.1, None], [], [91.7, 88.6, None]]\r\n```\r\n\r\nThis is exactly what I was expecting to get.\r\n\r\nNow if I do the same thing with a RecordArray:\r\n\r\n```python\r\narray2 = ak.Array(\r\n    [\r\n        {\r\n            'Muon_pt': [53.1, 30.2],\r\n        },\r\n        {\r\n            'Muon_pt': [],\r\n        },\r\n        {\r\n            'Muon_pt': [91.7, 88.6, 20.3],\r\n        }\r\n    ]\r\n)\r\nprint(\"array2:\")\r\nprint(array2)\r\nprint(array2.tolist())\r\n\r\nmask2 = array2.Muon_pt > 40\r\nprint(\"mask2:\")\r\nprint(mask2)\r\n\r\nprint()\r\nprint(\"slicing array2 with mask2:\")\r\nprint(array2[mask2])\r\nprint(\"masking array2 with mask2:\")\r\nprint(array2.mask[mask2])\r\nprint(ak.mask(array2, mask2))\r\n\r\nprint()\r\nprint(\"to list:\")\r\nprint()\r\nprint(\"slicing array2 with mask2:\")\r\nprint(array2[mask2].tolist())\r\nprint(\"masking array2 with mask2:\")\r\nprint(array2.mask[mask2].tolist())\r\nprint(ak.mask(array2, mask2).tolist())\r\n```\r\n\r\nThe output is:\r\n```python\r\narray2:\r\n[{Muon_pt: [53.1, 30.2]}, {Muon_pt: []}, {Muon_pt: [91.7, 88.6, 20.3]}]\r\n[{'Muon_pt': [53.1, 30.2]}, {'Muon_pt': []}, {'Muon_pt': [91.7, 88.6, 20.3]}]\r\nmask2:\r\n[[True, False], [], [True, True, False]]\r\n\r\nslicing array2 with mask2:\r\n[{Muon_pt: [53.1]}, {Muon_pt: []}, {Muon_pt: [91.7, 88.6]}]\r\nmasking array2 with mask2:\r\n[[{Muon_pt: [53.1, 30.2]}, None], ... 20.3]}, {Muon_pt: [91.7, 88.6, 20.3]}, None]]\r\n[[{Muon_pt: [53.1, 30.2]}, None], ... 20.3]}, {Muon_pt: [91.7, 88.6, 20.3]}, None]]\r\n\r\nto list:\r\n\r\nslicing array2 with mask2:\r\n[{'Muon_pt': [53.1]}, {'Muon_pt': []}, {'Muon_pt': [91.7, 88.6]}]\r\nmasking array2 with mask2:\r\n[[{'Muon_pt': [53.1, 30.2]}, None], [], [{'Muon_pt': [91.7, 88.6, 20.3]}, {'Muon_pt': [91.7, 88.6, 20.3]}, None]]\r\n[[{'Muon_pt': [53.1, 30.2]}, None], [], [{'Muon_pt': [91.7, 88.6, 20.3]}, {'Muon_pt': [91.7, 88.6, 20.3]}, None]]\r\n```\r\n\r\nBut what I was expecting was:\r\n\r\n```python\r\n[{'Muon_pt': [53.1, None]}, {'Muon_pt': []}, {'Muon_pt': [91.7, 88.6, None]}]\r\n```\r\n\r\nWhat I see is, that the shape of array2 changed and is now the shape of mask2. I would expect ak.mask to preserve the shape of the initial array.\r\n\r\nI don't know if this is the normal behavior of RecordArrays but I would be happy to find a way to get the expected output.\r\n\r\nThanks for all comments and suggestions.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"What's happening here is that the mask and the array are first broadcast together. In this case, this changes the type of the array:\r\n\r\n```python3\r\n>>> array2.type\r\n3 * {\"Muon_pt\": var * float64}\r\n\r\n>>> mask2.type\r\n3 * var * bool\r\n\r\n>>> ak.broadcast_arrays(array2,mask2)\r\n[<Array [[{Muon_pt: [53.1, 30.2, ... 20.3]}]] type='3 * var * {\"Muon_pt\": var * f...'>,\r\n <Array [[{Muon_pt: [True, True, ... False]}]] type='3 * var * {\"Muon_pt\": var * ...'>]\r\n```\r\n\r\nIt's not so obvious why `array2.mask[mask2]` doesn't produce the result that you expect until you consider what would happen if your record had more than a single field. In that case, if e.g. it had a list field called `x`, how should this be masked if `x` had a different shape to `Muon_pt`?\r\n\r\nMore specifically, you need to provide Awkward with your mask in a RecordArray, e.g. \r\n```python\r\nmask2 = ak.zip({\r\n   \"Muon_pt\": array2.Muon_pt > 40\r\n}, depth_limit=1)\r\n```\r\nThis will ensure that the broadcast result matches what you expect to happen.\r\n\r\nWhether this is the right approach for your problem, though, I can't say! :)\r\n",
     "createdAt":"2022-08-12T18:10:27Z",
     "number":3386425,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Pinging @jpivarski because I feel like there's more to add here, but *right now* my brain is not 100% working!",
        "createdAt":"2022-08-12T18:11:45Z",
        "number":3386433
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"This looks good! Sorry that I was too busy to help out earlier.",
        "createdAt":"2022-08-12T21:52:18Z",
        "number":3387494
       }
      ],
      "totalCount":2
     }
    },
    {
     "author":{
      "login":"lnoehte"
     },
     "body":"Thanks a lot for the quick response.\r\n\r\nWhen I saw the shape of `mask2` in `array2` I already suspected something happening when broadcasting, that I didn't expect.\r\n\r\nI also tried it with more than one field, like\r\n\r\n```python\r\narray2 = ak.Array(\r\n    [\r\n        {\r\n            'Muon_pt': [53.1, 30.2],\r\n            'Muon_eta': [1.1, 0.2],\r\n        },\r\n        {\r\n            'Muon_pt': [],\r\n            'Muon_eta': [],\r\n        },\r\n        {\r\n            'Muon_pt': [91.7, 88.6, 20.3],\r\n            'Muon_eta': [0.6, -0.5],\r\n        }\r\n    ]\r\n)\r\n```\r\n\r\nI tried to keep the first example simple, because it seems to yield the same result:\r\n\r\n```python\r\narray2:\r\n[{Muon_pt: [53.1, 30.2], Muon_eta: [1.1, 0.2], ... 20.3], Muon_eta: [0.6, -0.5]}]\r\n[{'Muon_pt': [53.1, 30.2], 'Muon_eta': [1.1, 0.2]}, {'Muon_pt': [], 'Muon_eta': []}, {'Muon_pt': [91.7, 88.6, 20.3], 'Muon_eta': [0.6, -0.5]}]\r\nmask2:\r\n[[True, False], [], [True, True, False]]\r\n\r\nslicing array2 with mask2:\r\n[{Muon_pt: [53.1], Muon_eta: [1.1]}, ... 91.7, 88.6], Muon_eta: [0.6, -0.5]}]\r\nmasking array2 with mask2:\r\n[[{Muon_pt: [53.1, 30.2], Muon_eta: [1.1, 0.2], ... Muon_eta: [0.6, -0.5]}, None]]\r\n[[{Muon_pt: [53.1, 30.2], Muon_eta: [1.1, 0.2], ... Muon_eta: [0.6, -0.5]}, None]]\r\n\r\nto list:\r\n\r\nslicing array2 with mask2:\r\n[{'Muon_pt': [53.1], 'Muon_eta': [1.1]}, {'Muon_pt': [], 'Muon_eta': []}, {'Muon_pt': [91.7, 88.6], 'Muon_eta': [0.6, -0.5]}]\r\nmasking array2 with mask2:\r\n[[{'Muon_pt': [53.1, 30.2], 'Muon_eta': [1.1, 0.2]}, None], [], [{'Muon_pt': [91.7, 88.6, 20.3], 'Muon_eta': [0.6, -0.5]}, {'Muon_pt': [91.7, 88.6, 20.3], 'Muon_eta': [0.6, -0.5]}, None]]\r\n[[{'Muon_pt': [53.1, 30.2], 'Muon_eta': [1.1, 0.2]}, None], [], [{'Muon_pt': [91.7, 88.6, 20.3], 'Muon_eta': [0.6, -0.5]}, {'Muon_pt': [91.7, 88.6, 20.3], 'Muon_eta': [0.6, -0.5]}, None]]\r\n```\r\nBut ultimately, I have a bunch of fields in that RecordArray.\r\n\r\nThanks a lot for the example using `ak.zip`.\r\nIf I see it correctly, this only works if I have only the \"Muon_pt\" field.\r\nI quickly tried to make it work with the new `array2` from above by having two identical fields with the keys `Muon_pt` and `Muon_eta` in the mask but till now I was unsuccessful with that approach.\r\n\r\nI am also wondering if I really need to have duplicates of this `array2.Muon_pt > 40` array for each field that exists in `array2` in the `mask2`, considering that I actually have a few more than just two. ",
     "createdAt":"2022-08-12T19:14:20Z",
     "number":3386750,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"@lnoehte it depends upon what you're trying to do here. The main issue here is that your record structures are not deeply nested, i.e. your array has type\r\n```python\r\n3 * {\"Muon_pt\": var * float64, \"Muon_eta\": var * float64}\r\n```\r\ninstead of \r\n```python\r\n3 * var * {\"Muon_pt\": float64, \"Muon_eta\": float64}\r\n```\r\n\r\nThe form of your array is an important choice that should make it easy to work with your data in a natural way. In the case of records, that means choosing whether to have `var * {'x': float6}` or `{'x': var * float64}`. In this instance, it feels like you want the former, not the latter. Otherwise, you need to slice each field, which is what we discussed above :)\r\n",
        "createdAt":"2022-08-15T10:22:54Z",
        "number":3397240
       },
       {
        "author":{
         "login":"lnoehte"
        },
        "body":"Thanks a lot for the reply.\r\nI believe it helped a lot for my understanding.\r\n\r\nMaybe I should explain my situation.\r\nI read a ROOT file with flat Ntuple like structure and I load branches with the `.arrays()` method. What I get after reading a subset of the branches is:\r\n```python\r\n1089694 * {\"Muon_pt\": var * float32, \"Muon_eta\": var * float32, \"Muon_phi\": var * float32, \"Muon_tightId\": var * bool, \"Muon_pfRelIso04_all\": var * float32, \"Muon_charge\": var * int32}\r\n```\r\nSo I don't have any influence in the original data structure. But I just found that using `how=\"zip\"` in the `.arrays()` method gives me:\r\n```python\r\n1089694 * {\"Muon\": var * {\"pt\": float32, \"eta\": float32, \"phi\": float32, \"tightId\": bool, \"pfRelIso04_all\": float32, \"charge\": int32}}\r\n```\r\nwhich is maybe a step in the right direction. \r\n\r\nI will keep testing the next days.\r\n",
        "createdAt":"2022-08-15T14:28:24Z",
        "number":3398826
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Ah, OK. This is commonly seen with reading ROOT files - the level of jaggedness isn't correlated by default between branches, so the record is added at a higher depth. You can zip together branches manually, and set the appropriate `depth_limit` if you want. Or, you can pass `how=\"zip\"` to `TTree.arrays()` from uproot, and uproot will do this for you.",
        "createdAt":"2022-08-15T14:34:42Z",
        "number":3398881
       },
       {
        "author":{
         "login":"lnoehte"
        },
        "body":"Due to other duties I didn't find enough time to follow up on this the last weeks.\r\nI wanted to continue the next days investigating how to mask my RecordArrays properly.\r\n\r\n@agoose77 Could you maybe help me out and give an example of how the RecordArray structure/layout should look like, to get the behavior I see when slicing, where broadcasting doesn't seem to be an issue, so I know what to aim for when trying different ways to load my input file with uproot.",
        "createdAt":"2022-08-31T14:42:17Z",
        "number":3518627
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"In my [above comment](https://github.com/scikit-hep/awkward/discussions/1585#discussioncomment-3397240), the difference between a record of arrays and array of records is:\r\n\r\nRecord of arrays:\r\n```python\r\n3 * {\"Muon_pt\": var * float64, \"Muon_eta\": var * float64}\r\n```\r\n\r\nArray of records:\r\n```python\r\n3 * var * {\"Muon_pt\": float64, \"Muon_eta\": float64}\r\n```\r\n\r\nUproot returns the \"record of arrays\", as by default there is no information about whether the branches are compatible with one another.\r\n\r\nYou can use pass `how=\"zip\"` into the `arrays()` method that `uproot` provides to instruct it to give you the branches as a zipped array. You might need to limit `arrays()` to only the branches that you want to zip together. A total example:\r\n\r\n```python\r\nevents = uproot.open(...)\r\n\r\nmomentum = events.arrays([\"Muon_pt\", \"Muon_eta\"], how=\"zip\")\r\n``` ",
        "createdAt":"2022-08-31T15:08:40Z",
        "number":3518884
       },
       {
        "author":{
         "login":"lnoehte"
        },
        "body":"It took a while again but I created some tests.\r\nI decided to prepare a test file to show my test results with the exact same structure as my input file.\r\n\r\nUnfortunately both array types don't broadcast properly. It is still unclear to me why broadcasting works for slicing but not for masking. It doesn't look to me anymore as if it was an issue with the type, since slicing works in all cases we discussed above.\r\n\r\nTo show what's happening with the arrays for the different array types, I prepared a notebook, which you can find here:\r\n[https://mybinder.org/v2/gh/lnoehte/test_uproot_awkward_with_nanoAOD.git/main](https://mybinder.org/v2/gh/lnoehte/test_uproot_awkward_with_nanoAOD.git/main)\r\n\r\nI would be curious to see an example with `ak.mask` that works with RecordArray structures, that have more than one field.\r\n\r\nThanks a lot in advance again.",
        "createdAt":"2022-09-15T10:03:35Z",
        "number":3652413
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"@lnoehte thank you for the MyBinder example, it helps to clarify your expectations. \r\n\r\nYour \"Automatic zip \u2192 Masking\" example is another instance of https://github.com/scikit-hep/awkward/discussions/1585#discussioncomment-3386425 \r\n\r\nRecords in Awkward make it possible for arrays to have dimensions that are not defined by a single number, i.e. each field of an array could be an array of different dimension. Therefore, for the most part, records behave like \"atoms\" in Awkward; many operations stop when they hit a record.\r\n\r\nHere's an excerpt of your data:\r\n```python\r\nimport awkward as ak\r\nimport numpy as np\r\n\r\narray = ak.from_iter(\r\n    [\r\n        {\r\n            \"Muon\": [\r\n                {\r\n                    \"pt\": 53.02759552001953,\r\n                    \"eta\": 1.1298828125,\r\n                    \"phi\": 1.251220703125,\r\n                    \"tightId\": True,\r\n                    \"pfRelIso04_all\": 0.018847892060875893,\r\n                    \"charge\": -1,\r\n                },\r\n                {\r\n                    \"pt\": 30.640729904174805,\r\n                    \"eta\": 0.162200927734375,\r\n                    \"phi\": -2.2509765625,\r\n                    \"tightId\": True,\r\n                    \"pfRelIso04_all\": 0.016491081565618515,\r\n                    \"charge\": 1,\r\n                },\r\n            ]\r\n        },\r\n        {\"Muon\": []},\r\n        {\r\n            \"Muon\": [\r\n                {\r\n                    \"pt\": 91.78971099853516,\r\n                    \"eta\": 0.66748046875,\r\n                    \"phi\": -1.208251953125,\r\n                    \"tightId\": True,\r\n                    \"pfRelIso04_all\": 0.0,\r\n                    \"charge\": -1,\r\n                },\r\n                {\r\n                    \"pt\": 88.6688232421875,\r\n                    \"eta\": -0.584716796875,\r\n                    \"phi\": 1.92626953125,\r\n                    \"tightId\": True,\r\n                    \"pfRelIso04_all\": 0.005749838892370462,\r\n                    \"charge\": 1,\r\n                },\r\n            ]\r\n        },\r\n        {\r\n            \"Muon\": [\r\n                {\r\n                    \"pt\": 36.98743438720703,\r\n                    \"eta\": -0.3990478515625,\r\n                    \"phi\": -1.376708984375,\r\n                    \"tightId\": True,\r\n                    \"pfRelIso04_all\": 0.0,\r\n                    \"charge\": 1,\r\n                },\r\n                {\r\n                    \"pt\": 32.94463348388672,\r\n                    \"eta\": -2.0595703125,\r\n                    \"phi\": 1.7509765625,\r\n                    \"tightId\": True,\r\n                    \"pfRelIso04_all\": 0.08915365487337112,\r\n                    \"charge\": -1,\r\n                },\r\n            ]\r\n        },\r\n        {\r\n            \"Muon\": [\r\n                {\r\n                    \"pt\": 16.899534225463867,\r\n                    \"eta\": 2.24853515625,\r\n                    \"phi\": -1.506591796875,\r\n                    \"tightId\": False,\r\n                    \"pfRelIso04_all\": 0.20170965790748596,\r\n                    \"charge\": -1,\r\n                }\r\n            ]\r\n        },\r\n        {\r\n            \"Muon\": [\r\n                {\r\n                    \"pt\": 42.16910171508789,\r\n                    \"eta\": 2.115234375,\r\n                    \"phi\": 0.918701171875,\r\n                    \"tightId\": True,\r\n                    \"pfRelIso04_all\": 0.046248190104961395,\r\n                    \"charge\": 1,\r\n                }\r\n            ]\r\n        },\r\n        {\"Muon\": []},\r\n        {\r\n            \"Muon\": [\r\n                {\r\n                    \"pt\": 3.306669235229492,\r\n                    \"eta\": 0.9600830078125,\r\n                    \"phi\": -0.5576171875,\r\n                    \"tightId\": False,\r\n                    \"pfRelIso04_all\": 0.11646004021167755,\r\n                    \"charge\": -1,\r\n                }\r\n            ]\r\n        },\r\n        {\r\n            \"Muon\": [\r\n                {\r\n                    \"pt\": 3.9849512577056885,\r\n                    \"eta\": 1.9267578125,\r\n                    \"phi\": -2.970703125,\r\n                    \"tightId\": False,\r\n                    \"pfRelIso04_all\": 2.214714288711548,\r\n                    \"charge\": 1,\r\n                }\r\n            ]\r\n        },\r\n        {\"Muon\": []},\r\n    ]\r\n)\r\n\r\n```\r\n\r\nIf we look at the type, we can see that there's an outermost \"Muon\" record:\r\n```python\r\n>>> array.type\r\n10 * {\r\n    Muon: var * {\r\n        pt: float64,\r\n        eta: float64,\r\n        phi: float64,\r\n        tightId: bool,\r\n        pfRelIso04_all: float64,\r\n        charge: int64\r\n    }\r\n}\r\n```\r\n\r\nTherefore, it's not surprising that this array has dimension 1:\r\n```python\r\n>>> array.ndim\r\n1\r\n```\r\n\r\nLooking at your mask, which is\r\n```python\r\n>>> array.Muon.eta.type # ignore what we do to this, just care about dimensionality\r\n10 * var * float64\r\n```\r\n\r\nwe see that it has two dimensions. Therefore, when the array and the mask are broadcast against one-another, we'll use left-broadcasting to make the dimensions agree, i.e.\r\n```\r\n10 * var * float64\r\n10 * var * {Muon: var ...}\r\n        ^^^ \r\n        new dimension added by broadcasting\r\n```\r\nIt therefore makes sense that the result of the mask has this new dimension:\r\n```python\r\n>>> array.mask[array.Muon.pt > 0].type\r\n10 * var * ?{\r\n    Muon: var * {\r\n        pt: float64,\r\n        eta: float64,\r\n        phi: float64,\r\n        tightId: bool,\r\n        pfRelIso04_all: float64,\r\n        charge: int64\r\n    }\r\n}\r\n```\r\n\r\nYou need to decide whether you want to mask the eta values _within_ the records, or whether you want to mask the records themselves. If you want to mask _within_ records (i.e. only touch array.Muon.eta, not array.Muon.phi) then you need to unzip, mask, and rezip the array. If you want to mask the record itself (which I believe you do), then the easiest thing to do is to pull the `Muon` field out of the array:\r\n```python\r\nmasked_muon = array.Muon.mask[array.Muon.eta > 0]\r\n```",
        "createdAt":"2022-09-15T22:32:50Z",
        "number":3657846
       },
       {
        "author":{
         "login":"lnoehte"
        },
        "body":"That explanation makes sense to me.\n\nIndeed, if I mask in eta, I also want to mask the same entries in phi and all the other fields, since they belong to the same particle.\n\nBecause of the \"Muon\" field around the array, I also showed the method when zipping manually.\nThis should already behave like pulling out the Muon field from your last example. Please correct me if I am mistaken. Unfortunately it still doesn't mask correctly but shows broadcast issues as well.",
        "createdAt":"2022-09-16T05:34:40Z",
        "number":3659412
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Your \"manual zip\" example looks as I would expect it - you're correctly masking out the _records_ (`['pt', 'eta', 'phi', 'tightId', 'pfRelIso04_all', 'charge']`) according to the value of `eta`. The option type masks-out those records that do not satisfy your mask.\r\n\r\nThe difference between slicing and masking is subtle - well observed! I realise that you looked into this in your Binder example; I was looking at this quite late last night, so I focussed on the particular challenge of masking.\r\n\r\nUltimately slicing and masking follow different paths. The masking code broadcasts the mask against the array, which stops at the `Muon` record as discussed above. Meanwhile, the _slicing_ path doesn't use broadcasting; it traverses into the array being sliced, following a set of rules. \r\n\r\nAt this point, I will ping @jpivarski to clarify our intentions here, because slicing and masking are highly similar operations, and I wonder whether this discrepancy is a good thing. I feel like we've covered this topic before, but I couldn't find anything in our issue tracker. Jim \u2013 do we _want_ slicing and masking to be different here? Is it intentional that the mask to stop at the first record, or is it just an artifact of stopping _broadcasting_ at records? i.e. do we need to change this, or is it deliberate?",
        "createdAt":"2022-09-16T07:57:32Z",
        "number":3660335
       },
       {
        "author":{
         "login":"lnoehte"
        },
        "body":"No worries, I've seen that is was quite late yesterday. I am super grateful to get so much support here.\r\n\r\nConcerning the \"manual zip\", it masks out the entries. I just realized that I might have read the structure incorrectly there and though it added a `None` at the wrong place when it might have done the right thing.\r\n\r\nI'll take another look at the \"manual zip\" method and come back to you later today. I hope with success. If not, probably with more questions ;)",
        "createdAt":"2022-09-16T10:07:09Z",
        "number":3661363
       },
       {
        "author":{
         "login":"lnoehte"
        },
        "body":"OK, I believe it looks fine to me as well.\r\nI must have been confused from all the curly brackets. :D\r\n\r\nOne small question about the type. What does the `?` mean? Is it a sign that it's a mixed type because of the `None` added?",
        "createdAt":"2022-09-16T13:00:17Z",
        "number":3662493
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"`?xxx` is the same as `option[xxx]`. It's just easier to read than `option[...]`, so we use it where possible. The type grammar is heavily based upon [datashape](https://datashape.readthedocs.io/en/latest/overview.html#option)",
        "createdAt":"2022-09-16T13:09:02Z",
        "number":3662551
       },
       {
        "author":{
         "login":"lnoehte"
        },
        "body":"Thanks a lot.\r\nI cannot mark replies as answers.\r\nDespite it being my \"answer\" (more a question), I'll mark this one here, since your first reply basically contains the answer to this discussion. I hope people will find it there.",
        "createdAt":"2022-09-20T06:13:04Z",
        "number":3686974
       }
      ],
      "totalCount":13
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-08-12T17:54:08Z",
  "number":1585,
  "title":"RecordArray masks differently than normal Array",
  "url":"https://github.com/scikit-hep/awkward/discussions/1585"
 },
 {
  "author":{
   "login":"Teddy-Curtis"
  },
  "body":"Hey!\r\n\r\nSo I am having a slight problem with the speed of reading events from a parquet dataset.\r\nEssentially, I have a large dataset in parquet files that I need to loop over to save individually for Pytorch.\r\nExample data point:\r\nak.Record of :\r\n``` \r\n{'node_data' : [1,4,2,5,7,8,....],  'edge_data' : [[1,2], [0,1], .....], .....}\r\n```\r\nNow the data is too large to be loaded in at once so I read it lazily:\r\n```\r\ndataset = ak.from_parquet(\"Documents/Data\", lazy=True)\r\nfor data in dataset:\r\n    do.....\r\n```\r\nHowever, with this, the cache ends up getting bigger and bigger until it crashes, so instead I try:\r\n```\r\ndataset = ak.from_parquet(\"Documents/Data\", lazy=True, lazy_cache=None)\r\nfor data in dataset:\r\n    do.....\r\n```\r\nBut now, even though I'm still only reading each datapoint once, it runs far slower than without setting lazy_cache=None. Is there any reason for this to be so much slower even though I am only reading each data point once?\r\n\r\nDoes anyone have any advice on how I should proceed?\r\nI'm guessing there might be a way to limit the amount that is cached so it still runs quickly without using up more and more ram, however I'm still a bit confused as to why the speed should change between the two scenarios.\r\n\r\nAny help would be extremely appreciated!\r\nThanks!\r\n\r\n\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Without seeing your file, I can't give a detailed explanation as to why this happens. But, roughly:\r\n\r\nWhen you pass `lazy=True`, Awkward reads the structure of the array, but not the contents. For partitioned arrays (Parquet files with row groups), this means that Awkward can read only a single row group if you slice it appropriately. The caching mechanism means that if you try and read two parts of the array that refer to the same portion of the underlying file, it will re-use the data from the cache, saving you a `read()` call.\r\n\r\nSetting `lazy_cache=None` means that Awkward must request the data from the file whenever the contents need to be resolved, because it does not use a cache. This will be slower in cases where you're reading overlapping parts of the same file. In your case, if a row-group spans more than one row of your file, it will re-read the entire row group for each iteration.\r\n\r\nWhat you probably want to do here is read a full row-group, and handle the laziness manually, e.g.\r\n```python\r\ndef iter_rows_lazy(file, **kwargs):\r\n    array = ak.from_parquet(file, lazy=True, **kwargs)\r\n    n_row_groups = len(ak.partitions(array))\r\n   \r\n    for i in range(n_row_groups):\r\n        yield from ak.from_parquet(file, row_groups=i, **kwargs)\r\n \r\n \r\n for row in iter_rows_lazy(\"Documents/Data\"):\r\n     do_something(row)\r\n\r\n```\r\n\r\nI've not tested this yet, so it might need some tweaks.",
     "createdAt":"2022-08-25T16:32:01Z",
     "number":3476394,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"Teddy-Curtis"
        },
        "body":"Thanks for the fast reply!\r\nTo give a bit more info about my problem, I have attached a file to this (I had to zip it so I could attach it, .parquet files aren't supported for upload)  [train_0.parquet.zip](https://github.com/scikit-hep/awkward/files/9431879/train_0.parquet.zip)\r\nFor some reason this happens even if I just read one individual file and not and entire folder of files. For example if I do:\r\n```\r\ndataset = ak.from_parquet(\"Documents/data/train_0.parquet\", lazy=True, lazy_cache=None)\r\n```\r\n```\r\n%%timeit\r\nfor data in dataset:\r\n    PT = data.pt\r\n```\r\nThis returns:\r\n> 4.16 s \u00b1 135 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nHowever, if instead do:\r\n```\r\ndataset = ak.from_parquet(\"Documents/data/train_0.parquet\", lazy=True)\r\n```\r\nand run the same loop I get \r\n> 317 ms \u00b1 4.57 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nSo 4.16s for the entire loop with lazy_cache=None compared to 317ms without setting lazy_cache!\r\n\r\n\r\nUsing your iter function though does help a lot, if I have two files in the train folder and do the same loop:\r\n```\r\ndataset = ak.from_parquet(\"Documents/data/\", lazy=True, lazy_cache=None)\r\n```\r\n> 13 s \u00b1 306 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nvs\r\n\r\n```\r\ndataset = ak.from_parquet(\"Documents/data/\", lazy=True)\r\n```\r\n> 692 ms \u00b1 15.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nvs\r\n```\r\nfor data in iter_rows_lazy(\"/home/emc21/Documents/hgcal_l1t_analysis/GNN/Data/raw/train\"):\r\n    PT = data.pt\r\n```\r\n> 1.52 s \u00b1 101 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nSo it is far quicker than with lazy_cache=None\r\n\r\nAny idea why it would still happen even if im just reading one file?\r\nThanks for the help, the function you gave helps tremendously!\r\n",
        "createdAt":"2022-08-26T09:18:46Z",
        "number":3481717
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"`iter_rows_lazy` main intention is to read the row-groups directly, but only one row-group at a time.  In your case, the Parquet file is not partitioned into more than one row group, so there is no benefit in terms of memory savings to use this function vs directly calling `ak.from_parquet(..., lazy=False)`\r\n\r\nThe function above is faster than lazily reading this single file because it's just reading the entire file into memory (less work!).\r\n\r\nHowever, I suspect that your dataset comprises of multiple files? If so, the function above _would_ help your memory problems, because each file would form a partition IIRC. Iterating over the outer rows would only need to load one row group at a time. \r\n\r\nThe reason you're observing no-cache lazy reading to be slower than the cached variant is that each operation on the lazy array will need to read data. So, for each iteration, Awkward Array is trying to resolve the virtual (lazy) node in the layout of the array and, without a cache, that means reading from disk. \r\n\r\nIn short, using `lazy_cache=None` is not something you ever should need to be doing. If you want to reduce memory usage, you need to read the array one row-group at a time. You can still do this with a single call to `ak.from_parquet(..., lazy=True, lazy_cache=cache)`; periodically deleting the cache will ensure your memory doesn't grow too large. The proper way to do this, though, is to only keep one row-group's cache at any one time. One way to do this is the method I suggested, another is to use `ak.partitions` to figure out how long each partition is. I prefer the former for simplicity.\r\n\r\nAs Jim says, all of this is going away in the future. Most users probably don't want lazy arrays, but instead just want to read one row-group at a time. We can improve our documentation on this. \r\nThe replacement, [`dask-awkward`](https://github.com/ContinuumIO/dask-awkward/) builds our laziness into a proper scheme that can be distributed across a network. One of the benefits should be that Dask has memory-aware scheduling, so in many cases memory should be something you don't intrinsically need to worry about.",
        "createdAt":"2022-08-26T10:38:24Z",
        "number":3482367
       },
       {
        "author":{
         "login":"Teddy-Curtis"
        },
        "body":"Ahhhh OK that makes a lot of sense now!\r\nHowever, even with the function you gave above:\r\n```\r\ndef iter_rows_lazy(file, **kwargs):\r\n    array = ak.from_parquet(file, lazy=True)\r\n    n_row_groups = len(ak.partitions(array))\r\n   \r\n    for i in range(n_row_groups):\r\n        yield from ak.from_parquet(file, row_groups=i, **kwargs)\r\n```\r\nI'm still getting a memory error after about 30,000 iterations: (skipping a couple of the first lines\r\n```\r\nTraceback (most recent call last):\r\n...\r\n...\r\n...\r\n  File \"HGCal/GNN/Data/directed2LayersK5/datasetClass.py\", line 82, in process\r\n    for i, graph in enumerate(iter_rows_lazy(file)):\r\n  File \"HGCal/GNN/Data/directed2LayersK5/datasetClass.py\", line 74, in iter_rows_lazy\r\n    yield from ak.from_parquet(file, row_groups=i, **kwargs)\r\n  File \"/home/user1/miniconda3/envs/tGPU/lib/python3.8/site-packages/awkward/operations/convert.py\", line 3960, in from_parquet\r\n    batches = dataset.read_row_group_batches()\r\n  File \"/home/user1/miniconda3/envs/tGPU/lib/python3.8/site-packages/awkward/operations/convert.py\", line 3469, in read_row_group_batches\r\n    batches.extend(self.read_row_group(i, columns).to_batches())\r\n  File \"/home/user1/miniconda3/envs/tGPU/lib/python3.8/site-packages/awkward/operations/convert.py\", line 3678, in read_row_group\r\n    return single_file.read_row_group(\r\n  File \"/home/user1/miniconda3/envs/tGPU/lib/python3.8/site-packages/pyarrow/parquet/__init__.py\", line 421, in read_row_group\r\n    return self.reader.read_row_group(i, column_indices=column_indices,\r\n  File \"pyarrow/_parquet.pyx\", line 1334, in pyarrow._parquet.ParquetReader.read_row_group\r\n  File \"pyarrow/_parquet.pyx\", line 1353, in pyarrow._parquet.ParquetReader.read_row_groups\r\n  File \"pyarrow/error.pxi\", line 117, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowMemoryError: realloc of size 1073741824 failed\r\n```\r\n\r\nSo I get the idea behind it, but it seems like it isn't completely refreshing the cache every time this is called:\r\n```\r\nfor i in range(n_row_groups):\r\n    yield from ak.from_parquet(file, row_groups=i, **kwargs)\r\n```\r\n\r\nSo maybe it's easier doing the other way with my own cache that I reset say every time a new row group is opened?\r\nWould this be done but just passing cache = {} into lazy_cache = cache then clearing that?\r\nFor example I've quickly written this:\r\n```\r\ndef iter_lazy(file, **kwargs):\r\n    cache = {}\r\n    dataset = ak.from_parquet(file, lazy=True, lazy_cache=cache, **kwargs)\r\n    partitions = ak.partitions(dataset)\r\n    part_idx = 0\r\n    part = partitions[part_idx]\r\n    for i, data in enumerate(dataset):\r\n        if i > part:\r\n            cache = {}\r\n            part_idx += 1\r\n            part += partitions[part_idx]\r\n        yield data\r\n```\r\n\r\nSorry for the mountain of questions!",
        "createdAt":"2022-08-26T13:31:22Z",
        "number":3483797
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"That looks like quite a big re-allocation. Do you have many columns in your dataset? You can \r\n```bash\r\npip install parquet-tools\r\nparquet-tools inspect /path/to/parquet-file.parquet\r\n```\r\nto inspect your file and e.g. determine how many row groups it has. It seems like you have a very large row group, so I'm wondering if the files are themselves not properly partitioned.",
        "createdAt":"2022-08-26T13:46:13Z",
        "number":3483953
       },
       {
        "author":{
         "login":"Teddy-Curtis"
        },
        "body":"Using parquet-tools on my file gives me:\r\n```\r\n\r\n############ file meta data ############\r\ncreated_by: parquet-cpp-arrow version 9.0.0\r\nnum_columns: 7\r\nnum_rows: 11141\r\nnum_row_groups: 1\r\nformat_version: 2.6\r\nserialized_size: 2168\r\n\r\n\r\n############ Columns ############\r\nitem\r\nitem\r\nitem\r\ngen_matched\r\npt\r\nbdteg\r\nquality\r\n\r\n############ Column(item) ############\r\nname: item\r\npath: node_pos.list.item.list.item\r\nmax_definition_level: 2\r\nmax_repetition_level: 2\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 12%)\r\n\r\n############ Column(item) ############\r\nname: item\r\npath: node_data.list.item.list.item\r\nmax_definition_level: 2\r\nmax_repetition_level: 2\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 3%)\r\n\r\n############ Column(item) ############\r\nname: item\r\npath: edge_data.list.item.list.item\r\nmax_definition_level: 2\r\nmax_repetition_level: 2\r\nphysical_type: INT64\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 19%)\r\n\r\n############ Column(gen_matched) ############\r\nname: gen_matched\r\npath: gen_matched\r\nmax_definition_level: 0\r\nmax_repetition_level: 0\r\nphysical_type: INT64\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: -4%)\r\n\r\n############ Column(pt) ############\r\nname: pt\r\npath: pt\r\nmax_definition_level: 0\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 24%)\r\n\r\n############ Column(bdteg) ############\r\nname: bdteg\r\npath: bdteg\r\nmax_definition_level: 0\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 18%)\r\n\r\n############ Column(quality) ############\r\nname: quality\r\npath: quality\r\nmax_definition_level: 0\r\nmax_repetition_level: 0\r\nphysical_type: INT64\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 7%)\r\n```\r\nSeems a bit weird that a couple of the columns are called 'item' instead they should be called node_pos, node_data, edge_data. Maybe that's what's causing me the issue? The *_data columns can be quite large but I only have 11141 rows and 7 columns. The file sizes aren't huge either with the biggest ones being ~50mb.\r\nAlso, if I output ak.partitions(dataset) for my entire dataset I get:\r\n```\r\n[11141, 13500, 13500, 13500, 13500, 11141, 13500, 13500, 13500, 13500, 11141, 13500, 13500, 13500, 13500, 13500, 13500, 3622, 11141, 11141, 13500, 222, 11141, 11141, 11141, 13500, 11141, 11141]\r\n```\r\nSo none are particularly huge compared to any others",
        "createdAt":"2022-08-26T14:06:23Z",
        "number":3484152
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Sorry, I should have clarified - can you do this for _all_ of your files? The file you uploaded is fairly small, but I'm wondering if one of the files is much larger / unpartitioned vs the others.\r\n\r\nRE the `item` name, it's the way that the Parquet schema describes a list. I assume you have a list-of-lists in `node_pos`, e.g. a list of 3-coordinate vectors.",
        "createdAt":"2022-08-26T15:12:22Z",
        "number":3484723
       },
       {
        "author":{
         "login":"Teddy-Curtis"
        },
        "body":">RE the item name, it's the way that the Parquet schema describes a list. I assume you have a list-of-lists in node_pos, e.g. a list of 3-coordinate vectors.\r\n\r\nOk that's exactly what I have so that should be fine. \r\nAs for the other files, if I find num_row_groups, they all equal 1, the most amount of rows is 13500 and the largest file size is 50.1MiB. \r\nMaybe I could try and split all the files into smaller ones first and then try and run the same loop and see if that does anything?",
        "createdAt":"2022-08-26T15:29:49Z",
        "number":3484905
       }
      ],
      "totalCount":7
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Thanks for answering this, @agoose77! The one point I want to add is that Awkward 1.x's lazy arrays are being removed: this is the biggest change from 1.x to 2.x. The v2 `from_parquet` function doesn't have a `lazy` option, but it does let you specify which columns and row groups you want to select from the file.\r\n\r\nAlso, [dask-awkward](https://github.com/ContinuumIO/dask-awkward/) runs on v2, so you'd be replacing any workflows like, \"Load lazy arrays and try to keep them from accidentally reading the whole file while performing computations on a subset of the file,\" with workflows like, \"Load Dask arrays, write computation, then call `.compute()` to have it load what it needs to, given that it can see your whole computation graph before it starts.\" This is a heads-up in case you want to avoid your current problems by trying out the new version.",
     "createdAt":"2022-08-25T17:27:29Z",
     "number":3476795,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-08-25T15:51:33Z",
  "number":1633,
  "title":"Going over all events once without a cache - speed issue",
  "url":"https://github.com/scikit-hep/awkward/discussions/1633"
 },
 {
  "author":{
   "login":"Saransh-cpp"
  },
  "body":"`Vector` currently overrides the `Numba` behavior according to the documentation present [here](https://awkward-array.readthedocs.io/en/latest/ak.behavior.html#overriding-behavior-in-numba), but when switching to `awkward._v2`, the [tests](https://github.com/scikit-hep/vector/blob/main/tests/backends/test_awkward_numba.py) (or using the Awkward-Numba backend in any manner) gives the following error -\r\n\r\n```py\r\n    def test():\r\n        @numba.njit\r\n        def extract(x):\r\n            return x[2][0]\r\n\r\n        array = vector.Array([[{\"x\": 1, \"y\": 2}], [], [{\"x\": 3, \"y\": 4}, {\"x\": 5, \"y\": 6}]])\r\n>       out = extract(array)\r\n\r\ntests/backends/test_awkward_numba.py:24:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n/home/saransh/.local/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args\r\n    error_rewrite(e, 'typing')\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ne = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\\nnon-precise type pyobject\\nDuring: typing of ...lowing argument(s):\\n- argument 0: Cannot determine Numba type of <class \\'vector.backends.awkward.VectorArray2D\\'>\\n')\r\nissue_type = 'typing'\r\n\r\n    def error_rewrite(e, issue_type):\r\n        \"\"\"\r\n        Rewrite and raise Exception `e` with help supplied based on the\r\n        specified issue_type.\r\n        \"\"\"\r\n        if config.SHOW_HELP:\r\n            help_msg = errors.error_extras[issue_type]\r\n            e.patch_message('\\n'.join((str(e).rstrip(), help_msg)))\r\n        if config.FULL_TRACEBACKS:\r\n            raise e\r\n        else:\r\n>           raise e.with_traceback(None)\r\nE           numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)\r\nE           non-precise type pyobject\r\nE           During: typing of argument at /mnt/d/OpenSource/HEP/vector/tests/backends/test_awkward_numba.py (21)\r\nE\r\nE           File \"tests/backends/test_awkward_numba.py\", line 21:\r\nE               def extract(x):\r\nE                   return x[2][0]\r\nE                   ^\r\nE\r\nE           This error may have been caused by the following argument(s):\r\nE           - argument 0: Cannot determine Numba type of <class 'vector.backends.awkward.VectorArray2D'>\r\n\r\n/home/saransh/.local/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError\r\n-------------------------------------------------- Captured log call ---------------------------------------------------\r\nDEBUG    numba.core.entrypoints:entrypoints.py:38 Loading extension: EntryPoint(name='init', value='awkward.numba:register', group='numba_extensions')\r\nDEBUG    numba.core.entrypoints:entrypoints.py:46 Extension loading failed for: EntryPoint(name='init', value='awkward.numba:register', group='numba_extensions')\r\nDEBUG    numba.core.entrypoints:entrypoints.py:38 Loading extension: EntryPoint(name='init', value='vector:register_numba', group='numba_extensions')\r\nDEBUG    numba.core.byteflow:byteflow.py:74 bytecode dump:\r\n>          0    NOP(arg=None, lineno=21)\r\n           2    LOAD_FAST(arg=0, lineno=21)\r\n           4    LOAD_CONST(arg=1, lineno=21)\r\n           6    BINARY_SUBSCR(arg=None, lineno=21)\r\n           8    LOAD_CONST(arg=2, lineno=21)\r\n          10    BINARY_SUBSCR(arg=None, lineno=21)\r\n          12    RETURN_VALUE(arg=None, lineno=21)\r\nDEBUG    numba.core.byteflow:byteflow.py:100 pending: deque([State(pc_initial=0 nstack_initial=0)])\r\nDEBUG    numba.core.byteflow:byteflow.py:103 stack: []\r\nDEBUG    numba.core.byteflow:byteflow.py:278 dispatch pc=0, inst=NOP(arg=None, lineno=21)\r\nDEBUG    numba.core.byteflow:byteflow.py:279 stack []\r\nDEBUG    numba.core.byteflow:byteflow.py:278 dispatch pc=2, inst=LOAD_FAST(arg=0, lineno=21)\r\nDEBUG    numba.core.byteflow:byteflow.py:279 stack []\r\nDEBUG    numba.core.byteflow:byteflow.py:278 dispatch pc=4, inst=LOAD_CONST(arg=1, lineno=21)\r\nDEBUG    numba.core.byteflow:byteflow.py:279 stack ['$x2.0']\r\nDEBUG    numba.core.byteflow:byteflow.py:278 dispatch pc=6, inst=BINARY_SUBSCR(arg=None, lineno=21)\r\nDEBUG    numba.core.byteflow:byteflow.py:279 stack ['$x2.0', '$const4.1']\r\nDEBUG    numba.core.byteflow:byteflow.py:278 dispatch pc=8, inst=LOAD_CONST(arg=2, lineno=21)\r\nDEBUG    numba.core.byteflow:byteflow.py:279 stack ['$6binary_subscr.2']\r\nDEBUG    numba.core.byteflow:byteflow.py:278 dispatch pc=10, inst=BINARY_SUBSCR(arg=None, lineno=21)\r\nDEBUG    numba.core.byteflow:byteflow.py:279 stack ['$6binary_subscr.2', '$const8.3']\r\nDEBUG    numba.core.byteflow:byteflow.py:278 dispatch pc=12, inst=RETURN_VALUE(arg=None, lineno=21)\r\nDEBUG    numba.core.byteflow:byteflow.py:279 stack ['$10binary_subscr.4']\r\nDEBUG    numba.core.byteflow:byteflow.py:138 end state. edges=[]\r\nDEBUG    numba.core.byteflow:byteflow.py:166 -------------------------Prune PHIs-------------------------\r\nDEBUG    numba.core.byteflow:byteflow.py:235 Used_phis: defaultdict(<class 'set'>, {State(pc_initial=0 nstack_initial=0): set()})\r\nDEBUG    numba.core.byteflow:byteflow.py:192 defmap: {}\r\nDEBUG    numba.core.byteflow:byteflow.py:193 phismap: defaultdict(<class 'set'>, {})\r\nDEBUG    numba.core.byteflow:byteflow.py:214 changing phismap: defaultdict(<class 'set'>, {})\r\nDEBUG    numba.core.byteflow:byteflow.py:223 keep phismap: {}\r\nDEBUG    numba.core.byteflow:byteflow.py:229 new_out: defaultdict(<class 'dict'>, {})\r\nDEBUG    numba.core.byteflow:byteflow.py:239 ----------------------DONE Prune PHIs-----------------------\r\nDEBUG    numba.core.byteflow:byteflow.py:150 block_infos State(pc_initial=0 nstack_initial=0):\r\nAdaptBlockInfo(insts=((0, {}), (2, {'res': '$x2.0'}), (4, {'res': '$const4.1'}), (6, {'index': '$const4.1', 'target': '$x2.0', 'res': '$6binary_subscr.2'}), (8, {'res': '$const8.3'}), (10, {'index': '$const8.3', 'target': '$6binary_subscr.2', 'res': '$10binary_subscr.4'}), (12, {'retval': '$10binary_subscr.4', 'castval': '$12return_value.5'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})\r\nDEBUG    numba.core.interpreter:interpreter.py:1342 label 0:\r\n    x = arg(0, name=x)                       ['x']\r\n    $const4.1 = const(int, 2)                ['$const4.1']\r\n    $6binary_subscr.2 = getitem(value=x, index=$const4.1, fn=<built-in function getitem>) ['$6binary_subscr.2', '$const4.1', 'x']\r\n    $const8.3 = const(int, 0)                ['$const8.3']\r\n    $10binary_subscr.4 = getitem(value=$6binary_subscr.2, index=$const8.3, fn=<built-in function getitem>) ['$10binary_subscr.4', '$6binary_subscr.2', '$const8.3']\r\n    $12return_value.5 = cast(value=$10binary_subscr.4) ['$10binary_subscr.4', '$12return_value.5']\r\n    return $12return_value.5                 ['$12return_value.5']\r\n\r\nDEBUG    numba.core.ssa:ssa.py:162 ==== SSA block analysis pass on 0\r\nDEBUG    numba.core.ssa:ssa.py:191 Running <numba.core.ssa._GatherDefsHandler object at 0x7f08be9159a0>\r\nDEBUG    numba.core.ssa:ssa.py:193 on stmt: x = arg(0, name=x)\r\nDEBUG    numba.core.ssa:ssa.py:193 on stmt: $const4.1 = const(int, 2)\r\nDEBUG    numba.core.ssa:ssa.py:193 on stmt: $6binary_subscr.2 = static_getitem(value=x, index=2, index_var=$const4.1, fn=<built-in function getitem>)\r\nDEBUG    numba.core.ssa:ssa.py:193 on stmt: $const8.3 = const(int, 0)\r\nDEBUG    numba.core.ssa:ssa.py:193 on stmt: $10binary_subscr.4 = static_getitem(value=$6binary_subscr.2, index=0, index_var=$const8.3, fn=<built-in function getitem>)\r\nDEBUG    numba.core.ssa:ssa.py:193 on stmt: $12return_value.5 = cast(value=$10binary_subscr.4)\r\nDEBUG    numba.core.ssa:ssa.py:193 on stmt: return $12return_value.5\r\nDEBUG    numba.core.ssa:ssa.py:154 defs defaultdict(<class 'list'>,\r\n            {'$10binary_subscr.4': [<numba.core.ir.Assign object at 0x7f08be8b7fa0>],\r\n             '$12return_value.5': [<numba.core.ir.Assign object at 0x7f08be8b7580>],\r\n             '$6binary_subscr.2': [<numba.core.ir.Assign object at 0x7f08be8b7d90>],\r\n             '$const4.1': [<numba.core.ir.Assign object at 0x7f08be915f40>],\r\n             '$const8.3': [<numba.core.ir.Assign object at 0x7f08be8b7280>],\r\n             'x': [<numba.core.ir.Assign object at 0x7f08be915ca0>]})\r\nDEBUG    numba.core.ssa:ssa.py:156 SSA violators set()\r\nDEBUG    numba.core.typeinfer:typeinfer.py:159 captured error\r\nTraceback (most recent call last):\r\n  File \"/home/saransh/.local/lib/python3.9/site-packages/numba/core/typeinfer.py\", line 155, in propagate\r\n    constraint(typeinfer)\r\n  File \"/home/saransh/.local/lib/python3.9/site-packages/numba/core/typeinfer.py\", line 229, in __call__\r\n    raise TypingError('non-precise type {}'.format(ty))\r\nnumba.core.errors.TypingError: non-precise type pyobject\r\nDuring: typing of argument at /mnt/d/OpenSource/HEP/vector/tests/backends/test_awkward_numba.py (21)\r\nDEBUG    numba.core.typeinfer:typeinfer.py:159 captured error\r\nTraceback (most recent call last):\r\n  File \"/home/saransh/.local/lib/python3.9/site-packages/numba/core/typeinfer.py\", line 155, in propagate\r\n    constraint(typeinfer)\r\n  File \"/home/saransh/.local/lib/python3.9/site-packages/numba/core/typeinfer.py\", line 229, in __call__\r\n    raise TypingError('non-precise type {}'.format(ty))\r\nnumba.core.errors.TypingError: non-precise type pyobject\r\nDuring: typing of argument at /mnt/d/OpenSource/HEP/vector/tests/backends/test_awkward_numba.py (21)\r\n```\r\n\r\nI am not sure how this `Numba` behavior is supposed to be overridden in `awkward._v2`. `Vector` currently does [this](https://github.com/scikit-hep/vector/blob/main/src/vector/backends/awkward.py#L1595-L1986) to override the behavior.\r\n\r\nAdditionally, I am not sure if this is a bug or a change in `awkward`'s API, as I couldn't find any documentation on this. This brings me to my last question - Is there any deployed documentation for `awkward._v2`?\r\n\r\nThank you!\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Whilst we're in the v1\u2192v2 transition, we haven't registered the NumPy or Numba dispatch mechanisms, so calling `np.func(array)` or jitting with Numba doesn't work out of the box. You can register the Numba machinery using the same code from v1 in `ak._connect._numba.__init__`:\r\n```python\r\n@numba.extending.typeof_impl.register(ak.highlevel.Array)\r\ndef typeof_Array(obj, c):\r\n    return obj.numba_type\r\n\r\n@numba.extending.typeof_impl.register(ak.highlevel.Record)\r\ndef typeof_Record(obj, c):\r\n    return obj.numba_type\r\n\r\n@numba.extending.typeof_impl.register(ak.highlevel.ArrayBuilder)\r\ndef typeof_ArrayBuilder(obj, c):\r\n    return obj.numba_type\r\n```",
     "createdAt":"2022-08-29T08:16:10Z",
     "number":3498085,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"Saransh-cpp"
        },
        "body":"Thank you for the answer, @agoose77! This makes sense!",
        "createdAt":"2022-08-29T15:33:35Z",
        "number":3501561
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-08-28T20:10:38Z",
  "number":1639,
  "title":"V2: Overriding behavior in `Numba`",
  "url":"https://github.com/scikit-hep/awkward/discussions/1639"
 },
 {
  "author":{
   "login":"ivirshup"
  },
  "body":"I would like to concatenate arrays of records without getting unions back. Here's a quick illustrative example of a use case:\r\n\r\n```python\r\nimport awkward._v2 as ak\r\n\r\nawk_a = ak.Array([\r\n    {\"a\": [1, 2, 3], \"b\": [1, 2]},\r\n    {\"a\": [4, 5], \"b\": [3, 4]},\r\n    {\"a\": [6], \"b\": [5]},\r\n])\r\n\r\n\r\nawk_b = ak.Array([\r\n    {\"a\": [1, 2, 3]},\r\n    {\"a\": [4, 5]},\r\n    {\"a\": [6]},\r\n])\r\n\r\nak.concatenate([awk_a, awk_b])\r\n```\r\n\r\nI would like to end up with a non-Union result here. E.g. something like:\r\n\r\n```python\r\n# '6 * {a: var * int64, b: var * int64}'\r\nak.Array([\r\n    {\"a\": [1, 2, 3], \"b\": [1, 2]},\r\n    {\"a\": [4, 5], \"b\": [3, 4]},\r\n    {\"a\": [6], \"b\": [5]},\r\n    {\"a\": [1, 2, 3], \"b\": []},\r\n    {\"a\": [4, 5], \"b\": []},\r\n    {\"a\": [6], \"b\": []},\r\n])\r\n\r\n# '6 * {a: var * int64, b: option[var * int64]}'\r\nak.Array([\r\n    {\"a\": [1, 2, 3], \"b\": [1, 2]},\r\n    {\"a\": [4, 5], \"b\": [3, 4]},\r\n    {\"a\": [6], \"b\": [5]},\r\n    {\"a\": [1, 2, 3]},\r\n    {\"a\": [4, 5]},\r\n    {\"a\": [6]},\r\n])\r\n```\r\n\r\nI would ideally like to be able to do this at concatenation time (and even be able to choose just to concatenate the intersection of keys). But, being able to convert from a union to an optional type would also work.\r\n\r\nI'm having some trouble finding docs for this, how would I go about doing this?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"I'd suggest transforming `awk_b` such that it has the necessary field. You could make use of the `ak.Array.__setitem__` feature here, which allows you to add fields to an existing array (advanced users note that you can modify arrays, but not layouts, so this feature replaces the existing layout with a new one). The nice benefit of using `__setitem__` (or indeed `ak.with_field`) is that it broadcasts, so by doing\r\n\r\n```python\r\nawk_b['b'] = None\r\n```\r\n\r\nyou end up with\r\n```python\r\n>>> awk_b.type.show()\r\n3 * {\r\n    a: var * int64,\r\n    b: ?unknown\r\n}\r\n```\r\n\r\nUpon concatenation, this then becomes:\r\n```python\r\n>>> ak.concatenate([awk_a, awk_b]).type.show()\r\n6 * {\r\n    a: var * int64,\r\n    b: option[var * int64]\r\n}\r\n```\r\n\r\nAwkward is still creating a union for the different layout nodes under the hood; `unknown` translates to `EmptyArray` in Awkward's layout node structure, whilst `var * int64` is a list-type over a NumPy array. Yet, Awkward has rules about which types can be merged, and which require a union, and `EmptyArray` can always be merged.\r\n",
     "createdAt":"2022-08-30T11:27:47Z",
     "number":3508086,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"Thanks for the info!\r\n\r\nCan I set field `\"b\"` not to be optional, but to be empty arrays of a fixed type? Maybe something like:\r\n\r\n```python\r\nak.with_field(awk_b, ListType(NumpyType('int64')), \"b\")\r\n```\r\n\r\n---------\r\n\r\nAre there function for operations on types? E.g. Find a common subset between two array types?",
        "createdAt":"2022-08-30T11:54:48Z",
        "number":3508319
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"At present the easiest ways to do this are to take an existing array and e.g. slice it to zero length\r\n```python3\r\nawk_b['b'] = awk_b['a'][..., :0]\r\n```\r\n\r\nIt really depends upon what type field `b` is supposed to be, and how it corresponds to the other fields.\r\n\r\nThe mechanism for building layouts from type information (besides directly instantiating `ak._v2.contents.Content` subclasses) is to use `ak._v2.from_buffers` with `form` information. There is no mapping from type objects to forms, so you'd need to create the form by hand, or from an existing array.\r\n\r\nThere are no type operations per-se. We do have routines that merge different types, so we have logic that can determine if types are mergeable. We don't do anything with subsets of fields, IIRC, though.\r\n\r\n## Advanced\r\nYou can manually build a `ak._v2.contents.Content` layout yourself if you know what you need. For example, the layout of `awk_b` after applying the above is:\r\n```python3\r\n<RecordArray is_tuple='false' len='3'>\r\n    <content index='0' field='a'>\r\n        <ListOffsetArray len='3'>\r\n            <offsets><Index dtype='int64' len='4'>\r\n                [0 3 5 6]\r\n            </Index></offsets>\r\n            <content><NumpyArray dtype='int64' len='6'>[1 2 3 4 5 6]</NumpyArray></content>\r\n        </ListOffsetArray>\r\n    </content>\r\n    <content index='1' field='b'>\r\n        <ListOffsetArray len='3'>\r\n            <offsets><Index dtype='int64' len='4'>\r\n                [0 0 0 0]\r\n            </Index></offsets>\r\n            <content><NumpyArray dtype='int64' len='0'>[]</NumpyArray></content>\r\n        </ListOffsetArray>\r\n    </content>\r\n</RecordArray>\r\n```\r\n\r\nSimilarly, you can use `ak._v2.from_buffers` to build an array from a form and a flat list of buffers. \r\n",
        "createdAt":"2022-08-30T12:19:46Z",
        "number":3508523
       },
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"From the other conversation, do you think `ak.transform` could be a good way to implement this?\r\n\r\nThat way I could recurse down a set of arrays and \"broadcast\" them to an inner or outer union before concatenation.",
        "createdAt":"2022-09-01T15:20:16Z",
        "number":3528156
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"The `__setitem__` that @agoose77 suggested,\r\n\r\n> At present the easiest ways to do this are to take an existing array and e.g. slice it to zero length\r\n> \r\n> ```python\r\n> awk_b['b'] = awk_b['a'][..., :0]\r\n> ```\r\n\r\ndoes use broadcasting to create the new field. (It's implemented with the same backend as `ak.transform`.) Since this high-level mechanism exists to do it, this is definitely simpler than rolling your own.",
        "createdAt":"2022-09-01T18:48:52Z",
        "number":3529757
       },
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"Just trying this out now, and running into an issue. I can get the expected result from:\r\n\r\n```python\r\nawk_a = ak.Array([\r\n    {\"a\": [1, 2, 3], \"b\": [1, 2]},\r\n    {\"a\": [4, 5], \"b\": [3, 4]},\r\n    {\"a\": [6], \"b\": [5]},\r\n])\r\n\r\n\r\nawk_b = ak.Array([\r\n    {\"a\": [1, 2, 3]},\r\n    {\"a\": [4, 5]},\r\n    {\"a\": [6]},\r\n])\r\n\r\nak.concatenate([awk_a, ak.with_field(awk_b, None, \"b\")]).type.show()\r\n```\r\n\r\n```\r\n6 * {\r\n    a: var * int64,\r\n    b: option[var * int64]\r\n}\r\n```\r\n\r\nHowever, I get a `union` from:\r\n\r\n```python\r\nawk_c = ak.Array([\r\n    [{\"a\": 1, \"b\": \"foo\"}],\r\n    [{\"a\": 2, \"b\": \"bar\"}, {\"a\": 3, \"b\": \"baz\"}]\r\n])\r\n\r\nawk_d = ak.Array([\r\n    [{\"a\": 4}, {\"a\": 5}],\r\n    [{\"a\": 6}],\r\n    [{\"a\": 7}],\r\n])\r\n\r\nak.concatenate([awk_c, ak.with_field(awk_d, None, \"b\")]).type.show()\r\n```\r\n\r\n```\r\n5 * union[\r\n    var * {\r\n        a: int64,\r\n        b: string\r\n    },\r\n    var * {\r\n        a: int64,\r\n        b: ?unknown\r\n    }\r\n]\r\n```\r\n\r\nIs this expected?",
        "createdAt":"2022-09-05T15:16:46Z",
        "number":3561037
       },
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"As an aside, is there a way to do something like:\r\n\r\n```python\r\nawk_c.astype(ak.types.from_datashape(\"2 * var * {a: int64, b: ?string}\"))\r\n```\r\n\r\nIf not, is there a fundamental barrier to something like this being possible?",
        "createdAt":"2022-09-05T15:22:12Z",
        "number":3561108
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"> Is this expected?\r\n\r\nI think you found a bug: we want \"string\" and \"option[unknown]\" to unify to \"option[string]\", and it's not recognizing that. It should first notice that one of the two is \"option\" and make the result type \"option\", then unify \"string\" (or \"X\") with \"unknown\" to get \"string\" (\"X\"), since \"unknown\" is an identity in type-unification. I _think_ this is supposed to happen somewhere in [simplify_uniontype](https://github.com/scikit-hep/awkward/blob/1bcfd70629aec4c1ef26bbf929d9183c28dbe491/src/awkward/_v2/contents/unionarray.py#L471-L661). It's interesting that it got it right when \"X\" is a primitive type, int64, but not when it's a list type. This is something we should promote into an issue.\r\n\r\nAs for casting an entire array as an entire type, that sounds like a good function to have, though it would take some thinking to do it right. You'd have to recurse down the array tree and the type tree, modifying array elements based on what we see in the type, and it wouldn't be one-to-one because high-level types are simpler than arrays. So, it's not a fundamental barrier, but it will be quicker for us to fix the bug in type-unification, above.",
        "createdAt":"2022-09-05T17:30:44Z",
        "number":3562477
       }
      ],
      "totalCount":7
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-08-30T10:07:36Z",
  "number":1647,
  "title":"concatenate arrays of records without getting Unions",
  "url":"https://github.com/scikit-hep/awkward/discussions/1647"
 },
 {
  "author":{
   "login":"grst"
  },
  "body":"I would like to follow up on a discussion [on gitter](https://gitter.im/Scikit-HEP/awkward-array?at=62d679e19f73251a2c5ba538) a while ago. Since then I have understood a bit better what I actually need and, while I'm still only scratching the surface, understood a bit better how awkward arrays work. \r\n\r\nWhat I need is a function like this: \r\n```python\r\ndef dim_len(array, dim):\r\n   \"\"\"\r\n   Get the length of the array in the given dimension.\r\n\r\n   Returns a non-negative integer if the array is regular in the given dimension and None otherwise. \r\n\r\n   Examples\r\n   --------\r\n   >>>  dim_len(ak.Array(np.array([[1,2,3],[3,4,5]])), 1) \r\n   3\r\n   >>> dim_len(ak.Array([[1,2,3],[3,4,5]])), 1)\r\n   None\r\n   >>> dim_len(ak.Array([{'a': 1}, {'b': 1}]), 0)\r\n   2\r\n   \"\"\"\r\n   raise NotImplementedError\r\n```\r\n\r\nIn the chat on gitter, it was suggested to recurse through the array types. Is this the way to go? It feels a bit weird that no such function is available as part of the library, I would have expected this to be a rather common use-case. \r\n\r\nIf using the array types is the way to go, could I please get your feedback on if I understood correctly the implications of each type: \r\n\r\n * `ArrayType`: outermost type, retrieve `.length` attribute\r\n * `ListType`: variable-length\r\n * `NumpyType`: regular type, retrieve `.size` attribute\r\n * `OptionType`: Does not represent an own dimension. Get the `.size` attribut of the child in `.content`. \r\n * `RecordType`: End of the recursion. Dimensions after a record type are not counted. \r\n * `RegularType`: regular type, retrieve `.size` attribute\r\n * `UnionType`: Does not represent an own dimension. Get `.size` attribute of the children in `.contents`?. *Could it be that different children have different sizes? Can there be a regular dimension after a UnionType?*\r\n * `UnknownType`: *??*\r\n \r\n CC @ivirshup",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Your table is mostly correct. Whilst I was thinking about this, I noticed that I initially had some confusion as to how types compare with layouts.\r\n\r\nThe notable difference between types and layouts is that in the type system, `UnknownType` and `NumpyType` represent atoms that have no length. Whereas, with layouts, the smallest unit is a singular array (`NumpyArray` or `EmptyArray`). The dimension to which the _types_ belong is their parent, e.g. `RegularType`, `ListType`, etc., whereas the _layouts_ list-types *are* the dimension.  Hence, if you encounter `UnknownType` or `NumpyType` (atoms with no length) during recursion you've gone too far!\r\n\r\nMeanwhile, the list-types, e.g. `RegularType`, `ListType`, correspond to inner-dimensions; the root `ArrayType` is _always_ present, and represents the outermost dimension. \r\n\r\nI wrote out an example of `dim_len`, exploding it into different routines to make it clear what is happening at each level:\r\n```python3\r\nimport awkward._v2 as ak\r\nimport typing as t\r\n\r\nSizeType = t.Union[int, None]\r\n\r\n\r\ndef _dim_size_next(node: ak.types.Type, depth: int) -> SizeType:\r\n    \"\"\"Walk into the contents of the given node.\r\n\r\n    :param node: type node\r\n    :param depth: depth above the intended type node\r\n    :returns: size of type, or None if irregular\r\n    \"\"\"\r\n    # Do we have a container?\r\n    if isinstance(node, (ak.types.ListType, ak.types.RegularType, ak.types.ArrayType)):\r\n        return _dim_size_impl(node.content, depth - 1)\r\n    else:\r\n        raise TypeError(f\"Unexpected node type: {node!r}\")\r\n\r\n\r\ndef _dim_size_at(node: ak.types.Type) -> SizeType:\r\n    \"\"\"Compute the size of the given type node\r\n\r\n    :param node: type node\r\n    :returns: size of type, or None if irregular\r\n    \"\"\"\r\n    if isinstance(node, ak.types.RegularType):\r\n        return node.size\r\n    elif isinstance(node, ak.types.ListType):\r\n        return None\r\n    elif isinstance(node, ak.types.ArrayType):\r\n        return node.length\r\n    else:\r\n        raise TypeError(f\"Unexpected node type: {node!r}\")\r\n\r\n\r\ndef _dim_size_transform(node: ak.types.Type) -> ak.types.Type:\r\n    \"\"\"Transform the given type node to ignore superfluous types\r\n    \r\n    :param node: type node\r\n    :returns: important type\r\n    \"\"\"\r\n    # These types are \"atoms\", and do not correspond to a dimension.\r\n    # As atoms, they have no length, so if we see them, it's an error.\r\n    if isinstance(node, (ak.types.NumpyType, ak.types.UnknownType)):\r\n        raise ValueError(f\"Cannot compute size of atom: {node!r}\")\r\n    # Meanwhile, unions *could* have an interpetable size, but we choose \r\n    # not to handle them. Records are never considered to have a size\r\n    elif isinstance(node, (ak.types.UnionType, ak.types.RecordType)):\r\n        raise TypeError(f\"Cannot compute size through branching types: {node!r}\")\r\n\r\n    # Whether recursing through, or looking at this dimension\r\n    # We don't care about options, so move through them\r\n    if isinstance(node, ak.types.OptionType):\r\n        return _dim_size_transform(node.content)\r\n    else:\r\n        return node\r\n\r\n\r\ndef _dim_size_impl(node: ak.types.Type, depth: int) -> SizeType:\r\n    \"\"\"Dispatcher to compute the size of the given type node\r\n\r\n    :param node: type node\r\n    :param depth: depth above the intended type node\r\n    :returns: size of type, or None if irregular\r\n    \"\"\"\r\n    # Depth-agnostic transforms\r\n    node = _dim_size_transform(node)\r\n\r\n    # Now consider whether we're recursing or at the appropriate depth\r\n    if depth > 0:\r\n        return _dim_size_next(node, depth)\r\n    else:\r\n        return _dim_size_at(node)\r\n\r\n\r\ndef dim_size(array: ak.Array, dim: int) -> SizeType:\r\n    \"\"\"Compute the size of a particular dimension of the given array\r\n\r\n    :param array: Awkward Array\r\n    :param dim: dimension of which to compute the size\r\n    :returns: size of dimension, or None if irregular\r\n    \"\"\"\r\n    return _dim_size_impl(array.type, depth=dim)\r\n```\r\n\r\nAwkward Array operations generally stop at records, so it doesn't make sense to compute the size of these types. However, it _is_ technically possible to compute the size of unions; you just need to resolve the branches at the end. I opted to just make this an error, but you could extend it. \r\n\r\nNote that this doesn't handle negative axis (dim) values. You could do this by first computing the depth of the array, e.g. via `ndim`. Again, this needs special handling for unions; `ndim` ignores the union dimension in these cases.\r\n\r\nYou might use this like:\r\n```python\r\nimport numpy as np\r\narr = ak.Array([\r\n    [\r\n        [\r\n            [1, 2, 3],\r\n            [4, 5, 6, 7]\r\n        ],\r\n        [\r\n            [1, 2, 3],\r\n            [4, 5, 6, 7]\r\n        ]\r\n    ]\r\n])\r\narr = ak.to_regular(arr, axis=2)\r\n\r\nprint(dim_size(arr, 0))\r\nprint(dim_size(arr, 1))\r\nprint(dim_size(arr, 2))\r\nprint(dim_size(arr, 3))\r\n```    \r\n    ",
     "createdAt":"2022-08-31T20:08:03Z",
     "number":3521282,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"As usual, @agoose77 beat me to an answer again! (Sarcasm: I don't mind!)\r\n\r\nI was writing an answer that uses the new `ak.transform` (PR #1610), which I think was inspired, or at least got bumped up in priority by your problem. (It fulfills an old issue #516.) The goal of this interface was to provide a public API for the function we use internally to define functions like these, which would streamline the process of absorbing it into the Awkward codebase, once we have a good idea of what the general API should look like.\r\n\r\nHere's what I came up with, and most of the complication is dealing with the fact that union types introduce branches.\r\n\r\n```python\r\ndef dim_len(array, axis):\r\n    if axis < 0:   # negative axis is another can of worms... maybe later\r\n        raise NotImplementedError\r\n    elif axis == 1:\r\n        return len(array)\r\n    else:\r\n        def size_at_depth(layout, depth, lateral_context, **kwargs):\r\n            if layout.is_NumpyType:\r\n                # if it's an embedded rectilinear array, we have to deal with its shape\r\n                # which might not be 1-dimensional\r\n                if layout.is_UnknownType:\r\n                    shape = (0,)\r\n                else:\r\n                    shape = layout.shape\r\n                numpy_axis = lateral_context[\"axis\"] - depth + 1\r\n                if not (1 <= numpy_axis < len(shape)):\r\n                    raise TypeError(f\"axis={lateral_context['axis']} is too deep\")\r\n                lateral_context[\"out\"] = shape[numpy_axis]\r\n                return layout.nplike.empty(1)\r\n\r\n            elif layout.is_ListType and depth == lateral_context[\"axis\"]:\r\n                if layout.is_RegularType:\r\n                    # if it's a regular list, you want the size\r\n                    lateral_context[\"out\"] = layout.size\r\n                else:\r\n                    # if it's an irregular list, you want a null token\r\n                    lateral_context[\"out\"] = -1\r\n                return layout.nplike.empty(1)\r\n\r\n            elif layout.is_RecordType:\r\n                # if it's a record, you want to stop descent with an error\r\n                raise TypeError(f\"axis={lateral_context['axis']} is too deep, reaches record\")\r\n\r\n            elif layout.is_UnionType:\r\n                # if it's a union, you could get the result of each union branch\r\n                # separately and see if they're all the same; if not, it's an error\r\n                result = None\r\n                for content in layout.contents:\r\n                    context = {\"axis\": lateral_context[\"axis\"]}\r\n                    ak.transform(size_at_depth, content, lateral_context=context, return_array=False)\r\n                    if result is None:\r\n                        result = context[\"out\"]\r\n                    elif result != context[\"out\"]:\r\n                        raise TypeError(f\"union results in different values at axis={lateral_context['axis']}\")\r\n                lateral_context[\"out\"] = result\r\n                return layout.nplike.empty(1)\r\n\r\n        # communicate with the recursive function using a context (lateral)\r\n        context = {\"axis\": axis}\r\n\r\n        # \"transform\" but we don't care what kind of array it returns\r\n        ak.transform(size_at_depth, array, lateral_context=context, return_array=False)\r\n\r\n        # you wanted the null token to be None\r\n        return None if context[\"out\"] == -1 else context[\"out\"]\r\n```\r\n\r\nThe primary purpose of `ak.transform` is to turn one array into another array, leaving all the node types that we don't care about intact. In your case, however, we don't care about the output array, so we pass `return_array=False` and terminate recursion by returning a dummy array, `np.empty(1)` (using `layout.nplike` instead of `np` so that it works on all backends, CuPy, etc.).\r\n\r\nWe care about NumpyType arrays that are the leaves of a tree, as they might have their own rectilinear `shape` that may be part of the answer. The UnknownType is for leaves whose `dtype` is unknown because they came from an untyped source (e.g. JSON) with no examples to disambiguate the type. So their `shape` is `(0,)` by definition. We could have left out the whole `layout.is_NumpyType` section by passing `numpy_to_regular=True` in `ak.transform`, so that the leaves would only ever be 1-D and all that rectilinear shape information would be in RegularArrays, rather than NumpyArray, but that would unnecessarily force non-contiguous arrays to have to be rewritten. All you want is the dimension, after all.\r\n\r\nWe care about ListType arrays because that's where the answer will usually come from. If it's regular, you get the `size`, and if it's not, you want a null token.\r\n\r\nWe care about RecordType because you want the recursion to stop there (which it wouldn't normally do).\r\n\r\nWe care about UnionType because you either want to stop there, as in @agoose77's solution, or you want to unify the results of each branch, which is possible. This is why I made the null token temporarily `-1`, so that it could be distinguished from `None`.\r\n\r\nStepping back, it's a fairly complex function (a page), but it works for every possible Awkward Array.\r\n\r\n**Edit:** Oh, I forgot to mention that we didn't have to mention option-types at any point, because they're irrelevant for this problem. `ak.transform` lets you skip past them. (When `size_at_depth` returns `None`, the recursion keeps going.)",
     "createdAt":"2022-08-31T20:57:46Z",
     "number":3521574,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"All of the node types are [documented here](https://awkward-array.readthedocs.io/en/latest/ak.layout.Content.html) (v1, but the information is still true). v2 adds the `is_*Type` attributes so that you don't have to construct type objects at each step while descending. The breakdown is like this:\r\n\r\n| layout node | types | reason for being |\r\n|:--|:--|:--|\r\n| EmptyArray | is_NumpyType, is_UnknownType | for data from untyped sources without instances to determine type |\r\n| NumpyArray | is_NumpyType | wraps a regular NumPy array (or CuPy, etc.) that holds data with `dtype` and `shape` |\r\n| RegularArray | is_ListType, is_RegularType | for a collection of lists that all have the same length |\r\n| ListArray | is_ListType | for a collection of lists of variable length, quantified by `starts` and `stops` indexes |\r\n| ListOffsetArray | is_ListType | for a collection of lists of variable length, quantified by a single `offsets` index |\r\n| RecordArray | is_RecordType | for tuples of fixed length and different types or records, which name the fields |\r\n| IndexedArray | is_IndexedType | like a lazy array-slice, to represent a small set of categorical strings with a large number of integers, for instance |\r\n| IndexedOptionArray | is_IndexedType, is_OptionType | same as the above but encoding missing values in the index |\r\n| ByteMaskedArray | is_OptionType | encodes missing values using a byte mask |\r\n| BitMaskedArray | is_OptionType | encodes missing values using a bit mask |\r\n| UnmaskedArray | is_OptionType | the type is option-type (potentially missing values), but no mask because there aren't any missing values |\r\n| UnionArray | is_UnionType | for data with mixed types |\r\n",
        "createdAt":"2022-08-31T21:12:44Z",
        "number":3521672
       },
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"Thanks for the thorough answer as always! Two questions on this:\r\n\r\n* Why use types for the conditional instead of the layout? This ties into some broader questions I'm still trying to figure out about when instances, layouts, types, or forms are used.\r\n* Would you recommend we use `transform` in all cases like this? I was thinking this could be resolved by the (very similar) `Layout.recursively_apply` (like `to_regular`)",
        "createdAt":"2022-09-01T13:02:30Z",
        "number":3526845
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I might be missing some things, so let me know if I've confused what you've said.\r\n\r\nThe `types` that Jim is referring to are slightly different to the type objects that I use in my recursive example: https://github.com/scikit-hep/awkward/discussions/1654#discussioncomment-3521282\r\n\r\nIt's an unfortunate naming clash, but we have:\r\n- layouts, which are the composable array building blocks\r\n- types, which are the user-visible type information\r\n- `Content.is_XXXType` which are the layout flags indicating what category a content is in\r\n\r\nThe types are useful in cases where you don't care about how a particular dimension is implemented, e.g. `ListArray` vs `ListOffsetArray`. They contain very little information besides size and type information, so it's mainly useful for examples such as this one.\r\n\r\nLayouts are the canonical source of truth, and sometimes we need to know exactly which layout we're dealing with in order to implement the correct logic. Internally, layouts are tightly coupled together (though we might be able to reduce this somewhat in future). In other cases, though, we just need to know approximately what kind of layout we have, e.g. is it an option-type, or a list. That's where the `is_XXXType` flags are useful. They're a lot more concise to read when skimming through the codebase for one thing.\r\n\r\nI am not sure what Jim's take is here, but I think in this particular instance the type-visitor approach is simpler. However, for anything more advanced, it is probably sensible to use `ak._v2.transform`. This is a high-level interface over the lower-level public (although, not for multiple-array broadcasting) layout visitor interface (`Content.recursively_apply`).",
        "createdAt":"2022-09-01T13:20:34Z",
        "number":3527000
       },
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"Would it be fair to say \"types\" are like abstract types or traits, while \"layouts\" are like concrete types (e.g. structs)?\r\n\r\n> The types that Jim is referring to are slightly different to the type objects that I use in my recursive example\r\n\r\nI'm a little confused by this. Isn't there a 1-to-1 relationship between the types you use and the `is_*Type` attributes? For each `.is_FooType` is there not an equivalent `isinstance(x, FooType)`?",
        "createdAt":"2022-09-01T14:01:45Z",
        "number":3527347
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"There is an injective relationship between the `ak._v2.types.Type` objects, and the `is_XXX` flags; we don't have an `ak._v2.types.IndexedType`, for example. That's because indexed-types are invisible to the user, they just shuffle things around. That might be the only unmapped type, though.\r\n\r\nYes, I'd argue that types are more abstract than layouts. They're conveying user-visible data rather than underlying details.",
        "createdAt":"2022-09-01T14:08:02Z",
        "number":3527396
       },
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"Alright, thanks for the extra details\r\n\r\n> we don't have an ak._v2.types.IndexedType, for example\r\n\r\nIs this the reason for having all these attributes instead of going through instance checks? It seems like it has effectively been made user visible through the attributes.",
        "createdAt":"2022-09-01T14:34:50Z",
        "number":3527715
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"The `array.type` attribute generates an array type from the array _form_, which is our serialisable layout metadata that combines with buffers to build a layout. So, at the layout level, although we can generate these type objects, they're generated lazily on demand from the form. \r\n\r\nThe `is_XXXType` flags are really just a quality of life improvement for reading (and writing) source code, as well as a slight performance boost (I suspect). \r\n\r\nMy advice in terms of thinking about these things is to treat the `array.type` attribute as a convient way to interact with the abstract user-facing structure of the array. For anything more detailed / advanced, go straight to the layouts (which are a richer source of information).",
        "createdAt":"2022-09-01T14:54:28Z",
        "number":3527913
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"What I didn't recognize when I started writing a solution using `ak.transform` is that this particular problem can be solved by looking at types alone, as @agoose77 did. And that solution is much simpler; I hadn't thought of it. The only possible down-side of having two ways of doing something, one of which can only be applied in a subset of cases, is if you later want to change the function beyond that subset, you wouldn't be able to do so by a small extension, you'd have to change your whole technique.\r\n\r\nSuppose, for instance, that instead of returning `None` for variable-length lists, you later want to return the minimum length or the maximum length. That information isn't in the type; you'd have to switch from recursing over types to recursing over arrays, and the way of doing that is entirely different, so it would mean starting over.\r\n\r\nIf you're certain that you'll only need type information, I think @agoose77 is right: the type-only recursion is much simpler. That's what I'd go for, if I'd thought of it first.\r\n\r\n### Why not plain recursion?\r\n\r\nYou _could_ recurse over the array layout nodes (subclasses of `ak._v2.contents.Content`) in the same way that you recurse over types (subclasses of `ak._v2.types.Type`). If that recursive function is going to be applied to arbitrary Awkward Arrays, it would just need to know the full set of `Content` subclasses, just as it now needs to know the full set of `Type` subclasses (neither of which is expected to change in the future). This function doesn't care about option-types: it wants to just recurse through them if it finds any in the tree, so it needs to catch the\r\n\r\n```python\r\nisinstance(type_node, OptionType)\r\n```\r\n\r\nso that it can continue down `type_node.content`, or similarly\r\n\r\n```python\r\nisinstance(layout_node, (BitMaskedArray, ByteMaskedArray, IndexedOptionArray, UnmaskedArray))\r\n```\r\n\r\nso that it can continue down `layout_node.content`. You would need to know the full set of types to check for and whether it has zero children (`EmptyArray` and `NumpyArray`), multiple children (`RecordArray` and `UnionArray`), or one child (everything else). Fortunately, the names of child/children attributes are systematic: `content` or `contents` for all layout nodes, type nodes, and Form nodes (explanation below).\r\n\r\nSo, you _could_ do this, and we started with this in the early days of writing functions on Awkward Arrays. In fact, the earliest functions were implemented as a method on all `Content` subclasses, following proper OOP, but this spread the definition of a single function across many files and we will not be adding new `Content` subclasses. So then we were writing these functions that did `isinstance` for every `Content` subclass, but most subclasses were just a pass-through and there was a lot of redundant boilerplate. Furthermore, functions of more than one array had to consider cases in which the array types didn't match at a given level, e.g. one is a `ListArray` and the other is a `ListOffsetArray`, and this mixes broadcasting with the calculation of the operation we're actually interested in, so we clearly had to separate the process of recursing over the tree (with possible broadcasting) from the calculation of the operation we're actually interested in.\r\n\r\nThis visitor pattern is what we came up with; actually, it's a rewrite of the original visitor pattern with a cleaned-up interface. And then we made it public through `ak.transform` so that other libraries can get the same benefits. So while you could write a direct recursive function over the layout nodes, you'd find that `ak.transform` would reduce boilerplate. The example above doesn't mention option-types, for instance.\r\n\r\n### Why the `is_*Type` attributes?\r\n\r\nThe next part is the `is_*Type` attributes. Yes, they're just to reduce typing and screen clutter:\r\n\r\n```python\r\nisinstance(layout_node, (BitMaskedArray, ByteMaskedArray, IndexedOptionArray, UnmaskedArray))\r\n```\r\n\r\nbecomes\r\n\r\n```python\r\nlayout_node.is_OptionType\r\n```\r\n\r\nMaybe it's marginally faster to execute, but that wasn't the motivation.\r\n\r\n### Relationship between `Content` types, `Type` types, `Form` types (for completeness) and `is_*Type` attributes\r\n\r\nInside of an ak.Array's `layout`, a tree of `Content` subclasses constitute the array itself. It would be possible to answer any question by examining this tree.\r\n\r\nBut several different `Content` subclasses correspond to the same high-level `Type`, and these are presented to the user as though they were the same. For instance, `ListArray` and `ListOffsetArray` are two ways of representing variable-length lists, the first is more versatile, the second is amenable to more optimizations, but a high-level user only cares that they have an array of lists. So when you invoke `ak.Array.type`, it creates a `Type` tree from the `Content` tree by simplifying/projecting away information. This `Type` is usually viewed as a [Datashape](https://datashape.readthedocs.io/en/latest/) string, which includes the length of the array, so the top-most element in this hierarchy is an `ArrayType` containing the length. (And `issubclass(ArrayType, Type)` is False: it's outside the hierarchy, similar to the way that `ak.Array` is not a `Content` subclass.)\r\n\r\nFor completeness, I should mention that we sometimes need low-level types, which are called `Form`s for a word like \"type\" that isn't \"type\" (or \"sort\", and \"kind\" has a [different meaning in type theory](https://en.wikipedia.org/wiki/Kind_(type_theory))). These are one-to-one with `Content` subclasses; they only lack the array data itself. It's used, for instance, as a template for constructing arrays in [ak.from_buffers](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_buffers.html)).\r\n\r\nThe `is_*Type` attributes were named \"Type\" because they mostly correspond to high-level `Type`s. But their exact definitions came from what combinations of `Content` subclasses we frequently found ourselves checking in `isinstance`, which has some overlaps, making it not a strict hierarchy, the way that actual `Type`s are. For instance,\r\n\r\n| layout node | is-attributes | Form | Type |\r\n|:--|:--|:--|:--|\r\n| ListArray | is_ListType | ListForm | ListType |\r\n| ListOffsetArray | is_ListType | ListOffsetForm | ListType |\r\n| RegularArray | is_ListType, is_RegularType | RegularForm | RegularType |\r\n\r\nbecause a lot of the calculations we wanted to apply were the same for all three list-like types, but some applied differently to regular lists.\r\n\r\nAlso, some `Content` elements correspond to no type: an `IndexedArray` containing a `Content` of type `T` has type `T`. When projecting the information out to make a `Type` tree, these nodes disappear entirely. However, there's an `is_IndexedType` attribute to detect it when recursing over layout nodes (as @agoose77 pointed out).\r\n\r\nSo maybe the \"Type\" part of the `is_*Type` attribute names should be changed. We still have time to do so: the `is_*Type` attributes are a v2 addition and v2 hasn't been finally/formally released yet. But if not \"Type,\" then what? Since they overlap, they look to me like [type classes in Haskell](https://en.wikipedia.org/wiki/Type_class), but \"Class\" would be a pretty bad name, given that this is in Python. Maybe `is_*TypeClass`?\r\n\r\n### Why are there so many ways of describing types in Awkward Array?\r\n\r\nI haven't even mentioned the hierarchies of type objects that enable iteration in Numba or JIT-compilation in C++ Cling. Considering these 2 and the above 3, that's 5 different systems for describing the type of an Awkward Array. That's why we're not going to be adding any new node types\u2014there's so much we'd have to change. It's a closed system, which is why it was important for it to be a complete system to start with (i.e. not just ragged arrays of numbers).",
        "createdAt":"2022-09-01T18:33:23Z",
        "number":3529654
       },
       {
        "author":{
         "login":"grst"
        },
        "body":"I'm a bit confused by the string representation, which results in two nested types: \r\n```python\r\n>>> ak.Array([\"foo\", \"foo\", \"bar\"]).type\r\nArrayType(ListType(NumpyType('uint8', parameters={'__array__': 'char'}, typestr='char'), parameters={'__array__': 'string'}, typestr='string'), 3)\r\n```\r\ncompared to\r\n```python\r\n>>> ak.Array([1, 2, 3]).type\r\nArrayType(NumpyType('int64'), 3)\r\n```\r\n\r\nThis is also an edge case for the `dim_len` implementation above:\r\n```python \r\n# expected: raise TypeError\r\n# actual: \r\n>>> dim_len(ak.Array([\"foo\", \"foo\", \"bar\"]), 1) \r\nNone\r\n```\r\n\r\nWhy can't the string array be a single `NumpyType` like its `int` counterpart? And where would you suggest it's best to handle this edge-case in the dim-len function? ",
        "createdAt":"2022-09-26T18:35:27Z",
        "number":3736747
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Regardless of how it's represented, an array of strings has to be implemented like an array of lists of uint8; the variable-length lists is the part that requires special care. (It can't be a NumpyType because NumPy doesn't even attempt to deal with variable-length strings.) In an earlier version of Awkward Array (version 0.x), strings were implemented as a special node type, separate from JaggedArray, leading to a lot of duplication of code.\r\n\r\nOur point of view on strings is that they're physically like lists of uint8, in how they're laid out in memory (and disk) and how they're processed in slicing and other basic operations, but they have special high-level behaviors: their `repr` shows them as letters in quotation marks, `==` compares whole strings as atomic units, not their character contents the way a list would, and so on. It's as though our strings are a subclass of lists, a specialization with overridden methods. Overriding records in Awkward Array is very common in HEP, so we're using the same `ak.behavior` mechanism to do both. (At least ideally; strings have some hard-coded specializations because they're _so_ special\u2014we're figuring out what to do about that.)\r\n\r\nTo handle it in `dim_len`, you can take a `Content`, `Form`, or `Type` subclass and ask\r\n\r\n```python\r\nnode.parameter(\"__array__\") in (\"string\", \"bytestring\")\r\n```\r\n\r\nif you want to exclude UTF-8 strings and unencoded bytestrings in particular, or\r\n\r\n```python\r\nnode.parameter(\"__array__\") is not None\r\n```\r\n\r\nif you want to exclude any overridden array types at all. If a parameter is not defined on a node, this function will return `None` (like a dict's `get`), so undefined and defined-to-be-None can be treated the same way.",
        "createdAt":"2022-09-26T18:54:07Z",
        "number":3736895
       }
      ],
      "totalCount":10
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-08-31T19:02:44Z",
  "number":1654,
  "title":"Getting size of a given dimension",
  "url":"https://github.com/scikit-hep/awkward/discussions/1654"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"There are two ways to overload an array that contains a named record:\r\n\r\n```python\r\nak._v2.behavior[\"*\", \"NameOfRecord\"] = ClassForArray\r\nak._v2.behavior[\".\", \"NameOfRecord\"] = ClassForArray\r\n```\r\n\r\nThe first of these overrides any kind of list that is surrounding the records, so that you can define vectorized operations.\r\n\r\nThe second only overrides one dimension above the records.\r\n\r\nThe second case was broken, never tested, and no one has ever asked me about it. In [#1651](https://github.com/scikit-hep/awkward/pull/1651), I dropped it for v2 with the idea that we can reintroduce it correctly if asked.\r\n\r\nWhat do you think? Does anybody need the `\".\"` method?\r\n\r\n@nsmith-, is this used in NanoEvents?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"nsmith-"
     },
     "body":"NanoEvents always uses `awkward.mixin_class` to define classes, and that uses only the `\"*\"` method: https://github.com/scikit-hep/awkward/blob/15318e1d40f3578c55195e6928e8c2b37d948071/src/awkward/behaviors/mixins.py#L46",
     "createdAt":"2022-09-01T16:28:14Z",
     "number":3528715,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-09-01T16:03:33Z",
  "number":1660,
  "title":"Ability to override behavior at exactly one dimension above a record",
  "url":"https://github.com/scikit-hep/awkward/discussions/1660"
 },
 {
  "author":{
   "login":"SimonHeybrock"
  },
  "body":"Hi!\r\nI am working on [Scipp](https://scipp.github.io), which supports some types of ragged data (see [Scipp Documentation on Binned Data](https://scipp.github.io/user-guide/binned-data.html) for details).\r\n\r\nGiven the interest in pydata/xarray#4285 for using Awkward arrays with Xarray, which is conceptually very close to what `scipp.DataArray` can do, I thought it is high time to kick off a discussion between us. I think there are a lot of similarities but also differences and it would be valuable to understand those, which might at some point even lead to support of a common API or functionality. Please also have a look in https://discuss.scientific-python.org/t/ragged-array-summit/465.\r\n\r\nFor starters here are high level similarities and differences I am aware of (note that I have used Awkward for little more than 10 minutes):\r\n\r\n- Scipp's binned data is conceptually very similar to `ak.layout.ListArray64`. We store an ND-array of start and stop indices, and a \"buffer\" (I think Awkward calls this \"content\")? I managed to convert scipp binned data into an Awkward list array without copying the buffer/content.\r\n- Scipp does *not* support nesting or similar more complex cases.\r\n- Scipp supports (1-D) `scipp.DataArray` and `scipp.Dataset` (similar to `xarray.DataArray/Dataset`) as the content. This seems similar to using records in Awkward\"s content, with slightly more structure. In particular, we distinguish data fields and coordinate fields (and only data fields are operated on in, e.g., arithmetic operations). This does not really matter in terms of data layout and memory handling, i.e., one could imagine swapping out the internals of how things are stored.\r\n- Scipp provides operations for (re)grouping and (re)binning ragged data in multiple dimensions.\r\n- Scipp is multi-threaded out-of-the-box (based on TBB). I could not find info on whether Awkward does this (or whether it relies on and can make use of multi-threading in the underlying (numpy) array library?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"> In particular, we distinguish data fields and coordinate fields (and only data fields are operated on in, e.g., arithmetic operations). \r\n\r\nHow does this behave in practice? Do you throw errors when attempting arithmetic operations on coordinate fields? \r\n\r\nAwkward does not have this distinction at the type level, instead we allow users to define how records should be added; we recently changed our model so that records by default do not add together, but library authors can opt-in by writing an implementation. You can look at the [vector library](https://vector.readthedocs.io/en/latest/) as an example of this, although it's a bit indirect as a consequence of supporting multiple \"backends\" besides Awkward. The [design prototype](https://vector.readthedocs.io/en/latest/usage/vector_design_prototype.html) perhaps shows how vector uses Awkward Array in a trivial sense.\r\n\r\nI wonder @jpivarski @ioanaif if there are merits for inverting this logic, so that the default case works, and behavior authors can opt-out? I believe one would do this by setting an `np.ufunc` rule for their behaviors. I don't have a strong opinion yet.\r\n\r\n> Scipp is multi-threaded out-of-the-box (based on TBB). I could not find info on whether Awkward does this (or whether it relies on and can make use of multi-threading in the underlying (numpy) array library?\r\n\r\nAwkward's CPU-kernels are currently single-threaded. We don't use NumPy for many of our \"kernels\" (the compiled loops that underpin the high-level operations and reductions). We are working towards CUDA support, which would give us local parallelism, although we do not yet have all the requried kernels implemented there ([see our roadmap](https://github.com/scikit-hep/awkward/wiki#gpu-backend)). I'm not totally sure on what kind of work we'll be doing to this in the near future (as in, I genuinely don't know). @jpivarski will have an answer.\r\n\r\nSeparately, we also have `dask-awkward` being developed, which will give us distributed computation (and, through multiple-processes, core-scaling). Clearly this is not the same as local parallelism, but it does somewhat address the problem.",
     "createdAt":"2022-09-02T11:40:49Z",
     "number":3535754,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"SimonHeybrock"
        },
        "body":"> > In particular, we distinguish data fields and coordinate fields (and only data fields are operated on in, e.g., arithmetic operations).\r\n> \r\n> How does this behave in practice? Do you throw errors when attempting arithmetic operations on coordinate fields?\r\n\r\nNo, because the field itself does not know that it is a coordinate. This is simply done in the bigger context. Consider this example:\r\n\r\n```python\r\n# this example is written for dense data arrays, but the same happens conceptually\r\n# when working with ragged data\r\nda1 += da2  # computes da1.values += da2.values *after* ensuring, e.g., that `da1.coords['x'].values`\r\n            # is identical to `da2.coords['x'].values`\r\nda1.coords['x'] += da2.coords['x']  # works, just operate on coord values\r\n````\r\n\r\n> Awkward does not have this distinction at the type level, instead we allow users to define how records should be added;\r\n\r\nCan you point me to where/how this can be customized?\r\n\r\n> > Scipp is multi-threaded out-of-the-box (based on TBB). I could not find info on whether Awkward does this (or whether it relies on and can make use of multi-threading in the underlying (numpy) array library?\r\n> \r\n> Awkward's CPU-kernels are currently single-threaded. We don't use NumPy for many of our \"kernels\" (the compiled loops that underpin the high-level operations and reductions). We are working towards CUDA support, which would give us local parallelism, although we do not yet have all the requried kernels implemented there ([see our roadmap](https://github.com/scikit-hep/awkward/wiki#gpu-backend)). I'm not totally sure on what kind of work we'll be doing to this in the near future (as in, I genuinely don't know). @jpivarski will have an answer.\r\n> \r\n> Separately, we also have `dask-awkward` being developed, which will give us distributed computation (and, through multiple-processes, core-scaling). Clearly this is not the same as local parallelism, but it does somewhat address the problem.\r\n\r\n\ud83d\udc4d ",
        "createdAt":"2022-09-02T11:53:57Z",
        "number":3535857
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Right, so this is a structure-level decision, when operating on the dataset (in your terminology) rather than the atoms (fields). \r\n\r\nTo overload how records are added together, we use our `ak.behavior` system, which maintains a table mapping NumPy ufuncs to overloads, and additionally contains special metadata (like `__array__`, which maps to a user-defined class for custom views over arrays):\r\n\r\nhttps://awkward-array.readthedocs.io/en/latest/ak.behavior.html#overriding-numpy-ufuncs-and-binary-operators\r\n\r\nYou can see an actual implementation of this [in the vector source code](https://github.com/scikit-hep/vector/blob/4e756fe2cd8a3430260e1cf4cf008874cfc1bb3c/src/vector/backends/awkward.py#L1510)\r\n",
        "createdAt":"2022-09-02T12:12:32Z",
        "number":3535981
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Hi! This is very interesting and I followed up on your call for a ragged array summit.\r\n\r\nIt looks to me like Awkward Array and Scipp are very different things:\r\n\r\n  * My take is that Scipp is in a category of libraries with xarray, AnnData, and Pandas, which aim to streamline data analysis by making common tasks easy: units/metadata, propagation of uncertainties, all of those histogramming/sparse data options, parallel processing, etc.\r\n  * Awkward Array aims to be in a category with NumPy: foundational, sometimes used directly by data analysts, sometimes as a backend by other user-facing libraries. (Although I originally intended Awkward to be user-facing, its first applications in high energy physics were usually mediated by the [Coffea](https://coffeateam.github.io/coffea/) library and current discussions with xarray and AnnData are making it sound like it's more useful for us to provide this low-level support. But I did intend from the beginning for Awkward Array to be \"a thing like NumPy.\")\r\n\r\nSome examples of this are the behavior-overriding mechanism that @agoose77 pointed to, and we're focusing on single-threaded computation so that [dask-awkward](https://github.com/ContinuumIO/dask-awkward/) can parallelize it in the horizontal direction (across many threads/processes/computers in a cluster). For vertical acceleration, @agoose77 pointed to the future GPU work: Awkward Arrays are built on top of rectilinear arrays from other libraries and we can swap NumPy for CuPy or JAX to do GPU computations. In the future, all of our structure-changing kernels (compiled as single-threaded C routines) will be ported to GPU equivalents so that the same operations can be performed on CPU and GPU.\r\n\r\nI should point out that the kernels we write ourselves are only the ones involved in structure-changing operations, such as broadcasting an array of lists with an array of lists of lists of lists. For any numeric elementwise calculations, we let NumPy (or CuPy, JAX, ...) calculate it. Any parallel processing or vectorization that the underlying \"flat math\" library can do, we get for free.\r\n\r\nSome of these computation functions can be built using JIT-compilation. Awkward has a Numba interface, so that users can write imperative code to iterate over an array of data structures. Some problems are easier to express imperatively than in an array-oriented way, and these implementations are often faster because they make only one pass over the data. We also have a new interface to C++ JIT-compilation using [ROOT](https://root.cern/)'s RDataFrame, though this is most likely of interest to high energy physicists.\r\n\r\nSo, we're likely working in different problem spaces, but if you're interested in interoperability, then there might be some interesting projects to collaborate on. At minimum, maybe users would want an easy (and zero-copy) way to convert data between the two libraries. Scipp has a lot of metadata, just as xarray does: if these are both targets for us, it would tell us what we would need to do to preserve that metadata through conversions, generalizing #1391.",
        "createdAt":"2022-09-02T17:22:32Z",
        "number":3538854
       },
       {
        "author":{
         "login":"SimonHeybrock"
        },
        "body":"> It looks to me like Awkward Array and Scipp are very different things:\r\n> \r\n>  * My take is that Scipp is in a category of libraries with xarray, AnnData, and Pandas, which aim to streamline data analysis by making common tasks easy: units/metadata, propagation of uncertainties, all of those histogramming/sparse data options, parallel processing, etc.\r\n> \r\n>  * Awkward Array aims to be in a category with NumPy: foundational, sometimes used directly by data analysts, sometimes as a backend by other user-facing libraries. (Although I originally intended Awkward to be user-facing, its first applications in high energy physics were usually mediated by the [Coffea](https://coffeateam.github.io/coffea/) library and current discussions with xarray and AnnData are making it sound like it's more useful for us to provide this low-level support. But I did intend from the beginning for Awkward Array to be \"a thing like NumPy.\")\r\n>\r\n> [...]\r\n> So, we're likely working in different problem spaces, but if you're interested in interoperability, then there might be some interesting projects to collaborate on.\r\n\r\nI think you summarized this quite well. But Scipp actually also had to cover parts of the second item: Since there was no \"thing like NumPy\" [matching our requirements] that could provide ragged arrays we developed this on our own. So this is a current point of duplication (which is not to imply that this is necessarily wrong).\r\n\r\nBeyond that, there is the question whether we have functionality in Scipp that would be valuable, e.g., to the community Awkward is serving. Aside from the xarray/DataArray discussion, there are binning/grouping algorithms that Scipp provides. These allow for mapping *table -> array of tables* or *array of tables -> array of tables* (where I mean table as a list of records). Our current implementation is restricted to operating on Scipp data types, but conceptually there is nothing major that prevents a rewrite which could operate on, e.g., Python dicts of Numpy-like buffers.",
        "createdAt":"2022-09-05T04:23:48Z",
        "number":3554448
       },
       {
        "author":{
         "login":"SimonHeybrock"
        },
        "body":"> Right, so this is a structure-level decision, when operating on the dataset (in your terminology) rather than the atoms (fields).\r\n\r\nYes. Having looked at the `ak.behavior` system, it looks like Scipp is essentially encoding a behavior for one particular content type (scipp.DataArray).\r\n\r\n> To overload how records are added together, we use our `ak.behavior` system, which maintains a table mapping NumPy ufuncs to overloads, and additionally contains special metadata (like `__array__`, which maps to a user-defined class for custom views over arrays):\r\n> \r\n> https://awkward-array.readthedocs.io/en/latest/ak.behavior.html#overriding-numpy-ufuncs-and-binary-operators\r\n\r\nI think it may be quite simple to replicate Scipp's behavior with this system, nice! One question though: I tried creating an Awkward array from \"slicable\" objects other than `numpy.ndarray`, for example:\r\n\r\n```python\r\nak.unflatten(scipp_dataarray, counts)  # has __getitem__ supporting int and slice\r\n```\r\nbut this raises an exception. What are the requirements for being able to do this? Basically, I am trying to see whether I can get this to work with a combination of Scipp and `ak.ListArray`, without having to decompose the Scipp object into a plain Python dict for use with `ak.RecordArray`.\r\n\r\n",
        "createdAt":"2022-09-05T04:58:29Z",
        "number":3554790
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Small disclaimer: we're about (in the next few months+) to move to the next major version of Awkward Array, which moves a lot of logic into the Python layer. Although this shouldn't make too many user-facing changes, there are places where things aren't identical. For now, I'll discuss the v2 API, which lives under the `awkward._v2` submodule. If you use the latest pre-releases (1.10.0rc1 as of writing, but we'll soon move to the 2.0 series), then you can try it out. I'll write out the full-name of API functions to make it easier to find them.\r\n\r\nMost of the (v2) high-level (`ak._v2`) API uses `ak._v2.operations.ak_to_layout.to_layout` under the hood to convert arbitrary Python objects into the known `ak._v2.contents.Content` objects. If you don't pass anything that Awkward recognises as an existing array object, e.g. an Awkward `Content`, or a NumPy array, then it falls back upon `ak._v2.operations.ak_from_iter.from_iter` to actually build the Array from an iterable object. So, if the `scipp_dataarray` isn't one of these subclasses, then we'll be falling back onto this pathway. It sounds like these objects aren't iterable, so we're failing to find any means of converting them. Note that, even if `scipp_dataarray` _were_ iterable, we probably wouldn't want `from_iter` to be the mechanism that Awkward uses to convert it into an Awkward Array; `from_iter` has to loop over (in C++) all of the elements of the array, and allocate intermediary buffers along the way.\r\n\r\nThis makes me think that we should expand the ability for Awkward to convert from other array formats. As well as handling CuPy and jax, we could add another pathway to try and invoke `__array__` (probably by asking-for-permission for `__array__`). This would support third-party libraries that implement this interface. We also discussed adding our own mechanism inspired by NEP18 to allow third-party libraries to share the Awkward API, and this would also extend to allow third-party array types to support Awkward by defining an `__awkward_array__` method #1126 \r\n\r\nAnyway, back to your problem. If you want to build an Awkward Array, then the lowest-level array Awkward wants is flat-columnar arrays, i.e. NumPy arrays, and a schema (form) that tells Awkward how these buffers are used to build the Array layout (see `ak._v2.operations.ak_from_buffers.from_buffers`). You can also build your array manually using the `ak._v2.contents.Content` layout classes, which are the structured nodes that define an Awkward Array. I can elaborate on this process if you need, but I thought I'd keep this reply shorter for now.\r\n\r\nFundamentally, Awkward Array *needs* to understand the type of the data that it operates upon. We can't, for example, operate on arbitrary Python values - we ultimately dispatch to NumPy (or the equivalent array library e.g. CuPy) to compute reductions, etc. At the high level, we restrict the leaf-values of an Array to be \r\n- numeric types (list, int, complex, ...)\r\n- datetime types\r\n- string/bytes types\r\nand, technically, tuples / records.",
        "createdAt":"2022-09-05T08:20:15Z",
        "number":3556544
       },
       {
        "author":{
         "login":"SimonHeybrock"
        },
        "body":"> Fundamentally, Awkward Array _needs_ to understand the type of the data that it operates upon. We can't, for example, operate on arbitrary Python values - we ultimately dispatch to NumPy (or the equivalent array library e.g. CuPy) to compute reductions, etc. At the high level, we restrict the leaf-values of an Array to be\r\n> \r\n>     * numeric types (list, int, complex, ...)\r\n> \r\n>     * datetime types\r\n> \r\n>     * string/bytes types\r\n>       and, technically, tuples / records.\r\n\r\nI suppose this is my question: Awkward seems to be able to deal with arbitrary arrays records and provides `ak.behavior` for  customizing operations. So what prevents it from being able to deal with other array-like objects (if the user provides custom behaviors)? For example, one could imagine directly using `pandas.DataFrame` as \"content\" (without representing it as a dict of NumPy arrays). Right now Awkward seems to only accept subclasses of `Content`, so `pandas.DataFrame` cannot by used directly, but why is that? Is there a fundamental reason?\r\n",
        "createdAt":"2022-09-05T08:42:08Z",
        "number":3556764
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I won't speak for @jpivarski, who might have a different take on things (and ultimately steers where this project is going), so I'll just put my short, incomplete take here.\r\n\r\nAt a design-level, Awkward Array is built around the concept of columnar arrays. This is both for performance and for ergonomics, as Awkward was originally intended for use in HEP, where (among a great deal of other things) it is important to be able to read subsets of large datasets by column. So, were we to support non-Numpy leaves, we'd want them to be structureless. The most appropriate generalisation on Awkward's existing behaviour would be to support non-Numpy flat array-like objects (with no \"fields\") at the leaves. I don't think this would be impossible (but I'll wait for @jpivarski to inevitably correct me ;)). However, outside of the `ufunc` mechanism (for which we delegate to NumPy), most structure / reduction operators involve calling a compiled kernel. Many kernels don't care about the underlying content, but the reduction kernels do. Specifically, the reduction kernels generalise on the traditional reducers from NumPy by reducing across sublists, e.g. whilst NumPy can only handle\r\n```python\r\n>>> np.min([1, 2, 3, 4])\r\n1\r\n```\r\nAwkward supports variable-length sublists, roughly:\r\n```python\r\n>>> min([\r\n    [1, 2, 3, 4],\r\n    [5, 6, 7]\r\n])\r\n[1, 5]\r\n```\r\nReductions along any axis ultimately invoke the compiled reduction kernels with a 2D array of variable-length sublists. So, were we to generalise the NumpyArray content to support this, there would need to be some kind of mechanism to implement and define these kernels. If these implementations were written in Python, Awkward would be a significant factor slower. \r\n\r\nAt a technical level, we assume that the internal Awkward content nodes form a closed set, so we don't have any support for subclassing or modifying these content classes.\r\n\r\nWe are exploring efforts to go the other way - i.e. Awkward Arrays as Pandas extension types: https://github.com/scikit-hep/awkward/wiki#pandas-integration\r\n\r\n",
        "createdAt":"2022-09-05T10:35:44Z",
        "number":3558233
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I think it would be reasonable for `to_layout` to recognize objects that have an `__array__` method and call that to get a NumPy array, which it can then wrap in the Content subclass named `NumpyArray`. This should be near the end of the list of types `to_layout` checks, so that (for instance) CuPy arrays don't get copied over to the CPU to turn them into NumPy arrays, which is what happens when you call their `__array__` method. We should think about the consequences of this, because there are a lot of array-like types out there for which calling `__array__` is destructive, such as Pint arrays with units: would users rather get an error when the units are stripped away, at least to know that it's happening?\r\n\r\nThen, for instance, a `pandas.Series` would get turned into an `np.ndarray` that can be a leaf-node in an Awkward Array's Content tree. But again, that's information-losing: it removes the Series's index. Maybe we should think about that more before just doing it.\r\n\r\n> Right now Awkward seems to only accept subclasses of `Content`, so `pandas.DataFrame` cannot by used directly, but why is that? Is there a fundamental reason?\r\n\r\nSo yes, that's fundamental. Data in an Awkward Array are not managed by Python, and it's so that we can do operations _like_ the reducer @agoose77 described. Also some advanced slices and restructuring operations.\r\n\r\nThere is a little flexibility in what type of object holds the numerical data, wrapped by the `NumpyArray` class. Right now, that object can be a NumPy `ndarray`, a CuPy `ndarray`, or a JAX DeviceArray (for autodiff). Whenever we add a new backend, we also define a new \"`nplike`\" that is a shim to make all of these array libraries duck-typable\u2014out of the box, they're maybe 90% compatible. Then we also have to make sure that our own functions that go beyond the rectilinear array libraries, such as @agoose77's reducer example, can be executed on the backends' data. (Hence, we need to duplicate our suite of CPU functions to also work on GPU pointers, which is [not done yet](https://github.com/scikit-hep/awkward/wiki#gpu-backend).) So the set of backends is slowly and laboriously extended; it's not something in which arbitrary objects can be dropped in. If they were arbitrary objects, there would be a lot of slicing operations, reducers, etc. that we wouldn't be able to implement (without doing them in slow Python for loops).",
        "createdAt":"2022-09-05T16:29:53Z",
        "number":3561899
       },
       {
        "author":{
         "login":"SimonHeybrock"
        },
        "body":"> We should think about the consequences of this, because there are a lot of array-like types out there for which calling `__array__` is destructive, such as Pint arrays with units: would users rather get an error when the units are stripped away, at least to know that it's happening?\r\n\r\n> There is a little flexibility in what type of object holds the numerical data, wrapped by the `NumpyArray` class. Right now, that object can be a NumPy `ndarray`, a CuPy `ndarray`, or a JAX DeviceArray (for autodiff). Whenever we add a new backend, we also define a new \"`nplike`\" that is a shim to make all of these array libraries duck-typable\u2014out of the box, they're maybe 90% compatible.\r\n\r\nThis sounds like you are almost there? If you can (1) work on arbitrary NumPy-like buffers, (2) have a method to request a list/dict of columns from the content (similar to `__array__` you mention, but supporting record-like objects such as `pandas.DataFrame` or `scipp.DataArray`, i.e., returning multiple columns/buffers), and (3) have a way of creating new content (a callable provided by the content?)?\r\n\r\nProbably a constructor callable is not sufficient for (3), since in binary operations will also need to operate on the non-column content such as the unit in you `pint` example. In Scipp we have a similar situation: All kernels are applied to array elements as well as the array unit.\r\n",
        "createdAt":"2022-09-06T05:17:59Z",
        "number":3565876
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"We can operate on arbitrary NumPy-like buffers, so it's not hard to write (for instance) an `ak.from_dataframe` function to zero-copy wrap a Pandas DataFrame. (It would put each `Series.values` in a `NumpyArray`, all the columns into a `RecordArray`, and wrap that up as an `ak.Array`, maybe carry some metadata as `parameters`.) But in such a case, we are controlling what we want to wrap and how we wrap it; I'm wavering on the idea of opening up the floodgates and ingesting anything with an `__array__` method.\r\n\r\nSimilarly for `scipp.DataArray`. We have a lot of functions named like `ak.from_*` and `ak.to_*`. That would put the conversion in Awkward, though by symmetry, it could also go in Scipp. (I don't have a strong opinion about that. We consider the set of from/to functions to be an open-ended set and can be third-party library-specific.) Zero-copy and round-trip fidelity are goals, but for technical reasons, it might be necessary to give up on one or the other.",
        "createdAt":"2022-09-06T15:57:36Z",
        "number":3573055
       }
      ],
      "totalCount":11
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-09-02T11:19:49Z",
  "number":1663,
  "title":"Ragged data in Awkward vs. Scipp",
  "url":"https://github.com/scikit-hep/awkward/discussions/1663"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"These are differences from 1.8.0 to 1.9.0. Most only affect Awkward Array version 2, in the `awkward._v2` submodule of 1.9.0. The exceptions are called out with \"v1\" in the subject line.\r\n\r\n## Features\r\n\r\n* feat: C++ refactoring: ak.to_pandas by @ioanaif in https://github.com/scikit-hep/awkward/pull/1369\r\n* feat: C++ refactoring: ak.nan_to_num by @ioanaif in https://github.com/scikit-hep/awkward/pull/1352\r\n* feat: C++ refactoring: ak.run_lengths by @ioanaif in https://github.com/scikit-hep/awkward/pull/1347\r\n* feat: add `is_tuple` describe operation by @agoose77 in https://github.com/scikit-hep/awkward/pull/1351\r\n* feat: C++ refactoring: ak.unzip by @ioanaif in https://github.com/scikit-hep/awkward/pull/1354\r\n* feat: pure Cling demo and improvements to C++ JIT infrastructure by @jpivarski in https://github.com/scikit-hep/awkward/pull/1359\r\n* feat: C++ refactoring: ak.broadcast_arrays by @ioanaif in https://github.com/scikit-hep/awkward/pull/1368\r\n* feat: irst version of ak._v2.from_parquet by @jpivarski in https://github.com/scikit-hep/awkward/pull/1338\r\n* feat: C++ refactoring: ak.copy by @ioanaif in https://github.com/scikit-hep/awkward/pull/1367\r\n* feat: C++ refactoring: ak.unflatten by @ioanaif in https://github.com/scikit-hep/awkward/pull/1360\r\n* feat: add `depth_limit` to `ak.broadcast_arrays` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1373\r\n* feat: Allow NumPy arrays in CppStatements; fix row_groups in single-file from_parquet. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1376\r\n* feat: This PR sets up the architecture to call CuPy Raw Kernels from Awkward. by @swishdiff in https://github.com/scikit-hep/awkward/pull/1355\r\n* feat: Reducers with axis=None and typetracers. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1380\r\n* feat: This PR adds the generated kernels and simplifies the template specialization generation process. by @swishdiff in https://github.com/scikit-hep/awkward/pull/1381\r\n* feat: High-level ak._v2.Array clean-ups. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1392\r\n* feat: This PR adds all the remaining kernels in the studies directory by @swishdiff in https://github.com/scikit-hep/awkward/pull/1390\r\n* feat: This PR adds JAX as a new nplike by @swishdiff in https://github.com/scikit-hep/awkward/pull/1399\r\n* feat: Passing behaviour in ak._v2 functions by @ioanaif in https://github.com/scikit-hep/awkward/pull/1415\r\n* feat: Enable broadcasting of string equality. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1427\r\n* feat: Enabled string/categorical behavior by @ioanaif in https://github.com/scikit-hep/awkward/pull/1421\r\n* feat: Implements ak.nan_to_none and all of the ak.nan* functions to override NumPy's. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1428\r\n* feat: Pretty-printing types by @jpivarski in https://github.com/scikit-hep/awkward/pull/1430\r\n* feat: Register both v1 and v2 Arrays in Numba entry_points. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1432\r\n* feat: Add Array and Record.__delitem__. And fix show(type=True). by @jpivarski in https://github.com/scikit-hep/awkward/pull/1433\r\n* feat: Enable categorical behavior - testing by @ioanaif in https://github.com/scikit-hep/awkward/pull/1434\r\n* feat: Enable mixins behavior by @ioanaif in https://github.com/scikit-hep/awkward/pull/1437\r\n* feat: Implementing ak._v2.to_parquet. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1440\r\n* feat: awkward to rdataframe by @ianna in https://github.com/scikit-hep/awkward/pull/1374\r\n* feat: Enable ak.firsts  by @ioanaif in https://github.com/scikit-hep/awkward/pull/1443\r\n* feat: Enable ak.singletons by @ioanaif in https://github.com/scikit-hep/awkward/pull/1444\r\n* feat: Revamping the to_json/from_json interface. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1449\r\n* feat: This PR attempts to add autodifferentiation support for Awkward Arrays using JAX pytrees. by @swishdiff in https://github.com/scikit-hep/awkward/pull/1447\r\n* feat: Adding a Forth Based Avro Reader by @aryan26roy in https://github.com/scikit-hep/awkward/pull/1491\r\n* feat: Adding repr overriden behavior by @ioanaif in https://github.com/scikit-hep/awkward/pull/1487\r\n* feat: rdataframe to awkward by @ianna in https://github.com/scikit-hep/awkward/pull/1474\r\n* feat: add missing `_like` methods to `TypeTracer` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1505\r\n* feat: from rdataframe for awkward arrays by @ianna in https://github.com/scikit-hep/awkward/pull/1508\r\n* feat: Add typeparser to v2. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1513\r\n* feat: add C++ headers-only distribution configuration by @ianna in https://github.com/scikit-hep/awkward/pull/1523\r\n* feat: Growable Buffer header by @ManasviGoyal in https://github.com/scikit-hep/awkward/pull/1535\r\n* feat: parquet redux by @martindurant in https://github.com/scikit-hep/awkward/pull/1476\r\n* feat: Templated LayoutBuilder by @ManasviGoyal in https://github.com/scikit-hep/awkward/pull/1494\r\n* feat: raise `AttributeError` for public Array attributes by @agoose77 in https://github.com/scikit-hep/awkward/pull/1573\r\n* feat: Public interface for layout.recursively_apply and broadcast_and_apply. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1610\r\n* feat: prevent reducers like ak.sum on records (v2) by @ioanaif in https://github.com/scikit-hep/awkward/pull/1607\r\n* feat: Drop `ak.behavior['.', 'Name'] = cls`, which isn't working/isn't tested. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1651\r\n* feat: retrieve multiple columns from RDataFrame in a single event loop by @ianna in https://github.com/scikit-hep/awkward/pull/1625\r\n\r\n## Bug-fixes\r\n\r\n* fix: Fixed ak.num with axis=0 in typetracer. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1329\r\n* fix: ak.flatten and ak.ravel should test for nplike.ndarray, not np.ndarray. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1340\r\n* fix: Straighten out error handling via a thread-local (but otherwise global) context by @jpivarski in https://github.com/scikit-hep/awkward/pull/1327\r\n* fix: Fix PR #788: avoid materializing VirtualArrays in ak.with_name. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1346\r\n* fix: fix docstring line in is_tuple (v1) by @agoose77 in https://github.com/scikit-hep/awkward/pull/1356\r\n* fix: fixes #1363 by ensuring that arguments documented as 'iterable of X' aren't used in 'len(X)'. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1364\r\n* fix: Allow NumPy arrays in CppStatements; fix row_groups in single-file from_parquet. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1372\r\n* fix: Fixes nonlocal reducers in which the first list is empty. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1378\r\n* fix: Fix _prettyprint after 'for i in range' changed to 'for i, val in enumerate'. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1384\r\n* fix: bump black to 22.3.0 due to click 8.1 release by @henryiii in https://github.com/scikit-hep/awkward/pull/1385\r\n* fix: No zero-length shortcuts for ak.argsort (v1 & v2). by @jpivarski in https://github.com/scikit-hep/awkward/pull/1387\r\n* fix: ErrorContexts should only contain strings. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1393\r\n* fix: ROOT doesn't recognize for-each iterators without operator== by @jpivarski in https://github.com/scikit-hep/awkward/pull/1398\r\n* fix: Implement `recursively_apply` for `Record` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1401\r\n* fix: `from_numpy` references `ListArray64` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1404\r\n* fix: Windows builds stopped working; be looser about directory name. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1407\r\n* fix: pypy 3.9 by @henryiii in https://github.com/scikit-hep/awkward/pull/1412\r\n* fix: replace llvmlite.ir instead of llvmlite.llvmpy.core by @Ahmad-AlSubaie in https://github.com/scikit-hep/awkward/pull/1413\r\n* fix: Fix performance issue in v2 tolist. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1418\r\n* fix: Fix iteration over NumpyArray type. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1419\r\n* fix: Removed bytemask() in favour of mask_as_bool() by @ioanaif in https://github.com/scikit-hep/awkward/pull/1410\r\n* fix: Fix ak._v2.to_arrow for sliced ListOffsetArray. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1425\r\n* fix: ListOffsetArray._reduce_next is not implemented for 32-bit. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1426\r\n* fix: Fixed miscellaneous optiontype-in-Parquet bugs by @jpivarski in https://github.com/scikit-hep/awkward/pull/1431\r\n* fix: Remove duplicated import of to/from-parquet by @douglasdavis in https://github.com/scikit-hep/awkward/pull/1435\r\n* fix: is_unique() for IndexedArray by @ioanaif in https://github.com/scikit-hep/awkward/pull/1429\r\n* fix: Fixes `to_layout` with `allow_records=False` and allows single-record writing to Arrow and Parquet by @jpivarski in https://github.com/scikit-hep/awkward/pull/1456\r\n* fix: Fix RDataFrame GetColumnNames order in test. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1457\r\n* fix: Streamline recursively_apply for small slices of big arrays. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1458\r\n* fix: _getitem_* functions must consistently set the slicer in handle_error. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1469\r\n* fix: `to_rdataframe` extensive tests and bug fixes by @ianna in https://github.com/scikit-hep/awkward/pull/1478\r\n* fix: Fix selecting columns from Parquet. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1486\r\n* fix: Fix categorical equality handling (bad copy-paste from v1). by @jpivarski in https://github.com/scikit-hep/awkward/pull/1492\r\n* fix: _to_numpy method should return a numpy array by @swishdiff in https://github.com/scikit-hep/awkward/pull/1496\r\n* fix: Fix/Fixed slicing shape for array of booleans by @ioanaif in https://github.com/scikit-hep/awkward/pull/1497\r\n* fix: Fixed typo in unmaskedarray.py. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1498\r\n* fix: Fix slicing for UnmaskedArrays (which come from Arrow). by @jpivarski in https://github.com/scikit-hep/awkward/pull/1499\r\n* fix: Fix for issue 1406 by @ioanaif in https://github.com/scikit-hep/awkward/pull/1502\r\n* fix: Update type-parser for v2 by @jpivarski in https://github.com/scikit-hep/awkward/pull/1514\r\n* fix: numba pre-commit issues by @ioanaif in https://github.com/scikit-hep/awkward/pull/1533\r\n* fix: Initialize values behind the mask in ak.to_numpy by @ioanaif in https://github.com/scikit-hep/awkward/pull/1531\r\n* fix: Fixed RecordArray.__repr__ (last vestige of 'override' misunderstanding). by @jpivarski in https://github.com/scikit-hep/awkward/pull/1537\r\n* fix: Error when using ak.copy in v2 by @ioanaif in https://github.com/scikit-hep/awkward/pull/1532\r\n* fix: ak._v2.is_none check for axis value  by @ioanaif in https://github.com/scikit-hep/awkward/pull/1539\r\n* fix: Solving the endian bug on MacOS by @aryan26roy in https://github.com/scikit-hep/awkward/pull/1538\r\n* fix: fix ak2 convert class name msg by @Moelf in https://github.com/scikit-hep/awkward/pull/1544\r\n* fix: GrowableBuffer.h's missing <cstring> import broke localbuild.py; fixing it. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1555\r\n* fix: Remove std::cout from ArrayBuilder code. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1561\r\n* fix: Missing `axis_wrap_if_negative`  Record method in both v1 and v2 by @ioanaif in https://github.com/scikit-hep/awkward/pull/1565\r\n* fix: ufuncs on records should not be allowed unless overridden by @ioanaif in https://github.com/scikit-hep/awkward/pull/1559\r\n* fix: add int64_t definition for Windows by @ianna in https://github.com/scikit-hep/awkward/pull/1572\r\n* fix: allow empty `RecordArray`s in `ak.to_layout` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1581\r\n* fix: tolist() bug from Uproot (longlong type for Index)  by @ioanaif in https://github.com/scikit-hep/awkward/pull/1567\r\n* fix: Lengths of empty regular slices by @ioanaif in https://github.com/scikit-hep/awkward/pull/1568\r\n* fix: RegularArray: maybe_toNumpy() by @ioanaif in https://github.com/scikit-hep/awkward/pull/1589\r\n* fix: Raises an exception on mixed Awkward/NumPy slices. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1598\r\n* fix: ak.concatenate (mergemany) should preserve regular-type. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1604\r\n* fix: ak.concatenate of identical Forms preserves the Form by @jpivarski in https://github.com/scikit-hep/awkward/pull/1605\r\n* fix: ak.concatenate should preserve regular-type for axis>0, too. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1609\r\n* fix: improve `ak.from_iter` performance for long (axis=0) arrays by @agoose77 in https://github.com/scikit-hep/awkward/pull/1614\r\n* fix: generator tolayout in cling by @ianna in https://github.com/scikit-hep/awkward/pull/1613\r\n* fix: Use dtype=np.int64, not int, for platform independence (Windows). by @jpivarski in https://github.com/scikit-hep/awkward/pull/1636\r\n* fix: support root empty field in Parquet file by @agoose77 in https://github.com/scikit-hep/awkward/pull/1619\r\n* fix: empty slice lists of record arrays (#1593) by @agoose77 in https://github.com/scikit-hep/awkward/pull/1597\r\n* fix: ak.from_iter should interpret top-level tuples as ak.Array (v1 and v2). by @jpivarski in https://github.com/scikit-hep/awkward/pull/1642\r\n* fix: Ensure that ak._v2.to_json raises errors when appropriate. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1649\r\n* fix: pass a copy of `RecordArray`'s internal fields in HL API by @Saransh-cpp in https://github.com/scikit-hep/awkward/pull/1650\r\n* fix: include `self._length` in `RegularArray.mergemany` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1645\r\n* fix: jagged slicing for `ListArray` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1408\r\n* fix: to_list must follow __getitem__ implementations, even in Records by @jpivarski in https://github.com/scikit-hep/awkward/pull/1652\r\n\r\n## Other\r\n\r\n* perf: `ak.concatenate` should flatten for first axis, single-array by @agoose77 in https://github.com/scikit-hep/awkward/pull/1641\r\n* refactor: cleanup reducer by @agoose77 in https://github.com/scikit-hep/awkward/pull/1365\r\n* refactor: split up functions by @martindurant in https://github.com/scikit-hep/awkward/pull/1397\r\n* refactor: Flatten directory structure under src/awkward/_v2/operations. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1467\r\n* refactor: Rename low-level methods to match high-level function names. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1468\r\n* refactor: Rename fillna -> fill_none. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1470\r\n* refactor: Refactoring to include index_nplike and reducers by @swishdiff in https://github.com/scikit-hep/awkward/pull/1490\r\n* refactor: LayoutBuilder migration to v2 by @ianna in https://github.com/scikit-hep/awkward/pull/1484\r\n* refactor: restructure cpp headers by @ianna in https://github.com/scikit-hep/awkward/pull/1524\r\n* refactor: migrate ArrayBuilder to new GrowableBuffer by @ianna in https://github.com/scikit-hep/awkward/pull/1542\r\n* refactor: builder options and layout builder updates by @ianna in https://github.com/scikit-hep/awkward/pull/1560\r\n* refactor: Renamed ak.to_pandas -> ak.to_dataframe. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1603\r\n* refactor: replace 3 ak.from_json* functions with one by @jpivarski in https://github.com/scikit-hep/awkward/pull/1617\r\n* refactor: from_rdataframe to use LayoutBuilder by @ianna in https://github.com/scikit-hep/awkward/pull/1620\r\n* docs: add Ahmad-AlSubaie as a contributor for code by @allcontributors in https://github.com/scikit-hep/awkward/pull/1416\r\n* docs: Update AwkwardForth documentation and move it from the wiki to the standard docs. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1422\r\n* docs: update README: from awkward-1.0 to awkward by @ianna in https://github.com/scikit-hep/awkward/pull/1475\r\n* docs: Add the beginning of an example of templated LayoutBuilder. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1485\r\n* docs: add ManasviGoyal as a contributor for code by @allcontributors in https://github.com/scikit-hep/awkward/pull/1540\r\n* docs: add aryan26roy as a contributor for code by @allcontributors in https://github.com/scikit-hep/awkward/pull/1541\r\n* docs: Doxygen Documentation of GrowableBuffer and LayoutBuilder. by @ManasviGoyal in https://github.com/scikit-hep/awkward/pull/1579\r\n* docs: Documented ak.from_json and completed it. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1621\r\n* docs: add Saransh-cpp as a contributor for code by @allcontributors in https://github.com/scikit-hep/awkward/pull/1653\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1370\r\n* ci: Protect test 1300 from ROOT without C++17 (or, at least, without std::optional). by @jpivarski in https://github.com/scikit-hep/awkward/pull/1383\r\n* ci: Build(deps): bump pypa/cibuildwheel from 2.3.1 to 2.4.0 by @dependabot in https://github.com/scikit-hep/awkward/pull/1394\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1395\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1409\r\n* ci: Pass on skipped v2 tests by @ioanaif in https://github.com/scikit-hep/awkward/pull/1445\r\n* ci: Build(deps): bump pypa/cibuildwheel from 2.4.0 to 2.5.0 by @dependabot in https://github.com/scikit-hep/awkward/pull/1455\r\n* ci: awkward rdataframe source tests by @ianna in https://github.com/scikit-hep/awkward/pull/1446\r\n* ci: Ignore a NumPy 1.22 warning in Numba and fix the flake8-print T001 --> T201 change. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1464\r\n* ci: Build(deps): bump docker/setup-qemu-action from 1.2.0 to 2.0.0 by @dependabot in https://github.com/scikit-hep/awkward/pull/1462\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1465\r\n* ci: Build(deps): bump pypa/cibuildwheel from 2.5.0 to 2.6.1 by @dependabot in https://github.com/scikit-hep/awkward/pull/1493\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1503\r\n* ci: Build(deps): bump pypa/cibuildwheel from 2.6.1 to 2.7.0 by @dependabot in https://github.com/scikit-hep/awkward/pull/1510\r\n* ci: Swap 'merged dtype same as NumPy' test of v1 for test of v2. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1515\r\n* ci: Build(deps): bump actions/upload-artifact from 2 to 3 by @dependabot in https://github.com/scikit-hep/awkward/pull/1519\r\n* ci: Build(deps): bump actions/checkout from 2 to 3 by @dependabot in https://github.com/scikit-hep/awkward/pull/1520\r\n* ci: Build(deps): bump actions/download-artifact from 2 to 3 by @dependabot in https://github.com/scikit-hep/awkward/pull/1521\r\n* ci: Build(deps): bump pypa/cibuildwheel from 2.7.0 to 2.8.0 by @dependabot in https://github.com/scikit-hep/awkward/pull/1536\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1527\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1543\r\n* ci: Build(deps): bump pypa/cibuildwheel from 2.8.0 to 2.8.1 by @dependabot in https://github.com/scikit-hep/awkward/pull/1556\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1566\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1576\r\n* ci: Build(deps): bump pypa/gh-action-pypi-publish from 1.5.0 to 1.5.1 by @dependabot in https://github.com/scikit-hep/awkward/pull/1564\r\n* ci: use GHA instead of Azure :hammer:  by @agoose77 in https://github.com/scikit-hep/awkward/pull/1550\r\n* ci: Upgrade Linux 18.04 -> 20.04 and DOS line endings. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1616\r\n* ci: lint PR titles according to conventional commits by @agoose77 in https://github.com/scikit-hep/awkward/pull/1615\r\n* ci: aarch64 should also use cibuildwheel v2.9.0 by @jpivarski in https://github.com/scikit-hep/awkward/pull/1627\r\n* ci: run semantic-pr-title in concurrency group by @agoose77 in https://github.com/scikit-hep/awkward/pull/1632\r\n* ci: add Linux ROOT build to run ROOT-based tests by @ianna in https://github.com/scikit-hep/awkward/pull/1629\r\n* package: fix MANIFEST to include cpp-headers by @agoose77 in https://github.com/scikit-hep/awkward/pull/1516\r\n* chore: remove unneeded lines by @henryiii in https://github.com/scikit-hep/awkward/pull/1518\r\n* chore: include Python 3.11 by @henryiii in https://github.com/scikit-hep/awkward/pull/1602\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1618\r\n\r\n## New Contributors\r\n* @Ahmad-AlSubaie made their first contribution in https://github.com/scikit-hep/awkward/pull/1413\r\n* @martindurant made their first contribution in https://github.com/scikit-hep/awkward/pull/1397\r\n* @aryan26roy made their first contribution in https://github.com/scikit-hep/awkward/pull/1491\r\n* @ManasviGoyal made their first contribution in https://github.com/scikit-hep/awkward/pull/1535\r\n* @Moelf made their first contribution in https://github.com/scikit-hep/awkward/pull/1544\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/1.8.0...v1.9.0\r\n\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v1.9.0'>Version 1.9.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-09-02T19:29:53Z",
  "number":1664,
  "title":"Version 1.9.0",
  "url":"https://github.com/scikit-hep/awkward/discussions/1664"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## Bug-fixes\r\n\r\n* fix: allow string arrays to merge without unions by @agoose77 in https://github.com/scikit-hep/awkward/pull/1671\r\n* fix: carry parameters through broadcasting by @agoose77 in https://github.com/scikit-hep/awkward/pull/1679\r\n\r\n## Other\r\n\r\n* refactor: move categorical functions to operations by @agoose77 in https://github.com/scikit-hep/awkward/pull/1674\r\n* chore: remove `dev/build-awkward.sh` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1675\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v1.10.0rc1...v1.10.0rc2\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v1.10.0rc2'>Version 1.10.0rc2</a>.</em>",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"@ivirshup and @grst, this release _should_ have everything you need. When it's available (~4 hours), can you test it?\r\n\r\nhttps://github.com/scikit-hep/awkward/actions/runs/3009821080\r\n\r\nIf not, we'll do an rc3 to try to get it stable before 1.10.0, which is also the last minor number before we split git branches into a v2-only and a v1-legacy branch (https://github.com/scikit-hep/awkward/issues/1626). After that, 1.10.x will be taking v1 bug-fixes, and we'll be dismantling v1 from `main` to be able to release 2.0.0rc1. You'll eventually be moving to 2.0.0rcX, which should only require replacing\r\n\r\n```python\r\nimport awkward._v2 as ak\r\n```\r\n\r\nwith\r\n\r\n```python\r\nimport awkward as ak\r\n```\r\n\r\nthough there will likely be a time-gap between releasing 1.10.0 and 2.0.0rc1, as we handle all the issues of dismantling v1 from `main`. We'd be able to put bug-fixes into 1.10.1, etc., but hopefully just v1 bug-fixes, not v2 bug-fixes. (Because v2 bug-fixes would have to be merged into both `main` and `main-v1`.)\r\n\r\nSo that's why we want to ensure that 1.10.0 has everything you need, so that you're not waiting for a v2 bug-fix while we're stuck with a still-transitioning `main` branch.",
     "createdAt":"2022-09-07T18:30:47Z",
     "number":3588056,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"Do you have a timeframe in mind for `1.10.0`?",
        "createdAt":"2022-09-08T12:09:43Z",
        "number":3594020
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"If you don't see any issues with it, I'll make it today.\r\n\r\nOn our side, we would _like_ to make it quickly because we're holding off on merging longer-term PRs so that this release consists of only dropping Python 3.6 and bug-fixes for you.",
        "createdAt":"2022-09-08T13:38:51Z",
        "number":3594966
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"We're looking at #1688 right now.",
        "createdAt":"2022-09-08T13:51:29Z",
        "number":3595086
       },
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"Do you have a timeframe for 2.0?\r\n\r\nIf you'd like to make the 1.10 release ASAP, I'd be inclined to say don't worry too much about us since we're aiming for the v2 api anyways.\r\n\r\nI am not sure I am going to be able to get the anndata PR basically done until at least this weekend.\r\n\r\nThere are a two major things to change there which involve aligning types and trying a different approach with the behaviors.",
        "createdAt":"2022-09-08T15:03:51Z",
        "number":3595744
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"No, we don't need the AnnData PR to be done\u2014that would be a big ask, considering how long it's been open and how much work there is in it. What we're trying to do is to get Awkward 1.10.0 to a plateau of usability so that it will be okay for us to not be able to give you bug-fix releases for a week or so while we do our infrastructure work.\r\n\r\nRight now, we will be including #1689, #1676, #1673, and #1562 in 1.10.0rc3 (not all of these are motivated by AnnData, some are older). That is our current best estimate of what will need to go into 1.10.0.",
        "createdAt":"2022-09-08T16:06:55Z",
        "number":3596718
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"In the end, only #1676 was held back. I'm starting the 1.10.0rc3 release now.\r\n\r\nhttps://github.com/scikit-hep/awkward/actions/runs/3017192412",
        "createdAt":"2022-09-08T17:50:53Z",
        "number":3598165
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"[awkward 1.10.0rc3](https://pypi.org/project/awkward/1.10.0rc3/) is ready to be pip-installed and tested. If that's stable enough for you for (let's say) the next two or three weeks, then we'll make a final release.\r\n\r\nAfter we've dismantled v1 from `main`, we'll be in a position to provide bug-fixes in the 2.0.0rcX series, which do `import awkward as ak` for v2.",
        "createdAt":"2022-09-08T22:17:49Z",
        "number":3601493
       },
       {
        "author":{
         "login":"ivirshup"
        },
        "body":"Sorry for the late response here, had a bit of a surprise conference.\r\n\r\nThe 1.10.0rc3 looks to only decrease errors on our test suite! Not blocked from our side at the moment",
        "createdAt":"2022-09-13T20:42:25Z",
        "number":3638883
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"[awkward 1.10.0rc4](https://pypi.org/project/awkward/1.10.0rc4/) is ready for testing. If you have no new issues by tomorrow, I'll release 1.10.0.\r\n\r\n@agoose77 has had a lot of success with the v1-removal so far (#1690), so maybe it won't be much of a gap between 1.10.0 and 2.0.0rc1 after all.",
        "createdAt":"2022-09-15T17:55:45Z",
        "number":3656347
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Unless there's any objection, I'm going to make the final 1.10.0 today, and then we'll start dismantling v1 from the `main` branch (meaning: get #1690 working and then apply it).",
        "createdAt":"2022-09-19T13:48:00Z",
        "number":3681209
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I'm going to release 1.10.0 (no rc) now.",
        "createdAt":"2022-09-19T16:48:43Z",
        "number":3682996
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"(It's done.)",
        "createdAt":"2022-09-19T21:59:09Z",
        "number":3684932
       }
      ],
      "totalCount":12
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-09-07T18:18:38Z",
  "number":1684,
  "title":"Version 1.10.0rc2",
  "url":"https://github.com/scikit-hep/awkward/discussions/1684"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n* feat: raise Error for Record.__setattr__ by @agoose77 in https://github.com/scikit-hep/awkward/pull/1699\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: allow string arrays to merge without unions by @agoose77 in https://github.com/scikit-hep/awkward/pull/1671\r\n* fix: carry parameters through broadcasting by @agoose77 in https://github.com/scikit-hep/awkward/pull/1679\r\n* fix: don't project `categorical` in `ak._v2.packed` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1689\r\n* fix: reduce runtime dependency from setuptools to just packaging by @veprbl in https://github.com/scikit-hep/awkward/pull/1562\r\n* fix: ensure that `__copy__` and `__deepcopy__` are enabled. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1695\r\n* fix: pass memo to deepcopy by @agoose77 in https://github.com/scikit-hep/awkward/pull/1698\r\n* perf: improve Array initialisation performance by @agoose77 in https://github.com/scikit-hep/awkward/pull/1700\r\n\r\n## Other\r\n\r\n* refactor: move categorical functions to operations by @agoose77 in https://github.com/scikit-hep/awkward/pull/1674\r\n* test: skip individual np.float16/128 tests because different systems have different combinations. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1691\r\n* ci: Build(deps): bump pypa/cibuildwheel from 2.9.0 to 2.10.0 by @dependabot in https://github.com/scikit-hep/awkward/pull/1692\r\n* chore: drop 3.6 support by @agoose77 in https://github.com/scikit-hep/awkward/pull/1661\r\n* chore: remove `dev/build-awkward.sh` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1675\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1673\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v1.9.0...v1.10.0\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v1.10.0'>Version 1.10.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-09-19T16:51:59Z",
  "number":1704,
  "title":"Version 1.10.0",
  "url":"https://github.com/scikit-hep/awkward/discussions/1704"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n_(none)_\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: support non-hashable parameter values in broadcasting by @agoose77 in https://github.com/scikit-hep/awkward/pull/1708\r\n\r\n## Other\r\n\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1706\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v1.10.0...v1.10.1rc1\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v1.10.1rc1'>Version 1.10.1rc1</a>.</em>",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"philippemiron"
     },
     "body":"Just to confirm that this fixes the issues I had with 1.10.0, cheers. ",
     "createdAt":"2022-09-22T14:37:45Z",
     "number":3710095,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-09-21T19:04:33Z",
  "number":1711,
  "title":"Version 1.10.1rc1",
  "url":"https://github.com/scikit-hep/awkward/discussions/1711"
 },
 {
  "author":{
   "login":"shaoweisong"
  },
  "body":"Hi experts,\r\nI am using a preselection framework to do the gen level study. After selecting gen particles, there are 800486 events left. 119 events have 4 genparticles, so the lenth of these event arrays is 4 (like event 538). And the length of other event arrays is 2 (like event 3), which means there are 2 particles in each of these events. \r\n<img width=\"453\" alt=\"1663838580440\" src=\"https://user-images.githubusercontent.com/76048397/191710121-52e26c01-e704-4e8f-a5f6-a885a1b8456b.png\">\r\nAnd for now, I need to remove the event arrays with 4 particles. I know in numpy array we can use np.delete(arr, index_list,axis=0) to do this kind of thing. But What should I do to remove the events corresponding to index_list in awkward array? \r\n<img width=\"217\" alt=\"1663840119713\" src=\"https://user-images.githubusercontent.com/76048397/191715850-24f2caec-6917-4772-88d9-224d87bded93.png\">\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"## Finding where sublists have a particular length\r\nBefore we look at removing the rows (sublists) that have lengths different from four, let's first _find_ where those sublists are. Then we can ask Awkward Array to remove them.\r\n\r\nConsider an example list `particle_x`:\r\n```python\r\n>>> particle_x = ak.Array([\r\n    [1.0, 2.0, 3.0, 4.0],\r\n    [5.0, 6.0],\r\n    []\r\n])\r\n>>> particle_x\r\n<Array [[1, 2, 3, 4], [5, 6], []] type='3 * var * float64'>\r\n```\r\n\r\nFirst of all, let's measure the length of each sublist in `particle_x`. If I compute `ak.num(particle_x, axis=-1)`, Awkward will return a list of integers that corresponds to the lengths of these sublists:\r\n```python\r\n>>> n_particles = ak.num(particle_x, axis=-1)\r\n>>> n_particles\r\n<Array [4, 2, 0] type='3 * int64'>\r\n```\r\n\r\nNow, if I want to only take the sublists that have length 4, I can build a boolean array from this result by asking whether the result of `ak.num()` is equal to 4:\r\n```python\r\n>>> has_four_particles = n_particles == 4\r\n>>> has_four_particles\r\n<Array [True, False, False] type='3 * bool'>\r\n```\r\n\r\n## Keeping only the required sublists\r\nNow that we know where the lists that we want to keep are, we need to work out what to do with them. Here, you need to decide whether you need the result of \"deleting\" the wrong-sized lists to have the same length as your original array. This might be if you want to compare some aspect of your transformed array with the original. \r\n\r\nIf you _do_ care about the length of your array, then you can use `array.mask[...]` to set the values you do not care about to `None`, e.g.:\r\n```python\r\n>>> particle_x.mask[has_four_particles]\r\n<Array [[1, 2, 3, 4], None, None] type='3 * option[var * float64]'>\r\n```\r\n\r\nYou can see that only the first list is retained, whilst the subsequent lists are set to `None`. This is why the `type` of the Array has changed from `'3 * var * float64'` to `'3 * option[var * float64]'`  - the `option` refers to potentially missing values.\r\n\r\nIf you _don't_ care about keeping the resulting list the same length, you can just directly filter your array like so:\r\n```python\r\n>>> particle_x[has_four_particles]\r\n<Array [[1, 2, 3, 4]] type='1 * var * float64'>\r\n```\r\n\r\nYou can see that this array has the same number of dimensions (2), but only one sublist (`[1, 2, 3, 4]`).\r\n\r\n## Final Solution\r\n\r\nTherefore, in your case, you want to compute\r\n```python\r\ngen_lv[\r\n    ak.num(gen_lv, axis=-1) == 4\r\n]\r\n```",
     "createdAt":"2022-09-22T11:22:46Z",
     "number":3708533,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"shaoweisong"
        },
        "body":"Thank you so much for your answer :)",
        "createdAt":"2022-09-23T05:11:52Z",
        "number":3714214
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-09-22T09:59:18Z",
  "number":1714,
  "title":"How to delete some events (know the index) in analysis using awkward??",
  "url":"https://github.com/scikit-hep/awkward/discussions/1714"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n_(none)_\r\n\r\n## Bug fixes and performance\r\n\r\n* fix: support non-hashable parameter values in broadcasting by @agoose77 in https://github.com/scikit-hep/awkward/pull/1708\r\n\r\n## Other\r\n\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1706\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v1.10.0...v1.10.1\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v1.10.1'>Version 1.10.1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-09-22T15:06:47Z",
  "number":1718,
  "title":"Version 1.10.1",
  "url":"https://github.com/scikit-hep/awkward/discussions/1718"
 },
 {
  "author":{
   "login":"thoglu"
  },
  "body":"Hi,\r\nI currently save data into parquet files, where I have a fixed amount of entries (lets say 1000) per file in various columns. Most of the columns have single float entries, but one column stores 2-d Arrays of which 1 dimension can vary, so something like DXN where N is different per entry. \r\n\r\nIf I do a `awkward.from_parquet(filename, columns=[*single_float_col_name*,...])`, it loads extremely fast.\r\nHowever if I include the `uneven` array column, it loads like a factor 100 or so slower.\r\n `awkward.from_parquet(filename, columns=[*single_float_col_name*, *uneven_col_name*])`\r\n\r\nCan I somehow specify row_groups or other loading options, choose a different dataformat, or give certain specs during parquet file creation in order to speed up the reading process when I include the uneven array?\r\n\r\nI am really only interested in one particular row (lets say row 533 of 1000 rows) and a subset of columns for this row... and one of those columns has an uneven array in it, as I said above.\r\n\r\nAny help appreciated.\r\nBest,\r\nThorsten\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I need to follow up on this when I have time to look things up, but I can provide some pointers in the meantime. There's another function, `ak.metadata_from_parquet`, which reads the (small) metadata of a Parquet file but not the (large) data. In this metadata, there are fields for `num_entries`, `num_row_groups`, and also row-group by row-group information about exactly which entries (rows) are in each row group.\r\n\r\nIf you have a specific entry/row to read, or a specific range, `entry_start:entry_stop`, this can be expanded to `row_group_start:row_group_stop` by rounding down the start index and rounding up the stop index. (There is no way to read _one_ entry; row groups are the smallest granularity that can be read, but you'll probably be able to ensure that you only read one row group.)\r\n\r\nThat trimming, to produce a given entry range by reading as few row groups as possible, could be automated, but it hasn't (yet). It would be a good feature for us to add.\r\n\r\nBut for now, if you can get that information for yourself from `ak.metadata_from_parquet`, you can use that information in `ak.from_parquet` by passing a `row_groups=[...]` argument, where `...` is a list of row group numbers.\r\n\r\nFor columns, it looks like you've already found the `column` argument, which takes a list of strings. Those strings are allowed to have glob-style wildcards: `columns=[\"*single_float_col_name*\", \"*uneven_col_name*\"]` (you just need quotes).",
     "createdAt":"2022-10-13T15:22:56Z",
     "number":3871287,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"thoglu"
        },
        "body":"ok thanks .. yeah currently in the file there is a global single row group, so I would have to add more row groups I guess and then be able to read the specific one I am interested in. Another option I was thinking is using hdf with an indexing table that is already in use in KM3Net (Tamas Gal wrote this a while back). Or is there any other file format that might allow to do this faster (SQL?) ?",
        "createdAt":"2022-10-13T15:31:03Z",
        "number":3871365
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Columnar formats are optimized for bulk reads, not individual row reading or incremental appending. A Parquet file, for instance, is immutable after it has been created, since all of the navigation information is stored in a footer. (Actually, I could imagine appending to a Parquet file by _overwriting_ that footer with another row group and an updated footer, but I don't think any existing Parquet tools work that way.) Adding data to a Parquet dataset generally means adding more Parquet files to a directory.\r\n\r\nBy \"columnar formats,\" I would generally include HDF5, since those arrays (\"Datasets\") are accessed in bulk. But HDF5 has a lot of different ways of storing rectilinear arrays, and some of those methods might be append or insertion-friendly. HDF5 doesn't deal well with variable-length data, if that's why you were interested in Parquet and Awkward Array.\r\n\r\nIf you're really inserting rows frequently and interested in accessing individual rows, you're probably right that a traditional database is more appropriate. SQL is just the query language, do you mean SQLite (a file), BerkleyDB (a UNIX standard), PostgreSQL (a service), ...? If you need nested structures with random access/insertion, maybe MongoDB, which deals with JSON data transactionally?\r\n\r\nThe \"either/or\" thing here is that non-columnar formats are _not_ optimized for bulk reads. If you need high performance for one type of access or another type of access, you'll need the data to be laid out in one way or another way; it can't be both. It's a tradeoff.",
        "createdAt":"2022-10-13T16:05:54Z",
        "number":3871688
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Wellllllll... maybe uncompressed, memory-mapped Feather might be a \"best of both\" for individual row reading and bulk, sequential reading, but not row-insertion. And maybe appending can also be efficient, as long as the insertion happens in large chunks: a batch at a time. You can't optimize all desiderata in one format. I wrote a survey of tradeoffs once: https://indico.cern.ch/event/658060/contributions/2898569/",
        "createdAt":"2022-10-13T16:12:34Z",
        "number":3871746
       },
       {
        "author":{
         "login":"thoglu"
        },
        "body":"yes I meant sqllite ... in principle I am only interested in a particular item at a time, so this might be a good option. One more question: Is there a header in parquet into which information can be stored? Wouldn't it be possible to have an index table there, where for each item (including the variable-length array entries) there is some starting index (e.g. which byte) and how many bytes the data structure takes? Or is something like this already being done ? Or would this not help ?\r\n\r\n**edit:** ok looking at your slides, it seems \"random accessibility\" is what I want (incl. variable length arrays - and no nested structures - I also do not need to append/add new data once it is created.. it can be immutable). It looks like root would do that, but I dont really want to use root if I dont have to ;) So would you also now ( a few years after those slides) say either root or something like sql lite (or feather?) ?",
        "createdAt":"2022-10-13T16:23:48Z",
        "number":3871850
       },
       {
        "author":{
         "login":"thoglu"
        },
        "body":"> Wellllllll... maybe uncompressed, memory-mapped Feather might be a \"best of both\" for individual row reading and bulk, sequential reading, but not row-insertion. And maybe appending can also be efficient, as long as the insertion happens in large chunks: a batch at a time. You can't optimize all desiderata in one format. I wrote a survey of tradeoffs once: https://indico.cern.ch/event/658060/contributions/2898569/\r\n\r\nHow would you do row-reading with feather? \r\nIn pyarrow no row option seems to be there (https://arrow.apache.org/docs/python/generated/pyarrow.feather.read_table.html),\r\nhowever this is the suggested option in the awkward documentation to transform to ak array.\r\n\r\nWith pandas.read_table (https://pandas.pydata.org/docs/reference/api/pandas.read_table.html) ?\r\n\r\nBest,\r\nThorsten",
        "createdAt":"2022-10-13T16:44:25Z",
        "number":3872009
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"(On Feather, I meant in principle, based on what the file format is like, not necessarily that any libraries use the data-on-disk in that way.)\r\n\r\nIf you're interested in single-entry access, that sounds like SQL systems. SQLite files are row-oriented, but that's what you want. I don't know what limitations SQLite has on value types, but if that's limiting, MongoDB will generalize them into JSON.",
        "createdAt":"2022-10-13T17:55:44Z",
        "number":3872536
       }
      ],
      "totalCount":6
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-10-13T12:12:45Z",
  "number":1785,
  "title":"Fast way to read a particular row from and specific columns with uneven entries (which file format?!, saving/loading options etc.)",
  "url":"https://github.com/scikit-hep/awkward/discussions/1785"
 },
 {
  "author":{
   "login":"fspinna"
  },
  "body":"Hi everyone,\r\nI'm relatively new to Numba and even newer to awkward arrays. I'm trying to understand how to use njitted functions with awkward arrays as input. For example, let's say that I've a list of numpy arrays having different lengths:\r\n\r\n```python\r\nimport numpy as np\r\nimport numba as nb\r\nimport awkward as ak\r\n\r\nn_rep = 100\r\nlists = [np.array([1, 2, 3]), np.array([1,2])] * n_rep\r\nak_array = ak.Array(lists)\r\nlists_numba = nb.typed.List(lists)  # numba typed list\r\n```\r\n\r\nI want to take the mean with axis=1. Of course, I can do it natively with awkward arrays, but to do it with numba I have to iterate over each np.array in the list, so the function will look like something like this:\r\n\r\n```python\r\n@nb.njit\r\ndef nb_mean(arr):\r\n  means = np.empty(shape=len(arr))\r\n  for i in range(len(arr)):\r\n    means[i] = np.mean(arr[i])\r\n  return means\r\n\r\nnb_mean(lists_numba)\r\n```\r\nIf I pass the awkward array to this function, numba raises this error:\r\n\r\n```python\r\nTypingError: Failed in nopython mode pipeline (step: nopython frontend)\r\nNo implementation of function Function(<function mean at 0x7f05dd254b00>) found for signature:\r\n \r\n >>> mean(ak.ArrayView(ak.NumpyArrayType(array(int64, 1d, A), none, {}), None, ()))\r\n \r\nThere are 2 candidate implementations:\r\n  - Of which 2 did not match due to:\r\n  Overload in function 'Numpy_method_redirection.generic': File: numba/core/typing/npydecl.py: Line 379.\r\n    With argument(s): '(ak.ArrayView(ak.NumpyArrayType(array(int64, 1d, A), none, {}), None, ()))':\r\n   Rejected as the implementation raised a specific error:\r\n     TypeError: array does not have a field with key 'mean'\r\n   \r\n   (https://github.com/scikit-hep/awkward-1.0/blob/1.10.1/src/awkward/_connect/_numba/layout.py#L339)\r\n  raised from /usr/local/lib/python3.7/dist-packages/awkward/_connect/_numba/layout.py:339\r\n\r\nDuring: resolving callee type: Function(<function mean at 0x7f05dd254b00>)\r\nDuring: typing of call at <ipython-input-27-1014e122c504> (8)\r\n\r\n\r\nFile \"<ipython-input-27-1014e122c504>\", line 8:\r\ndef nb_mean(arr):\r\n    <source elided>\r\n  for i in range(len(arr)):\r\n    means[i] = np.mean(arr[i])\r\n    ^\r\n```\r\n\r\nThe code works if I first convert the awkward array with np.array as in the following:\r\n\r\n```python\r\n@nb.njit\r\ndef nb_ak_mean(arr):\r\n  means = np.empty(shape=len(arr))\r\n  for i in range(len(arr)):\r\n    means[i] = np.mean(np.array(arr[i]))  # convert first to np.array then apply np.mean\r\n  return means\r\n```\r\n\r\nSo basically, If I try to pass directly ak_array to the `np.mean` without first converting it with `np.array` function, numba raises an error. \r\nThese are the performance of the 3 approaches:\r\n\r\n```python\r\n%timeit ak.mean(ak_array, axis=1)\r\n%timeit nb_ak_mean(ak_array)\r\n%timeit nb_mean(lists_numba)\r\n\r\n1.42 ms \u00b1 131 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)  # native awkward\r\n74 \u00b5s \u00b1 1.33 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)  # numba + awkward\r\n7.61 \u00b5s \u00b1 41.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)  # numba + List\r\n```\r\n\r\nThe approach using the List object from numba seem to be significantly faster than using awkward arrays. Am I doing something wrong?\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"ianna"
     },
     "body":"@fspinna - which version of awkward do you use?\r\n```python\r\n>>> ak.__version__\r\n'2.0.0rc1'\r\n```\r\n",
     "createdAt":"2022-10-24T13:29:25Z",
     "number":3950729,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"fspinna"
        },
        "body":"numba == 0.56.3\r\nawkward == 1.10.1\r\nnumpy == 1.21.6",
        "createdAt":"2022-10-24T13:36:49Z",
        "number":3950809
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Awkward Array (and Numba's) performance can be measured approximately as `time = initial_cost + rate*amount_of_work`. In this case, your array is too small for the work-scaling to be properly measured, i.e. the setup costs (`initial_cost`) are dominating the performance.\r\n\r\nIf you change your array to be more like ~1,000,000 elements, then the performance difference between the two Numba jitted cases is closer to a factor of 4:[^caveat]\r\n```\r\n%timeit ak.mean(ak_array, axis=1)\r\n%timeit nb_ak_mean(ak_array)\r\n%timeit nb_mean(lists_numba)\r\n763 ms \u00b1 19.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n117 ms \u00b1 9.63 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n28.8 ms \u00b1 1.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\nThe rest of this difference probably stems from the use of `np.array()` instead of `np.asarray()`. In Numba's case, we have a faster implementation of array conversion for `asarray` than `array`; `asarray` only handles 1D contiguous arrays, and directly interprets them as a numba `Array`, whereas the `np.array` function performs a copy (it can handle >1D arrays, and these can be non-contiguous in Awkward). This closes the performance gap, and now the Awkward jitted case is (barely) faster than the numba list jitted case:\r\n```\r\n%timeit nb_ak_mean(ak_array)\r\n18.3 ms \u00b1 703 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nOf course, Numba isn't always able to eek out the best performance if you use NumPy operations. A bare loop is usually the fastest solution, and can help avoid these kinds of quirks.\r\n\r\nIn general, though, don't worry about this kind of performance difference if it's not bottlenecking your workflow. There are always things you can do to improve performance, e.g. handling raggedness explicitly in your kernel by flattening the array and passing in the sublist lengths, but it's not always worth the extra maintenance burden and code complexity.\r\n\r\n[^caveat]: Note that this isn't a perfect test; Awkward Array should (in general) have a better memory layout than multiple small NumPy arrays; Awkward usually represents raggedness as variable-length views over a contiguous buffer (certainly not always, but often). This represents a more favourable scenario in terms of memory access.",
     "createdAt":"2022-10-24T13:48:51Z",
     "number":3951007,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"fspinna"
        },
        "body":"Thanks! Perfectly clear. This fits exactly my use case. Tested on my machine and your version is significantly faster than the numba List version.",
        "createdAt":"2022-10-24T14:02:28Z",
        "number":3951259
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"That's right: if the total time for a Numba-JITted function to run is \u00b5s, we don't know how it will scale (the `rate` part) until we give it a larger problem. This is likely dominated by `initial_cost`.\r\n\r\n--------------\r\n\r\nAnother consideration is that Python lists defined like\r\n\r\n```python\r\nlists = [np.array([1, 2, 3]), np.array([1, 2])] * n_rep\r\n```\r\n\r\nare actually repeated references to the same two arrays, not a dataset consisting of 5 \u00d7 `n_rep` numbers (as the Awkward Array is, since the ak.Array constructor iterates over the data, copying it into a columnar representation).\r\n\r\nThese two small arrays are easily small enough to fit in a CPU's cache (even the L1 cache, which is typically between 8 kB and 64 kB), so it does not need to pull data from main memory in the calculation. If the calculation you're performing is simple enough, getting data from main memory can easily become the bottleneck.\r\n\r\nGetting estimates from [this infographic](http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/),\r\n\r\n  * the `+` in your `np.mean` takes < 1 clock cycle\r\n  * the floating point `/` in your `np.mean` takes 10\u201240 clock cycles (`np.sum` would be very different from `np.mean`!)\r\n  * fetching data from L1 takes 3\u20124 clock cycles\r\n  * fetching data from main memory takes 100\u2012150 clock cycles\r\n\r\n(Note: that's _after_ you already have an `n_rep` large enough that the process isn't dominated by unboxing/boxing data as you enter and leave the Numba context. I'm assuming you've already applied @agoose77's correction.)\r\n\r\nNaively, clock ticks translate into real time by dividing by your clock speed (~3 GHz), but even this is complicated by the fact that the CPU actually runs the process in pipelines and speculatively evaluates based on guesses that can later be invalidated. The main takeaway from these numbers is the relative orders of magnitude.\r\n\r\nVery likely, the real dataset you're interested in is not the same two Python objects repeated many times; I'll bet it's a large set of distinct lists, all with different values and lengths. While `%timeit` is correctly measuring the rate of _this_ test, you're probably interested in a different test. (Supposedly objective performance testing is full of subjective choices!)\r\n\r\n--------\r\n\r\nAnother thing to be aware of is that this line:\r\n\r\n```python\r\n    means[i] = np.mean(np.asarray(arr[i]))\r\n```\r\n\r\ndoes not actually copy the data in the Awkward Array, so it's safe to use even if `arr[i]` is big. (Note that this is `np.asarray`, not `np.array`, as @agoose77 pointed out.) It has to create an [ArrayModel struct](https://github.com/numba/numba/blob/f4b6fe0124fa7734ba2bb28c7685db5b6306c486/numba/core/datamodel/models.py#L871-L884) on the stack, which is something like 64 bytes (depending on how many dimensions the array has), but that's a constant-size metadata specifying pointers to the object, data type, `shape` and `strides`, not the variable-sized data _in_ the array. It should be _relatively_ fast. (I can't put a number on it.)\r\n\r\nIt would be nice if you didn't have to do the explicit cast at all; that's open issue #509.\r\n\r\nBut as @agoose77 pointed out, vectorized functions inside of the JIT-compiled context aren't advantageous as they are in pure Python. You can sometimes do better with an explicitly defined function because LLVM will recognize that you don't need the full features of an `np.ndarray` object, just the ability to iterate over the contents, and it might be able to optimize that down into fewer instructions. I haven't tested it, but this might be a faster `nb_ak_mean`:\r\n\r\n```python\r\n@nb.njit\r\ndef nb_ak_mean(arr):\r\n  means = np.zeros(shape=len(arr))\r\n  for i, x in enumerate(arr):\r\n    for j in range(len(x)):\r\n      means[i] += x[j]\r\n    means[i] /= len(x)   # maybe you'll get some NaNs from division-by-zero\r\n  return means\r\n```\r\n\r\nRefactoring this into helper functions (one iterates, the other computes the mean) shouldn't be any slower because `@nb.njit` functions that call `@nb.njit` functions skip the boxing/unboxing steps and may even be inlined by LLVM. Doing it in one function vs. multiple functions is not a performance/readability tradeoff, but functions that require an ArrayModel struct like `np.mean` do have to do the extra work of making that struct, small as it may be.",
        "createdAt":"2022-10-24T15:31:02Z",
        "number":3952453
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"> The rest of this difference probably stems from the use of `np.array()` instead of `np.asarray()`. In Numba's case, we have a faster implementation of array conversion for `asarray` than `array`; `asarray` only handles 1D contiguous arrays, and directly interprets them as a numba `Array`, whereas the `np.array` function performs a copy (it can handle >1D arrays, and these can be non-contiguous in Awkward).\r\n\r\nI missed this and it's absolutely true: `np.array` is a much slower implementation than `np.asarray`; it's more general and does more runtime conversion. What I was saying applies to `np.asarray`. In fact, I'm going to change history and fix the text in my comment.\r\n\r\n--------------\r\n\r\nFor reference, these are the `np.array` and `np.asarray` implementations. The `np.array` builds up a lot of runtime code to check the sizes of iterables, to see if they are regular enough to be converted into an n-dimensional NumPy array, whereas the `np.asarray` implementation takes compile-time information and only applies to regular-typed arrays. (In fact, it should someday be expanded to regular n-dimensional arrays; this `np.asarray` only works on 1D arrays.)\r\n\r\nhttps://github.com/scikit-hep/awkward/blob/2bdd11426b83ff52c6a3ecad4c64d1841663e9f9/src/awkward/_connect/numba/arrayview.py#L856-L1002\r\n\r\nThis Python code runs during Numba's compilation phase.\r\n\r\n* `overload_np_array` creates Python code in strings and evaluates them as a new Numba function to check the size of the Awkward iterable\u2014whatever its type may be\u2014allocates a new NumPy array, and copies the data into that new array. That's expensive!\r\n* `type_asarray` checks the type information of the Awkward Array: it can't be a variable-length list\r\n* `lower_asarray` generates LLVM bytecode that only builds the ~64 byte metadata for the lowered NumPy array.",
        "createdAt":"2022-10-24T16:09:06Z",
        "number":3952751
       },
       {
        "author":{
         "login":"fspinna"
        },
        "body":"Thanks a lot for the details. Very interesting indeed!",
        "createdAt":"2022-10-24T20:27:08Z",
        "number":3954639
       }
      ],
      "totalCount":4
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-10-24T12:45:26Z",
  "number":1824,
  "title":"njit to speed up functions on awkward arrays",
  "url":"https://github.com/scikit-hep/awkward/discussions/1824"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"This is the first (pre) release of Awkward 2, after `main` and `main-v1` have diverged (considerably). There is no `awkward._v2` submodule in this release. Any instances of\r\n\r\n```python\r\nimport awkward._v2 as ak\r\n```\r\n\r\nneed to be changed to\r\n\r\n```python\r\nimport awkward as ak\r\n```\r\n\r\nto use this release. Also, there is no guarantee that Awkward 1 code will work in this release; it is a (slightly) backward incompatible major release, but not final until 2.0.0 (no \"rc\" number).\r\n\r\n## New features\r\n\r\n* feat: replace v1 with v2 (preserve-history) by @agoose77 in https://github.com/scikit-hep/awkward/pull/1721\r\n* feat: add _v2 proxy module by @agoose77 in https://github.com/scikit-hep/awkward/pull/1730\r\n* feat: make ak.from_json with schema ignore unspecified fields (instead of error). by @jpivarski in https://github.com/scikit-hep/awkward/pull/1770\r\n* feat: add new `\"fields\"` key to `RecordForm` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1773\r\n* feat: support scalars in `TypeTracer` operations by @agoose77 in https://github.com/scikit-hep/awkward/pull/1774\r\n* feat: add `RegularArray._reduce_next` implementation by @agoose77 in https://github.com/scikit-hep/awkward/pull/1811\r\n* feat: Array Builder in Numba - add missing methods by @ianna in https://github.com/scikit-hep/awkward/pull/1677\r\n* feat: to/from RDataFrame support for bool type arrays by @ianna in https://github.com/scikit-hep/awkward/pull/1829\r\n* feat: preserve `None` in `np.ravel()` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1826\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: ensure `behavior` is taken from wrapped array in `ak.Array` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1715\r\n* refactor!: disable iteration of records by @agoose77 in https://github.com/scikit-hep/awkward/pull/1725\r\n* fix: Record is not in contents, but in record by @ianna in https://github.com/scikit-hep/awkward/pull/1739\r\n* fix: use proper lengths in `ByteMaskedArray.mergemany` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1749\r\n* fix: don't assume trailing `.` for module name in `is_XXX_buffer` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1743\r\n* fix: kernel name in `IndexedArray` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1754\r\n* fix: use raise with `ak._errors.wrap_error` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1760\r\n* fix: use dict representation of Form in pickling by @agoose77 in https://github.com/scikit-hep/awkward/pull/1768\r\n* fix: ignore unknown JAX \"buffer\" types & fix nplike mixing by @agoose77 in https://github.com/scikit-hep/awkward/pull/1769\r\n* fix: propagate mask through `from_numpy` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1777\r\n* fix: use the flag for non-persistence by @ianna in https://github.com/scikit-hep/awkward/pull/1781\r\n* fix: simplify `ListOffsetArray_reduce_nonlocal_outstartsstops` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1796\r\n* fix: allow empty sublists in ak.run_lengths by @agoose77 in https://github.com/scikit-hep/awkward/pull/1795\r\n* fix: `ak.prod` for booleans had been incorrectly casting output by @jpivarski in https://github.com/scikit-hep/awkward/pull/1827\r\n* fix: visit all layout nodes with `ak.fill_none(..., axis=None)` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1830\r\n* perf: only stringify function arguments if nplike is delayed. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1825\r\n\r\n## Other\r\n\r\n* refactor: replace Behavior with collections.ChainMap by @agoose77 in https://github.com/scikit-hep/awkward/pull/1731\r\n* refactor: rename `ak.nplike` \u2192 `ak.nplikes` & `nplikes.of` \u2192`nplikes.nplike_of` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1744\r\n* refactor: split error-handling code from `_util` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1752\r\n* refactor: drop unused code from Form, rename serialisation methods by @agoose77 in https://github.com/scikit-hep/awkward/pull/1748\r\n* refactor: reduce qualified names of Content, Form, Type subclasses by @jpivarski in https://github.com/scikit-hep/awkward/pull/1757\r\n* refactor: make _v2 facade a static module. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1758\r\n* refactor: move `is_XXX_buffer` into respective nplikes by @agoose77 in https://github.com/scikit-hep/awkward/pull/1761\r\n* refactor: cleanup `_connect` submodule by @agoose77 in https://github.com/scikit-hep/awkward/pull/1763\r\n* test: ensure that parameters are kept for `NumpyArray`s under ufunc operations by @agoose77 in https://github.com/scikit-hep/awkward/pull/1712\r\n* test: remove `ak.behavior` usage in tests by @agoose77 in https://github.com/scikit-hep/awkward/pull/1716\r\n* test: use a temp dir for writting out the test files by @ianna in https://github.com/scikit-hep/awkward/pull/1804\r\n* build(deps): bump amannn/action-semantic-pull-request from 4 to 5.0.1 by @dependabot in https://github.com/scikit-hep/awkward/pull/1788\r\n* build(deps): bump docker/setup-qemu-action from 2.0.0 to 2.1.0 by @dependabot in https://github.com/scikit-hep/awkward/pull/1783\r\n* build(deps): bump amannn/action-semantic-pull-request from 5.0.1 to 5.0.2 by @dependabot in https://github.com/scikit-hep/awkward/pull/1798\r\n* build(deps): bump pypa/cibuildwheel from 2.10.2 to 2.11.1 by @dependabot in https://github.com/scikit-hep/awkward/pull/1787\r\n* ci: bump pypa/cibuildwheel from 2.10.0 to 2.10.2 by @dependabot in https://github.com/scikit-hep/awkward/pull/1740\r\n* ci: run Codecov for commits to main by @agoose77 in https://github.com/scikit-hep/awkward/pull/1751\r\n* ci: enable versions for built docs by @agoose77 in https://github.com/scikit-hep/awkward/pull/1812\r\n* ci: use custom PR message action by @agoose77 in https://github.com/scikit-hep/awkward/pull/1816\r\n* docs: update docstring of `ak.with_name` to reflect `None` interpretation by @agoose77 in https://github.com/scikit-hep/awkward/pull/1665\r\n* docs: merge new docs into main by @agoose77 in https://github.com/scikit-hep/awkward/pull/1776\r\n* docs: remove `vector_design_prototype`'s reference by @Saransh-cpp in https://github.com/scikit-hep/awkward/pull/1803\r\n* docs: fix redirects by @agoose77 in https://github.com/scikit-hep/awkward/pull/1815\r\n* docs: fix kernel generation by @agoose77 in https://github.com/scikit-hep/awkward/pull/1800\r\n* docs: enable plausible.js analytics by @agoose77 in https://github.com/scikit-hep/awkward/pull/1822\r\n* docs: add vertical margin between badges by @agoose77 in https://github.com/scikit-hep/awkward/pull/1832\r\n* docs: add notes on \"declaring `ArrayBuilder` type\" by @agoose77 in https://github.com/scikit-hep/awkward/pull/1831\r\n* chore: update CITATION.cff by @agoose77 in https://github.com/scikit-hep/awkward/pull/1717\r\n* chore: use codecov action by @agoose77 in https://github.com/scikit-hep/awkward/pull/1722\r\n* chore: add isort by @henryiii in https://github.com/scikit-hep/awkward/pull/1666\r\n* chore: set `VERSION_INFO` contents to `2.0.0rc1` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1723\r\n* chore: remove sre_parse dependency from Lark-generated parser. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1759\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1733\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1779\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1801\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v1.10.1...v2.0.0rc1\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.0rc1'>Version 2.0.0rc1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-10-25T20:51:05Z",
  "number":1833,
  "title":"Version 2.0.0rc1",
  "url":"https://github.com/scikit-hep/awkward/discussions/1833"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"This is the first release in which the C++ code has been moved into a separate package, `awkward-cpp`. The pure Python `awkward` package is version-locked to version `1` of `awkward-cpp`.\r\n\r\n## New features\r\n\r\n* feat: better mask_identity defaults for reducer-like functions. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1873\r\n* feat: np.matmul should raise NotImplementedError until it gets implemented by @ioanaif in https://github.com/scikit-hep/awkward/pull/1877\r\n* feat: more concise pretty-print. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1861\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: ensure that behaviors are propagated through `ak.XXX` operations by @agoose77 in https://github.com/scikit-hep/awkward/pull/1869\r\n* fix: try to fix long long to double by @ianna in https://github.com/scikit-hep/awkward/pull/1860\r\n* fix: set appropriate error message for decimal types in arrow by @agoose77 in https://github.com/scikit-hep/awkward/pull/1871\r\n\r\n## Other\r\n\r\n* wip: run deploy for PR by @agoose77 in https://github.com/scikit-hep/awkward/pull/1891\r\n* docs: Fixed typo in ak.argsort page by @Laurits7 in https://github.com/scikit-hep/awkward/pull/1880\r\n* docs: add Laurits7 as a contributor for doc by @allcontributors in https://github.com/scikit-hep/awkward/pull/1881\r\n* ci: use Python consistently by @agoose77 in https://github.com/scikit-hep/awkward/pull/1892\r\n* ci: set tokens & disable release for C++ [skip ci] by @agoose77 in https://github.com/scikit-hep/awkward/pull/1894\r\n* ci: simplify hash computation [skip ci] by @agoose77 in https://github.com/scikit-hep/awkward/pull/1897\r\n* ci: add option to manually release awkward on PyPI by @agoose77 in https://github.com/scikit-hep/awkward/pull/1898\r\n* ci: set constant `$SOURCE_DATE_EPOCH` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1900\r\n* chore: rename is_XXXType to is_XXX (lowercase) by @agoose77 in https://github.com/scikit-hep/awkward/pull/1876\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1878\r\n* chore: add README.md to awkward-cpp by @agoose77 in https://github.com/scikit-hep/awkward/pull/1895\r\n\r\n## New Contributors\r\n* @Laurits7 made their first contribution in https://github.com/scikit-hep/awkward/pull/1880\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.0.0rc3...v2.0.0rc4\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.0rc4'>Version 2.0.0rc4</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-11-19T19:12:14Z",
  "number":1903,
  "title":"Version 2.0.0rc4",
  "url":"https://github.com/scikit-hep/awkward/discussions/1903"
 },
 {
  "author":{
   "login":"maplexgitx0302"
  },
  "body":"Dear experts,\r\n\r\nI have a basic question, but I couldn't find the solution. I think there must be some easy way to do this, so my question is: \r\n\r\nSuppose I have an ```ak.Array```\r\n```python\r\nx1 = ak.Array([\r\n    [1,2,3],\r\n    [4,5,6,7],\r\n    [8,9],\r\n])\r\n\r\nx2 = ak.Array([\r\n    [[1,2], [3,4], [5,6]],\r\n    [[1,1], [2,2], [3,3], [4,4]],\r\n    [[1,2,3], [4,5,6]],\r\n])\r\n```\r\nIf I also have another list (or array) that have the same length to ```len(x)```, let's say\r\n```python\r\ny = [2,0,1] # the column index I want to choose for each row\r\n# i.e., choose column 2 from row 0, choose column 0 from row 1, choose column 1 from row 2\r\n```\r\nIs there any way to select data so that the output would give me\r\n```\r\nresult_from_x1 = ak.Array([3,4,9])\r\nresult_from_x2 = ak.Array([[5,6], [1,1], [4,5,6]])\r\n```\r\nI have tried something like ```x[:, y]``` or ```x[ak.zip([range(len(x), y)])]```, but somehow I couldn't get the desired result :( I think I might miss somewhere in the document. Thank you very much!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Hi @r08222011 \r\n\r\nWhat you're describing here is an advanced index. NumPy has its own concept of this (that Awkward supports), e.g.\r\n```python\r\nx1[[0, 1,2], [2, 0, 1]]\r\n```\r\nwhere each array in the subscript operation picks out a component from the corresponding dimension. In other words, \r\n```python\r\n[[0, 1,2], [2, 0, 1]]\r\n```\r\nis interpreted as a one dimensional array of `N` dimensional tuples:\r\n```python\r\n[(0, 2), (1, 0), (2, 1)] \r\n```\r\nto build the result.\r\n\r\nHowever, this means that the result is *always* one dimensional. \r\n\r\nAwkward also has an additional indexing mode, if you pass in a ragged array which has the same number of dimensions as your `x1` array (i.e. with length-1 lists in the final dimension), it will pick out the values you're interested in, albeit with an additional dimension\r\n\r\n```python\r\nix = ak.Array([\r\n    [2],\r\n    [0],\r\n    [1]\r\n])\r\nx1[ix]\r\n```\r\n\r\nYou can simply slice the result:\r\n\r\n```python\r\nix = ak.Array([\r\n    [2],\r\n    [0],\r\n    [1]\r\n])\r\nx1[ix][:, 0]\r\n```",
     "createdAt":"2022-11-22T16:02:26Z",
     "number":4207888,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Thanks, @agoose77! This is a more complete explanation of the same thing I was trying:\r\n\r\n```python\r\n>>> x1[[[2], [0], [1]]][:, 0]\r\n<Array [3, 4, 9] type='3 * int64'>\r\n\r\n>>> x2[[[2], [0], [1]]][:, 0]\r\n<Array [[5, 6], [1, 1], [4, 5, 6]] type='3 * var * int64'>\r\n```",
        "createdAt":"2022-11-22T16:05:09Z",
        "number":4207923
       },
       {
        "author":{
         "login":"maplexgitx0302"
        },
        "body":"@agoose77 and @jpivarski , thank you for your reply! These explanations are really helpful and I understand now :) Thank you very much and have a great day!",
        "createdAt":"2022-11-22T16:09:07Z",
        "number":4207960
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Note that this differs to NumPy (our Awkward indexing \"wins out\"). Even if `ix` is regular, this works. @jpivarski I don't recall that always being the rules, but we've discussed this so many times that I've lost track of things. Are we happy that `ix` does not need to be ragged (apparently)?",
        "createdAt":"2022-11-22T16:09:30Z",
        "number":4207964
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-11-22T15:52:08Z",
  "number":1906,
  "title":"Select each row of data with different column indices",
  "url":"https://github.com/scikit-hep/awkward/discussions/1906"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"This is _one of_ the last pre-releases before 2.0.0. Most of the focus now is on last-minute API changes; the API can't change without a deprecation cycle after 2.0.0.\r\n\r\n## New features\r\n\r\n* feat: made 'very optional' arguments keyword-only by @jpivarski in https://github.com/scikit-hep/awkward/pull/1905\r\n* feat: rename `toXXX` methods by @agoose77 in https://github.com/scikit-hep/awkward/pull/1919\r\n* feat: replace nplike with backend in `to_buffers` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1942\r\n* feat: add rich IPython display hook to array/record by @agoose77 in https://github.com/scikit-hep/awkward/pull/1938\r\n* feat: move long-range metadata checks to constructors by @jpivarski in https://github.com/scikit-hep/awkward/pull/1939\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: refactor '_nextcarry-outindex' to have the same signature everywhere by @ioanaif in https://github.com/scikit-hep/awkward/pull/1911\r\n* fix: ignore .nox by @ianna in https://github.com/scikit-hep/awkward/pull/1912\r\n* refactor!: make `Content` initialisers take `nplike`, `parameters` as keyword by @agoose77 in https://github.com/scikit-hep/awkward/pull/1921\r\n* fix: backends should have defaults for user-facing operations by @agoose77 in https://github.com/scikit-hep/awkward/pull/1940\r\n* fix: consolidate regular indexing by @agoose77 in https://github.com/scikit-hep/awkward/pull/1943\r\n* fix: IndexedArray.project() preserves parameters. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1949\r\n* fix: preserve strings in `ak.ravel` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1934\r\n* fix: UnionArray.simplified preserves parameters. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1950\r\n\r\n## Other\r\n\r\n* refactor: introduce `backend` as a higher abstraction than `nplike` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1922\r\n* refactor: replace `simplify_optiontype` and `simplify_uniontype` with `simplified` classmethod by @jpivarski in https://github.com/scikit-hep/awkward/pull/1928\r\n* refactor: clarify input types for user overloads by @agoose77 in https://github.com/scikit-hep/awkward/pull/1951\r\n* refactor: removed the 'ak._util.extra' argument helper. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1955\r\n* test: add @ioana's tests of to_arraylib. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1908\r\n* test: cover the failure modes of unflatten by @agoose77 in https://github.com/scikit-hep/awkward/pull/1930\r\n* docs: fix broken build by @agoose77 in https://github.com/scikit-hep/awkward/pull/1907\r\n* docs: cleanup documentation by @agoose77 in https://github.com/scikit-hep/awkward/pull/1946\r\n* docs: fix shorthand by @agoose77 in https://github.com/scikit-hep/awkward/pull/1952\r\n* docs: retested and modernized all the docstrings in highlevel.py. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1954\r\n* chore: fix NumPy bound & noxfile installs by @agoose77 in https://github.com/scikit-hep/awkward/pull/1913\r\n* chore: run `yesqa` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1915\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1920\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1953\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.0.0rc4...v2.0.0rc5\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.0rc5'>Version 2.0.0rc5</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-06T01:36:58Z",
  "number":1958,
  "title":"Version 2.0.0rc5",
  "url":"https://github.com/scikit-hep/awkward/discussions/1958"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"The main purpose of this release is to fix Uproot.\r\n\r\n## New features\r\n\r\n* feat: ak.from_rdataframe should accept a single string 'columns'. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1956\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: `__array__ = 'sorted_map'` should only be allowed on RecordArrays, not lists. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1959\r\n* fix: unknown type column by @ianna in https://github.com/scikit-hep/awkward/pull/1960\r\n* fix: add IPython as Sphinx extension by @agoose77 in https://github.com/scikit-hep/awkward/pull/1966\r\n\r\n## Other\r\n\r\n_(none!)_\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.0.0rc5...v2.0.0rc6\r\n\r\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.0rc6'>Version 2.0.0rc6</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-06T20:01:05Z",
  "number":1967,
  "title":"Version 2.0.0rc6",
  "url":"https://github.com/scikit-hep/awkward/discussions/1967"
 },
 {
  "author":{
   "login":"agoose77"
  },
  "body":"This discussion is inspired by https://github.com/scikit-hep/awkward/pull/1968, but more generally is something I've been thinking about for a while.\r\n\r\nDisclaimer: I have a strong opinion here, but I don't think there's necessarily a clear objective solution, so whatever comes out of this discussion is fine-by-me\u2122\r\n\r\n## The Problem\r\nOur current API strategy for the `ak.contents.Content` objects sees us breaking convention, and using private attributes of other classes, e.g. `layout._backend` or `layout._getitem_next()`. This brings with it some problems:\r\n1. Conventionally, private attributes are not descriptors e.g. `@property` is not conventionally added to private methods. Therefore, either we stick with this convention and cannot use these for our private impl, or we need to break convention.\r\n2. \"Actually private\" state is harder to see. It's hard to scope APIs when we have no clear scoping rules. We would need to resort to name mangling if we wanted to make this clear, and then add \"private\" descriptors, which incurs the above problem. \r\n3. We use private attributes as fast-paths (i.e. avoid the public property), which means private is not private.\r\n\r\nI understand this convention to be solving the following problem: _we don't want external developers to rely on the same set of API methods that we use to implement Awkward_. We want a smaller development burden by reducing scope.\r\n\r\nI could write a lot of text here, but the short version is that it feels like we're compromising our _internal_ API for the sake of an external one. I do feel like the public-private conventions outlined above have value, both for readability and coupling. Whilst we have highly-coupled layouts (all layouts need to maintain some awareness of the others), this dependence is increasingly limited to a number of well-defined interfaces such as `simplified` and `starts`, `stops`, etc. Although coupling is usually bad from a \"can I introduce a new type here\", it's also bad from a \"how hard is it to change design assumptions\". We can't avoid the former (it's a design decision, and there's really no other way to do the is in OOP), but we can address the latter.\r\n\r\nUsually, one doesn't have to make this compromise; either anything public can be called by users, or a private *API* is distinguished from the public interface (e.g. our `NumpyLike` object, if it weren't exposed via `backend.nplike`). We can't easily do the latter, because the layout objects _are_ a fundamental type, and there's an overwhelming disadvantage to wrapping/unwrapping them in some other protected API for the sake of reducing API scope. We have elected not to do the former, for reasons that I do think have merit.\r\n\r\n## The Solution(s)?\r\n### Don't hide things from external developers\r\nIf it were exclusively my decision for a personal project, I'd favour the internal API following the \"usual\" conventions, and use e.g. documentation to make it clear to external developers that they should only use certain API methods. In my experience, developers will ignore a `_` prefix if the method works; they'll just more readily accept a breakage down the line. It would be nice if we could e.g. move all of the `_reduce_next` functions to `reduce_next_`, given that they're called by other classes. Whereas, `ListOffsetArray._offsets` should not be writable by other classes. \r\n\r\n### Use a custom naming convention\r\nI think the outstanding argument against \"Don't hide things from external developers\" is the one outlined above: we don't want to maintain the entire layout API for N versions without strong motivation. Therefore, I wonder if we could consider defining our own \"private-public\" convention. One option is to explicitly enumerate the public methods in the documentation. This is the most elegant solution from a code perspective, but I suspect a valid criticism is that some developers will never open the documentation, and it's unreasonable to define that as \"wrong\". Thus, my compromise would be to define a naming convention for internal \"public\" attributes of layout classes.\r\n\r\n```python\r\nclass ListOffsetArray:\r\n\r\n    @property\r\n    def offsets(self):\r\n        return self._offsets\r\n\r\n\r\n    def internal_reduce_next(self):\r\n        ...\r\n\r\n    def internal_getitem_at(self):\r\n        ...\r\n```\r\n\r\nI don't *love* this - it's slightly ugly. But, it makes it clear that these are public methods of the layout, just \"internal\" to Awkward.\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"We currently have three formal levels of scoping, defined by two boundaries:\r\n\r\n  1. public, high-level: for data analysts and interactive users\r\n  2. public, low-level: for downstream developers\r\n  3. internal: for our own use\r\n\r\nThe boundary between 2 and 3 is the Python convention of leading underscores: if a module/function/class/method is not hidden behind an underscore at some level, we can assume that external users or developers will start using it and depending on it. If we want to change such a thing, we can either declare the old behavior a bug (never intended) or go through a deprecation cycle.\r\n\r\nThe boundary between 1 and 2 is less sharp: `ak.Array`, `ak.Record`, `ak.ArrayBuilder`, the `ak.*` functions, slicing, and ufuncs are high-level; anything behind the `layout` property and names in submodules (not the top `ak` namespace) are low-level but still public.\r\n\r\nIt's important for the boundary between 2 and 3 to be enforced and based on a rule that we don't make up: we can't assume that anybody reads a warning we put in the documentation before they start depending on our public API. Even if they're contentious and _intend_ to read all the disclaimers, discovery is hard and they might not find it. We can assume that developers are aware of Python conventions (because otherwise, what can we assume?).\r\n\r\nThe boundary between 1 and 2 is less important; it's more for organizing user experience, so that we can give powerful tools to developers and also not overwhelm interactive users.\r\n\r\nI agree that it would be useful to go further and subdivide level 3. Encapsulation is useful for programming\u2014it keeps a codebase from becoming spaghetti. Both are important, but encapsulation within a codebase is qualitatively different from encapsulation between codebases maintained by people who might not know each other. Within a codebase, we can create a convention and follow it, and even if we get PRs from new contributors, they'll be reviewed by people who know the conventions and we can make sure that the code adheres to them before it gets merged.\r\n\r\nDependencies between codebases, however, are much stickier: we don't review how others use our code, or even know about it (though I'm trying to address that with GitHub searches). They might end up building huge castles on it that would be hard to fix if we change anything in our API. And even when a fix is possible, version X of Awkward wouldn't work with version Y of their package (where Y is before the fix).\r\n\r\nSo yes, we should have good encapsulation between the organelles of our codebase, but it is more important to maintain good encapsulation between cells\u2014different codebases/packages. The consequences of those mistakes are more dire. Internally refactoring our own codebase may be difficult, but we don't need to get anyone's permission to do it; we can do it at any time. Refactoring among codebases/packages requires coordination among people who might not even be in communication (yet), and we might have conflicting requirements that can't even be satisfied without someone giving up on their goals. The cell membranes are a different order of thing than the organelle boundaries because we don't know what other cells we're going to meet in our travels; we always know what organelles we have internally.\r\n\r\n(I've rambled; sorry!)\r\n\r\nAbout organizing within our codebase, breaking up level 3 into multiple levels, we already use some encapsulation practices. By convention (so far, and only 95% followed), a `Content` instance's method can only access a `Content` instance's attributes if\r\n\r\n  * it is the same object: `self._whatever`\r\n  * it is another object of the same class, after verifying that that is so: `if isinstance(other, MyClass): other._whatever` (this will make it hard to change `Content` attributes, but we don't plan to)\r\n  * a recursive function X is passing down calls to more X: `def _something(self, *args): self._content._something(*args)`\r\n  * the attribute is common to all `Content` subclasses: `other._parameters`, `other._backend`\r\n  * similarly for `Form` subclasses and `Type` subclasses, within their inheritance trees\r\n\r\nAlso (about 95% followed), _all_ data attributes are private (start with an underscore) with read-only public properties. Those public properties (no underscore) are part of our guaranteed API (levels 1 & 2).\r\n\r\nI don't think we have any private _properties_, since we also prefer plain argument lists for our internal functions, without defaults, as we would have in the public API. A property is just a method without \"`()`\" in its call syntax; there's no point in syntactic sugar for internal functions: `self._something()` is as easy as `self._something` and it's easier for us to maintain if all internal calls have the same form.\r\n\r\nWith the above rules, a deep call into some object's content of content of content would look like this:\r\n\r\n```python\r\nself._content.content.content   # only the first gets an underscore\r\n```\r\n\r\nand you see that pattern quite a bit.\r\n\r\nOne thing that we use a lot that violates the normal Python notion of public/private is that our `ak.*` implementations, which are not part of the `Content` hierarchy at all, call private `Content` methods:\r\n\r\n```python\r\ndef ak_whatever_impl(array, *args):\r\n    layout = ak.operations.ak_to_layout._impl(array, ...)\r\n\r\n    out = layout._start_recursion(*args)\r\n\r\n    return ak._util.wrap(out, highlevel, behavior)\r\n```\r\n\r\nThe `_start_recursion` method is private/underscored/hidden so that it's not part of our guaranteed API (levels 1 & 2) and thus we can change it at any time. It is part of our internal protocol: every `Content` subclass is guaranteed to have this method, and that is a contract with ourselves, not with the outside world. We can change our internal contract in a PR, including one submitted by an outside contributor, and not have to do a deprecation cycle to inform users and downstream developers about the change.\r\n\r\nPython's standard convention has only one way of making a public-private distinction (the underscore), and we've already used it up in making that more-important distinction: communicating \"do not use\" to users and downstream developers. We'll need another marker if we're going to make a distinction between `Content` methods that `ak.*` functions can call and `Content` methods that only the `Content` can call on itself. Double-underscore is not what we want because it has a \"private vs protected\" meaning within Python.\r\n\r\nPerhaps they could be free functions in a hidden submodule? We have no qualms about calling functions in hidden submodules such as `_util`, `_broadcasting`, etc. The usual pattern is one function that starts the recursion\u2014setting everything up, not reentrant, followed by `Content._whatever` that can call any other `Content._whatever` until it reaches the leaves of the tree. The distinction between `_start_whatever` and `_whatever` is currently made only in the names; perhaps the `_start_whatever`s could be functions in a hidden submodule and the `_whatever`s could remain methods. That would introduce a useful distinction in addition to the \"level 3 internal\" vs \"level 4 internal\".\r\n\r\nIt's probably clear from the above that I disagree with solution 1, \"Don't hide things from external developers.\" Whatever we do with our internal organelles, the cell membranes are more important and must be maintained. You also suggested listing the public API in documentation, but we have to assume that users and downstream developers\u2014even in good faith\u2014won't find the relevant documentation. However we signify the boundary between \"level 2\" and \"level 3,\" it has to be based on rules that are familiar to the whole Python community, not a rule we make up and post somewhere. The only public/private rule the Python community has is the underscore rule.\r\n\r\nAs for solution 2, \"Use a custom naming convention,\" yes, it would have to be something we impose on ourselves like this, not something we impose on the world, but we have more options than just names. The hidden submodule is a thing that we can do that has more structure than a name.\r\n\r\nBut even if we go with names, how about reversing the default, so that \"level 4 private-to-class\" are just underscores but \"level 3 private-to-codebase\" have special prefixes: underscore + some letter? Like \"`_a_`\" for Awkward Array?\r\n\r\n```python\r\nclass BitMaskedArray(Content):\r\n    def _something(self, ...):\r\n        ...\r\n\r\n    def _a_something(self, ...):\r\n        ...\r\n```\r\n\r\nwhere `_something` is \"level 4 private-to-class\" and can only be used by the class, while `_a_something` is \"level 3 private-to-codebase\" and can be used anywhere in Awkward Array, but not outside of Awkward Array. Outside users and developers will know to avoid it because it starts with an underscore.\r\n\r\nLet's not go crazy introducing many levels of privateness, but I see the value of having a level 3/level 4 distinction. Incidentally, Scala has this concept of private-to-class, private-to-module/namespace, and one of those nested namespaces is your whole codebase, so we're not the first to need something like this. (Though, of course, Scala actually enforces that encapsulation at compile-time, but we live in Python. Maybe, if we're happy with whatever rule we come up with, we can enforce it as a flake8 extension. There's already a placeholder for that, enforcing the AK001 exception-raising rule.)",
     "createdAt":"2022-12-07T17:52:18Z",
     "number":4335724,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Long reply, thank you for your thoughtful response.\r\n\r\nOne of my concerns here is that doing nothing compromises our ability to encapsulate, but also that any solution we come up with might also do that. \r\n\r\nWith that in mind, you've provided a comprehensive solution in #1972. This is particularly useful in that you enumerate the near-final public API for our content classes (and caught some bugs along the way).\r\n\r\nIntroducing a formal convention for a restricted set of `_` prefixed names moves the burden of remembering these onto us, the developers. The benefit of the Python convention is that seeing `XXX._YYY` is _always_ a mistake unless `XXX` is `self` or `YYY` is a magic method. My reservation with making this our solution is that now we have no visual discriminator of what is correct and what is encapsulation-violating.\r\n\r\nMemorising L3 rules also works better with L3 code that is a protocol, e.g. `_getitem_next`, than something that is unique to a particular class/set of classes, e.g. if we have `isinstance(self._content, ListOffsetArray)` followed by some code that expects a public member of `ListOffsetArray`. I can't think of any examples, but I wouldn't want to discount this case up-front. \r\n\r\nMy contention is that a non-prefixed naming convention should be fairly obvious. My original suggestion was e.g.\r\n```python\r\nclass Content:\r\n    ....\r\n\r\n    def private_getitem_range(self, where):\r\n        ...\r\n\r\n```\r\nIf we don't think the word \"private\" is sufficient, we could also use\r\n```python\r\nclass Content:\r\n    ....\r\n\r\n    def restricted_getitem_range(self, where):\r\n        ...\r\n\r\n```\r\nor\r\n\r\n```python\r\nclass Content:\r\n    ....\r\n\r\n    def internal_getitem_range(self, where):\r\n        ...\r\n\r\n```\r\n\r\nIf these still don't feel strong enough, what about an inverse naming convention:\r\n\r\n```python\r\nclass Content:\r\n    ....\r\n\r\n    def _pub_getitem_range(self, where):\r\n        ...\r\n\r\n```\r\nwhere \"_pub\" denotes internal methods / attributes that non-self can call?\r\n\r\nIt does mean that we would have funky-looking descriptors:\r\n\r\n```python\r\nclass Content:\r\n    ....\r\n\r\n    @property\r\n    def _pub_backend(self) -> ak._backends.Backend:\r\n        ...\r\n\r\n```\r\n\r\nBut I also don't hate that.\r\n\r\nI started an example in #1975 \r\n\r\n> there's no point in syntactic sugar for internal functions\r\n\r\nI don't find myself agreeing with this point - I think syntactic sugar is nearly always an improvement. Properties make it possible to replace a trivial attribute with a computed value on a per-object basis, for example. Again, we do less of that because we don't accept arbitrary objects in many places of the layout handling code (a new place would be the backend system, which is powered by protocols rather than direct types).\r\n\r\nThat said, I don't think we need many L3 public properties, and in the code that we're designing this convention for, we are unlikely to need many properties; most of our existing properties are L2 public, e.g. `_starts` doesn't exist for some layouts, but `starts` _does_. For e.g. `ListOffsetArray` it computes `starts` from `_offsets`, whilst for `ListArray` it just returns `_starts`. If these properties were L3, that would be a problem that we'd want to tackle, but fortunately these are L2.",
        "createdAt":"2022-12-08T07:31:43Z",
        "number":4340626
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Even though I'm in favour of an in-class solution that makes it clear that an attribute is truly private, or internally private, I don't like the fact that it would warrant this kind of treatment for any class that we expose to users. I'm thinking particularly of `Backend` here. To stop users probing the `nplike` mechanism, we either need to make the `nplike` and `index_nplike` attributes L3, e.g. \r\n```python\r\nclass NumpyBackend:\r\n    @property\r\n    def _pub_nplike(self) -> Numpy:\r\n        return Numpy.instance()\r\n   ...\r\n```\r\nor we need to change the `Content` `api` so that `.backend` is an opaque object e.g. string. Then e.g. `RecordArray` would need to regularise its `backend` argument, which is not ideal - I'd prefer for the high level backend abstraction to remain at the high-level.",
        "createdAt":"2022-12-08T14:38:29Z",
        "number":4343980
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"After our awkward-uproot meeting, we discussed the problem-at-hand, and settled upon a solution. There is no perfect solution to this, so we're settling upon Python \"private\" meaning L3/L4. To distinguish between L3 and L4, we will use a set of rules outlined [here](https://github.com/scikit-hep/awkward/pull/1972#issuecomment-1343114659) which will be enforced by a linter.",
        "createdAt":"2022-12-08T20:37:09Z",
        "number":4346770
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"Further ideas were noted down here: https://github.com/scikit-hep/awkward/issues/2108",
        "createdAt":"2023-01-31T12:14:46Z",
        "number":4829330
       }
      ],
      "totalCount":4
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-12-07T11:42:28Z",
  "number":1969,
  "title":"Distinguishing public and public-internal APIs",
  "url":"https://github.com/scikit-hep/awkward/discussions/1969"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n_(none!)_\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: EmptyArray.is_numpy should be False. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1971\r\n* fix: add return_value='simplified' to ak.transform and revamp ak.firsts/ak.singletons by @jpivarski in https://github.com/scikit-hep/awkward/pull/1968\r\n* fix: don't try to support Awkward 1.x pickles. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1974\r\n* fix: ak_from_parquet by @ioanaif in https://github.com/scikit-hep/awkward/pull/1977\r\n* fix: ak.Record dict constructor should retain type. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1981\r\n\r\n## Other\r\n\r\n* refactor: rename `Form` to `form_cls` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1976\r\n* refactor: hide Content recursion entry points in `ak._do` submodule. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1972\r\n* docs: Jim's documentation touch-ups (API sidebar, obsolete kernels intro) by @jpivarski in https://github.com/scikit-hep/awkward/pull/1982\r\n* ci: build(deps): bump pypa/gh-action-pypi-publish from 1.5.1 to 1.6.1 by @dependabot in https://github.com/scikit-hep/awkward/pull/1948\r\n* ci: build(deps): bump pypa/cibuildwheel from 2.11.2 to 2.11.3 by @dependabot in https://github.com/scikit-hep/awkward/pull/1965\r\n* ci: build(deps): bump pypa/gh-action-pypi-publish from 1.6.1 to 1.6.4 by @dependabot in https://github.com/scikit-hep/awkward/pull/1970\r\n* chore: fix RTD configuration by @agoose77 in https://github.com/scikit-hep/awkward/pull/1979\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.0.0rc6...v2.0.0rc7\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.0rc7'>Version 2.0.0rc7</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-08T23:04:14Z",
  "number":1984,
  "title":"Version 2.0.0rc7",
  "url":"https://github.com/scikit-hep/awkward/discussions/1984"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"This will very likely be the last pre-release before the final 2.0.0 release. If all goes well, that will be six hours after now. (\"Now\" is 16:00 UTC, December 9, 2022, so the final release will likely be at 22:00 UTC.)\r\n\r\n## New features\r\n\r\n_(none!)_\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix!: always broadcast `with_field` assignments against existing array by @agoose77 in https://github.com/scikit-hep/awkward/pull/1962\r\n* fix: replace `axis_wrap_if_negative` with `maybe_posaxis`, simpler and more correct by @jpivarski in https://github.com/scikit-hep/awkward/pull/1986\r\n\r\n## Other\r\n\r\n* refactor: hide L3 API by @agoose77 in https://github.com/scikit-hep/awkward/pull/1983\r\n* docs: revamp README.md, CONTRIBUTING.md, and move 'papers and talks'. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1985\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.0.0rc7...v2.0.0rc8\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.0rc8'>Version 2.0.0rc8</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-09T16:00:55Z",
  "number":1989,
  "title":"Version 2.0.0rc8",
  "url":"https://github.com/scikit-hep/awkward/discussions/1989"
 },
 {
  "author":{
   "login":"dr-stringfellow"
  },
  "body":"Hi, I am using the following python:\r\n/cvmfs/sft.cern.ch/lcg/views/LCG_102b_cuda/x86_64-centos7-gcc8-opt/bin/python3\r\n\r\nand added the following libraries:\r\n\r\npip3 install -t locallibs/ uproot awkward fastjet\r\ncd locallibs/\r\nexport PYTHONPATH=${PWD}:${PYTHONPATH}\r\ncd -\r\n\r\nVersions:\r\npython 3.9.12, GCC 8.3.0\r\nawkward 2.0.0\r\nuproot 5.0.0\r\nfastjet 3.4.0.1\r\n\r\nHowever, I get the following error when using fastjet:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/work/submit/bmaier/abcnet/znunu_jets/test/OTPU/scripts/cluster_results.py\", line 203, in <module>\r\n    dict_dataset[dset] = _dict_data(sets[dset],njets=9)\r\n  File \"/work/submit/bmaier/abcnet/znunu_jets/test/OTPU/scripts/cluster_results.py\", line 192, in _dict_data\r\n    cluster = _cluster(jets)\r\n  File \"/work/submit/bmaier/abcnet/znunu_jets/test/OTPU/scripts/cluster_results.py\", line 181, in _cluster\r\n    cluster = fastjet.ClusterSequence(array, jetdef)\r\n  File \"/work/submit/bmaier/abcnet/znunu_jets/test/OTPU/scripts/locallibs/fastjet/__init__.py\", line 215, in __init__\r\n    fastjet._pyjet.AwkwardClusterSequence.__init__(\r\n  File \"/work/submit/bmaier/abcnet/znunu_jets/test/OTPU/scripts/locallibs/fastjet/_pyjet.py\", line 20, in __init__\r\n    self._jagedness = self._check_jaggedness(data)\r\n  File \"/work/submit/bmaier/abcnet/znunu_jets/test/OTPU/scripts/locallibs/fastjet/_pyjet.py\", line 45, in _check_jaggedness\r\n    if self._check_general_jaggedness(data) or self._check_listoffset(data):\r\n  File \"/work/submit/bmaier/abcnet/znunu_jets/test/OTPU/scripts/locallibs/fastjet/_pyjet.py\", line 160, in _check_general_jaggedness\r\n    ak.layout.IndexedArray64,\r\nAttributeError: module 'awkward' has no attribute 'layout'\r\n```\r\n\r\nDo you have any idea how I could solve this? \r\nThank you!\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"ianna"
     },
     "body":"Hi @dr-stringfellow. Thanks for reporting it. I think, you need a newer version of FastJet that has not been released yet. The error you observe comes from the fact that this version is using Awkward v1. I can see that this has already been fixed in the GitHub: https://github.com/scikit-hep/fastjet/blob/main/src/fastjet/_pyjet.py by @jpivarski in https://github.com/scikit-hep/fastjet/pull/156\r\n\r\n\r\n",
     "createdAt":"2022-12-14T10:28:51Z",
     "number":4398653,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ianna"
        },
        "body":"@dr-stringfellow - please, try the latest FastJet release https://github.com/scikit-hep/fastjet/releases/tag/v3.4.0.3 and let us know if there are any issues. Thanks!",
        "createdAt":"2022-12-20T15:07:48Z",
        "number":4459726
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-12-14T09:53:28Z",
  "number":2004,
  "title":"AttributeError: module 'awkward' has no attribute 'layout'",
  "url":"https://github.com/scikit-hep/awkward/discussions/2004"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n* feat: add `ak.without_field` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1963\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: missed a NumpyArray.raw call without an underscore. by @jpivarski in https://github.com/scikit-hep/awkward/pull/1993\r\n* fix: add `Record.copy` by @agoose77 in https://github.com/scikit-hep/awkward/pull/1996\r\n* fix: support empty record arrays in `ak.to_numpy` by @agoose77 in https://github.com/scikit-hep/awkward/pull/2012\r\n* fix: widen input support for `ak.type()` by @agoose77 in https://github.com/scikit-hep/awkward/pull/2009\r\n\r\n## Other\r\n\r\n* docs: restore branch preview by @agoose77 in https://github.com/scikit-hep/awkward/pull/1994\r\n* docs: add note in README.md about pip installing through git. by @jpivarski in https://github.com/scikit-hep/awkward/pull/2005\r\n* docs: redirect paths for user-guide by @agoose77 in https://github.com/scikit-hep/awkward/pull/2007\r\n* docs: remove mention of numexpr by @agoose77 in https://github.com/scikit-hep/awkward/pull/2011\r\n* docs: first pass on subset of user guide by @agoose77 in https://github.com/scikit-hep/awkward/pull/2010\r\n* ci: output linkcheck information by @agoose77 in https://github.com/scikit-hep/awkward/pull/1987\r\n* ci: wip for deployment on AWS by @agoose77 in https://github.com/scikit-hep/awkward/pull/2002\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/1999\r\n* chore: remove old `num` kernels by @agoose77 in https://github.com/scikit-hep/awkward/pull/1998\r\n* chore: add ABI version number to AwkwardForth by @jpivarski in https://github.com/scikit-hep/awkward/pull/2001\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.0.0...v2.0.1\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.1'>Version 2.0.1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-15T19:10:43Z",
  "number":2014,
  "title":"Version 2.0.1",
  "url":"https://github.com/scikit-hep/awkward/discussions/2014"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n* feat: add ak.drop_none() by @ioanaif in https://github.com/scikit-hep/awkward/pull/1904\r\n\r\n## Bug-fixes and performance\r\n\r\n_(none!)_\r\n\r\n## Other\r\n\r\n_(none!)_\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.0.1...v2.0.2\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.2'>Version 2.0.2</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-16T18:54:22Z",
  "number":2015,
  "title":"Version 2.0.2",
  "url":"https://github.com/scikit-hep/awkward/discussions/2015"
 },
 {
  "author":{
   "login":"ZhenxuanZhang-Jensen"
  },
  "body":"Hi experts, I want to add a new field in an already zipped jagged array. For example, if I zip 4D info into a muons object, then I can call pt,eta,phi,charge like this: `muons.Muon.pt`. However, if I want to add a new field such as 2*pt into this muons object, then I can't do this with muons.Muon['2pt'] = 2 * arrays['Muon_pt\"]. Is there anything I misunderstand or how can I add a new field in this jagged array? Could you please help me? thanks\r\n```\r\nmuons = ak.zip({\r\n    \"pt\": arrays[\"Muon_pt\"],\r\n    \"eta\": arrays[\"Muon_eta\"],\r\n    \"phi\": arrays[\"Muon_phi\"],\r\n    \"charge\": arrays[\"Muon_charge\"],\r\n})\r\n```",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"The syntax you are describing (`muons.Muon['2pt'] = 2 * arrays['Muon_pt\"]`) is not quite right for assigning to an array. It also doesn't match up with the `muons` array that you've defined above, so I will assume it's a typing mistake?\r\n\r\nIf you have an array `muons` defined as \r\n```python\r\nmuons = ak.zip({\r\n    \"pt\": arrays[\"Muon_pt\"],\r\n    \"eta\": arrays[\"Muon_eta\"],\r\n    \"phi\": arrays[\"Muon_phi\"],\r\n    \"charge\": arrays[\"Muon_charge\"],\r\n})\r\n```\r\nthen you can assign a new/existing field with \r\n```python\r\nmuons['pt'] = ...\r\n```\r\n\r\nwhere `...` is a Python expression.\r\n\r\nThis might fail if the expression cannot be broadcast against the existing array. ",
     "createdAt":"2022-12-19T11:20:27Z",
     "number":4447980,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Just to keep everything cross-linked, the question was also asked on https://stackoverflow.com/q/74846508/1623645.",
        "createdAt":"2022-12-19T17:58:35Z",
        "number":4451201
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Hi Angus, actually, I did type wrong, but not the `muons.Muon`, is the 'charge' array should change to 'mass' array. Since we can separate the 'object' and 'kinematic variables' automatically with an underline when we zip 4D info with awkward(not sure why yet). for example:\r\n```\r\nwith uproot.open(file, timeout = 1800) as f:\r\n tree = f[\"Events\"]\r\nmuons = tree.arrays(['Muon_pt','Muon_eta','Muon_phi','Muon_mass'], library = \"ak\", how = \"zip\")\r\n```\r\nthen the `muons` can call the pt,eta,phi,mass in this way automatically: `muons.Muon.pt`, `muons.Muon.eta`... For this reason, I don't know how to add a new field in this `muons.Muon`. But I found that if I assign it to a new value and reassign it to muons.Muon which can work like:\r\n```\r\nMuon = muons.Muon\r\nMuon['pt2'] = ...\r\nmuons.Muon = Muon\r\nmuons.Muon.pt2\r\n```\r\nIn this way, I can add this pt2 field in the `muons.Muon`.",
        "createdAt":"2022-12-21T07:56:45Z",
        "number":4465333
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"@ZhenxuanZhang-Jensen Oh, I learned something about `uproot`'s `zip` argument from this \u2014 it removes the common prefix of like-named ragged branches.\r\n\r\nBack to your question.\r\n\r\nYou cannot set the value of a field with `array.field = value`. In fact, if you already have a field with the name `field`, it will raise an exception:\r\n```pycon\r\n>>> import uproot\r\n>>> import skhep_testdata\r\n>>> import awkward as ak\r\n>>> arrays = uproot.open(skhep_testdata.data_path(\"uproot-HZZ.root\"))[\"events\"].arrays(how=\"zip\")\r\n>>> arrays.Photon = 2\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[29], line 1\r\n----> 1 arrays.Photon = 2\r\n\r\nFile ~/Git/awkward-1.0/src/awkward/highlevel.py:1160, in Array.__setattr__(self, name, value)\r\n   1158     super().__setattr__(name, value)\r\n   1159 elif name in self._layout.fields:\r\n-> 1160     raise ak._errors.wrap_error(\r\n   1161         AttributeError(\r\n   1162             \"fields cannot be set as attributes. use #__setitem__ or #ak.with_field\"\r\n   1163         )\r\n   1164     )\r\n   1165 else:\r\n   1166     raise ak._errors.wrap_error(\r\n   1167         AttributeError(\r\n   1168             \"only private attributes (started with an underscore) can be set on arrays\"\r\n   1169         )\r\n   1170     )\r\n\r\nAttributeError: fields cannot be set as attributes. use #__setitem__ or #ak.with_field\r\n```\r\n\r\nThe proper syntax for setting a field is to use the subscript operator, e.g. `array['x'] = ...`. However, for _nested_ fields, e.g. `arrays.Photon.Px`, you need to use multiple strings \r\n```python\r\narrays['Photon', 'Px'] = ...\r\n``` \r\nor tuple of strings--\r\n```python\r\narrays[('Photon', 'Px')] = ...\r\n```\r\n\r\nYou can't use `arrays.Photon['Px'] = ...` \u2014 see [the user guide](https://awkward-array.org/doc/main/user-guide/how-to-restructure-add-fields.html#using-array-x) for details as to why this doesn't work.\r\n\r\nSo, if `uproot` has given you an array with a type like\r\n```python\r\n2421 * {\r\n    Muon: var * {\r\n        pt: float32,\r\n        eta: float32,\r\n        phi: float32,\r\n        mass: float32\r\n    }\r\n}\r\n```\r\nSo, you just need to write\r\n```python\r\nmuons['Muon', 'pt2'] = 2 * muons.Muon.pt\r\n```",
        "createdAt":"2022-12-21T11:34:43Z",
        "number":4466865
       },
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Thank you so much! The answer looks very nice to me.",
        "createdAt":"2022-12-21T12:14:46Z",
        "number":4467134
       }
      ],
      "totalCount":4
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-12-19T04:14:52Z",
  "number":2017,
  "title":"how to add new field in a 'zip' jagged array",
  "url":"https://github.com/scikit-hep/awkward/discussions/2017"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n* feat: add data-touch reporting to the type-tracer. by @jpivarski in https://github.com/scikit-hep/awkward/pull/2027\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: extend TypeTracerArray with __eq__, __ne__, and __array_ufunc__. by @jpivarski in https://github.com/scikit-hep/awkward/pull/2021\r\n* fix: add support for Long64_t by @ianna in https://github.com/scikit-hep/awkward/pull/2023\r\n* fix: replace protocol with direct subclass by @agoose77 in https://github.com/scikit-hep/awkward/pull/2029\r\n* fix: support `UnknownLength` in `ak.types.ArrayType` by @agoose77 in https://github.com/scikit-hep/awkward/pull/2031\r\n* refactor!: use exclusively `axis=-1` reduction for `axis=None` by @agoose77 in https://github.com/scikit-hep/awkward/pull/2020\r\n\r\n## Other\r\n\r\n* refactor: add array comparison test helper by @agoose77 in https://github.com/scikit-hep/awkward/pull/2024\r\n* docs: add sitemap by @agoose77 in https://github.com/scikit-hep/awkward/pull/2026\r\n* ci: drop pages deployment by @agoose77 in https://github.com/scikit-hep/awkward/pull/2025\r\n* ci: fix flake8 warning by @agoose77 in https://github.com/scikit-hep/awkward/pull/2030\r\n* chore: update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/awkward/pull/2022\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.0.2...v2.0.3\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.3'>Version 2.0.3</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-23T18:09:11Z",
  "number":2035,
  "title":"Version 2.0.3",
  "url":"https://github.com/scikit-hep/awkward/discussions/2035"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"This follows quickly on https://github.com/scikit-hep/awkward/releases/tag/v2.0.3, which removed a feature and a function argument. Removing the feature is still the right thing to do (see the 2.0.3 release notes), but the function argument needs to go through a deprecation cycle, since libraries like dask-awkward pass arguments through to Awkward. Removing `flatten_records` as an argument introduces an error, even if the surviving case of `flatten_records=False` is desired.\r\n\r\nThis will also be a good exercise of the [deprecation schedule](https://github.com/scikit-hep/awkward/wiki#api-breaking-changes-after-20) in 2.x.\r\n\r\n## New features\r\n\r\n_(none!)_\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: soft-deprecate `flatten_records` instead of hard-deprecation by @agoose77 in https://github.com/scikit-hep/awkward/pull/2036\r\n\r\n## Other\r\n\r\n_(none!)_\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/awkward/compare/v2.0.3...v2.0.4\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/awkward/releases/tag/v2.0.4'>Version 2.0.4</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-23T19:44:25Z",
  "number":2037,
  "title":"Version 2.0.4",
  "url":"https://github.com/scikit-hep/awkward/discussions/2037"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"### Description of new feature\n\nOne thing that's a little annoying right now is getting data from Awkward Arrays into plots. A lot of my tutorials include stanzas like\r\n\r\n```python\r\nhist.Hist.new.Reg(100, 0, 5).Double().fill(ak.flatten(\r\n\r\n    awkward_array_goes_here,\r\n\r\naxis=None)).plot()\r\n```\r\n\r\nbut in the heat of interactive data analysis, you have some intermediate array result that you want to quickly plot to see what's happening. I find myself saying that the value of array-oriented interfaces is the quick interleaving of single-operation-multiple-data and feedback about that step, so that you catch mathematical errors early. (For instance, an unexpected spike at zero, a surprisingly long tail, or smeared trigger thresholds, missing mass peaks, etc.) However, the above is not easy to quickly plot.\r\n\r\nWe generally try to minimize method names in the ak.Array namespace, but suppose we add one more named `hist`, which flattens data and fills a histogram from the hist library? With the exception of selecting fields from the Awkward record array (if any), most of the arguments would be passed directly to hist for hist to take care of (and ModuleNotFoundError if hist can't be imported).\r\n\r\nSome features that would be nice to have:\r\n\r\n   * The ak.Array must either have no nested RecordArrays or must take a string/tuple of strings for which fields to put in the histogram. After applying those fields, the resulting columns must be non-RecordArrays. If the string conforms to `[A-Za-z_][A-Za-z_0-9]*`, we should use `__getattr__` instead of `__getitem__` so that Vector properties like `\"pt\"` can be used on an array of Cartesian vectors. Maybe that field selector can be a dict if one needs to map Awkward field names to hist axis names that aren't exactly the same.\r\n   * After ensuring that the selected fields (if any) broadcast to one another, they should each be flattened to one-dimensional NumPy arrays before passing to hist's `fill`. Nones and NaNs are removed (unless hist automatically ignores NaNs). Yes, I like to insist that flattening isn't always the right thing to do, and so it shouldn't be done automatically, but it's so often the right thing to do that it ought to be a default. (Auto-flattening in a method named `hist` is a different proposition from auto-flattening every time `__array__` is called; that would be too much/dangerous.)\r\n   * A pre-existing histogram can be specified as an argument: `hist=h`, in analogy with Matplotlib's `ax=ax`. In this case, the existing histogram is filled.\r\n   * If no `hist` is given, the method should create a new histogram, but what syntax should it use to do so? `hist.numpy.histogram`? The hist constructor? Would that require users to `import hist` so they can say things like `hist.new.Reg`?\r\n   * The return value should always be a histogram object, not a plot. The chain `array.hist(*args).plot()` is not unwieldy. Or maybe, should it be a proxy that must be continued with a `hist.Hist.new`-like chain, excepting that it does the `fill` implicitly?\r\n   * What about scatter plots? Should we just say that we always do 2-dimensional histograms instead of scatter plots, so that we don't run into problems with lots of marker objects from large arrays? (Maybe Matplotlib handles that gracefully when the input type is NumPy?) At least, we don't want the `hist` return value to sometimes be \"a thing that can be plotted\" and sometimes be \"a plot.\" It should always be one or the other.\r\n\r\nOh, and this would only be for v2. It might even be listed in a \"reasons to switch\" that we'll be publicizing next year.\r\n\r\n@henryiii and @amangoel185 may have opinions about how this hist integration/ergonomics should go.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"henryiii"
     },
     "body":"`awkward_array_goes_here.hist.Reg(100, 0, 5).Double().plot()`? Or `.hist.new.` to give options to add other interfaces like `.hist.auto()`, for example, which would do the numpy-like auto binning? Using QuickConstruct is the right thing, I think, because it doesn't require imports to work. We can add some infrastructure for auto-fill, I think.\r\n\r\nI'd not worry about adding a mapping of hist axes names; you can already set a label to show something different than the name, or change the names later.\r\n\r\nI'd also like to remove the need to add the `.flatten` if we can when passing to hist's `.fill`. Hist's fill could be smart enough to do that, I think? At least if it matches.\r\n\r\nScatter plots would have a different name, right? `awkward_array_goes_here.scatter_plot` or something, given it makes a 2D array and then plots it.",
     "createdAt":"2022-06-03T15:49:18Z",
     "number":4669606,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"I'm slightly in favour of *not* adding an `ak.hist` feature to the Awkward namespace, mainly because I think that these features should be added to hist.[^note] \r\n\r\nIn your example you note that the awkward array ends up appearing half-way into the code block. Interestingly, I tend to make my fill calls comprise solely of the identifiers, e.g.\r\n\r\n```python3\r\n...\r\nn_saturated_left = ak.count_nonzero(left.is_saturated, axis=-1)\r\nn_saturated_right = ak.count_nonzero(right.is_saturated, axis=-1)\r\n\r\nreturn (\r\n    Hist.new.Integer(0, 128).Int64().fill(n_saturated_left).fill(n_saturated_right)\r\n)\r\n```\r\nI think I've been subconciously driven to do this because the complexity of the hist construction otherwise adds too much noise to the code. That said, I think this noise is fairly inescapable - histograms need a lot of detail to be constructed, so perhaps they should be their own line of code.\r\n\r\nI ended up making a list of features that I think we/hist could benefit from. I'll enumerate them here:\r\n\r\n- `Hist.fill` could invoke `ak.flatten(..., None)`\r\n- `Hist.fill` could accept mappings\r\n- `ak.flatten(..., None)` could support preserving records\r\n- External addon namespace\r\n\r\nI think first-and-foremostly, making `hist.fill` call `ak.ravel` on the array would solve many of my 1D-hist cases. Even for ND histograms, I can take care of broadcasting, but ravelling as well is a bit of a pain, e.g. some code I wrote yesterday:\r\n```python\r\narrays = np.broadcast_arrays(\r\n    zone, n_saturated > 0, n_ringing_chain, n_non_ringing_chain\r\n)\r\nhist.fill(\r\n    *[\r\n        np.ravel(x)\r\n        for x in arrays\r\n    ]\r\n)\r\n```\r\nvs\r\n```python\r\narrays = np.broadcast_arrays(\r\n    zone, n_saturated > 0, n_ringing_chain, n_non_ringing_chain\r\n)\r\nhist.fill(*arrays)\r\n```\r\n\r\nI don't know whether I'd go as far as wanting `hist` to perform broadcasting automatically. As useful as that sounds, I feel like it could be problematic given that the user should expect that hist doesn't do anything weird with their data. I'm 100% open to people changing my mind on that particular issue.\r\n\r\nI wonder whether it would be helpful for hist to support mappings, a bit like matplotlib does with `data=`. Then we'd also have a solution for the current requirement that the user repeat the field names:\r\n\r\n```python\r\narray = ak.zip({\r\n    'mag': [1, 0.2, 0.3, 0.9, 0.8, 0.4],\r\n    'x': [10, 20, 30, 4, 5, 6],\r\n})\r\nhist = Hist.new.Reg(1024, 0, 1, name=\"mag\").Reg(1024, 0, 10, name=\"x\").Int64().fill(array)\r\nhist.plot()\r\n```\r\n\r\nActually, this now makes me think about #984, which would make it easier to work with matplotlib as well. Right now, we have to flatten the fields manually, and then rebuild the array with `ak.zip`:\r\n```python\r\nhist.fill(*[ak.flatten(x) for x in x in ak.unzip(array)])\r\n```\r\nBut with better `ak.flatten`, and adding support to hist for Awkward arrays (both w.r.t flattening and records), we could have\r\n```python\r\nhist.fill(array)\r\n```\r\n\r\nHaving written all of that, I'm slightly nervous that I'm proposing adding Awkward features to Hist, which might really be a bad design choice in terms of inversion of responsibility.\r\n\r\nIf we *did* add hist support to Awkward, are we making a special exception for it because of the convenience? Would there be merit in adding some kind of \"extern\" namespace to the array that avoids clashing with user fields and allows for future extension, e.g.\r\n\r\n```python\r\narray._.hist\r\narray.lib.hist\r\narray.ext.hist\r\n```\r\n\r\n\r\n[^note]: that is, if we add better Awkward support to hist vs hist-specific improvements to Awkward.",
     "createdAt":"2022-06-03T17:32:54Z",
     "number":4669607,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Regardless of whether hist does broadcasting (it seems like that would make it depend too much on Awkward), what I'm thinking about here is the ergonomics of getting a quick plot. Similarly, we have `to_list` as a method on ak.Array in addition to it being a function because when you're typing an array expression, going to the beginning of the line, adding \"`ak.to_list(`\", then going to the end and adding \"`)`\" is more effort than typing \"`.tolist()`\" at the end, where you already are. (_Especially_ if you're working on someone else's terminal and ctrl-A doesn't go to the beginning of the line, but instead does Select All!)\r\n\r\nWhat I'm thinking about here would intentionally be a thin interface, relying on hist to interpret its arguments. The QuickConstruct method seems like the right thing to do, but only if it doesn't mean that we have to implement hist proxy work-alikes: intermediate objects of the same sort that hist defines to do QuickConstruct, but our own because we need it to do a `fill` at the end. If we can accomplish that through some coordination between Awkward and hist, so much the better. (For instance, hist adds a \"`fill`\" that can be put at the beginning of the QuickConstruct chain, which is an odd thing to do, but it would give Awkward a backdoor do use hist's own implementation and still insert its directive to fill with a given set of arrays.)\r\n\r\nWe have other connections with external libraries, all in the src/awkward/_connect directory; this would probably go there, too.",
     "createdAt":"2022-06-03T18:29:59Z",
     "number":4669608,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"> it seems like that would make it depend too much on Awkward\r\n\r\nWell, I'd suggest it just uses `np.broadcast_arrays` as that would be implementation agnostic?\r\n\r\nI understand the benefit of the method chaining vs function composition. I actually just switched out my own Dask helper library to use function composition in order to make the abstraction thinner, but I can see that if you don't use Black and/or want to adapt your coding style, it's not as frictionless.\r\n\r\nRecognising that my opinion is towards \"let users type more if it means easier to read code\", I'm going to put it aside for now ;)\r\n\r\nWhat about a simple hist-fill method? Assuming we want to rely explicitly on hist, the user could be allowed to construct the hist factory, or just pass in the field names if required, e.g.\r\n```python\r\narray = ak.zip({\r\n    'mag': [1, 0.2, 0.3, 0.9, 0.8, 0.4],\r\n    'x': [10, 20, 30, 4, 5, 6],\r\n})\r\n\r\narray.hist(\"mag\") # 1D\r\narray.hist(\"mag\", \"x\") # 2D\r\narray.hist(Hist.new.Reg(32, 0, 1, name=\"mag\").Reg(32, 0, 10, name=\"x\").Int64()) # Fill 2D hist\r\n```\r\n\r\nOr, do you think a full-blown ak-namespaced proxy is better, e.g.\r\n```python\r\narray.hist.Reg(32, 0, 1, name=\"mag\").Reg(32, 0, 10, name=\"x\").Int64() # Fill 2D hist\r\n```",
     "createdAt":"2022-06-03T19:19:45Z",
     "number":4669609,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"@henryiii this is the discussion I referenced!\r\n\r\nToday we mused a `Hist.fill_flattened()` function that is an opt-in to flattening and broadcasting. It was suggested that this should support Pandas, NumPy and Awkward, and an effort to remove broadcasted `None`s should be performed.\r\n\r\nThis also relates to work on `TypeTracerArray.touch_data()`, which we would like for `hist.fill_flattened` to be able to handle. ",
     "createdAt":"2023-01-12T16:40:17Z",
     "number":4669612,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"agoose77"
        },
        "body":"We could implement this as a protocol, i.e. where any argument has a `_hist_flattened_` member, perform `arrays = arg._hist_flattened_(*args)`. This would be less use for NumPy or Pandas, which are unlikely to add support for Hist directly in my view, but it would be a simple way of decoupling Awkward from Hist: Hist knows nothing of Awkward, and Awkward only knows that it must return 1D compatible arrays. This might also support positional and named axes, but I've not thought that far ahead.",
        "createdAt":"2023-01-12T17:28:56Z",
        "number":4670063
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I came up with [a hypothetical API for this](https://gist.github.com/agoose77/f755d96061a36795dda592276d63f506), which let's libraries implement an interface, or explicitly register support. I'm not wedded to this design, but I think it's a good starting point for a discussion?",
        "createdAt":"2023-01-13T10:49:51Z",
        "number":4676816
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":5
  },
  "createdAt":"2022-06-03T15:16:14Z",
  "number":2109,
  "title":"Convenience methods for plotting and histogramming",
  "url":"https://github.com/scikit-hep/awkward/discussions/2109"
 },
 {
  "author":{
   "login":"dr-stringfellow"
  },
  "body":"### Version of Awkward Array\r\n\r\n1.8.0rc3\r\n\r\n### Version of numpy\r\n\r\n1.21.2\r\n\r\n### Description and code to reproduce\r\n\r\nCasting first as numpy array makes the code crash even though there is an outer cast to ak.Array in both cases\r\n\r\n```python\r\nimport numpy as np\r\nimport awkward as ak\r\n\r\narr = ak.Array([[[1,2,3],[4,5,6],[0,0,0]],[[10,11,12],[0,0,0],[0,0,0]]])\r\nprint(\"First attempt -- works\")\r\nprint(arr[ak.num(arr[arr > 0], axis=2) > 0])\r\n\r\narr = ak.Array(np.array([[[1,2,3],[4,5,6],[0,0,0]],[[10,11,12],[0,0,0],[0,0,0]]]))\r\nprint(\"Second attempt -- crashes\")\r\nprint(arr[ak.num(arr[arr > 0], axis=2) > 0])\r\n```\r\n\r\n\r\n\r\n### Error\r\n```pytb\r\n  File \"/local/usr/lib/python3.9/site-packages/awkward/operations/structure.py\", line 256, in num\r\n    out = layout.num(axis=axis)\r\nValueError: 'axis' out of range for 'num'\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.8.0rc3/src/libawkward/array/NumpyArray.cpp#L1598)\r\n```",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Hi @dr-stringfellow, thanks for filing a bug report! \r\n\r\nThis is actually a design decision, and not a bug, but I can appreciate that it very much looks like a bug at first glance! You did help me discover a bug in our next version v2, though!\r\n\r\nAwkward Array builds upon the advanced indexing of NumPy. In NumPy, if you index with a boolean array, the result is a flat array. Because your second `arr` is NumPy-like (totally regular) layout, Awkward uses the NumPy indexing behavior, not Awkward's extension.\r\n\r\nIf you were to convert your array to a _jagged_ array, then it would index properly. A jagged array is any array with `var` in the `type`, even if every sublist has the same length.\r\n\r\nThere are some discussions about making this nicer, but for now you need to ensure that `arr > 0` is jagged. You can do this with multiple calls to `ak.from_regular` on `arr`, or on `arr > 0`. In either case, the result will be a jagged array.\r\n\r\nI think this does demonstrate that we need to improve the docs here. I was looking for any mention of this behavior, but it is currently missing (at least, in an obvious sense)!\r\n\r\nhttps://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#nested-indexing\r\n",
     "createdAt":"2022-03-09T21:36:20Z",
     "number":8187044,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-03-09T20:41:27Z",
  "number":2973,
  "title":"Slicing with Awkward Arrays versus slicing with NumPy arrays",
  "url":"https://github.com/scikit-hep/awkward/discussions/2973"
 }
]
[
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Add `known_base_form` option (`awkward.forms.Form | None`) to `uproot.dask` so that opening root files at the client can be avoided for mature analyses where the base forms of all files are well known. This is pertinent for root tree with large numbers of keys like NanoAOD.\r\n\r\nRight now it is mutually exclusive with `open_files=True`, since it brings no advantage in that case.\r\n\r\nAn initial implementation is included.\r\n\r\nI tested the speedup with the following code:\r\n```python3\r\nimport uproot\r\nimport json\r\nimport gzip\r\nimport awkward\r\n\r\nimport pyinstrument\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    prof = pyinstrument.Profiler()\r\n    prof.start()\r\n\r\n    with gzip.open(\"nano_form.json.gz\", \"r\") as in_form:\r\n\tknown_form = awkward.forms.from_json(in_form.read())\r\n\r\n    for _ in range(20):\r\n\tevents = uproot.dask({\"tests/samples/nano_dy.root\": {\"object_path\": \"Events\", \"steps\": [[0,40]]}}, open_files=False, known_base_form=known_form)\r\n\r\n    prof.stop()\r\n    print(prof.output_text(unicode=True, color=True, show_all=True))\r\n\r\n    prof = pyinstrument.Profiler()\r\n    prof.start()\r\n\r\n    for _ in range(20):\r\n        events = uproot.dask({\"tests/samples/nano_dy.root\": {\"object_path\": \"Events\", \"steps\": [[0,40]]}}, open_files=False)\r\n\r\n    prof.stop()\r\n    print(prof.output_text(unicode=True, color=True, show_all=True))\r\n```\r\n\r\nWhich on a arm64 macbook pro with SSD gives about a 6x speedup. The gain will be enormous when using xrootd.\r\n",
  "closed_at":"2024-01-03T18:56:09Z",
  "comments":0,
  "created_at":"2024-01-02T23:58:51Z",
  "draft":false,
  "id":2063083406,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5jFl5C",
  "number":1077,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-03T18:56:09Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: supply a pre-calculated base form to avoid file opening",
  "updated_at":"2024-01-03T18:56:09Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"NONE",
  "body":"In scenarios where multiple dask workers are running on a machine (for example, a local cluster with O(10) workers), only one dask worker will be busy deserializing data from root; multiple threads could be employed in parallel while those other workers are idle/waiting. Especially relevant for dask-localcluster setups\r\n\r\nfollowing from this discussion:\r\nhttps://iris-hep.slack.com/archives/C03UA418VJ8/p1704815356020659",
  "closed_at":null,
  "comments":1,
  "created_at":"2024-01-09T16:24:49Z",
  "id":2072719031,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss57iy63",
  "number":1079,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Expose decompression_executor in uproot.dask",
  "updated_at":"2024-02-01T14:44:48Z",
  "user":"MDQ6VXNlcjM4MjE3Mjc0"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Ruff has tools for this, and there are NumPy 2.0 prereleases (or there will be soon).",
  "closed_at":null,
  "comments":3,
  "created_at":"2024-01-11T16:28:07Z",
  "id":2077102536,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss57zhHI",
  "number":1080,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Test against NumPy 2.0",
  "updated_at":"2024-02-15T15:27:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/astral-sh/ruff-pre-commit: v0.1.11 \u2192 v0.1.13](https://github.com/astral-sh/ruff-pre-commit/compare/v0.1.11...v0.1.13)\n<!--pre-commit.ci end-->",
  "closed_at":"2024-01-18T15:33:38Z",
  "comments":1,
  "created_at":"2024-01-15T19:10:35Z",
  "draft":false,
  "id":2082559432,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5kHoBW",
  "number":1082,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-18T15:33:38Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2024-01-18T15:33:39Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"The last successful build was on September 25, 2023. It looks like `v5.0.12` was the latest (already existing) git tag at that time.\r\n\r\nThe very first error after that is:\r\n\r\n> The configuration key \"build.os\" is required to build your documentation. Read more at https://docs.readthedocs.io/en/stable/config-file/v2.html#build-os\r\n\r\nIt looks like we just need to add\r\n\r\n```yaml\r\nbuild:\r\n  os: ubuntu-22.04\r\n```\r\n\r\nto\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/1f69682c2a69834baa8405a2a84413a0dba6f10f/.readthedocs.yml#L1-L11\r\n\r\nIt's probably very easy! But this failure in documentation-building should have become an alert somehow. I'll look into how to make that happen.",
  "closed_at":"2024-01-16T18:34:18Z",
  "comments":0,
  "created_at":"2024-01-16T17:25:01Z",
  "id":2084483408,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss58PrFQ",
  "number":1083,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Fix the ReadTheDocs build",
  "updated_at":"2024-01-16T18:34:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2024-01-16T18:34:17Z",
  "comments":4,
  "created_at":"2024-01-16T17:39:18Z",
  "draft":false,
  "id":2084503964,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5kOSiy",
  "number":1084,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-16T18:34:17Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: fix ReadTheDocs documentation",
  "updated_at":"2024-01-16T18:34:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2024-01-23T08:05:55Z",
  "comments":5,
  "created_at":"2024-01-18T16:47:41Z",
  "draft":false,
  "id":2088682372,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5kcjEV",
  "number":1085,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-23T08:05:55Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add dask_to_root",
  "updated_at":"2024-01-23T08:05:56Z",
  "user":"MDQ6VXNlcjcwNDQxNjQx"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2024-01-22T10:45:24Z",
  "comments":2,
  "created_at":"2024-01-18T16:54:16Z",
  "draft":false,
  "id":2088694126,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5kclom",
  "number":1086,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-22T10:45:24Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: pandas performance on files with many branches",
  "updated_at":"2024-01-22T10:45:25Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @bnavigator as a contributor for test.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot5/pull/396#issuecomment-1898865940)\n\n[skip ci]",
  "closed_at":"2024-01-18T16:58:35Z",
  "comments":0,
  "created_at":"2024-01-18T16:58:18Z",
  "draft":false,
  "id":2088701107,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5kcnLb",
  "number":1087,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-18T16:58:35Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add bnavigator as a contributor for test",
  "updated_at":"2024-01-18T16:58:36Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2024-01-26T19:04:39Z",
  "comments":1,
  "created_at":"2024-01-19T14:32:23Z",
  "draft":false,
  "id":2090688949,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5kjgsU",
  "number":1088,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-26T19:04:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: recorrds -> records",
  "updated_at":"2024-01-26T19:04:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjE4NTI0NDc=",
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"@lgray mentioned https://pypi.org/project/cramjam/ to me and it looks like a nice solution to provide many of the ROOT compression algorithms in a single dependency-free package. Additionally, it allows to declare the output length so it can pre-allocate the buffer, which may provide some speedup for algorithms other than lz4, which is the only one currently using the uncompressed size hint:\r\nhttps://github.com/scikit-hep/uproot5/blob/fd0637bf978d75619e125a7f624827348f757865/src/uproot/compression.py#L194\r\n\r\nThis is an internal feature and would not provide any user enhancement other than a potential speed-up",
  "closed_at":"2024-02-14T17:05:54Z",
  "comments":6,
  "created_at":"2024-01-19T15:48:24Z",
  "id":2090817727,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss58n1i_",
  "number":1089,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Use cramjam to provide compression utilities",
  "updated_at":"2024-02-14T17:05:54Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Fixes #1089 \r\n\r\n~~zlib remains the standard python library since the `deflate` interface of cramjam doesn't calculate the zlib headers for the streams.~~\r\nIn addition to the standard python, this PR also lets uproot.ZLIB use the isal zlib python library, which is optimized on multiple architectures and typically 3-5x faster. If available, uproot allows users to opt in to using that library for zlib compression.\r\n\r\nI also found a buffer size hinting option that was not being used for the standard python zlib, that may help.\r\n\r\nGenerally - we now have fewer dependencies for uproot through this package. Wheel coverage for cramjam looks sufficient for our platform requirements.\r\n\r\n@jpivarski @nsmith- @martindurant",
  "closed_at":"2024-02-14T17:05:53Z",
  "comments":24,
  "created_at":"2024-01-20T19:31:31Z",
  "draft":false,
  "id":2092214145,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5kojQD",
  "number":1090,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-14T17:05:53Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: use cramjam for lzma, lz4, and zstd, opt-in use of isal for zlib",
  "updated_at":"2024-02-14T17:05:53Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"NONE",
  "body":"Uproot errors out while opening root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root with the following stack trace:\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'5.2.1'\r\n>>> filename = \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\"\r\n>>> file = uproot.open(filename)\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/fsspec/registry.py in get_filesystem_class(protocol)\r\n    235         try:\r\n--> 236             register_implementation(protocol, _import_class(bit[\"class\"]))\r\n    237         except ImportError as e:\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/fsspec/registry.py in _import_class(cls, minv)\r\n    270         s3 = mod == \"s3fs\"\r\n--> 271         mod = importlib.import_module(mod)\r\n    272         if s3 and mod.__version__.split(\".\") < [\"0\", \"5\"]:\r\n\r\n/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\n/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/_bootstrap.py in _gcd_import(name, package, level)\r\n\r\n/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/_bootstrap.py in _find_and_load(name, import_)\r\n\r\n/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\n/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/_bootstrap.py in _load_unlocked(spec)\r\n\r\n/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/_bootstrap_external.py in exec_module(self, module)\r\n\r\n/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/fsspec_xrootd/__init__.py in <module>\r\n     11 from ._version import version as __version__\r\n---> 12 from .xrootd import XRootDFile, XRootDFileSystem\r\n     13 \r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/fsspec_xrootd/xrootd.py in <module>\r\n     17 from fsspec.spec import AbstractBufferedFile  # type: ignore[import-not-found]\r\n---> 18 from XRootD import client  # type: ignore[import-not-found]\r\n     19 from XRootD.client.flags import (  # type: ignore[import-not-found]\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/XRootD/client/__init__.py in <module>\r\n      2 \r\n----> 3 from .glob_funcs import glob, iglob\r\n      4 from .filesystem import FileSystem as FileSystem\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/XRootD/client/glob_funcs.py in <module>\r\n     25 \r\n---> 26 from XRootD.client.filesystem import FileSystem\r\n     27 \r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/XRootD/client/filesystem.py in <module>\r\n     25 \r\n---> 26 from pyxrootd import client\r\n     27 from XRootD.client.responses import XRootDStatus, StatInfo, StatInfoVFS\r\n\r\nImportError: dlopen(/Users/saransh/Code/HEP/coffea/.env/lib/python3.11/site-packages/pyxrootd/client.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/libXrdCl.3.dylib\r\n  Referenced from: <F808E69B-2BDA-3D93-B243-5E7E79525063> /Users/saransh/Code/HEP/coffea/.env/lib/python3.11/site-packages/pyxrootd/client.cpython-311-darwin.so\r\n  Reason: tried: '/Users/saransh/Code/HEP/coffea/.env/lib/python3.11/site-packages/pyxrootd/../lib/libXrdCl.3.dylib' (no such file), '/Users/saransh/Code/HEP/coffea/.env/lib/python3.11/site-packages/pyxrootd/../lib/libXrdCl.3.dylib' (no such file), '/opt/homebrew/lib/libXrdCl.3.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libXrdCl.3.dylib' (no such file), '/opt/homebrew/lib/libXrdCl.3.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libXrdCl.3.dylib' (no such file)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-73489904e2c3> in <cell line: 0>()\r\n----> 1 file = uproot.open(filename)\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/uproot/reading.py in open(path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\r\n    139         )\r\n    140 \r\n--> 141     file = ReadOnlyFile(\r\n    142         file_path,\r\n    143         object_cache=object_cache,\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/uproot/reading.py in __init__(self, file_path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\r\n    557             file_path, self._options\r\n    558         )\r\n--> 559         self._source = source_cls(file_path, **self._options)\r\n    560 \r\n    561         self.hook_before_get_chunks()\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/uproot/source/fsspec.py in __init__(self, file_path, **options)\r\n     29     def __init__(self, file_path: str, **options):\r\n     30         super().__init__()\r\n---> 31         self._fs, self._file_path = fsspec.core.url_to_fs(\r\n     32             file_path, **self.extract_fsspec_options(options)\r\n     33         )\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/fsspec/core.py in url_to_fs(url, **kwargs)\r\n    381     }\r\n    382     kwargs = {k: v for k, v in kwargs.items() if k not in known_kwargs}\r\n--> 383     chain = _un_chain(url, kwargs)\r\n    384     inkwargs = {}\r\n    385     # Reverse iterate the chain, creating a nested target_* structure\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/fsspec/core.py in _un_chain(path, kwargs)\r\n    330     for bit in reversed(bits):\r\n    331         protocol = kwargs.pop(\"protocol\", None) or split_protocol(bit)[0] or \"file\"\r\n--> 332         cls = get_filesystem_class(protocol)\r\n    333         extra_kwargs = cls._get_kwargs_from_urls(bit)\r\n    334         kws = kwargs.pop(protocol, {})\r\n\r\n~/Code/HEP/coffea/.env/lib/python3.11/site-packages/fsspec/registry.py in get_filesystem_class(protocol)\r\n    236             register_implementation(protocol, _import_class(bit[\"class\"]))\r\n    237         except ImportError as e:\r\n--> 238             raise ImportError(bit[\"err\"]) from e\r\n    239     cls = registry[protocol]\r\n    240     if getattr(cls, \"protocol\", None) in (\"abstract\", None):\r\n\r\nImportError: Unable to load filesystem from EntryPoint(name='root', value='fsspec_xrootd:XRootDFileSystem', group='fsspec.specs')\r\n```\r\n\r\n---\r\n\r\nInspecting the first `ImportError`:\r\n```python\r\nImportError: dlopen(/Users/saransh/Code/HEP/coffea/.env/lib/python3.11/site-packages/pyxrootd/client.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/libXrdCl.3.dylib\r\n  Referenced from: <F808E69B-2BDA-3D93-B243-5E7E79525063> /Users/saransh/Code/HEP/coffea/.env/lib/python3.11/site-packages/pyxrootd/client.cpython-311-darwin.so\r\n  Reason: tried: '/Users/saransh/Code/HEP/coffea/.env/lib/python3.11/site-packages/pyxrootd/../lib/libXrdCl.3.dylib' (no such file), '/Users/saransh/Code/HEP/coffea/.env/lib/python3.11/site-packages/pyxrootd/../lib/libXrdCl.3.dylib' (no such file), '/opt/homebrew/lib/libXrdCl.3.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libXrdCl.3.dylib' (no such file), '/opt/homebrew/lib/libXrdCl.3.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libXrdCl.3.dylib' (no such file)\r\n```\r\n\r\nI think uproot is looking for `libXrdCl.3.dylib` in the wrong directories. The file is present in -\r\n```console\r\n(.env) saransh@saranshmacair .env % ls lib/python3.11/site-packages/pyxrootd/ | grep libXrdCl.3.dylib\r\nlibXrdCl.3.dylib\r\n```\r\n\r\n---\r\n\r\nI don't know if this is an uproot issue, a fsspec_xrootd issue, or an XRootD issue. This could also be an issue with my local setup. Any help would be appreciated, thank you!\r\n",
  "closed_at":null,
  "comments":4,
  "created_at":"2024-01-22T09:59:39Z",
  "id":2093533907,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss58yMrT",
  "number":1091,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Uproot (or fsspec_xrootd) looks for `libXrdCl.3.dylib` in the wrong directories",
  "updated_at":"2024-02-15T11:44:47Z",
  "user":"MDQ6VXNlcjc0MDU1MTAy"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/astral-sh/ruff-pre-commit: v0.1.13 \u2192 v0.1.14](https://github.com/astral-sh/ruff-pre-commit/compare/v0.1.13...v0.1.14)\n<!--pre-commit.ci end-->",
  "closed_at":"2024-01-26T15:47:33Z",
  "comments":0,
  "created_at":"2024-01-22T19:17:15Z",
  "draft":false,
  "id":2094587205,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5kwebi",
  "number":1092,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-26T15:47:33Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2024-01-26T15:47:34Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"reproducer:\r\n\r\n```python3\r\nimport uproot\r\n\r\nfor _ in range(200):\r\n    uproot.dask({\r\n\t\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"\r\n    })\r\n```\r\n\r\nThis particular instance leaks ~30MB per open. This adds up very quickly if you need to extract the form of hundreds of files in a remote process as evident from https://github.com/CoffeaTeam/coffea/issues/1007 where this bug manifested pretty nastily.",
  "closed_at":"2024-01-24T20:25:50Z",
  "comments":7,
  "created_at":"2024-01-23T05:09:11Z",
  "id":2095252590,
  "labels":null,
  "locked":true,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss584wRu",
  "number":1093,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Every `uproot.dask` call increases memory footprint by 30 MB (it's in `dask.base.function_cache`)",
  "updated_at":"2024-01-24T20:25:50Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Change `uproot.dask`'s `allow_read_errors_with_report` argument to also allow a tuple of exceptions to be passed in. Original `=True` behavior keeps the `(OSError,)` only tuple",
  "closed_at":"2024-01-23T20:22:00Z",
  "comments":1,
  "created_at":"2024-01-23T17:49:14Z",
  "draft":false,
  "id":2096644834,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5k3du2",
  "number":1094,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-23T20:22:00Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: allow user to supply tuple of allowed exceptions",
  "updated_at":"2024-01-23T20:27:28Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"## Reproducing:\r\n\r\nYou need a small root file - like `myfile.root`. I'm going to assume the tree in `myfile.root` you want to open is called `mytree`:\r\n\r\n1. rename `myfile.root` to `myfile.root.1`\r\n2. In python, run:\r\n   import uproot\r\n   uproot.dask('myfile.root.1:mytree')\r\n3. You'll get file-not-found exception.\r\n\r\n## Why would you name a file like this?\r\n\r\nThe ATLAS production system often names files with the `.1` for whatever reason. As a result, I often find myself accessing files with names like that.\r\n\r\n## Workaround\r\n\r\nUse the dictionary specification method: `uproot.dask({ 'myfile.root.1': 'mytree'})`",
  "closed_at":"2024-01-24T17:48:35Z",
  "comments":3,
  "created_at":"2024-01-24T01:37:30Z",
  "id":2097264980,
  "labels":null,
  "locked":true,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss59AblU",
  "number":1095,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Uproot.dask fails to open files with names like `myfile.root.1`",
  "updated_at":"2024-01-24T17:48:35Z",
  "user":"MDQ6VXNlcjE3NzgzNjY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Want to support library export as Polars.\r\n\r\n[Polars](https://github.com/pola-rs/polars)",
  "closed_at":null,
  "comments":5,
  "created_at":"2024-01-24T03:21:41Z",
  "id":2097346081,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss59AvYh",
  "number":1096,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"open",
  "state_reason":null,
  "title":"Want to support library export as Polars.",
  "updated_at":"2024-01-29T18:58:02Z",
  "user":"MDQ6VXNlcjczMDU4MDYy"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"there is a strange issue when trying to read a branch from an ATLAS physlite file. \r\nI use uproot 5.2.1 and awkward 2.5.2.\r\n\r\nHere the smallest reproducible example:\r\n\r\n```\r\nimport uproot\r\nimport awkward as ak\r\nfn='root://xcache.af.uchicago.edu:1094//'\r\nfn+='root://dcgftp.usatlas.bnl.gov:1094//pnfs/usatlas.bnl.gov/LOCALGROUPDISK/rucio/data18_13TeV/04/9a/DAOD_PHYSLITE.34857549._000001.pool.root.1'\r\nwith uproot.open(fn) as f:\r\n    tree=f['CollectionTree;1']\r\n    a=tree[\"PrimaryVerticesAuxDyn.trackParticleLinks\"].array()\r\n```\r\n\r\n\r\nthis file is already cached so you should be able to access it without authentication.\r\n\r\nthis is the result:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[3], line 7\r\n      5 with uproot.open(fn) as f:\r\n      6     tree=f['CollectionTree;1']\r\n----> 7     a=tree[\"PrimaryVerticesAuxDyn.trackParticleLinks\"].array()\r\n      8     print(a.to_list()[:2])\r\n\r\nFile /venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py:1815, in TBranch.array(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library, ak_add_doc)\r\n   1812                 ranges_or_baskets.append((branch, basket_num, range_or_basket))\r\n   1814 interp_options = {\"ak_add_doc\": ak_add_doc}\r\n-> 1815 _ranges_or_baskets_to_arrays(\r\n   1816     self,\r\n   1817     ranges_or_baskets,\r\n   1818     branchid_interpretation,\r\n   1819     entry_start,\r\n   1820     entry_stop,\r\n   1821     decompression_executor,\r\n   1822     interpretation_executor,\r\n   1823     library,\r\n   1824     arrays,\r\n   1825     False,\r\n   1826     interp_options,\r\n   1827 )\r\n   1829 _fix_asgrouped(\r\n   1830     arrays,\r\n   1831     expression_context,\r\n   (...)\r\n   1835     ak_add_doc,\r\n   1836 )\r\n   1838 if array_cache is not None:\r\n\r\nFile /venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py:3142, in _ranges_or_baskets_to_arrays(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays, update_ranges_or_baskets, interp_options)\r\n   3139     pass\r\n   3141 elif isinstance(obj, tuple) and len(obj) == 3:\r\n-> 3142     uproot.source.futures.delayed_raise(*obj)\r\n   3144 else:\r\n   3145     raise AssertionError(obj)\r\n\r\nFile /venv/lib/python3.9/site-packages/uproot/source/futures.py:38, in delayed_raise(exception_class, exception_value, traceback)\r\n     34 def delayed_raise(exception_class, exception_value, traceback):\r\n     35     \"\"\"\r\n     36     Raise an exception from a background thread on the main thread.\r\n     37     \"\"\"\r\n---> 38     raise exception_value.with_traceback(traceback)\r\n\r\nFile /venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py:3084, in _ranges_or_baskets_to_arrays.<locals>.basket_to_array(basket)\r\n   3081 context = dict(branch.context)\r\n   3082 context[\"forth\"] = forth_context[branch.cache_key]\r\n-> 3084 basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n   3085     basket.data,\r\n   3086     basket.byte_offsets,\r\n   3087     basket,\r\n   3088     branch,\r\n   3089     context,\r\n   3090     basket.member(\"fKeylen\"),\r\n   3091     library,\r\n   3092     interp_options,\r\n   3093 )\r\n   3094 if basket.num_entries != len(basket_arrays[basket.basket_num]):\r\n   3095     raise ValueError(\r\n   3096         \"\"\"basket {} in tree/branch {} has the wrong number of entries \"\"\"\r\n   3097         \"\"\"(expected {}, obtained {}) when interpreted as {}\r\n   (...)\r\n   3105         )\r\n   3106     )\r\n\r\nFile /venv/lib/python3.9/site-packages/uproot/interpretation/objects.py:145, in AsObjects.basket_array(self, data, byte_offsets, basket, branch, context, cursor_offset, library, options)\r\n    134 assert basket.byte_offsets is not None\r\n    136 if self._forth and (\r\n    137     isinstance(\r\n    138         library,\r\n   (...)\r\n    143     )\r\n    144 ):\r\n--> 145     output = self.basket_array_forth(\r\n    146         data,\r\n    147         byte_offsets,\r\n    148         basket,\r\n    149         branch,\r\n    150         context,\r\n    151         cursor_offset,\r\n    152         library,\r\n    153         options,\r\n    154     )\r\n    156 else:\r\n    157     output = ObjectArray(\r\n    158         self._model, branch, context, byte_offsets, data, cursor_offset\r\n    159     ).to_numpy()\r\n\r\nFile /venv/lib/python3.9/site-packages/uproot/interpretation/objects.py:231, in AsObjects.basket_array_forth(self, data, byte_offsets, basket, branch, context, cursor_offset, library, options)\r\n    229 # this thread tries to do it!\r\n    230 try:\r\n--> 231     output = self._discover_forth(\r\n    232         data, byte_offsets, branch, context, cursor_offset\r\n    233     )\r\n    234 except CannotBeForth:\r\n    235     self._forth = False\r\n\r\nFile /venv/lib/python3.9/site-packages/uproot/interpretation/objects.py:390, in AsObjects._discover_forth(self, data, byte_offsets, branch, context, cursor_offset)\r\n    383 chunk = uproot.source.chunk.Chunk.wrap(\r\n    384     branch.file.source, data[byte_offsets[i] : byte_offsets[i + 1]]\r\n    385 )\r\n    386 cursor = uproot.source.cursor.Cursor(\r\n    387     0, origin=-(byte_offsets[i] + cursor_offset)\r\n    388 )\r\n--> 390 context[\"forth\"].gen.reset_active_node()\r\n    391 output[i] = self._model.read(\r\n    392     chunk, cursor, context, branch.file, branch.file.detached, branch\r\n    393 )\r\n    395 derived_form = context[\"forth\"].gen.model.derive_form()\r\n\r\nAttributeError: 'NoneType' object has no attribute 'reset_active_node'\r\n```",
  "closed_at":"2024-01-24T22:20:02Z",
  "comments":10,
  "created_at":"2024-01-24T21:01:47Z",
  "id":2099096531,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss59HavT",
  "number":1099,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"can't read a branch",
  "updated_at":"2024-01-25T08:55:06Z",
  "user":"MDQ6VXNlcjE3MzY5ODQ="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"The TBranch described in #1099 contains many empty lists, and its AwkwardForth code can't be discovered until it sees at least one non-empty list. That happens after cycling from one TBasket to the next (about a dozen TBaskets, actually), and the `context[\"forth\"].vm` state is `None`, rather than nonexistent, due to\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/e592ae333c2d8d2ee7d65f1a2de1fcb3a8fea13f/src/uproot/interpretation/objects.py#L237\r\n\r\nSo the check for \"Does an AwkwardForth VM exist?\" should check for both the missing attribute case and the attribute is equal to None case.",
  "closed_at":"2024-01-24T22:20:01Z",
  "comments":3,
  "created_at":"2024-01-24T21:55:42Z",
  "draft":false,
  "id":2099174961,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5lAFIb",
  "number":1100,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-24T22:20:01Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: state of context[\"forth\"] after an entire TBasket is incomplete",
  "updated_at":"2024-01-24T22:20:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"NONE",
  "body":"Follow up of #1099 \r\n\r\nPython version: 3.11.6\r\nUproot version: 5.2.2rc2.dev13+gc128e4b (installed via pip from the main branch after #1100 had been merged)\r\nzstandard version: 0.22.0 (I had to install it when I first tried to read in the file)\r\n \r\nTrying to read the `parentLinks` or `childLinks` from a `TruthParticle` in a PhysLite file sometimes results in an error and sometimes works while running on the exact same input file.\r\n\r\nThe typename of the branch is:\r\n`std::vector<std::vector<ElementLink<DataVector<xAOD::TruthParticle_v1>>>>`\r\n\r\nReading in with numpy works:\r\n\r\n```\r\nwith uproot.open(path + filename) as f:\r\n    data = f[tree][\"TruthBottomAuxDyn.childLinks\"].array(library=\"np\")\r\n\r\ndata\r\narray([<STLVector [] at 0x01a123aa3dd0>, <STLVector [] at 0x01a123bae310>,\r\n       <STLVector [] at 0x01a123bce450>, ...,\r\n       <STLVector [] at 0x01a1204fdb50>, <STLVector [] at 0x01a1204fdb90>,\r\n       <STLVector [] at 0x01a1204fdbd0>], dtype=object)\r\n```\r\n\r\nThe first none-empty entry contains:\r\n```\r\n[[<ElementLink<DataVector<xAOD::TruthParticle_v1>> (version 1) at 0x01a123ad3550>],\r\n [<ElementLink<DataVector<xAOD::TruthParticle_v1>> (version 1) at 0x01a123ad26d0>],\r\n [<ElementLink<DataVector<xAOD::TruthParticle_v1>> (version 1) at 0x01a123ad1b10>,\r\n  <ElementLink<DataVector<xAOD::TruthParticle_v1>> (version 1) at 0x01a123ad3e10>,\r\n  <ElementLink<DataVector<xAOD::TruthParticle_v1>> (version 1) at 0x01a12384bc50>,\r\n  <ElementLink<DataVector<xAOD::TruthParticle_v1>> (version 1) at 0x01a123a806d0>],\r\n [<ElementLink<DataVector<xAOD::TruthParticle_v1>> (version 1) at 0x01a123a81290>]]\r\n```\r\n\r\nThe first of these single entries has:\r\n```\r\n{'m_persKey': 779635413,\r\n 'm_persIndex': 3,\r\n '_typename': 'ElementLink<DataVector<xAOD::TruthParticle_v1>>'}\r\n```\r\n\r\n\r\nWhen reading with awkward, 10 out of 20 attempts succeed (I counted) and the other 10 throw an error (again, always reading the exact same file).\r\nThe succeeding ones:\r\n```\r\nwith uproot.open(path + filename) as f:\r\n    print(f[tree][\"TruthBottomAuxDyn.childLinks\"].typename)\r\n    data = f[tree][\"TruthBottomAuxDyn.childLinks\"].array()\r\n\r\ndata\r\n[[],\r\n [],\r\n [],\r\n [],\r\n [],\r\n [],\r\n [],\r\n [],\r\n [],\r\n [],\r\n ...,\r\n [],\r\n [],\r\n [],\r\n [],\r\n [],\r\n [],\r\n [],\r\n [],\r\n []]\r\n--------------------------------------------------------------------------------\r\ntype: 40000 * var * var * struct[{\r\n    m_persKey: uint32,\r\n    m_persIndex: uint32\r\n}, parameters={\"__record__\": \"ElementLink<DataVector<xAOD::TruthParticle_v1>>\"}]\r\n\r\n```\r\n\r\nThe failing ones:\r\n\r\n```\r\nwith uproot.open(path + filename) as f:\r\n    print(f[tree][\"TruthBottomAuxDyn.childLinks\"].typename)\r\n    data = f[tree][\"TruthBottomAuxDyn.childLinks\"].array()\r\n\r\ndata\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[105], line 10\r\n      8 with uproot.open(path + filename) as f:\r\n      9     print(f[tree][\"TruthBottomAuxDyn.childLinks\"].typename)\r\n---> 10     data = f[tree][\"TruthBottomAuxDyn.childLinks\"].array()\r\n     12 data\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\behaviors\\TBranch.py:1815, in TBranch.array(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library, ak_add_doc)\r\n   1812                 ranges_or_baskets.append((branch, basket_num, range_or_basket))\r\n   1814 interp_options = {\"ak_add_doc\": ak_add_doc}\r\n-> 1815 _ranges_or_baskets_to_arrays(\r\n   1816     self,\r\n   1817     ranges_or_baskets,\r\n   1818     branchid_interpretation,\r\n   1819     entry_start,\r\n   1820     entry_stop,\r\n   1821     decompression_executor,\r\n   1822     interpretation_executor,\r\n   1823     library,\r\n   1824     arrays,\r\n   1825     False,\r\n   1826     interp_options,\r\n   1827 )\r\n   1829 _fix_asgrouped(\r\n   1830     arrays,\r\n   1831     expression_context,\r\n   (...)\r\n   1835     ak_add_doc,\r\n   1836 )\r\n   1838 if array_cache is not None:\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\behaviors\\TBranch.py:3142, in _ranges_or_baskets_to_arrays(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays, update_ranges_or_baskets, interp_options)\r\n   3139     pass\r\n   3141 elif isinstance(obj, tuple) and len(obj) == 3:\r\n-> 3142     uproot.source.futures.delayed_raise(*obj)\r\n   3144 else:\r\n   3145     raise AssertionError(obj)\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\source\\futures.py:38, in delayed_raise(exception_class, exception_value, traceback)\r\n     34 def delayed_raise(exception_class, exception_value, traceback):\r\n     35     \"\"\"\r\n     36     Raise an exception from a background thread on the main thread.\r\n     37     \"\"\"\r\n---> 38     raise exception_value.with_traceback(traceback)\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\behaviors\\TBranch.py:3111, in _ranges_or_baskets_to_arrays.<locals>.basket_to_array(basket)\r\n   3108 basket = None\r\n   3110 if len(basket_arrays) == branchid_num_baskets[branch.cache_key]:\r\n-> 3111     arrays[branch.cache_key] = interpretation.final_array(\r\n   3112         basket_arrays,\r\n   3113         entry_start,\r\n   3114         entry_stop,\r\n   3115         branch.entry_offsets,\r\n   3116         library,\r\n   3117         branch,\r\n   3118         interp_options,\r\n   3119     )\r\n   3120     # no longer needed, save memory\r\n   3121     basket_arrays.clear()\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\interpretation\\objects.py:475, in AsObjects.final_array(self, basket_arrays, entry_start, entry_stop, entry_offsets, library, branch, options)\r\n    473     output = awkward.concatenate(trimmed, mergebool=False, highlevel=False)\r\n    474 else:\r\n--> 475     output = numpy.concatenate(trimmed)\r\n    477 self.hook_before_library_finalize(\r\n    478     basket_arrays=basket_arrays,\r\n    479     entry_start=entry_start,\r\n   (...)\r\n    484     output=output,\r\n    485 )\r\n    487 output = library.finalize(\r\n    488     output, branch, self, entry_start, entry_stop, options\r\n    489 )\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\highlevel.py:1527, in Array.__array_function__(self, func, types, args, kwargs)\r\n   1513 def __array_function__(self, func, types, args, kwargs):\r\n   1514     \"\"\"\r\n   1515     Intercepts attempts to pass this Array to those NumPy functions other\r\n   1516     than universal functions that have an Awkward equivalent.\r\n   (...)\r\n   1525     See also #__array_ufunc__.\r\n   1526     \"\"\"\r\n-> 1527     return ak._connect.numpy.array_function(\r\n   1528         func, types, args, kwargs, behavior=self._behavior, attrs=self._attrs\r\n   1529     )\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_connect\\numpy.py:102, in array_function(func, types, args, kwargs, behavior, attrs)\r\n    100 function = implemented.get(func)\r\n    101 if function is not None:\r\n--> 102     return function(*args, **kwargs)\r\n    103 # Use NumPy's implementation\r\n    104 else:\r\n    105     all_arguments = chain(args, kwargs.values())\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_connect\\numpy.py:142, in implements.<locals>.decorator.<locals>.ensure_valid_args(*args, **kwargs)\r\n    138     names = \", \".join(provided_invalid_names)\r\n    139     raise TypeError(\r\n    140         f\"Awkward NEP-18 overload was provided with unsupported argument(s): {names}\"\r\n    141     )\r\n--> 142 return function(*args, **kwargs)\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_dispatch.py:62, in named_high_level_function.<locals>.dispatch(*args, **kwargs)\r\n     60 # Failed to find a custom overload, so resume the original function\r\n     61 try:\r\n---> 62     next(gen_or_result)\r\n     63 except StopIteration as err:\r\n     64     return err.value\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_concatenate.py:66, in concatenate(arrays, axis, mergebool, highlevel, behavior, attrs)\r\n     63     yield arrays\r\n     65 # Implementation\r\n---> 66 return _impl(arrays, axis, mergebool, highlevel, behavior, attrs)\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_concatenate.py:114, in _impl(arrays, axis, mergebool, highlevel, behavior, attrs)\r\n    112 # Now that we're sure `arrays` is not a singular array\r\n    113 with HighLevelContext(behavior=behavior, attrs=attrs) as ctx:\r\n--> 114     content_or_others = ensure_same_backend(\r\n    115         *(\r\n    116             ctx.unwrap(\r\n    117                 x,\r\n    118                 allow_record=axis != 0,\r\n    119                 allow_unknown=False,\r\n    120                 primitive_policy=\"pass-through\",\r\n    121             )\r\n    122             for x in arrays\r\n    123         )\r\n    124     )\r\n    126 contents = [x for x in content_or_others if isinstance(x, ak.contents.Content)]\r\n    127 if len(contents) == 0:\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_concatenate.py:116, in <genexpr>(.0)\r\n    112 # Now that we're sure `arrays` is not a singular array\r\n    113 with HighLevelContext(behavior=behavior, attrs=attrs) as ctx:\r\n    114     content_or_others = ensure_same_backend(\r\n    115         *(\r\n--> 116             ctx.unwrap(\r\n    117                 x,\r\n    118                 allow_record=axis != 0,\r\n    119                 allow_unknown=False,\r\n    120                 primitive_policy=\"pass-through\",\r\n    121             )\r\n    122             for x in arrays\r\n    123         )\r\n    124     )\r\n    126 contents = [x for x in content_or_others if isinstance(x, ak.contents.Content)]\r\n    127 if len(contents) == 0:\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_layout.py:146, in HighLevelContext.unwrap(self, obj, allow_record, allow_unknown, none_policy, primitive_policy, string_policy, use_from_iter, regulararray)\r\n    142 from awkward.operations.ak_to_layout import _impl as to_layout_impl\r\n    144 self.update(obj)\r\n--> 146 return to_layout_impl(\r\n    147     obj,\r\n    148     allow_record=allow_record,\r\n    149     allow_unknown=allow_unknown,\r\n    150     none_policy=none_policy,\r\n    151     use_from_iter=use_from_iter,\r\n    152     primitive_policy=primitive_policy,\r\n    153     string_policy=string_policy,\r\n    154     regulararray=regulararray,\r\n    155 )\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_to_layout.py:177, in _impl(obj, allow_record, allow_unknown, none_policy, regulararray, use_from_iter, primitive_policy, string_policy)\r\n    175     return obj.snapshot()\r\n    176 elif numpy.is_own_array(obj):\r\n--> 177     promoted_layout = ak.operations.from_numpy(\r\n    178         obj,\r\n    179         regulararray=regulararray,\r\n    180         recordarray=True,\r\n    181         highlevel=False,\r\n    182     )\r\n    183     return _handle_array_like(\r\n    184         obj, promoted_layout, primitive_policy=primitive_policy\r\n    185     )\r\n    186 elif Cupy.is_own_array(obj):\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_dispatch.py:39, in named_high_level_function.<locals>.dispatch(*args, **kwargs)\r\n     35 @wraps(func)\r\n     36 def dispatch(*args, **kwargs):\r\n     37     # NOTE: this decorator assumes that the operation is exposed under `ak.`\r\n     38     with OperationErrorContext(name, args, kwargs):\r\n---> 39         gen_or_result = func(*args, **kwargs)\r\n     40         if isgenerator(gen_or_result):\r\n     41             array_likes = next(gen_or_result)\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_from_numpy.py:55, in from_numpy(array, regulararray, recordarray, highlevel, behavior, attrs)\r\n     11 @high_level_function()\r\n     12 def from_numpy(\r\n     13     array,\r\n   (...)\r\n     19     attrs=None,\r\n     20 ):\r\n     21     \"\"\"\r\n     22     Args:\r\n     23         array (np.ndarray): The NumPy array to convert into an Awkward Array.\r\n   (...)\r\n     52     See also #ak.to_numpy and #ak.from_cupy.\r\n     53     \"\"\"\r\n     54     return wrap_layout(\r\n---> 55         from_arraylib(array, regulararray, recordarray),\r\n     56         highlevel=highlevel,\r\n     57         behavior=behavior,\r\n     58     )\r\n\r\nFile ~\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_layout.py:347, in from_arraylib(array, regulararray, recordarray)\r\n    344         return ByteMaskedArray(Index8(mask), data, valid_when=False)\r\n    346 if array.dtype == np.dtype(\"O\"):\r\n--> 347     raise TypeError(\"Awkward Array does not support arrays with object dtypes.\")\r\n    349 if isinstance(array, numpy.ma.MaskedArray):\r\n    350     mask = numpy.ma.getmask(array)\r\n\r\nTypeError: Awkward Array does not support arrays with object dtypes.\r\n\r\nThis error occurred while calling\r\n\r\n    ak.concatenate(\r\n        [array([<STLVector [] at 0x01a12231aa90>, <STLVector [] at 0x01a11c9b...\r\n    )\r\n```\r\n\r\nSo it looks like sometimes it falls back to numpy and than fails by attempting to concatenate the STLVector objects.\r\nHowever, the result is non-deterministic as it sometimes succeeds with creating the awkward array and sometimes it fails.",
  "closed_at":"2024-02-08T16:21:26Z",
  "comments":7,
  "created_at":"2024-01-25T08:54:18Z",
  "id":2099881197,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss59KaTt",
  "number":1101,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Uproot attempts to convert NumPy `dtype=object` array directly into Awkward Array",
  "updated_at":"2024-02-08T16:21:26Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I can get a few branches read in using uproot.dask no problem. As long as I don't have a dask cluster. With a distributed dask cluster it breaks. Here the simplest reproducible example. File is already cached so no need for authentication.\r\n\r\n```\r\nfrom dask.distributed import Client\r\nclient = Client()\r\n\r\nimport dask_awkward as dak\r\nimport uproot\r\n\r\ndef my_name_filter(name):\r\n    return name in [\r\n        \"AnalysisElectronsAuxDyn.pt\",\r\n        \"AnalysisElectronsAuxDyn.eta\",\r\n        \"AnalysisElectronsAuxDyn.phi\",\r\n        \"AnalysisElectronsAuxDyn.m\",\r\n    ]\r\n\r\ntree = uproot.dask(\r\n[{'root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/mc20_13TeV/c3/e9/DAOD_PHYSLITE.34869306._000001.pool.root.1': 'CollectionTree'}],\r\n    filter_name=my_name_filter\r\n)\r\n\r\nak_arr = tree.compute()\r\nak_arr.show()\r\n```\r\n\r\n\r\nHere error:\r\n```\r\n2024-01-25 20:16:57,382 - distributed.protocol.pickle - ERROR - Failed to serialize <ToPickle: HighLevelGraph with 1 layers.\r\n<dask.highlevelgraph.HighLevelGraph object at 0x7fc1c118b550>\r\n 0. from-uproot-8587817a131b75a5ffe8abc755185aa3\r\n>.\r\nTraceback (most recent call last):\r\n  File \"/venv/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 63, in dumps\r\n    result = pickle.dumps(x, **dump_kwargs)\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/venv/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 68, in dumps\r\n    pickler.dump(x)\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/venv/lib/python3.9/site-packages/distributed/protocol/pickle.py\", line 81, in dumps\r\n    result = cloudpickle.dumps(x, **dump_kwargs)\r\n  File \"/venv/lib/python3.9/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\r\n    cp.dump(obj)\r\n  File \"/venv/lib/python3.9/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\r\n    return super().dump(obj)\r\nTypeError: cannot pickle '_thread.lock' object\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nFile /venv/lib/python3.9/site-packages/distributed/protocol/pickle.py:63, in dumps(x, buffer_callback, protocol)\r\n     62 try:\r\n---> 63     result = pickle.dumps(x, **dump_kwargs)\r\n     64 except Exception:\r\n\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nFile /venv/lib/python3.9/site-packages/distributed/protocol/pickle.py:68, in dumps(x, buffer_callback, protocol)\r\n     67 buffers.clear()\r\n---> 68 pickler.dump(x)\r\n     69 result = f.getvalue()\r\n\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nFile /venv/lib/python3.9/site-packages/distributed/protocol/serialize.py:353, in serialize(x, serializers, on_error, context, iterate_collection)\r\n    352 try:\r\n--> 353     header, frames = dumps(x, context=context) if wants_context else dumps(x)\r\n    354     header[\"serializer\"] = name\r\n\r\nFile /venv/lib/python3.9/site-packages/distributed/protocol/serialize.py:76, in pickle_dumps(x, context)\r\n     74     writeable.append(not f.readonly)\r\n---> 76 frames[0] = pickle.dumps(\r\n     77     x,\r\n     78     buffer_callback=buffer_callback,\r\n     79     protocol=context.get(\"pickle-protocol\", None) if context else None,\r\n     80 )\r\n     81 header = {\r\n     82     \"serializer\": \"pickle\",\r\n     83     \"writeable\": tuple(writeable),\r\n     84 }\r\n\r\nFile /venv/lib/python3.9/site-packages/distributed/protocol/pickle.py:81, in dumps(x, buffer_callback, protocol)\r\n     80     buffers.clear()\r\n---> 81     result = cloudpickle.dumps(x, **dump_kwargs)\r\n     82 except Exception:\r\n\r\nFile /venv/lib/python3.9/site-packages/cloudpickle/cloudpickle.py:1479, in dumps(obj, protocol, buffer_callback)\r\n   1478 cp = Pickler(file, protocol=protocol, buffer_callback=buffer_callback)\r\n-> 1479 cp.dump(obj)\r\n   1480 return file.getvalue()\r\n\r\nFile /venv/lib/python3.9/site-packages/cloudpickle/cloudpickle.py:1245, in Pickler.dump(self, obj)\r\n   1244 try:\r\n-> 1245     return super().dump(obj)\r\n   1246 except RuntimeError as e:\r\n\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[3], line 16\r\n      3     return name in [\r\n      4         \"AnalysisElectronsAuxDyn.pt\",\r\n      5         \"AnalysisElectronsAuxDyn.eta\",\r\n      6         \"AnalysisElectronsAuxDyn.phi\",\r\n      7         \"AnalysisElectronsAuxDyn.m\",\r\n      8     ]\r\n     10 tree = uproot.dask(\r\n     11     [{'root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/mc20_13TeV/c3/e9/DAOD_PHYSLITE.34869306._000001.pool.root.1': 'CollectionTree'}],\r\n     12     filter_name=my_name_filter,\r\n     13     # library='ak'\r\n     14 )\r\n---> 16 ak_arr = tree.compute()\r\n     17 ak_arr.show()\r\n\r\nFile /venv/lib/python3.9/site-packages/dask/base.py:342, in DaskMethodsMixin.compute(self, **kwargs)\r\n    318 def compute(self, **kwargs):\r\n    319     \"\"\"Compute this dask collection\r\n    320 \r\n    321     This turns a lazy Dask collection into its in-memory equivalent.\r\n   (...)\r\n    340     dask.compute\r\n    341     \"\"\"\r\n--> 342     (result,) = compute(self, traverse=False, **kwargs)\r\n    343     return result\r\n\r\nFile /venv/lib/python3.9/site-packages/dask/base.py:628, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    625     postcomputes.append(x.__dask_postcompute__())\r\n    627 with shorten_traceback():\r\n--> 628     results = schedule(dsk, keys, **kwargs)\r\n    630 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile /venv/lib/python3.9/site-packages/distributed/protocol/serialize.py:379, in serialize(x, serializers, on_error, context, iterate_collection)\r\n    377     except Exception:\r\n    378         raise TypeError(msg) from exc\r\n--> 379     raise TypeError(msg, str_x) from exc\r\n    380 else:  # pragma: nocover\r\n    381     raise ValueError(f\"{on_error=}; expected 'message' or 'raise'\")\r\n\r\nTypeError: ('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 1 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7fc1c118b550>\\n 0. from-uproot-8587817a131b75a5ffe8abc755185aa3\\n>')\r\n```",
  "closed_at":"2024-01-26T14:31:03Z",
  "comments":7,
  "created_at":"2024-01-25T19:17:56Z",
  "id":2101015461,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss59OvOl",
  "number":1102,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"With dask cluster uproot.dask breaks",
  "updated_at":"2024-01-26T14:31:03Z",
  "user":"MDQ6VXNlcjE3MzY5ODQ="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2024-01-26T14:31:02Z",
  "comments":2,
  "created_at":"2024-01-25T20:58:00Z",
  "draft":false,
  "id":2101156524,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5lGxuw",
  "number":1103,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-26T14:31:02Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: any Locks in Models must be transient",
  "updated_at":"2024-01-26T14:31:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"This is a clone of https://github.com/dask-contrib/dask-awkward/pull/459 adapted to uproot.",
  "closed_at":"2024-01-26T17:37:29Z",
  "comments":3,
  "created_at":"2024-01-25T23:19:36Z",
  "draft":false,
  "id":2101323821,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5lHVqX",
  "number":1104,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-26T17:37:29Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: better path handling in uproot.dask_write",
  "updated_at":"2024-01-26T17:37:29Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2024-01-26T19:07:29Z",
  "comments":1,
  "created_at":"2024-01-26T10:31:11Z",
  "draft":false,
  "id":2101972088,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5lJYGv",
  "number":1105,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-26T19:07:29Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: add dask_write to read-the-docs",
  "updated_at":"2024-01-26T19:07:30Z",
  "user":"MDQ6VXNlcjcwNDQxNjQx"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"When packaging the [latest version (5.2.2) on nixpkgs](https://github.com/NixOS/nixpkgs/pull/284112), I noticed that the following tests were failing on MacOS (both `x86_64` and `aarch64`) while they work fine on linux:\r\n\r\n```\r\nERROR tests/test_0692_fsspec_writing.py::test_fsspec_writing_http - socket.gaierror: [Errno 8] nodename nor servname provided, or not known\r\nFAILED tests/test_0692_fsspec_reading.py::test_open_fsspec_ssh[FSSpecSource] - socket.gaierror: [Errno 8] nodename nor servname provided, or not known\r\nFAILED tests/test_0692_fsspec_reading.py::test_open_fsspec_ssh[None] - socket.gaierror: [Errno 8] nodename nor servname provided, or not known\r\nFAILED tests/test_0692_fsspec_writing.py::test_fsspec_writing_ssh[ssh://] - socket.gaierror: [Errno 8] nodename nor servname provided, or not known\r\nFAILED tests/test_0692_fsspec_writing.py::test_fsspec_writing_ssh[simplecache::ssh://] - socket.gaierror: [Errno 8] nodename nor servname provided, or not known\r\n```\r\n\r\nHere is the full stacktrace for one of them:\r\n```\r\n/nix/store/3c4cf1cinrh8lgqnx7vz9wbcm4rjirir-python3.11-paramiko-3.3.1/lib/python3.11/site-packages/paramiko/client.py:377: in connect\r\n    to_try = list(self._families_and_addresses(hostname, port))\r\n        allow_agent = True\r\n        auth_strategy = None\r\n        auth_timeout = None\r\n        banner_timeout = None\r\n        channel_timeout = None\r\n        compress   = False\r\n        disabled_algorithms = None\r\n        errors     = {}\r\n        gss_auth   = False\r\n        gss_deleg_creds = True\r\n        gss_host   = None\r\n        gss_kex    = False\r\n        gss_trust_dns = True\r\n        hostname   = 'localhost'\r\n        key_filename = None\r\n        look_for_keys = True\r\n        passphrase = None\r\n        password   = None\r\n        pkey       = None\r\n        port       = 22\r\n        self       = <paramiko.client.SSHClient object at 0x167642990>\r\n        sock       = None\r\n        timeout    = None\r\n        transport_factory = None\r\n        username   = '_nixbld1'\r\n/nix/store/3c4cf1cinrh8lgqnx7vz9wbcm4rjirir-python3.11-paramiko-3.3.1/lib/python3.11/site-packages/paramiko/client.py:202: in _families_and_addresses\r\n    addrinfos = socket.getaddrinfo(\r\n        guess      = True\r\n        hostname   = 'localhost'\r\n        port       = 22\r\n        self       = <paramiko.client.SSHClient object at 0x167642990>\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nhost = 'localhost', port = 22, family = <AddressFamily.AF_UNSPEC: 0>\r\ntype = <SocketKind.SOCK_STREAM: 1>, proto = 0, flags = 0\r\n\r\n    def getaddrinfo(host, port, family=0, type=0, proto=0, flags=0):\r\n        \"\"\"Resolve host and port into list of address info entries.\r\n    \r\n        Translate the host/port argument into a sequence of 5-tuples that contain\r\n        all the necessary arguments for creating a socket connected to that service.\r\n        host is a domain name, a string representation of an IPv4/v6 address or\r\n        None. port is a string service name such as 'http', a numeric port number or\r\n        None. By passing None as the value of host and port, you can pass NULL to\r\n        the underlying C API.\r\n    \r\n        The family, type and proto arguments can be optionally specified in order to\r\n        narrow the list of addresses returned. Passing zero as a value for each of\r\n        these arguments selects the full range of results.\r\n        \"\"\"\r\n        # We override this function since we want to translate the numeric family\r\n        # and socket type values to enum constants.\r\n        addrlist = []\r\n>       for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\nE       socket.gaierror: [Errno 8] nodename nor servname provided, or not known\r\n\r\naddrlist   = []\r\nfamily     = <AddressFamily.AF_UNSPEC: 0>\r\nflags      = 0\r\nhost       = 'localhost'\r\nport       = 22\r\nproto      = 0\r\ntype       = <SocketKind.SOCK_STREAM: 1>\r\n```\r\n",
  "closed_at":null,
  "comments":0,
  "created_at":"2024-01-26T22:56:07Z",
  "id":2103027347,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss59WaaT",
  "number":1108,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"[MacOS] socket.gaierror: [Errno 8] nodename nor servname provided, or not known",
  "updated_at":"2024-01-26T22:56:34Z",
  "user":"MDQ6VXNlcjMzMDU4NzQ3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/psf/black-pre-commit-mirror: 23.12.1 \u2192 24.1.1](https://github.com/psf/black-pre-commit-mirror/compare/23.12.1...24.1.1)\n<!--pre-commit.ci end-->",
  "closed_at":"2024-02-01T15:21:59Z",
  "comments":0,
  "created_at":"2024-01-29T19:30:55Z",
  "draft":false,
  "id":2106264615,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5lXg1l",
  "number":1110,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-01T15:21:59Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2024-02-01T15:22:00Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"> Actually, yes! One thing uproot is often doing is decompressing many small chunks and then concatenating them into a larger contiguous buffer. We could save some additional allocation and copy time if we can decompress into a buffer at an arbitrary offset.\r\n\r\n_Originally posted by @nsmith- in https://github.com/scikit-hep/uproot5/issues/1089#issuecomment-1900706532_\r\n\r\nIf this has to bypass the `basket_array` concatenation, it is a big-project.",
  "closed_at":null,
  "comments":0,
  "created_at":"2024-01-30T16:49:44Z",
  "id":2108319442,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss59qmbS",
  "number":1112,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Use cramjam to fill an allocated array with `uproot.AsDtypeInPlace`",
  "updated_at":"2024-01-30T16:50:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"(Tests should not create any non-temporary files.)",
  "closed_at":"2024-02-09T15:02:15Z",
  "comments":0,
  "created_at":"2024-01-30T21:42:10Z",
  "id":2108835281,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss59skXR",
  "number":1113,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Something in the test suite creates files named `\"\\\\/\"` and `tmp/`.",
  "updated_at":"2024-02-09T15:02:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Thi PR solves issues\r\n* https://github.com/scikit-hep/uproot5/issues/1101 \r\n* and 4/6 errors in  https://github.com/scikit-hep/uproot5/issues/951 i.e. the TypeErrors: 'Awkward Array does not support arrays with object dtypes.'",
  "closed_at":"2024-02-08T16:21:24Z",
  "comments":0,
  "created_at":"2024-02-01T15:58:56Z",
  "draft":false,
  "id":2112858544,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5luDRq",
  "number":1114,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-08T16:21:24Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: attempt to concatenate numpy and awkward arrays",
  "updated_at":"2024-02-08T16:21:25Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This is still work in progress. I'll update this comment to when there are new changes.\r\n\r\nCurrent status:\r\n- Seems like everything is read correctly (in a very simple test), but the column logic is broken.\n- The writing part hasn't been updated.",
  "closed_at":null,
  "comments":0,
  "created_at":"2024-02-01T17:52:27Z",
  "draft":true,
  "id":2113094266,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5lu3sO",
  "number":1115,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"feat: add support for RNTuple RC2",
  "updated_at":"2024-02-07T21:01:23Z",
  "user":"MDQ6VXNlcjc1OTY4Mzc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"In uproot 5.1.2, I was able to parallelize the reading of ROOT files with the following example below.  However, with the new release `uproot 5.2.2`, I get this error: \r\n```python\r\nAttributeError(\"'FSSpecSource' object has no attribute '_fh'\")\r\n```\r\n\r\n```python\r\nimport numpy as np\r\nimport uproot\r\nimport yaml\r\nfrom numpy.typing import NDArray\r\n\r\nfrom dask.base import compute\r\nfrom dask.delayed import delayed\r\nfrom dask.distributed import Client\r\n\r\ncli = Client(n_workers=5)\r\n\r\n@delayed\r\ndef get_array( bin_id: str, inmap: uproot.ReadOnlyDirectory) -> tuple[str, NDArray[np.float64]]:\r\n    outdata_bin = inmap[f\"nHit{bin_id.zfill(2)}/data/count\"].array().to_numpy()\r\n    return bin_id, outdata_bin\r\n\r\n@delayed\r\ndef get_bkg( bin_id: str, inmap: uproot.ReadOnlyDirectory) -> tuple[str, NDArray[np.float64]]:\r\n    outdata_bin = inmap[f\"nHit{bin_id.zfill(2)}/bkg/count\"].array().to_numpy()\r\n    return bin_id, outdata_bin\r\n\r\nmaptree=\"./maptree.root\"\r\n\r\nwith uproot.open(maptree) as inmap:\r\n    analysis_bin_names = inmap[\"BinInfo/name\"].array().to_numpy().astype(str)\r\n\r\n    tasks = [get_array(bin_id, inmap) for bin_id in analysis_bin_names]\r\n    tasks_bkg = [get_bkg(bin_id, inmap) for bin_id in analysis_bin_names]\r\n\r\n    counts_info = compute(*tasks)\r\n    bkg_info = compute(*tasks_bkg)\r\n    data = dict(counts_info)\r\n    bkg = dict(bkg_info)\r\ncli.close()\r\n```\r\n\r\nAn example of the file can be retrieved here: https://data.hawc-observatory.org/datasets/geminga2017/geminga2017-download/maptree.root\r\n\r\nAdditionally, now with the new release I try to parallelize reading larger of +1GB ROOT files and what it used to take < 1 minute now runs for +3 minutes. ",
  "closed_at":"2024-02-06T16:31:49Z",
  "comments":6,
  "created_at":"2024-02-05T18:31:40Z",
  "id":2119211439,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5-UJmv",
  "number":1117,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Issue with parallelization for 5.2.2",
  "updated_at":"2024-02-06T16:31:50Z",
  "user":"MDQ6VXNlcjgyNjkzMDI0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"I have not tested this as a fix for #1117, but it seemed like an obviously missing thing that should be filled in anyway.",
  "closed_at":"2024-02-06T16:31:04Z",
  "comments":0,
  "created_at":"2024-02-05T19:09:56Z",
  "draft":false,
  "id":2119277508,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5mEBn-",
  "number":1118,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-06T16:31:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: missing '_fh' and '_file' attributes after unpickling",
  "updated_at":"2024-02-06T16:31:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/astral-sh/ruff-pre-commit: v0.1.14 \u2192 v0.2.1](https://github.com/astral-sh/ruff-pre-commit/compare/v0.1.14...v0.2.1)\n<!--pre-commit.ci end-->",
  "closed_at":"2024-02-14T17:45:43Z",
  "comments":1,
  "created_at":"2024-02-05T19:23:17Z",
  "draft":false,
  "id":2119297529,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5mEF_l",
  "number":1119,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-14T17:45:43Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2024-02-14T17:45:44Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":null,
  "comments":1,
  "created_at":"2024-02-05T19:53:19Z",
  "draft":false,
  "id":2119349107,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5mERfU",
  "number":1120,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"feat: expose decompression_executor and interpretation_executor in uproot dask",
  "updated_at":"2024-02-09T14:53:52Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjcwNDQxNjQx",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This docstring:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/6e3d3130c218ab7318e3610c28717c8fdaa6f89c/src/uproot/writing/_dask_write.py#L60-L92\r\n\r\nshould get passed through to the Uproot documentation. The documentation was last built 3 hours ago, so it's not stalled again (#1083). It probably just needs to be added to [prepare_docstrings.py](https://github.com/scikit-hep/uproot5/blob/main/docs-sphinx/prepare_docstrings.py) somewhere.",
  "closed_at":"2024-02-07T21:33:32Z",
  "comments":0,
  "created_at":"2024-02-06T19:42:04Z",
  "id":2121542696,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5-dCwo",
  "number":1121,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"`uproot.dask_write` is missing from documentation",
  "updated_at":"2024-02-07T21:33:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2024-02-07T21:33:31Z",
  "comments":1,
  "created_at":"2024-02-06T20:13:13Z",
  "draft":false,
  "id":2121589416,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5mL89j",
  "number":1122,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-07T21:33:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: fix dask_write docs",
  "updated_at":"2024-02-07T21:33:32Z",
  "user":"MDQ6VXNlcjcwNDQxNjQx"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"In testing fsspec writing for configurations in scheme + slash_prefix a \"\\\\\" directory gets made with what should be temporary files. \r\n\r\nMoving to the tmp_path before creating the file doesn't work, thus we can simply remove the newly created folder. \r\n\r\nWe can do this immediately after testing each file (`test_fsspec_writing_local_uri` in `test_0692_fsspec_writing`) or after all the versions of that test have been executed. \r\n\r\n ",
  "closed_at":"2024-02-09T15:02:15Z",
  "comments":0,
  "created_at":"2024-02-07T13:28:51Z",
  "draft":false,
  "id":2123041496,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5mQ0di",
  "number":1123,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-09T15:02:14Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: test suite creates files not in tmp_path",
  "updated_at":"2024-02-09T15:02:15Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"NONE",
  "body":"I am encountering an issue when using  `multiprocessing.Pool` for reading large ROOT files  with `uproot 5.2.2`. The file is never read and the process just hangs for minutes. \r\n\r\nWith `uproot 5.1.2`, the file is read within less than a minute. For your convenience, here's a link to a sample ROOT file: https://www.dropbox.com/scl/fi/60cff3r4bu6oly87axwig/sample.root?rlkey=irzcxsykn6a6lmcrlj2d3z2od&dl=0\r\n\r\n```python\r\nimport multiprocessing\r\nimport uproot as up\r\nfrom numpy.typing import NDArray\r\n\r\nndarray = NDArray[np.float64]\r\n\r\ndef parallel_func(\r\n    bin_id: str, root_directory: up.ReadOnlyDirectory\r\n) -> tuple[str, ndarray]:\r\n\r\n    directory_key = data_bin_defintion(bin_id)\r\n    return bin_id, root_directory[directory_key].array().to_numpy()  # type: ignore\r\n\r\ndef read_sample_maptree(file_name: str) -> dict[str, ndarray]:\r\n    with up.open(file_name) as infile:  # type: ignore\r\n        data = {}\r\n        analysis_bin_ids: NDArray[np.str_] = infile[\"BinInfo/name\"].array()  # type: ignore\r\n\r\n        parallalel_args = [(bin_id, infile) for bin_id in analysis_bin_ids]\r\n\r\n        with multiprocessing.Pool(processes=5) as pool:\r\n            results = list(pool.starmap(parallel_func, parallalel_args))\r\n\r\n        data = dict(results)\r\n    return data\r\n\r\nindata = read_sample_maptree(configured_args.outfile)\r\nprint(indata.keys())\r\n```\r\nHere's a quick look at how long it takes with `uproot 5.1.2`\r\n```python\r\n  _     ._   __/__   _ _  _  _ _/_   Recorded: 17:18:38  Samples:  1\r\n /_//_/// /_\\ / //_// / //_'/ //     Duration: 5.003     CPU time: 4.975\r\n/   _/                      v4.6.2\r\n\r\nProgram: simulate_maptree.py -o sample.root\r\n\r\n4.907 <module>  simulate_maptree.py:1\r\n\u2514\u2500 4.907 main  simulate_maptree.py:117\r\n   \u2514\u2500 4.907 read_sample_maptree  simulate_maptree.py:100\r\n      \u2514\u2500 4.907 Pool.starmap  multiprocessing/pool.py:369\r\n            [6 frames hidden]  multiprocessing, threading, <built-in>\r\n               4.907 lock.acquire  <built-in>\r\n```\r\nLet me know if you need more information from my side. Thanks in advance!",
  "closed_at":"2024-02-09T20:58:09Z",
  "comments":5,
  "created_at":"2024-02-08T09:39:50Z",
  "id":2124726580,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5-pME0",
  "number":1124,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Parallel reading of ROOT file with 5.2.2 just hangs and nothings is read",
  "updated_at":"2024-02-09T20:58:09Z",
  "user":"MDQ6VXNlcjgyNjkzMDI0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"It frequently needs to be restarted manually, as the only test that failed (due to a GitHub Actions timeout). Watching it interactively, it does sometimes take a long time. Especially this one:\r\n\r\n```\r\ntests/test_0692_fsspec_reading.py::test_fsspec_globbing_xrootd[FSSpecSource-root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_*.root:Events]\r\n```\r\n\r\nMaybe we need to drop this test. The server just isn't reliable or responsive enough.",
  "closed_at":null,
  "comments":0,
  "created_at":"2024-02-14T17:18:51Z",
  "id":2134817435,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5_Prqb",
  "number":1125,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"open",
  "state_reason":null,
  "title":"The `tests/test_0692_fsspec_reading.py::test_fsspec_globbing_xrootd` test sometimes takes forever",
  "updated_at":"2024-02-14T17:18:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This adds fixes the infinite recursion issue when trying to add keys which have a colon in their name: \r\n\r\nhttps://github.com/scikit-hep/uproot5/issues/974\r\n\r\nIf we have a file with a key name \"one:two\", but also a TTree \"one\" which contains a \"two\" named branch:\r\n\r\n```\r\n with uproot.recreate(newfile) as f:\r\n        f[\"one:two\"] = \"together\"\r\n        array = ak.Array([\"one\", \"two\", \"three\"])\r\n        f[\"one\"] = {\"two\": array}\r\n\r\n```\r\n the behaviour will be the following when reading the two:\r\n```\r\n with uproot.open(newfile) as f:\r\n        f[\"one:two\"] == \"together\"\r\n        f[\"one\"][\"two\"].array() == [\"one\", \"two\", \"three\"]\r\n\r\n```",
  "closed_at":"2024-02-15T17:15:19Z",
  "comments":0,
  "created_at":"2024-02-15T15:50:46Z",
  "draft":false,
  "id":2136875993,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5m_uIP",
  "number":1127,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-15T17:15:19Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: allow colon in key names",
  "updated_at":"2024-02-15T17:15:19Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Commit ec80b887 broke support for writing histograms that fulfill the Unified Histogram Interface, but are neither from hist nor from boost_histogram. UHI compatible histograms always have a function `variances`, but do not have a `storage_type` attribute, so that attribute should only be checked if the histogram is indeed from boost_histogram.",
  "closed_at":"2024-02-19T14:13:22Z",
  "comments":4,
  "created_at":"2024-02-16T13:28:05Z",
  "draft":false,
  "id":2138626345,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5nFpP4",
  "number":1128,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-19T14:13:22Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: allow writing generic UHI-compatible histograms",
  "updated_at":"2024-02-19T14:14:29Z",
  "user":"MDQ6VXNlcjE2NDAzODY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"### The problem\r\n\r\nTrying to save ntuple (TTree) with more than 2 GB of data and **no compression** fails with following error:\r\n\r\n```\r\nerror: 'i' format requires -2147483648 <= number <= 2147483647\r\n```\r\n\r\nThe minimum code to reproduce:\r\n```\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport uproot\r\ndata_dict = {\r\n        \"x\": np.ones(1000_000_000, dtype=np.float64),\r\n}\r\nwith uproot.recreate(Path('file.root'), compression=None) as fout:\r\n    fout[\"tree\"] = data_dict\r\n```\r\n\r\n### Details\r\n\r\nMore details with my original code from which the problem is below. In the comments below I've also provided more examples.\r\n\r\nI was trying to write an ROOT ntuple with following code:\r\n\r\n```\r\nimport time\r\nfrom pathlib import Path\r\nimport h5py\r\nfrom hdf import peak_count\r\nimport uproot\r\n\r\ndef convert_hdf_to_ntuple(input_path: Path):\r\n    ntuple_path = input_path.with_suffix('.root')\r\n    print(f\"Saving ntuple to {ntuple_path}\")\r\n    before_write = time.time()\r\n    ntuple_path.unlink(missing_ok=True)\r\n\r\n    uproot.create(ntuple_path, compression=None)\r\n\r\n    file = uproot.reading.ReadOnlyFile(ntuple_path)\r\n    print(f\"file 64 bit (check via file.is_64bit) {file.is_64bit}\")\r\n    \r\n    for channel_no in range(4):\r\n        with h5py.File(input_path, 'r') as f, uproot.update(ntuple_path) as fout:\r\n            print(f\"Processing channel {channel_no}\")\r\n            gain_mV = f[f'channel_{channel_no}'].attrs['gain_mV']\r\n            offset_mV = f[f'channel_{channel_no}'].attrs['offset_mV']\r\n            horiz_interval_ns = f[f'channel_{channel_no}'].attrs['horiz_interval_ns']\r\n            fout[f'channel_{channel_no}/gain_mV'] = str(gain_mV)\r\n            fout[f'channel_{channel_no}/offset_mV'] = str(offset_mV)\r\n            fout[f'channel_{channel_no}/horiz_interval_ns'] = str(horiz_interval_ns)\r\n\r\n            peaks_in_bucket = 10000000\r\n            for peak_type in ['positive', 'negative']:\r\n                print(f\"Processing {peak_type} peaks\")\r\n                total_number_of_peaks = peak_count(f, channel_no, peak_type)\r\n                for i in range(0, total_number_of_peaks, peaks_in_bucket):\r\n                    dict_bucket = {}\r\n                    for name, dataset in f[f'channel_{channel_no}/{peak_type}'].items():\r\n                        dict_bucket[name] = dataset[i:i + peaks_in_bucket]\r\n                    dict_bucket['peak_value_mV'] = dict_bucket['peak_value'] * gain_mV\r\n                    dict_bucket['peak_length_ns'] = dict_bucket['peak_length'] * horiz_interval_ns\r\n                    dict_bucket['peak_start_us'] = dict_bucket['peak_start'] * horiz_interval_ns / 1000\r\n                    dict_bucket['peak_cfd_us'] = dict_bucket['peak_cfd_index'] * horiz_interval_ns / 1000\r\n                    dict_bucket['peak_rise_ns'] = dict_bucket['rise_time'] * horiz_interval_ns\r\n                    dict_bucket['peak_area_ns_mV'] = dict_bucket['peak_area'] * horiz_interval_ns * gain_mV\r\n                    dict_bucket['peak_baseline_mV'] = dict_bucket['peak_baseline'] * gain_mV - offset_mV\r\n                    dict_bucket['peak_noise_mV'] = dict_bucket['peak_noise'] * gain_mV\r\n                    dict_bucket['peak_fwhm_ns'] = dict_bucket['peak_fwhm'] * horiz_interval_ns\r\n\r\n                    del dict_bucket['peak_value']\r\n                    del dict_bucket['peak_length']\r\n                    del dict_bucket['peak_area']\r\n                    del dict_bucket['peak_cfd_index']\r\n                    del dict_bucket['rise_time']\r\n                    del dict_bucket['peak_baseline']\r\n                    del dict_bucket['peak_noise']\r\n                    del dict_bucket['peak_fwhm']\r\n                    if i == 0:\r\n                        fout[f'channel_{channel_no}/{peak_type}'] = dict_bucket\r\n                    else:\r\n                        fout[f'channel_{channel_no}/{peak_type}'].extend(dict_bucket)\r\n                    print(f\"num entries {fout[f'channel_{channel_no}/{peak_type}'].num_entries} , num baskets {fout[f'channel_{channel_no}/{peak_type}'].num_baskets}\")\r\n\r\n    after_write = time.time()\r\n    print(f\"Writing took {after_write - before_write:.3f} s\")\r\n```\r\n\r\nThis works nicely until files are small, say smaller than 2GB.\r\n\r\nWhen trying to save larger file I get following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/net/people/plgrid/plgkongruencj/2022-krakow-lgad/src/convert_from_lv1_to_lv2.py\", line 146, in <module>\r\n    main()\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/net/people/plgrid/plgkongruencj/2022-krakow-lgad/src/convert_from_lv1_to_lv2.py\", line 134, in main\r\n    convert_hdf_to_ntuple(input_path)\r\n  File \"/net/people/plgrid/plgkongruencj/2022-krakow-lgad/src/root.py\", line 58, in convert_hdf_to_ntuple\r\n    fout[f'channel_{channel_no}/{peak_type}'] = dict_bucket\r\n    ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py\", line 984, in __setitem__\r\n    self.update({where: what})\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py\", line 1555, in update\r\n    uproot.writing.identify.add_to_directory(v, name, directory, streamers)\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py\", line 152, in add_to_directory\r\n    tree.extend(data)\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py\", line 1834, in extend\r\n    self._cascading.extend(self._file, self._file.sink, data)\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py\", line 816, in extend\r\n    totbytes, zipbytes, location = self.write_np_basket(\r\n                                   ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py\", line 1427, in write_np_basket\r\n    self._freesegments.write(sink)\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascade.py\", line 782, in write\r\n    super().write(sink)\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascade.py\", line 132, in write\r\n    dependency.write(sink)\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascade.py\", line 102, in write\r\n    tmp = self.serialize()\r\n          ^^^^^^^^^^^^^^^^\r\n  File \"/memfs/7649613/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascade.py\", line 377, in serialize\r\n    format.pack(\r\nstruct.error: 'i' format requires -2147483648 <= number <= 2147483647\r\n```\r\n\r\nI saw similar error reported long ago here: https://github.com/scikit-hep/uproot3/issues/462\r\n\r\nAlso - when looking at the source code of `extend` method in `class NTuple(CascadeNode)` it seems that all calls to `add_rblob` are with `big=False` argument. which suggest that only 4-byte pointers are being used.\r\n\r\nSee:\r\nhttps://github.com/scikit-hep/uproot5/blob/8a42e7de07a74a81e300410bfcc2b1c5b3339820/src/uproot/writing/_cascadentuple.py#L779\r\n\r\nThis is my uproot version:\r\n\r\n```python\r\nPython 3.11.3 (main, Nov 19 2023, 23:25:18) [GCC 12.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'5.2.2'\r\n```\r\n",
  "closed_at":"2024-02-19T15:59:21Z",
  "comments":4,
  "created_at":"2024-02-16T21:16:13Z",
  "id":2139499026,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5_hioS",
  "number":1130,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Writing ntuple larger than 2GB fails when no compression is used",
  "updated_at":"2024-02-19T15:59:22Z",
  "user":"MDQ6VXNlcjczNzQ3MzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @YSelfTool as a contributor for code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot5/pull/1128#issuecomment-1952538757)\n\n[skip ci]",
  "closed_at":"2024-02-19T14:14:43Z",
  "comments":0,
  "created_at":"2024-02-19T14:14:24Z",
  "draft":false,
  "id":2142471228,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5nSzm2",
  "number":1131,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-02-19T14:14:43Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add YSelfTool as a contributor for code",
  "updated_at":"2024-02-19T14:14:44Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 }
]
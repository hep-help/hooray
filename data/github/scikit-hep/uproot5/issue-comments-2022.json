[
 {
  "author_association":"CONTRIBUTOR",
  "body":"Adding this in here to make sure we don't lose useful information: https://root-forum.cern.ch/t/how-to-enable-tbufferfile-kstreamedmemberwise-for-specific-branches-in-a-ttree/43788/6 .",
  "created_at":"2022-06-24T15:39:22Z",
  "id":1165695069,
  "issue":38,
  "node_id":"IC_kwDOD6Q_ss5Fexhd",
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "total_count":1
  },
  "updated_at":"2022-06-24T15:39:22Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"NONE",
  "body":"I stumbled across this issue today and I am more confused than ever.\r\n\r\nTL;DR version: I can read in `TEfficiency` if I open `skhep_testdata.data_path(\"uproot-issue38c.root\")` before my file.\r\n\r\n### does not work\r\n```python\r\nimport uproot\r\nimport skhep_testdata\r\n\r\nwith uproot.open(skhep_testdata.data_path(\"uproot-issue209.root\")) as fp:\r\n    eff = fp[\"TEfficiencyName\"]\r\n    print(eff)\r\n```\r\n\r\n### but this works\r\n```python\r\nimport uproot\r\nimport skhep_testdata\r\n\r\nwith uproot.open(skhep_testdata.data_path(\"uproot-issue38c.root\")) as fp:\r\n    hist = fp[\"TEfficiencyName\"] # need to load the TEfficiency\r\n\r\nwith uproot.open(skhep_testdata.data_path(\"uproot-issue209.root\")) as fp:\r\n    eff = fp[\"TEfficiencyName\"]\r\n    print(eff)\r\n```\r\n\r\nWhat kind of black magic is included in \"uproot-issue38c.root\" and how can I add to my files?",
  "created_at":"2022-09-29T16:31:55Z",
  "id":1262523927,
  "issue":38,
  "node_id":"IC_kwDOD6Q_ss5LQJYX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-29T16:31:55Z",
  "user":"MDQ6VXNlcjEyMTMyNzY="
 },
 {
  "author_association":"MEMBER",
  "body":"There is a global state change when you open a file (i.e. the black magic). There's a global `uproot.classes` dict with Python Models for C++ class name-version pairs, such as TEfficiency version XYZ. When trying to read an instance of the class from a file, it first uses the Model in the global `uproot.classes`, which defines some deserialization procedure. If that deserialization procedure fails, it then tries reading the specific file's TStreamerInfo, which encodes deserialization procedures for each class name-version in the file (maybe\u2014some TStreamerInfos are missing some classes). If the second try fails, you get an error message. New class name-version combinations are added to the `uproot.classes` dict when they're learned.\r\n\r\nAlthough you'd think that a particular class name-version pair would always have the same deserialization procedure, maybe the file was made with a custom-compiled version of ROOT, which has new C++ members added to a class without a new version number, or maybe the file was hadd'ed with another that does, etc. That's why we have a try, try-again procedure, and even that might fail if it's weird enough and doesn't declare its weirdness.\r\n\r\nTo get more insight into what's going on in this case, you can look at\r\n\r\n```python\r\nfp.file.show_streamers(\"TEfficiency\")\r\n```\r\n\r\n([uproot.ReadOnlyFile.show_streamers](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyFile.html#show-streamers)) to see if there are different versions of some class (maybe one of the classes TEfficiency inherits from or contains) or if they have the same version but nevertheless different deserialization procedures (described as a sequence of member data and their types).",
  "created_at":"2022-09-29T16:48:26Z",
  "id":1262541640,
  "issue":38,
  "node_id":"IC_kwDOD6Q_ss5LQNtI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-29T16:48:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim,\r\n\r\nThank you for the explanation. I've been trying to figure out the differences between the two files since I wanted to rewrite my old files with newer ROOT to add the streamers (if there is a way).\r\n\r\n> fp.file.show_streamers(\"TEfficiency\")\r\n\r\nUnfortunately, the output of that line is identical for both `skhep_testdata.data_path(\"uproot-issue209.root\")` and `skhep_testdata.data_path(\"uproot-issue38c.root\")`. There must be another difference between these two files.\r\nAccording the the [uproot test_0038-memberwise-serialization.py](https://github.com/scikit-hep/uproot5/blob/main/tests/test_0038-memberwise-serialization.py), `uproot-issue209.root` should not contain any streamers (at least it fails also without `reset_classes`), yet `fp.file.show_streamers(\"TEfficiency\")` reports them.\r\n\r\n<details>\r\n  <summary>output of fp.file.show_streamers(\"TEfficiency\") for uproot-issue209.root</summary>\r\n<pre>\r\nTHashList (v0): TList (v5)\r\n\r\nTAttAxis (v4)\r\n    fNdivisions: int (TStreamerBasicType)\r\n    fAxisColor: short (TStreamerBasicType)\r\n    fLabelColor: short (TStreamerBasicType)\r\n    fLabelFont: short (TStreamerBasicType)\r\n    fLabelOffset: float (TStreamerBasicType)\r\n    fLabelSize: float (TStreamerBasicType)\r\n    fTickLength: float (TStreamerBasicType)\r\n    fTitleOffset: float (TStreamerBasicType)\r\n    fTitleSize: float (TStreamerBasicType)\r\n    fTitleColor: short (TStreamerBasicType)\r\n    fTitleFont: short (TStreamerBasicType)\r\n\r\nTAxis (v10): TNamed (v1), TAttAxis (v4)\r\n    fNbins: int (TStreamerBasicType)\r\n    fXmin: double (TStreamerBasicType)\r\n    fXmax: double (TStreamerBasicType)\r\n    fXbins: TArrayD (TStreamerObjectAny)\r\n    fFirst: int (TStreamerBasicType)\r\n    fLast: int (TStreamerBasicType)\r\n    fBits2: unsigned short (TStreamerBasicType)\r\n    fTimeDisplay: bool (TStreamerBasicType)\r\n    fTimeFormat: TString (TStreamerString)\r\n    fLabels: THashList* (TStreamerObjectPointer)\r\n    fModLabs: TList* (TStreamerObjectPointer)\r\n\r\nTH1 (v8): TNamed (v1), TAttLine (v2), TAttFill (v2), TAttMarker (v2)\r\n    fNcells: int (TStreamerBasicType)\r\n    fXaxis: TAxis (TStreamerObject)\r\n    fYaxis: TAxis (TStreamerObject)\r\n    fZaxis: TAxis (TStreamerObject)\r\n    fBarOffset: short (TStreamerBasicType)\r\n    fBarWidth: short (TStreamerBasicType)\r\n    fEntries: double (TStreamerBasicType)\r\n    fTsumw: double (TStreamerBasicType)\r\n    fTsumw2: double (TStreamerBasicType)\r\n    fTsumwx: double (TStreamerBasicType)\r\n    fTsumwx2: double (TStreamerBasicType)\r\n    fMaximum: double (TStreamerBasicType)\r\n    fMinimum: double (TStreamerBasicType)\r\n    fNormFactor: double (TStreamerBasicType)\r\n    fContour: TArrayD (TStreamerObjectAny)\r\n    fSumw2: TArrayD (TStreamerObjectAny)\r\n    fOption: TString (TStreamerString)\r\n    fFunctions: TList* (TStreamerObjectPointer)\r\n    fBufferSize: int (TStreamerBasicType)\r\n    fBuffer: double* (TStreamerBasicPointer)\r\n    fBinStatErrOpt: TH1::EBinErrorOpt (TStreamerBasicType)\r\n    fStatOverflows: TH1::EStatOverflows (TStreamerBasicType)\r\n\r\nTCollection (v3): TObject (v1)\r\n    fName: TString (TStreamerString)\r\n    fSize: int (TStreamerBasicType)\r\n\r\nTSeqCollection (v0): TCollection (v3)\r\n\r\nTList (v5): TSeqCollection (v0)\r\n\r\nTAttMarker (v2)\r\n    fMarkerColor: short (TStreamerBasicType)\r\n    fMarkerStyle: short (TStreamerBasicType)\r\n    fMarkerSize: float (TStreamerBasicType)\r\n\r\nTAttFill (v2)\r\n    fFillColor: short (TStreamerBasicType)\r\n    fFillStyle: short (TStreamerBasicType)\r\n\r\nTAttLine (v2)\r\n    fLineColor: short (TStreamerBasicType)\r\n    fLineStyle: short (TStreamerBasicType)\r\n    fLineWidth: short (TStreamerBasicType)\r\n\r\nTString (v2)\r\n\r\nTObject (v1)\r\n    fUniqueID: unsigned int (TStreamerBasicType)\r\n    fBits: unsigned int (TStreamerBasicType)\r\n\r\nTNamed (v1): TObject (v1)\r\n    fName: TString (TStreamerString)\r\n    fTitle: TString (TStreamerString)\r\n\r\nTEfficiency (v2): TNamed (v1), TAttLine (v2), TAttFill (v2), TAttMarker (v2)\r\n    fBeta_alpha: double (TStreamerBasicType)\r\n    fBeta_beta: double (TStreamerBasicType)\r\n    fBeta_bin_params: vector<pair<double,double> > (TStreamerSTL)\r\n    fConfLevel: double (TStreamerBasicType)\r\n    fFunctions: TList* (TStreamerObjectPointer)\r\n    fPassedHistogram: TH1* (TStreamerObjectPointer)\r\n    fStatisticOption: TEfficiency::EStatOption (TStreamerBasicType)\r\n    fTotalHistogram: TH1* (TStreamerObjectPointer)\r\n    fWeight: double (TStreamerBasicType)\r\n</pre>\r\n</details>\r\n\r\n> different deserialization procedures (described as a sequence of member data and their types).\r\n\r\nBut then I would expect the deserialization to return garbage of sorts (e.g. interpreting data for the wrong slots).\r\nHowever, reading my old hists with ROOT and comparing them to uproot (with the workaround of loading `uproot-issue38c.root` first), the only difference I see are the **under- and overflow bins**, which is just the difference between `np.array(root_hist)` vs `uproot_hist.to_numpy()`\r\n\r\n\r\nAnd my own files show a slight difference (older version of TH1):\r\n```diff\r\n29c29\r\n< TH1 (v8): TNamed (v1), TAttLine (v2), TAttFill (v2), TAttMarker (v2)\r\n---\r\n> TH1 (v7): TNamed (v1), TAttLine (v2), TAttFill (v2), TAttMarker (v2)\r\n51d50\r\n<     fStatOverflows: TH1::EStatOverflows (TStreamerBasicType)\r\n```\r\n\r\n\r\n",
  "created_at":"2022-09-30T09:10:43Z",
  "id":1263316923,
  "issue":38,
  "node_id":"IC_kwDOD6Q_ss5LTK-7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T09:10:43Z",
  "user":"MDQ6VXNlcjEyMTMyNzY="
 },
 {
  "author_association":"MEMBER",
  "body":"Uproot has built-in Models for TH1 (v8), but not for TH1 (v7).\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/70db4e1a6eaf6697e0f17d25e4dc619cf098670e/src/uproot/models/TH.py#L1021-L1026\r\n\r\nThe purpose of this is to avoid having to read TStreamerInfo for the most common/most up-to-date files, but fall back on reading TStreamerInfo if necessary. Reading the file with the TH1 (v7) in it will change the global state of the `uproot.classes` dict, but reading the file with the TH1 (v8) in it will only change it if it finds that the presumed data layout (the built-in Model) is wrong.",
  "created_at":"2022-09-30T17:08:22Z",
  "id":1263815502,
  "issue":38,
  "node_id":"IC_kwDOD6Q_ss5LVEtO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T17:08:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I wanted to rewrite my old files with newer ROOT to add the streamers (if there is a way).\r\n\r\nSince both files produce output with `fp.file.show_streamers()`, they both have streamers.\r\n\r\nI just checked Uproot's built-in streamer for TH1 (v8), and it's the same as the TH1 (v8) in your file:\r\n\r\n```\r\nTH1 (v8): TNamed (v1), TAttLine (v2), TAttFill (v2), TAttMarker (v2)\r\n    fNcells: int (TStreamerBasicType)\r\n    fXaxis: TAxis (TStreamerObject)\r\n    fYaxis: TAxis (TStreamerObject)\r\n    fZaxis: TAxis (TStreamerObject)\r\n    fBarOffset: short (TStreamerBasicType)\r\n    fBarWidth: short (TStreamerBasicType)\r\n    fEntries: double (TStreamerBasicType)\r\n    fTsumw: double (TStreamerBasicType)\r\n    fTsumw2: double (TStreamerBasicType)\r\n    fTsumwx: double (TStreamerBasicType)\r\n    fTsumwx2: double (TStreamerBasicType)\r\n    fMaximum: double (TStreamerBasicType)\r\n    fMinimum: double (TStreamerBasicType)\r\n    fNormFactor: double (TStreamerBasicType)\r\n    fContour: TArrayD (TStreamerObjectAny)\r\n    fSumw2: TArrayD (TStreamerObjectAny)\r\n    fOption: TString (TStreamerString)\r\n    fFunctions: TList* (TStreamerObjectPointer)\r\n    fBufferSize: int (TStreamerBasicType)\r\n    fBuffer: double* (TStreamerBasicPointer)\r\n    fBinStatErrOpt: TH1::EBinErrorOpt (TStreamerBasicType)\r\n    fStatOverflows: TH1::EStatOverflows (TStreamerBasicType)\r\n```\r\n\r\nSo whether it tries to read your file with the TH1 (v8) in it first or last, it does not change global state because the Model in `uproot.classes` _before_ reading the file agrees with the TStreamerInfo _in_ the file.\r\n\r\nBut reading the file with the TH1 (v7) in it _does_ change `uproot.classes`, since it doesn't know about v7, so it has to look in the file's TStreamerInfo and update `uproot.classes`.",
  "created_at":"2022-09-30T17:15:54Z",
  "id":1263822153,
  "issue":38,
  "node_id":"IC_kwDOD6Q_ss5LVGVJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T17:15:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This feature is currently in development at #609.",
  "created_at":"2022-06-17T14:51:49Z",
  "id":1158949762,
  "issue":197,
  "node_id":"IC_kwDOD6Q_ss5FFCuC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-17T14:51:49Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"We're getting ready to split the Uproot repo into `main`, which will swap Awkward 1 for Awkward 2, and `main-v4`, which will be for bug-fixes to Uproot 4 (starting with 4.3.0). It's hard to maintain two branches, so I'd like to close old pull requests. Is there any part of this that you'd like to see merged into `main` and `main-v4` or just `main`?",
  "created_at":"2022-06-21T12:24:58Z",
  "id":1161679054,
  "issue":314,
  "node_id":"IC_kwDOD6Q_ss5FPdDO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T12:24:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think we can close this for now. Perhaps in the future we can revisit. At least some work has been documented.",
  "created_at":"2022-06-21T12:31:17Z",
  "id":1161685932,
  "issue":314,
  "node_id":"IC_kwDOD6Q_ss5FPeus",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T12:31:17Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yeah, for now let's close this... Jim knows very well that I probably broke my brain trying to understand parts of this back then... and I'm happy to try to stab this again in the future when I have more spare cycles, or if we want to get a student working on this... I have a perfect project for them.",
  "created_at":"2022-06-21T13:07:07Z",
  "id":1161723826,
  "issue":314,
  "node_id":"IC_kwDOD6Q_ss5FPn-y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T13:07:07Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! @kratsg, what about #318? That's also about memberwise deserialization\u2014can that be closed, too?",
  "created_at":"2022-06-21T13:24:57Z",
  "id":1161745718,
  "issue":314,
  "node_id":"IC_kwDOD6Q_ss5FPtU2",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-06-21T13:24:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yeah, that's where it would be nice to get a student/fellow to jump in. I have ROOT files they can use with RooFit/RooStats stuff which are always memberwise-serialized and it's a good test bench to try and get uproot to read those out correctly.",
  "created_at":"2022-06-21T13:29:11Z",
  "id":1161750933,
  "issue":314,
  "node_id":"IC_kwDOD6Q_ss5FPumV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T13:29:11Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I agree; we'd need some concerted effort from someone who can concentrate on just this for at least a month. Knowing that you have a lot of sample files is a good thing.",
  "created_at":"2022-06-21T13:44:20Z",
  "id":1161769760,
  "issue":314,
  "node_id":"IC_kwDOD6Q_ss5FPzMg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T13:44:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Even pedantically worse -- ROOT generates them in different formats pre 6.18 and post 6.20 -- where they're not backward-compatible readable even within ROOT... so it will be interesting to see if uproot can bridge this gap.",
  "created_at":"2022-06-21T13:46:05Z",
  "id":1161771855,
  "issue":314,
  "node_id":"IC_kwDOD6Q_ss5FPztP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T13:46:05Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"We're getting ready to split the Uproot repo into `main`, which will swap Awkward 1 for Awkward 2, and `main-v4`, which will be for bug-fixes to Uproot 4 (starting with 4.3.0). It's hard to maintain two branches, so I'd like to close old pull requests. Is there any part of this that you'd like to see merged into `main` and `main-v4` or just `main`?",
  "created_at":"2022-06-21T12:25:10Z",
  "id":1161679254,
  "issue":318,
  "node_id":"IC_kwDOD6Q_ss5FPdGW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T12:25:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"See #314.",
  "created_at":"2022-06-21T13:28:30Z",
  "id":1161750067,
  "issue":318,
  "node_id":"IC_kwDOD6Q_ss5FPuYz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T13:28:30Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"NONE",
  "body":"Basically, it improves speed by not writing small baskets. So is there a way to set the basket size per tree, such as \r\n```\r\nuproot.newtree(..., flushsize=100000)\r\n```\r\nAnd then do:\r\n```\r\nfile.extend(..., flush=False)\r\n```\r\nAnd this way, keep stuff in memory and only write to file when the basket is full. So is this (https://indico.cern.ch/event/840667/contributions/3527109/attachments/1908764/3153297/uproot-irisfellow-final.pdf) out of date for uproot4 now?",
  "created_at":"2022-01-31T01:04:59Z",
  "id":1025285883,
  "issue":428,
  "node_id":"IC_kwDOD6Q_ss49HJ77",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-31T01:05:23Z",
  "user":"MDQ6VXNlcjY2NTgzMTk="
 },
 {
  "author_association":"MEMBER",
  "body":"Uproot 3 is out of date now. Reik's IRIS-HEP presentation on Uproot TTree-writing was the Uproot 3 implementation. The Uproot 4 implementation (#321) now completely replaces it,\r\n\r\n  * with a different public API (not much different, mostly [allowing assignment](https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file) and [uproot.WritableDirectory.mktree](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableDirectory.html#mktree) instead of introducing an `uproot.newtree` function),\r\n  * with considerably reorganized internal mechanics for maintainability,\r\n  * without flushing logic, which is what you're referring to above. All that Uproot 3's `flushsize` did (or attempted to do; it [sometimes didn't work](https://github.com/scikit-hep/uproot3/issues/522)) was gather array data until the limit is reached, then concatenate and write. That's easy enough to do in a script, but mixing that logic with the file-writing logic made quite a mess. So we also dropped `append`.\r\n\r\nThe downside of dropping the `flushsize` logic is that each [uproot.WritableTree.extend](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableTree.html#extend) is inexorably bound to one TBasket\u2014no exceptions. Writing files with lots of little TBaskets is problematic. Hence, the new interface, if used unwisely, would lead users to create files with lots of little TBaskets. I just hope I've posted enough warnings to guard against that.\r\n\r\nThis PR doesn't seem to introduce any performance improvements, not like #426 for histograms. It's just where I tested it at scale and fixed a few bugs that were revealed by doing so.",
  "created_at":"2022-01-31T22:20:35Z",
  "id":1026270482,
  "issue":428,
  "node_id":"IC_kwDOD6Q_ss49K6US",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-31T22:20:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"#692 would likely be the best way forward. Hopefully, we can find someone to take this up again (as I understand it, @nsmith- is very busy!). I can help with ensuring that it fits the `Source` protocol (connection to Uproot).",
  "created_at":"2022-12-14T19:20:08Z",
  "id":1352029851,
  "issue":443,
  "node_id":"IC_kwDOD6Q_ss5Qllab",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-14T19:20:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We're getting ready to split the Uproot repo into `main`, which will swap Awkward 1 for Awkward 2, and `main-v4`, which will be for bug-fixes to Uproot 4 (starting with 4.3.0). It's hard to maintain two branches, so I'd like to close old pull requests. Is there any part of this that you'd like to see merged into `main` and `main-v4` or just `main`?",
  "created_at":"2022-06-21T12:25:28Z",
  "id":1161679592,
  "issue":444,
  "node_id":"IC_kwDOD6Q_ss5FPdLo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T12:25:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Work on this has not progressed beyond what's in this PR, and it can be closed. We can use it in a new PR at a future point.",
  "created_at":"2022-06-21T13:14:10Z",
  "id":1161732187,
  "issue":444,
  "node_id":"IC_kwDOD6Q_ss5FPqBb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T13:14:10Z",
  "user":"MDQ6VXNlcjQ2NTYzOTE="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, thanks!",
  "created_at":"2022-06-21T13:23:43Z",
  "id":1161744051,
  "issue":444,
  "node_id":"IC_kwDOD6Q_ss5FPs6z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T13:23:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> it seems that Uproot correctly goes into fallback mode if `entry_stop` is given but not if it isn't. **That's an Uproot bug.**\r\n\r\nIf `entry_stop` is given and is small enough, then the request is only one TBasket and therefore one part of a multi-part GET. This HTTP server responds correctly to that case. So the above-mentioned bug can be triggered by\r\n\r\n  * asking for more than one TBranch and `entry_stop=1` or\r\n  * asking for `entry_stop` larger than the first TBasket, which is 7981 for `\"B0_M\"`.\r\n\r\nInstead of raising an error during the handling of the HTTP:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/22407793f9eab02ce73d6843bb3395bedacab50b/src/uproot/source/http.py#L401-L406\r\n\r\nnow we just switch over to the fallback:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/0268c4b652b1dcc6e89c5656052bd2e221b4db23/src/uproot/source/http.py#L406-L407\r\n\r\nI was able to read your data, but it's hard to put this into a testing suite, since it relies on an external site. (And it relies on the external HTTP server not being _improved_ in the future.)\r\n\r\nFixed in #594.",
  "created_at":"2022-05-11T20:52:57Z",
  "id":1124284886,
  "issue":507,
  "node_id":"IC_kwDOD6Q_ss5DAznW",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2022-05-11T20:52:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We're getting ready to split the Uproot repo into `main`, which will swap Awkward 1 for Awkward 2, and `main-v4`, which will be for bug-fixes to Uproot 4 (starting with 4.3.0). It's hard to maintain two branches, so I'd like to close old pull requests. Is there any part of this that you'd like to see merged into `main` and `main-v4` or just `main`?\r\n\r\nNote: this was never merged or reviewed because it was a draft. I assumed that you were still planning on adding to it, though it's been dormant for a while. If you want, we can apply it to `main` before the split, so that this, as it is, will apply to both Uproot 4 bug-fixes and Uproot 5.",
  "created_at":"2022-06-21T12:47:38Z",
  "id":1161702538,
  "issue":527,
  "node_id":"IC_kwDOD6Q_ss5FPiyK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T12:47:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The failing test is the one with ROOT. One of the changes in `main` makes tests of ROOT behavior less strict, since the way that they handle TStreamerInfo when updating a file has apparently changed.",
  "created_at":"2022-06-21T12:48:47Z",
  "id":1161703829,
  "issue":527,
  "node_id":"IC_kwDOD6Q_ss5FPjGV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T12:48:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I marked it with draft because I need to solve the == None / is None problem (which is a PyROOT Python antipattern). Then I forgot about it. I can see if it can be revived.",
  "created_at":"2022-06-21T15:30:14Z",
  "id":1161918706,
  "issue":527,
  "node_id":"IC_kwDOD6Q_ss5FQXjy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T15:30:14Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Since it's isolated, just a few places in the codebase and all of them in tests, I think the \"`# noqa: E711 (ROOT null check)`\" comments are acceptable. They're noisy and really call attention to the fact that this is not the normal way of doing things.\r\n\r\nEven if ROOT changes, there are versions of ROOT out there that require `== None`. I suppose we could require a very recent version of ROOT for the tests, though I'd rather not if it's just for this. I'd be more inclined to find another way of writing the tests! (These tests require the non-existence of a LeafCount... Maybe there's a way to do it, other than the null-pointer check.)",
  "created_at":"2022-06-21T15:42:12Z",
  "id":1161932168,
  "issue":527,
  "node_id":"IC_kwDOD6Q_ss5FQa2I",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T15:42:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Could it be `assert not nb2.GetLeaf(\"nb2\").GetLeafCount()`? That would also trigger if GetLeafCount returned 0, is that a possibility to worry about?",
  "created_at":"2022-06-21T18:28:16Z",
  "id":1162160679,
  "issue":527,
  "node_id":"IC_kwDOD6Q_ss5FRSon",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T18:28:16Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> Could it be `assert not nb2.GetLeaf(\"nb2\").GetLeafCount()`? That would also trigger if GetLeafCount returned 0, is that a possibility to worry about?\r\n\r\nThe return value of [TLeaf::GetLeafCount](https://root.cern.ch/doc/master/classTLeaf.html#a6fb5b55f85ae4b64447e48d247fc5ddf) is typed (`TLeaf*`), so only checking to see if it's \"falsy\" would be enough.\r\n\r\nIt's a nice way out. I tend to avoid relying on truthy and falsy tests, but my reasons are undermined by cases that have a checked type.",
  "created_at":"2022-06-21T18:37:42Z",
  "id":1162171741,
  "issue":527,
  "node_id":"IC_kwDOD6Q_ss5FRVVd",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-06-21T18:37:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry that I missed this earlier!\r\n\r\nI noticed that the Awkward badge was out of date and wanted to make both of these be \"3.6\u20123.10\" (3.6 is included and I prefer a range over enumeration, especially now that it's contiguous). That badge update is in #534, along with actually testing 3.10. Awkward Array with Python 3.10 might have to compile from source; that might fail, but Awkward 1.8.0 [is due](https://github.com/scikit-hep/awkward-1.0#roadmap), so I should be putting that in PyPI pretty soon. #534 can wait for that if necessary.",
  "created_at":"2022-01-04T18:27:29Z",
  "id":1005065593,
  "issue":528,
  "node_id":"IC_kwDOD6Q_ss476BV5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-04T18:27:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@henryiii Everything went without troubles (though Awkward Array for Python 3.10 takes a little longer to compile, at least until Awkward 1.8.0 is out). I requested you as a reviewer just to make sure you're aware of this change and don't foresee problems downstream.",
  "created_at":"2022-01-04T18:42:01Z",
  "id":1005076007,
  "issue":534,
  "node_id":"IC_kwDOD6Q_ss476D4n",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-04T18:42:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, sounds good. Only thing missing for 3.10 in general is Numba, and that should be out any day. So far the addition of 3.10 has been very smooth - I do foresee a few more bumps for the 3.11 transition - it will be removing a lot of old Python 2.7 compatibility shims that packages may still be using. The performance changes are also occasionally affecting the C API (but pybind11 insulates downstream packages from that - but as a pybind11 developer I have to deal with some of them :) )",
  "created_at":"2022-01-04T18:55:08Z",
  "id":1005084796,
  "issue":534,
  "node_id":"IC_kwDOD6Q_ss476GB8",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-01-04T18:55:08Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"See the explanation on #537. There were two memory leaks; I fixed the one in Uproot. It may be the biggest one, so that might unblock you.\r\n\r\nThe other memory leak is in NumPy, and I don't know how to deal with that. I went to report it and found a paper trail that goes back to 2009 (pre-GitHub)! At least, I gave them a very detailed reproducer.",
  "created_at":"2022-01-14T02:54:09Z",
  "id":1012703195,
  "issue":536,
  "node_id":"IC_kwDOD6Q_ss48XJ_b",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-01-14T02:54:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for fixing this so fast, at least on the side of uproot! I tested it and I can live with the remaining memory leak due to numpy. Hope they will fix it at some point.",
  "created_at":"2022-01-14T11:11:51Z",
  "id":1013027208,
  "issue":536,
  "node_id":"IC_kwDOD6Q_ss48YZGI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-14T11:11:51Z",
  "user":"MDQ6VXNlcjIyNTQ0MzAy"
 },
 {
  "author_association":"MEMBER",
  "body":"I hadn't thought of this as a potential use, but since \"`/`\" hides the distinction between paths into nested directories and paths into nested branches, it makes sense that one would try passing such a path into `classname_of`, `streamer_of`, `class_of`, `title_of`, etc.\r\n\r\nThe difficulty is that the path-handling is a specialized implementation in [uproot.ReadOnlyDirectory.key](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyDirectory.html#key)\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/3fe33aff6d6ed88203eed90f4edb5f2bc720f3e6/src/uproot/reading.py#L2006-L2021\r\n\r\nwhich would have to get generalized out to be able to use them in the other functions that take paths.\r\n\r\nOr maybe it just means that [uproot.TBranch.HasBranches](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.HasBranches.html) objects (i.e. TTree and all TBranch classes) should have a `key` method? (What about TBranches that contain TBranches? Would that work? I'm talking to myself...) Such a method can't return an [uproot.reading.ReadOnlyKey](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyKey.html) because TTrees and TBranches don't have them the way that [uproot.ReadOnlyDirectory](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyDirectory.html) does, but maybe a mocked \"FakeKey\" could duck-type to supply the appropriate information...",
  "created_at":"2022-01-18T18:26:21Z",
  "id":1015692410,
  "issue":542,
  "node_id":"IC_kwDOD6Q_ss48ijx6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-18T18:26:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This feature request is still open, but I'm not going to attempt to get it in before the 5.0.0 release (next Monday). I'm just removing that label.",
  "created_at":"2022-11-28T20:37:11Z",
  "id":1329733533,
  "issue":542,
  "node_id":"IC_kwDOD6Q_ss5PQh-d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-28T20:37:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You're absolutely right, and I'm sorry this had to happen at the end of a GB-scale writing session.\r\n\r\nI semi-independently came to the conclusion that 971d308c236624b772b6b83394e38da54f4ebd74 is what was needed, in that I read the first half of your report, looked into it at all the places where `Tree._branch_data` was used, and concluded that any extraction of its contents should first check the `\"kind\"` field. Then I saw that you had figured that out, too, so I have even more confidence that that's what went wrong. This list of dicts was intended to hold the changing state of a TBranch during the write, but then two types of such dicts were needed to handle nested data structures. You must be writing some interesting data type.\r\n\r\nThe `self._num_baskets >= self._basket_capacity - 1` check that you're referring to is checking for when the TTree metadata is too small to write the locations of additional TBaskets. There's a way to set the initial number of such slots (I think it's 100), but instead of messing with that, you could write excessively small TBaskets (very small data in each call to `extend`), which would exhaust the number of preallocated TBasket locations. That would trigger the TTree metadata-rewrite sooner, to reproduce this bug.",
  "created_at":"2022-01-21T19:57:32Z",
  "id":1018816506,
  "issue":546,
  "node_id":"IC_kwDOD6Q_ss48uef6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-21T19:57:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I was in a hurry when I wrote the above. For additional clarification, the \"TTree metadata object\" contains arrays that specify the number of bytes, number of entries, and file-seek positions of all the TBaskets for each TBranch. Objects on disk can't grow without potentially overwriting the object written just after it, so we have to preallocate the TTree with a given size, and that means that the number of bytes, number of entries, and seek-position arrays have a maximum capacity. Each time you call `extend`, you add one TBasket to each TBranch. When adding the next TBasket would go over capacity, the TTree metadata needs to be rewritten.\r\n\r\nRewriting the TTree metadata doesn't mean rewriting all the previous TBaskets. What happens is that we find a new, larger block of space in the file (probably at the end of the file), allocate the bigger one, copy all the metadata to the new spot, including seek positions to the already-written TBaskets, point the TDirectory at the new TTree metadata, and then invalidate the old spot. (Yes, the file becomes fragmented, but these metadata objects are not very large. If the space can be used by a new allocation in the future, it will be, but probably not in a perfectly-fitting way.)\r\n\r\nThat's why it took so long to encounter this bug: you were probably writing reasonably-sized TBaskets, and the initial TTree metadata allocation is large. To reproduce the bug, you can write artificially small TBaskets, since the capacity gets used up by the _number_ of TBaskets, not their individual sizes.",
  "created_at":"2022-01-21T21:49:38Z",
  "id":1018889687,
  "issue":546,
  "node_id":"IC_kwDOD6Q_ss48uwXX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-21T21:49:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the explanation and the commit. For me #547 fixes it.",
  "created_at":"2022-01-24T10:54:36Z",
  "id":1019967913,
  "issue":546,
  "node_id":"IC_kwDOD6Q_ss48y3mp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-24T10:54:36Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"The failure seems to be in pip\u2014it's trying to import the `dataclass` module for `rich`, even though this is a Python 3.6 environment.\r\n\r\nPerhaps the pip version can be pinned? @henryiii, you said we had reason to drop Python 3.6 because manylinux1 is now unsupported. Uproot doesn't need manylinux1 because it's pure Python. But maybe we should drop Python 3.6 in projects across the board, to simplify maintenance?\r\n\r\nYou know what? Before we do that, let me (or you) redraw the pip download statistics for Scikit-HEP packages vs Python version. The PyPI statistics include Python version.",
  "created_at":"2022-01-31T20:36:51Z",
  "id":1026186212,
  "issue":551,
  "node_id":"IC_kwDOD6Q_ss49Klvk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-31T20:36:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Pip dropped 3.6 support, and added rich. The fact it's got rich vendored means it's trying to use pip 22 on 3.6. Why? I have no idea. We need to look into it. I can soon.\n\nI'd recommend dropping 3.6 across the board. Though not instantly (maybe right after Awkward 2.0? 1.8?). Even if we have a few 3.6 users, we aren't \"breaking\" them, just not shipping new versions for them. Unless pip is broken. ;)",
  "created_at":"2022-01-31T21:44:39Z",
  "id":1026242449,
  "issue":551,
  "node_id":"IC_kwDOD6Q_ss49KzeR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-31T21:44:39Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"The problem is conda-forge didn't pick up the new minimum requirement for pip: https://github.com/conda-forge/pip-feedstock/pull/88",
  "created_at":"2022-01-31T22:38:46Z",
  "id":1026283013,
  "issue":551,
  "node_id":"IC_kwDOD6Q_ss49K9YF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-01-31T22:38:46Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"This is patched, though it might break again if pip makes a new release (today or tomorrow) before pip-feedstock is patched.",
  "created_at":"2022-02-02T17:20:46Z",
  "id":1028170555,
  "issue":551,
  "node_id":"IC_kwDOD6Q_ss49SKM7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-02T17:20:46Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Now zipp is broken (https://github.com/jaraco/zipp/blob/v3.7.0/setup.cfg#L19). Being patched.",
  "created_at":"2022-02-02T18:10:25Z",
  "id":1028215946,
  "issue":551,
  "node_id":"IC_kwDOD6Q_ss49SVSK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-02T18:10:25Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Now that https://github.com/conda-forge/conda-forge-repodata-patches-feedstock/pull/221#event-5998004717 is in, here goes nothing...",
  "created_at":"2022-02-02T18:59:15Z",
  "id":1028264801,
  "issue":551,
  "node_id":"IC_kwDOD6Q_ss49ShNh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-02T18:59:15Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"(looks like it takes a bit of time to propagate, or maybe it's a pinning release, etc)",
  "created_at":"2022-02-02T19:01:39Z",
  "id":1028266841,
  "issue":551,
  "node_id":"IC_kwDOD6Q_ss49ShtZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-02T19:01:39Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> (looks like it takes a bit of time to propagate, or maybe it's a pinning release, etc)\r\n\r\nAt the moment it takes up to ~2 hours unfortunately.",
  "created_at":"2022-02-02T19:19:31Z",
  "id":1028277355,
  "issue":551,
  "node_id":"IC_kwDOD6Q_ss49SkRr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-02T19:19:31Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"Looks like we are good! :)",
  "created_at":"2022-02-02T21:26:05Z",
  "id":1028372682,
  "issue":551,
  "node_id":"IC_kwDOD6Q_ss49S7jK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-02T21:26:05Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"While looking at `awkward` I ran into https://github.com/scikit-hep/awkward-1.0/issues/1185 and https://github.com/scikit-hep/awkward-1.0/pull/1196, so it looks to already be changed there (I see the same warning in `1.7.0`, but the change looks like it came after that release).",
  "created_at":"2022-02-01T12:58:07Z",
  "id":1026814721,
  "issue":552,
  "node_id":"IC_kwDOD6Q_ss49M_MB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-01T12:58:07Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, I think this is an already-fixed bug (warnings are bugs, at least \"future bugs\"). That makes this a duplicate and I'll close it.\r\n\r\nYou saw it in 1.7.0 also (and older); I suspect it started to appear when you switched to Python 3.10.",
  "created_at":"2022-02-01T17:23:14Z",
  "id":1027096608,
  "issue":552,
  "node_id":"IC_kwDOD6Q_ss49OEAg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-01T17:23:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I might be misunderstanding the setup, but I think this still exists in `uproot`. On master I see `distutils` used: https://github.com/scikit-hep/uproot4/blob/6fd8863d932ca52f45d84cf5eec5b9c81b1bcfec/src/uproot/extras.py#L14\r\n\r\nThe warnings above also include those coming from `awkward`, but I think an update of `awkward` will only make some go away. I started seeing this when switching from Python 3.8 to Python 3.9.",
  "created_at":"2022-02-01T17:32:08Z",
  "id":1027104988,
  "issue":552,
  "node_id":"IC_kwDOD6Q_ss49OGDc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-01T17:32:08Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, it still exists in Uproot. Oh! This is an Uproot issue! Reopening now.",
  "created_at":"2022-02-01T17:34:07Z",
  "id":1027106890,
  "issue":552,
  "node_id":"IC_kwDOD6Q_ss49OGhK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-01T17:34:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry for the confusion! Indeed, with `awkward` showing up a lot here, and me linking to it, that was not very clear.",
  "created_at":"2022-02-01T17:35:45Z",
  "id":1027108435,
  "issue":552,
  "node_id":"IC_kwDOD6Q_ss49OG5T",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-01T17:35:45Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"Fixed by #564.",
  "created_at":"2022-05-11T20:19:01Z",
  "id":1124249569,
  "issue":552,
  "node_id":"IC_kwDOD6Q_ss5DAq_h",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-11T20:19:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"[issue-553.txt](https://github.com/scikit-hep/uproot4/files/8003033/issue-553.txt)\r\n",
  "created_at":"2022-02-04T12:38:25Z",
  "id":1029949621,
  "issue":553,
  "node_id":"IC_kwDOD6Q_ss49Y8i1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-04T12:38:25Z",
  "user":"MDQ6VXNlcjEzMjAxNzMx"
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know what to make of this file. (**Edit:** by the end of this comment, I do. Sorry for the stream-of-consciousness.)\r\n\r\nBy looking at the raw data, it's clear that the actual numbers start after the first byte, so it would seem that the appropriate interpretation would be\r\n\r\n```python\r\nuproot.AsJagged(uproot.AsDtype(\r\n    np.dtype([(\"x\", \">f8\"),(\"y\", \">f8\"),(\"z\", \">f8\"),(\"t\", \">f8\")])\r\n), header_bytes=1)   # skip 1 byte in each entry\r\n```\r\n\r\nor equivalently, the `AsArray(...)` that it originally expected. However, the data in each entry is an apparently random number of bytes\u2014hopefully always larger than `fNPoints * 4 * 8`. The reason both of these interpretations are failing is because it can't cast the remaining bytes as 8-byte floats. Even if each entry's binary blob were at least a multiple of 8 bytes or 32 bytes, the resulting array would then have too much data per entry.\r\n\r\nHere's some of my debugging code, trying to go through it very carefully:\r\n\r\n```python\r\nimport uproot\r\nimport numpy as np\r\nbranch = uproot.open(\"issue-553.root\")[\"pndsim/GeoTracks.fPoints\"]\r\nbasket = branch.basket(0)\r\ninterpretation = uproot.AsJagged(uproot.AsDtype(np.dtype(\">f8\")), header_bytes=1)\r\ninterpretation.basket_array(basket.data, basket.byte_offsets, basket, branch, {}, 0, uproot.interpretation.library.NumPy)\r\n# ValueError because the basket.byte_offsets don't correspond to multiples of 8 bytes\r\n```\r\n\r\nThe first entry has a multiple of 8 bytes (after skipping the 1-byte header), but the second does not. This first `basket` has four entries, which have to be interpreted together.\r\n\r\nThere doesn't seem to be any way to know how much data to interpret as points without looking at `fNpoints`, a _totally different branch_. So I guess this is something that would require a \"custom streamer,\" out of scope for Uproot because custom streamers are written in C++, not encoded by the TStreamerInfo in the file. This particular example seems pretty simple, so we should be able to hack something together, but this is at the limits of what Uproot would ever be able to do.\r\n\r\nI just noticed that `fNpoints` is a jagged array:\r\n\r\n```python\r\n>>> tree = uproot.open(\"issue-553.root:pndsim\")\r\n>>> tree[\"GeoTracks/GeoTracks.fNpoints\"].array()\r\n<Array [[464], [248, 8, 8, ... 12, 8], [464]] type='100 * var * uint32'>\r\n```\r\n\r\nso these points would have to be interpreted as an array of `100 * var * var * {x: float64, y: float64, z: float64, t: float64}` (doubly nested jagged array). That's why the expected interpretation involved two nested `AsArrays`! But it couldn't interpret it without external information\u2014the `fNpoints`\u2014so it didn't know where to stop looping. We'll have to manually pull that information into it.\r\n\r\nTo start with, we can get the raw bytes for any entry with [uproot.TBranch.debug_array](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#debug-array). Using that in production code is a hack. I was misled by the first entry (`0`) because it has only one sub-entry, so let's do the second entry (`1`), because it has a non-trivial number (4).\r\n\r\n```python\r\n>>> tree[\"GeoTracks/GeoTracks.fNpoints\"].array()[1]\r\n<Array [248, 8, 8, 8] type='4 * uint32'>\r\n>>> tree[\"GeoTracks/GeoTracks.fPoints\"].debug_array(1)\r\narray([  1,   0,   0, ..., 246,  99, 155], dtype=uint8)\r\n```\r\n\r\nIt seems that each of the four sub-entries has a different one-byte header. That's why an `AsJagged` isn't going to work, since it can only skip bytes at the beginning of an _entry_, not a _sub-entry_. The numbers in `fNpoints` actually count each 4-vector as 4, so let's start by integer-dividing them:\r\n\r\n```python\r\n>>> num_vectors = tree[\"GeoTracks/GeoTracks.fNpoints\"].array() // 4\r\n>>> num_vectors\r\n<Array [[116], [62, 2, 2, ... 2, 3, 2], [116]] type='100 * var * uint32'>\r\n```\r\n\r\nFor each entry, such as the entry number `1` that we're looking at now, we can get the raw bytes by\r\n\r\n```python\r\n>>> entry_raw_bytes = tree[\"GeoTracks/GeoTracks.fPoints\"].debug_array(1)\r\n```\r\n\r\nI'm not sure, but I think that `debug_array` might be re-reading the whole TBasket from disk every time you ask for an entry, which would be a performance bug if we leave it that way. (**Edit:** it does and it would. That's why \"`debug_array`\" starts with the word \"debug.\") We'll have to get back to that. Let's just see if we can get the data out.\r\n\r\n```python\r\n>>> entry_vector_arrays = []\r\n>>> pos = 0\r\n>>> for n in num_vectors[1]:\r\n...     pos += 1\r\n...     entry_vector_arrays.append(\r\n...         entry_raw_bytes[pos : pos + (n*4*8)].view(\r\n...             np.dtype([(\"x\", \">f8\"),(\"y\", \">f8\"),(\"z\", \">f8\"),(\"t\", \">f8\")])\r\n...         )\r\n...     )\r\n...     pos += n*4*8\r\n```\r\n\r\nLooking at these `entry_vector_arrays`, they look right. Getting all of these together\u2014not just for entry `1`, but for all the entries\u2014can be done with [ak.from_iter](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_iter.html).\r\n\r\nNow we should address this problem of re-reading a whole TBasket when you only want one entry. Instead of `debug_array` for getting one entry, we can get the whole array with\r\n\r\n```python\r\n>>> raw_interpretation = uproot.AsJagged(uproot.AsDtype(\"u1\"))\r\n>>> raw_entries = tree[\"GeoTracks/GeoTracks.fPoints\"].array(raw_interpretation)\r\n>>> raw_entries\r\n<Array [[1, 0, 0, 0, 0, ... 36, 96, 74, 108]] type='100 * var * uint8'>\r\n```\r\n\r\nEach entry in `raw_entries` is like `entry_raw_bytes` above. So the whole procedure from start to finish would be:\r\n\r\n```python\r\nimport uproot\r\nimport numpy as np\r\nimport awkward as ak\r\n\r\ntree = uproot.open(\"issue-553.root:pndsim\")\r\n\r\nnum_vectors = tree[\"GeoTracks/GeoTracks.fNpoints\"].array() // 4\r\n\r\nraw_interpretation = uproot.AsJagged(uproot.AsDtype(\"u1\"))\r\nraw_entries = tree[\"GeoTracks/GeoTracks.fPoints\"].array(raw_interpretation)\r\n\r\nvector_arrays = []\r\nfor i, num in enumerate(num_vectors):\r\n    entry_vector_arrays = []\r\n    pos = 0\r\n    for n in num:\r\n        pos += 1\r\n        recarray = np.asarray(raw_entries[i][pos : pos + (n*4*8)]).view(\r\n            np.dtype([(\"x\", \">f8\"),(\"y\", \">f8\"),(\"z\", \">f8\"),(\"t\", \">f8\")])\r\n        )\r\n        entry_vector_arrays.append([\r\n            {\"x\": v[\"x\"], \"y\": v[\"y\"], \"z\": v[\"z\"], \"t\": v[\"t\"]} for v in recarray\r\n        ])\r\n        pos += n*4*8\r\n    vector_arrays.append(entry_vector_arrays)\r\n\r\nvectors = ak.from_iter(vector_arrays)\r\n```\r\n\r\nwhere\r\n\r\n```python\r\n>>> vectors\r\n<Array [[[{x: 0, y: 0, ... t: 5.32e-08}]]] type='100 * var * var * {\"x\": float64...'>\r\n```\r\n\r\nand\r\n\r\n```python\r\n>>> vectors[1, 1, 0]\r\n<Record ... y: 4.04, z: 477, t: 1.88e-08} type='{\"x\": float64, \"y\": float64, \"z\"...'>\r\n```\r\n\r\netc. This is not a solution to Uproot's reading of the file\u2014I have no idea how the above could be encoded automatically. ROOT can do it because it has [TGeoTrack](https://root.cern.ch/doc/master/classTGeoTrack.html) with its custom streamers in its codebase. To do the same in Uproot, we'd need a specialized \"`Model_TGeoTrack`\" (as we do for some deep built-int classes like `TList`, but generally not for physics objects, particularly ones that have been split). I'll have to label this issue as \"won't fix.\"",
  "created_at":"2022-02-04T18:23:10Z",
  "id":1030239593,
  "issue":553,
  "node_id":"IC_kwDOD6Q_ss49aDVp",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-02-04T18:23:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for your help!\r\n\r\nSetting the interpretation this way and iterating over the events seems to work, but unfortunately it's very slow for large files. So I think for now I'd still have to use a ROOT macro to read this file.\r\n\r\nStill, thank you so much for your help!",
  "created_at":"2022-02-07T11:36:18Z",
  "id":1031369804,
  "issue":553,
  "node_id":"IC_kwDOD6Q_ss49eXRM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-07T11:36:18Z",
  "user":"MDQ6VXNlcjEzMjAxNzMx"
 },
 {
  "author_association":"MEMBER",
  "body":"This is something that could be sped up with [AwkwardForth](https://github.com/scikit-hep/awkward-1.0/wiki/AwkwardForth-documentation), but we're mostly using that for library code, rather than specific applications, so that the benefit-to-effort ratio is optimized. But, just pointing it out\u2014cheers!",
  "created_at":"2022-02-07T13:07:55Z",
  "id":1031449978,
  "issue":553,
  "node_id":"IC_kwDOD6Q_ss49eq16",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-07T13:07:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for putting together the nice minimal reproducing example and potential solution! Usually, though, it's easier if reproducers are code in the GitHub markdown because then I can just copy-paste it, and the `identify.txt` (for `identify.py`) doesn't specify which version it is a difference relative to. I guessed that it's relative to 4.1.9, the latest release since December 8 (until today). However, it's _exactly_ the same as `identify.py` from 4.1.9, so I think maybe it's not the file you intended to send. If you have a potential solution, submitting it as a pull request makes the review process super quick and easy: it even runs all the tests for you.\r\n\r\nAnyway\u2014no problem\u2014I found where it's failing, it's because the leaf-type is `kDouble32` (it's a [TLeafD32](https://root.cern.ch/doc/master/classTLeafD32.html)). The titles of these branches are where their parameterization is stored\u2014how many bits, the lowest and highest possible values\u2014and that's why it needs to parse the title as data. Your title contains a human-readable description of the data, a reasonable-sounding thing to include in a title! So what should we do in this situation? Maybe if the title is not parsable, it should use the default parameters? That's what #561 does.\r\n\r\nIf you like that solution (e.g. it's what you intended to send in `identify.txt`), then we're on the same page and I'll merge it in.",
  "created_at":"2022-02-14T22:30:15Z",
  "id":1039637907,
  "issue":558,
  "node_id":"IC_kwDOD6Q_ss49952T",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-14T22:30:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Many thanks for the detailed answer and the many tips. You are right, I accidentally copied the wrong identify.py file. So here is, what we did:\r\nWe replaced the lines 244-279 from version 4.1.9 by the following lines of code \r\n```python\r\n    else:\r\n        try:\r\n            source = title[left : right + 1]\r\n            try:\r\n                parsed = ast.parse(source).body[0].value\r\n            except SyntaxError:\r\n                raise UnknownInterpretation(\r\n                    \"cannot parse streamer title {0} (as Python)\".format(repr(source)),\r\n                    branch.file.file_path,\r\n                    branch.object_path,\r\n                )\r\n\r\n            transformed = ast.Expression(_float16_double32_walk_ast(parsed, branch, source))\r\n            spec = eval(compile(transformed, repr(title), \"eval\"))\r\n            if (\r\n                len(spec) == 2\r\n                and uproot._util.isnum(spec[0])\r\n                and uproot._util.isnum(spec[1])\r\n            ):\r\n                low, high = spec\r\n                num_bits = None\r\n\r\n            elif (\r\n                len(spec) == 3\r\n                and uproot._util.isnum(spec[0])\r\n                and uproot._util.isnum(spec[1])\r\n                and uproot._util.isint(spec[2])\r\n            ):\r\n                low, high, num_bits = spec\r\n\r\n            else:\r\n                raise UnknownInterpretation(\r\n                    \"cannot interpret streamer title {0} as (low, high) or \"\r\n                    \"(low, high, num_bits)\".format(repr(source)),\r\n                    branch.file.file_path,\r\n                    branch.object_path,\r\n                )\r\n        except (UnknownInterpretation):\r\n            low, high, num_bits = 0, 0, \"no brackets\"  # distinct from \"None\"\r\n```\r\nSo, basically we just put the existing code in a try statement and added another except (UnknownInterpretation). But your solution also seems to work for my problem, so I assume we are on the same page and you can merge it. Thank you very much for your help!",
  "created_at":"2022-02-21T14:31:22Z",
  "id":1046940808,
  "issue":558,
  "node_id":"IC_kwDOD6Q_ss4-ZwyI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-21T14:31:22Z",
  "user":"MDQ6VXNlcjUyNDk3Njcw"
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, looking at this code again, I noticed that the PR would complain about titles that can't be parsed as Python syntax. Your example can be parsed, because \"`length [cm]`\" is valid Python code, in which `length` is an array-like object, sliced by some variable named `cm`. In your case, it failed upon evaluation (neither of those variables exist), but if we're allowing titles to be arbitrary strings, such as human-readable documentation, there would be a lot of cases in which they're also not valid Python syntax. So I moved the `ast.parse` line into the `try`-`except`, as you have it.\r\n\r\n(Your `try`-`except` wraps around the code that's raising the exceptions, which would make the code path a little hard to follow, so I prefer the version in the PR. It's going through testing again with my latest change, but once it passes the tests, it will be merged. Thanks for trying it!)",
  "created_at":"2022-02-21T15:09:14Z",
  "id":1046975928,
  "issue":558,
  "node_id":"IC_kwDOD6Q_ss4-Z5W4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-21T15:09:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hmm. Those functions have standard docstrings:\r\n\r\n```python\r\n>>> import uproot\r\n>>> import skhep_testdata\r\n>>> f = uproot.open(skhep_testdata.data_path(\"uproot-histograms.root\"))\r\n>>> one = f[\"one\"]\r\n>>> one\r\n<TH1F (version 2) at 0x7f53bb2660d0>\r\n>>> help(one.to_numpy)\r\n```\r\n\r\n(help fills the screen with my chosen pager).\r\n\r\nLet me try JupyterLab. Shift-tab worked for me in JupyterLab. (I didn't know about that feature; thanks!) I have version 3.2.5-2 of JupyterLab...\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/153950708-8c414b68-a791-45cc-839a-40939b0053c2.png)\r\n\r\nI found [this on StackOverflow](https://stackoverflow.com/q/59008693/1623645).\r\n\r\nAnd I played around with it a bit more. The object needs to be defined in Python before shift-tab can work\u2014how else can JupyterLab internally call `help` on it?\u2014and you can't shift-tab on an object that gets defined in the same cell. Since you haven't evaluated that cell yet, it doesn't exist in Python yet. Given that your code example includes the definition of `obj` in the same block of text where you're trying to get the help, I'm guessing that's what happened here. It's because you arranged the code in a `with` block (a good practice!) that you can't evaluate only part of it. In my example, I put the definition of `one` and the `one.to_numpy` shift-tab in the same cell (without having `one` evaluated from a previous attempt) and saw the same behavior.\r\n\r\nGuessing that that's what the problem is, I'm going to close this issue. If I'm wrong, feel free to tell me so (I'll still see messages here).",
  "created_at":"2022-02-14T21:46:23Z",
  "id":1039600405,
  "issue":559,
  "node_id":"IC_kwDOD6Q_ss499wsV",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-02-14T21:46:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the detailed explanation.\r\n\r\nThe auto-complete engine either deduce type from static analysis or by looking up in-memory objects. Removing `with` allows the second mechanism and solves the problem.",
  "created_at":"2022-02-15T04:27:28Z",
  "id":1039845281,
  "issue":559,
  "node_id":"IC_kwDOD6Q_ss49-seh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-15T04:27:28Z",
  "user":"MDQ6VXNlcjMyMDU5Mzg="
 },
 {
  "author_association":"NONE",
  "body":"PS: I know this does not sounds reasonable, but in some tests I removed the `with`, and the `one` is indeed in the memory, while I still cannot complete it... by that time I think the auto-complete engine sometimes cannot deduce the type of in-memory object.. Yet I cannot reproduce it now....  \r\n\r\nSeems to be a Heisenbug.. :( I dedice to let it go. ",
  "created_at":"2022-02-15T04:32:03Z",
  "id":1039847588,
  "issue":559,
  "node_id":"IC_kwDOD6Q_ss49-tCk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-02-15T04:32:03Z",
  "user":"MDQ6VXNlcjMyMDU5Mzg="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, then we can call this the first formal request for Uproot to have static typing. That would be a _major_ project.\r\n\r\nActually, no: even with full static typing, Python + mypy + Jupyter would not be able to figure out that `obj` has type [Histogram](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TH1.Histogram.html) because that depends on values derived from a file.\r\n\r\nBut there are other places where static typing could help Jupyter discover docstrings without having to evaluate them. Just not this one.",
  "created_at":"2022-02-15T18:26:37Z",
  "id":1040626635,
  "issue":559,
  "node_id":"IC_kwDOD6Q_ss4-BrPL",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2022-02-15T18:26:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ahh so I see you and hear your talk today! LOL",
  "created_at":"2022-08-03T19:50:23Z",
  "id":1204406801,
  "issue":559,
  "node_id":"IC_kwDOD6Q_ss5HycoR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-03T19:50:23Z",
  "user":"MDQ6VXNlcjMyMDU5Mzg="
 },
 {
  "author_association":"MEMBER",
  "body":"As you can see from the code where the warning happens, we prefer `pandas.RangeIndex`, but accept `pandas.Int64Index` for some old versions of Pandas.\r\n\r\nHowever,\r\n\r\n```python\r\ngetattr(pandas, \"RangeIndex\", pandas.Int64Index)\r\n```\r\n\r\ndoes eagerly evaluate `pandas.Int64Index`, and that's a deprecation that will be an error in the future. (It makes you wish for a language in which function arguments can be specified as lazily evaluated: the `getattr` function isn't going to _need_ the default argument if `pandas.RangeIndex` exists.)\r\n\r\nSo I've reorganized this function in a way that should work in both new and old Pandas versions, and also removed our dependence on `distutils`, which is also deprecated in favor of `setuptools`.",
  "created_at":"2022-02-21T15:57:09Z",
  "id":1047023047,
  "issue":562,
  "node_id":"IC_kwDOD6Q_ss4-aE3H",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "heart":1,
   "total_count":2
  },
  "updated_at":"2022-02-21T15:57:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think you're making a mistake (that a lot of people make) in thinking of `TTree.arrays` as a method for computation: it's a method for reading data from disk. I should never have added the ability to evaluate expressions in that method, since this keeps coming up\u2014not just your question, but others in which people are trying to shoehorn multi-statement calculations into the expression strings.\r\n\r\nOne tip-off is that you're comparing with `numexpr.evaluate`, which is a method (and a whole library) for computation. Uproot is just I/O; what computation it does (in the `expressions` and `cut` arguments) is just reading into temporary arrays and evaluating the expression in the string with Python. By adding the convenience of doing that in one line with the reading, I've created the expectation that this is how computation happens, and led a lot of people into unproductive alleys based on that expectation. This is a lesson for me in design: sometimes adding \"shortcuts/conveniences/another way of doing something\" can make things very inconvenient for users\u2014I'm sorry about that!\r\n\r\nIf you want to make the second line possible because you want the `\"Muon_momentum\"` data to be in the file, so that you can give the file to someone else and maybe open it in ROOT, UnROOT, Laurelin, or another Uproot somewhere, the only way that's currently implemented is to read from one file/TTree and write to another. Modifying a TTree in place is not in-principle impossible, but it would be a major project to implement. (I wish I could find the GitHub Issues or Discussions where I've outlined what would be involved, but I can't find them.)\r\n\r\nIf you know all of the above (that `TTree.arrays` is meant for I/O, and `expressions`/`cuts` are really just evaluating in Python) and just want to be able to add variables to the context of the expressions that are evaluated in Python\u2014that is, add `muon_momentum` to the dict of local or global variables that `TTree.arrays` passes into `eval`, then that would be a matter of getting it into this dict:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/027f7a11e4032eb26a05659e5fe1c9371c52f46d/src/uproot/language/python.py#L429\r\n\r\nBut _please don't!_ That would make the I/O-compute confusion even worse!",
  "created_at":"2022-02-21T14:12:10Z",
  "id":1046921763,
  "issue":563,
  "node_id":"IC_kwDOD6Q_ss4-ZsIj",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-02-21T14:12:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That's weird.\r\n\r\nSince Dec 5, 2020, the `uproot4` package has been a single-file package whose contents are created by the installation process:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/916085ae24c382404254756c86afe760acdece56/build-uproot4.sh#L10-L16\r\n\r\nThat is, `uproot4` consists entirely of `from uproot import *`. As a package, `uproot4` requires `uproot>=4.0.0`, so it really is a pass-through of whatever's in `uproot`. The package exists as a shim in case anybody got used to\r\n\r\n```python\r\nimport uproot4\r\n```\r\n\r\nand they don't want to change that one line to\r\n\r\n```python\r\nimport uproot as uproot4\r\n```\r\n\r\nFor the record, name-changing was a terrible idea. Never do it. It has consequences that don't go away.\r\n\r\nAlso, you're correct: Uproot 4.x is a new package with a completely new implementation and mostly the same interface as Uproot 3.x. It's a lot more maintainable and has a superset of Uproot 3.x's capabilities. Because of the deep changes, Uproots 3 and 4 coexisted (in the sense that you could import both in the same Python session), so that users could gradually transition, and particularly because it took so long to finish adding file-writing support to Uproot 4.\r\n\r\nBut if you're ever thinking of doing this, provide one package as a subpackage of the other, as `awkward._v2` is a trial run of version 2 inside `awkward` (and `ROOT::Experimental` is ROOT 7 inside ROOT 6). The package renaming was a terrible idea.\r\n\r\nI don't know what to do about this `uproot4` shim not seeing all of `uproot`'s symbols (including its version). Is it really needed for something?",
  "created_at":"2022-03-01T23:48:14Z",
  "id":1055982022,
  "issue":568,
  "node_id":"IC_kwDOD6Q_ss4-8QHG",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2022-03-01T23:48:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for this answer, this is reassuring. I don't really need \"uproot4\" explicitly I guess, \"uproot\" should be just fine (as long as \">=4\" is specified in requirements.txt). In some scripts I am using both versions explicitly (I have not yet figured out how to replicate uproot3's `flatten=True` option of `tree.arrays()` in uproot4 for library=\"pd\", but I admit that I have not read all the docs yet), this is why I installed uproot4 for naming symmetry reasons :)\r\nEdit: My bad, I just realized this is actually the new default in uproot4, so there is no flatten keyword at all.",
  "created_at":"2022-03-02T00:11:54Z",
  "id":1055996104,
  "issue":568,
  "node_id":"IC_kwDOD6Q_ss4-8TjI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-02T19:27:41Z",
  "user":"MDQ6VXNlcjEyMTQzMDEw"
 },
 {
  "author_association":"MEMBER",
  "body":"I'll be closing this because I can't reproduce it. I created a new virtual machine (to be sure that it doesn't have any Python on it), and ran\r\n\r\n```\r\nubuntu@ip-172-31-20-20:~$ pip install uproot4\r\nDefaulting to user installation because normal site-packages is not writeable\r\nCollecting uproot4\r\n  Downloading uproot4-4.0.0-py3-none-any.whl (6.2 kB)\r\nCollecting uproot>=4.0.0\r\n  Downloading uproot-4.2.2-py2.py3-none-any.whl (298 kB)\r\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 298.4/298.4 KB 12.2 MB/s eta 0:00:00\r\nCollecting numpy\r\n  Downloading numpy-1.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\r\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 16.8/16.8 MB 27.6 MB/s eta 0:00:00\r\nRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from uproot>=4.0.0->uproot4) (59.6.0)\r\nInstalling collected packages: numpy, uproot, uproot4\r\n  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/home/ubuntu/.local/bin' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\nSuccessfully installed numpy-1.22.3 uproot-4.2.2 uproot4-4.0.0\r\nubuntu@ip-172-31-20-20:~$ python\r\nPython 3.10.4 (main, Apr  2 2022, 09:04:19) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot4\r\n>>> uproot4.recreate\r\n<function recreate at 0x7f4668456560>\r\n```\r\n\r\nwhich is what it's supposed to do. Maybe something got tangled in your installation?\r\n\r\nAnyway, the right thing to do nowadays is to install the `uproot` package directly. (Which will smoothly transition over to being version 5 later this year, without any name-change mess, just an ordinary version update.)",
  "created_at":"2022-05-11T21:09:05Z",
  "id":1124297326,
  "issue":568,
  "node_id":"IC_kwDOD6Q_ss5DA2pu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-11T21:09:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"This is an example file to reproduce the error:\r\n[MockSource_mctruth.txt](https://github.com/scikit-hep/uproot4/files/8187500/MockSource_mctruth.txt)\r\n",
  "created_at":"2022-03-04T16:59:06Z",
  "id":1059340176,
  "issue":569,
  "node_id":"IC_kwDOD6Q_ss4_JD-Q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-04T16:59:06Z",
  "user":"U_kgDOBgRteQ"
 },
 {
  "author_association":"MEMBER",
  "body":"I'm digging into this, but the first thing I can say is, try\r\n\r\n```python\r\n>>> events[\"MCTruthEvent/TObject/fBits\"].array(uproot.AsDtype(\">i4\"))\r\n<Array [50331648] type='1 * int32'>\r\n```\r\n\r\nNow I'm looking into why `fBits` is coming up as 8-bit. The streamer says otherwise:\r\n\r\n```python\r\n>>> events.file.streamer_named(\"TObject\").show()\r\nTObject (v1)\r\n    fUniqueID: unsigned int (TStreamerBasicType)\r\n    fBits: unsigned int (TStreamerBasicType)\r\n```\r\n\r\nbut there must be a reason, likely having to do with making other files readable. I'll let you know what I find.",
  "created_at":"2022-03-04T19:23:26Z",
  "id":1059448953,
  "issue":569,
  "node_id":"IC_kwDOD6Q_ss4_Jeh5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-04T19:23:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's because it's being explicitly set to be 8-bit:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/916085ae24c382404254756c86afe760acdece56/src/uproot/interpretation/identify.py#L381-L387\r\n\r\nbecause that was the case in a file made by Delphes, reported in #438 and \"fixed\" in PR #454. As can be seen in the PR write-up, there wasn't anything special in the file saying, \"I'm a weird file with 8-byte fBits!\" If I fix it for this, I'll break it for that. I have to think about what to do about this.\r\n",
  "created_at":"2022-03-04T19:35:02Z",
  "id":1059456424,
  "issue":569,
  "node_id":"IC_kwDOD6Q_ss4_JgWo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-04T19:35:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The Delphes file i uploaded in the linked issue doesn't seem to work on this PR so not sure what the plan is here. Certainly Delphes is a common enough output format that uproot should consider supporting on some level, but not really sure.",
  "created_at":"2022-03-04T23:10:47Z",
  "id":1059595903,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_KCZ_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-04T23:10:47Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't see how we can make both work. The Delphes file has some 4-byte `fBits` and some 6-byte `fBits`, but `fBits` is supposed to be 4 bytes. You can read it without `AsGrouped`, right? If you avoid reading the `fBits`, that works, doesn't it?",
  "created_at":"2022-03-04T23:17:58Z",
  "id":1059598571,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_KDDr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-04T23:17:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yeah but I don't know how Delphes is doing it. It doesn't touch fBits directly.. that's all on ROOT.",
  "created_at":"2022-03-05T04:20:31Z",
  "id":1059680835,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_KXJD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-05T04:20:31Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I thought that Delphes had an independent ROOT-writer\u2014writing ROOT files without using ROOT, the way that Uproot does. (Or maybe I'm thinking of GEANT?)",
  "created_at":"2022-03-05T13:07:23Z",
  "id":1059761682,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_Kq4S",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-05T13:07:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> I thought that Delphes had an independent ROOT-writer\u2014writing ROOT files without using ROOT, the way that Uproot does. (Or maybe I'm thinking of GEANT?)\r\n\r\nI think you're thinking of GEANT. Delphes requires ROOT.",
  "created_at":"2022-03-05T18:19:27Z",
  "id":1059809694,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_K2me",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-05T18:19:27Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"NONE",
  "body":"Thank you @jpivarski. This would solve our issue. (We're also just using ROOT to write out the file.)",
  "created_at":"2022-03-07T11:23:15Z",
  "id":1060565922,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_NvOi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-07T11:23:15Z",
  "user":"U_kgDOBgRteQ"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I realize that it breaks @kratsg's use-case, but this is closest to the correct way to interpret ROOT files. What I'm acknowledging is that I never really fixed #438\u2014I just made it less noisy and broke other people's use-cases in the process. I have to revert that.\r\n\r\nIn the old PR, I asked about just avoiding `AsGrouped` for these `fBits` we don't understand from Delphes. (Admittedly, Delphes is just using ROOT, so what is not understood is what ROOT is doing, but Delphes is presumably making dictionaries in a different way.) Is it possible for your workflow to run without `AsGrouped`, @kratsg? You weren't actually interested in the `fBits` values themselves.\r\n\r\nI'll be merging this PR. If we have to do something to fix the original #438, it would be a separate PR. (Maybe `AsGrouped` should just ignore `fBits`... by name? That seems arbitrary.)",
  "created_at":"2022-03-07T17:38:12Z",
  "id":1060948890,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_PMua",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-07T17:38:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yeah, I'll drop `fBits` -- but we need to probably loop up with coffea to update the code there to make sure coffea doesn't break.. @nsmith- ",
  "created_at":"2022-03-07T17:40:55Z",
  "id":1060951532,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_PNXs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-07T17:40:55Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"NONE",
  "body":"Thanks @jpivarski - will there be a new tag with this fix? ",
  "created_at":"2022-03-08T09:58:59Z",
  "id":1061601404,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_RsB8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-08T09:58:59Z",
  "user":"U_kgDOBgRteQ"
 },
 {
  "author_association":"MEMBER",
  "body":"It's [4.2.1](https://github.com/scikit-hep/uproot4/releases/tag/4.2.1).",
  "created_at":"2022-03-08T15:29:55Z",
  "id":1061902280,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_S1fI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-08T15:29:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Yeah, I'll drop `fBits` -- but we need to probably loop up with coffea to update the code there to make sure coffea doesn't break.. @nsmith-\r\n\r\nI'll need some help here. I search for `fBits` in coffea and get nothing https://github.com/CoffeaTeam/coffea/search?q=fBits",
  "created_at":"2022-03-08T21:53:00Z",
  "id":1062249775,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_UKUv",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-03-08T21:53:00Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Oh, it looks like it was already removed back then when I was sturggling with this.. and I'm on an old version of my old branch \ud83d\ude05 my bad....",
  "created_at":"2022-03-09T00:13:37Z",
  "id":1062417295,
  "issue":570,
  "node_id":"IC_kwDOD6Q_ss4_UzOP",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-03-09T00:13:37Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"If this is related to the hack for performance in doubly-nested structures, that hack will need to give way to AwkwardForth at some point. Adding an AwkwardForth implementation to all types is @aryan26roy's May\u2012July 2022 project, so... soon!\r\n\r\nI actually hadn't realized that you were still relying on that hack. It doesn't even work for all doubly-nested structures, just a particular combination of vectors and pointers.\r\n\r\nIs it the case that 007b10014003a74d0e89e0166e21a6218de0d445 is preventing the code flow from entering the gateway into that hack, which is this block:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/be92c96e9c3f73ee08277f20e68b81a489f8fdcb/src/uproot/interpretation/objects.py#L142-L154\r\n\r\n?\r\n\r\nI've started downloading the file to check.",
  "created_at":"2022-03-14T15:36:58Z",
  "id":1066963522,
  "issue":572,
  "node_id":"IC_kwDOD6Q_ss4_mJJC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-14T15:36:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the quick answer! Indeed, we are heavily relying on these doubly-nested structures, unfortunately. It's roughly 20% of your MC and reconstruction data `;)`\r\n\r\nYes I can confirm that `awkward_can_optimize` returns `True` for `uproot 4.1.2` but `False` for `uproot 4.1.3`.\r\n\r\nEdit: messed up `False` and `True` `;)`",
  "created_at":"2022-03-14T16:04:17Z",
  "id":1067000420,
  "issue":572,
  "node_id":"IC_kwDOD6Q_ss4_mSJk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-14T16:14:52Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"The backdoor in Awkward Array relies on the type matching exactly, and 007b10014003a74d0e89e0166e21a6218de0d445 changes the `\"uproot\"` parameter by one key-value pair. I think that including the `\"inner_shape\"` at that point was a mistake, so if @nsmith- actually needs it, I'll show him how to get it through the NumpyForm instead (and then merging this would require coordination with Coffea, so I hope it's simpler than that).\r\n\r\nBut this is a good reminder of how much of a difference this makes. It will be great to get this all converted to AwkwardForth, and then I won't have to keep calling it a \"hack.\" (Maybe I could call it a \"prototype that we didn't go with, in favor of AwkwardForth.\")",
  "created_at":"2022-03-14T18:35:35Z",
  "id":1067157325,
  "issue":572,
  "node_id":"IC_kwDOD6Q_ss4_m4dN",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-03-14T18:35:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Many thanks for the quick fix `:)`",
  "created_at":"2022-03-14T21:14:41Z",
  "id":1067300581,
  "issue":572,
  "node_id":"IC_kwDOD6Q_ss4_nbbl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-14T21:14:41Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and also @nsmith-: you didn't actually need the `inner_shape` to be in the parameters, did you? (That information is in the NumpyForm itself. Adding it to the parameters made the back-door for this data type in Awkward to not recognize it.)\r\n\r\nThe AsArray has `inner_shape`, too. The information is not lost\u2014I just don't know if you're relying on it in Coffea. (If so, I'll tell you where to get the information from instead of the parameter.)",
  "created_at":"2022-03-14T18:31:53Z",
  "id":1067153986,
  "issue":573,
  "node_id":"IC_kwDOD6Q_ss4_m3pC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-14T18:31:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It looks like in coffea we don't care about any of the uproot fields in the parameters https://github.com/CoffeaTeam/coffea/blob/ff7878ce20db5d425134808ecc0c1a55fddaa5fa/coffea/nanoevents/mapping/uproot.py#L59 so I think its probably safe to delete this parameter. I cannot be sure why I added it there.",
  "created_at":"2022-03-14T19:20:16Z",
  "id":1067197269,
  "issue":573,
  "node_id":"IC_kwDOD6Q_ss4_nCNV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-14T19:20:16Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks @jpivarski I confirm that it works now just like before 4.1.3! `:)`",
  "created_at":"2022-03-14T19:35:35Z",
  "id":1067210661,
  "issue":573,
  "node_id":"IC_kwDOD6Q_ss4_nFel",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-14T19:35:35Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"> I cannot be sure why I added it there.\r\n\r\nProbably because you were adding it to the constructor, and there are other constructor arguments in the parameters, so I think you were going for symmetry.\r\n\r\nOkay, I'll merge this and make a release!",
  "created_at":"2022-03-14T20:00:15Z",
  "id":1067229786,
  "issue":573,
  "node_id":"IC_kwDOD6Q_ss4_nKJa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-14T20:00:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's not compression\u2014that's where you see the error because decompression does a checksum and catches the error before you get any farther. The error is most likely a matter of Uproot's file writer declaring that a header has N bytes when it actually has M bytes, with the M \u2012 N difference coming from the difference between the length of the string and the length of the bytestring.\r\n\r\n```python\r\n>>> len(\"\ud83d\ude00\")\r\n1\r\n>>> len(\"\ud83d\ude00\".encode(\"utf-8\"))\r\n4\r\n```\r\n\r\nSomewhere in src/uproot/writing, there is very likely a `len(name)` that should be `len(name.encode(\"utf-8\"))`.\r\n\r\nI tried it with \u03c0 (`\"\\u03c0\"`) because the emoji characters are in a newer table system of Unicode than the more basic non-ASCII characters. For instance, Java would get them wrong. The test with \u03c0 revealed the same thing.\r\n\r\nI also verified that ROOT can be used as a test of a properly made file:\r\n\r\n```python\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"tmp2.root\", \"RECREATE\")\r\n>>> h = ROOT.TH1F(\"\\u03c0\", \"\", 100, -5, 5)\r\n>>> h.FillRandom(\"gaus\", 10000)\r\n>>> h.Write()\r\n437\r\n>>> f.Close()\r\n```\r\n\r\n```python\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"tmp2.root\")\r\n>>> f.ls()\r\nTFile**\t\ttmp2.root\t\r\n TFile*\t\ttmp2.root\t\r\n  KEY: TH1F\t\u03c0;1\t\r\n>>> h = f.Get(\"\\u03c0\")\r\n>>> h.Draw()\r\nInfo in <TCanvas::MakeDefCanvas>:  created default TCanvas with name c1\r\n```\r\n\r\nSo it's Uproot-writing's problem, and not \"Uproot-reading and ROOT-reading\"'s problem. At least that narrows it down.",
  "created_at":"2022-03-21T16:23:11Z",
  "id":1074115873,
  "issue":576,
  "node_id":"IC_kwDOD6Q_ss5ABbUh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-21T16:23:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Somewhere in src/uproot/writing, there is very likely a `len(name)` that should be `len(name.encode(\"utf-8\"))`.\r\n\r\nYour idea looks to be correct, this seems to be the relevant part (for this example at least):\r\nhttps://github.com/scikit-hep/uproot4/blob/8e3ab33c2d137079db53cd2c21b4c3ac7e1dcffc/src/uproot/writing/_cascade.py#L1605-L1607\r\n\r\ndiff:\r\n\r\n```diff\r\n- strings_size += (1 if len(name) < 255 else 5) + len(name)  \r\n+ strings_size += (1 if len(name) < 255 else 5) + len(name.encode(\"utf-8\"))\r\n```\r\n\r\nPerhaps the other lines would also have to change for other cases, they do not for this example though.\r\n\r\nI am not sure to what extent this currently affects files written with Unicode characters and without compression. The histogram reading seems to be working fine in that case. I am surprised by this, but perhaps it is just a coincidence that this works anyway.",
  "created_at":"2022-03-21T17:51:32Z",
  "id":1074226863,
  "issue":576,
  "node_id":"IC_kwDOD6Q_ss5AB2av",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-21T17:51:32Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"In fact, there are other places where it was already right, such as this one:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/8e3ab33c2d137079db53cd2c21b4c3ac7e1dcffc/src/uproot/writing/_cascadetree.py#L842-L846\r\n\r\n(This is an advertisement for putting things into helper functions instead of doing them inline. Then you can catch all the places where you do the same sort of thing, and can ensure that it's right.)\r\n\r\nI'll make a PR with this fix. Thanks for finding it!",
  "created_at":"2022-03-21T18:52:56Z",
  "id":1074288390,
  "issue":576,
  "node_id":"IC_kwDOD6Q_ss5ACFcG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-21T18:52:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thank you!",
  "created_at":"2022-03-21T19:01:19Z",
  "id":1074298432,
  "issue":576,
  "node_id":"IC_kwDOD6Q_ss5ACH5A",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-21T19:01:19Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Working with a single file:\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> import skhep_testdata\r\n>>> filename1 = skhep_testdata.data_path(\"uproot-Zmumu.root\") + \":events\"\r\n>>> filename2 = skhep_testdata.data_path(\"uproot-Zmumu-uncompressed.root\") + \":events\"\r\n>>> filename3 = skhep_testdata.data_path(\"uproot-Zmumu-zlib.root\") + \":events\"\r\n>>> filename4 = skhep_testdata.data_path(\"uproot-Zmumu-lzma.root\") + \":events\"\r\n>>> from pprint import pprint as pp      # For pretty printing\r\n>>>\r\n>>> pp(uproot.dask(filename1)\r\n... )\r\n{'E1': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'E2': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Event': dask.array<from-value, shape=(2304,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'M': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Q1': dask.array<from-value, shape=(2304,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Q2': dask.array<from-value, shape=(2304,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Run': dask.array<from-value, shape=(2304,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Type': dask.array<from-value, shape=(2304,), dtype=object, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'eta1': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'eta2': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'phi1': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'phi2': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pt1': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pt2': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'px1': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'px2': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'py1': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'py2': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pz1': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pz2': dask.array<from-value, shape=(2304,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>\r\n}\r\n```\r\n\r\nFor multiple files (concatenation with each file as a single chunk):\r\n```python\r\n>>> pp(uproot.dask([filename1, filename2, filename3, filename4]))\r\n{'E1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'E2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Event': dask.array<concatenate, shape=(9216,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'M': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Q1': dask.array<concatenate, shape=(9216,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Q2': dask.array<concatenate, shape=(9216,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Run': dask.array<concatenate, shape=(9216,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Type': dask.array<concatenate, shape=(9216,), dtype=object, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'eta1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'eta2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'phi1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'phi2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pt1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pt2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'px1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'px2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'py1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'py2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pz1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pz2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>}\r\n```\r\n\r\nNot implemented error when a different library is used (currently default is \"np\"):\r\n```python\r\n>>> uproot.dask(filename1,library=\"ak\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/kmk/uprootall/uproot4/venv/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 709, in dask\r\n    raise NotImplementedError()\r\nNotImplementedError\r\n```\r\n\r\nExample calling `compute()` on array:\r\n```python\r\n>>> dask_dict = uproot.dask([filename1, filename2, filename3, filename4])\r\n>>> dask_dict['px1']\r\ndask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>\r\n>>> dask_dict['px1'].compute()\r\narray([-41.19528764,  35.11804977,  35.11804977, ...,  32.37749196,\r\n        32.37749196,  32.48539387])\r\n```",
  "created_at":"2022-03-24T11:37:51Z",
  "id":1077533373,
  "issue":578,
  "node_id":"IC_kwDOD6Q_ss5AOdq9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-24T11:37:51Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Error checking when dask isn't installed:\r\n```python\r\n>>> uproot.dask(filename1)\r\nTraceback (most recent call last):\r\n  File \"/home/kmk/uprootall/uproot4/env/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 776, in dask\r\n    import dask\r\nModuleNotFoundError: No module named 'dask'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/kmk/uprootall/uproot4/env/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 779, in dask\r\n    raise ModuleNotFoundError(\r\nModuleNotFoundError: The uproot.dask function requires dask as a dependancy. Please use one of the commands below, depending on your requirements.\r\npython -m pip install dask                # Install only core parts of dask\r\npython -m pip install \"dask[complete]\"    # Install everything\r\npython -m pip install \"dask[array]\"       # Install requirements for dask array\r\n```",
  "created_at":"2022-03-24T11:59:11Z",
  "id":1077551177,
  "issue":578,
  "node_id":"IC_kwDOD6Q_ss5AOiBJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-24T11:59:11Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"This is looking very nice! It definitely qualifies. I just cloned it and tried out all of the features, including adding a print-out to the `TBranch.array` method just to verify that it doesn't get evaluated until `compute`.\r\n\r\nIt's great as-is, but if this were the normal coding period, there would be next steps (as always in development...):\r\n\r\n   1. New features need tests. The test would be a new file in `tests` using this PR number and feature name (`test_0578-dask-for-numpy.py`) that does `pytest.importorskip` to only run the test if Dask is available. (See examples in the other tests.) New testing dependencies can be added in `setup.py` and `.github/workflows/build-test.yml`.\r\n   2. Suppose that users want smaller chunk sizes than the size of the file (e.g. files are too big). If they repartition the Dask array to change its chunk sizes, the repartitioning step in the DAG would still call `ttree[key].array`, which reads an entire TBranch from disk, just to return a smaller slice. (That is, repartitioning would make the problem worse by making some nodes in the DAG redundantly read what other nodes have already read.) The `ttree[key].array` function has `entry_start` and `entry_stop` arguments that restrict what it reads to an interval of entries. How would you add a `step_size` argument to `uproot.dask` in analogy to the one in [uproot.lazy](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.lazy.html)?\r\n      a. Only for the case in which `step_size` is an integer?\r\n      b. Also for the case in which `step_size` is a string, representing memory size? Hint: see [TTree.num_entries_for](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#num-entries-for).\r\n   3. Sometimes, just opening files is an expensive operation that we want to avoid on the Dask client (e.g. there are too many files). We want to delay the tasks of opening the files and getting TTree objects out of them, so that this is only done on distributed workers, not locally on the user's client. This would have some drawbacks: you can only know the names and dtypes of the TBranches if you've read at least one TTree metadata (and assume they all have the same names and dtypes), and you can only know the lengths and chunk sizes of the arrays if you've read all TTree metadatas. It would be pretty hard to do much without knowing the TBranch names and dtypes, but [Dask allows arrays with unknown chunk sizes](https://docs.dask.org/en/stable/array-chunks.html#unknown-chunks). How would you add a `open_files=False` option that (a) reads TTree metadata only from the first file, (b) assumes all files have the same names and dtypes, and (c) creates Dask arrays with unknown chunks that open files upon `compute`? Unlike (2), this goes beyond what `uproot.lazy` could do.",
  "created_at":"2022-03-24T20:24:16Z",
  "id":1078193548,
  "issue":578,
  "node_id":"IC_kwDOD6Q_ss5AQ-2M",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-03-24T20:24:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I have written some tests as mentioned in point 1 above. They're in `tests/test_0578-dask-for-numpy.py`.\r\n\r\nI have also pushed some basic code for `step_size`. It works perfectly when `step_size` is an integer as documented below.\r\n\r\nImports:\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> filename1 = skhep_testdata.data_path(\"uproot-Zmumu.root\") + \":events\"\r\n>>> filename2 = skhep_testdata.data_path(\"uproot-Zmumu-uncompressed.root\") + \":events\"\r\n>>> from pprint import pprint as pp      # For pretty printing\r\n```\r\n\r\n2 different numbered chunk-sizes:\r\n```python\r\n>>> pp( uproot.dask(filename1,step_size=30) )   # Numbered chunk size\r\n{'E1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'E2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'Event': dask.array<concatenate, shape=(2304,), dtype=int32, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'M': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'Q1': dask.array<concatenate, shape=(2304,), dtype=int32, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'Q2': dask.array<concatenate, shape=(2304,), dtype=int32, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'Run': dask.array<concatenate, shape=(2304,), dtype=int32, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'Type': dask.array<concatenate, shape=(2304,), dtype=object, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'eta1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'eta2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'phi1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'phi2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'pt1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'pt2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'px1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'px2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'py1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'py2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'pz1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>,\r\n 'pz2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(30,), chunktype=numpy.ndarray>}\r\n\r\n>>> pp( uproot.dask(filename1,step_size=56) )   # Numbered chunk size\r\n{'E1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'E2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'Event': dask.array<concatenate, shape=(2304,), dtype=int32, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'M': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'Q1': dask.array<concatenate, shape=(2304,), dtype=int32, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'Q2': dask.array<concatenate, shape=(2304,), dtype=int32, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'Run': dask.array<concatenate, shape=(2304,), dtype=int32, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'Type': dask.array<concatenate, shape=(2304,), dtype=object, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'eta1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'eta2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'phi1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'phi2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'pt1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'pt2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'px1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'px2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'py1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'py2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'pz1': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>,\r\n 'pz2': dask.array<concatenate, shape=(2304,), dtype=float64, chunksize=(56,), chunktype=numpy.ndarray>}\r\n```\r\n\r\nThe dask concatenate and multidim tests that were written earlier, still pass (but take significantly more time for the reason explained below) which means the data seems to be passed on correctly.\r\n\r\nKnown issue:  When `step_size` is given a memory string, the chunk-size always ends up being `(1,)` which is 1 record per chunk and this is slowing the function down significantly. I think I have used the `TBranch.num_entries_for` function incorrectly, so some further help on that would be greatly appreciated!",
  "created_at":"2022-04-06T12:01:08Z",
  "id":1090187635,
  "issue":578,
  "node_id":"IC_kwDOD6Q_ss5A-vFz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-06T12:01:08Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"![image](https://user-images.githubusercontent.com/53650538/171455676-b94d7565-cc0e-4239-8cc8-497388eaec8f.png)\r\nAs discussed, I checked and confirmed that the current implementation of this function makes smaller chunks at the end of intermediate files instead of a direct concatenation.",
  "created_at":"2022-06-01T16:38:48Z",
  "id":1143856020,
  "issue":578,
  "node_id":"IC_kwDOD6Q_ss5ELduU",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-06-01T16:38:48Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Provided these test workflows run properly, I'm done from my side regarding points 1 and 2, including tests.\r\n>1. New features need tests. The test would be a new file in tests using this PR number and feature name (test_0578-dask-for-numpy.py) that does pytest.importorskip to only run the test if Dask is available. (See examples in the other tests.) New testing dependencies can be added in setup.py and .github/workflows/build-test.yml.\r\n>2. Suppose that users want smaller chunk sizes than the size of the file (e.g. files are too big). If they repartition the Dask array to change its chunk sizes, the repartitioning step in the DAG would still call ttree[key].array, which reads an entire TBranch from disk, just to return a smaller slice. (That is, repartitioning would make the problem worse by making some nodes in the DAG redundantly read what other nodes have already read.) The ttree[key].array function has entry_start and entry_stop arguments that restrict what it reads to an interval of entries. How would you add a step_size argument to uproot.dask in analogy to the one in [uproot.lazy](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.lazy.html)?\r\na. Only for the case in which step_size is an integer?\r\nb. Also for the case in which step_size is a string, representing memory size? Hint: see [TTree.num_entries_for](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#num-entries-for).",
  "created_at":"2022-06-01T17:02:54Z",
  "id":1143882332,
  "issue":578,
  "node_id":"IC_kwDOD6Q_ss5ELkJc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-01T17:02:54Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"The current PR removes the \"padded\" flow bins at read time such that `hist.Hist.values(flow=True)` and `TH1.values(flow=True)` return the same, which is what the current tests rely on.\r\n\r\nInstead of doing that we can return flow bins even if not present in the original `hist.Hist` padded with 0/Nans. I don't have a particular preference, but it will no longer be consistent with the hist behaviour. \r\n\r\nI think masked array would be a way to indicate whether over/underflow was present in the original object, but indeed I assumed that would be causing backcomp issues, so it's not done here.",
  "created_at":"2022-04-01T16:52:05Z",
  "id":1086132486,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5AvREG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-01T16:52:05Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"We want this to be consistent with hist (that's what the UHI is supposed to do). I'm going to ask @henryiii to look at it, because he has a better perspective of histogram consistency.",
  "created_at":"2022-04-01T16:58:54Z",
  "id":1086137873,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5AvSYR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-01T16:58:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't want to let this go unaddressed. I checked it out today and tried some tests with it; one of the things that I found is that it breaks `h_read.to_hist()` because the implementation of [TH1.to_boost](https://github.com/scikit-hep/uproot4/blob/c968c5fa26047f0e21614854b98e07de7c4789c8/src/uproot/behaviors/TH1.py#L295-L327) depends on `TH1.values(flow=False)` having a different shape from `TH1.values(flow=True)`, which this PR eliminates.\r\n\r\nHere's a comparison between `h_read` (an Uproot TH1 object, which always has flow bins because it's from ROOT) and `h` (a hist histogram with no flow bins):\r\n\r\n```python\r\n>>> import uproot, hist, numpy as np, mplhep as hep\r\n>>> \r\n>>> h = hist.new.Reg(20, 0, 20, name='msd', flow=False).Weight().fill(np.random.normal(10, 6, 1000))\r\n>>> \r\n>>> with uproot.recreate('test.root') as fout:\r\n...      fout['test'] = h\r\n... \r\n>>> with uproot.open('test.root') as fin:\r\n...      h_read = fin['test']\r\n... \r\n>>> h.values(flow=False).shape\r\n(20,)\r\n>>> h_read.values(flow=False).shape\r\n(20,)\r\n>>> \r\n>>> h.values(flow=True).shape\r\n(20,)\r\n>>> h_read.values(flow=True).shape\r\n(20,)\r\n>>> \r\n>>> h.axes[0].edges.shape\r\n(21,)\r\n>>> h_read.axes[0].edges(flow=False).shape\r\n(21,)\r\n>>> \r\n>>> h.axes[0].edges.shape\r\n(21,)\r\n>>> h_read.axes[0].edges(flow=True).shape\r\n(23,)\r\n```\r\n\r\nYet another difference is that `edges` is a property in hist and a method in Uproot. It has to be a method to take a `flow` argument, and it needs a `flow` argument to include or exclude the overflow bins.\r\n\r\n```python\r\n>>> h_read.axes[0].edges(flow=False)\r\narray([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\r\n       13., 14., 15., 16., 17., 18., 19., 20.])\r\n>>> h_read.axes[0].edges(flow=True)\r\narray([-inf,   0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,\r\n        10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,\r\n        inf])\r\n```\r\n\r\nClearly, boost-histogram/hist and Uproot are interpreting these differently. I might have thought that Uproot was not adhering to the [UHI protocol](https://uhi.readthedocs.io/en/latest/plotting.html), but the protocol doesn't specify the `flows` argument or `edges` property/method. So they disagree in their interpretation of unspecified extensions.\r\n\r\nDigging yet further into this, I noticed that a hist histogram with flow bins _does_ interpret the `flow` argument the same way Uproot (in `main`) does:\r\n\r\n```python\r\n>>> h2 = hist.new.Reg(20, 0, 20, name='msd', flow=True).Weight().fill(np.random.normal(10, 6, 1000))\r\n>>> h2.values(flow=False).shape\r\n(20,)\r\n>>> h2.values(flow=True).shape\r\n(22,)\r\n>>> h2.axes[0].edges.shape\r\n(21,)\r\n```\r\n\r\nThe difference is just that ROOT histograms always have flow bins\u2014you can't turn them off.\r\n\r\nGiven the difference in data model between n-dimensional, mixed-axis, optional-flow boost-histogram/hist histograms and 1\u20123-dimensional, floating-point-axis, required-flow ROOT histograms, it's understandable that serializing through ROOT won't preserve boost-histogram/hist in a lossless way. Lossless serialization is [being discussed elsewhere](https://github.com/scikit-hep/boost-histogram/discussions/726).\r\n\r\nSo as I currently understand it, this PR is not correct (it's fixing something that's not broken, and breaking it in the process) because in `main`, Uproot histograms and hist histograms with flow bins agree on the meaning of the `flow` argument. Uproot histograms and hist histograms do not agree on `edges`. If hist is to be taken as the standard, then Uproot needs to turn `edges` into a property with `flow` always `False`. These `edges` are used in [TH1.to_numpy](https://github.com/scikit-hep/uproot4/blob/c968c5fa26047f0e21614854b98e07de7c4789c8/src/uproot/behaviors/TH1.py#L275-L293), in which the interpretation of the flow bins being bins with infinite area is convenient, but could be reimplemented directly in `to_numpy`.\r\n\r\n@henryiii, is this right?\r\n\r\nEven though `flow` and `edges` weren't accepted into the specification, could they form some \"deuterocanonical\" or \"optional\" part of the spec, perhaps with terminology like \"may\" instead of \"must\"? It seems we need something to break the symmetry of \"if you're going to implement `edges`, please do it like this, not that.\"",
  "created_at":"2022-04-04T22:15:27Z",
  "id":1088065714,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A2pCy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-04T22:15:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I sadly have to say I don't really see how the behaviour qualifies as non-broken. The default read, without specifying flow is \r\n```\r\nh.values().shape, h_read.values().shape\r\n>>> ((20,), (18,))\r\n```\r\nwhere I would expect both to match \r\n```\r\nh.axes[0].edges.shape, h_read.axes[0].edges().shape\r\n>>> ((21,), (21,))\r\n```\r\nI don't think user should be required to externally bookkeep or \"duck type\" whether flow bins were used or not to read the histogram back correctly. The current behaviour simply treats edge bins as flow bins regardless if they in fact are flow bins. In particular, I find the following combo to be super confusing\r\n````\r\nh.values(flow=False).shape, h_read.values(flow=True).shape\r\n>>> ((20,), (20,))\r\n````\r\n~ number of values same for `flow=False/True`\r\n````\r\nh.values(flow=False).shape, h_read.values(flow=False).shape\r\n>>> ((20,), (18,))\r\n````\r\n~ number of values different for `flow=False`, which is the default. I don't really see a way that on inspecting a random TH1 anyone won't be confused with a histogram of 18 values and 21 edges.\r\n\r\n>  it breaks h_read.to_hist()\r\n\r\nAh, guess a round trip test is missing. \r\n\r\n> implementation of [TH1.to_boost](https://github.com/scikit-hep/uproot4/blob/c968c5fa26047f0e21614854b98e07de7c4789c8/src/uproot/behaviors/TH1.py#L295-L327) depends on TH1.values(flow=False) having a different shape from TH1.values(flow=True)\r\n\r\nThis could probably be made to work with the proposed change. Indeed `h.values(flow=False)` and `h.values(flow=True)` already have the same shape if `flow=False`, so there already is an inconsistency to `hist`. \r\n\r\nAt a minimum, the flow bins should be filled with nans/zeros if missing at write time to prevent this strangeness. Whether these are then stripped back at read time under the hood of `.values()` (as proposed in the PR) or just returned plain, I don't have a strong opinion on. The round trip wouldn't be perfect but would hit a steady state after one iteration. ",
  "created_at":"2022-04-04T23:22:59Z",
  "id":1088110150,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A2z5G",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-04T23:22:59Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"Something seemed amiss at the beginning, though it took me some time to remember this stuff well enough to say what it is. The problem is that my interpretation of the `flow` argument in `values` and `variances` is that it should change the `shape` of the return value. This PR makes it not change the `shape` of the return value, breaking my understanding of what this `flow` argument means.\r\n\r\nThe ultimate problem here is that `flow` is not specified. There wasn't agreement among all parties to require `flow` in UHI, but @henryiii and I both wanted to include it in Uproot and hist, and we thought that we were understanding it in the same way. That was wrong, and the subtleties are just starting to come out now.\r\n\r\nThe shape of the return value of `values` and `variances` depends on `flow` in hist, not just Uproot, so that part of my understanding is right. However, hist has the added complication that some histograms don't have flow bins.\r\n\r\n```python\r\n>>> without_flow = hist.new.Reg(20, 0, 20, name='msd', flow=False).Weight().fill(np.random.normal(10, 6, 1000))\r\n>>> with_flow = hist.new.Reg(20, 0, 20, name='msd', flow=True).Weight().fill(np.random.normal(10, 6, 1000))\r\n\r\n>>> without_flow.values(flow=False).shape\r\n(20,)\r\n>>> without_flow.values(flow=True).shape\r\n(20,)\r\n\r\n>>> with_flow.values(flow=False).shape\r\n(20,)\r\n>>> with_flow.values(flow=True).shape\r\n(22,)\r\n```\r\n\r\nWhen a hist histogram does have flow bins, `flow` changes the shape of `values`. It does that in Uproot's main branch, too, so that's a point of agreement. However, it does not do that in this PR, and that's why I'm saying that it's \"breaking\" this feature.\r\n\r\nThe surprise, for me, is that on a hist histogram without flow bins, `flow` does not change the shape of `values`. Some fake values have to be added when converting a hist histogram into a TH1 subclass in a ROOT file, and that's what's not happening here (hist satisfies the `try` block; the `except` shows what would happen without the `flow` argument\u2014we'd always assume `flow=False`):\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/c968c5fa26047f0e21614854b98e07de7c4789c8/src/uproot/writing/identify.py#L261-L280\r\n\r\nUproot's writing code assumes that `obj.values(flow=True)` will include the flow bins if they exist and create them if they do not. That assumption is wrong, and hence the above is a bug.\r\n\r\nThe question is, where's the bug: in Uproot or in hist? One school of thought is that Uproot is right and hist histograms without flow bins should create fake flow bins when `flow=True`. Another school of thought is that hist is right and Uproot should be the one to create those fake flow bins. We'll need some guidance from the spec: even though UHI isn't going to require adherents to implement a `flow` argument, it should provide some agreement on how it ought to behave if it _is_ implemented.\r\n\r\nI'm with you on the user not being required to know whether a histogram has flow bins: that would be the most useful. I would have thought that the whole reason one wants a `flow` argument is because they can't remember, don't know, or don't want to know whether a particular histogram has flow bins, so they say `flow=True` and always get flow bins or `flow=False` and always not get flow bins. That's the \"Uproot is right\" school of thought. Talking through this is convincing me\u2014I think I should open an issue on hist (or UHI) to say that it should behave the way I think it should.\r\n\r\nThe discrepancy with `edges` being a method or a property is separate, though it's also a case where we'd like guidance from the spec on non-required parts of the spec.\r\n\r\nIn summary, though, you're absolutely right that users shouldn't have to keep track of this, but this PR is the wrong fix. Uproot and hist are in agreement (at least) that `flow=True` is supposed to change the shape of `values` for a histogram with flow bins.",
  "created_at":"2022-04-05T00:23:58Z",
  "id":1088147421,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A28_d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T00:23:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hist cannot \"invent\" empty flow bins with `flow=True`, since _both `flow=True` and `flow=False` are no-copy operations_. You are, in both cases, getting a view on the underlying data, regardless of it it has flow bins (using strides and offsets). If we invented flow bins and filled them with NaNs or 0s (and there would be disagreement on which one), then we'd have to copy _the entire histogram's data_ to do so.\r\n\r\n`flow=True` means \"give me the entire histogram, I understand flow bins and how to deal with them\", while `flow=False` is the default because it's simple, requires a less-complex mental model, and is enough for most uses. If you don't have flow bins to begin with, they are equivalent.",
  "created_at":"2022-04-05T00:53:21Z",
  "id":1088164196,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A3BFk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T00:54:01Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I actually agree with this. I haven't really spent much time with flow bins prior to this issue happening, so never gave it any thought but it seems more intuitive to me that passing `flow=True` would return the same shape every time whether the flow bins existed or not (still either nans or zeros). Aka \"uproot is right\"\r\n\r\nI am still not sure what you mean that is wrong about the PR. The `flow=True/False` should still be changing the return shape if the flow bins existed in the first place. As implemented only the nan padded dimensions are stripped at read time and they are padded only if the flow bins were missing.\r\n",
  "created_at":"2022-04-05T00:58:14Z",
  "id":1088166209,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A3BlB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T00:58:14Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"I had to write that whole issue twice because GitHub lost it with a connectivity flicker the first time. Sorry I missed your comments.\r\n\r\n> If we invented flow bins and filled them with NaNs or 0s (and there would be disagreement on which one), then we'd have to copy _the entire histogram's data_ to do so.\r\n\r\nThat is correct: what I'm proposing would make `obj.values(flow=True)` a copy operation when `obj` does not have built-in flow bins. Did we say somewhere that `values` has to be a view?\r\n\r\nIt's certainly an option to keep hist's behavior the way it is and change Uproot to not assume that hist will create those bins. In fact, it amounts to removing the `try` block (above) and keeping only the `except`. That would do the copy that we need to impute flow bins, to fit TH1's data model. (Still, there's a choice of zero versus nan: the code above and I both vote for zero.)\r\n\r\nHowever, this really isn't what I thought the `flow` argument meant: I thought it meant \"ensure there are flow bins when `flow=True` and ensure there are not flow bins when `flow=False`, so that I don't need to know if the object has built-in flow bins or not.\" I thought it was a way of regularizing inputs into a single, explicitly specified format. Since TH1s always have flow bins, I can go on thinking that about Uproot's histograms, but hist's interpretation seems a bit subtle.\r\n\r\n> I am still not sure what you mean that is wrong about the PR. The `flow=True/False` should still be changing the return shape if the flow bins existed in the first place.\r\n\r\nIn this PR (quoting from above),\r\n\r\n```python\r\n>>> import uproot, hist, numpy as np, mplhep as hep\r\n>>> \r\n>>> h = hist.new.Reg(20, 0, 20, name='msd', flow=False).Weight().fill(np.random.normal(10, 6, 1000))\r\n>>> \r\n>>> with uproot.recreate('test.root') as fout:\r\n...      fout['test'] = h\r\n... \r\n>>> with uproot.open('test.root') as fin:\r\n...      h_read = fin['test']\r\n... \r\n>>> h.values(flow=False).shape\r\n(20,)\r\n>>> h_read.values(flow=False).shape\r\n(20,)\r\n>>> \r\n>>> h.values(flow=True).shape\r\n(20,)\r\n>>> h_read.values(flow=True).shape\r\n(20,)\r\n```\r\n\r\n`h_read` has built-in flow because it is a TH1, but `h_read.values(flow=False)` and `h_read.values(flow=True)` both return arrays with the same shape: `(20,)`. Uproot and hist already agree on this point: when a histogram has built-in flow (like `h_read`), the `flow` argument should change the shape of the return value. That's what I mean about this PR breaking that feature.",
  "created_at":"2022-04-05T01:31:05Z",
  "id":1088181487,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A3FTv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T01:31:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"To me, `flow=True` means \"give me all the flow bins you have\", not \"inject flow bins even if they don't apply\". Setting `underflow=False` or `overflow=False` on an axis means that flow doesn't apply. If I have a radius, with only overflow, I don't want to suddenly get both underflow and overflow, I just want to get that one-sided overflow bin that is normally hidden. If I have a Boolean axes, I don't want flow bins inject to it - they are meaningless. It is also very useful that it's no copy, as then `.view(flow=...)` matches `.values(flow=...)`, and `.view(...)` is guaranteed to be no-copy.\r\n\r\nIf you really want this behavior, I'd recommend adding `.values(flow=True, fill=0)` and `.values(flow=True, fill=math.nan)`. This will remove the ambiguity with what to fill it with (which there's already disagreement even with just two opinions), and will not break the (to me) the most important reason to have this argument, the \"please give me exactly what you have\" case.\r\n\r\nA user can always check the axis.traits if they need to handle flow bins differently. It is rather annoying that there's no `.edges(flow=True)` (I originally had those as methods, but @HDembinski changed them all into properties in https://github.com/scikit-hep/boost-histogram/pull/121 three years ago), but `.to_numpy(flow=True)` actually gives you the `flow=True` edges if you don't want to check `axes.traits`.",
  "created_at":"2022-04-05T03:31:44Z",
  "id":1088231289,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A3Rd5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T03:38:28Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"This is also undefined for axes that can't have an underflow bin. Do you add an underflow bin for Categorical axes? (This is the same problem as Boolean, but even weirder since you do have one flow bin, but not the other). \"I want exactly what is defined\" is a _very_ important thing to be able to request, while \"I want a array with flow bins regardless of whether they exist or not on the axis\" is wasteful even without the copy, and mostly only useful when you don't want to bother to check to see if flow bins exist or not. Given they are meaningless and you are not checking to see if they are defined, are you really even using them in that case?",
  "created_at":"2022-04-05T03:36:38Z",
  "id":1088233234,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A3R8S",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T03:36:38Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"So right now we have (main)\r\n```\r\n# No flow bins\r\nh = hist.new.Reg(20, 0, 20, name='msd', flow=False).Weight().fill(np.random.normal(10, 6, 1000))\r\nprint(h.values(flow=False).shape, h_read.values(flow=False).shape)\r\nprint(h.values(flow=True).shape, h_read.values(flow=True).shape)\r\n>>> (20,) (18,)\r\n(20,) (20,)\r\n\r\n# Flow bins \r\nh2 = hist.new.Reg(20, 0, 20, name='msd', flow=True).Weight().fill(np.random.normal(10, 6, 1000))\r\nprint(h2.values(flow=False).shape, h2_read.values(flow=False).shape)\r\nprint(h2.values(flow=True).shape, h2_read.values(flow=True).shape)\r\n>>> (20,) (20,)\r\n(22,) (22,)\r\n```\r\nwhile this PR proposes \r\n\r\n```\r\n# No flow bins\r\nh = hist.new.Reg(20, 0, 20, name='msd', flow=False).Weight().fill(np.random.normal(10, 6, 1000))\r\nprint(h.values(flow=False).shape, h_read.values(flow=False).shape)\r\nprint(h.values(flow=True).shape, h_read.values(flow=True).shape)\r\n>>> (20,) (20,)\r\n(20,) (20,)\r\n\r\n# Flow bins \r\nh2 = hist.new.Reg(20, 0, 20, name='msd', flow=True).Weight().fill(np.random.normal(10, 6, 1000))\r\nprint(h2.values(flow=False).shape, h2_read.values(flow=False).shape)\r\nprint(h2.values(flow=True).shape, h2_read.values(flow=True).shape)\r\n>>> (20,) (20,)\r\n(22,) (22,)\r\n```\r\nwhich \"fixes\" the discrepancy to `hist` behaviour.\r\n\r\nI would say that \r\n> the flow argument should change the shape of the return value\r\n\r\nis a reasonable expectation only if the flow bins actually exist. This PR maintains that, but in the case of flow bins not being initiated it fixes the current behaviour of treating real edge bins as flow bins, which really needs to go.\r\n\r\nNow since ROOT doesn't let us not write out flow bins, we have to add the padding with nans/zeros to fix the underlying problem, but the choice is if we a) strip the padding on reading in uproot to match hist signature (and lose the universality of `flow=True` changing shape) as proposed in the PR or b) leave the padding in on read (which is internally self-consistent because we had to create it to write the file), preserve the `flow=True` always changing shape property, but return a different shape than `hist` does and have an imperfect round-trip `hist(flow=False)` -> `uproot.TH1(flow padded)` -> `hist(flow=True)`.\r\n",
  "created_at":"2022-04-05T09:18:59Z",
  "id":1088468524,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A4LYs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T09:18:59Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"@andrzejnovak, see PR #582, which should replace this one. I'm accepting hist's interpretation of the `flow` argument as the correct one (see https://github.com/scikit-hep/uhi/issues/58#issuecomment-1089081501), so Uproot has to consume `obj.values(flow=True)` as an expression that might return an array without flow bins.\r\n\r\nThat's the Uproot bug that you originally encountered. However, padding and then unpadding is complicating the situation even more\u2014it will likely have unforeseen consequences in other use-cases. To fix just the bug at its source (in histogram-writing and nowhere else), we need to verify that the histogram object actually has flow bins, so that we interpret the `values` array correctly. Using only the UHI + `flow` argument extension, this can be done with\r\n\r\n```diff\r\ndiff --git a/src/uproot/writing/identify.py b/src/uproot/writing/identify.py\r\nindex f9c7f02f..c56282fa 100644\r\n--- a/src/uproot/writing/identify.py\r\n+++ b/src/uproot/writing/identify.py\r\n@@ -263,6 +263,13 @@ def to_writable(obj):\r\n             data = obj.values(flow=True)\r\n             fSumw2 = obj.variances(flow=True)\r\n \r\n+            # and flow=True is different from flow=False (obj actually has flow bins)\r\n+            if (\r\n+                data.shape == obj.values(flow=False).shape\r\n+                and fSumw2.shape == obj.variances(flow=True).shape\r\n+            ):\r\n+                raise TypeError\r\n+\r\n         except TypeError:\r\n             # flow=True is not supported, fallback to allocate-and-fill\r\n```\r\n\r\n(Raising `TypeError` puts it into the code block that handles histograms that don't support the `flow` argument, which is also what we should do if there are no flow bins in `obj`.)\r\n\r\nPR #582 includes your original test. A hist histogram with no flow bins returns the same thing as its round-tripped histogram with `flow=False` (this is the default value of `flow`). A hist histogram with flow bins returns the same thing as its round-tripped histogram with `flow=True` or `flow=False`. That's 3 of the 4 logically possible comparisons.\r\n\r\nThe fourth is to take a hist histogram without flow bins, pass it through a ROOT file, and then ask for `values(flow=True)`. Under hist's interpretation of the `flow` argument, which we are accepting, this _should not_ return the same result as `values(flow=True)` on a TH1. `values(flow=True)` on a histogram with no flow bins does not include flow bins in the output, and `values(flow=True)` on a histogram with flow bins, as every TH1 would, does include flow bins in the output. This is not the only way in which a hist histogram differs from the TH1 that represents it in a ROOT file: the ROOT conversion is lossy.\r\n\r\nFor lossless serialization, we need scikit-hep/boost-histogram#726.",
  "created_at":"2022-04-05T17:44:58Z",
  "id":1089106484,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A6nI0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T17:44:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"\ud83d\udc4d IIUC this corresponds to the above-mentioned option b), the except block takes the no-flow `values()` and pads zeros at the edges to make it ROOT compliant. Should fix the original issue, thanks.",
  "created_at":"2022-04-05T18:04:14Z",
  "id":1089129367,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A6suX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T18:04:14Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, this corresponds to the \"hist is right\" interpretation of the `flow` argument. Uproot has been choosing to impute missing flow bins with zeros for a long time now, so that's a minimal change. Nothing has been done, so far, about `edges` being a method, rather than a property.\r\n\r\nThanks for understanding. I didn't want this fix to become a surprise in someone else's workflow, so it has to be as minimal as possible.",
  "created_at":"2022-04-05T18:10:27Z",
  "id":1089136654,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5A6ugO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-05T18:10:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> \r\n\r\n\r\n\r\n> A user can always check the axis.traits if they need to handle flow bins differently. It is rather annoying that there's no `.edges(flow=True)` (I originally had those as methods, but @HDembinski changed them all into properties in [scikit-hep/boost-histogram#121](https://github.com/scikit-hep/boost-histogram/pull/121) three years ago), but `.to_numpy(flow=True)` actually gives you the `flow=True` edges if you don't want to check `axes.traits`.\r\n\r\nThe reason for that was that `.to_numpy` automatically gives you consistent edges and values. When you make `edges` and `values` methods with a flow `flow` argument, you have to make sure they are consistent. Since users rarely need to mess with the flow bins, the current API design is better.",
  "created_at":"2022-04-09T11:11:22Z",
  "id":1093918639,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5BM9-v",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-09T11:11:22Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"> This is also undefined for axes that can't have an underflow bin. Do you add an underflow bin for Categorical axes? (This is the same problem as Boolean, but even weirder since you do have one flow bin, but not the other). \"I want exactly what is defined\" is a _very_ important thing to be able to request, while \"I want a array with flow bins regardless of whether they exist or not on the axis\" is wasteful even without the copy, and mostly only useful when you don't want to bother to check to see if flow bins exist or not. Given they are meaningless and you are not checking to see if they are defined, are you really even using them in that case?\r\n\r\nHaving overflow and underflow for a category axis makes no sense, since categories are not ordered. There simply some axes which do not need underflow and/or overflow bins and should not have any.",
  "created_at":"2022-04-09T11:13:17Z",
  "id":1093920890,
  "issue":580,
  "node_id":"IC_kwDOD6Q_ss5BM-h6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-09T11:13:29Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"This is intentional; it's because the TBranches you're asking for are jagged with different length lists in the arrays. You're asking for all TBranches because you don't specify them. \"Differently jagged\" arrays can't be put into a single DataFrame because we represent the jaggedness in DataFrames via a MultiIndex, and a DataFrame can only have one index.\r\n\r\nThere's a more complete explanation in Discussion https://github.com/scikit-hep/uproot4/discussions/114.",
  "created_at":"2022-04-08T16:18:59Z",
  "id":1093057323,
  "issue":583,
  "node_id":"IC_kwDOD6Q_ss5BJrsr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-08T16:18:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Excellent, thanks for the link to the more detailed discussion. However, after reading the discussion, I'm unsure what differences to expect between the individual DataFrames in the tuple, how many DataFrames to expect, and how many columns should each of them have.",
  "created_at":"2022-04-08T16:48:17Z",
  "id":1093081499,
  "issue":583,
  "node_id":"IC_kwDOD6Q_ss5BJxmb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-08T16:48:17Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"MEMBER",
  "body":"You'll get the minimum possible number of DataFrames: all TBranches with alignable jaggedness will be in the same DataFrame, and they won't be in any of the other DataFrames (they'll be exclusively partitioned among DataFrames).\r\n\r\nThe non-jagged TBranches will be in all of the DataFrames, since their values need to get duplicated to make them fit into each of the DataFrames, and it's a different duplication in each.\r\n\r\nIf you want more control over the situation, specify an explicit list of TBranches to read. (You'll want that when you scale up to large datasets, anyway, since you'll need to start controlling memory use.) The TBranches that can all go in one DataFrame will be the ones that all correspond to the same particle: you'll have a muon DataFrame, an electron DataFrame, a jet DataFrame, etc. If you're selecting TBranches that you think all have the same jaggedness, such as all the muon attributes, but it returns more than one DataFrame, then they don't really have the same jaggedness and you've discovered a bug in your data preparation. (It's happened before, I can't find the link, but somebody was surprised by how the DataFrames split up and it turned out that a cut applied to one attribute was slightly different from the cut applied to all others. So it's good information!)\r\n\r\nIf this sounds cumbersome, you might want to avoid using Pandas. Pandas's range of data structures is not a good fit for general HEP data\u2014we support it in Uproot because it's good for some simple cases. Ask yourself, for instance, how you're going to do combinatorics with the data in DataFrames, or some other step that you're going to need to do. All of the steps of applying cuts and making plots are exactly as easy to do outside of Pandas as within it. What Pandas adds is the ability to do SQL-like joins, which we don't do often in HEP.",
  "created_at":"2022-04-08T17:15:28Z",
  "id":1093103268,
  "issue":583,
  "node_id":"IC_kwDOD6Q_ss5BJ26k",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-08T17:15:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the detailed reply. I did not produce the ROOT file and I think I need to look into how it is made to understand what jaggedness it has. A way to see which branches are jagged would help; does this exist?\r\n\r\nI don't understand the argument why the non-jagged branches need to be in all DataFrames - I think that is the bit that confuses most users (judging by past issues). To me, it would make more sense if all the non-jagged entries were in one DataFrame and each jagged group in a separate DataFrame. \r\n\r\nYep, I'll definitely load just a subset of branches for proper analysis, I was just surprised by the tuple of datasets when exploring.\r\n\r\nI'm not adamant about using Pandas; your Awkward Arrays look very interesting in particular, however, the documentation has a lot of TODOs at this point, which is what's stopping me.",
  "created_at":"2022-04-11T14:06:30Z",
  "id":1095097486,
  "issue":583,
  "node_id":"IC_kwDOD6Q_ss5BRdyO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-11T14:06:30Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"MEMBER",
  "body":"> A way to see which branches are jagged would help; does this exist?\r\n\r\nA TBranch's `typename` has the C++ name of the type, though it might be more useful to look at the `interpretation`. Different C++ types, like `float[]` and `std::vector<float>`, are both `AsJagged(AsDtype(\"float32\"))`.\r\n\r\nFor a quick look, you can get an ASCII table of all of them by calling TTree's `show` method.\r\n\r\n> I don't understand the argument why the non-jagged branches need to be in all DataFrames - I think that is the bit that confuses most users (judging by past issues). To me, it would make more sense if all the non-jagged entries were in one DataFrame and each jagged group in a separate DataFrame.\r\n\r\nThe non-jagged data are in the DataFrames so that they can be related to the jagged data. It's reasonable, for instance, to want to know which event number or MET value to associate with a given muon or jet. If they were separate, you'd have to use a [Pandas merge](https://pandas.pydata.org/docs/user_guide/merging.html) to duplicate them appropriately, as in this minimal example:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> array = ak.Array([{\"x\": 1.1, \"y\": [1]}, {\"x\": 2.2, \"y\": [1, 2]}, {\"x\": 3.3, \"y\": [1, 2, 3]}])\r\n>>> ak.to_pandas(array)\r\n                  x  y\r\nentry subentry        \r\n0     0         1.1  1\r\n1     0         2.2  1\r\n      1         2.2  2\r\n2     0         3.3  1\r\n      1         3.3  2\r\n      2         3.3  3\r\n```\r\n\r\nThis is a matter of defaults. Uproot (and Awkward Array) combine as much as possible into a single DataFrame, but not any more than is possible (the original question). A different default would be to keep the non-jagged data separate from the jagged data to make users have to do the merge manually. With that default, this function would almost always return a tuple because most TTrees have both jagged and non-jagged data. A more extreme default would be to return every TBranch separately (as Series) and make the user put everything together, but that would be nearly the same thing as `library=\"np\"`.\r\n\r\nTo deviate from the default, select sets of TBranches explicitly. Here's an example that selects just the non-jagged (`AsDtype`) ones:\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> tree = uproot.open(skhep_testdata.data_path(\"uproot-HZZ.root\"))[\"events\"]\r\n\r\n>>> # check your filter in tree.keys to see what it gives you, without reading data\r\n>>> tree.keys(filter_branch=lambda branch: isinstance(branch.interpretation, uproot.AsDtype))\r\n['NJet', 'NMuon', 'NElectron', 'NPhoton', 'MET_px', 'MET_py', 'MChadronicBottom_px', 'MChadronicBottom_py',\r\n 'MChadronicBottom_pz', 'MCleptonicBottom_px', 'MCleptonicBottom_py', 'MCleptonicBottom_pz',\r\n 'MChadronicWDecayQuark_px', 'MChadronicWDecayQuark_py', 'MChadronicWDecayQuark_pz',\r\n 'MChadronicWDecayQuarkBar_px', 'MChadronicWDecayQuarkBar_py', 'MChadronicWDecayQuarkBar_pz',\r\n 'MClepton_px', 'MClepton_py', 'MClepton_pz', 'MCleptonPDGid', 'MCneutrino_px', 'MCneutrino_py',\r\n 'MCneutrino_pz', 'NPrimaryVertices', 'triggerIsoMu24', 'EventWeight']\r\n\r\n>>> # if it's good, apply it to tree.arrays to read the data\r\n>>> tree.arrays(filter_branch=lambda branch: isinstance(branch.interpretation, uproot.AsDtype), library=\"pd\")\r\n      NJet  NMuon  NElectron  ...  NPrimaryVertices  triggerIsoMu24  EventWeight\r\n0        0      2          0  ...                 6            True     0.009271\r\n1        1      1          0  ...                18            True     0.000331\r\n2        0      2          0  ...                16            True     0.005080\r\n3        3      2          0  ...                 8            True     0.007081\r\n4        2      2          2  ...                 2            True     0.008536\r\n...    ...    ...        ...  ...               ...             ...          ...\r\n2416     1      1          0  ...                 9            True     0.009260\r\n2417     2      1          0  ...                18            True     0.000331\r\n2418     1      1          0  ...                 9            True     0.004153\r\n2419     2      1          0  ...                 6            True     0.008829\r\n2420     0      1          0  ...                12            True     0.008755\r\n\r\n[2421 rows x 28 columns]\r\n```\r\n\r\n> I'm not adamant about using Pandas; your Awkward Arrays look very interesting in particular, however, the documentation has a lot of TODOs at this point, which is what's stopping me.\r\n\r\nI need to just remove the pages that have TODOs in them. There's a lot of documentation and the project's stable\u2014I was just overeager in how much I thought I would have time to write about. I got the same comment [here](https://news.ycombinator.com/item?id=29576629); the TODOs are giving the wrong impression.",
  "created_at":"2022-04-11T16:09:21Z",
  "id":1095253234,
  "issue":583,
  "node_id":"IC_kwDOD6Q_ss5BSDzy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-11T16:09:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Excellent, the `show()` method and *branch filtering based on interpretation* give me everything I need to do what I want. Thanks for the tips and helpful examples!\r\n\r\nI understand your reasoning about the defaults, and while not what I'd choose, it's a fair choice. It'd be great if the workaround you posted here using the branch filtering based on interpretation could be mentioned in the documentation.\r\n\r\nRegarding Awkward documentation - please don't remove the pages! The documentation overview looks great, and I'm aware that the project is relatively mature, but it is difficult to use it instead of, e.g., Pandas, if the documentation is not there. My first check was how to filter the data quickly, e.g., retain only events where `col1 > 3 and col2 < 0`. Then I saw that the [page](https://awkward-array.org/how-to-filter.html?highlight=cuts) doesn't exist yet, which means I'm back to Pandas for exploratory analysis (its `query()` is most useful and easy to use).",
  "created_at":"2022-04-11T17:32:31Z",
  "id":1095336237,
  "issue":583,
  "node_id":"IC_kwDOD6Q_ss5BSYEt",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-11T17:32:31Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"MEMBER",
  "body":"> My first check was how to filter the data quickly, e.g., retain only events where `col1 > 3 and col2 < 0`. Then I saw that the [page](https://awkward-array.org/how-to-filter.html?highlight=cuts) doesn't exist yet, which means I'm back to Pandas for exploratory analysis (its `query()` is most useful and easy to use).\r\n\r\nBecause that information is here: https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#filtering\r\n\r\nThe existence of a page that says \"TODO\" gives the impression that this information hasn't been written down, rather than being written down in another place. What I'm intending to do (c.f. https://github.com/scikit-hep/awkward-1.0/issues/1366#issuecomment-1065256551) is simplify its organization so that readers are funneled into the places where the things they're looking for have been written.",
  "created_at":"2022-04-11T18:03:32Z",
  "id":1095365996,
  "issue":583,
  "node_id":"IC_kwDOD6Q_ss5BSfVs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-11T18:03:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Aha! Well, I did not realize that the documentation is in two places. Perhaps adding links to the old pages on the unfinished new pages could be a quick stop-gap solution. I'll comment on the issue you linked.",
  "created_at":"2022-04-11T18:26:36Z",
  "id":1095402944,
  "issue":583,
  "node_id":"IC_kwDOD6Q_ss5BSoXA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-11T18:26:36Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"NONE",
  "body":"Getting a few files:\r\n```python\r\n>>> import uproot\r\n>>> import skhep_testdata\r\n>>> filename1 = skhep_testdata.data_path(\"uproot-Zmumu.root\") + \":events\"\r\n>>> filename2 = skhep_testdata.data_path(\"uproot-Zmumu-uncompressed.root\") + \":events\"\r\n>>> filename3 = skhep_testdata.data_path(\"uproot-Zmumu-zlib.root\") + \":events\"\r\n>>> filename4 = skhep_testdata.data_path(\"uproot-Zmumu-lzma.root\") + \":events\"\r\n```\r\n\r\n\r\n```python\r\n>>> uproot.dask([filename1,filename2,filename3,filename4])\r\n{'Type': dask.array<concatenate, shape=(9216,), dtype=object, chunksize=(2304,), chunktype=numpy.ndarray>, 'Run': dask.array<concatenate, shape=(9216,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>, 'Event': dask.array<concatenate, shape=(9216,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>, 'E1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'px1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'py1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'pz1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'pt1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'eta1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'phi1': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'Q1': dask.array<concatenate, shape=(9216,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>, 'E2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'px2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'py2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'pz2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'pt2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'eta2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'phi2': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>, 'Q2': dask.array<concatenate, shape=(9216,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>, 'M': dask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>} \r\n```\r\n...returns a python dict\r\n\r\n```python\r\n>>> n_dict = uproot.dask([filename1,filename2,filename3,filename4])\r\n>>> n_dict['px1']\r\ndask.array<concatenate, shape=(9216,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>\r\n```\r\n...returns a dask array\r\n\r\nThe requested array is only read when `.compute()` is called;\r\n```python\r\n>>> n_dict['px1'].compute()\r\narray([-41.19528764,  35.11804977,  35.11804977, ...,  32.37749196,\r\n        32.37749196,  32.48539387])\r\n```\r\n\r\nIn a scenario where only a certain slice of the data is required;\r\n```python\r\n>>> n_dict['px1'][0:100]\r\ndask.array<getitem, shape=(100,), dtype=float64, chunksize=(100,), chunktype=numpy.ndarray>\r\n```\r\n...only the data requested from the particular slice is read in.\r\n\r\nobserving the task graph;\r\n```python\r\n>>> n_dict['px1'][0:100].visualize()\r\n```",
  "created_at":"2022-04-16T04:22:34Z",
  "id":1100565445,
  "issue":585,
  "node_id":"IC_kwDOD6Q_ss5BmUvF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-16T04:22:34Z",
  "user":"MDQ6VXNlcjcwOTkwMzQx"
 },
 {
  "author_association":"MEMBER",
  "body":"I've looked into this, and should at least write down some notes about it. First, it's important to know that this is not TTree data: it's a free-floating object in the file. (Generally, those have been _easier_ than objects in TTrees, but it doesn't look like it will be here.)\r\n\r\nYou're right that that Uproot's view of the TStreamerObjectAny doesn't seem to know that it's an array of length 2. That's probably in a member of TStreamerObjectAny somewhere, which we're not yet interpreting.\r\n\r\nThe other issue is that the generated code tries to load \"`vector<double>`\" as a class name, without recognizing that it's a `std::vector` container of `double` data. Uproot's containers haven't been invoked for that\u2014this may be another way that the TStreamerObjectAny implementation is naive: it's assuming that the `typename` is a classname. `TStreamerObjectAny.class_code` should be calling `uproot.interpretation.identify.parse_typename`.\r\n\r\nThis doesn't look like a small project (a \"quick fix\"), but it could be a medium-sized project.",
  "created_at":"2022-04-20T16:23:53Z",
  "id":1104134668,
  "issue":586,
  "node_id":"IC_kwDOD6Q_ss5Bz8IM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-20T16:23:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"So are you saying that `diff` should be plus and `mean` should be minus?\r\n\r\nOr that `low` and `high` should be redefined?\r\n\r\n@kratsg was the original author (in #240). @kratsg, what do you think? Is this a correct interpretation?",
  "created_at":"2022-04-21T16:30:21Z",
  "id":1105446010,
  "issue":587,
  "node_id":"IC_kwDOD6Q_ss5B48R6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-21T16:30:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Well, I'm surprised anyone is using `diff`. I only needed it at the time for validation/xchecks, but it doesn't really have a lot of physical intuition. Both `high` and `low` errors are supposed to be positive as defined since they go in different directions.",
  "created_at":"2022-04-21T16:37:40Z",
  "id":1105452344,
  "issue":587,
  "node_id":"IC_kwDOD6Q_ss5B4904",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-21T16:37:40Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Should it be removed? @andrzejnovak, what were you using it for?\r\n\r\nIn principle, all one needs is `high` and `low`, since the others can be derived from it. `mean` is useful because it turns a graph of asymmetric errors into a graph of symmetric errors. Does `mean` go in the right direction? (I would have thought that `mean` and `diff` should be opposites, whatever they are.)",
  "created_at":"2022-04-21T16:45:21Z",
  "id":1105458524,
  "issue":587,
  "node_id":"IC_kwDOD6Q_ss5B4_Vc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-21T16:45:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Does `mean` go in the right direction? (I would have thought that `mean` and `diff` should be opposites, whatever they are.)\r\n\r\nThey are meant to be opposites. I tried to balance it. One is \"+\" and one is \"-\". You could think of `diff` as a measure of the asymmetry.",
  "created_at":"2022-04-21T17:06:42Z",
  "id":1105477963,
  "issue":587,
  "node_id":"IC_kwDOD6Q_ss5B5EFL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-21T17:06:53Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I was using it to get \"bin-width\" though of course it can just be calculated from the separate err calls. Seems we can close it then, I was just expecting a different meaning behind the name.",
  "created_at":"2022-04-22T07:31:25Z",
  "id":1106107603,
  "issue":587,
  "node_id":"IC_kwDOD6Q_ss5B7dzT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-22T07:31:25Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay!",
  "created_at":"2022-04-22T11:41:37Z",
  "id":1106431284,
  "issue":587,
  "node_id":"IC_kwDOD6Q_ss5B8s00",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-22T11:41:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ah, never mind, I'm trying to call ak.fields on a numpy array.....",
  "created_at":"2022-04-26T08:55:47Z",
  "id":1109533369,
  "issue":588,
  "node_id":"IC_kwDOD6Q_ss5CIiK5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-04-26T08:55:47Z",
  "user":"MDQ6VXNlcjEyOTk2NzYz"
 },
 {
  "author_association":"MEMBER",
  "body":"Using a recent Uproot, I got a different error (infinite recursion), but still something to be dealt with. Thanks for providing the files, especially since the before and after will help in reverse-engineering the `RVec` serialization.",
  "created_at":"2022-05-05T18:23:42Z",
  "id":1118911116,
  "issue":589,
  "node_id":"IC_kwDOD6Q_ss5CsTqM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-05T18:23:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes I use a previous version of uproot with SWAN (it was the only configuration we have found to deal with our FCC stuff). In all case, say me if you manage to solve the issue and how. Thanks for your help.",
  "created_at":"2022-05-05T21:38:36Z",
  "id":1119070975,
  "issue":589,
  "node_id":"IC_kwDOD6Q_ss5Cs6r_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-05T21:38:36Z",
  "user":"MDQ6VXNlcjczNDQzOTY4"
 },
 {
  "author_association":"NONE",
  "body":"I don't know if it can be link or not to the uproot issue, but since this week I have an issue with root when I tried to explore the new .root file (everything run well with the old file) with a TBrowser (and it display an issue about TStreamer, RVec and std::vector), it can may be help. The root error is: \r\n\r\nWarning in <TStreamerInfo::Build>: Due to some major, backward-incompatible improvements planned for ROOT::RVec, direct I/O of ROOT::RVec objects will break between v6.24 and v6.26. Please use std::vectors instead. See the release notes of v6.24 for more information.\r\nWarning in <TStreamerInfo::Build>: Due to some major, backward-incompatible improvements planned for ROOT::RVec, direct I/O of ROOT::RVec objects will break between v6.24 and v6.26. Please use std::vectors instead. See the release notes of v6.24 for more information.\r\nError in <TStreamerInfo::Build>: __gnu_cxx::__normal_iterator<const float*,vector<float,ROOT::Detail::VecOps::RAdoptAllocator<float> > >, discarding: const float* _M_current, no [dimension]",
  "created_at":"2022-05-11T12:18:46Z",
  "id":1123681779,
  "issue":589,
  "node_id":"IC_kwDOD6Q_ss5C-gXz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-11T12:18:46Z",
  "user":"MDQ6VXNlcjczNDQzOTY4"
 },
 {
  "author_association":"MEMBER",
  "body":"Whatever that is, it has nothing to do with this issue. (By the way, I'm working on it now.)",
  "created_at":"2022-05-11T18:27:30Z",
  "id":1124154905,
  "issue":589,
  "node_id":"IC_kwDOD6Q_ss5DAT4Z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-11T18:27:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The serialization of `ROOT::VecOps::RVec` appears to be exactly the same as that of `std::vector`. But we don't trust the TStreamerInfo for `std::vector`, either, especially since we often override it to decode `std::vector<numeric_type>` more quickly with `AsJagged`, rather than `AsObjects`. So I added (in PR #593) exactly the same thing for `ROOT::VecOps::RVec`. If there is any difference between the two container types for type-combinations that I can't test because they aren't in your file, then we'll discover them down the road.\r\n\r\nThanks for the sample!",
  "created_at":"2022-05-11T19:59:26Z",
  "id":1124233215,
  "issue":589,
  "node_id":"IC_kwDOD6Q_ss5DAm__",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-11T19:59:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I will pip upgrade my uproot and trying this, thanks !",
  "created_at":"2022-05-11T21:28:45Z",
  "id":1124311781,
  "issue":589,
  "node_id":"IC_kwDOD6Q_ss5DA6Ll",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-11T21:28:45Z",
  "user":"MDQ6VXNlcjczNDQzOTY4"
 },
 {
  "author_association":"MEMBER",
  "body":"It will be release 4.2.3. I'm waiting for #595 to get merged before deploying the new release.",
  "created_at":"2022-05-11T21:38:47Z",
  "id":1124318735,
  "issue":589,
  "node_id":"IC_kwDOD6Q_ss5DA74P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-11T21:38:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's ready: https://pypi.org/project/uproot/4.2.3/\r\n\r\nGo ahead and `pip install -U uproot` now!",
  "created_at":"2022-05-11T21:51:33Z",
  "id":1124327343,
  "issue":589,
  "node_id":"IC_kwDOD6Q_ss5DA9-v",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-11T21:51:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi! Thanks for your first pull request, but did you know that `library=\"pd\"` can be passed to any array-fetching function (e.g. [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays))? That does what this `to_dataframe` would do. (It's [here in the documentation](https://uproot.readthedocs.io/en/latest/basic.html#reading-a-tbranch-as-an-array).)",
  "created_at":"2022-05-17T01:27:55Z",
  "id":1128302241,
  "issue":598,
  "node_id":"IC_kwDOD6Q_ss5DQIah",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-17T01:27:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Hi! Thanks for your first pull request, but did you know that `library=\"pd\"` can be passed to any array-fetching function (e.g. [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays))? That does what this `to_dataframe` would do. (It's [here in the documentation](https://uproot.readthedocs.io/en/latest/basic.html#reading-a-tbranch-as-an-array).)\r\n\r\nThank you very much. There are few challenges for using `library=\"pd\"`. It does not play nice with \"vector\" leaf type, it creates subentry for an events, instead we want 1 event per row in the df. Also `tree.arrays(library=\"pd\")` takes way too long to run when \"vector\" leaf type variables are present in the tree.",
  "created_at":"2022-05-17T12:43:40Z",
  "id":1128822564,
  "issue":598,
  "node_id":"IC_kwDOD6Q_ss5DSHck",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-17T12:43:40Z",
  "user":"MDQ6VXNlcjU3OTY2ODc1"
 },
 {
  "author_association":"MEMBER",
  "body":"By \"vector\" leaf type, do you mean jagged arrays? A function that puts a list of numbers into a Pandas cell (as opposed to expanding that list into one number per cell, tracked by a MultiIndex) would have to have a name that clearly specifies that that's what it does.\r\n\r\nI don't know of any Pandas functions that can deal adequately with non-numeric values in cells. It represents them as Python lists or NumPy arrays, and then you're forced to manipulate them with Python iteration, using one of the various `map` functions in Pandas. That doesn't scale to large datasets.",
  "created_at":"2022-05-17T12:52:26Z",
  "id":1128831019,
  "issue":598,
  "node_id":"IC_kwDOD6Q_ss5DSJgr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-17T12:52:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you so much for your help!",
  "created_at":"2022-05-17T14:26:42Z",
  "id":1128940588,
  "issue":598,
  "node_id":"IC_kwDOD6Q_ss5DSkQs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-05-17T14:26:42Z",
  "user":"MDQ6VXNlcjU3OTY2ODc1"
 },
 {
  "author_association":"MEMBER",
  "body":"Earlier today, I told @douglasdavis about your work, since you'll eventually be using https://github.com/ContinuumIO/dask-awkward/, and also we want to be sure you're using Dask effectively. He might have some ideas about making a Dask array from _N_ files without opening all _N_ files (opening just the first one to get the names and types). We necessarily would _not_ know how many entries each file has or what the total number of entries is.",
  "created_at":"2022-06-02T18:03:14Z",
  "id":1145158007,
  "issue":602,
  "node_id":"IC_kwDOD6Q_ss5EQbl3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-02T18:03:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for pinging me here @jpivarski - yes it's certainly straightforward to delay opening files. The absolute simplest case would be something of the form:\r\n\r\n```python\r\nfrom dask.delayed import delayed\r\nimport dask.array as da\r\n\r\nimport uproot\r\nfrom uproot.behaviors.TTree import TTree\r\n\r\n\r\n@delayed\r\ndef delayed_tree(file_name: str, tree_name: str):\r\n    return uproot.open(file_name)[tree_name]\r\n\r\n\r\n@delayed\r\ndef delayed_array(tree: TTree, branch: str):\r\n    return tree.arrays(branch, library=\"np\", how=tuple)[0]\r\n\r\n\r\ndef arrays_from_uproot(files: list[str], tree: str, branches: list[str] | None = None):\r\n    # use tree0 to get metadata; assume all other files have the same\r\n    # structure.\r\n    tree0 = uproot.open(files[0])[tree]\r\n\r\n    # this is probably too simple; here just for example.\r\n    if branches is None:\r\n        branches = tree0.keys()\r\n\r\n    # do something to determine the NumPy dtype for each branch (for\r\n    # example we'll use a lambda function which can do this).\r\n    get_dtype = lambda tree, branch: tree[branch].interpretation.numpy_dtype\r\n    dtypes = {branch: get_dtype(tree0, branch) for branch in branches}\r\n\r\n    # here `trees` is a list of Delayed objects that represent an\r\n    # eventual concrete uproot TTree. We end up with one\r\n    # delayed TTree object per file.\r\n    trees = [delayed_tree(f, tree) for f in files]\r\n\r\n    # intermediate results is going to be a dictionary with a branch\r\n    # as the key and a list of delayed arrays as the value.\r\n    # {\"branch\": [delayedarray1, delayedarray2, delayedarray3, ...], ...}\r\n    intermediate_results = {\r\n        branch: [delayed_array(tree, branch) for tree in trees] for branch in branches\r\n    }\r\n\r\n    # now we'll convert the list of delayed arrays\r\n    # into single dask arrays. First we convert the\r\n    # delayed object (wrapping an array) into an actual\r\n    # dask.array Array object, we then concatenate the\r\n    # list of Array objects into a single Array object.\r\n    # The arrays will be partitioned such\r\n    # that each daN is a chunk.\r\n    results = {}\r\n    for branch, delayeds in intermediate_results.items():\r\n        arrays = [\r\n            da.from_delayed(d, shape=(np.nan,), dtype=dtypes[branch]) for d in delayeds\r\n        ]\r\n        results[branch] = da.concatenate(arrays, allow_unknown_chunksizes=True)\r\n\r\n    return results\r\n```",
  "created_at":"2022-06-02T19:55:20Z",
  "id":1145283488,
  "issue":602,
  "node_id":"IC_kwDOD6Q_ss5EQ6Og",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-06-02T20:18:35Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Running the function:\r\n\r\nImports:\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> filename1 = skhep_testdata.data_path(\"uproot-Zmumu.root\") + \":events\"\r\n>>> filename2 = skhep_testdata.data_path(\"uproot-Zmumu-uncompressed.root\") + \":events\"\r\n>>> from pprint import pprint as pp      # For pretty printing\r\n```\r\n\r\nShowing variable chunksizes:\r\n```python\r\n>>> pp(uproot.dask([filename1, filename2]))\r\n{'E1': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'E2': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Event': dask.array<concatenate, shape=(4608,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'M': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Q1': dask.array<concatenate, shape=(4608,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Q2': dask.array<concatenate, shape=(4608,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Run': dask.array<concatenate, shape=(4608,), dtype=int32, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'Type': dask.array<concatenate, shape=(4608,), dtype=object, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'eta1': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'eta2': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'phi1': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'phi2': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pt1': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pt2': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'px1': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'px2': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'py1': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'py2': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pz1': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>,\r\n 'pz2': dask.array<concatenate, shape=(4608,), dtype=float64, chunksize=(2304,), chunktype=numpy.ndarray>}\r\n>>>\r\n>>> pp(uproot.dask([filename1, filename2],open_files=False))\r\n{'E1': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'E2': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'Event': dask.array<concatenate, shape=(nan,), dtype=int32, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'M': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'Q1': dask.array<concatenate, shape=(nan,), dtype=int32, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'Q2': dask.array<concatenate, shape=(nan,), dtype=int32, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'Run': dask.array<concatenate, shape=(nan,), dtype=int32, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'Type': dask.array<concatenate, shape=(nan,), dtype=object, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'eta1': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'eta2': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'phi1': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'phi2': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'pt1': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'pt2': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'px1': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'px2': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'py1': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'py2': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'pz1': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>,\r\n 'pz2': dask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>}\r\n>>>\r\n>>> uproot.dask([filename1, filename2])['px1'].chunks\r\n((2304, 2304),)\r\n>>>\r\n>>> uproot.dask([filename1, filename2],open_files=False)['px1'].chunks\r\n((nan, nan),)\r\n```\r\n\r\nCalling compute:\r\n```python\r\n>>> dask_dict = uproot.dask([filename1, filename2],open_files=False)\r\n>>> dask_dict['px1']\r\ndask.array<concatenate, shape=(nan,), dtype=float64, chunksize=(nan,), chunktype=numpy.ndarray>\r\n>>> dask_dict['px1'].compute()\r\narray([-41.19528764,  35.11804977,  35.11804977, ...,  32.37749196,\r\n        32.37749196,  32.48539387])\r\n```\r\n\r\n\r\nAlso, pre-commit seems to pass on github actions, probably a config error on my local install then.",
  "created_at":"2022-06-03T18:20:17Z",
  "id":1146240963,
  "issue":603,
  "node_id":"IC_kwDOD6Q_ss5EUj_D",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-03T18:20:17Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Tests completed, and rewrote some earlier code to better make use of dask.",
  "created_at":"2022-06-07T14:06:04Z",
  "id":1148724286,
  "issue":603,
  "node_id":"IC_kwDOD6Q_ss5EeCQ-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-07T14:06:04Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"It has been a goal of mine to get a CITATION.cff for Uproot. The only thing that has been keeping me from it is the process of defining the author list. Just as in [awkward/CITATION.cff](https://github.com/scikit-hep/awkward/blob/main/CITATION.cff), I wanted to highlight the main authors and get full information for them. The list of GitHub userids that Zenodo pulls in doesn't include institutions, email addresses, or ORCIDs.\r\n\r\nDo you have an idea in mind for curating the list of authors? I'm thinking I should acknowledge the fact that I got a _lot_ of help on the remote backends part.\r\n\r\nAlso desirable for CITATION.cff (across projects, in my opinion): it should reference the versionless DOI for the repo. For instance, in Awkward Array, it's\r\n\r\nhttps://github.com/scikit-hep/awkward/blob/8a4ea2b8c1c0cb2c92dab3565511bf5a717b7703/CITATION.cff#L4\r\n\r\nwhich always goes to the latest version, whatever that is. It's therefore a stable identifier across time. For Uproot, this would be `10.5281/zenodo.4340632`.",
  "created_at":"2022-06-07T00:18:33Z",
  "id":1148060263,
  "issue":604,
  "node_id":"IC_kwDOD6Q_ss5EbgJn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-07T00:18:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> It has been a goal of mine to get a CITATION.cff for Uproot. The only thing that has been keeping me from it is the process of defining the author list. Just as in [awkward/CITATION.cff](https://github.com/scikit-hep/awkward/blob/main/CITATION.cff), I wanted to highlight the main authors and get full information for them. The list of GitHub userids that Zenodo pulls in doesn't include institutions, email addresses, or ORCIDs.\r\n> \r\n> Do you have an idea in mind for curating the list of authors? I'm thinking I should acknowledge the fact that I got a _lot_ of help on the remote backends part.\r\n\r\nNot really anything rigorous. I think on a smaller project like `pyhf` it is easier to define who the \"development\" team is and treat them as [the `authors`](https://github.com/scikit-hep/pyhf/blob/aed69b54e13340129251cabe7840f31a525f0cde/CITATION.cff#L4). So I think that you'd either need to create an authorship criteria that seems fair to you, or just make selections on who has historically provided the help on the backends and then put that together. I assume that you mean contributions from across https://github.com/scikit-hep/uproot3 and https://github.com/scikit-hep/uproot4?\r\n\r\n> Also desirable for CITATION.cff (across projects, in my opinion): it should reference the versionless DOI for the repo. For instance, in Awkward Array, it's\r\n> \r\n> https://github.com/scikit-hep/awkward/blob/8a4ea2b8c1c0cb2c92dab3565511bf5a717b7703/CITATION.cff#L4\r\n> \r\n> which always goes to the latest version, whatever that is. It's therefore a stable identifier across time. For Uproot, this would be `10.5281/zenodo.4340632`.\r\n\r\nYup. That's what I linked to above in\r\n\r\n> especially as there is already a [Zenodo archive for `uproot`](https://doi.org/10.5281/zenodo.4340632)\r\n\r\nand we do [the same for `pyhf`](https://github.com/scikit-hep/pyhf/blob/aed69b54e13340129251cabe7840f31a525f0cde/CITATION.cff#L19).\r\n",
  "created_at":"2022-06-07T00:31:21Z",
  "id":1148067529,
  "issue":604,
  "node_id":"IC_kwDOD6Q_ss5Ebh7J",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-07T00:31:21Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"NONE",
  "body":"Perhaps related to this issue: is there a recommended list of proceedings/papers to cite for uproot (also for awkward)?\r\nShould it be somewhere in the README?",
  "created_at":"2022-06-22T08:51:57Z",
  "id":1162831020,
  "issue":604,
  "node_id":"IC_kwDOD6Q_ss5FT2Ss",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-06-22T08:51:57Z",
  "user":"MDQ6VXNlcjY5NzE3"
 },
 {
  "author_association":"MEMBER",
  "body":"Uproot doesn't have one yet (I have to draw up an author list), but Awkward Array does:\r\n\r\nhttps://github.com/scikit-hep/awkward/blob/main/CITATION.cff\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/175057897-4be0110d-cfc6-4072-8cda-224fba5c825e.png)\r\n",
  "created_at":"2022-06-22T14:43:24Z",
  "id":1163196023,
  "issue":604,
  "node_id":"IC_kwDOD6Q_ss5FVPZ3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T14:43:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Perhaps related to this issue: is there a recommended list of proceedings/papers to cite for uproot (also for awkward)? Should it be somewhere in the README?\r\n\r\n@jpivarski I think @jpata is referring to something along the lines of how [`pyhf`'s `CITATION.cff` includes other papers to cite in the `references` section](https://github.com/scikit-hep/pyhf/blob/86229ad3257ddbbdb78548d7ceade5c9ab683a0c/CITATION.cff#L43-L71) and then we also ask for people to cite both the software and the JOSS paper in [the README](https://github.com/scikit-hep/pyhf/blob/86229ad3257ddbbdb78548d7ceade5c9ab683a0c/README.rst?plain=1#L310-L330) and [the docs](https://github.com/scikit-hep/pyhf/blob/86229ad3257ddbbdb78548d7ceade5c9ab683a0c/docs/citations.rst?plain=1#L11-L15).",
  "created_at":"2022-06-22T14:53:39Z",
  "id":1163211146,
  "issue":604,
  "node_id":"IC_kwDOD6Q_ss5FVTGK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T14:53:39Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"In that case, no, I don't think there's a good paper that would be an overview of Uproot or Awkward Array in general. Each is covering something specific, or was written at a time when the idea wasn't fully formed. I would just cite the software, not any accompanying paper.",
  "created_at":"2022-06-22T17:17:57Z",
  "id":1163403619,
  "issue":604,
  "node_id":"IC_kwDOD6Q_ss5FWCFj",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-06-22T17:17:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is done.",
  "created_at":"2022-10-31T20:52:12Z",
  "id":1297671056,
  "issue":604,
  "node_id":"IC_kwDOD6Q_ss5NWOOQ",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-10-31T20:52:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, we're _walking back_ from an implementation like that: in Uproot 3, `extend` collected until it reached a tipping point before dumping to the file, but that\r\n\r\n   * mixed accumulation code with writing code in an unmaintainable way\r\n   * introduces a new kind of mutable state that must be context-managed by the user (have to dump the last unwritten entries after the loop)\r\n   * some users wanted more control over the number of entries in a TBasket.\r\n\r\nSo we took advantage of the chance to change interface in Uproot 4 to make `extend` behave more simply.\r\n\r\nThis accumulation can be written rather generically, so it makes sense to have it in the Uproot codebase, but I'm not sure exactly where to put it. The warning would still need to be in the documentation, but it would point to this as a solution that the user has to explicitly opt-into (because you need to expose the context manager). Here's what a generic accumulator would look like (with MockWritableTree for a quick test):\r\n\r\n```python\r\nimport numpy as np\r\n\r\n\r\nclass MockWritableTree:\r\n    def extend(self, data):\r\n        print(f\"WritableTree.extend called with {len(data['x'])} entries\")\r\n\r\n\r\nclass Accumulator:\r\n    def __init__(self, tree, dump_num_entries):\r\n        self.tree = tree\r\n        self.dump_num_entries = dump_num_entries\r\n        self.current_num_entries = 0\r\n        self.buffer = None\r\n\r\n    def fill(self, data):\r\n        if self.buffer is None:\r\n            self.buffer = {key: [value] for key, value in data.items()}\r\n        else:\r\n            for key, value in self.buffer.items():\r\n                value.append(data[key])\r\n\r\n        for value in data.values():\r\n            self.current_num_entries += len(value)\r\n            break\r\n\r\n        if self.current_num_entries >= self.dump_num_entries:\r\n            self.flush()\r\n\r\n    def flush(self):\r\n        if self.buffer is not None:\r\n            out = {key: np.concatenate(value) for key, value in self.buffer.items()}\r\n            self.tree.extend(out)\r\n            self.current_num_entries = 0\r\n            self.buffer = None\r\n\r\n    def __enter__(self):\r\n        return self\r\n\r\n    def __exit__(self, exc_type, exc_val, exc_tb):\r\n        self.flush()\r\n\r\n\r\ntree = MockWritableTree()\r\n\r\nwith Accumulator(tree, 100) as accumulator:\r\n    print(\"begin\")\r\n\r\n    for _ in range(100):\r\n        num_entries = np.random.poisson(20)\r\n        data = {\r\n            \"x\": np.random.normal(0, 1, num_entries),\r\n            \"y\": np.random.normal(0, 1, num_entries),\r\n        }\r\n        print(f\"Accumulator.fill called with {len(data['x'])} entries\")\r\n        accumulator.fill(data)\r\n\r\n    print(\"end\")\r\n```\r\n```\r\nbegin\r\nAccumulator.fill called with 17 entries\r\nAccumulator.fill called with 13 entries\r\nAccumulator.fill called with 25 entries\r\nAccumulator.fill called with 26 entries\r\nAccumulator.fill called with 27 entries\r\nWritableTree.extend called with 108 entries\r\nAccumulator.fill called with 14 entries\r\nAccumulator.fill called with 24 entries\r\nAccumulator.fill called with 22 entries\r\nAccumulator.fill called with 19 entries\r\nAccumulator.fill called with 23 entries\r\nWritableTree.extend called with 102 entries\r\nAccumulator.fill called with 13 entries\r\nAccumulator.fill called with 13 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 27 entries\r\nWritableTree.extend called with 118 entries\r\nAccumulator.fill called with 17 entries\r\nAccumulator.fill called with 15 entries\r\nAccumulator.fill called with 18 entries\r\nAccumulator.fill called with 16 entries\r\nAccumulator.fill called with 31 entries\r\nAccumulator.fill called with 13 entries\r\nWritableTree.extend called with 110 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 14 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 18 entries\r\nAccumulator.fill called with 19 entries\r\nWritableTree.extend called with 118 entries\r\nAccumulator.fill called with 16 entries\r\nAccumulator.fill called with 18 entries\r\nAccumulator.fill called with 18 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 16 entries\r\nWritableTree.extend called with 112 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 16 entries\r\nAccumulator.fill called with 22 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 21 entries\r\nWritableTree.extend called with 103 entries\r\nAccumulator.fill called with 20 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 24 entries\r\nAccumulator.fill called with 15 entries\r\nWritableTree.extend called with 103 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 12 entries\r\nAccumulator.fill called with 17 entries\r\nAccumulator.fill called with 20 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 22 entries\r\nWritableTree.extend called with 115 entries\r\nAccumulator.fill called with 17 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 16 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 26 entries\r\nWritableTree.extend called with 103 entries\r\nAccumulator.fill called with 20 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 24 entries\r\nAccumulator.fill called with 22 entries\r\nAccumulator.fill called with 17 entries\r\nWritableTree.extend called with 106 entries\r\nAccumulator.fill called with 16 entries\r\nAccumulator.fill called with 18 entries\r\nAccumulator.fill called with 22 entries\r\nAccumulator.fill called with 24 entries\r\nAccumulator.fill called with 20 entries\r\nWritableTree.extend called with 100 entries\r\nAccumulator.fill called with 22 entries\r\nAccumulator.fill called with 18 entries\r\nAccumulator.fill called with 27 entries\r\nAccumulator.fill called with 19 entries\r\nAccumulator.fill called with 30 entries\r\nWritableTree.extend called with 116 entries\r\nAccumulator.fill called with 27 entries\r\nAccumulator.fill called with 22 entries\r\nAccumulator.fill called with 12 entries\r\nAccumulator.fill called with 15 entries\r\nAccumulator.fill called with 17 entries\r\nAccumulator.fill called with 26 entries\r\nWritableTree.extend called with 119 entries\r\nAccumulator.fill called with 20 entries\r\nAccumulator.fill called with 15 entries\r\nAccumulator.fill called with 15 entries\r\nAccumulator.fill called with 19 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 16 entries\r\nWritableTree.extend called with 108 entries\r\nAccumulator.fill called with 19 entries\r\nAccumulator.fill called with 16 entries\r\nAccumulator.fill called with 19 entries\r\nAccumulator.fill called with 20 entries\r\nAccumulator.fill called with 20 entries\r\nAccumulator.fill called with 24 entries\r\nWritableTree.extend called with 118 entries\r\nAccumulator.fill called with 23 entries\r\nAccumulator.fill called with 22 entries\r\nAccumulator.fill called with 27 entries\r\nAccumulator.fill called with 20 entries\r\nAccumulator.fill called with 18 entries\r\nWritableTree.extend called with 110 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 17 entries\r\nAccumulator.fill called with 16 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 21 entries\r\nAccumulator.fill called with 21 entries\r\nWritableTree.extend called with 117 entries\r\nAccumulator.fill called with 17 entries\r\nend\r\nWritableTree.extend called with 17 entries\r\n```\r\n\r\nThe context manager is what ensures that the last 17 entries get written (after \"end\").",
  "created_at":"2022-06-07T14:32:32Z",
  "id":1148757949,
  "issue":605,
  "node_id":"IC_kwDOD6Q_ss5EeKe9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-07T14:32:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The `Accumulator` looks nice. Probably will copy that into my code. If it could be part of Uproot, it would be perfect.",
  "created_at":"2022-06-07T14:39:26Z",
  "id":1148766833,
  "issue":605,
  "node_id":"IC_kwDOD6Q_ss5EeMpx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-07T14:39:26Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I can confirm this bug. I think it happens only if the tree is in a (sub)directory, because when exceeding the number of baskets, `_move_tree` moves the tree in `WritableFile._trees`, but not in the (sub)directory.\r\nHere's a simple reproducer:\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport uproot\r\nimport awkward as ak\r\nimport numpy as np\r\n\r\n# works\r\nwith uproot.recreate(\"test.root\") as root_file:\r\n    root_file.mktree(\"tree\", {\"branch\": np.float32}, initial_basket_capacity=3)\r\n    for i in range(4):\r\n        print(i)\r\n        root_file[\"tree\"].extend({\"branch\": np.zeros(10)})\r\n\r\n# works, too\r\nwith uproot.recreate(\"test.root\") as root_file:\r\n    root_file.mktree(\"dir/tree\", {\"branch\": np.float32}, initial_basket_capacity=3)\r\n    for i in range(4):\r\n        print(i)\r\n        tree = next((tree for tree in root_file._file._trees.values() if \"/\".join(tree.path) == \"dir/tree\"))\r\n        tree.extend({\"branch\": np.zeros(10)})\r\n\r\n# doesn't work\r\nwith uproot.recreate(\"test.root\") as root_file:\r\n    root_file.mktree(\"dir/tree\", {\"branch\": np.float32}, initial_basket_capacity=3)\r\n    for i in range(4):\r\n        print(i)\r\n        root_file[\"dir/tree\"].extend({\"branch\": np.zeros(10)})\r\n```",
  "created_at":"2022-07-25T15:27:38Z",
  "id":1194201298,
  "issue":606,
  "node_id":"IC_kwDOD6Q_ss5HLhDS",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-07-25T15:27:38Z",
  "user":"MDQ6VXNlcjE2NDAzODY="
 },
 {
  "author_association":"MEMBER",
  "body":"This was a subtle issue with caches, solved by manually checking to see if any active (writable) TTrees own the TDirectory they live in, and if so, that should supersede any subdirectory owned by a root directory.\r\n\r\nAnyway, the symptom was quite a corner case: it only happens if you're filling a TTree inside a subdirectory (as @YSelfTool said) and you pull the TTree from the TDirectory via\r\n\r\n```python\r\noutputRootFile[t].extend(...)\r\n```\r\n\r\nor\r\n\r\n```python\r\nroot_file[\"dir/tree\"].extend(...)\r\n```\r\n\r\ninstead of holding a reference to it like\r\n\r\n```python\r\nwritable_tree.extend(...)\r\n```\r\n\r\n_and also_ cross a threshold in which the WritableTree has to move itself in the file because its metadata changes size (which is why it depends on `stepSize`).\r\n\r\nBut I'm glad you found it! The fix is targeting both 5.0.0rc2 (`main`, #677) and 4.3.5 (`main-v4`, #678).",
  "created_at":"2022-08-12T00:28:37Z",
  "id":1212616906,
  "issue":606,
  "node_id":"IC_kwDOD6Q_ss5IRxDK",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "hooray":1,
   "total_count":2
  },
  "updated_at":"2022-08-12T00:28:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Update: If I use another library (e.g. `'np'`), I get a different error message:\r\n```pycon\r\n>>> f['MGTree']['event'].array(library=\"np\")\r\n[...]\r\nattempting to get bytes 3221179793:3221179799\r\noutside expected range 0:11791 for this Chunk\r\nin file gerda-run0100-20190104T135505Z-cal-ged-tier1.root\r\nin object /MGTree;1\r\n```",
  "created_at":"2022-06-11T09:58:26Z",
  "id":1152892079,
  "issue":607,
  "node_id":"IC_kwDOD6Q_ss5Et7yv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-11T09:58:26Z",
  "user":"MDQ6VXNlcjIwMzU4MTky"
 },
 {
  "author_association":"MEMBER",
  "body":"See the comment in #674: although I've fixed the exact issue you reported (thanks!), I don't think we're going to be solve all of the issues in such a complex data type (sorry).\r\n\r\nAt least the error message is better\u2014it says there's an issue in deserialization, which there is.",
  "created_at":"2022-08-11T21:14:18Z",
  "id":1212498751,
  "issue":607,
  "node_id":"IC_kwDOD6Q_ss5IRUM_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-11T21:14:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I understand, thanks a lot for letting me know! Do you have a recommendation on how to proceed? I really need to decode those files into Python data types.",
  "created_at":"2022-08-12T08:47:11Z",
  "id":1212878152,
  "issue":607,
  "node_id":"IC_kwDOD6Q_ss5ISw1I",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-12T08:47:11Z",
  "user":"MDQ6VXNlcjIwMzU4MTky"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I looked into it (spent a few hours on it) and there's just too much advanced ROOT I/O in this data type to figure out.\r\n\r\nGenerally, the hardest part is determining whether objects have headers or not and how big those headers are, and the first issue is that kind of issue: one of MGTEvent's base classes moves the file pointer 2 bytes too far. Arbitrarily moving that back, I can see that the first event's `fEventType` is `0`, `fETotal` is `86.98358966676925`, and `fTime` is `1440608785.364249` (August 26, 2015).\r\n\r\nBut then past that, `fWaveforms` and `fDigitizerData` had to be skipped for a reason I didn't investigate and we hit `fActiveID`, an `std::vector<bool>` written with memberwise serialization. Memberwise serialization is a long-standing unimplemented case in Uproot (#38). It's a second I/O system within ROOT I/O that we just haven't had the time to reverse engineer. (There are no specifications at this level of detail.)\r\n\r\nCan this file be rewritten with `splitLevel=99`, to try to get each field into its own TBranch? That makes things a lot easier (and faster[^1]), and even if there are still some TBranches that can't be interpreted, they might not be the ones you need.\r\n\r\nAlso if you can rewrite the file, manually converting it into simpler data types (analogous to NanoAOD) would also make things a lot easier (and faster[^1]). In essence, this is avoiding the more advanced features of ROOT I/O.\r\n\r\n[^1]: Faster: as a single TBranch, the serialization format is complex and we're forced to iterate over the fields of each struct, one by one. When split into multiple TBranches, some of them have homogeneous type (e.g. a big block of integers with nothing in between). We don't need to iterate over data of homogeneous type; we can just cast it as a NumPy array, which takes the same (small) amount of time independently of how large the array is.",
  "created_at":"2022-08-12T16:01:03Z",
  "id":1213271850,
  "issue":607,
  "node_id":"IC_kwDOD6Q_ss5IUQ8q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-12T16:01:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim, thanks a lot for looking into it! Unfortunately, there's years and years of data in ROOT format that I'd like to be able to decode on-the-fly (and, in my specific case, re-encode in HDF5). Rewriting it or converting it to some other intermediate format would be complicated.\r\n\r\nAt this point, I'd rather implement some decoder for the original DAQ files.\r\n\r\nThanks a lot again!",
  "created_at":"2022-08-26T19:55:45Z",
  "id":1228877539,
  "issue":607,
  "node_id":"IC_kwDOD6Q_ss5JPy7j",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-26T19:55:45Z",
  "user":"MDQ6VXNlcjIwMzU4MTky"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Final usage:\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> from pprint import pprint as pp\r\n>>> file1 = skhep_testdata.data_path(\"uproot-Zmumu.root\") + \":events\"\r\n>>> file2 = skhep_testdata.data_path(\"uproot-HZZ.root\") + \":events\"\r\n>>>\r\n>>> for val in uproot.num_entries([file1,file2]):\r\n...     print(val)\r\n...\r\n2304\r\n2421\r\n```\r\nThis PR is now ready for review.",
  "created_at":"2022-06-17T14:49:36Z",
  "id":1158947668,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FFCNU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-17T14:49:36Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"From your example, it looks like the output is an iterable of numbers. The input paths can be changed by the `_regularize_files` function\u2014it might be expanding wildcards on local file systems, maybe also expanding home directory tildes and turning relative paths into absolute paths. It would be much safer to use this function if the output had the actual paths used associated with the numbers of entries. I think we discussed `yield file_path, object_path, num_entries`. That would provide the most information (and there are idioms for wrapping that up as a dict, etc.).",
  "created_at":"2022-06-17T14:54:08Z",
  "id":1158951806,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FFDN-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-17T14:54:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for the review! Sorry I forgot we discussed about yielding all 3 vars.\r\n\r\nNew usage:\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> file0 = '~/.local/skhepdata/uproot-Zmumu.root:events' # including the tilde\r\n>>> file2 = skhep_testdata.data_path(\"uproot-HZZ.root\") + \":events\"\r\n>>>\r\n>>> for val in uproot.num_entries([file0,file2]):\r\n...     print(val)\r\n...\r\n('~/.local/skhepdata/uproot-Zmumu.root', 'events', 2304)\r\n('/home/kmk/.local/skhepdata/uproot-HZZ.root', 'events', 2421)\r\n>>> # Internally, the tilde is expanded for our use, but then only the representation provided by the user is yielded\r\n```\r\nI just realised the problem with the tests having paths specific to my computer. Do I change them to `/home/runner` or just test the `num_entries` obtained?",
  "created_at":"2022-06-17T16:44:57Z",
  "id":1159056476,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FFcxc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-17T16:46:11Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"No, the path locations should be independent of where the tests run. `/home/runner` is a GitHub Actions thing; it would be different on different CIs and we wouldn't want to change them all the time (or explain to a user who tries to run the tests why the tests aren't working).\r\n\r\nYou can get a full path to `uproot-Zmumu.root` by calling\r\n\r\n```python\r\nskhep_testdata.data_path(\"uproot-Zmumu.root\")\r\n```\r\n\r\nwhich also downloads it, if it's not already in `~/.local`. (See the other tests; they all use this pattern.)",
  "created_at":"2022-06-17T16:48:11Z",
  "id":1159059470,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FFdgO",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-06-17T16:48:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I dont think this is failing due to something I changed. Because other builds seem to have succeded.",
  "created_at":"2022-06-17T17:24:53Z",
  "id":1159091628,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FFlWs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-17T17:25:32Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"> I dont think this is failing due to something I changed. Because other builds seem to have succeded.\r\n\r\nIt looked like ROOT's behavior changed (#612).",
  "created_at":"2022-06-17T18:50:19Z",
  "id":1159147758,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FFzDu",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-06-17T18:50:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and if you merge with `main`, the failing test should be fixed. (I meant to do that before re-running those tests...)",
  "created_at":"2022-06-17T20:09:50Z",
  "id":1159196153,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FF-35",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-17T20:09:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I have moved the functions to `_util.py` but this was causing a circular import error as `HasBranches` and `_NoClose` classes could not be moved from `behaviors/TBranch.py`. I have kind of avoided that by:\r\n- Importing the required classes only inside the `regularize_files` and `regularize_object_path` functions.\r\n- Passing the `HasBranches` class into `_regularize_files_inner` as a new argument, as it didn't make sense to import it at every call of `_regularize_files_inner`\r\n\r\nI could possibly define `_regularize_files_inner` inside `regularize_files` in order to avoid the unnecessary argument (already tested this and it works). Would you suggest that? Please have a look and suggest further changes if necessary.",
  "created_at":"2022-06-18T04:22:01Z",
  "id":1159358239,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FGmcf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-18T04:22:01Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"Importing `_util` late is one solution, but Python also makes a distinction between\r\n\r\n```python\r\nimport uproot._util\r\n```\r\n\r\nand\r\n\r\n```python\r\nfrom uproot._util import _regularize_files\r\n```\r\n\r\nIf it knows that it only has to import one symbol, it somehow manages to avoid the circularity. We use that in a few places.\r\n\r\nIn general, I don't like to make functions nested unless the inner one is enclosing some variables of the outer one's scope, which can simplify it by reducing the number of arguments. It's like avoiding too-deep class hierarchies in object-oriented programming: we _can_ nest then, but we shouldn't do so unless there's a reason. Another potential reason is that it's the only way to make the helper function absolutely private in Python. Nested functions, unlike underscored functions, absolutely can't be accessed. That could be a good reason for this pair, because there's no reason the helper function could ever be used by anything else. However, that logic wasn't applied to the rest of the Uproot codebase.\r\n\r\nI should look at what you've done, because late import is also a fine solution to the problem. (I can't check today, though.) Just as long as it minimizes complexity, I'm happy.",
  "created_at":"2022-06-19T17:40:02Z",
  "id":1159782320,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FIN-w",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-06-19T17:40:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks!",
  "created_at":"2022-06-20T13:48:08Z",
  "id":1160472218,
  "issue":609,
  "node_id":"IC_kwDOD6Q_ss5FK2aa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-20T13:48:08Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @aryan26roy for code\r\n",
  "created_at":"2022-07-09T03:08:51Z",
  "id":1179467866,
  "issue":610,
  "node_id":"IC_kwDOD6Q_ss5GTUBa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:08:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/645) to add @aryan26roy! :tada:",
  "created_at":"2022-07-09T03:08:59Z",
  "id":1179467889,
  "issue":610,
  "node_id":"IC_kwDOD6Q_ss5GTUBx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:08:59Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"Is this superseded by #644? If so, let's close the old ones so that I don't get confused about which one to look at.",
  "created_at":"2022-07-09T03:25:04Z",
  "id":1179469654,
  "issue":610,
  "node_id":"IC_kwDOD6Q_ss5GTUdW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:25:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Those arguments are required; they were consolidated into a `context` because more will be added and we want to be able to do that without touching 400 `awkward_form` calls throughout the codebase.\r\n\r\nAre you using this function? I had thought of it as an internal step in the process of executing `array`/`arrays`/`iterate`. Is there a particular class from which you call it, so that that class can be made to insert the defaults?\r\n\r\nIf _any_ of the `awkward_form` methods are being called, we could make `context=None` by default everywhere and have that be a signal to inject centrally maintained defaults.\r\n\r\n--------\r\n\r\nTo directly answer your question, the required argument is the explicit dict given in the docstring, though that's not a friendly user interface (wasn't intended to be...).",
  "created_at":"2022-06-21T19:12:26Z",
  "id":1162215190,
  "issue":614,
  "node_id":"IC_kwDOD6Q_ss5FRf8W",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T19:12:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sounds good. We don't use that method directly, but it is used by coffea, e.g., here:\r\n\r\nhttps://github.com/andrewhennessee/coffea/blob/44396eab7e2ed580fae982768edf3fb8ea27d3f1/coffea/nanoevents/mapping/uproot.py#L94\r\n\r\n  I'll file an issue/pr with them. Thanks!",
  "created_at":"2022-06-21T20:37:24Z",
  "id":1162321428,
  "issue":614,
  "node_id":"IC_kwDOD6Q_ss5FR54U",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T20:37:24Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #618 reverted the `Interpretation.awkward_form` argument list, which was merged into `main-v4` (and `main`), and `main-v4` was used to release [Uproot 4.3.0](https://pypi.org/project/uproot/4.3.0/), the first in the Uproot 4 bug-fix line. From our end, it seems to have worked; let me know if updating to the latest Uproot doesn't fix this problem.",
  "created_at":"2022-06-21T22:42:06Z",
  "id":1162434040,
  "issue":614,
  "node_id":"IC_kwDOD6Q_ss5FSVX4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-21T22:42:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"We use some other private methods near this one:\r\nhttps://github.com/CoffeaTeam/coffea/blob/master/coffea/nanoevents/mapping/uproot.py#L100-L105\r\nPerhaps this should change as well?",
  "created_at":"2022-06-22T15:20:54Z",
  "id":1163251242,
  "issue":614,
  "node_id":"IC_kwDOD6Q_ss5FVc4q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T15:20:54Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know of a reason to change those. The `awkward_form` methods (specifically) are\r\n\r\n   * numerous: definitions and references appeared in 400 places throughout the codebase\r\n   * have a lot of arguments, and whenever we want to do a new thing with them, another argument had to be added (that's why we got `index_format`, `header`, `tobject_header` distinct from `header`, etc.). Every time an argument is added, it means making changes at all 400 locations, so I did it one last time, turning all of the arguments into a single `context` dict. Now when we want to add functionality, we add keys to the dict. This is already how the `read` methods work.\r\n\r\nThe \"fixing\" and \"removing Uproot parameters\" functions don't have that problem. In fact, these two steps in the process are probably going to be removed: they take data generated by a slow Python crawl, call `ak.from_iter` on them, and then fix the data types for things like Python `int` \u2192 `np.int64` \u2192 `np.int32` if the TBranch was 32-bit. That won't be necessary when the Forth code generates Awkward Arrays with the right types to begin with.",
  "created_at":"2022-06-22T17:27:30Z",
  "id":1163412923,
  "issue":614,
  "node_id":"IC_kwDOD6Q_ss5FWEW7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T17:27:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The above commit has all instances of `awkward.layout` removed and replaced with `awkward.contents` and `awkward.index`. Classes which were separate in akv1 but are now combined in akv2, for eg `ListOffsetArray64` have been replaced by their corresponding classes in akv2.\r\n\r\n@jpivarski There are 6 places in `uproot.interpretation.library` where I need some help in deciding what to do. I have marked all these locations with `# PLEASE SEE: ...`\r\n2 of them were in the form `cls = getattr(awkward.layout, form[\"class\"])`. Now, I'm not really sure if I should have replaced it with `awkward.contents` or `awkward.index`. With some minor context from other functions, I decided to go with `awkward.contents` for now. Please suggest. For the other 4, some tests are failing because we aren't initialising the `RecordArray` class right. Changing `keys` to `fields` didn't seem to work. Please suggest.\r\n\r\nAryan's work had a few instances of `awkward.layout` so I changed that too, I'll revert before merging if those changes were not to be done, or if they can possibly interfere with his work.\r\n\r\nLastly as discussed, the typeparser was added to v2. The release hasn't made it to pypi at the time of writing, so I just did `ip install -U git+https://github.com/scikit-hep/awkward@main` to test it out, which gave me the correct `1.9.0rc6 version`. But despite that, the number of failing tests just went from 57 to 52. A lot of tests are failing due to `TypeError: object of type 'awkward._ext.ArrayType' has no len()` that is called somewhere down the stacktrace of the newly added `from_datashape()` function.\r\n\r\nEssentially this had the same effect as directly importing ak_v1 inline and using that v1 parser, as the same tests fail, and the same tests are resolved (the 5 which were failing earlier). Please have a look at the last few tests in the CI for the error.",
  "created_at":"2022-06-22T18:11:22Z",
  "id":1163453058,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5FWOKC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T18:14:01Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I found 7 major ways in which the tests are failing:\r\n\r\n1. `awkward.operations.convert.to_list()` throws `TypeError: use ak._v2.operations.convert.to_list for v2 arrays (for now)`\r\n    Edit: I just realised this is because of the test importing ak_v1 and then using ak.to_list(). We can just remove these tests for now.\r\n\r\n2. `awkward.operations.describe.type()` throws \r\n```\r\nE           TypeError: unrecognized array type: <Array [{x: 1.1, y: 1, z: 97}, ..., {...}] type='5 * {x: float64, y: int32,...'>\r\nE\r\nE           (https://github.com/scikit-hep/awkward-1.0/blob/1.9.0rc6/src/awkward/operations/describe.py#L191)\r\n``` \r\nI believe this is because somewhere in the tests we might be using ak_v1 arrays and that is conflicting.\r\n\r\n3. `tests/test_0034-generic-objects-in-ttrees.py` is particularly troublesome because it is comparing 2 json strings where one actually has ak_v1-specific classes like `ListOffsetArray64`. So all the tests here fail somewhere along `_v2.types.numpytype.py`.\r\n\r\n4. Another error is `AttributeError: type object 'Form' has no attribute 'fromjson'` for which I'm sure there is a good replacement. Please suggest.\r\n\r\n5. \r\n```\r\nE       AssertionError: assert False\r\nE        +  where False = isinstance(<Array [{i8: -15, f8: -14.9}, ..., {...}] type='420 * {i8: int64, f8: float64}'>, <class 'awkward.highlevel.Array'>)\r\nE        +    where <class 'awkward.highlevel.Array'> = <module 'awkward' from '/home/kmk/uprootall/uproot5/venv/lib/python3.8/site-packages/awkward/__init__.py'>.Array\r\n```\r\nI'm not sure what this is, but you provided a few notes on the highlevel submodule, I'll go through that, some code and get back to this test.\r\n\r\n6. The changed API for RecordArray is causing issues, as mentioned in the comment above. `TypeError: __init__() got an unexpected keyword argument 'keys'`\r\n\r\n7. Lastly the various TypeErrors caused during parsing through the `from _datashape()` function.\r\nExamples are:\r\n`TypeError: not a NumPy dtype or an Awkward datashape: int32`\r\n`TypeError: not a NumPy dtype or an Awkward datashape: {\"x\": float64, \"y\": 3 * float64}` etc...\r\n\r\nBut somewhere along the stacktrace the typeparser code in awkward is trying to use `PrimitiveType` and `RecordType` which as you explained were replaced with corresponding Numpy types.\r\nExamples of errors thrown are:\r\n`TypeError: object of type 'awkward._ext.RecordType' has no len()`\r\n`TypeError: object of type 'awkward._ext.PrimitiveType' has no len()`",
  "created_at":"2022-06-22T18:39:44Z",
  "id":1163477977,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5FWUPZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T18:46:12Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"> I need some help in deciding what to do. I have marked all these locations with `# PLEASE SEE: ...`\r\n\r\nDone.\r\n\r\n> Lastly as discussed, the typeparser was added to v2. The release hasn't made it to pypi at the time of writing, so I just did `ip install -U git+https://github.com/scikit-hep/awkward@main` to test it out, which gave me the correct `1.9.0rc6 version`. But despite that, the number of failing tests just went from 57 to 52. A lot of tests are failing due to `TypeError: object of type 'awkward._ext.ArrayType' has no len()` that is called somewhere down the stacktrace of the newly added `from_datashape()` function.\r\n\r\nI'll be fixing (mostly replacing) the new `from_datashape` pretty soon.\r\n\r\nThis error doesn't sound like it's related, since `awkward._ext.*` is a bunch of v1 classes (with the exception of the ForthMachine that Aryan is using).\r\n\r\nIf you need to make pytest's output less noisy, you can put a\r\n\r\n```python\r\n@pytest.mark.skip(reason=\"FIXME: remove this mark before end of PR\")\r\n```\r\n\r\ndecorator on every test that fails except the one you're working on. The pytest output will list a lot of skipped tests at the end. Then, you can leave the ones related to `from_datashape` until the end.\r\n\r\n> 1. `awkward.operations.convert.to_list()` throws `TypeError: use ak._v2.operations.convert.to_list for v2 arrays (for now)`\r\n>    Edit: I just realised this is because of the test importing ak_v1 and then using ak.to_list(). We can just remove these tests for now.\r\n\r\nBetter yet: replace the v1 `to_list` with a v2 `to_list`. The feature of having v1 functions raise an exception when they encounter a v2 object was to help find these issues. Not all of the tests have such checks, unfortunately, so we can't rely on it as an absolute indicator, but it helps with the ones that it does catch.\r\n\r\n> 2. `awkward.operations.describe.type()` throws\r\n>\r\n> I believe this is because somewhere in the tests we might be using ak_v1 arrays and that is conflicting.\r\n\r\nThis `awkward.operations.describe.type` is a v1 function; it should be replaced with v2.\r\n\r\nOn Zoom, I said that the submodules within `awkward._v2.operations` were flattened, so\r\n\r\n```python\r\nawkward.operations.describe.type\r\n```\r\n\r\nbecomes\r\n\r\n```python\r\nawkward._v2.operations.type\r\n```\r\n\r\nBut for `type` specifically, ak.Arrays and ak.Records now have a `type` property that can be used directly; no need to use the function.\r\n\r\n> 3. `tests/test_0034-generic-objects-in-ttrees.py` is particularly troublesome because it is comparing 2 json strings where one actually has ak_v1-specific classes like `ListOffsetArray64`. So all the tests here fail somewhere along `_v2.types.numpytype.py`.\r\n\r\nIf the new JSON string doesn't have the \"`64`\"s, you can drop them from the test. It's changing the test, but we understand why.\r\n\r\n> 4. Another error is `AttributeError: type object 'Form' has no attribute 'fromjson'` for which I'm sure there is a good replacement. Please suggest.\r\n\r\n```python\r\nawkward._v2.forms.from_json\r\n```\r\n\r\n> ```\r\n> E       AssertionError: assert False\r\n> E        +  where False = isinstance(<Array [{i8: -15, f8: -14.9}, ..., {...}] type='420 * {i8: int64, f8: float64}'>, <class 'awkward.highlevel.Array'>)\r\n> E        +    where <class 'awkward.highlevel.Array'> = <module 'awkward' from '/home/kmk/uprootall/uproot5/venv/lib/python3.8/site-packages/awkward/__init__.py'>.Array\r\n> ```\r\n\r\nIs this `isinstance` being given a `awkward.highlevel.Array` when it should be given a `awkward._v2.highlevel.Array`?\r\n\r\n(It's also possible to drop the word \"`highlevel.`\" from both of the above.)\r\n\r\n> 6. The changed API for RecordArray is causing issues, as mentioned in the comment above. `TypeError: __init__() got an unexpected keyword argument 'keys'`\r\n\r\nThe argument named \"`keys`\" has become \"`fields`\". (In v1, several different words were used for this concept; in v2, they were all consolidated to just one: \"`fields`\".)\r\n\r\n> 7. Lastly the various TypeErrors caused during parsing through the `from _datashape()` function.\r\n>    Examples are:\r\n>    `TypeError: not a NumPy dtype or an Awkward datashape: int32`\r\n>    `TypeError: not a NumPy dtype or an Awkward datashape: {\"x\": float64, \"y\": 3 * float64}` etc...\r\n> \r\n> But somewhere along the stacktrace the typeparser code in awkward is trying to use `PrimitiveType` and `RecordType` which as you explained were replaced with corresponding Numpy types. Examples of errors thrown are: `TypeError: object of type 'awkward._ext.RecordType' has no len()` `TypeError: object of type 'awkward._ext.PrimitiveType' has no len()`\r\n\r\nI don't see a way for v1 types to get into the v2 `from_datashape`, but I'll be looking into that soon.\r\n",
  "created_at":"2022-06-22T20:41:31Z",
  "id":1163580387,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5FWtPj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T20:41:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski \r\nI reset the entire branch (thus deleting all my commits) and then merged the branch we were working on. Yet a test seems to be failing. Currently, there is no difference between this branch and the branch we were working on during the meeting.\r\n\r\nIs it possible, we might not have pushed a change? I remember you correcting the test that is currently failing.",
  "created_at":"2022-07-05T20:55:47Z",
  "id":1175488895,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5GEIl_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-05T20:58:41Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"The failure is in the thing I thought I'd have to change in Awkward (and is therefore temporary):\r\n\r\n```\r\nE             - 1 * [var * (int64, BDSOutputROOTGeant4Data::ParticleInfo[name: string, charge: int64, mass: float64]), parameters={\"__array__\": \"sorted_map\"}]\r\nE             + 1 * [var * (int64, struct[{name: string, charge: int64, mass: float64}, parameters={\"__record__\": \"BDSOutputROOTGeant4Data::ParticleInfo\"}]), parameters={\"__array__\": \"sorted_map\"}]\r\n```\r\n\r\nThe question is whether a record name like `\"BDSOutputROOTGeant4Data::ParticleInfo\"` can be put before the square brackets, in place of `\"struct\"`. The version of Awkward that CI is using has a different opinion, so just change this test to what Awkward wants it to be. We might have to change it again because I don't think a name with \"`::`\" in it should be allowed to do that. (So I guess that means that the latest-released Awkward is correct, and the `main` branch needs to be reverted, at least for cases like this.)",
  "created_at":"2022-07-05T21:13:36Z",
  "id":1175504135,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5GEMUH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-05T21:13:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski \r\nSo while solving the erronous test, I realised that a few more tests that depended on ROOT were also failing.\r\n\r\nIn order to solve them, I had to change the arguments of from_datashape. As already discussed, in ak v1 the default for high-level was False, and in v2 the default is True. Due to this change, the instances of from_datashape in uproot now have to explicitly mention `highlevel=False`.\r\n\r\nNow, based on our meet, we did talk about how maybe it should have been `highlevel=True`, but assuming the code before the update to v2 was correct, I think that the outermost length of the awkward array has already been dealt with in this case. Please have a look and suggest accordingly.",
  "created_at":"2022-07-07T09:14:28Z",
  "id":1177294310,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5GLBXm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-07T09:19:57Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"As I said in Slack, we're having trouble with #644: it doesn't pass all tests, and so we won't merge it yet. The trouble has been traced back to Awkward Array, and it will take a long time to get an Awkward Array fix into PyPI so that Uproot's CI can see it.\r\n\r\nTherefore, we'll merge this one first, and do #644 later.",
  "created_at":"2022-07-08T22:30:52Z",
  "id":1179407247,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5GTFOP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-08T22:30:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @kkothari2001 for code",
  "created_at":"2022-07-09T03:10:15Z",
  "id":1179468042,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5GTUEK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:10:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/646) to add @kkothari2001! :tada:",
  "created_at":"2022-07-09T03:10:24Z",
  "id":1179468057,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5GTUEZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:10:24Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @kkothari2001 for code",
  "created_at":"2022-07-09T03:25:27Z",
  "id":1179469701,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5GTUeF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:25:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\n@kkothari2001 already contributed before to code",
  "created_at":"2022-07-09T03:25:28Z",
  "id":1179469703,
  "issue":620,
  "node_id":"IC_kwDOD6Q_ss5GTUeH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:25:28Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"I need to figure out git problems.",
  "created_at":"2022-06-22T12:18:11Z",
  "id":1163026631,
  "issue":621,
  "node_id":"IC_kwDOD6Q_ss5FUmDH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T12:18:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Wrong: this one has to go into `main-v4`.",
  "created_at":"2022-06-22T12:27:52Z",
  "id":1163035534,
  "issue":623,
  "node_id":"IC_kwDOD6Q_ss5FUoOO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T12:27:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The version number bump is redundant with the state of the file in `main-v4`, but I did this just to trigger the tests.",
  "created_at":"2022-06-22T13:52:19Z",
  "id":1163127186,
  "issue":624,
  "node_id":"IC_kwDOD6Q_ss5FU-mS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T13:52:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is also a test of the CI machinery. The commits to `main-v4` have to run tests that trigger on a merge to `main-v4` and the commits to `main` have to run tests that trigger on a merge to `main`.\r\n\r\nThat seems to be working.",
  "created_at":"2022-06-22T14:27:49Z",
  "id":1163174692,
  "issue":625,
  "node_id":"IC_kwDOD6Q_ss5FVKMk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-22T14:27:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Day 2 log:\r\nhttps://github.com/jblomer/root/blob/ntuple-binary-format-v1/tree/ntuple/v7/doc/specifications.md#page-list-envelope\r\n\r\nin this section, the analogy is:\r\n| RNTuple | Parquet | TTree |\r\n| -----------  | ----------- | -------- |\r\n| Cluster   |  Row Group | Cluster |\r\n| Column   |  Column | Branch |\r\n| Page   |  Page | Basket |\r\n\r\nWhere page and basket are the granularity of I/O operation, and the cluster/row group is useful for chunked parallel processing.\r\n\r\nThe main difference of RNTuple is that it can have multiple clusters(!), where in Parquet the row groups would govern chunking for all columns. This is highlighted under `Column Group Record Frame` section of the RNTuple spec as:\r\n>Otherwise the enclosing list frame in the footer envelope is empty and all clusters span all columns.\r\n\r\nThis means that in order to find an event `N` in column `Col`:\r\n1. find out which cluster group does `Col` belong to\r\n2. find the cluster inside the cluster group (need `N` for offset calculation, think `fBasketEntry` in TTree)\r\n3. find the inner page list inside the cluster\r\n4. find the page description inside the inner page list\r\n5. follow page description jump to bytes and parse.",
  "created_at":"2022-06-28T20:02:29Z",
  "id":1169175660,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5FsDRs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-28T20:03:18Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"## Notes on `Field` and `Column`:\r\n\r\n`field` is what user sees at the top level, and `field` can have sub-`field` attached to it, this is encoded as `Parent Field ID` in `field description`. At the bottom level, `column` each represents collection of primitive types of data (in `page`s), and need to be attached to field, multiple columns can be attached to the same field.\r\n\r\nHere's my current undersdtanding of a `RNTuple` that has two `fields`: `[\"v_muon_tlvs\"::vector{Math::LorentzVector}, \"passtrigger\"::Bool]`:\r\n\r\n```mermaid\r\ngraph TB\r\n\r\n    top[\"RecordArray (top level)\"] --> tlv[\"ListOffsetArray (v_muon_tlvs)\"]\r\n    tlv --> tlv_offset[/\"NumpyArray (_0::uint32)\"\\]\r\n    top --> bool[\"NumpyArray (passtrigger::Bool)\"]\r\n    bool --> c5((column))\r\n    tlv --> tlvrecord[\"RecordArray  (::LorentzVector)\"]\r\n    tlvrecord --> pt[\"NumpyArray (pt::float32)\"]\r\n    pt --> c1((column))\r\n    tlvrecord --> eta[\"NumpyArray (eta::float32)\"]\r\n    eta --> c2((column))\r\n    tlvrecord --> phi[\"NumpyArray (phi::float32)\"]\r\n    phi --> c3((column))\r\n    tlvrecord --> mass[\"NumpyArray (mass::float32)\"]\r\n    mass --> c4((column))\r\n```",
  "created_at":"2022-06-29T15:42:09Z",
  "id":1170142526,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5FvvU-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-29T19:22:58Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Here's how to plug that into Awkward Array:\r\n\r\n```python\r\nimport awkward._v2 as ak\r\nimport numpy as np\r\n\r\nform = ak.forms.ListOffsetForm(\r\n    \"u32\",\r\n    ak.forms.RecordForm(\r\n        [\r\n            ak.forms.NumpyForm(\"float64\", form_key=\"key1\"),\r\n            ak.forms.NumpyForm(\"float64\", form_key=\"key2\"),\r\n            ak.forms.NumpyForm(\"float64\", form_key=\"key3\"),\r\n            ak.forms.NumpyForm(\"float64\", form_key=\"key4\"),\r\n        ],\r\n        [\r\n            \"px\",\r\n            \"py\",\r\n            \"pz\",\r\n            \"E\",\r\n        ],\r\n        form_key=\"whatever\",\r\n    ),\r\n    form_key=\"key0\",\r\n)\r\n\r\nform = form.select_columns(\"p*\")   # optional projections to choose columns to read\r\n\r\nprint(\"form\")\r\nprint(form)\r\n\r\nclass Container:\r\n    def __getitem__(self, name):\r\n        if name == \"key0-offsets\":\r\n            return np.array([0, 3, 3, 5], np.uint32)\r\n        elif name == \"key1-data\":\r\n            return np.array([1.1, 2.1, 3.1, 4.1, 5.1])\r\n        elif name == \"key2-data\":\r\n            return np.array([1.2, 2.2, 3.2, 4.2, 5.2])\r\n        elif name == \"key3-data\":\r\n            return np.array([1.3, 2.3, 3.3, 4.3, 5.3])\r\n        elif name == \"key4-data\":\r\n            return np.array([1.4, 2.4, 3.4, 4.4, 5.4])\r\n\r\ncontainer = Container()\r\n\r\nlength = 3\r\nprint(\"\\nlength\", length)\r\n\r\narray = ak.from_buffers(form, length, container)\r\n\r\nprint(\"array\")\r\narray.show(type=True)\r\n```",
  "created_at":"2022-06-29T16:40:02Z",
  "id":1170222190,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5FwCxu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-29T16:40:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @Moelf for code\r\n",
  "created_at":"2022-07-09T03:11:10Z",
  "id":1179468157,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5GTUF9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:11:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/647) to add @Moelf! :tada:",
  "created_at":"2022-07-09T03:11:18Z",
  "id":1179468186,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5GTUGa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:11:18Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"A concrete example to understand how top_fields <- fields <- column relates to each other.\r\n\r\nthe `LV` is a simple struct with 4 float fields, which should be the same for `Math::LorentzVector`.\r\n\r\n## User sees four \"column\"s, we should call them fields to be consistent with RNTuple:\r\n1. one_integers (`int32`)\r\n2. two_v_floats (`vector<float>`)\r\n3. three_LV (`LV`)\r\n4. four_v_LVs (`vector<LV>`)\r\n\r\n## the field records:\r\n```python\r\n[MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=0, struct_role=0, flags=0, field_name='one_integers', type_name='std::int32_t', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=1, struct_role=1, flags=0, field_name='two_v_floats', type_name='std::vector<float>', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=2, struct_role=2, flags=0, field_name='three_LV', type_name='LV', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=3, struct_role=1, flags=0, field_name='four_v_LVs', type_name='std::vector<LV>', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=1, struct_role=0, flags=0, field_name='_0', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=2, struct_role=0, flags=0, field_name='pt', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=2, struct_role=0, flags=0, field_name='eta', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=2, struct_role=0, flags=0, field_name='phi', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=2, struct_role=0, flags=0, field_name='mass', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=3, struct_role=2, flags=0, field_name='_0', type_name='LV', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=9, struct_role=0, flags=0, field_name='pt', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=9, struct_role=0, flags=0, field_name='eta', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=9, struct_role=0, flags=0, field_name='phi', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=9, struct_role=0, flags=0, field_name='mass', type_name='float', type_alias='', field_desc='')]\r\n```\r\n\r\nObservation:\r\n1. each top-level field must have `parent_field_id` == their own index\r\n2. take the row with `field_name='two_v_floats'` as example, `struct_role=1` means it's a parent field of \"vector\" field, and looking down, we see the next field with `parent_field_id == 1` indeed stores the data for this jagged field\r\n```python\r\n...parent_field_id=1, struct_role=0, flags=0, field_name='_0', type_name='float'...\r\n```\r\n3. for the most complicated field, `four_v_LVs`, we see that it has one sub field:\r\n```python\r\nparent_field_id=3, struct_role=2, flags=0, field_name='_0', type_name='LV', type_alias='', field_desc=''),\r\n```\r\nwhich then has 4 sub fields to represent the four fields the struct `LV` has:\r\n```python\r\nMetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=9, struct_role=0, flags=0, field_name='pt', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=9, struct_role=0, flags=0, field_name='eta', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=9, struct_role=0, flags=0, field_name='phi', type_name='float', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=9, struct_role=0, flags=0, field_name='mass', type_name='float', type_alias='', field_desc='')\r\n ```\r\n\r\n## the column records:\r\n```python\r\n[MetaData('ColumnRecordFrame', type=11, nbits=32, field_id=0, flags=0),\r\n MetaData('ColumnRecordFrame', type=2, nbits=32, field_id=1, flags=5),\r\n MetaData('ColumnRecordFrame', type=2, nbits=32, field_id=3, flags=5),\r\n MetaData('ColumnRecordFrame', type=8, nbits=32, field_id=4, flags=0),\r\n MetaData('ColumnRecordFrame', type=8, nbits=32, field_id=5, flags=0),\r\n MetaData('ColumnRecordFrame', type=8, nbits=32, field_id=6, flags=0),\r\n MetaData('ColumnRecordFrame', type=8, nbits=32, field_id=7, flags=0),\r\n MetaData('ColumnRecordFrame', type=8, nbits=32, field_id=8, flags=0),\r\n MetaData('ColumnRecordFrame', type=8, nbits=32, field_id=10, flags=0),\r\n MetaData('ColumnRecordFrame', type=8, nbits=32, field_id=11, flags=0),\r\n MetaData('ColumnRecordFrame', type=8, nbits=32, field_id=12, flags=0),\r\n MetaData('ColumnRecordFrame', type=8, nbits=32, field_id=13, flags=0)]\r\n ```\r\n\r\nobservations:\r\n1. the two columns with `type = 2` (Index32) are the \"offset\" for the jagged top field, `flags=5` mean non-negative\r\n2. there's no `field_id = 2` because that field is `field_name='three_LV'`, all the data are in the fields (pt, eta, phi, m)\r\n3. there's also no column attached to `field_id=9` because that's the `LV` under the parent `vector<LV>` field, which again has no information other than type name, the offset is stored in column with `field_id==3`",
  "created_at":"2022-07-10T16:12:33Z",
  "id":1179756418,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5GUaeC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-10T16:14:17Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"the `form_key` (`col-N`) directly corresponds to the `N-th` column, this should make building column data reader easier. \r\n\r\n```bash\r\npython -c 'from pprint import pprint; import skhep_testdata; import uproot as up; filename = skhep_t\r\nestdata.data_path(\"test_ntuple_int_vfloat_tlv_vtlv.root\"); r = up.open(filename)[\"ntuple\"]; print(r.to_akform())'\r\n```\r\n\r\n<details> <summary> output </summary>\r\n\r\n```python\r\n{\r\n    \"class\": \"RecordArray\",\r\n    \"contents\": {\r\n        \"one_integers\": {\r\n            \"class\": \"NumpyArray\",\r\n            \"primitive\": \"int32\",\r\n            \"form_key\": \"col-0\"\r\n        },\r\n        \"two_v_floats\": {\r\n            \"class\": \"ListOffsetArray\",\r\n            \"offsets\": \"u32\",\r\n            \"content\": {\r\n                \"class\": \"NumpyArray\",\r\n                \"primitive\": \"float32\",\r\n                \"form_key\": \"col-4\"\r\n            },\r\n            \"form_key\": \"col-1\"\r\n        },\r\n        \"three_LV\": {\r\n            \"class\": \"RecordArray\",\r\n            \"contents\": {\r\n                \"pt\": {\r\n                    \"class\": \"NumpyArray\",\r\n                    \"primitive\": \"float32\",\r\n                    \"form_key\": \"col-5\"\r\n                },\r\n                \"eta\": {\r\n                    \"class\": \"NumpyArray\",\r\n                    \"primitive\": \"float32\",\r\n                    \"form_key\": \"col-6\"\r\n                },\r\n                \"phi\": {\r\n                    \"class\": \"NumpyArray\",\r\n                    \"primitive\": \"float32\",\r\n                    \"form_key\": \"col-7\"\r\n                },\r\n                \"mass\": {\r\n                    \"class\": \"NumpyArray\",\r\n                    \"primitive\": \"float32\",\r\n                    \"form_key\": \"col-8\"\r\n                }\r\n            },\r\n            \"form_key\": \"whatever\"\r\n        },\r\n        \"four_v_LVs\": {\r\n            \"class\": \"ListOffsetArray\",\r\n            \"offsets\": \"u32\",\r\n            \"content\": {\r\n                \"class\": \"RecordArray\",\r\n                \"contents\": {\r\n                    \"pt\": {\r\n                        \"class\": \"NumpyArray\",\r\n                        \"primitive\": \"float32\",\r\n                        \"form_key\": \"col-10\"\r\n                    },\r\n                    \"eta\": {\r\n                        \"class\": \"NumpyArray\",\r\n                        \"primitive\": \"float32\",\r\n                        \"form_key\": \"col-11\"\r\n                    },\r\n                    \"phi\": {\r\n                        \"class\": \"NumpyArray\",\r\n                        \"primitive\": \"float32\",\r\n                        \"form_key\": \"col-12\"\r\n                    },\r\n                    \"mass\": {\r\n                        \"class\": \"NumpyArray\",\r\n                        \"primitive\": \"float32\",\r\n                        \"form_key\": \"col-13\"\r\n                    }\r\n                },\r\n                \"form_key\": \"whatever\"\r\n            },\r\n            \"form_key\": \"col-3\"\r\n        }\r\n    },\r\n    \"form_key\": \"toplevel\"\r\n}\r\n```\r\n\r\n</details>",
  "created_at":"2022-07-11T04:18:20Z",
  "id":1179948585,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5GVJYp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-12T23:34:09Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski can use some brain power here, any recursive/ tree/state-machine come to your mind?\r\n\r\nRight now the code is sort of recursive but doesn't feel robust and absolutely not readable.",
  "created_at":"2022-07-11T04:21:17Z",
  "id":1179950207,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5GVJx_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-11T04:21:17Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"What problem are you trying to improve the solution for? Interpreting the RNTuple metadata as a tree (specifically, a Form tree)?\r\n\r\nDo they give you the leaves first and then give you information on how the leaves are combined into bigger structures? If so, the code doesn't have to be recursive; you can build up Form objects from the leaves upward:\r\n\r\n```python\r\n# learn about a leaf, make it\r\nstack.append(ak._v2.forms.NumpyForm(\"int32\"))\r\n# learn about a leaf, make it\r\nstack.append(ak._v2.forms.NumpyForm(\"float64\"))\r\n# learn that the last two belong to a struct\r\ntmp = ak._v2.forms.RecordForm([stack.pop(), stack.pop()], [\"x\", \"y\"])\r\nstack.append(tmp)\r\n# learn that all of this should be in a list\r\ntmp = ak._v2.forms.ListOffsetForm(\"i32\", stack.pop())\r\nstack.append(tmp)\r\n```\r\n\r\nBut that only works if you get the information in a leaf-first recursive walk order. If the information is presented in exactly the opposite way, then maybe you can read it all into a list and then walk over the list in reverse. (This metadata is not large.)\r\n\r\nFor as many details as I know, this is as detailed as I can get right now.",
  "created_at":"2022-07-11T11:32:23Z",
  "id":1180297863,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5GWeqH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-11T11:32:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\npython -c 'import numpy as np; from pprint import pprint; import skhep_testdata; import uproot as up; import awkward as ak; filename = skhep_testdata.data_p\r\nath(\"test_ntuple_int_vfloat_tlv_vtlv.root\"); r = up.open(filename)[\"ntuple\"]; l = r.page_list_envelopes.pagelinklist[0][0]; desc = r.read_pagelist(l, 0); print(r.read_pagedesc(desc, np.dtype(\"i\r\nnt32\")))'\r\n[9 8 7 6 5]\r\n```\r\n\r\nthe cursor debug of the page is\r\n```\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  9   0   0   0   8   0   0   0   7   0   0   0   6   0   0   0   5   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n```",
  "created_at":"2022-07-12T20:57:50Z",
  "id":1182494428,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5Ge27c",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-12T20:57:50Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\npython -c 'import numpy as np; from pprint import pprint; import skhep_testdata; import uproot as up; import awkward as ak; filename = skhep_testdata.data_p\r\nath(\"test_ntuple_int_vfloat_tlv_vtlv.root\"); r = up.open(filename)[\"ntuple\"]; form = r.to_akform(); cont = r.col_container(); print(form); array = ak._v2.from_buffers(form, 5, cont); pprint(ak.\r\n_v2.operations.to_list(array))'\r\n```\r\nnote: the logic right now reads the first page from each column, we need to add logic to find page(s) given a entry range.\r\n\r\n<details> <summary> click to expand output </summary>\r\n\r\n```\r\n{\r\n    \"class\": \"RecordArray\",\r\n    \"contents\": {\r\n        \"one_integers\": {\r\n            \"class\": \"NumpyArray\",\r\n            \"primitive\": \"int32\",\r\n            \"form_key\": \"field-0\"\r\n        },\r\n        \"two_v_floats\": {\r\n            \"class\": \"ListOffsetArray\",\r\n            \"offsets\": \"u32\",\r\n            \"content\": {\r\n                \"class\": \"NumpyArray\",\r\n                \"primitive\": \"float32\",\r\n                \"form_key\": \"field-4\"\r\n            },\r\n            \"form_key\": \"field-1\"\r\n        },\r\n        \"three_LV\": {\r\n            \"class\": \"RecordArray\",\r\n            \"contents\": {\r\n                \"pt\": {\r\n                    \"class\": \"NumpyArray\",\r\n                    \"primitive\": \"float32\",\r\n                    \"form_key\": \"field-5\"\r\n                },\r\n                \"eta\": {\r\n                    \"class\": \"NumpyArray\",\r\n                    \"primitive\": \"float32\",\r\n                    \"form_key\": \"field-6\"\r\n                },\r\n                \"phi\": {\r\n                    \"class\": \"NumpyArray\",\r\n                    \"primitive\": \"float32\",\r\n                    \"form_key\": \"field-7\"\r\n                },\r\n                \"mass\": {\r\n                    \"class\": \"NumpyArray\",\r\n                    \"primitive\": \"float32\",\r\n                    \"form_key\": \"field-8\"\r\n                }\r\n            },\r\n            \"form_key\": \"whatever\"\r\n        },\r\n        \"four_v_LVs\": {\r\n            \"class\": \"ListOffsetArray\",\r\n            \"offsets\": \"u32\",\r\n            \"content\": {\r\n                \"class\": \"RecordArray\",\r\n                \"contents\": {\r\n                    \"pt\": {\r\n                        \"class\": \"NumpyArray\",\r\n                        \"primitive\": \"float32\",\r\n                        \"form_key\": \"field-10\"\r\n                    },\r\n                    \"eta\": {\r\n                        \"class\": \"NumpyArray\",\r\n                        \"primitive\": \"float32\",\r\n                        \"form_key\": \"field-11\"\r\n                    },\r\n                    \"phi\": {\r\n                        \"class\": \"NumpyArray\",\r\n                        \"primitive\": \"float32\",\r\n                        \"form_key\": \"field-12\"\r\n                    },\r\n                    \"mass\": {\r\n                        \"class\": \"NumpyArray\",\r\n                        \"primitive\": \"float32\",\r\n                        \"form_key\": \"field-13\"\r\n                    }\r\n                },\r\n                \"form_key\": \"whatever\"\r\n            },\r\n            \"form_key\": \"field-3\"\r\n        }\r\n    },\r\n    \"form_key\": \"toplevel-whatever\"\r\n}\r\n[{'four_v_LVs': [{'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0}],\r\n  'one_integers': 9,\r\n  'three_LV': {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n  'two_v_floats': [9.0, 8.0, 7.0, 6.0]},\r\n {'four_v_LVs': [{'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0}],\r\n  'one_integers': 8,\r\n  'three_LV': {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n  'two_v_floats': [5.0, 4.0, 3.0]},\r\n {'four_v_LVs': [{'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0},\r\n                 {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0}],\r\n  'one_integers': 7,\r\n  'three_LV': {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0},\r\n  'two_v_floats': [2.0, 1.0]},\r\n {'four_v_LVs': [{'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0},\r\n                 {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0},\r\n                 {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0}],\r\n  'one_integers': 6,\r\n  'three_LV': {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0},\r\n  'two_v_floats': [0.0, -1.0]},\r\n {'four_v_LVs': [{'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 19.0, 'mass': 19.0, 'phi': 19.0, 'pt': 19.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 18.0, 'mass': 18.0, 'phi': 18.0, 'pt': 18.0},\r\n                 {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0},\r\n                 {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0},\r\n                 {'eta': 17.0, 'mass': 17.0, 'phi': 17.0, 'pt': 17.0}],\r\n  'one_integers': 5,\r\n  'three_LV': {'eta': 16.0, 'mass': 16.0, 'phi': 16.0, 'pt': 16.0},\r\n  'two_v_floats': [-2.0]}]\r\n```\r\n\r\n</details>",
  "created_at":"2022-07-12T23:28:24Z",
  "id":1182594995,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5GfPez",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-12T23:45:45Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"## what we use for `form_key`:\r\n\r\nSince a column must have a parent field (i.e. all the splitting is done at `field_records`, where fields can have `parent_field_id` not equal to its own index and so on), it's much easier to use index in the `field_records` as `form_key`.\r\n\r\nIt's also trivial for records in the `column_records` because each record as a `cr.field_id` field",
  "created_at":"2022-07-12T23:35:38Z",
  "id":1182598443,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5GfQUr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-12T23:35:38Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"to make multiple pages, need to make >65536 bytes, for example, 5e4 ints is stored as:\r\n```python\r\nipython -c 'import numpy as np; from pprint import pprint; import skhep_testdata; import uproot\r\n as up; import awkward as ak; filename = skhep_testdata.data_path(\"test_ntuple_int_5e4.root\"); r = up.open(filename)[\"ntuple\"]; pprint(ak._v2\r\n.operations.to_list(r.arrays())); pprint(r.cluster_summaries)'\r\n[MetaData('PageDescription', num_elements=16384, locator=MetaData('Locator', num_bytes=65536, offset=1035)), \r\nMetaData('PageDescription', num_elements=16384, locator=MetaData('Locator', num_bytes=65536, offset=66605)), \r\nMetaData('PageDescription', num_elements=17232, locator=MetaData('Locator', num_bytes=68928, offset=132175))]\r\n[{'one_integers': 50000}]\r\n```",
  "created_at":"2022-08-01T03:31:32Z",
  "id":1200654997,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5HkIqV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-01T03:37:17Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"some testing scripts:\r\n```python\r\nipython -c 'import numpy as np; from pprint import pprint; import skhep_testdata; import uproot as up; i\r\nmport awkward as ak; filename = skhep_testdata.data_path(\"test_ntuple_int_5.root\"); r = up.open(filename)[\"ntuple\"]; pprint(ak._v2.operations\r\n.to_list(r.arrays()));'\r\n[{'one_integers': 9},\r\n {'one_integers': 8},\r\n {'one_integers': 7},\r\n {'one_integers': 6},\r\n {'one_integers': 5}]\r\n\r\nipython -c 'import numpy as np; from pprint import pprint; import skhep_testdata; import uproot as up; i\r\nmport awkward as ak; filename = skhep_testdata.data_path(\"test_ntuple_int_5.root\"); r = up.open(filename)[\"ntuple\"]; pprint(ak._v2.operations\r\n.to_list(r.arrays(entry_stop=3)));'\r\n[{'one_integers': 9}, {'one_integers': 8}, {'one_integers': 7}]\r\n```\r\n",
  "created_at":"2022-08-01T04:45:34Z",
  "id":1200699282,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5HkTeS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-01T04:45:34Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski if we can merge https://github.com/scikit-hep/scikit-hep-testdata/pull/80 (and make a release?) first then I can add more unit test to this PR before we merge it",
  "created_at":"2022-08-01T05:02:28Z",
  "id":1200708942,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5HkV1O",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-01T05:02:43Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"All of the pull requests into scikit-hep-testdata with new ROOT files have been merged and released as version 0.4.16.",
  "created_at":"2022-08-01T17:59:17Z",
  "id":1201529715,
  "issue":630,
  "node_id":"IC_kwDOD6Q_ss5HneNz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-01T17:59:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sounds good to me. I'll be merging this into `main` (Uproot v5 development). Should it also be in `main-v4` (Uproot v4 maintenance)? We're maintaining two central branches until Awkward v1 is dropped in December. If this is important for any Uproot v4 updates, I'll cherry-pick this PR commit into the `main-v4` branch.",
  "created_at":"2022-06-29T12:13:31Z",
  "id":1169906049,
  "issue":631,
  "node_id":"IC_kwDOD6Q_ss5Fu1mB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-29T12:13:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"These kinds of updates will only go to `main`. Maybe `main-v4` doesn't need the GitHub Actions Dependabot.",
  "created_at":"2022-06-29T12:21:16Z",
  "id":1169913149,
  "issue":632,
  "node_id":"IC_kwDOD6Q_ss5Fu3U9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-06-29T12:21:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@aryan26roy, this is a suite of test-stubs that you can use to set up test-driven development for adding Forth code. Only that which is tested by these tests needs to be developed as part of your project. The comments in the code with your name are cross-listed with the test numbers, so that you can plan which ones to solve when. By the end of your project, those comment lines should be removed (even the ones you can't do anything with, the \"untested\" ones).\r\n\r\nLet me know if it would be easier for you to merge your branch first or for me to merge this one first. The target is `main`, not `main-v4`.",
  "created_at":"2022-07-01T02:25:39Z",
  "id":1171857542,
  "issue":637,
  "node_id":"IC_kwDOD6Q_ss5F2SCG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-01T02:25:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Note to self: remember to cherry-pick the merge commit from `main` to `main-v4`.",
  "created_at":"2022-07-02T17:36:49Z",
  "id":1172933522,
  "issue":638,
  "node_id":"IC_kwDOD6Q_ss5F6YuS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-02T17:36:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@tamasgal, the backdoor that you rely on that calls specialized C++ for doubly nested lists (`std::vector<float>[]`, if I remember right) will be closed in Awkward v2, which corresponds to Uproot v5. The last time that we accidentally closed this door was in issue #572. At that time, I told you about the project that @aryan26roy is working on now (see https://github.com/scikit-hep/uproot5/issues/572#issuecomment-1066963522).\r\n\r\nAs you can see in this PR, that work has started. Actually, this PR is after @aryan26roy has already settled on a way to proceed, and he is now adding AwkwardForth to everything. In the near future, it would be a good time to test your custom Interpretations to be sure that the right AwkwardForth is generated for them. That will probably be after this PR is closed, I'm just giving you a heads-up.\r\n\r\nMaybe a first step would be to ensure that we're planning to generate AwkwardForth along all the code paths that your Interpretations touch. [These tests](https://github.com/scikit-hep/uproot5/blob/aryan-forth-reader-latest/tests/test_0637-setup-tests-for-AwkwardForth.py) were made to cover all the code paths, and those sites where AwkwardForth need to be generated are annotated with comments like\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/92b0b08552fa6f5797fb8b373ee8b660388e9e58/src/uproot/streamers.py#L1225\r\n\r\nHowever, there were a few of these that didn't seem to have any test cases, like this one:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/92b0b08552fa6f5797fb8b373ee8b660388e9e58/src/uproot/streamers.py#L1333\r\n\r\nIf your data and Interpretations help us increase our coverage, that would be great! (If not, these sites would toggle a flag on AwkwardForth generation saying to give up and run slow, which is no worse than they are right now.)\r\n\r\nI should also let you know that the current `main` branch is for Uproot v5 and it already has your backdoor removed. The `main-v4` branch is what we use to make new bug-fix releases for Uproot v4.",
  "created_at":"2022-07-19T19:15:11Z",
  "id":1189459477,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5G5bYV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-19T19:15:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sorry for the very late response, I was very busy and abroad and just got a little time window to look over the notifications.\r\n\r\nThese are awesome news and it seems to be a huge achievement!\r\n\r\nI am very curious about the new AwkwardForth implementation and will test them as soon as possible. However, please do not expect any extensive feedback before mid September.",
  "created_at":"2022-08-16T15:50:04Z",
  "id":1216824342,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5Ih0QW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-16T15:50:04Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Alright, I had a little exploration, but not sure what exactly I should expect, so let me boldly post something which I guess should work out of the box. This uses a test file sample available in the `km3net_testdata` Python package (I attached the file here: [km3net_online.root.zip](https://github.com/scikit-hep/uproot5/files/9470112/km3net_online.root.zip)). I think this file should also be part of the uproot testdata package.\r\n\r\nIn the below REPL session, I am trying to access `f[\"KM3NET_EVENT/triggeredHits\"].array()` which is a vector of vectors of `JDAQTriggeredHit`, a simple structure which I used to interpret with the following `uproot3` code:\r\n\r\n```python\r\n            triggered_hits = f[\"KM3NET_EVENT/triggeredHits\"].array(\r\n                uproot3.asjagged(\r\n                    uproot3.astable(\r\n                        uproot3.asdtype(\r\n                            [\r\n                                (\"dom_id\", \">i4\"),\r\n                                (\"channel_id\", \"u1\"),\r\n                                (\"time\", \"<u4\"),\r\n                                (\"tot\", \"u1\"),\r\n                                (\" cnt\", \"u4\"),\r\n                                (\" vers\", \"u2\"),\r\n                                (\"trigger_mask\", \">u8\"),\r\n                            ]\r\n                        )\r\n                    ),\r\n                    skipbytes=10,\r\n                )\r\n            )\r\n```\r\n\r\nHere is the sample session with the current `aryan-forth-reader-latest` branch of `uproot5`:\r\n\r\n\r\n```python\r\nIn [1]: import uproot\r\nuproot.\r\nIn [2]: uproot.__version__\r\nOut[2]: '5.0.0rc1'\r\n\r\nIn [3]: uproot.__file__\r\nOut[3]: '/Users/tamasgal/Dev/uproot5/src/uproot/__init__.py'\r\n\r\nIn [4]: from km3net_testdata import data_path\r\n\r\nIn [5]: f = uproot.open(data_path(\"online/km3net_online.root\"))\r\n\r\nIn [6]: f.keys()\r\nOut[6]:\r\n['JTRIGGER::JTriggerParameters;1',\r\n 'META;1',\r\n 'META/JMeta;1',\r\n 'META/JDataWriter;1',\r\n 'E;1',\r\n 'KM3NET_TIMESLICE;1',\r\n 'KM3NET_TIMESLICE_L0;1',\r\n 'KM3NET_TIMESLICE_L1;1',\r\n 'KM3NET_TIMESLICE_L2;1',\r\n 'KM3NET_TIMESLICE_SN;1',\r\n 'KM3NET_EVENT;1',\r\n 'KM3NET_SUMMARYSLICE;1']\r\n\r\nIn [7]: f[\"KM3NET_EVENT\"].keys()\r\nOut[7]:\r\n['KM3NET_EVENT',\r\n 'KM3NET_EVENT/KM3NETDAQ::JDAQPreamble',\r\n 'KM3NET_EVENT/KM3NETDAQ::JDAQEventHeader',\r\n 'KM3NET_EVENT/triggeredHits',\r\n 'KM3NET_EVENT/snapshotHits']\r\n\r\nIn [8]: f[\"KM3NET_EVENT/triggeredHits\"].keys()\r\nOut[8]: []\r\n\r\nIn [9]: f[\"KM3NET_EVENT/triggeredHits\"].array()\r\n---------------------------------------------------------------------------\r\nCannotBeAwkward                           Traceback (most recent call last)\r\nInput In [9], in <cell line: 1>()\r\n----> 1 f[\"KM3NET_EVENT/triggeredHits\"].array()\r\n\r\nFile ~/Dev/uproot5/src/uproot/behaviors/TBranch.py:2025, in TBranch.array(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library)\r\n   2019             for (\r\n   2020                 basket_num,\r\n   2021                 range_or_basket,\r\n   2022             ) in branch.entries_to_ranges_or_baskets(entry_start, entry_stop):\r\n   2023                 ranges_or_baskets.append((branch, basket_num, range_or_basket))\r\n-> 2025 _ranges_or_baskets_to_arrays(\r\n   2026     self,\r\n   2027     ranges_or_baskets,\r\n   2028     branchid_interpretation,\r\n   2029     entry_start,\r\n   2030     entry_stop,\r\n   2031     decompression_executor,\r\n   2032     interpretation_executor,\r\n   2033     library,\r\n   2034     arrays,\r\n   2035     False,\r\n   2036 )\r\n   2038 _fix_asgrouped(\r\n   2039     arrays, expression_context, branchid_interpretation, library, None\r\n   2040 )\r\n   2042 if array_cache is not None:\r\n\r\nFile ~/Dev/uproot5/src/uproot/behaviors/TBranch.py:3316, in _ranges_or_baskets_to_arrays(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays, update_ranges_or_baskets)\r\n   3313     pass\r\n   3315 elif isinstance(obj, tuple) and len(obj) == 3:\r\n-> 3316     uproot.source.futures.delayed_raise(*obj)\r\n   3318 else:\r\n   3319     raise AssertionError(obj)\r\n\r\nFile ~/Dev/uproot5/src/uproot/source/futures.py:36, in delayed_raise(exception_class, exception_value, traceback)\r\n     32 def delayed_raise(exception_class, exception_value, traceback):\r\n     33     \"\"\"\r\n     34     Raise an exception from a background thread on the main thread.\r\n     35     \"\"\"\r\n---> 36     raise exception_value.with_traceback(traceback)\r\n\r\nFile ~/Dev/uproot5/src/uproot/behaviors/TBranch.py:3260, in _ranges_or_baskets_to_arrays.<locals>.basket_to_array(basket)\r\n   3257 context = dict(branch.context)\r\n   3258 context[\"forth\"] = forth_context\r\n-> 3260 basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n   3261     basket.data,\r\n   3262     basket.byte_offsets,\r\n   3263     basket,\r\n   3264     branch,\r\n   3265     context,\r\n   3266     basket.member(\"fKeylen\"),\r\n   3267     library,\r\n   3268 )\r\n   3269 if basket.num_entries != len(basket_arrays[basket.basket_num]):\r\n   3270     raise ValueError(\r\n   3271         \"\"\"basket {} in tree/branch {} has the wrong number of entries \"\"\"\r\n   3272         \"\"\"(expected {}, obtained {}) when interpreted as {}\r\n   (...)\r\n   3280         )\r\n   3281     )\r\n\r\nFile ~/Dev/uproot5/src/uproot/interpretation/objects.py:153, in AsObjects.basket_array(self, data, byte_offsets, basket, branch, context, cursor_offset, library)\r\n    151 output = None\r\n    152 if isinstance(library, uproot.interpretation.library.Awkward):\r\n--> 153     form = self.awkward_form(branch.file)\r\n    155     if awkward_can_optimize(self, form):\r\n    156         import awkward._connect._uproot\r\n\r\nFile ~/Dev/uproot5/src/uproot/interpretation/objects.py:129, in AsObjects.awkward_form(self, file, context, index_format, header, tobject_header, breadcrumbs)\r\n    127     return self._model.awkward_form(self._branch.file, context)\r\n    128 else:\r\n--> 129     return self._model.awkward_form(self._branch.file, context)\r\n\r\nFile ~/Dev/uproot5/src/uproot/containers.py:1102, in AsVector.awkward_form(self, file, context)\r\n   1098 def awkward_form(self, file, context):\r\n   1099     awkward = uproot.extras.awkward()\r\n   1100     return awkward.forms.ListOffsetForm(\r\n   1101         context[\"index_format\"],\r\n-> 1102         uproot._util.awkward_form(self._values, file, context),\r\n   1103         parameters={\"uproot\": {\"as\": \"vector\", \"header\": self._header}},\r\n   1104     )\r\n\r\nFile ~/Dev/uproot5/src/uproot/_util.py:584, in awkward_form(model, file, context)\r\n    581     return _primitive_awkward_form[model]\r\n    583 else:\r\n--> 584     return model.awkward_form(file, context)\r\n\r\nFile ~/Dev/uproot5/src/uproot/model.py:661, in Model.awkward_form(cls, file, context)\r\n    636 @classmethod\r\n    637 def awkward_form(cls, file, context):\r\n    638     \"\"\"\r\n    639     Args:\r\n    640         cls (subclass of :doc:`uproot.model.Model`): This class.\r\n   (...)\r\n    659     Awkward Array.\r\n    660     \"\"\"\r\n--> 661     raise uproot.interpretation.objects.CannotBeAwkward(\r\n    662         classname_decode(cls.__name__)[0]\r\n    663     )\r\n\r\nCannotBeAwkward: JDAQTriggeredHit\r\n```\r\n\r\nAnother example is the summary slices, which is slightly larger struct. The `uproot4` interpretation works with this one:\r\n\r\n```\r\nuproot.interpretation.numerical.AsDtype(\r\n                [\r\n                    (\" cnt\", \"u4\"),\r\n                    (\" vers\", \"u2\"),\r\n                    (\" cnt2\", \"u4\"),\r\n                    (\" vers2\", \"u2\"),\r\n                    (\" cnt3\", \"u4\"),\r\n                    (\" vers3\", \"u2\"),\r\n                    (\"detector_id\", \">i4\"),\r\n                    (\"run\", \">i4\"),\r\n                    (\"frame_index\", \">i4\"),\r\n                    (\" cnt4\", \"u4\"),\r\n                    (\" vers4\", \"u2\"),\r\n                    (\"UTC_seconds\", \">u4\"),\r\n                    (\"UTC_16nanosecondcycles\", \">u4\"),\r\n                ]\r\n            )\r\n\r\n```\r\n\r\nHere is the continuation of the session above with the current `aryan-forth-reader-latest` branch of `uproot5`, reading the summary slice data:\r\n\r\n```python\r\nIn [11]: f[\"KM3NET_SUMMARYSLICE/KM3NET_SUMMARYSLICE\"].keys()\r\nOut[11]:\r\n['KM3NETDAQ::JDAQPreamble',\r\n 'KM3NETDAQ::JDAQSummarysliceHeader',\r\n 'vector<KM3NETDAQ::JDAQSummaryFrame>']\r\n\r\nIn [12]: f[\"KM3NET_SUMMARYSLICE/KM3NET_SUMMARYSLICE/vector<KM3NETDAQ::JDAQSummaryFrame>\"]\r\nOut[12]: <TBranchElement 'vector<KM3NETDAQ::JDAQSummaryFrame>' at 0x000106b20dc0>\r\n\r\nIn [13]: f[\"KM3NET_SUMMARYSLICE/KM3NET_SUMMARYSLICE/vector<KM3NETDAQ::JDAQSummaryFrame>\"].keys()\r\nOut[13]: []\r\n\r\nIn [14]: f[\"KM3NET_SUMMARYSLICE/KM3NET_SUMMARYSLICE/vector<KM3NETDAQ::JDAQSummaryFrame>\"].array()\r\n---------------------------------------------------------------------------\r\nCannotBeAwkward                           Traceback (most recent call last)\r\nInput In [14], in <cell line: 1>()\r\n----> 1 f[\"KM3NET_SUMMARYSLICE/KM3NET_SUMMARYSLICE/vector<KM3NETDAQ::JDAQSummaryFrame>\"].array()\r\n\r\nFile ~/Dev/uproot5/src/uproot/behaviors/TBranch.py:2025, in TBranch.array(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library)\r\n   2019             for (\r\n   2020                 basket_num,\r\n   2021                 range_or_basket,\r\n   2022             ) in branch.entries_to_ranges_or_baskets(entry_start, entry_stop):\r\n   2023                 ranges_or_baskets.append((branch, basket_num, range_or_basket))\r\n-> 2025 _ranges_or_baskets_to_arrays(\r\n   2026     self,\r\n   2027     ranges_or_baskets,\r\n   2028     branchid_interpretation,\r\n   2029     entry_start,\r\n   2030     entry_stop,\r\n   2031     decompression_executor,\r\n   2032     interpretation_executor,\r\n   2033     library,\r\n   2034     arrays,\r\n   2035     False,\r\n   2036 )\r\n   2038 _fix_asgrouped(\r\n   2039     arrays, expression_context, branchid_interpretation, library, None\r\n   2040 )\r\n   2042 if array_cache is not None:\r\n\r\nFile ~/Dev/uproot5/src/uproot/behaviors/TBranch.py:3316, in _ranges_or_baskets_to_arrays(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays, update_ranges_or_baskets)\r\n   3313     pass\r\n   3315 elif isinstance(obj, tuple) and len(obj) == 3:\r\n-> 3316     uproot.source.futures.delayed_raise(*obj)\r\n   3318 else:\r\n   3319     raise AssertionError(obj)\r\n\r\nFile ~/Dev/uproot5/src/uproot/source/futures.py:36, in delayed_raise(exception_class, exception_value, traceback)\r\n     32 def delayed_raise(exception_class, exception_value, traceback):\r\n     33     \"\"\"\r\n     34     Raise an exception from a background thread on the main thread.\r\n     35     \"\"\"\r\n---> 36     raise exception_value.with_traceback(traceback)\r\n\r\nFile ~/Dev/uproot5/src/uproot/behaviors/TBranch.py:3260, in _ranges_or_baskets_to_arrays.<locals>.basket_to_array(basket)\r\n   3257 context = dict(branch.context)\r\n   3258 context[\"forth\"] = forth_context\r\n-> 3260 basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n   3261     basket.data,\r\n   3262     basket.byte_offsets,\r\n   3263     basket,\r\n   3264     branch,\r\n   3265     context,\r\n   3266     basket.member(\"fKeylen\"),\r\n   3267     library,\r\n   3268 )\r\n   3269 if basket.num_entries != len(basket_arrays[basket.basket_num]):\r\n   3270     raise ValueError(\r\n   3271         \"\"\"basket {} in tree/branch {} has the wrong number of entries \"\"\"\r\n   3272         \"\"\"(expected {}, obtained {}) when interpreted as {}\r\n   (...)\r\n   3280         )\r\n   3281     )\r\n\r\nFile ~/Dev/uproot5/src/uproot/interpretation/objects.py:153, in AsObjects.basket_array(self, data, byte_offsets, basket, branch, context, cursor_offset, library)\r\n    151 output = None\r\n    152 if isinstance(library, uproot.interpretation.library.Awkward):\r\n--> 153     form = self.awkward_form(branch.file)\r\n    155     if awkward_can_optimize(self, form):\r\n    156         import awkward._connect._uproot\r\n\r\nFile ~/Dev/uproot5/src/uproot/interpretation/objects.py:129, in AsObjects.awkward_form(self, file, context, index_format, header, tobject_header, breadcrumbs)\r\n    127     return self._model.awkward_form(self._branch.file, context)\r\n    128 else:\r\n--> 129     return self._model.awkward_form(self._branch.file, context)\r\n\r\nFile ~/Dev/uproot5/src/uproot/containers.py:1102, in AsVector.awkward_form(self, file, context)\r\n   1098 def awkward_form(self, file, context):\r\n   1099     awkward = uproot.extras.awkward()\r\n   1100     return awkward.forms.ListOffsetForm(\r\n   1101         context[\"index_format\"],\r\n-> 1102         uproot._util.awkward_form(self._values, file, context),\r\n   1103         parameters={\"uproot\": {\"as\": \"vector\", \"header\": self._header}},\r\n   1104     )\r\n\r\nFile ~/Dev/uproot5/src/uproot/_util.py:584, in awkward_form(model, file, context)\r\n    581     return _primitive_awkward_form[model]\r\n    583 else:\r\n--> 584     return model.awkward_form(file, context)\r\n\r\nFile ~/Dev/uproot5/src/uproot/model.py:661, in Model.awkward_form(cls, file, context)\r\n    636 @classmethod\r\n    637 def awkward_form(cls, file, context):\r\n    638     \"\"\"\r\n    639     Args:\r\n    640         cls (subclass of :doc:`uproot.model.Model`): This class.\r\n   (...)\r\n    659     Awkward Array.\r\n    660     \"\"\"\r\n--> 661     raise uproot.interpretation.objects.CannotBeAwkward(\r\n    662         classname_decode(cls.__name__)[0]\r\n    663     )\r\n\r\nCannotBeAwkward: KM3NETDAQ::JDAQSummaryFrame\r\n```",
  "created_at":"2022-09-01T13:12:28Z",
  "id":1234262493,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5JkVnd",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "rocket":1,
   "total_count":2
  },
  "updated_at":"2022-09-01T13:14:11Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Did these `CannotBeAwkward` errors happen before we changed anything? As far I know, we haven't touched the `awkward_form` method. The new procedure generates a Form while walking through the read-step of the first entry (and it's another issue about how we're going to maintain consistency between that Form with the Form that comes from the `awkward_form` method).\r\n\r\nThis PR is not intended to add interpretation capabilities, just to replace the implementation. The `awkward_form` method is called _before_ invoking the read-implementation that has changed. That's why I'm confused about seeing a _difference_.\r\n\r\n---------------\r\n\r\nHmm... Actually, all of your interpretations are [structured array](https://numpy.org/doc/stable/user/basics.rec.html) AsDtype. All of @aryan26roy's work has been on AsObjects and AsStrings. There's no reason that an AsDtype can't be Awkward. In fact, it wouldn't even need AwkwardForth to generate it, even if it's within an AsJagged, as in your first example. The AwkwardForth work in this branch is irrelevant to these particular `Interpretations`. There is still the doubly-jagged data from #572 that will depend on the AwkwardForth work, but that's not these examples.\r\n\r\nSo, is it possible to read the two cases you described above in Uproot 5 `main`? As `library=\"np\"` and/or `library=\"ak\"`? Because _those two cases_ shouldn't have anything to do with developments in this branch.",
  "created_at":"2022-09-01T16:29:13Z",
  "id":1234513934,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5JlTAO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T16:29:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah, my bad, I thought this branch also includes new interpretation mechanics. Sorry `;)` I also realised I picked the wrong branch, which does not contain double nested structures.\r\n\r\nAnyways, I can confirm that the doubly nested things work absolutely fine and return the correct type. Here is an example with a single branch which is doubly nested:\r\n\r\n```python\r\nIn [1]: import uproot\r\n\r\nIn [2]: from km3net_testdata import data_path\r\n\r\nIn [3]: f = uproot.open(data_path(\"offline/km3net_offline.root\"))\r\n\r\nIn [4]: f[\"E/Evt/trks/trks.rec_stages\"].array()\r\nOut[4]: <Array [[[1, 3, 5, 4], [1, ... 1], [1], [1]]] type='10 * var * var * int32'>\r\n\r\nIn [5]: uproot.__version__\r\nOut[5]: '5.0.0rc1'\r\n\r\n\u2591 tamasgal@silentbox-(5):uproot5 \ue0a0 aryan-forth-reader-latest uproot5 py-system took 1m 37s\r\n\u2591 21:08:09 > git describe\r\nfatal: No annotated tags can describe '7f35e2c3c8202cfc6f8e9a35146f455be9c50517'.\r\n```\r\n\r\n> Did these CannotBeAwkward errors happen before we changed anything? \r\n\r\nYes, similar `CannotBeAwkward` errors appear with uproot4 with the two examples in https://github.com/scikit-hep/uproot5/pull/644#issuecomment-1234262493 (I roughly checked the traceback and the code snippets seem to be the same for both `uproot4` and `uproot5`, including the `AssertionErrors`), so it's the same behaviour on this branch.\r\n\r\n> So, is it possible to read the two cases you described above in Uproot 5 main? As library=\"np\" and/or library=\"ak\"? \r\n\r\nNo, on the `main` branch I get a `ValueError` with the message\r\n\r\n> unknown class JDAQTriggeredHit that cannot be skipped because its number of bytes is unknown\r\n\r\nSee below. Do you want me to open a new issue and follow it up there?\r\n\r\n```python\r\nIn [9]: f[\"KM3NET_EVENT/triggeredHits\"].array(library=\"ak\")\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nInput In [9], in <cell line: 1>()\r\n----> 1 f[\"KM3NET_EVENT/triggeredHits\"].array(library=\"ak\")\r\n\r\nFile ~/Dev/uproot5/src/uproot/behaviors/TBranch.py:1895, in TBranch.array(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library)\r\n   1889             for (\r\n   1890                 basket_num,\r\n   1891                 range_or_basket,\r\n   1892             ) in branch.entries_to_ranges_or_baskets(entry_start, entry_stop):\r\n   1893                 ranges_or_baskets.append((branch, basket_num, range_or_basket))\r\n-> 1895 _ranges_or_baskets_to_arrays(\r\n   1896     self,\r\n   1897     ranges_or_baskets,\r\n   1898     branchid_interpretation,\r\n   1899     entry_start,\r\n   1900     entry_stop,\r\n   1901     decompression_executor,\r\n   1902     interpretation_executor,\r\n   1903     library,\r\n   1904     arrays,\r\n   1905     False,\r\n   1906 )\r\n   1908 _fix_asgrouped(\r\n   1909     arrays, expression_context, branchid_interpretation, library, None\r\n   1910 )\r\n   1912 if array_cache is not None:\r\n\r\nFile ~/Dev/uproot5/src/uproot/behaviors/TBranch.py:3180, in _ranges_or_baskets_to_arrays(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays, update_ranges_or_baskets)\r\n   3177     pass\r\n   3179 elif isinstance(obj, tuple) and len(obj) == 3:\r\n-> 3180     uproot.source.futures.delayed_raise(*obj)\r\n   3182 else:\r\n   3183     raise AssertionError(obj)\r\n\r\nFile ~/Dev/uproot5/src/uproot/source/futures.py:36, in delayed_raise(exception_class, exception_value, traceback)\r\n     32 def delayed_raise(exception_class, exception_value, traceback):\r\n     33     \"\"\"\r\n     34     Raise an exception from a background thread on the main thread.\r\n     35     \"\"\"\r\n---> 36     raise exception_value.with_traceback(traceback)\r\n\r\nFile ~/Dev/uproot5/src/uproot/behaviors/TBranch.py:3124, in _ranges_or_baskets_to_arrays.<locals>.basket_to_array(basket)\r\n   3121 interpretation = branchid_interpretation[branch.cache_key]\r\n   3122 basket_arrays = branchid_arrays[branch.cache_key]\r\n-> 3124 basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n   3125     basket.data,\r\n   3126     basket.byte_offsets,\r\n   3127     basket,\r\n   3128     branch,\r\n   3129     branch.context,\r\n   3130     basket.member(\"fKeylen\"),\r\n   3131     library,\r\n   3132 )\r\n   3133 if basket.num_entries != len(basket_arrays[basket.basket_num]):\r\n   3134     raise ValueError(\r\n   3135         \"\"\"basket {} in tree/branch {} has the wrong number of entries \"\"\"\r\n   3136         \"\"\"(expected {}, obtained {}) when interpreted as {}\r\n   (...)\r\n   3144         )\r\n   3145     )\r\n\r\nFile ~/Dev/uproot5/src/uproot/interpretation/objects.py:133, in AsObjects.basket_array(self, data, byte_offsets, basket, branch, context, cursor_offset, library)\r\n    129     output = self.basket_array_forth(\r\n    130         data, byte_offsets, basket, branch, context, cursor_offset, library\r\n    131     )\r\n    132 else:\r\n--> 133     output = ObjectArray(\r\n    134         self._model, branch, context, byte_offsets, data, cursor_offset\r\n    135     ).to_numpy()\r\n    137 self.hook_after_basket_array(\r\n    138     data=data,\r\n    139     byte_offsets=byte_offsets,\r\n   (...)\r\n    145     library=library,\r\n    146 )\r\n    148 return output\r\n\r\nFile ~/Dev/uproot5/src/uproot/interpretation/objects.py:696, in ObjectArray.to_numpy(self)\r\n    694 output = numpy.empty(len(self), dtype=numpy.dtype(object))\r\n    695 for i in range(len(self)):\r\n--> 696     output[i] = self[i]\r\n    697 return output\r\n\r\nFile ~/Dev/uproot5/src/uproot/interpretation/objects.py:711, in ObjectArray.__getitem__(self, where)\r\n    707     chunk = uproot.source.chunk.Chunk.wrap(self._branch.file.source, data)\r\n    708     cursor = uproot.source.cursor.Cursor(\r\n    709         0, origin=-(byte_start + self._cursor_offset)\r\n    710     )\r\n--> 711     return self._model.read(\r\n    712         chunk,\r\n    713         cursor,\r\n    714         self._context,\r\n    715         self._branch.file,\r\n    716         self._detached_file,\r\n    717         self._branch,\r\n    718     )\r\n    720 elif isinstance(where, slice):\r\n    721     return ObjectArray(\r\n    722         self._model,\r\n    723         self._branch,\r\n   (...)\r\n    727         self._cursor_offset,\r\n    728     )\r\n\r\nFile ~/Dev/uproot5/src/uproot/containers.py:1102, in AsVector.read(self, chunk, cursor, context, file, selffile, parent, header)\r\n   1099     if length == 0 and helper_obj.is_forth():\r\n   1100         forth_obj.var_set = True\r\n-> 1102     values = _read_nested(\r\n   1103         self._values, length, chunk, cursor, context, file, selffile, parent\r\n   1104     )\r\n   1106 if helper_obj.is_forth():\r\n   1107     forth_obj.go_to(temp)\r\n\r\nFile ~/Dev/uproot5/src/uproot/containers.py:64, in _read_nested(model, length, chunk, cursor, context, file, selffile, parent, header)\r\n     62 else:\r\n     63     for i in range(length):\r\n---> 64         values[i] = model.read(chunk, cursor, context, file, selffile, parent)\r\n     65 return values\r\n\r\nFile ~/Dev/uproot5/src/uproot/model.py:824, in Model.read(cls, chunk, cursor, context, file, selffile, parent, concrete)\r\n    813 if helper_obj.is_forth():\r\n    814     temp = forth_obj.add_node(\r\n    815         \"pass\",\r\n    816         helper_obj.get_pre(),\r\n   (...)\r\n    822         {},\r\n    823     )\r\n--> 824 self.read_members(chunk, cursor, context, file)\r\n    825 if helper_obj.is_forth():\r\n    826     forth_obj.go_to(temp)\r\n\r\nFile ~/Dev/uproot5/src/uproot/model.py:1481, in UnknownClass.read_members(self, chunk, cursor, context, file)\r\n   1478             cursor.skip(self._num_bytes - cursor.displacement(self._cursor))\r\n   1480         else:\r\n-> 1481             raise ValueError(\r\n   1482                 \"\"\"unknown class {} that cannot be skipped because its \"\"\"\r\n   1483                 \"\"\"number of bytes is unknown\r\n   1484 in file {}\"\"\".format(\r\n   1485                     self.classname, file.file_path\r\n   1486                 )\r\n   1487             )\r\n\r\nValueError: unknown class JDAQTriggeredHit that cannot be skipped because its number of bytes is unknown\r\nin file /Users/tamasgal/Dev/uproot5/venv/lib/python3.9/site-packages/km3net_testdata/data/online/km3net_online.root\r\n```",
  "created_at":"2022-09-02T19:19:00Z",
  "id":1235827088,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5JqTmQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T20:46:13Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"> No, on the `main` branch I get a `ValueError` with the message\r\n> \r\n> > unknown class JDAQTriggeredHit that cannot be skipped because its number of bytes is unknown\r\n> \r\n> See below. Do you want me to open a new issue and follow it up there?\r\n\r\nIf this is a new issue (i.e. it used to be interpreted and now it is not), then yes, I should look at it and find out what broke. If this type couldn't be read before, then it's not likely that I'll be able to solve it\u2014these issues got progressively more complex and I got progressively less time. In the past, you found custom Interpretations that worked for your classes; we can keep using that solution as long as nothing has gotten worse.\r\n\r\nThe AwkwardForth implementation (which is _porting_ the Python implementation, not adding new capabilities) should be invoked on hand-made Interpretations as well as automatically detected ones. That's what I want to make sure will work for you: it's possible that some of your hand-made Interpretations are providing coverage over parts of the codebase that our built-in test suite isn't. I think there are approximately 4 places in streamers.py where we should be adding AwkwardForth code, but we don't have any sample ROOT files + automatically identified Interpretations that cover these parts of the code, so we can't be certain that our implementations are correct. (In fact, I think we're leaving them as `NotImplementedErrors`.) If your Interpretations cover those parts, then (1) we'll be able to actually test them and (2) our final product won't be broken when you start using Uproot v5.\r\n\r\nThat's what we were looking for.\r\n\r\nAnd, wait a minute\u2014@aryan26roy, doesn't he need to set a `_use_forth = True` flag to the `AsObjects` Interpretation for it to actually use the AwkwardForth, at the moment?",
  "created_at":"2022-09-02T19:44:32Z",
  "id":1235842537,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5JqXXp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T19:44:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski yes, the exact flag that @tamasgal  needs to set is \"interpretation._forth=True\" to actually trigger the forth generation and reading process.",
  "created_at":"2022-09-02T19:50:23Z",
  "id":1235846163,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5JqYQT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T19:50:23Z",
  "user":"MDQ6VXNlcjUwNTc3ODA5"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"My custom interpretations seem to work fine! I added a test (https://github.com/scikit-hep/uproot5/pull/701) and here is the covereage in `streamers.py` before:\r\n\r\n```\r\nsrc/uproot/streamers.py                       546    191    65%   69, 71, 73, 75, 77, 81, 83, 85, 87, 89, 91, 95, 100, 102, 104, 106, 108, 112, 114, 116, 118, 120, 122, 126, 146, 164-174, 189, 213, 286, 288, 314, 364, 385, 423-429, 437-439, 465, 481, 503, 534-535, 548, 563-566, 574, 587-601, 621-633, 664, 669, 737-750, 770, 790-860, 915-918, 922-925, 960-961, 1018-1042, 1049, 1055, 1060, 1070, 1144-1148, 1173, 1194-1213, 1216-1235, 1238-1242, 1262, 1269, 1343-1351, 1408-1444, 1447-1451, 1464-1478, 1556-1560\r\n```\r\n\r\nand after adding the test:\r\n\r\n```\r\nsrc/uproot/streamers.py                       546     65    88%   89, 95, 102, 120, 126, 146, 189, 286, 385, 423-429, 503, 534-535, 548, 563-566, 587-601, 621-633, 741, 743, 797-806, 915-918, 922-925, 1055, 1060, 1148, 1173, 1194-1213, 1238-1242, 1262, 1269, 1348-1351, 1439-1442, 1447-1451\r\n```\r\n\r\nI think that's already a good jump in coverage (from 65% to 88%), but I will check if I can squeeze out more.\r\n\r\n",
  "created_at":"2022-09-02T20:43:12Z",
  "id":1235880772,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5JqgtE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T20:43:12Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah, if this provides more coverage, then it's great to add it! I'm a little confused as to how it could add more coverage to the AwkwardForth if the Interpretation is an `AsDtype`. (I'm talking about what I see in #701.)\r\n\r\n@aryan26roy, am I missing something?",
  "created_at":"2022-09-02T20:51:10Z",
  "id":1235885432,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5Jqh14",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T20:51:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I have not checked the exact lines, so those might not be related to AwkwardForth ;)\r\n\r\nBtw. I just realised that I don't have any AsObject interpretations in my codebase (anymore). Sorry for the confusion!\r\n\r\nEdit: I'd move the tests to a different place then...",
  "created_at":"2022-09-02T20:59:16Z",
  "id":1235890532,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5JqjFk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T21:00:16Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Getting that increase in coverage is a boon, anyway. If the coverage increase is still there without the `interpretation._forth = True` line, or when applied to the `main` branch, then we'd want to include it nevertheless (maybe in a PR that goes directly to `main`, and hopefully avoid whatever pre-commit was doing). That resolves what I considered issue (1), Uproot coverage.\r\n\r\nFor issue (2), if you don't have any custom `AsObjects` Interpretations, then that's great news: nothing will break!\r\n\r\nThe doubly-nested data that performance-regressed in issue #572 is relevant to AwkwardForth, though. Since it's a doubly nested list, it must be `AsObjects`, and we'll want to test its performance with AwkwardForth. The AwkwardForth implementation might be a little slower than the custom back-door we implemented with C++, but that custom back-door has to go away (it was always intended to be temporary\u2014though \"temporary\" has been years by now). What we need to ensure is that the AwkwardForth is no more than a little slower, like 50%, not catastrophically slower, like factors of 10\u00d7 or 100\u00d7.",
  "created_at":"2022-09-02T21:13:55Z",
  "id":1235903128,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5JqmKY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T21:13:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes I see, OK I'll put those two tests into a PR towards main. A tiny test for the doubly-nested data is coming :)",
  "created_at":"2022-09-02T21:15:54Z",
  "id":1235904179,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5Jqmaz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T21:15:54Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I just checked the coverage with those two tests on `main` (without forth) but there is no increase -- the coverage of `streamers.py` there is 88%, so that was a dud. It might be that I missed to install some dependency which skipped some other tests while switching between the virtualenvs.\r\n\r\nAnyways, the doubly-nested structure test is in-place, so feel free to copy it over since the pre-commit bombed my PR `;D`\r\n\r\nThis will do it in `tests/test_0637-setup-tests-for-AwkwardForth.py`:\r\n\r\n```python\r\ndef test_82():\r\n    with uproot.open(skhep_testdata.data_path(\"uproot-issue-214.root\")) as file:\r\n        branch = file[\"E/Evt/trks/trks.rec_stages\"]\r\n        interp = uproot.interpretation.identify.interpretation_of(branch, {}, False)\r\n        interp._forth = True\r\n        py = branch.array(interpretation=interp, library=\"ak\")\r\n        assert py[0][0][0] == 1\r\n```\r\n\r\nAll in all, I am pretty sure that at least KM3NeT is good to go with uproot5 without the need to change existing code, thanks `:)`\r\n\r\nI will report back on the performance once I get my hands on a larger dataset!",
  "created_at":"2022-09-02T21:42:11Z",
  "id":1235917741,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5Jqput",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T21:47:36Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I checked a few files which I had on my local drive and I don't see any significant performance difference in the read-out of doubly-nested data between uproot4 and this branch. For whatever reason, this branch performs even slightly better `;)`\r\n\r\nFor example with the file from https://github.com/scikit-hep/uproot5/issues/572 and reading the whole dataset via `f[\"E/Evt/trks/trks.rec_stages\"].array()` I get\r\n\r\n```\r\nthis branch: 222 ms \u00b1 5.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\nuproot4: 279 ms \u00b1 6.15 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```",
  "created_at":"2022-09-02T23:03:20Z",
  "id":1235964176,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5Jq1EQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T23:03:20Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Great! There were other changes, especially in how Awkward Arrays are constructed, so there are plenty of candidates to credit the speed-up with. (It's probably a balance between some things being faster and others being slower.) If the AwkwardForth wasn't working for some reason, it would be a lot worse:\r\n\r\n![awkwardforth-in-uproot-performance](https://user-images.githubusercontent.com/1852447/188245965-abc8b0c9-3ebe-4079-8dcf-8a5c3bc2d8f5.png)",
  "created_at":"2022-09-02T23:27:57Z",
  "id":1235975707,
  "issue":644,
  "node_id":"IC_kwDOD6Q_ss5Jq34b",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2022-09-02T23:27:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I should remember that I can't do these things concurrently.",
  "created_at":"2022-07-09T03:23:47Z",
  "id":1179469513,
  "issue":646,
  "node_id":"IC_kwDOD6Q_ss5GTUbJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:23:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I should remember that I can't do these things concurrently.",
  "created_at":"2022-07-09T03:23:37Z",
  "id":1179469493,
  "issue":647,
  "node_id":"IC_kwDOD6Q_ss5GTUa1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-09T03:23:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, so when writing a histogram axis, we need to recognize when it has a `transform` and, if so, treat it like a variable-binned axis. (There's no equivalent of the `transform` metadata in TH*.)\r\n\r\nHow can we recognize that a hist (or, more generally) boost-histogram axis has a `transform`? Is it an attribute or property?",
  "created_at":"2022-07-15T15:37:43Z",
  "id":1185662899,
  "issue":651,
  "node_id":"IC_kwDOD6Q_ss5Gq8ez",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-15T15:37:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Maybe this helps?\r\n\r\n```python\r\n# %%\r\nimport hist\r\n\r\ndef do_print(h):\r\n    a = h.axes[0]\r\n    print(a.transform)\r\n    print(type(a.transform))\r\n    print(bool(a.transform))\r\n\r\nprint(\"transformed:\")\r\nh = hist.Hist(hist.axis.Regular(3, 1, 1e4, transform=hist.axis.transform.log))\r\ndo_print(h)\r\nprint(\"not transformed:\")\r\nh = hist.Hist(hist.axis.Regular(3, 1, 1e4))\r\ndo_print(h)\r\n```\r\noutput:\r\n```text\r\ntransformed:\r\nlog\r\n<class 'boost_histogram.axis.transform.Function'>\r\nTrue\r\nnot transformed:\r\nNone\r\n<class 'NoneType'>\r\nFalse\r\n```",
  "created_at":"2022-07-15T16:29:45Z",
  "id":1185705100,
  "issue":651,
  "node_id":"IC_kwDOD6Q_ss5GrGyM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-15T16:32:02Z",
  "user":"MDQ6VXNlcjE1Nzc0ODM4"
 },
 {
  "author_association":"MEMBER",
  "body":"This is a feature request, similar to #672, but the scope is smaller: only the boost-histogram/hist \u2192 ROOT TH* direction needs to be considered. A transformation of an axis should be evaluated at the time of conversion and variable bin edges should be hard-coded into the ROOT file (because TH* has no concept of transformed axes).",
  "created_at":"2022-08-11T19:54:31Z",
  "id":1212425494,
  "issue":651,
  "node_id":"IC_kwDOD6Q_ss5IRCUW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-11T19:54:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"\"feature\" is probably more descriptive, but it's worth emphasizing that a naive user (like myself) is likely to get the wrong values for the bin edges without ever seeing a warning or encountering an error.",
  "created_at":"2022-08-11T20:06:10Z",
  "id":1212435987,
  "issue":651,
  "node_id":"IC_kwDOD6Q_ss5IRE4T",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-11T20:06:10Z",
  "user":"MDQ6VXNlcjE1Nzc0ODM4"
 },
 {
  "author_association":"MEMBER",
  "body":"Good point. I've implemented it in #675 (targeting 5.0.0rc2) and #676 (targeting 4.3.5).",
  "created_at":"2022-08-11T21:56:12Z",
  "id":1212535781,
  "issue":651,
  "node_id":"IC_kwDOD6Q_ss5IRdPl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-11T21:56:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hey @kkothari2001 thanks for the ping! I'll take a look at the PR in detail when I get a chance. \r\n\r\nBut in the mean time I wanted to point you to a proof-of-concept for reading ROOT into dask-awkward here: https://github.com/ContinuumIO/dask-awkward/pull/44 - the method I used in that PoC does not use delayed, directly constructing so-called \"high level\" task graphs (dask-awkward has a generic API for creating dask-awkward arrays from any mapping with `dask_awkward.from_map`). The main benefit of the `from_map` API over using delayed is that the task graph created by `from_map` can be optimized in ways that task graphs of delayed objects cannot.",
  "created_at":"2022-07-19T16:12:45Z",
  "id":1189273037,
  "issue":652,
  "node_id":"IC_kwDOD6Q_ss5G4t3N",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-07-19T16:12:45Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks! I'll have a look at it.",
  "created_at":"2022-07-19T16:37:56Z",
  "id":1189314936,
  "issue":652,
  "node_id":"IC_kwDOD6Q_ss5G44F4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-07-19T16:37:56Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"cc @jpivarski @douglasdavis \r\n\r\nI worked on the optimization suggested by @douglasdavis and used from_map\r\n\r\nThe issue we discussed in the meet regarding having many iterables of different lengths has been somewhat solved, but @douglasdavis will have to confirm if it is optimal. Apart from that dask-awkward has been added to testing dependencies for Python 3.8 since dask_awkward has a Python version requirement of >=3.8.",
  "created_at":"2022-08-01T15:13:16Z",
  "id":1201335419,
  "issue":652,
  "node_id":"IC_kwDOD6Q_ss5Hmux7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-01T15:13:16Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like upgrading flake8 is not a good idea, at least not yet.\r\n\r\n```\r\nflake8...................................................................Failed\r\n- hook id: flake8\r\n- exit code: 1\r\n\r\nmultiprocessing.pool.RemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\r\n    return list(map(*args))\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/flake8/checker.py\", line 621, in _run_checks\r\n    return checker.run_checks()\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/flake8/checker.py\", line 531, in run_checks\r\n    self.run_ast_checks()\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/flake8/checker.py\", line 435, in run_ast_checks\r\n    for (line_number, offset, text, _) in runner:\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/bugbear.py\", line 61, in run\r\n    if self.should_warn(e.message[:4]):\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/bugbear.py\", line 158, in should_warn\r\n    if code[:i] in self.options.select:\r\nTypeError: argument of type 'NoneType' is not iterable\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/bin/flake8\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/flake8/main/cli.py\", line 22, in main\r\n    app.run(argv)\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/flake8/main/application.py\", line 336, in run\r\n    self._run(argv)\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/flake8/main/application.py\", line 325, in _run\r\n    self.run_checks()\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/flake8/main/application.py\", line 229, in run_checks\r\n    self.file_checker_manager.run()\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/flake8/checker.py\", line 250, in run\r\n    self.run_parallel()\r\n  File \"/pc/clone/G-LGPmdTTlijI9eDY1QUrQ/py_env-python3/lib/python3.10/site-packages/flake8/checker.py\", line 217, in run_parallel\r\n    for ret in pool_map:\r\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 448, in <genexpr>\r\n    return (item for chunk in result for item in chunk)\r\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 870, in next\r\n    raise value\r\nTypeError: argument of type 'NoneType' is not iterable\r\n```",
  "created_at":"2022-08-03T14:07:35Z",
  "id":1203999882,
  "issue":657,
  "node_id":"IC_kwDOD6Q_ss5Hw5SK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-03T14:07:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Updated bugbear to a version that supports flake8 5. Here's the output:\r\n\r\n* B904: Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None`\r\n* B903 Data class should either be immutable or use `__slots__` to save memory.\r\n* B023 Function definition does not bind loop variable\r\n\r\nI'm not a fan of B903, besides giving the wrong suggestion (NamedTuples add a bunch of Tuple API that you usually don't want/need - a `@dataclass(frozen=True)` is much better. Adding slots is fine, though), though it only shows up once, so probably could look into it. Edit: this is triggering on an Exception, so I'd say that's a false positive, and the question is whether to ignore it this one place, or everywhere.\r\n\r\nThe other two are good things to fix, IMO.\r\n\r\n```\r\nsrc/uproot/interpretation/identify.py:437:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:27:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:53:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:78:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:165:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:188:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:208:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:228:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:248:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:265:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/extras.py:282:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/interpretation/library.py:563:17: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/interpretation/library.py:1197:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/interpretation/library.py:1239:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/interpretation/objects.py:583:1: B903 Data class should either be immutable or use __slots__ to save memory. Use collections.namedtuple to generate an immutable class, or enumerate the attributes in a __slot__ declaration in the class to leave attributes mutable.\r\nsrc/uproot/_dask.py:203:38: B023 Function definition does not bind loop variable 'original'.\r\nsrc/uproot/_dask.py:280:36: B023 Function definition does not bind loop variable 'entry_step'.\r\nsrc/uproot/_dask.py:280:48: B023 Function definition does not bind loop variable 'entry_stop'.\r\nsrc/uproot/_dask.py:283:51: B023 Function definition does not bind loop variable 'ttree'.\r\nsrc/uproot/_dask.py:283:58: B023 Function definition does not bind loop variable 'key'.\r\nsrc/uproot/_dask.py:284:37: B023 Function definition does not bind loop variable 'inner_shape'.\r\nsrc/uproot/_dask.py:285:17: B023 Function definition does not bind loop variable 'dask_arrays'.\r\nsrc/uproot/_dask.py:286:71: B023 Function definition does not bind loop variable 'dt'.\r\nsrc/uproot/_dask.py:411:38: B023 Function definition does not bind loop variable 'original'.\r\nsrc/uproot/_dask.py:475:32: B023 Function definition does not bind loop variable 'entry_step'.\r\nsrc/uproot/_dask.py:475:44: B023 Function definition does not bind loop variable 'entry_stop'.\r\nsrc/uproot/_dask.py:476:36: B023 Function definition does not bind loop variable 'i'.\r\nsrc/uproot/writing/_cascadetree.py:146:29: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/writing/_cascadetree.py:170:29: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/source/file.py:38:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/language/python.py:23:9: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/language/python.py:168:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/language/python.py:384:17: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/_util.py:85:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/_util.py:237:44: B023 Function definition does not bind loop variable 'trans'.\r\nsrc/uproot/interpretation/numerical.py:344:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\nsrc/uproot/interpretation/numerical.py:546:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling.  See https://docs.python.org/3/tutorial/errors.html#exception-chaining for details.\r\n```\r\n\r\nDo you want to skip those or fix those?",
  "created_at":"2022-08-18T14:31:26Z",
  "id":1219569805,
  "issue":657,
  "node_id":"IC_kwDOD6Q_ss5IsSiN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-18T14:34:33Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"With one exception, I made B904 do `from err` so that it's explicitly doing what it had been implicitly doing. I find chained exceptions to be useful debugging output. We can decide on a case-by-case basis when to inhibit it with `from None`, but it's better to say, \"turn down the noise\" in the debugging output than it is to say, \"I didn't get the information I needed.\"\r\n\r\nFor B903, I just made the one class immutable. Why not, after all?\r\n\r\nMost of the B023 warnings are not bad cases and I `# noqa: B023`'ed each one. I did the same in Awkward. I'm on the fence about this warning: I can see what it's saying (Python does not have loop block scope, and it's easy to make a function that uses a different value of the loop variable than you intended), but when I check each one of these, it happens to be a safe usage. Maybe it's good to have this warning because\r\n\r\n```python\r\n>>> functions = []\r\n>>> for i in range(10):\r\n...     functions.append(lambda: i)\r\n... \r\n>>> [f() for f in functions]\r\n[9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\r\n```\r\n\r\ncan be very surprising and a hard bug to find and fix. When we've certified that a particular usage is safe (by eye), we can put on a `# noqa: B023`, but not until we've looked.",
  "created_at":"2022-08-18T16:46:32Z",
  "id":1219712303,
  "issue":657,
  "node_id":"IC_kwDOD6Q_ss5Is1Uv",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2022-08-18T16:46:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"By the way, this should be back-ported to `main-v4` and make a new v4 release, not just `main`/v5. It's important enough to warrant a new v4 release.",
  "created_at":"2022-08-03T14:01:14Z",
  "id":1203992173,
  "issue":658,
  "node_id":"IC_kwDOD6Q_ss5Hw3Zt",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-03T14:01:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @kakwok for code",
  "created_at":"2022-08-04T15:04:48Z",
  "id":1205382929,
  "issue":660,
  "node_id":"IC_kwDOD6Q_ss5H2K8R",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-04T15:04:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/663) to add @kakwok! :tada:",
  "created_at":"2022-08-04T15:04:57Z",
  "id":1205383095,
  "issue":660,
  "node_id":"IC_kwDOD6Q_ss5H2K-3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-04T15:04:57Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know why the v4 branch is complaining, whereas the v5 branch is not, and these should (ideally) be handled more centrally than tagging each use\u2014that was probably done on the v5 branch. Nevertheless, we don't plan to develop this branch much more, just bug-fixes, so obfuscating it with \"`# noqa: B023`\" is not a problem here\u2014just don't do it in the `main` (v5) branch!",
  "created_at":"2022-08-04T13:52:54Z",
  "id":1205289653,
  "issue":661,
  "node_id":"IC_kwDOD6Q_ss5H10K1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-04T13:52:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This fix has gone into [Uproot 4.3.4](https://pypi.org/project/uproot/4.3.4/). It's ready to use!",
  "created_at":"2022-08-04T15:30:09Z",
  "id":1205413836,
  "issue":661,
  "node_id":"IC_kwDOD6Q_ss5H2SfM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-04T15:30:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"## Variant (Union) logic\r\n\r\nWe make a field with type `std::variant<std::int32_t,double>`,and actual content: `[1.0, 4, 3, 2, 1]`\r\n\r\nthe field and column records look like the following:\r\n```python\r\n# fields\r\n([MetaData('Field, parent_field_id=0, struct_role=3, field_name='variant_int32_double', type_name='std::variant<std::int32_t,double>', type_alias='', field_desc=''),\r\n  MetaData('Field, parent_field_id=0, struct_role=0, field_name='_0', type_name='std::int32_t', type_alias='', field_desc=''),\r\n  MetaData('Field, parent_field_id=0, struct_role=0, field_name='_1', type_name='double', type_alias='', field_desc='')],\r\n# columns\r\n [MetaData('ColumnRecordFrame', type=3, nbits=64, field_id=0, flags=0),\r\n  MetaData('ColumnRecordFrame', type=11, nbits=32, field_id=1, flags=0),\r\n  MetaData('ColumnRecordFrame', type=7, nbits=64, field_id=2, flags=0)])\r\n```\r\n\r\nLet's focus on the `Switch` column, which has column id 0, treating Switch column's content as `uint64`, we see `[8589934592 4294967296 4294967297 4294967298 4294967299]`, since each number is split into:\r\n>Lower 44 bits like kIndex64, higher 20 bits are a dispatch tag to a column ID\r\n\r\n we look at the raw bits of these five numbers `numpy.unpackbits`:\r\n```python\r\nraw_bits = numpy.unpackbits(D[\"column-0\"].view(numpy.uint8))\r\n\r\n0000000000000000000000000000000000000010000000000000000000000000\r\n0000000000000000000000000000000000000001000000000000000000000000\r\n0000000100000000000000000000000000000001000000000000000000000000\r\n0000001000000000000000000000000000000001000000000000000000000000\r\n0000001100000000000000000000000000000001000000000000000000000000\r\n```\r\n\r\nsince the last four elements in actual content are all `int32`, I would expect the \"higher 20 bits\" to be the same for the last four numbers, but that doesn't make sense, 20 bits from right-hand-side don't make to where \"1\" is.\r\n\r\nideas? @jpivarski \r\n- - -\r\nIf we pretend the split is actually 32bits - 32bits, then we can get reasonable results:\r\n```python\r\nprint(numpy.bitwise_and(D[\"column-0\"], numpy.uint64(0x00000000ffffffff)))\r\nprint(D[\"column-0\"] >> 32)\r\n\r\n# kIndex\r\n[0 0 1 2 3]\r\n# column ids\r\n[2 1 1 1 1]\r\n```",
  "created_at":"2022-08-12T05:42:00Z",
  "id":1212749033,
  "issue":662,
  "node_id":"IC_kwDOD6Q_ss5ISRTp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-12T05:58:57Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"oh, it's was very recently implemented:\r\nhttps://github.com/root-project/root/commit/f4b676688a64cb7e7de7368561cab54a6aaaf1de\r\n\r\n- - -\r\ncan confirm, using 6.27, and we cann see the 20-44 bits split:\r\n```python\r\n        print(numpy.bitwise_and(D[\"column-0\"], numpy.uint64(0x00000000000fffff)))\r\n        print(D[\"column-0\"] >> 44)\r\n[0 0 1 2 3]\r\n[2 1 1 1 1]\r\n```",
  "created_at":"2022-08-12T11:38:27Z",
  "id":1213020902,
  "issue":662,
  "node_id":"IC_kwDOD6Q_ss5ITTrm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-12T15:21:03Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\n>>> import numpy as np\r\n>>> from pprint import pprint\r\n>>> import skhep_testdata\r\n>>> import uproot as up\r\n>>> import awkward as ak\r\n>>> filename = skhep_testdata.data_path(\"test_ntuple_stl_containers.root\")\r\n>>> r = up.open(filename)[\"ntuple\"]\r\n>>> pprint(r.arrays().to_list())\r\n[{'string': 'one',\r\n  'tuple_int32_string': {'_0': 1, '_1': 'one'},\r\n  'variant_int32_float': 1.0,\r\n  'vector_int32': [1],\r\n  'vector_string': ['one'],\r\n  'vector_tuple_int32_string': [{'_0': 1, '_1': 'one'}],\r\n  'vector_variant_int32_float': [1],\r\n  'vector_vector_int32': [[1]],\r\n  'vector_vector_string': [['one']]},\r\n {'string': 'two',\r\n  'tuple_int32_string': {'_0': 2, '_1': 'two'},\r\n  'variant_int32_float': 2.0,\r\n  'vector_int32': [1, 2],\r\n  'vector_string': ['one', 'two'],\r\n  'vector_tuple_int32_string': [{'_0': 1, '_1': 'one'}, {'_0': 2, '_1': 'two'}],\r\n  'vector_variant_int32_float': [1, 2.0],\r\n  'vector_vector_int32': [[1], [2]],\r\n  'vector_vector_string': [['one'], ['two']]}]\r\n```",
  "created_at":"2022-08-12T22:42:52Z",
  "id":1213571198,
  "issue":662,
  "node_id":"IC_kwDOD6Q_ss5IVaB-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-17T20:54:20Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski eh, pre-commit CI is doing some stripping and then flake8 errors on the stripped stuff, what's going on?",
  "created_at":"2022-08-18T13:02:11Z",
  "id":1219466167,
  "issue":662,
  "node_id":"IC_kwDOD6Q_ss5Ir5O3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-18T13:02:11Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Today's meeting will be a good chance to ask @henryiii about that. I've had to hold back pre-commit in #657 because the update is broken; I wonder if this is a partial update? It's definitely an incompatibility between the part that removes unnecessary \"`# noqa`\" and the part that needs those noqas.",
  "created_at":"2022-08-18T13:09:37Z",
  "id":1219473765,
  "issue":662,
  "node_id":"IC_kwDOD6Q_ss5Ir7Fl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-18T13:09:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I wonder if changing the order helps? idk how it determines \"unnecessary\" without flake8 running through it",
  "created_at":"2022-08-18T13:15:18Z",
  "id":1219479630,
  "issue":662,
  "node_id":"IC_kwDOD6Q_ss5Ir8hO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-18T13:15:18Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"@kkothari2001, you'll want to merge `main` (get everything) or `pre-commit-ci-update-config` (just get the flake8-fix) to fix the tests.",
  "created_at":"2022-08-18T17:27:00Z",
  "id":1219748390,
  "issue":679,
  "node_id":"IC_kwDOD6Q_ss5Is-Im",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-18T17:27:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@douglasdavis, you said that you want to subscribe to this PR and check @kkothari2001's usage of `from_map` and high-level graphs.",
  "created_at":"2022-08-18T17:28:22Z",
  "id":1219749756,
  "issue":679,
  "node_id":"IC_kwDOD6Q_ss5Is-d8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-18T17:28:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This looks good as a replacement for the arrays backed by low level graphs. I think there is potential to squash some code duplication, but that would require the answer to my inline question above to be \"yes\" and I just realized one can use `uproot.dask` without having dask-awkward installed, so I think the answer is \"no\" :) So I think this is good, and perhaps there will be a future where we can remove some code duplication if some of the dask-awkward from_map logic can eventually make it into upstream dask.",
  "created_at":"2022-08-18T17:58:30Z",
  "id":1219776857,
  "issue":679,
  "node_id":"IC_kwDOD6Q_ss5ItFFZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-18T17:58:47Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, `dask-awkward` is not on `conda-forge` yet, so it needs to be pip-installed even if one's main installed is conda (and if Dask is installed via conda).",
  "created_at":"2022-08-18T14:40:29Z",
  "id":1219579804,
  "issue":680,
  "node_id":"IC_kwDOD6Q_ss5IsU-c",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-18T14:40:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Replaced by #690.",
  "created_at":"2022-08-24T21:43:34Z",
  "id":1226427663,
  "issue":680,
  "node_id":"IC_kwDOD6Q_ss5JGc0P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T21:43:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"inefficient but works already, this is reading across two clusters\r\n```python\r\n> python -c 'from pprint import pprint; import uproot as up; import \r\nskhep_testdata; filename = skhep_testdata.data_path(\"test_ntuple_large_bit_int64.root\"); r = up.open(filena\r\nme)[\"ntuple\"]; pprint(r.arrays(entry_start=1, entry_stop=11111113)[-10:].to_list())'\r\n[{'one_bit': False, 'two_int64': 11111104},\r\n {'one_bit': True, 'two_int64': 11111105},\r\n {'one_bit': False, 'two_int64': 11111106},\r\n {'one_bit': False, 'two_int64': 11111107},\r\n {'one_bit': True, 'two_int64': 11111108},\r\n {'one_bit': True, 'two_int64': 11111109},\r\n {'one_bit': False, 'two_int64': 11111110},\r\n {'one_bit': False, 'two_int64': 11111111},\r\n {'one_bit': True, 'two_int64': 11111112},\r\n {'one_bit': True, 'two_int64': 11111113}]\r\n```",
  "created_at":"2022-08-19T05:18:50Z",
  "id":1220262997,
  "issue":682,
  "node_id":"IC_kwDOD6Q_ss5Iu7xV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-19T05:19:01Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\n> python -c 'from pprint import pprint; import uproot as up; import skhep_testdata; f\r\nilename = skhep_testdata.data_path(\"test_ntuple_large_bit_int64.root\"); r = up.open(filename)[\"ntuple\"]; pprint(r.arrays(ent\r\nry_start=0, entry_stop=50).to_list())'\r\n[{'one_bit': True, 'two_int64': 0},\r\n {'one_bit': False, 'two_int64': 1},\r\n {'one_bit': False, 'two_int64': 2},\r\n {'one_bit': False, 'two_int64': 3},\r\n {'one_bit': False, 'two_int64': 4},\r\n {'one_bit': True, 'two_int64': 5},\r\n {'one_bit': False, 'two_int64': 6},\r\n {'one_bit': False, 'two_int64': 7},\r\n {'one_bit': False, 'two_int64': 8},\r\n {'one_bit': False, 'two_int64': 9},\r\n {'one_bit': True, 'two_int64': 10},\r\n {'one_bit': False, 'two_int64': 11},\r\n```\r\n\r\nturns out I had one extra `unpackbit` at a different location due to wrong design decision made earlier, now with `bitorder = \"little\"` it works flawlessly",
  "created_at":"2022-08-20T03:30:57Z",
  "id":1221222954,
  "issue":682,
  "node_id":"IC_kwDOD6Q_ss5IymIq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-20T03:30:57Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski ready in the sense that with this:\r\n1. we can read bits column now\r\n2. we can also read multiple clusters\r\n\r\none \"problem\" is we don't have test files for `2.` because it would be at least O(10) MB, we forgot we ask ROOT ppl how top make small cluster limit",
  "created_at":"2022-08-20T05:00:18Z",
  "id":1221233287,
  "issue":682,
  "node_id":"IC_kwDOD6Q_ss5IyoqH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-20T05:00:18Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and one more thing: after we drop Python 3.6, the minimum version of NumPy becomes 1.16.5, which is still too early to assume that [np.unpackbits](https://numpy.org/doc/stable/reference/generated/numpy.unpackbits.html) has a `bitorder` argument.\r\n\r\nFortunately, there's a cool hack:\r\n\r\n```python\r\n>>> # the default\r\n>>> np.unpackbits(np.array([123, 71], \"u1\"))\r\narray([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1], dtype=uint8)\r\n\r\n>>> # is the same is bitorder=\"b\", which we don't want\r\n>>> np.unpackbits(np.array([123, 71], \"u1\"), bitorder=\"b\")\r\narray([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1], dtype=uint8)\r\n\r\n>>> # we want bitorder=\"l\"\r\n>>> np.unpackbits(np.array([123, 71], \"u1\"), bitorder=\"l\")\r\narray([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0], dtype=uint8)\r\n\r\n>>> # so don't specify a bitorder (\"b\") and rotate in groups of 8:\r\n>>> np.unpackbits(np.array([123, 71], \"u1\")).reshape(-1, 8)[:, ::-1].reshape(-1)\r\narray([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0], dtype=uint8)\r\n```\r\n\r\nThe `bitorder` flag might be faster (haven't checked), but the reshape-slice-reshape method will always work.",
  "created_at":"2022-08-20T16:54:06Z",
  "id":1221366124,
  "issue":682,
  "node_id":"IC_kwDOD6Q_ss5IzJFs",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-08-20T16:54:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski addressed the variable name style, dtype and bitorder comment",
  "created_at":"2022-08-20T22:34:07Z",
  "id":1221419782,
  "issue":682,
  "node_id":"IC_kwDOD6Q_ss5IzWMG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-21T03:28:03Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"`tests/test_0637-setup-tests-for-AwkwardForth.py::test_03` timed out (after 6 hours) on Python 3.8 & 3.10, but not 3.9, on Windows. Is this a known flake? Shouldn't this be time capped via pytest-timeout?",
  "created_at":"2022-08-24T13:24:19Z",
  "id":1225722455,
  "issue":683,
  "node_id":"IC_kwDOD6Q_ss5JDwpX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T13:24:19Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Every check is under 10 minutes now. It used to take more than that just to install ROOT. :)",
  "created_at":"2022-08-24T13:45:11Z",
  "id":1225748935,
  "issue":683,
  "node_id":"IC_kwDOD6Q_ss5JD3HH",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2022-08-24T13:45:11Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"@ianna, this is a good model to look at for installing ROOT in CI. (For the ROOT CI job in Awkward for testing your RDataFrame bridge.)",
  "created_at":"2022-08-24T19:47:59Z",
  "id":1226183909,
  "issue":683,
  "node_id":"IC_kwDOD6Q_ss5JFhTl",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-08-24T19:47:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"For the sake of cross-referencing, this seems to have the same goal as https://github.com/scikit-hep/awkward/pull/1562, but it's less complicated.",
  "created_at":"2022-08-24T19:30:46Z",
  "id":1226153046,
  "issue":684,
  "node_id":"IC_kwDOD6Q_ss5JFZxW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T19:30:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I have a sneaky objective in this one, unlike https://github.com/scikit-hep/awkward/pull/1562 where I'm completely honest and above board. ;)",
  "created_at":"2022-08-24T19:55:51Z",
  "id":1226201254,
  "issue":684,
  "node_id":"IC_kwDOD6Q_ss5JFlim",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T19:55:51Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Dropping flake8 on the reasoning that it's in pre-commit and doesn't need to be in `[test]`.",
  "created_at":"2022-08-24T18:40:42Z",
  "id":1226096065,
  "issue":686,
  "node_id":"IC_kwDOD6Q_ss5JFL3B",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-08-24T18:40:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As soon as this passes, I'll merge it.",
  "created_at":"2022-08-24T18:42:17Z",
  "id":1226098333,
  "issue":686,
  "node_id":"IC_kwDOD6Q_ss5JFMad",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-08-24T18:42:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll rebase my PRs after that, since this overlaps with them.",
  "created_at":"2022-08-24T18:43:11Z",
  "id":1226100134,
  "issue":686,
  "node_id":"IC_kwDOD6Q_ss5JFM2m",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T18:43:11Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Observation: only \"(ubuntu-latest, 3.8)\" is running the ROOT tests, but that's okay. We only need one.\r\n\r\nhttps://github.com/scikit-hep/uproot5/runs/8001891523?check_suite_focus=true\r\n\r\n```\r\n=========================== short test summary info ============================\r\nSKIPPED [1] tests/test_0001-source-class.py:96: RECHECK: example.com is flaky, too\r\nSKIPPED [1] tests/test_0001-source-class.py:135: RECHECK: example.com is flaky, too\r\nSKIPPED [1] tests/test_0001-source-class.py:251: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0001-source-class.py:272: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0001-source-class.py:296: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0001-source-class.py:318: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0001-source-class.py:338: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0001-source-class.py:380: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0001-source-class.py:407: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0006-notify-when-downloaded.py:65: RECHECK: example.com is flaky, too\r\nSKIPPED [1] tests/test_0006-notify-when-downloaded.py:81: RECHECK: example.com is flaky, too\r\nSKIPPED [1] tests/test_0006-notify-when-downloaded.py:97: RECHECK: example.com is flaky, too\r\nSKIPPED [1] tests/test_0006-notify-when-downloaded.py:147: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0006-notify-when-downloaded.py:169: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0006-notify-when-downloaded.py:191: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0007-single-chunk-interface.py:76: RECHECK: example.com is flaky, too\r\nSKIPPED [1] tests/test_0007-single-chunk-interface.py:100: RECHECK: example.com is flaky, too\r\nSKIPPED [1] tests/test_0007-single-chunk-interface.py:120: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0007-single-chunk-interface.py:141: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0007-single-chunk-interface.py:162: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0031-test-stl-containers.py:17: Implement non-memberwise std::map; we have a sample (map<string,double>)\r\nSKIPPED [1] tests/test_0167-use-the-common-histogram-interface.py:134: Something's wrong with uproot-issue33.root and boost-histogram\r\nSKIPPED [1] tests/test_0302-pickle.py:56: RECHECK: Run2012B_DoubleMuParked.root is super-flaky right now\r\nSKIPPED [1] tests/test_0325-fix-windows-file-uris.py:10: Drive letters only parsed on Windows.\r\nSKIPPED [1] tests/test_0416-writing-compressed-data.py:61: could not import 'zstandard': No module named 'zstandard'\r\nSKIPPED [1] tests/test_0416-writing-compressed-data.py:150: could not import 'zstandard': No module named 'zstandard'\r\nSKIPPED [1] tests/test_0416-writing-compressed-data.py:242: could not import 'zstandard': No module named 'zstandard'\r\nSKIPPED [1] tests/test_0416-writing-compressed-data.py:336: could not import 'zstandard': No module named 'zstandard'\r\n================== 554 passed, 28 skipped in 92.77s (0:01:32) ==================\r\n```",
  "created_at":"2022-08-24T19:13:13Z",
  "id":1226134187,
  "issue":686,
  "node_id":"IC_kwDOD6Q_ss5JFVKr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T19:13:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Another observation: all/only the Linuxes test XRootD, and the only one that doesn't test `zstandard` decompression is the one that tests ROOT. Which may be because of some conda/pip overshadowing. But I guess that's okay, as `zstandard`-reading is tested elsewhere.",
  "created_at":"2022-08-24T19:17:36Z",
  "id":1226139241,
  "issue":686,
  "node_id":"IC_kwDOD6Q_ss5JFWZp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T19:17:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is done.",
  "created_at":"2022-10-31T20:51:30Z",
  "id":1297670463,
  "issue":687,
  "node_id":"IC_kwDOD6Q_ss5NWOE_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-31T20:51:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(This was my secret motivation for #684)",
  "created_at":"2022-08-24T20:48:58Z",
  "id":1226310965,
  "issue":688,
  "node_id":"IC_kwDOD6Q_ss5JGAU1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T20:48:58Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I see! Is this done; was it that easy?",
  "created_at":"2022-08-24T20:54:44Z",
  "id":1226323098,
  "issue":688,
  "node_id":"IC_kwDOD6Q_ss5JGDSa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T20:54:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"\ud83e\udd37 All I could think of, at least. The only other thing is the conda-forge recipe will need an update after this gets released.",
  "created_at":"2022-08-24T21:04:38Z",
  "id":1226343621,
  "issue":688,
  "node_id":"IC_kwDOD6Q_ss5JGITF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T21:04:38Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Then perhaps I should include this and make an Uproot release. From `main`, I would only be releasing a v5 pre-release; maybe that doesn't count for what conda-forge picks up. (I've noticed that it doesn't follow pre-releases.)",
  "created_at":"2022-08-24T21:08:27Z",
  "id":1226351545,
  "issue":688,
  "node_id":"IC_kwDOD6Q_ss5JGKO5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-24T21:08:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is looking good. But how will an fsspec-capable Source be enabled? The other Sources are identified by URI scheme, but a URI with `https://` or `root://` could be handled by either FSSpecSource or the non-fsspec ones. (`s3://` would be unambiguous.)\r\n\r\nThe backend that runs can't be determined by what packages are installed (i.e. attempt to `import fsspec` and then run different code, depending on whether it raises MissingModuleException) because that would make debugging user issues more complicated.\r\n\r\nIf it's opt-in (only used if FSSpecSource is explicitly selected in `uproot.open`), then there will be people who would have wanted to use it that never find out about it.\r\n\r\nIf it's something that Coffea or coffea-casa triggers when the time is right, that's fine.",
  "created_at":"2022-08-25T19:10:40Z",
  "id":1227657240,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JLJAY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-25T19:10:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"One option is to simply make uproot depend on fsspec. Then fsspec is responsible for picking the right implementation for a given protocol. (we could provide an override option) Of course, this means that we have to make sure the fsspec http and xrootd implementations are as robust and performant as the current implementations. This is something @ScottDemarest was interested in looking into.\r\n\r\nI know one potential (possibly premature) optimization is to notify downstream executors as soon as some portion of data is available. We can do this within the constraints of the fsspec public API by using async filesystem implementation with an event loop created and held as a resource by uproot (essentially copying the sync wrapper implementation that fsspec uses internally). If we were to do that, it would also be nice to drop the custom executor and futures implementation here and adopt the standard library one, now that python 2 is not supported.",
  "created_at":"2022-08-25T19:24:03Z",
  "id":1227669248,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JLL8A",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-25T19:24:03Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> If we want uproot to be \"batteries included\" we should add these to the regular install requirements if we adopt FSSpec as the standard source.\r\n\r\nI get pulled in both directions about whether to include batteries or not.\r\n\r\nOn the one hand,\r\n\r\n  * \"Easy install\" was the top reason that people gave for why they use Uproot in the first place. Adding dependencies jeopardizes that, though the case against fsspec is overstated. fsspec is a very easy dependency, and it's pure Python and everything.\r\n  * @agoose77 is WASM-building Awkward for use in Thebe-powered tutorials. We need to keep dependencies minimal for the web (mostly pure Python and not too large to download). Again, using this as a case against fsspec, even if we decide to do a similar in-browser tutorial for Uproot (haven't talked about it), would be overstating it. fsspec is small (141 kB).\r\n\r\nOn the other hand,\r\n\r\n  * It's annoying to install a package, try to use it, and immediately be told that you have to install another package. It's even worse if you aren't using the package interactively (or directly: it's a hidden dependency of the one you are using). It could even be a multi-pass cycle of install, try to use, install more, try to use again, install yet more.\r\n  * The Python world has well-established dependency resolution tools, but optional dependencies require us to effectively reimplement that, including minimum versions.\r\n  * Some of these dependencies implement rather core features. In particular, I'm recently swayed by the fact that we're now saying Awkward's primary on-disk format is Parquet, but to use Parquet you have to install pyarrow (and fsspec). I'm only being held back on that one by the WASM target: pyarrow is huge and a monster to compile.\r\n\r\nCurrently, the PyPI versions of Uproot and Awkward Array have minimal dependencies, while the conda-forge recipes are batteries included. We've also considered\r\n\r\n```bash\r\npip install uproot[complete]   # and\r\npip install awkward[complete]\r\n```\r\n\r\nbut then the `[complete]` labels would basically be synonyms of `[dev]` and `[test]`. I don't know what should be in one label and not the others (except maybe `[test]` should include pytest, the others not).\r\n\r\n-------------------------\r\n\r\nEnlarging the scope of this, and perhaps answering my previous question: how about if fsspec-enabled remote access _replaces_ the current HTTPSource and XRootDSource?\r\n\r\nThe HTTPSource was written using only the Python standard library in an extremely dependency-averse mood. It should have been based on requests. (I wasn't familiar with how basic that is to the ecosystem, at least on par with NumPy.) I think we would have had fewer issues with HTTP redirects if we had gone this route from the start.\r\n\r\nI have had _so much_ help from user-developers fixing up the XRootDSource, which I'm grateful for, but it indicates how tricky it is. By now, there's a lot of code in XRootDSource that I don't understand. If that was used as a model to develop xrootd-fsspec, then maybe we should discontinue this implementation in favor of that one.\r\n\r\nThe story for users would be simpler: \"If you want to access remote files (any non-file URI), you need fsspec (or install `uproot[complete]`).\" And then that comes with its optional dependencies, like s3fs. Then Uproot is out of the business of remote access and relies on the same remote access libraries as everybody else.\r\n\r\nWe could make this part of the Uproot 4 \u2192 5 switch. What do you think?\r\n\r\n(Our comments crossed. It sounds like we have similar thoughts on this.)",
  "created_at":"2022-08-25T19:36:37Z",
  "id":1227682667,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JLPNr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-25T19:36:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> The story for users would be simpler: \"If you want to access remote files (any non-file URI), you need fsspec (or install `uproot[complete]`).\"\r\n\r\n> One option is to simply make uproot depend on fsspec.\r\n\r\nYes, Uproot could strictly depend on fsspec, since that's pure Python and small. I don't know if this slippery slope extends to lz4 and xxhash (not pure Python, but small), since LZ4 is a not-uncommon ROOT compression setting. Or ZStandard. Maybe it does, since these are things a user might not even know they need: they just open a TTree and it fails. Dependencies for extra user-visible features like hist and Dask are different, so the slippery slope does stop at some point.",
  "created_at":"2022-08-25T19:57:44Z",
  "id":1227699476,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JLTUU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-25T19:57:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I do sometimes wonder if \"easy to install\" means simply that pip worked, or that the user was happy to see that pip did not bring in more than one package. But that's a bit beside the point\r\n\r\nIt should be noted that fsspec is lightweight because it is very much **not** batteries-included. For example, in a fresh environment, if you try to `pip install fsspec` and then run\r\n```python\r\nwith fsspec.open(\"http://google.com\") as f:\r\n    print(f.read(10))\r\n```\r\nyou will get an error:\r\n```\r\nImportError: HTTPFileSystem requires \"requests\" and \"aiohttp\" to be installed\r\n```\r\nand if you go install those, then you end up with 12  packages!\r\n```\r\nInstalling collected packages: urllib3, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, aiosignal, aiohttp\r\nSuccessfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 attrs-22.1.0 certifi-2022.6.15 charset-normalizer-2.1.1 frozenlist-1.3.1 idna-3.3 multidict-6.0.2 requests-2.28.1 urllib3-1.26.12 yarl-1.8.1\r\n```\r\n\r\nThe same goes for fsspec-xrootd, but doubly-so: the first error will ask you to install `fsspec-xrootd`, then immediately afterwards it will again ask you to figure out how to install `xrootd` (because it still is not pip installable). In that spirit, I would say adding `fsspec` and `fsspec-xrootd` to the explicit dependencies of uproot is OK because they are very lightweight and themselves will ask the user to install further packages.",
  "created_at":"2022-08-25T20:08:40Z",
  "id":1227709118,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JLVq-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-25T20:08:40Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> I do sometimes wonder if \"easy to install\" means simply that pip worked, or that the user was happy to see that pip did not bring in more than one package.\r\n\r\nI think that's true, so having a tree of transitive dependencies is not off the table. (Earlier, I had been considering cases of a hard-to-access DAQ-or-whatever, without outside network/pip access, but not anymore. There are [freezing options](https://docs.python-guide.org/shipping/freezing/).)\r\n\r\nIt is necessary for all of those transitive dependencies to successfully install, however. Some cases of [lz4 installation](https://pypi.org/project/lz4/#files) have broken because they assume liblz4.so is available on the OS somewhere. I don't know if that's still true.\r\n\r\nHaving Uproot strictly depend on fsspec, and then fsspec asks the user to install requests and aiohttp, would still be an improvement over not having Uproot strictly depend on fsspec, since it would only be one round of telling the user to go install something else, rather than two rounds. I can imagine that getting very annoying.\r\n\r\n**Edit:** Got it: you said all of that, but include both fsspec and fsspec-xrootd, since they're at the same level.",
  "created_at":"2022-08-25T20:25:14Z",
  "id":1227724988,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JLZi8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-25T20:26:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I'm personally +1 on making fsspec a dependency:\r\n- IO is very important in uproot\r\n- fsspec is lightweight RE dependencies\r\n- fsspec is near \"standard\" for IO in Python ecosystem\r\n- we can drop code :partying_face: ",
  "created_at":"2022-08-25T20:46:39Z",
  "id":1227746473,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JLeyp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-25T20:46:52Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Hello @jpivarski, allow me a quick comment - these are important matters and something that IMO you should raise more widely, nice with org admins and most usefully with users via Gitter/... I would be a bit scared myself if you were going to *by default*, meaning for standard releases, to increase significantly the dependencies. Indeed, as you said, the success of uproot has been \"small and easy\". I won't go into big thoughts here, so making a single one at this point: ask yourself the percentage of (the very large number of) uproot users who actually need ffspec ... or have even just heard of it! Adding something that brings in effectively an order magnitude more dependencies for a permil user base seems awkward.",
  "created_at":"2022-08-26T07:53:11Z",
  "id":1228174416,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JNHRQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-26T07:56:48Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"To be clear, adding fsspec as a dependency will bring just 1 new package in by default (namely, fsspec) What one then needs to do is bring in additional packages depending on their remote IO needs, all in an opt-in fashion.\r\nBut this is not really so much about users wanting fsspec: this is a library that makes uproot development easier by abstracting storage. With this dependency, it would be much easier to, for example, implement the much-desired glob feature, or to add headers (token auth!) to http IO, etc. And we would not be rolling our own http, as the standard library is not nearly sufficient to handle everything, case in point #121 #176 etc.",
  "created_at":"2022-08-26T14:56:04Z",
  "id":1228598878,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JOu5e",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-26T14:56:04Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Of course we still should solicit a lot more feedback on this. @chrisburr in particular may have thoughts",
  "created_at":"2022-08-26T14:57:26Z",
  "id":1228600399,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JOvRP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-26T14:57:26Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Remaining test failures seem unrelated?\r\n```\r\nFAILED tests/test_0652_dask-for-awkward.py::test_dask_concatenation - TypeError: could not convert data into an ak.Array\r\nFAILED tests/test_0652_dask-for-awkward.py::test_delay_open - TypeError: could not convert data into an ak.Array\r\n```",
  "created_at":"2022-08-26T15:19:45Z",
  "id":1228627836,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JO198",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-26T15:19:45Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"@kkothari2001, are the failing Dask tests ones that require a particular version of dask-awkward, and should therefore be protected by `pytest.importorskip` or maybe `pytest.mark.skipif(dask_awkward.version)`?",
  "created_at":"2022-08-26T15:56:24Z",
  "id":1228667153,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JO_kR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-26T15:56:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I had a look, it is breaking due to either changes made in dask-awkward code itself, or from updating to awkward `v1.9.0rc5` to `v1.9.0rc10` in commit `7b5ca3` in dask-awkward.\r\n\r\nOn my computer, I tried backporting to dask-awkward version `2022.7a0` it passes, ig for now we can work with that, then figure the reason for the error.\r\n\r\n",
  "created_at":"2022-08-26T16:47:55Z",
  "id":1228714181,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JPLDF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-26T16:47:55Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"On second thought, there is known to be some discrepancy between my personal dev setup and the PR tests (because I've been installing the latest dask-awkward/main) let's just skip the tests with now with `pytest.mark.skip` and I'll resolve this issue in my currently open PR #679.",
  "created_at":"2022-08-26T17:24:03Z",
  "id":1228743813,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JPSSF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-26T17:24:03Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"The failing test can be fixed by merging in https://github.com/scikit-hep/uproot5/pull/694.",
  "created_at":"2022-08-29T17:39:12Z",
  "id":1230638294,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5JWgzW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-08-29T17:39:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"We have not published fsspec for python 3.6. Should we do this or will uproot soon drop support for 3.6?",
  "created_at":"2022-09-26T19:45:04Z",
  "id":1258534377,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5LA7Xp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-26T19:45:04Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Uproot will drop support for Python 3.6: #687. Only inertia has kept this from happening so far: if it's the slightest bit easier for the fsspec source, that would definitely push it over the edge.\r\n\r\nThen we're getting on a schedule of dropping Python versions when they're end-of-lifed (can't find the GitHub reference where that was stated, but it's the plan).",
  "created_at":"2022-09-26T19:56:41Z",
  "id":1258546580,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5LA-WU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-26T19:56:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith-, I'm marking this PR as \"inactive.\" I would like to follow up on it someday, though it may get to the point where you'd want to re-apply these changes to a new PR, branched off of `main`.",
  "created_at":"2022-11-28T19:08:04Z",
  "id":1329609045,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5PQDlV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-28T19:08:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It's this section that would need modifying to consider other array formats:\r\nhttps://github.com/scikit-hep/uproot5/blob/92e91f31976127ab5418abba95380ccc31ad4d8a/src/uproot/writing/identify.py#L192-L203\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/92e91f31976127ab5418abba95380ccc31ad4d8a/src/uproot/writing/identify.py#L422-L642",
  "created_at":"2022-08-30T07:28:17Z",
  "id":1231258601,
  "issue":695,
  "node_id":"IC_kwDOD6Q_ss5JY4Pp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T20:13:04Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski and I discussed this in a meeting, and concluded that this feature request is probably out of scope for uproot.\r\n\r\nThe tuple interface for writing hists is used to support numpy histograms (which return the counts and edges as a tuple). Extending this to support non-Numpy arrays is therefore not a goal, rather the user would be encouraged to load their data with Numpy (which is a dependency of uproot to begin with), or use something like `hist` / `boost-histogram`.\r\n\r\nThanks for the FR Jerry!",
  "created_at":"2022-09-21T14:33:54Z",
  "id":1253801319,
  "issue":695,
  "node_id":"IC_kwDOD6Q_ss5Ku31n",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-21T14:33:54Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"it just appears to be weird that:\r\n```python\r\na = oldtree.arrays() #say this is an awkward array\r\nwith up.recreate(\"blah.root\") as file:\r\n    file[\"newtree\"] = a\r\n```\r\ndoesn't work.\r\n\r\n\r\nI understand and agree we don't care about histogram due to python's type system limitation (i.e. too much manual work and two-tuple of `any` interface is already taken somewhat)",
  "created_at":"2022-09-21T15:53:28Z",
  "id":1253903452,
  "issue":695,
  "node_id":"IC_kwDOD6Q_ss5KvQxc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-21T15:55:09Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"To provide a little more context, the primary, supported interface for putting histograms in a file is for the right-hand-side to be an instance of a recognized histogram library, such as boost-histogram or hist. (Earlier, I included physt in that, but the field of histogram libraries in Python has narrowed, at least in HEP.) This is the forward-looking way to do things, since adding more metadata or data (such as `fSumw2`) is something we can do indefinitely with named types like `hist.Hist`.\r\n\r\nThe concession of accepting a tuple of arrays is because this is what `np.histogram` returns, and it's been doing so for long enough that Matplotlib and probably others (including `hist.numpy.histogram`...) also return. It's a convention, rather than a named type, but widely recognized and\r\n\r\n```python\r\nwith up.recreate(\"blah.root\") as file:\r\n    file[\"newtree\"] = np.histogram(...)\r\n```\r\n\r\nis a convenient thing to do, so we'll play along.\r\n\r\nHowever, it's not a convention we invented, we don't have control over it, and therefore we can't extend it. We can't extend it to include the equivalent of `fSumw2` because `np.histogram` would not give us that data. After a user provides `weights=` to `np.histogram` and it returns a 2-tuple, knowledge about the weights is gone. Circumventing it, like\r\n\r\n```python\r\nwith up.recreate(\"blah.root\") as file:\r\n    file[\"newtree\"] = np.histogram(..., weights=my_weights) + (weights_to_sumw2(..., my_weights),)\r\n```\r\n\r\nis no longer convenient. Instead of implementing that `weights_to_sumw2`, which is inconvenient to use and has to agree in binning with `np.histogram`, we should just use a histogram library. (Also, the particular extension above wouldn't work because `np.histogramdd` returns a 3-tuple for 2D histograms.)\r\n\r\nAnother way to extend NumPy's convention is to allow non-NumPy arrays, such as `array.array` or Awkward Arrays. I don't think it makes much sense to do this, either, because the widely accepted convention takes a tuple of NumPy arrays: NumPy, Matplotlib, and hist.numpy all return tuples of `np.ndarray`. Python's builtin `array.array` is too infrequently used to be relevant, and I'd be very surprised if anything returns histogram contents and edges as Awkward Arrays.\r\n\r\nOne possible exception: I just noticed that CuPy's `cp.histogram` returns CuPy arrays:\r\n\r\n```python\r\n>>> import cupy as cp\r\n>>> bin_contents, bin_edges = cp.histogram(cp.array([1, 2, 3, 4, 5.5]))\r\n>>> type(bin_contents), type(bin_edges)\r\n(cupy._core.core.ndarray, cupy._core.core.ndarray)\r\n```\r\n\r\nso maybe this one should be recognized. But it's a short list. Okay, also PyTorch:\r\n\r\n```python\r\n>>> import torch\r\n>>> bin_contents, bin_edges = torch.histogram(torch.tensor([1, 2, 3, 4, 5.5]))\r\n>>> type(bin_contents), type(bin_edges)\r\n(torch.Tensor, torch.Tensor)\r\n```\r\n\r\nThese have special types because they support GPUs (probably also TensorFlow and JAX).\r\n\r\nBut if we do recognize arrays from these libraries, we can do it by checking\r\n\r\n```python\r\nhasattr(array_like, \"dtype\") and hasattr(array_like, \"shape\")\r\n```\r\n\r\nwhich includes all of the NumPy array-like libraries, which we can call `np.asarray(array_like)` on, and it does not include Python's `array.array` or Awkward Arrays.",
  "created_at":"2022-09-21T17:43:00Z",
  "id":1254027526,
  "issue":695,
  "node_id":"IC_kwDOD6Q_ss5KvvEG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-21T17:43:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> However, it's not a convention we invented, we don't have control over it, and therefore we can't extend it. \r\n\r\nJust a small note, we _can_ extend it - we don't require that anyone else support our extension, but it starts to place more importance on this unstructured interface than we'd ideally want.",
  "created_at":"2022-09-21T17:55:13Z",
  "id":1254039971,
  "issue":695,
  "node_id":"IC_kwDOD6Q_ss5KvyGj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-21T17:55:13Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I meant \"can\" in a socially responsible sense.  `:)`",
  "created_at":"2022-09-21T18:10:01Z",
  "id":1254057445,
  "issue":695,
  "node_id":"IC_kwDOD6Q_ss5Kv2Xl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-21T18:10:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"sorry, again, I'm not talking about putting histogram into WritableDirectory anymore sorry for the confusion.\r\n\r\nThe original post used histogram as example and I regret that because it's somewhat a special case.\r\n\r\nThe real-world problem that triggered this [FR] was that I was trying to clean-up a \"corrupted\" .root files (one with un-finished TBasket in the middle of the file), and I realized I can't just:\r\n```python\r\nnewfile[\"mytree\"] = oldfile[\"mytree\"].arrays()\r\n```",
  "created_at":"2022-09-21T18:49:14Z",
  "id":1254097032,
  "issue":695,
  "node_id":"IC_kwDOD6Q_ss5KwACI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-21T18:49:14Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh! You're talking about making a TTree. It recognizes a dict of Awkward Arrays, so you can do\r\n\r\n```python\r\nnewfile[\"mytree\"] = oldfile[\"mytree\"].arrays(how=dict)\r\n```\r\n\r\nbut I admit that's un-guessable.\r\n\r\nThe code that recognizes the right-hand side of `=` as a thing that can be a TTree is already a bit more complicated than I'd like, but I suppose it could be made to recognize a single Awkward record array in addition to a dict of arrays.",
  "created_at":"2022-09-21T18:57:35Z",
  "id":1254104456,
  "issue":695,
  "node_id":"IC_kwDOD6Q_ss5KwB2I",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-09-21T18:57:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski can we even write in the sumw2 information from `uproot`? because that's needed, and right now it's wrong\r\n```julia\r\npywith(up.recreate(\"/tmp/example.root\")) do file\r\n        file[\"shit\"] = np.array([-1,1,2]), np.array([0,1,2,3])\r\n    end\r\n\r\n#reading it back\r\nfSumw2 => [0.0, -1.0, 1.0, 2.0, 0.0]\r\n```",
  "created_at":"2022-09-01T22:04:41Z",
  "id":1234830348,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5JmgQM",
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "total_count":1
  },
  "updated_at":"2022-09-01T22:05:45Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"The 2-tuple of arrays is recognized as a histogram with this:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/49d40bd623b4b2ca19369ae07f9756e65dc6d122/src/uproot/writing/identify.py#L420-L427\r\n\r\nand this is the TH1 that it creates:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/49d40bd623b4b2ca19369ae07f9756e65dc6d122/src/uproot/writing/identify.py#L453-L471\r\n\r\nThe `data` and the `fSumw2` are deliberately set to the same input. We can see the conversion without having to actually write to a file using [uproot.to_writable](https://uproot.readthedocs.io/en/latest/uproot.writing.identify.to_writable.html):\r\n\r\n```python\r\n>>> uproot.to_writable((np.asarray([-3, -2, -1]), np.asarray([0, 1, 2, 3])))\r\n<TH1D (version 3) at 0x7eff8030c730>\r\n```\r\n\r\nand that writable has a JSON form, so we can look directly at the `fSumw2` that it makes:\r\n\r\n```python\r\n>>> uproot.to_writable((np.asarray([-3, -2, -1]), np.asarray([0, 1, 2, 3]))).tojson()[\"fSumw2\"]\r\n[0.0, -3.0, -2.0, -1.0, 0.0]\r\n```\r\n\r\nand we can even go full round trip to PyROOT with [uproot.pyroot.to_pyroot](https://uproot.readthedocs.io/en/latest/uproot.pyroot.to_pyroot.html):\r\n\r\n```python\r\n>>> uproot.pyroot.to_pyroot(uproot.to_writable((np.asarray([-3, -2, +1]), np.asarray([0, 1, 2, 3]))))\r\n<cppyy.gbl.TH1D object at 0x55f83cd0e580>\r\n```\r\n\r\nand look at its JSON serialization with [TBufferJSON.ToJSON](https://root.cern/doc/master/classTBufferJSON.html):\r\n\r\n```python\r\n>>> json.loads(\r\n...     str(\r\n...         ROOT.TBufferJSON.ToJSON(\r\n...             uproot.pyroot.to_pyroot(\r\n...                 uproot.to_writable((np.asarray([-1, -2, -3]), np.asarray([0, 1, 2, 3])))\r\n...             )\r\n...         )\r\n...     )\r\n... )[\"fSumw2\"]\r\n[0, -1, -2, -3, 0]\r\n```\r\n\r\nBut okay, maybe `fSumw2` shouldn't be identical to `data`? Maybe it should be the absolute value?",
  "created_at":"2022-09-01T22:30:41Z",
  "id":1234852738,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5JmluC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T22:32:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`fSumw2` represents the _square_ of the value? I had been misinterpreting that.\r\n\r\n```python\r\n>>> h = ROOT.TH1F(\"h\", \"\", 3, 0.0, 3.0)\r\n>>> h.Fill(0.5, -3.0)\r\n1\r\n>>> h.Fill(1.5, -2.0)\r\n2\r\n>>> h.Fill(2.5, -1.0)\r\n3\r\n>>> json.loads(str(ROOT.TBufferJSON.ToJSON(h)))[\"fSumw2\"]\r\n[0, 9, 4, 1, 0]\r\n```",
  "created_at":"2022-09-01T22:36:51Z",
  "id":1234856656,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5JmmrQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T22:36:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"No, that's not quite it. Here's another histogram with the same values but different `fSumw2`.\r\n\r\n```python\r\n>>> h = ROOT.TH1F(\"h\", \"\", 3, 0.0, 3.0)\r\n>>> h.Fill(0.5, -1.0); h.Fill(0.5, -1.0); h.Fill(0.5, -1.0)\r\n1\r\n1\r\n1\r\n>>> h.Fill(1.5, -1.0); h.Fill(1.5, -1.0)\r\n2\r\n2\r\n>>> h.Fill(2.5, -1.0)\r\n3\r\n>>> json.loads(str(ROOT.TBufferJSON.ToJSON(h)))[\"fSumw2\"]\r\n[0, 3, 2, 1, 0]\r\n```\r\n\r\nThis is looking like information that you can't determine from the values `np.histogram` gives you. These two ways of filling `h` result in the same values but different `fSumw2`.",
  "created_at":"2022-09-01T22:40:14Z",
  "id":1234858698,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5JmnLK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T22:40:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"My assumption had been that [np.histogram](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html) was called without a `weights` argument. After the histogram has been filled and we're left with only the 2-tuple of arrays, we can't know what the `weights` had been, if anything.",
  "created_at":"2022-09-01T22:42:33Z",
  "id":1234860236,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5JmnjM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T22:42:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It's the sum of weight^2 of all the events ever filled into that bin, it's critical for doing bin errors correctly (and when you do calculations with histograms)",
  "created_at":"2022-09-01T22:43:21Z",
  "id":1234860740,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5JmnrE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T22:43:55Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"We can default to assuming all weights was 1 so just set keep doing what we do now, but we should add a 3-tuple logic so user can supply sumw2 array manually ",
  "created_at":"2022-09-01T22:45:15Z",
  "id":1234861931,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5Jmn9r",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T22:45:15Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"We didn't invent the 2-tuple convention, and I think a 3-tuple would be interpreted as a 2D histogram. (These are things NumPy made up a long time ago and it's a pervasive enough convention that a lot of other libraries recognize it, such as Matplotlib.)\r\n\r\nI think the \"right\" way to deal with this is to not set the `fSumw2` from a NumPy histogram (i.e. not lie) and if you want weights, you have to use a real histogramming package like boost-histogram or hist. They have different `Storage` types for with-errors, without-errors, etc.",
  "created_at":"2022-09-01T22:48:36Z",
  "id":1234863996,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5Jmod8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T22:48:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Until now, any histogram-writing without error information has assumed that the histograms were filled with all weights equal to 1. This might have been the right assumption for some people and the wrong assumption for others, but it really broke down when the weight was not only not 1, but negative.\r\n\r\n(What it has to do with the last bin is a question for the internals of ROOT: we gave it values, it showed us that it got them, and then it drew something strange. But clearly, we were breaking one of ROOT's assumptions, so it's undefined behavior.)\r\n\r\nHistograms from NumPy have no weight information and should set `fSumw2` to `[]`. Histograms from boost-histogram or hist have weight information if you set the appropriate [Storage](https://hist.readthedocs.io/en/latest/user-guide/storages.html) option. If `Storage` is not `Weight`, these histograms should also have a `fSumw2` of `[]`. I have evidence that ROOT expects `fSumw2` to be `[]` and interprets that as \"no weights\":\r\n\r\n```python\r\n>>> h = ROOT.TH1F(\"h\", \"\", 3, 0.0, 3.0)\r\n>>> json.loads(str(ROOT.TBufferJSON.ToJSON(h)))[\"fSumw2\"]\r\n[]\r\n```\r\n\r\nIt draws differently (skyline instead of error bars).\r\n\r\nPR #698 fixes this for Uproot v5 and PR #699 fixes this for Uproot v4. If you (@dcervenkov) can try what you were doing that led to this discovery and everything looks okay, then I'll merge the PRs and make a new bug-fix release for the v4 series right away. Thanks for finding it!",
  "created_at":"2022-09-01T23:35:10Z",
  "id":1234909033,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5Jmzdp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T23:35:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"LGTM as far as bug fixing goes.\r\n\r\nBut I still have the related question of how to write a histogram with error (or equivalently sumw2)  to root",
  "created_at":"2022-09-01T23:38:38Z",
  "id":1234911659,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5Jm0Gr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T23:42:09Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"NumPy's tuple representation leaves a lot to be desired\u2014I checked and a 3-tuple would indeed be interpreted as a 2D histogram. This is why I've said ([page 28 onward](https://indico.cern.ch/event/587955/contributions/3012337/) and [Henry at SciPy](https://conference.scipy.org/proceedings/scipy2020/pdfs/henry_schreiner.pdf)) that one of the things the scientific Python ecosystem lacks is HEP-style histograms. If we're doing anything non-trivial, and plotting error bars on a histogram is well beyond what someone outside of HEP would call trivial, we have to go to a package that actually specializes in histogram objects.\r\n\r\nHere are some tests that use hist to make histograms,\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/49d40bd623b4b2ca19369ae07f9756e65dc6d122/tests/test_0580-round-trip-for-no-flow-histograms.py#L14-L46\r\n\r\nbut I don't think they include errors. The [hist documentation](https://hist.readthedocs.io/) covers all of this, and in particular, the page on [Storages](https://hist.readthedocs.io/en/latest/user-guide/storages.html) shows how to include weights.\r\n\r\nThere are some weighted histograms in the Uproot tests, but they're round trips from ROOT to Uproot to ROOT again.",
  "created_at":"2022-09-01T23:49:18Z",
  "id":1234918334,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5Jm1u-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T23:52:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski I can confirm that both #698 and #699 fix the issue. Feel free to close this issue. Thanks for the speedy resolution!",
  "created_at":"2022-09-02T13:28:56Z",
  "id":1235507711,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5JpFn_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T13:29:11Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"for posterity, here's how you would do it if you're on the Julia -> PythonCall -> pyROOT (cursed) stack\r\n```julia\r\nusing PythonCall\r\nROOT = pyimport(\"ROOT\")\r\n\r\n# h is a FHist.jl histogram but you just need two arrays\r\nbc = bincounts(h)\r\nbe = binerrors(h)\r\n\r\nfile = ROOT.TFile(\"/tmp/example.root\", \"recreate\")\r\n# 100 bins from 0 to 1\r\nth1d = ROOT.TH1D(\"blah\", \"blah blah\", 100, 0, 1)\r\nfor i in eachindex(bc)\r\n    # ROOT has under/overflow bins outside of normal range\r\n    # so we're skipping `0`-th bin by adhering Julia's 1-based index\r\n    th1d.SetBinContent(i, bc[i])\r\n    # similarly, we're skipping overflow bin\r\n    th1d.SetBinError(i, be[i])\r\nend\r\nth1d.Write()\r\nfile.Close()\r\n```\r\n\r\n\r\n### round trip\r\n```julia\r\njulia> binerrors(h)\r\n100-element Vector{Float64}:\r\n 0.0\r\n 0.000339284442870306\r\n 0.0003276982770686565\r\n 0.0020320752377659085\r\n...\r\n\r\njulia> round_h = UnROOT.parseTH(ROOTFile(\"/tmp/example.root\")[\"blah\"]);\r\n\r\n# the third array we read back is the sumW2, so we take sqrt() to recover error\r\njulia> sqrt.(round_h[3])\r\n100-element Vector{Float64}:\r\n 0.0\r\n 0.000339284442870306\r\n 0.0003276982770686565\r\n 0.0020320752377659085\r\n 0.0021085552210994537\r\n...\r\n```\r\n\r\n",
  "created_at":"2022-09-02T21:45:22Z",
  "id":1235918878,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5JqqAe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T21:54:05Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@dcervenkov you might be interested in this workaround:\r\n```python\r\ndef make_TH1D(title, entries, edges, sumw2=None):\r\n    with_flow = numpy.empty(len(entries) + 2, dtype=\">f8\")\r\n    with_flow[1:-1] = entries\r\n    with_flow[0] = 0\r\n    with_flow[-1] = 0\r\n    fEntries=entries.sum()\r\n    fTsumw, fTsumw2, fTsumwx, fTsumwx2 = uproot.writing.identify._root_stats_1d(entries, edges)\r\n    fNbins = len(edges) - 1\r\n    fXmin, fXmax = edges[0], edges[-1]\r\n    return uproot.writing.identify.to_TH1x(\r\n            None,\r\n            title,\r\n            data=with_flow,\r\n            fEntries=fEntries,\r\n            fTsumw=fTsumw,\r\n            fTsumw2=fTsumw2,\r\n            fTsumwx=fTsumwx,\r\n            fTsumwx2=fTsumwx2,\r\n            fSumw2=sumw2,\r\n            fXaxis=uproot.writing.identify.to_TAxis(\r\n                fName=\"xaxis\",\r\n                fTitle=\"\",\r\n                fNbins=fNbins,\r\n                fXmin=fXmin,\r\n                fXmax=fXmax,\r\n                fXbins=edges,\r\n                )\r\n            )\r\n```\r\n\r\nthen you can encode sumw2 yourself:\r\n```python\r\n\r\nIn [84]: t1 = make_TH1D(\"hist\", np.array([1,2,3]), np.array([1,10,20,30]), np.array([0.0, 0.5, 0.8, 2.3, 0.0]))\r\n\r\nIn [86]: import ROOT\r\n\r\nIn [87]: with up.recreate(\"/tmp/test.root\") as file:\r\n    ...:     file[\"hist\"] = t1\r\n\r\nIn [91]: f = ROOT.TFile(\"/tmp/test.root\")\r\n\r\nIn [92]: list(ROOT.TH1D(f.Get(\"hist\")).GetSumw2())\r\nOut[92]: [0.0, 0.5, 0.8, 2.3, 0.0]\r\n```\r\n\r\nnotice you need to pad sumw2 by two zeros because of over/underflow",
  "created_at":"2022-09-08T20:05:45Z",
  "id":1241177571,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5J-t3j",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T00:49:38Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi @Moelf, thanks for the shout-out. #699 and release 4.3.5 fixed my issue. However, what you suggest is a way to write TH1D with Sumw2 - doesn't this already work when you use histograms that support this, i.e., `hist` or `boost_histogram`? Or am I missing something?",
  "created_at":"2022-09-09T08:14:14Z",
  "id":1241652849,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5KAh5x",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T08:14:14Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I don't think they include errors (mostly equivalent to sumw2)",
  "created_at":"2022-09-09T12:21:46Z",
  "id":1241910792,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5KBg4I",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T12:22:08Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"hist and boost-histogram have errors (and therefore `fSumw2`, when converted to ROOT) if you set the [Storage type](https://hist.readthedocs.io/en/latest/user-guide/storages.html) to `Weight` instead of, say, `Double`.",
  "created_at":"2022-09-09T13:37:07Z",
  "id":1241985884,
  "issue":696,
  "node_id":"IC_kwDOD6Q_ss5KBzNc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T13:37:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That's right, and also this is handled correctly by `concatenate`, `arrays` and `array`:\r\n\r\n```python\r\n>>> uproot.concatenate('empty_branches_tree_file.root')\r\n<Array [] type='0 * {int_branch: int32, float_branch: float32}'>\r\n>>> uproot.open('empty_branches_tree_file.root:tree').arrays()\r\n<Array [] type='0 * {int_branch: int32, float_branch: float32}'>\r\n>>> uproot.open('empty_branches_tree_file.root:tree/int_branch').array()\r\n<Array [] type='0 * int32'>\r\n>>> next(uproot.open('empty_branches_tree_file.root:tree').iterate())\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nStopIteration\r\n```\r\n\r\nSo it's something in the `uproot.dask` logic, and @kkothari2001 should take a look. I'll put this file in [scikit-hep-testdata](https://github.com/scikit-hep/scikit-hep-testdata) so that we can add it to the tests.\r\n\r\nThanks!",
  "created_at":"2022-09-01T18:55:02Z",
  "id":1234660764,
  "issue":697,
  "node_id":"IC_kwDOD6Q_ss5Jl22c",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T18:55:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's ready: [scikit-hep-testdata 0.4.20](https://pypi.org/project/scikit-hep-testdata/0.4.20/) has uproot-issue-697.root, which should be used in a test to ensure that `uproot.dask` can make an empty Dask array from it. (Maybe that array will need to have one partition, but that partition can be an empty array.)",
  "created_at":"2022-09-01T19:26:16Z",
  "id":1234691556,
  "issue":697,
  "node_id":"IC_kwDOD6Q_ss5Jl-Xk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-01T19:26:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hey @masonproffitt , thanks for finding the bug! I'm solving it in #700. \r\n\r\n@jpivarski  please confirm the behaviour:\r\n- When there are one or many files, each of which is empty, a single dask chunk is returned which will return an empty numpy/awkward array.\r\n- When there are multiple files with some (but not all) empty branches, we just skip over this file , no *empty-array* dask chunk is created here.\r\n\r\n(PS: Sorry, I didn't mean to be nitpicky with the PR title, I accidentally thought this issue was the PR I created because of the similar titles and intended to correct my own mistake).",
  "created_at":"2022-09-02T04:44:45Z",
  "id":1235061348,
  "issue":697,
  "node_id":"IC_kwDOD6Q_ss5JnYpk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T04:44:45Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"What you describe sounds like a good behavior to adopt. For any valid ROOT files, the user will get an array out, though it might be empty.\r\n\r\nNo problem changing the issue title name. I will sometimes change an issue title name as I find out more about it (if, for instance, the original title described a symptom and we later discover the cause). The issue titles are our list of to-do items, and we should feel free to make them more understandable to ourselves so that it's easier to find actionable tasks.",
  "created_at":"2022-09-02T15:56:45Z",
  "id":1235664169,
  "issue":697,
  "node_id":"IC_kwDOD6Q_ss5Jpr0p",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T15:56:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, I'm done with the PR and was waiting for your review. Thanks for the review!",
  "created_at":"2022-09-02T16:23:35Z",
  "id":1235691502,
  "issue":700,
  "node_id":"IC_kwDOD6Q_ss5Jpyfu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T16:23:35Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"It's hard for me to scan, but it looks like the only non-formatting change is the addition of `test_82` in tests/test_0637-setup-tests-for-AwkwardForth.py.\r\n\r\nAha: that's what it is. pre-commit went crazy with formatting. I don't know why that would happen, since both this PR and #644 ought to be seeing the same pre-commit configurations.\r\n\r\nBut maybe @aryan26roy can just manually add the new test (commit f9e6254dea0c906a75bed9075c860920317b55b2)?",
  "created_at":"2022-09-02T20:54:14Z",
  "id":1235887380,
  "issue":701,
  "node_id":"IC_kwDOD6Q_ss5JqiUU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T20:54:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah, yes the pre-commit went crazy, oh dear \ud83d\ude06 I pushed the second test. I created this branch off of `aryan-forth-reader-latest` so it should be fine, no idea what happened with the pre-commit.\r\n\r\nYes, adding them manually is fine with me.",
  "created_at":"2022-09-02T21:02:56Z",
  "id":1235894416,
  "issue":701,
  "node_id":"IC_kwDOD6Q_ss5JqkCQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T21:02:56Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I will close this. The only relevant test is posted here: https://github.com/scikit-hep/uproot5/pull/644#issuecomment-1235917741",
  "created_at":"2022-09-02T21:46:00Z",
  "id":1235919243,
  "issue":701,
  "node_id":"IC_kwDOD6Q_ss5JqqGL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-02T21:46:00Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Before you start working on this, remember to \"Update branch\".",
  "created_at":"2022-09-21T13:13:40Z",
  "id":1253693145,
  "issue":702,
  "node_id":"IC_kwDOD6Q_ss5KudbZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-21T13:13:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks @jpivarski. I'll need to do that. But since this PR is getting merged in December, I think we should keep this PR open for now and then eventually close it. I can do a separate PR later (with the same text copy-pasted from here). That'll save us from having to merge with main every time and also allow us to alter `_dask.py` without worrying about merge conflicts in the future. Since this is mainly just a doc update, I believe it will be easier to do that.\r\n\r\nI also notice that I have not actually pushed any of the docs updates I have written, I'll just commit and push them shortly.",
  "created_at":"2022-09-21T13:31:35Z",
  "id":1253716776,
  "issue":702,
  "node_id":"IC_kwDOD6Q_ss5KujMo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-21T13:31:35Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"@kkothari2001, I'm taking this out of draft mode to finish it up, so that it's ready for the 2.0 release. (That is, I'll take over the PR now; no need to push anything, but if you have any commits in your local git repo, let me know what they are by Slack so that I can merge them in.)",
  "created_at":"2022-11-28T19:13:24Z",
  "id":1329616483,
  "issue":702,
  "node_id":"IC_kwDOD6Q_ss5PQFZj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-28T19:13:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's done! Anyone looking at the Uproot docs will see `uproot.lazy` disappear and `uproot.dask` appear a week early, but everything else is unchanged. I think that's okay; it won't interrupt work the way that the much more significant changes in the Awkward docs would, if we were to release those early.\r\n\r\nOh, and thanks @kkothari2001 for getting this started!",
  "created_at":"2022-11-28T20:52:31Z",
  "id":1329748106,
  "issue":702,
  "node_id":"IC_kwDOD6Q_ss5PQliK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-28T20:52:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"We have 4 code paths each being handled by separate helper functions, combinatoric-ally 2 (library =np or ak) * 2 (open_files = True or False). Out of 4, 3 already used the `from_map` optimisation. The optimization for the fourth one is added over here. While doing this, I was wondering how `open_files=False` deals with chunks with empty arrays, so I added the tests here too.\r\n\r\nI'd love if @douglasdavis  could have a look, feel free to! Although I'm not sure he'd be able to comment. Because all changes in this PR are only in the usage of core Uproot functions like `regularize_object_path` (internal helper used for opening root files) and `ttree.arrays()`.",
  "created_at":"2022-09-04T15:21:30Z",
  "id":1236363366,
  "issue":703,
  "node_id":"IC_kwDOD6Q_ss5JsWhm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-04T15:21:30Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"If it's using `from_map` (the part I don't know well) in a similar way as the other 3 code paths, then it should be fine because you understand how to use `from_map`. I'll merge it!",
  "created_at":"2022-09-05T14:13:36Z",
  "id":1237100929,
  "issue":703,
  "node_id":"IC_kwDOD6Q_ss5JvKmB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-05T14:13:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"can you provide a sample file?",
  "created_at":"2022-09-05T13:33:28Z",
  "id":1237038697,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5Ju7Zp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-05T13:33:28Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"NONE",
  "body":"this link has a large file...\r\n\r\nurl: https://pan.baidu.com/s/1HPTYiVL41k3vRzVQzSEA6A \r\npd:\uff1ak2os",
  "created_at":"2022-09-06T02:39:58Z",
  "id":1237596993,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5JxDtB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-06T02:39:58Z",
  "user":"MDQ6VXNlcjIyMDI2NTQ2"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@HeConnor \r\n![image](https://user-images.githubusercontent.com/5306213/188708979-cc135b21-a80b-4965-8cec-3d8ddcfb7bd0.png)\r\n\r\n\r\nplease upload somewhere else, try https://wormhole.app/",
  "created_at":"2022-09-06T18:15:09Z",
  "id":1238496125,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5J0fN9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-06T18:15:57Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"NONE",
  "body":"well, please click this link to download: https://wormhole.app/zpd04#7vAvcrhFcXjqltpAzEnVJQ\r\n",
  "created_at":"2022-09-07T03:03:21Z",
  "id":1238853390,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5J12cO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-07T03:03:21Z",
  "user":"MDQ6VXNlcjIyMDI2NTQ2"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"what happened to this file? i.e., how did you produce it:\r\n```\r\nroot [3] .ls\r\nTFile**\t\ttest.root\tROOT file with histograms\r\n TFile*\t\ttest.root\tROOT file with histograms\r\n  KEY: TTree\tCoincidences;-32768\tThe root tree for coincidences [current cycle]\r\n  KEY: TTree\tCoincidences;32767\tThe root tree for coincidences [backup cycle]\r\n```\r\n\r\nthat cycle number is abnormal (overflowed). How many times did you \"modify\" this file?\r\n```\r\njulia> typemax(Int16)\r\n32767\r\n\r\njulia> typemax(Int16) + one(Int16)\r\n-32768\r\n```",
  "created_at":"2022-09-07T13:36:40Z",
  "id":1239401324,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5J38Ns",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-07T13:52:28Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"you can manually specify cycle number:\r\n```python\r\nIn [1]: import uproot as up\r\n\r\nIn [2]: f = up.open(\"./test.root\")\r\n\r\nIn [3]: f.keys()\r\nOut[3]: \r\n['Coincidences;-32768',\r\n 'Coincidences;-32767',\r\n 'latest_event_ID;1',\r\n 'total_nb_primaries;1',\r\n 'Hits;1',\r\n 'OpticalData;1']\r\n\r\nIn [5]: f[\"Coincidences\"].num_entries\r\nOut[5]: 34393000\r\n\r\n# ROOT detectes `-32768` is the current cycle, so let's use that \r\nIn [6]: f[\"Coincidences;-32768\"].num_entries\r\nOut[6]: 34393400\r\n```",
  "created_at":"2022-09-07T13:50:59Z",
  "id":1239419497,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5J4App",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-08T13:46:09Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski might want to look at the logic that determines which cycle is the newest one, which is failing here. For reference, UnROOT.jl does it correctly by pure accident (I don't remember testing this edge case):\r\n```julia\r\nusing UnROOT\r\n\r\njulia> t = LazyTree(\"/home/akako/Downloads/test.root\", \"Coincidences\");\r\n\r\njulia> length(t)\r\n34393400\r\n```",
  "created_at":"2022-09-07T13:56:20Z",
  "id":1239426480,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5J4CWw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-07T13:56:20Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"NONE",
  "body":"em... thank you. I didn't notice it\r\n\r\n1. I want to confirm it: \r\n\r\n```\r\nKEY: TTree  Coincidences;-32768  The root tree for coincidences [current cycle]\r\nKEY: TTree   Coincidences;32767   The root tree for coincidences [backup cycle]   \r\n```\r\nthe difference between 'current cycle' and 'backup cycle' was 400 coincidences event?\r\n\r\n34393400 - 34393000 = 400 ?\r\n\r\n\r\n2. About \"what happened to this file? i.e., how did you produce it:\"\r\n\r\n- Monte-Carlo simulation; this problem was found with exactly one of the root files\r\n\r\n\r\n3. About \"How many times did you \"modify\" this file?\":\r\n- many times....... I didn't konwn.",
  "created_at":"2022-09-08T08:57:14Z",
  "id":1240428962,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5J73Gi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-08T08:57:14Z",
  "user":"MDQ6VXNlcjIyMDI2NTQ2"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski https://jiling.web.cern.ch/jiling/dump/issue704.root for the `test.root` file",
  "created_at":"2022-09-09T01:36:40Z",
  "id":1241402499,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5J_kyD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T01:57:41Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I was curious about what ROOT does here, so I looked at the blame for [TKey](https://github.com/root-project/root/blob/87a998d48803bc207288d90038e60ff148827664/io/io/src/TKey.cxx#L579):\r\n\r\n![image](https://user-images.githubusercontent.com/1248413/189387571-e5750eca-a00e-4fd9-8aaf-8f2e3604fa6d.png)\r\n\r\nThis LoC was last changed 23 years ago when the repo was converted from CVS to SVN :open_mouth: \r\n\r\nIt looks like (although I don't know ROOT's internals anywhere near \"intimately\") ROOT takes negative cycles and negates them. That seems ... strange, but interesting nonetheless. I'm obviously assuming that there's nothing else happening with cycle handling, which is likely false.",
  "created_at":"2022-09-09T15:38:43Z",
  "id":1242135948,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5KCX2M",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T15:38:43Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"If an integer loops around from successive incrementation, it's still the case that larger (less negative) values have been incremented more times. Negating the cycle number (as above) should therefore be wrong. It would not give you the newest key. In fact, if `fCycle` is equivalent to `-fCycle`, then there's a ceiling for cycle numbers: they can never tell you the newness of a key past `2**15`.\r\n\r\nI think that Uproot should continue to default to the largest cycle number for a given name. It's always possible to select a cycle number explicitly (with a `\"name;cycle\"` key string).",
  "created_at":"2022-09-09T15:50:56Z",
  "id":1242149854,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5KCbPe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T15:50:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">  it's still the case that larger (less negative) values have been incremented more times.\r\n\r\nRight, that's just ... weird. \r\n\r\n> I think that Uproot should continue to default to the largest cycle number for a given name. It's always possible to select a cycle number explicitly (with a \"name;cycle\" key string).\r\n\r\nYeah, anyone hitting those kind of cycle numbers is probably going to fail the assumption that they've single-overflowed, so we can't really reconstruct any useful information. I assume negative cycles are only useful to indicate that a cycle *might* have overflowed, right? Could uproot raise an error if you try to default-select a cycle which happens to be negative (i.e. `\"the key you've selected might have overflowed. Please select the cycle explicitly with ...\"`)? \r\n\r\nI don't hugely know what I'm talking about here w.r.t serialisation, so feel free to ignore these suggestions if they don't work.",
  "created_at":"2022-09-09T16:04:33Z",
  "id":1242165540,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5KCfEk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T16:04:33Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, I've examined the file and it's weird.\r\n\r\nFirst of all, there are on the order of 30k TBaskets in a sample TBranch, so the writer must have replaced the TTree metadata with nearly every new TBasket, creating a new cycle number each time.\r\n\r\n```python\r\n>>> file = uproot.open(\"issue-704.root\")\r\n>>> file[\"Coincidences;-32768\"][\"runID\"].num_baskets\r\n34394\r\n>>> file[\"Coincidences;-32768\"][\"runID\"].num_entries\r\n34393400\r\n```\r\n\r\nI don't know what program was used to write this\u2014ROOT, Geant, UnROOT, groot, ...\u2014but it's inefficient. It wasn't Uproot: Uproot doesn't create new cycle numbers when it has to replace the TTree metadata. The reason a program needs to replace the TTree metadata is because the TBranches in the TTree have a fixed number of pointers to TBasket locations in the file. When you add a TBasket, one of the TBranches might run out of \"slots,\" in which case the TTree metadata needs to be rewritten with more slots. ROOT's behavior is to create a new TTree with a higher cycle number and delete the third-most recent, so that there are only two at any time. Uproot ensures that there is only one at any time, so it can reuse the same cycle number. But the thing that puzzles me is that when a writer runs out of slots, you'd think it would make the new number of slots twice or a constant-factor as large as the old number of slots, so that the number of slots grows exponentially with TBaskets. Then the writer only needs to replace the TTree metadata logarithmically often. Here, the number of TBaskets, 34394, is at the same scale as the cycle number, `2**15`, only a little bit larger. So the writer must have been growing the number of slots linearly, not exponentially.\r\n\r\nIn fact, this TBranch has only one more \"slot\" left:\r\n\r\n```python\r\n>>> file[\"Coincidences;-32768\"][\"runID\"].member(\"fBasketSeek\").shape\r\n(34395,)\r\n```\r\n\r\nAdding another TBasket would force the TTree metadata to be replaced again.\r\n\r\n> About \"what happened to this file? i.e., how did you produce it:\"\r\n> Monte-Carlo simulation; this problem was found with exactly one of the root files\r\n\r\nI meant \"What program produced this file?\" If it's ROOT, it's probably in some strange parameterization because I doubt it would do this with default configuration. Maybe it's an auto-flush parameter? No, that just says how often to write TBaskets, not how to configure the new TTree metadata, which would be a deep detail in any implementation. Some Monte Carlos, like Geant, have their own ROOT file-writer, and it might have this inefficiency. If what we're looking at is the default configuration of some ROOT file-writer, then the authors of that program probably want to know that it has this inefficiency and can be fixed.\r\n\r\nSo that's why we normally wouldn't encounter files with such large cycle numbers: `2**15` and larger is not an unreasonable number of TBaskets, but the number of times the TTree gets rewritten _should be_ the logarithm of that, less than 100, if the base is at least 1.1 or so (i.e. replacement TTree metadata has at least 10% more slots than the last one). `2**15` is an unreasonable cycle number, as I'm sure the ROOT developers had in mind when they defined it to be a 2-byte integer.\r\n\r\n## What to do about it\r\n\r\nInterpreting the cycle numbers as signed, the highest cycle number does not have the largest number of entries.\r\n\r\nInterpreting the cycle numbers as unsigned, the highest cycle number does not have the largest number of entries.\r\n\r\nInterpreting the cycle numbers as signed and taking the absolute value, the highest absolute value cycle number _does_ have the largest number of entries.\r\n\r\nAnd, as @agoose77 found, that's what ROOT seems to be doing. There must be some strange logic on the writer side to ensure that the largest absolute value cycle number is the latest one, because that's not what happens when you repeatedly add one to an integer that wraps around (the newest becomes less and less negative).\r\n\r\nSo that's what I'll do:\r\n\r\n```diff\r\n--- a/src/uproot/reading.py\r\n+++ b/src/uproot/reading.py\r\n@@ -2047,7 +2047,7 @@ class ReadOnlyDirectory(Mapping):\r\n                 return key\r\n             elif cycle is None and last is None:\r\n                 last = key\r\n-            elif cycle is None and last.fCycle < key.fCycle:\r\n+            elif cycle is None and abs(last.fCycle) < abs(key.fCycle):\r\n                 last = key\r\n \r\n         if last is not None:\r\n```\r\n\r\nThis isn't going to affect files with cycle numbers much less than `2**15`, the normal case.",
  "created_at":"2022-09-19T19:33:06Z",
  "id":1251459780,
  "issue":704,
  "node_id":"IC_kwDOD6Q_ss5Kl8LE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-19T19:33:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The bytes blob inside a RNTuple is pre-pended by not TKey but `RBlob` (and these 'RBlob's are not put in the TFile-wide directory):\r\n```python\r\nIn [34]: t = ROOT.TFile(\"/home/akako/Documents/github/scikit-hep-testdata/src/skhep_testdata/data/test_ntuple_int_10.root\")\r\n\r\nIn [35]: t.Map()\r\n20220628/101853  At:100    N=112       TFile                     \r\n20220628/101853  At:212    N=405       StreamerInfo   CX =  2.73 \r\n20220628/101853  At:617    N=120       ROOT::Experimental::RNTuple            \r\n20220628/101853  At:737    N=95        KeysList                  \r\n20220628/101853  At:832    N=167       RBlob          CX =  1.16 \r\n20220628/101853  At:999    N=74        RBlob                     \r\n20220628/101853  At:1073   N=94        RBlob                     \r\n20220628/101853  At:1167   N=115       RBlob          CX =  1.20 \r\n20220628/101853  At:1282   N=39        FreeSegments              \r\n20220628/101853  At:1321   N=1         END \r\n\r\nIn [49]: rn = up.open(\"/home/akako/Documents/github/scikit-hep-testdata/src/skhep_testdata/data/test_ntuple_int_10.root\")[\"ntuple\"]\r\n\r\nIn [50]: rn._members\r\nOut[50]: \r\n{'fCheckSum': 1700499286,\r\n 'fVersion': 0,\r\n 'fSize': 48,\r\n 'fSeekHeader': 866,\r\n 'fNBytesHeader': 133,\r\n 'fLenHeader': 159,\r\n 'fSeekFooter': 1201,\r\n 'fNBytesFooter': 81,\r\n 'fLenFooter': 104,\r\n 'fReserved': 0}\r\n```\r\n\r\n### First `RBlob` is header:\r\n```python\r\nIn [51]: 866+133 == 832+167\r\nOut[51]: True\r\n```\r\n\r\n### Second `RBlob` is the column content \r\n```python\r\nIn [20]: link = rn.page_list_envelopes.pagelinklist[0][0]\r\n\r\nIn [21]: rn.pagelist(link)\r\nOut[21]: [MetaData('PageDescription', num_elements=10, locator=MetaData('Locator', num_bytes=40, offset=1033))]\r\n\r\nIn [23]: 1033+40 == 1073\r\nOut[23]: True\r\n```\r\n\r\n### Third `RBlob` is one of the page list:\r\n```python\r\nIn [56]: 1073+94\r\nOut[56]: 1167\r\n\r\nIn [62]: rn.page_list_envelopes.pagelinklist[0][0].chunk\r\nOut[62]: <Chunk 1107-1167>\r\n```\r\n\r\n### Fourth (last) `RBlob` is footer:\r\n\r\n```python\r\nIn [52]: 1201+81 == 1167+115\r\nOut[52]: True\r\n```",
  "created_at":"2022-09-05T15:50:05Z",
  "id":1237237572,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5Jvr9E",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-07T00:24:18Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\nIn [20]: akform = ak._v2.from_iter([{\"one\": 1, \"two\": 2.0}]).layout.form\r\n\r\nIn [21]: with up.recreate(\"/tmp/test.root\") as file:\r\n    ...:     file.mktree(\"Events\", {\"one\":\"int\"})\r\n    ...:     file.mkntuple(\"ntuple\", akform)\r\n\r\nIn [23]: up.open(\"/tmp/test.root\")[\"ntuple\"]._members\r\nOut[23]: \r\n{'fCheckSum': 1700499286,\r\n 'fVersion': 0,\r\n 'fSize': 48,\r\n 'fSeekHeader': 866,\r\n 'fNBytesHeader': 133,\r\n 'fLenHeader': 159,\r\n 'fSeekFooter': 1201,\r\n 'fNBytesFooter': 81,\r\n 'fLenFooter': 104,\r\n 'fReserved': 0}\r\n\r\nIn [22]: rn = up.open(\"/home/akako/Documents/github/scikit-hep-testdata/src/skhep_testdata/data/test_ntuple_int_10.root\")[\"ntuple\"]\r\n\r\nIn [24]: rn._members\r\nOut[24]: \r\n{'fCheckSum': 1700499286,\r\n 'fVersion': 0,\r\n 'fSize': 48,\r\n 'fSeekHeader': 866,\r\n 'fNBytesHeader': 133,\r\n 'fLenHeader': 159,\r\n 'fSeekFooter': 1201,\r\n 'fNBytesFooter': 81,\r\n 'fLenFooter': 104,\r\n 'fReserved': 0}\r\n```",
  "created_at":"2022-09-05T19:24:33Z",
  "id":1237407359,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5JwVZ_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-05T19:25:08Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"in a different file, the `stl_container` test file, we have:\r\n- one header RBlob\r\n- one footer RBlob\r\n- one chunk for pagelinklist (the 30 entries share same chunk just at different offset)\r\n```python\r\nIn [48]: rn.page_list_envelopes.pagelinklist[0][0].chunk\r\nOut[48]: <Chunk 0-1104>\r\n```\r\n- 30 page data blobs:\r\n```python\r\nIn [49]: len(rn.page_list_envelopes.pagelinklist[0])\r\nOut[49]: 30\r\n```",
  "created_at":"2022-09-05T21:24:02Z",
  "id":1237465969,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5Jwjtx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-05T21:24:47Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\nIn [227]: with up.recreate(\"/tmp/test.root\") as file:\r\n     ...:     file.mkntuple(\"ntuple\", akform)\r\n\r\nIn [228]: up.open(\"/tmp/test.root\")[\"ntuple\"].header\r\nOut[228]: MetaData('HeaderReader', env_header={'env_version': 1, 'min_version': 1}, feature_flag=0, rc_tag=1, name='ntuple', ntuple_description='', writer_identifier='uproot 5.0.0rc2', field_records=[MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=0, struct_role=0, flags=0, field_name='one_integers', type_name='std::int32_t', type_alias='', field_desc='')], column_records=[MetaData('ColumnRecordFrame', type=11, nbits=32, field_id=0, flags=0)], alias_columns=[], extra_type_infos=[], crc32=3689245460)\r\n```",
  "created_at":"2022-09-06T21:24:19Z",
  "id":1238665487,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5J1IkP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-06T21:24:19Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\nIn [444]: with up.recreate(\"/tmp/test.root\") as file:\r\n     ...:     akform =  ak._v2.forms.RecordForm([ak._v2.forms.NumpyForm('float64'), ak._v2.forms.NumpyForm('int32'), ak._v2.forms.Numpy\r\n     ...: Form('bool')], ['one', 'two', 'three'])\r\n     ...:     file.mkntuple(\"ntuple\", akform)\r\nup\r\nIn [445]: up.open(\"/tmp/test.root\")[\"ntuple\"].header.field_records\r\nOut[445]: \r\n[MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=0, struct_role=0, flags=0, field_name='one', type_name='double', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=1, struct_role=0, flags=0, field_name='two', type_name='std::int32_t', type_alias='', field_desc=''),\r\n MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=2, struct_role=0, flags=0, field_name='three', type_name='bit', type_alias='', field_desc='')]\r\n\r\nIn [446]: up.open(\"/tmp/test.root\")[\"ntuple\"].header.column_records\r\nOut[446]: \r\n[MetaData('ColumnRecordFrame', type=7, nbits=64, field_id=0, flags=0),\r\n MetaData('ColumnRecordFrame', type=11, nbits=32, field_id=1, flags=0),\r\n MetaData('ColumnRecordFrame', type=6, nbits=1, field_id=2, flags=0)]\r\n```",
  "created_at":"2022-09-07T00:14:30Z",
  "id":1238769897,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5J1iDp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-07T00:14:30Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"working on Footer, turns out it's not empty even if the RNTuple itself is empty:\r\n```python\r\nIn [26]: rn = up.open(\"./test_ntuple_int_empty.root\")[\"ntuple\"]\r\n\r\nIn [27]: rn.header\r\nOut[27]: MetaData('HeaderReader', env_header={'env_version': 1, 'min_version': 1}, feature_flag=0, rc_tag=1, name='ntuple', ntuple_description='', writer_identifier='ROOT v6.26/06', field_records=[MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=0, struct_role=0, flags=0, field_name='one_integers', type_name='std::int32_t', type_alias='', field_desc='')], column_records=[MetaData('ColumnRecordFrame', type=11, nbits=32, field_id=0, flags=0)], alias_columns=[], extra_type_infos=[], crc32=4183493504)\r\n\r\n# this file is written out by uproot, header is bit-perfect matched, look at the crc32\r\nIn [28]: up.open(\"/tmp/test.root\")[\"ntuple\"].header\r\nOut[28]: MetaData('HeaderReader', env_header={'env_version': 1, 'min_version': 1}, feature_flag=0, rc_tag=1, name='ntuple', ntuple_description='', writer_identifier='ROOT v6.26/06', field_records=[MetaData('FieldRecordFrame', field_version=0, type_version=0, parent_field_id=0, struct_role=0, flags=0, field_name='one_integers', type_name='std::int32_t', type_alias='', field_desc='')], column_records=[MetaData('ColumnRecordFrame', type=11, nbits=32, field_id=0, flags=0)], alias_columns=[], extra_type_infos=[], crc32=4183493504)\r\n\r\nIn [29]: rn.footer\r\nOut[29]: MetaData('Footer', env_header={'env_version': 1, 'min_version': 1}, feature_flag=0, header_crc32=4183493504, extension_links=[], col_group_records=[], cluster_summaries=[], cluster_group_records=[MetaData('ClusterGroupRecord', num_clusters=0, page_list_link=MetaData('EnvLink', env_uncomp_size=16, locator=MetaData('Locator', num_bytes=16, offset=1039)))], meta_block_links=[])\r\n```",
  "created_at":"2022-09-07T15:33:40Z",
  "id":1239548460,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5J4gIs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-07T15:34:07Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\nIn [43]: with up.recreate(\"/tmp/test.root\") as file:\r\n    ...:     akform =  ak._v2.forms.RecordForm([ak._v2.forms.NumpyForm('int32')], ['one_integers'])\r\n    ...:     file.mkntuple(\"ntuple\", akform)\r\n\r\n\r\nIn [44]: up.open(\"/tmp/test.root\")[\"ntuple\"].footer\r\n#[  1   0   1   0   0   0   0   0   0   0   0   0 128  11  91 249 248 255\r\n# 255 255   0   0   0   0 248 255 255 255   0   0   0   0 248 255 255 255\r\n#  0   0   0   0 224 255 255 255   1   0   0   0  24   0   0   0   0   0\r\n#   0   0  16   0   0   0  16   0   0   0  15   4   0   0   0   0   0   0\r\n# 248 255 255 255   0   0   0   0 107 140  67 241]\r\n\r\nMetaData('Footer', env_header={'env_version': 1, 'min_version': 1}, feature_flag=0, header_crc32=4183493504, extension_links=[], col_group_records=[], cluster_summaries=[], cluster_group_records=[MetaData('ClusterGroupRecord', num_clusters=0, page_list_link=MetaData('EnvLink', env_uncomp_size=16, locator=MetaData('Locator', num_bytes=16, offset=1039)))], meta_block_links=[], crc32=4047735915)\r\n```",
  "created_at":"2022-09-07T16:18:14Z",
  "id":1239608305,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5J4uvx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-07T16:19:16Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"ROOT is happy with the (empty file) footer for now\r\n```python\r\nIn [16]: with up.recreate(\"/tmp/test.root\") as file:\r\n    ...:     akform =  ak._v2.forms.RecordForm([ak._v2.forms.NumpyForm('int32')], ['one_integers'])\r\n    ...:     file.mkntuple(\"ntuple\", akform)\r\n\r\nIn [17]: ROOT.Experimental.RNTupleReader.Open(\"ntuple\", \"/tmp/test.root\").PrintInfo()\r\n[ROOT.NTuple] Warning /home/conda/feedstock_root/build_artifacts/root_base_1659775868695/work/root-source/tree/ntuple/v7/src/RNTupleSerialize.cxx:1105 in static ROOT::Experimental::RResult<void> ROOT::Experimental::Internal::RNTupleSerializer::DeserializeHeaderV1(const void*, uint32_t, ROOT::Experimental::RNTupleDescriptorBuilder&):0: RuntimeWarning: Pre-release format version: RC 1\r\n************************************ NTUPLE ************************************\r\n* N-Tuple : ntuple                                                             *\r\n* Entries : 0                                                                  *\r\n********************************************************************************\r\n* Field 1   : one_integers (std::int32_t)                                      *\r\n********************************************************************************\r\n```",
  "created_at":"2022-09-08T18:39:21Z",
  "id":1241089009,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5J-YPx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-08T18:39:38Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\nIn [73]: with up.recreate(\"/tmp/test.root\") as file:\r\n    ...:     akform =  ak._v2.forms.RecordForm([ak._v2.forms.NumpyForm('int32')], ['one_integers\r\n    ...: '])\r\n    ...:     file.mkntuple(\"ntuple\", akform)\r\n    ...:     print(type(file))\r\n    ...:     a = file[\"ntuple\"]\r\n    ...:     print(type(a))\r\n<class 'uproot.writing.writable.WritableDirectory'>\r\n<class 'uproot.writing.writable.WritableNTuple'>\r\n```",
  "created_at":"2022-09-09T18:05:34Z",
  "id":1242307130,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5KDBo6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T18:05:34Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\nIn [28]: with up.recreate(\"/tmp/test.root\") as file:\r\n    ...:     akform =  ak._v2.forms.RecordForm([ak._v2.forms.NumpyForm('int32')], ['one_integers\r\n    ...: '])\r\n    ...:     file.mkntuple(\"ntuple\", akform)\r\n    ...:     a = file[\"ntuple\"]\r\n    ...:     a.extend(array)\r\n    ...:     file.close()\r\n\r\nIn [29]: up.open(\"/tmp/test.root\")[\"ntuple\"].footer\r\nOut[29]: MetaData('Footer', env_header={'env_version': 1, 'min_version': 1}, feature_flag=0, header_crc32=4183493504, extension_links=[], col_group_records=[], cluster_summaries=[MetaData('ClusterSummaryRecord', num_first_entry=0, num_entries=10)], cluster_group_records=[MetaData('ClusterGroupRecord', num_clusters=0, page_list_link=MetaData('EnvLink', env_uncomp_size=16, locator=MetaData('Locator', num_bytes=16, offset=1601)))], meta_block_links=[], crc32=2251102086)\r\n```",
  "created_at":"2022-09-09T21:04:34Z",
  "id":1242469968,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5KDpZQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-09T21:06:47Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"reminder that the list of list of list (of pages) is real:\r\n(file from:\r\n- https://github.com/scikit-hep/uproot5/pull/682\r\n\r\n)\r\n```python\r\nref = up.open(\"/home/akako/Documents/github/scikit-hep-testdata/src/skhep_testdata/data/test_ntuple_large_bit_int64.root\")[\"ntuple\"]\r\n\r\nIn [6]: ref.page_list_envelopes.pagelinklist\r\nOut[6]:\r\n[[InnerListLocator(<Chunk 0-110112>, Cursor(20), num_pages=170),\r\n  InnerListLocator(<Chunk 0-110112>, Cursor(2760), num_pages=1356)],\r\n [InnerListLocator(<Chunk 0-110112>, Cursor(24484), num_pages=190),\r\n  InnerListLocator(<Chunk 0-110112>, Cursor(27544), num_pages=1523)],\r\n [InnerListLocator(<Chunk 0-110112>, Cursor(51940), num_pages=190),\r\n  InnerListLocator(<Chunk 0-110112>, Cursor(55000), num_pages=1523)],\r\n [InnerListLocator(<Chunk 0-110112>, Cursor(79396), num_pages=190),\r\n  InnerListLocator(<Chunk 0-110112>, Cursor(82456), num_pages=1523)],\r\n [InnerListLocator(<Chunk 0-110112>, Cursor(106852), num_pages=22),\r\n  InnerListLocator(<Chunk 0-110112>, Cursor(107224), num_pages=179)]]\r\n```",
  "created_at":"2022-09-12T17:05:13Z",
  "id":1244033371,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5KJnFb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-12T17:06:28Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"for maintainers' sanity, increased the eagerness of some footer reading so you can see the list of list of list structure directly instead of following an indirection via the (fake, i.e. not in RNTuple spec) `InnerListLocator`:\r\n```python\r\nIn [157]: ref = up.open(\"/home/akako/Documents/github/scikit-hep-testdata/src/skhep_testdata/data/test_n\r\n     ...: tuple_int_10.root\")[\"ntuple\"]\r\nref.page\r\nIn [158]: ref.page_list_envelopes\r\nOut[158]: MetaData('PageLink', env_header={'env_version': 1, 'min_version': 1}, pagelinklist=[[[MetaData('PageDescription', num_elements=10, locator=MetaData('Locator', num_bytes=40, offset=1033))]]], crc32=2120950660)\r\n\r\nIn [159]: ref.page_list_envelopes.pagelinklist\r\nOut[159]: [[[MetaData('PageDescription', num_elements=10, locator=MetaData('Locator', num_bytes=40, offset=1033))]]]\r\n```\r\n\r\nmore importantly, this triple-list IS contiguous in memory, so this change conforms with the design pattern of hierarchical readers",
  "created_at":"2022-09-12T18:15:54Z",
  "id":1244120229,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5KJ8Sl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-12T18:23:35Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"```python\r\nIn [149]: array\r\nOut[149]: <Array [{one_integers: 9}, {...}, ..., {...}] type='10 * {one_integers: int32}'>\r\n\r\nIn [150]: with up.recreate(\"/tmp/test.root\") as file:\r\n     ...:     akform =  ak._v2.forms.RecordForm([ak._v2.forms.NumpyForm('int32')], ['one_integers'])\r\n     ...:     file.mkntuple(\"ntuple\", akform)\r\n     ...:     a = file[\"ntuple\"]\r\n     ...:     a.extend(array)\r\n     ...:     file.close()\r\n     ...:     rn = up.open(\"/tmp/test.root\")[\"ntuple\"]\r\n     ...:     assert ak.all(rn.arrays()[\"one_integers\"]  == np.array([9,8,7,6,5,4,3,2,1,0]))\r\n```\r\n\r\n## comment:\r\nnow we can do round trip with ourselves but ROOT doesn't like it, there are two visible problems:\r\n1. ROOT thinks the total # of entries is 0 for some reason\r\n2. https://github.com/root-project/root/blob/ef62da7335eecdea3df98269f2d0cccbda600b98/tree/ntuple/v7/src/RPageStorageFile.cxx#L383 is violated (when doing ntuple.LoadEntry(0)\r\n\r\nfor 1, I tried to look at the source code, and I found that https://github.com/root-project/root/blob/ef62da7335eecdea3df98269f2d0cccbda600b98/tree/ntuple/v7/inc/ROOT/RNTuple.hxx#L196, where the fSource is a pointer to a RPageSource , and that eventually calls https://github.com/root-project/root/blob/ef62da7335eecdea3df98269f2d0cccbda600b98/tree/ntuple/v7/src/RPageStorage.cxx#L94\r\n\r\nthe problem: I've never seen \"descriptor\" and I'm not sure what they are referring to / if it's part of the old world or not\r\n\r\nfor 2, I suspect it has something to do with 1, if ROOT doesn't detect our rntuple has X number of events, we probably have some metadata regarding clusters incorrect, because I would expect the total entry numbers is just a sum of cluster_summaries in the footer.",
  "created_at":"2022-09-16T17:01:16Z",
  "id":1249588171,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5KezPL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-23T14:03:00Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"@Moelf, I'm marking this PR as \"inactive.\" When the RNTuple-writing feature is eventually implemented, it might need to be overlaid on a more recent version of `main` (that is, by closing this and reopening it as a new PR), but I'd like this to remain in the list of PRs as a reminder to get back to it and a knowledge base of how to do it.",
  "created_at":"2022-11-28T19:09:51Z",
  "id":1329611628,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5PQENs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-28T19:09:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"looks like we can still just update? I will try to make test pass somewhat, but since it's not usable I won't try to advocate for a merge",
  "created_at":"2022-11-28T22:31:56Z",
  "id":1329840977,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5PQ8NR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-28T22:32:27Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Got it, thanks! We might be doing a lot of bug-fixes in the coming weeks when a wave of non-early-adopter users get Uproot 5. It shouldn't impact the RNTuple submodule, but `main` will move.",
  "created_at":"2022-11-28T22:37:56Z",
  "id":1329845505,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5PQ9UB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-28T22:37:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`awkward._v2` is now just `awkward`. We couldn't find a clean way to make a fake submodule point at its parent. (You'd think you could, but there were hidden subtleties.) That's the reason for the first failure. The `main` branch takes care of that for most of the codebase, but if you have new code in this PR branch that assumes the existence of a `_v2` submodule, you should be able to fix it by just removing that \"`._v2`\" string.",
  "created_at":"2022-11-28T22:40:26Z",
  "id":1329847361,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5PQ9xB",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-11-28T22:40:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's because the class name was `TParameter<Long64_t>` in the TKey, but `TParameter<long long>` in the TStreamerInfo.\r\n\r\nIn all of our tests, the template name is always based on C fundamental types, not ROOT's aliases:\r\n\r\n```\r\nBDSOutputROOTEventSampler<float>\r\nROOT::Detail::VecOps::RAdoptAllocator<float>\r\nROOT::Math::LorentzVector<ROOT::Math::PxPyPzE4D<float> >\r\nROOT::Math::PxPyPzE4D<float>\r\nTMatrixT<double>\r\nTMatrixTBase<double>\r\nTMatrixTSym<double>\r\nTParameter<bool>\r\nTParameter<double>\r\nTParameter<int>\r\npair<TLorentzVector,int>\r\npair<double,double>\r\npair<int,BDSOutputROOTGeant4Data::ParticleInfo>\r\npair<int,int>\r\npair<long,int>\r\npair<string,double>\r\npair<string,int>\r\npair<string,string>\r\npair<unsigned int,string>\r\nvector<bool>\r\nvector<char>\r\nvector<double>\r\nvector<float>\r\nvector<int>\r\nvector<pair<TLorentzVector,int> >\r\nvector<set<unsigned int> >\r\nvector<short>\r\nvector<string>\r\nvector<unsigned char>\r\nvector<unsigned int>\r\nvector<unsigned long long>\r\nvector<unsigned long>\r\nvector<unsigned short>\r\nvector<vector<float> >\r\nvector<vector<int> >\r\n```\r\n\r\nso I suppose the proper thing to do here is to always translate ROOT's aliases into C's fundamental type names, as part of the \"classname regularization\" step.",
  "created_at":"2022-09-19T22:26:12Z",
  "id":1251633584,
  "issue":707,
  "node_id":"IC_kwDOD6Q_ss5Kmmmw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-19T22:26:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This last pull from `main` should fix the test (from PR #719).",
  "created_at":"2022-09-20T16:28:10Z",
  "id":1252602246,
  "issue":710,
  "node_id":"IC_kwDOD6Q_ss5KqTGG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T16:28:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, it was just failing due to the bug that #719 fixed. The other update is not important; go ahead and merge as-is.",
  "created_at":"2022-09-20T21:14:02Z",
  "id":1252920010,
  "issue":710,
  "node_id":"IC_kwDOD6Q_ss5KrgrK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T21:14:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I had been assuming that this was merged a month ago. I'm going to see how easy it is to get in now. Doesn't #749 need to be applied on top of these changes? Are they really independent?",
  "created_at":"2022-10-28T22:39:09Z",
  "id":1295577798,
  "issue":710,
  "node_id":"IC_kwDOD6Q_ss5NOPLG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-28T22:39:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks\u2014the intent was to be lazy about importing possibly unnecessary libraries and also to avoid assigning too large of a set of Python data types to be interpreted as writable TTrees, but the holes in this assignment don't make sense (what other data types would we write to a file, that is a dict but not a TTree?) and was accidentally environment-dependent (the interpretation depended on whether `awkward` was installed).\r\n\r\n#779 should fix it.",
  "created_at":"2022-11-11T00:05:58Z",
  "id":1311060859,
  "issue":711,
  "node_id":"IC_kwDOD6Q_ss5OJTN7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-11T00:05:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It looks like we never close the executors, so their resources (threads) accumulate until exhaustion of the limit.",
  "created_at":"2022-09-16T20:28:55Z",
  "id":1249783474,
  "issue":712,
  "node_id":"IC_kwDOD6Q_ss5Kfi6y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-16T20:28:55Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"It was this (from @douglasdavis on Slack):\r\n\r\n> dask_awkward.testutils has moved to dask_awkward.lib.testutils: https://github.com/ContinuumIO/dask-awkward/pull/76\r\n\r\nI'll open a PR for just this, which we can merge against. @Moelf was also seeing this issue, so we need a separate commit that can be merged into all the open PRs.",
  "created_at":"2022-09-17T16:10:09Z",
  "id":1250097544,
  "issue":713,
  "node_id":"IC_kwDOD6Q_ss5KgvmI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-17T16:10:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's in #714.",
  "created_at":"2022-09-17T16:19:33Z",
  "id":1250098898,
  "issue":713,
  "node_id":"IC_kwDOD6Q_ss5Kgv7S",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-17T16:19:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"FYI, @douglasdavis: `assert_eq` has a hidden dependency on `pyarrow`. Did you intend to make `dask_awkward` strictly depend on `pyarrow`?\r\n\r\n```\r\ntests/test_0700-dask-empty-arrays.py:11: in <module>\r\n    from dask_awkward.lib.testutils import assert_eq\r\n../../../miniconda3/envs/test/lib/python3.8/site-packages/dask_awkward/lib/__init__.py:16: in <module>\r\n    from dask_awkward.lib.io.parquet import from_parquet, to_parquet\r\n../../../miniconda3/envs/test/lib/python3.8/site-packages/dask_awkward/lib/io/parquet.py:7: in <module>\r\n    import pyarrow.parquet as pq\r\nE   ModuleNotFoundError: No module named 'pyarrow'\r\n```",
  "created_at":"2022-09-17T16:25:23Z",
  "id":1250099755,
  "issue":714,
  "node_id":"IC_kwDOD6Q_ss5KgwIr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-17T16:25:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If the tests pass, @agoose77 or @Moelf, you can approve this PR, merge it into `main`, and then use it to fix the tests in your PRs: #713 and #705.\r\n\r\nWhen running locally,\r\n\r\n```bash\r\npip install -U dask-awkward\r\n```\r\n\r\n(or maybe you're using a tool that automatically pulls the latest dependencies into your venv, in which case, good for you!)",
  "created_at":"2022-09-17T16:32:12Z",
  "id":1250100728,
  "issue":714,
  "node_id":"IC_kwDOD6Q_ss5KgwX4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-17T16:32:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I've added the LB constaint to `pyproject.toml`, and also removed the manual install of `dask-awkward` from the CI (it's installed later in the job by the dev dependencies).",
  "created_at":"2022-09-17T16:49:49Z",
  "id":1250104078,
  "issue":714,
  "node_id":"IC_kwDOD6Q_ss5KgxMO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-17T16:49:49Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, if it was not in pyproject.toml, then I was probably wrong about dask-awkward not declaring its dependencies. Sorry for the confusion, @douglasdavis!",
  "created_at":"2022-09-17T18:14:49Z",
  "id":1250117802,
  "issue":714,
  "node_id":"IC_kwDOD6Q_ss5Kg0iq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-17T18:14:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The only change I made was to add `>= VERSION` to the `dask-awkward` dependency for uproot5, so that we explicitly don't request a version before they made this change.",
  "created_at":"2022-09-18T13:00:46Z",
  "id":1250266176,
  "issue":714,
  "node_id":"IC_kwDOD6Q_ss5KhYxA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-18T13:00:46Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I'm going to merge this because several PRs need it.",
  "created_at":"2022-09-18T15:36:12Z",
  "id":1250332957,
  "issue":714,
  "node_id":"IC_kwDOD6Q_ss5KhpEd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-18T15:36:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@agoose77, you've already weighed in on what you think in the issue comments, but I'm going to always ask for reviews from now on, even if they're easy. If you agree with this fix, you can also merge it. (What I'm saying here is that I won't be making any more changes, which is what \"request to review\" is supposed to mean.)",
  "created_at":"2022-09-19T20:56:55Z",
  "id":1251539027,
  "issue":715,
  "node_id":"IC_kwDOD6Q_ss5KmPhT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-19T20:56:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I just remembered that I'll need to make a corresponding v4 version of this PR. I'll do that soon.",
  "created_at":"2022-09-19T21:59:46Z",
  "id":1251607494,
  "issue":715,
  "node_id":"IC_kwDOD6Q_ss5KmgPG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-19T21:59:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This passed tests, but I think it was because it ran before the boost-histogram update. Therefore, we'll do the same procedure:\r\n\r\n  1. #719 gets approved and merged into `main`.\r\n  2. This PR updates to `main`.\r\n  3. The tests re-run and pass (again).\r\n  4. This PR gets approved and merged into `main`.",
  "created_at":"2022-09-19T23:11:40Z",
  "id":1251661767,
  "issue":715,
  "node_id":"IC_kwDOD6Q_ss5KmtfH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-19T23:11:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"If #721 gets merged, I'm happy with this being merged.",
  "created_at":"2022-09-20T07:42:54Z",
  "id":1251966119,
  "issue":715,
  "node_id":"IC_kwDOD6Q_ss5Kn3yn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T07:42:54Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll take that as an approval of both #721 and #715 and enable auto-merge.",
  "created_at":"2022-09-20T16:37:14Z",
  "id":1252615045,
  "issue":715,
  "node_id":"IC_kwDOD6Q_ss5KqWOF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T16:37:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The two that failed are both network issues. I'm going to assume that it was a one-time thing. When the restarted tests pass, I'll merge this PR.",
  "created_at":"2022-09-20T21:12:15Z",
  "id":1252918410,
  "issue":716,
  "node_id":"IC_kwDOD6Q_ss5KrgSK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T21:12:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It is my belief that this will pass tests when #719 is merged in. Therefore,\r\n\r\n  1. #719 gets approved and merged into `main`.\r\n  2. This PR updates to `main`.\r\n  3. The tests re-run and pass.\r\n  4. This PR gets approved and merged into `main`.",
  "created_at":"2022-09-19T23:05:33Z",
  "id":1251657586,
  "issue":717,
  "node_id":"IC_kwDOD6Q_ss5Kmsdy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-19T23:05:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"If #718 gets merged, I'm happy with this being merged. It looks like an unrelated test deprecation warning that's failing the CI.",
  "created_at":"2022-09-20T07:44:21Z",
  "id":1251967751,
  "issue":717,
  "node_id":"IC_kwDOD6Q_ss5Kn4MH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T07:44:21Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! #717 and #718 are in sync, so I'll merge them both.",
  "created_at":"2022-09-20T21:17:03Z",
  "id":1252922639,
  "issue":717,
  "node_id":"IC_kwDOD6Q_ss5KrhUP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T21:17:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think this passed accidentally because the new boost-histogram isn't getting picked up by the `main-v4` branch. Nevertheless, let's keep the two PRs in step with each other.\r\n\r\n  1. #720 gets approved and merged into `main-v4`.\r\n  2. This PR updates to `main-v4`.\r\n  3. The tests re-run and pass (again).\r\n  4. This PR gets approved and merged into `main-v4`.",
  "created_at":"2022-09-19T23:07:31Z",
  "id":1251658947,
  "issue":718,
  "node_id":"IC_kwDOD6Q_ss5KmszD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-19T23:07:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I see the point and the value of this suggested change, but we'll have to manually make the same change to #717. And note that #717 is the forward-looking PR that goes into `main` (v5); this is a backport to `main-v4` (v4).\r\n\r\nIf you apply this to both PRs, I'll take it. Otherwise, we can leave it in both.\r\n\r\n(By the way, I'm assuming that your refactor is readability-motivated, and I agree that it's an improvement. If it's performance-motivated, I wouldn't worry about it because it's inside of an unlikely `if` branch.)",
  "created_at":"2022-09-20T16:36:15Z",
  "id":1252613345,
  "issue":718,
  "node_id":"IC_kwDOD6Q_ss5KqVzh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T16:36:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, it's primarily safety - we will never have the regex go out of sync with the if branch, but closely followed by readability.",
  "created_at":"2022-09-20T17:31:45Z",
  "id":1252683400,
  "issue":718,
  "node_id":"IC_kwDOD6Q_ss5Kqm6I",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T17:31:45Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! #717 and #718 are in sync, so I'll merge them both.",
  "created_at":"2022-09-20T21:17:05Z",
  "id":1252922675,
  "issue":718,
  "node_id":"IC_kwDOD6Q_ss5KrhUz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T21:17:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Specifically, it's for this:\r\n\r\nhttps://github.com/scikit-hep/boost-histogram/blob/0c8659c202b0e4c5c2a061d26a4de6d526b01676/src/boost_histogram/_internal/hist.py#L617-L625\r\n\r\nOkay sure, the Python 3.6 case fails because it's getting an older boost-histogram. I'm going to add a fallback to this and #720.",
  "created_at":"2022-09-19T22:59:15Z",
  "id":1251653853,
  "issue":719,
  "node_id":"IC_kwDOD6Q_ss5Kmrjd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-19T22:59:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"FYI, the reason for the old bh on conda is that 3.6 is dead there, they aren't building new packages for 3.6, and haven't in a while. I would not use conda for 3.6 anymore, it's historical and unsupported at this point. 3.7 might also be following soon(ish).\r\n",
  "created_at":"2022-09-20T15:33:39Z",
  "id":1252535061,
  "issue":719,
  "node_id":"IC_kwDOD6Q_ss5KqCsV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T15:33:39Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll take that as an approval of both #721 and #715 and enable auto-merge.",
  "created_at":"2022-09-20T16:37:20Z",
  "id":1252615225,
  "issue":721,
  "node_id":"IC_kwDOD6Q_ss5KqWQ5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T16:37:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Kicking CI",
  "created_at":"2022-09-20T17:58:09Z",
  "id":1252712657,
  "issue":721,
  "node_id":"IC_kwDOD6Q_ss5KquDR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T17:58:09Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski do you know why we slice the values for string categories here?\r\n\r\nhttps://github.com/scikit-hep/uproot5/blame/092bc679b215d565652c23b04a782dbf6ba38e3a/src/uproot/behaviors/TH1.py#L317-L318\r\n\r\nI assume this is the reason for this error; the subsequent branch for weight / non-weight handling erroneously chooses the non-weight path:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blame/092bc679b215d565652c23b04a782dbf6ba38e3a/src/uproot/behaviors/TH1.py#L321-L325 \r\n\r\nRegardless of what we're supposed to do with `sumw2` in the first sample, we probably ought to use the type of `storage` to determine whether to set weights, because that will better reflect our intention.",
  "created_at":"2022-09-20T07:56:25Z",
  "id":1251980527,
  "issue":722,
  "node_id":"IC_kwDOD6Q_ss5Kn7Tv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T07:56:33Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think this is related to the problems I am facing in https://github.com/scikit-hep/uproot5/pull/764. I am looking into it and will probably also solve this issue.\r\n\r\nThis code reproduces the problem with the weights. I will add a test based on the code below.\r\n\r\n```\r\nimport hist\r\n\r\nprint(f'{hist.__version__=}')\r\nimport uproot\r\n\r\nprint(f'{uproot.__version__=}')\r\nimport numpy as np\r\nimport ROOT\r\nimport pytest\r\n\r\nnewfile = \"tmp.root\"\r\nh = ROOT.TH1D(\"h\", \"h\", 10, 0., 1.)\r\nh.FillRandom(\"gaus\", 10000)\r\n\r\nassert len(h.GetSumw2()) == 0  # it is supposed to be zero\r\n\r\nfout = ROOT.TFile(newfile, \"RECREATE\")\r\nh.Write()\r\nfout.Close()\r\n\r\n# open same hist with uproot\r\nwith uproot.open(newfile) as fin:\r\n    h1 = fin[\"h\"]\r\n\r\nassert len(h1.axes) == 1\r\nassert h1.axis(0).edges().tolist() == pytest.approx([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\r\nassert len(h1.member(\"fSumw2\")) == 0\r\n\r\n# convert to hist\r\nh2 = h1.to_hist()\r\nassert str(h2.storage_type) == \"<class 'boost_histogram.storage.Double'>\"\r\nassert len(h2.axes) == 1\r\n# why is this failing? returns [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\r\nassert h2.axes[0].edges.tolist() == pytest.approx([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\r\n\r\n# write and read again\r\nwith uproot.recreate(newfile) as fout2:\r\n    fout2[\"h\"] = h2\r\n\r\nwith uproot.open(newfile) as fin2:\r\n    h3 = fin2[\"h\"]\r\n\r\n# problems start here\r\nassert len(h3.axes) == 1\r\nassert h3.axis(0).edges().tolist() == pytest.approx([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\r\n\r\n# ERROR\r\nassert len(h3.member(\"fSumw2\")) == 0, \"this should be 0, but it's not!\"\r\n```",
  "created_at":"2022-10-31T21:35:49Z",
  "id":1297715059,
  "issue":722,
  "node_id":"IC_kwDOD6Q_ss5NWY9z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-31T21:51:25Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"This is done: PR #749.",
  "created_at":"2022-11-10T23:30:42Z",
  "id":1311040410,
  "issue":723,
  "node_id":"IC_kwDOD6Q_ss5OJOOa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-10T23:30:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm not sure I like this feature.",
  "created_at":"2022-09-20T15:01:31Z",
  "id":1252489374,
  "issue":724,
  "node_id":"IC_kwDOD6Q_ss5Kp3ie",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-20T15:01:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski this feature has been added to `_dask.py` and this PR can be closed.",
  "created_at":"2022-11-09T14:37:29Z",
  "id":1308861442,
  "issue":725,
  "node_id":"IC_kwDOD6Q_ss5OA6QC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-09T14:37:29Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"Issue #727 and its associated PRs has added all of the people indicated above to all-contributors.",
  "created_at":"2022-09-22T19:22:32Z",
  "id":1255453062,
  "issue":726,
  "node_id":"IC_kwDOD6Q_ss5K1LGG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:22:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know by 15:00 Geneva-time tomorrow (Friday) if anyone has any objections, otherwise, I'll merge it at that time.",
  "created_at":"2022-09-23T00:58:34Z",
  "id":1255697535,
  "issue":726,
  "node_id":"IC_kwDOD6Q_ss5K2Gx_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-23T00:58:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hey @jpivarski, here is my ORCID : https://orcid.org/0000-0001-5220-8725",
  "created_at":"2022-09-23T04:27:37Z",
  "id":1255788734,
  "issue":726,
  "node_id":"IC_kwDOD6Q_ss5K2dC-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-23T04:30:18Z",
  "user":"MDQ6VXNlcjUwNTc3ODA5"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Here's my ORCHID: https://orcid.org/0000-0001-8593-722X",
  "created_at":"2022-09-23T09:07:39Z",
  "id":1255968635,
  "issue":726,
  "node_id":"IC_kwDOD6Q_ss5K3I97",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-23T09:07:39Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"Looks like everything is done. Thanks, everyone!",
  "created_at":"2022-09-23T13:45:23Z",
  "id":1256235977,
  "issue":726,
  "node_id":"IC_kwDOD6Q_ss5K4KPJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-23T13:45:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Here's the \"cite this repository\" drop-down on the top-right of the GitHub homepage:\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/191975139-2005e38d-b8d0-4d34-9fc9-639b78d93d03.png)\r\n\r\nAnd this is what Zenodo looks like when you follow the DOI link (i.e. the [![DOI 10.5281/zenodo.4340632](https://zenodo.org/badge/DOI/10.5281/zenodo.4340632.svg)](https://doi.org/10.5281/zenodo.4340632) badge):\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/191976486-d5bf4ce6-75aa-4506-b964-31898cf288f6.png)\r\n",
  "created_at":"2022-09-23T13:55:32Z",
  "id":1256249038,
  "issue":726,
  "node_id":"IC_kwDOD6Q_ss5K4NbO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-23T13:55:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @veprbl for code and infra",
  "created_at":"2022-09-22T19:15:01Z",
  "id":1255445226,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1JLq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:15:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/728) to add @veprbl! :tada:",
  "created_at":"2022-09-22T19:15:10Z",
  "id":1255445377,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1JOB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:15:10Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @nikoladze for code",
  "created_at":"2022-09-22T19:17:13Z",
  "id":1255447365,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1JtF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:17:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/729) to add @nikoladze! :tada:",
  "created_at":"2022-09-22T19:17:21Z",
  "id":1255447509,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1JvV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:17:21Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @klieret for docs",
  "created_at":"2022-09-22T19:18:10Z",
  "id":1255448349,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1J8d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:18:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/730) to add @klieret! :tada:",
  "created_at":"2022-09-22T19:18:18Z",
  "id":1255448491,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1J-r",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:18:18Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @dcervenkov for code",
  "created_at":"2022-09-22T19:19:00Z",
  "id":1255449230,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1KKO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:19:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/731) to add @dcervenkov! :tada:",
  "created_at":"2022-09-22T19:19:08Z",
  "id":1255449379,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1KMj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:19:08Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @beojan for code",
  "created_at":"2022-09-22T19:19:55Z",
  "id":1255450240,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1KaA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:19:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/732) to add @beojan! :tada:",
  "created_at":"2022-09-22T19:20:03Z",
  "id":1255450392,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1KcY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:20:03Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"This is everybody that I know of. Sorry for the extra emails!",
  "created_at":"2022-09-22T19:21:28Z",
  "id":1255451992,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1K1Y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:21:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski if you've added me to the CITATION.cff intentionally, and that is expected to be synchronised with all-contributors, then I should probably be added too. Otherwise, if you want to wait until I've gotten around to bigger contributions, no problem!",
  "created_at":"2022-09-22T19:24:52Z",
  "id":1255455498,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1LsK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:24:52Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"\ud83e\udd26\u200d\u2642\ufe0f\r\n\r\nhttps://github.com/all-contributors please add @agoose77 for code and maintenance\r\n\r\nThanks!",
  "created_at":"2022-09-22T19:30:19Z",
  "id":1255460778,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1M-q",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2022-09-22T19:30:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Does the bot only respond to open issues/PRs?\r\n\r\n@all-contributors please add @agoose77 for code and maintenance\r\n\r\nNo, I just pasted a URL instead of the at-sign-name. This should do it.",
  "created_at":"2022-09-22T19:32:08Z",
  "id":1255462518,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1NZ2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:32:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/733) to add @agoose77! :tada:",
  "created_at":"2022-09-22T19:32:17Z",
  "id":1255462651,
  "issue":727,
  "node_id":"IC_kwDOD6Q_ss5K1Nb7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-22T19:32:17Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"I see that this is not quite ready for review yet; there are still some tests to work through, and I made some suggestions in the code.\r\n\r\nAnother thing I should point out is that we no longer need a `_libraries_lazy` because we no longer have an `uproot.lazy`:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/366bbe85d29bfc8064a35fb5edd3d2da39a597f2/src/uproot/interpretation/library.py#L1121-L1160\r\n\r\nThis was for the set of libraries that can be used in `uproot.lazy`; only Awkward. There are no longer any references to `_libraries_lazy` in the codebase, so you can safely delete it.",
  "created_at":"2022-10-20T17:27:20Z",
  "id":1285906458,
  "issue":734,
  "node_id":"IC_kwDOD6Q_ss5MpWAa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-20T17:27:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I attached issue #668 to this PR because this PR would fix that issue.",
  "created_at":"2022-11-10T23:29:01Z",
  "id":1311039314,
  "issue":734,
  "node_id":"IC_kwDOD6Q_ss5OJN9S",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-10T23:29:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"2 tests remain, I will solve them once awkard-pandas has a new release.\r\n1 test (relating to HTTP sockets) was failing on my local machine, I don't think it is because of something I did. Could be that the service is temporarily down.\r\n\r\ncc @jpivarski ",
  "created_at":"2022-11-15T18:36:22Z",
  "id":1315715613,
  "issue":734,
  "node_id":"IC_kwDOD6Q_ss5ObDod",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-15T18:36:22Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"As a quick-fix (without looking into it), I restarted the CI jobs.\r\n\r\nI'll return to this PR soon.",
  "created_at":"2022-11-15T19:31:23Z",
  "id":1315773115,
  "issue":734,
  "node_id":"IC_kwDOD6Q_ss5ObRq7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-15T19:31:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Forgot to add awkward-pandas to dependancies. Solving this right now.",
  "created_at":"2022-11-16T18:08:53Z",
  "id":1317442585,
  "issue":734,
  "node_id":"IC_kwDOD6Q_ss5OhpQZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-16T18:08:53Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"The failure now is because this PR branch needs to be brought up to date with `main`, specifically #784.",
  "created_at":"2022-11-28T18:48:18Z",
  "id":1329583954,
  "issue":734,
  "node_id":"IC_kwDOD6Q_ss5PP9dS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-28T18:48:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Huh, I thought I'd installed pre-commit on this repo already. Oops!",
  "created_at":"2022-09-26T14:18:28Z",
  "id":1258112059,
  "issue":736,
  "node_id":"IC_kwDOD6Q_ss5K_UQ7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-26T14:18:28Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I think this PR and #741 are both attempting to do the same thing. We both dropped the \"`_v2`\" everywhere, but I think our other commits differ.",
  "created_at":"2022-09-30T15:45:53Z",
  "id":1263736444,
  "issue":736,
  "node_id":"IC_kwDOD6Q_ss5LUxZ8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T15:45:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think we can close this; I started work on dropping the v2 prefix, but I wasn't sure if the plan was to make uproot5 require `awkward >= 2.0`. :)",
  "created_at":"2022-09-30T16:29:31Z",
  "id":1263780580,
  "issue":736,
  "node_id":"IC_kwDOD6Q_ss5LU8Lk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T16:29:31Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> I wasn't sure if the plan was to make uproot5 require `awkward >= 2.0`\r\n\r\nIt will require it. I probably need to make the pyproject.toml changes you made here in #741.",
  "created_at":"2022-09-30T18:30:15Z",
  "id":1263890036,
  "issue":736,
  "node_id":"IC_kwDOD6Q_ss5LVW50",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T18:30:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"just write to a local file and `xrdcp`",
  "created_at":"2022-09-28T15:20:07Z",
  "id":1261072646,
  "issue":738,
  "node_id":"IC_kwDOD6Q_ss5LKnEG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-28T15:20:07Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes, that's the workaround I'm currently using, but I'm wondering if one could use `iterate` and `recreate` to process large file(s) that cannot fit into local storage by chunks.",
  "created_at":"2022-09-28T15:40:51Z",
  "id":1261099223,
  "issue":738,
  "node_id":"IC_kwDOD6Q_ss5LKtjX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-28T15:40:51Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah, we're not _planning_ on supporting it, though there's a placeholder for it: the local file-based Sink is implemented as an abstraction that could (in principle) be replaced by a remote file Sink.\r\n\r\nIf it is implemented, I don't see a way to get the performance reasonable for any format except RNTuple. TTrees, for instance, involve a lot of seeking forward and back to keep the file state valid. For a remote file, that means lots of round-trip communications, which is bad for low-latency environments. Maybe we could let the file state get invalid, but we do have to physically write some things because we can't keep it all in memory and dump at the end. (Or if you can, make a memory file and send it all at once!)\r\n\r\nFor RNTuple, it's plausible, since all information that isn't known until a time $T$ in writing is written after data written at time $t < T$. That is, all of the \"number of entries,\" \"where to find chunks,\" etc. are in a footer that gets written last or repeatedly re-written. So the RNTuple part could plausibly be written over a low-latency network well, but it's embedded within traditional ROOT I/O that will still require some seeking back and forth.\r\n\r\nIf we do implement remote Sinks, so that RNTuple can take advantage of them (and we let TTree be inefficient), we'd probably want to do it through https://github.com/CoffeaTeam/fsspec-xrootd to simplify the interface.\r\n\r\nBottom line: not planning on it, but we _could_ change our plans, depending on how much RNTuple improves the situation.\r\n\r\n(I'm going to make this a Discussion, rather than an Issue.)",
  "created_at":"2022-09-28T19:23:14Z",
  "id":1261367914,
  "issue":738,
  "node_id":"IC_kwDOD6Q_ss5LLvJq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-28T19:23:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"c054c716f048a99ab0174ff90a1939068861c6c8 is the one thing that was in #736 but not this PR. Now this one can supercede it.",
  "created_at":"2022-09-30T20:01:41Z",
  "id":1263969848,
  "issue":741,
  "node_id":"IC_kwDOD6Q_ss5LVqY4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T20:01:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I'm looking at the CI issues unless you'd prefer to.",
  "created_at":"2022-09-30T20:04:53Z",
  "id":1263972317,
  "issue":741,
  "node_id":"IC_kwDOD6Q_ss5LVq_d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T20:04:53Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski oh, simple - we just need #742. I was just re-implementing that.",
  "created_at":"2022-09-30T20:08:49Z",
  "id":1263975805,
  "issue":741,
  "node_id":"IC_kwDOD6Q_ss5LVr19",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T20:08:49Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"This PR was not targeting Uproot 5.0.0rc4. That was supposed to be only dropping Python 3.6 and nothing else. There's no rush on this PR or 5.0.0rc5.",
  "created_at":"2022-09-30T20:24:16Z",
  "id":1263990090,
  "issue":741,
  "node_id":"IC_kwDOD6Q_ss5LVvVK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T20:24:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"3d37ed74a9689e941222f6af950c627b75a6b33b installs Awkward from source to give this PR a fair test. (It's supposed to demonstrate testing against Awkward `main`.) That CI step, marked \"TEMPORARY!\" should be removed before this PR is done.\r\n\r\nThe 3.11 tests are failing because they can't find a suitable version of Awkward Array _before_ the latest wheel is replaced by the \"TEMPORARY!\" CI step. I think we can just ignore those.",
  "created_at":"2022-09-30T20:36:59Z",
  "id":1264001259,
  "issue":741,
  "node_id":"IC_kwDOD6Q_ss5LVyDr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T20:36:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Any reason not to do this, @henryiii? (Have I missed anything in the update?)\r\n\r\nIf I merge this now, it will be the only change relative to 5.0.0rc3: https://github.com/scikit-hep/uproot5/compare/v5.0.0rc3...main\r\n\r\nThen I can immediately release 5.0.0rc4 and that will be the only change between versions: dropping Python 3.6 (and the version number).\r\n\r\nThis doesn't affect the `main-v4` branch, though we may need to remove Python 3.6 from that, too, so that it can be supported in the long term. (It's a separate issue, not related to this PR.)",
  "created_at":"2022-09-30T15:50:18Z",
  "id":1263740959,
  "issue":742,
  "node_id":"IC_kwDOD6Q_ss5LUygf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T15:50:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Here's the issue that forced me to drop macOS Python 3.11 testing:\r\n\r\nhttps://github.com/actions/setup-python/issues/512",
  "created_at":"2022-09-30T19:02:25Z",
  "id":1263917639,
  "issue":742,
  "node_id":"IC_kwDOD6Q_ss5LVdpH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T19:02:25Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"IMO, I believe that's a OpenSSL GitHub Actions bug, not a Python 3.11 one; if you want to make the SSL tests skippable then `@pytest.mark.ssl` + add `ssl` as a mark + `-m \"not ssl\"` would be a better way to skip them.",
  "created_at":"2022-09-30T19:38:23Z",
  "id":1263949045,
  "issue":742,
  "node_id":"IC_kwDOD6Q_ss5LVlT1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-09-30T19:38:39Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! I'll take that as approval of the backport (#747) as well.",
  "created_at":"2022-10-05T19:22:40Z",
  "id":1268857242,
  "issue":746,
  "node_id":"IC_kwDOD6Q_ss5LoTma",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-05T19:22:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This PR solves issue #723 ,",
  "created_at":"2022-10-06T15:28:54Z",
  "id":1270271078,
  "issue":749,
  "node_id":"IC_kwDOD6Q_ss5Ltsxm",
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "total_count":1
  },
  "updated_at":"2022-10-06T15:28:54Z",
  "user":"MDQ6VXNlcjUwNTc3ODA5"
 },
 {
  "author_association":"MEMBER",
  "body":"Oh wait\u2014one more thing: AwkwardForth should be used by default now.\r\n\r\nWhenever a new AsObjects is constructed, it gets a `_forth = True` attribute (or that can be a class object attribute). Users who know the secret code can opt-out by setting an instance's `_forth = False`, but that's an exceptional, unusual situation.",
  "created_at":"2022-10-11T13:44:39Z",
  "id":1274709851,
  "issue":749,
  "node_id":"IC_kwDOD6Q_ss5L-odb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-11T13:44:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"e946dea8b3db97e6929da6f46fb61c0847722bb7 was the last step needed for this PR, apart from getting the tests to work again. The last error in the logs (one of 6 errors total) was due to some AwkwardForth cases being declared as NotImplementedError. In general, NotImplementedError for AwkwardForth can be replaced with turning off the Forth generation and proceeding with the old code path. So you don't have to handle more Forth cases, just ensure that it gives up appropriately if it can't proceed.\r\n\r\nThat will \"hide\" these cases, since they're no longer revealed by NotImplementedError, which trades the good thing of getting Uproot to run for the bad thing of not knowing which code path was taken in a given case. In practice, this will mean that some data types will still have the old deserialization speed and it becomes harder to debug performance issues.\r\n\r\nDoes the \"give up\" flag leave any permanent indicator on the AsObjects object, so that we have something to check if a particular deserialization seems to be very slow?\r\n\r\nBy the way, in the case that I looked at, TClonesArray, it would be very hard to write Forth. TClonesArray is a messy, complex thing.\r\n\r\nBut 9a7845e25b82c3415fc28ebaa6f4338313749429 was a good thing to have fixed. So I guess not all of these are things to be skipped. (I only looked at the last one in the logs.)",
  "created_at":"2022-10-17T18:18:45Z",
  "id":1281290659,
  "issue":749,
  "node_id":"IC_kwDOD6Q_ss5MXvGj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-17T18:18:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hey, so some of the tests are failing because of the difference is Awkward Array layout. I will have to figure out those and there's one that seems to be reading strings incorrectly. From what I expected, these errors are much more tame.",
  "created_at":"2022-10-17T18:21:26Z",
  "id":1281293582,
  "issue":749,
  "node_id":"IC_kwDOD6Q_ss5MXv0O",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-17T18:21:26Z",
  "user":"MDQ6VXNlcjUwNTc3ODA5"
 },
 {
  "author_association":"MEMBER",
  "body":"A different layout is significant. I guess my shortcut of checking only the last failure was misrepresentative.\r\n\r\nWhen you're dealing with the NotImplementedError, be sure to do all of the cases in which the AwkwardForth is deliberately not implemented. I'm going to call the TClonesArray one hopeless, as are any with memberwise serialization, but if one of them looks like it could be easy and just fills in a case we didn't have a test for before, then at least put a note in a comment that such-and-such a test reaches that part of the code and could be implemented someday.",
  "created_at":"2022-10-17T18:27:25Z",
  "id":1281300507,
  "issue":749,
  "node_id":"IC_kwDOD6Q_ss5MXxgb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-17T18:27:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I should have expanded on the comment. The difference that I spotted was a spelling mistake and some layout parameters that should not have been there. The difference in layout is not in the logical structure (at least I haven't spotted any).\r\n ",
  "created_at":"2022-10-17T19:04:35Z",
  "id":1281339691,
  "issue":749,
  "node_id":"IC_kwDOD6Q_ss5MX7Er",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-17T19:04:35Z",
  "user":"MDQ6VXNlcjUwNTc3ODA5"
 },
 {
  "author_association":"MEMBER",
  "body":"With the latest Awkward Array, all tests pass except for the two that you knew about.",
  "created_at":"2022-10-29T01:58:37Z",
  "id":1295685111,
  "issue":749,
  "node_id":"IC_kwDOD6Q_ss5NOpX3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-29T01:58:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"if you are using uproot maybe just use it all the way? `hadd` is very relaxed about error and we don't know what it does exactly",
  "created_at":"2022-10-06T17:36:02Z",
  "id":1270452682,
  "issue":750,
  "node_id":"IC_kwDOD6Q_ss5LuZHK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-06T17:36:02Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"True, but we want to be able to deal with ROOT files that come from any acceptable sources, and `hadd` is an acceptable (and widely used) source.\r\n\r\nThis is a follow-up of a conversation we were having on https://gitter.im/Scikit-HEP/uproot .\r\n\r\nThis file has a zero-length TBasket, which is a strange thing (probably something you'd only get from `hadd`), but nobody said that isn't allowed. The problem is that Uproot isn't attempting to read what I think should be the correct range for the empty bytes, 101498:101498. Instead, it's reading a zero-length thing from 101577:101577. We could make `Cursor` not even try to read data when the range has zero length, but then we wouldn't have caught this error: why is it attempting to start at 101577? I'll go check it out.",
  "created_at":"2022-10-06T17:41:09Z",
  "id":1270458111,
  "issue":750,
  "node_id":"IC_kwDOD6Q_ss5Luab_",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-10-06T17:41:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Aha: this is an \"embedded TBasket\" (usually made if the file closes early, as a way to recover what would otherwise be lost data), and these are usually serialized with a second TKey. In addition to having no data, this doesn't have a second TKey. (Maybe the second TKey is only ever written when its data are written, and that didn't happen this time because there's no data?)\r\n\r\nThe unexpected seek point, 101577, is 79 bytes (the first TKey's size) after the end of the first TKey, 101498.\r\n\r\nNow that it's clear where that number is coming from, I have no qualms about skipping a read of length zero, which would be correct whether a file has a second TKey or not.",
  "created_at":"2022-10-06T17:51:53Z",
  "id":1270469267,
  "issue":750,
  "node_id":"IC_kwDOD6Q_ss5LudKT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-06T17:51:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@agoose77, if you approve either one of these (#751 or #752), I'll consider them both approved and will auto-merge them both. Thanks!",
  "created_at":"2022-10-06T18:10:15Z",
  "id":1270488925,
  "issue":751,
  "node_id":"IC_kwDOD6Q_ss5Luh9d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-06T18:10:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@agoose77, since this is an add-on for #751, I'll just be squash-and-merging this.",
  "created_at":"2022-10-06T23:13:54Z",
  "id":1270831408,
  "issue":753,
  "node_id":"IC_kwDOD6Q_ss5Lv1kw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-06T23:13:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Since the column projection logic is handled on the `dask-awkward` side I think a simple sanity check would suffice. Just do a read followed by a `getitem` and see that you get what is expected\r\n```python\r\ndata = uproot.dask(..., library=\"ak\")\r\ndata = data[[\"branch1\", \"branch2\"]]\r\nresult = data.compute()\r\nassert ... # check that result is equal to the same thing but not dask\r\n```\r\nThis doesn't test that column projection takes place, but it may potentially catch future bugs in the workflow that column projection targets (read followed by getitem). It'd be a good idea to add a test that triggers both `_UprootRead` and `_UprootOpenAndRead`",
  "created_at":"2022-10-11T16:46:24Z",
  "id":1274987583,
  "issue":755,
  "node_id":"IC_kwDOD6Q_ss5L_sQ_",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-10-11T16:55:47Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"duplicate of \r\n- https://github.com/scikit-hep/uproot5/issues/750\r\n?\r\n\r\nwhat version of uproot are you on",
  "created_at":"2022-10-12T17:40:55Z",
  "id":1276524110,
  "issue":756,
  "node_id":"IC_kwDOD6Q_ss5MFjZO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-12T17:42:32Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a further elaboration of the problem, or a different but similar problem, or something. It's following up on Gitter (@johanwulff is the same author).",
  "created_at":"2022-10-12T17:44:21Z",
  "id":1276527621,
  "issue":756,
  "node_id":"IC_kwDOD6Q_ss5MFkQF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-12T17:44:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"So the uproot version is 4.3.7 so it should include the latest bugfix which also explains why the error message is different now.",
  "created_at":"2022-10-13T16:15:58Z",
  "id":1277866731,
  "issue":756,
  "node_id":"IC_kwDOD6Q_ss5MKrLr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-13T16:15:58Z",
  "user":"MDQ6VXNlcjQ4MjgwNzYx"
 },
 {
  "author_association":"MEMBER",
  "body":"I labeled this as a bug from the API design perspective. At the very least, if you do not accept the keyword `mode`, then uproot.open should raise an error for every unknown keyword that is passed as `**options`.",
  "created_at":"2022-10-18T11:44:14Z",
  "id":1282254594,
  "issue":758,
  "node_id":"IC_kwDOD6Q_ss5MbacC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-18T11:44:43Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"You're looking for up.recreate?",
  "created_at":"2022-10-18T12:46:58Z",
  "id":1282333082,
  "issue":758,
  "node_id":"IC_kwDOD6Q_ss5Mbtma",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-18T12:46:58Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> I labeled this as a bug from the API design perspective. At the very least, if you do not accept the keyword `mode`, then uproot.open should raise an error for every unknown keyword that is passed as `**options`.\r\n\r\nThis is true. Unhandled options ought to raise an error, and I think it was originally designed to do that (always popping that dict and dealing with `len != 0` at the end). What happened here is unintended behavior.\r\n\r\n--------\r\n\r\nOn handling `mode`, we have to make choices between being Python-like and being ROOT-like: see also the handling of cycle numbers in TDirectories. For opening files, there's such a vast difference between opening a ROOT file for reading and opening a ROOT file for writing, that we took the ROOT-like names for these, `update` and `recreate`, and made them separate functions instead of arguments to a single function.\r\n\r\nTo clarify \"such a vast difference between opening a ROOT file for reading and opening a ROOT file for writing,\" I specifically mean ROOT files because of the way that they act as filesystems in a file. (The same is true of HDF5, and h5py also does not follow Python's `open` semantics; not exactly, anyway.) The \"I\" and the \"O\" implementations in Uproot have almost zero overlap, and so driving them with the same object would be inviting bugs: the object would have to remember whether it's a reader or a writer and never get these functionalities crossed. Thus, we have `ReadOnlyDirectory` and `WritableDirectory`, etc.\r\n\r\nAlthough now that I think about it, `uproot.open` is just a function, not a class constructor. We could have it return different object types, depending on the value of a `mode` parameter:\r\n\r\n  * `mode=\"r\"` (default) would do what it currently does, returning a `ReadOnlyDirectory`,\r\n  * `mode=\"w\"` would effectively call `uproot.recreate`, returning a `WritableDirectory`, and\r\n  * `mode=\"a\"` (or `mode=\"r+w\"`?) would effectively call `uproot.update`. However, I don't know whether this should be `\"a\"` or `\"r+w\"` because we're neither talking about appending nor overwriting: the access is higher-level than the stream of bytes (or Unicode code-points) that `open` usually returns.\r\n\r\nOn the third hand, I'm thinking this wouldn't be a good interface after all. It looks like Python's `open`, but `uproot.open` takes a lot more arguments and most of those arguments have no meaning for writable files. Adding this way of calling `uproot.open` to get `uproot.recreate` introduces ambiguity about what to do with the `http_handler` and such (writable files are not writable over a network).\r\n\r\nSince readers and writers have such different feature sets, creating them with different functions more clearly separates the ways you can configure those different feature sets.\r\n\r\nBut maybe, in addition to raising errors on unrecognized `options`, we should recognize `mode` and return a helpful error message: like, if the user passes `\"w\"`, we can say to use `uproot.recreate`. A user unfamiliar with ROOT wouldn't have guessed that name.",
  "created_at":"2022-10-18T17:37:15Z",
  "id":1282769367,
  "issue":758,
  "node_id":"IC_kwDOD6Q_ss5MdYHX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-18T17:37:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> On the third hand\r\n\r\n:-)\r\n\r\n> I'm thinking this wouldn't be a good interface after all. It looks like Python's open, but uproot.open takes a lot more arguments and most of those arguments have no meaning for writable files. Adding this way of calling uproot.open to get uproot.recreate introduces ambiguity about what to do with the http_handler and such (writable files are not writable over a network).\r\n\r\nI can only disagree from my naive user perspective.\r\n\r\nWe want to avoid surprise, no? uproot.open looks at first glance like the familiar builtin open. Yes, I am ignoring all the complicated keywords that it has, which I never needed so far. You are telling me, that the familiar pattern that I learned - I can use open to read and write files with mode=\"r\" and mode=\"w\" - only works halfway. Now I have to remember that this familiar pattern does not work with uproot. The pattern is broken and that's extra mental load for me.\r\n\r\nThe Zen of Python says: There should be one-- and preferably only one --obvious way to do it. For a ROOT user, it may be obvious to call recreate, but using \"recreate\" for writing was already artificial and pattern breaking in the ROOT days. Using `open` for reading and writing was already established standard in C and C++ when ROOT was written, see e.g. https://cplusplus.com/reference/cstdio/fopen. Unfortunately, the ROOT authors ignored all established standards when designing ROOT.\r\n\r\nIn general, I believe that design should be guided - but not dictated - by implementation. The implementation follows the design, not the other way round. I think it is fair to raise an error in uproot.open when users try to pass options that don't work for writing files. I haven't looked into the details, maybe you are right and mixing all the options of recreate into open would be bad, but maybe you don't have to do that. Maybe you can have a basic `uproot.open` that works like the builtin open and covers 99% of simple use cases and a `read` or `load` (strawman name) which has all customization options specific for reading files. In the Python stdlib, they often offer an expert-level function with many options and full control, and a simple interface for the masses.\r\n\r\nIf that still does not convince you, then please use the signature `uproot.open(path, *, ...)` with the star so that positional arguments after `path` are an error and raise an error when the user passes `mode` as a keyword.",
  "created_at":"2022-10-19T21:42:26Z",
  "id":1284604627,
  "issue":758,
  "node_id":"IC_kwDOD6Q_ss5MkYLT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-19T21:44:16Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"On Uproot's open/recreate split: I believe this is a case that was _guided_ by implementation. The separation between read-code and write-code is pretty stark; arguably, they could have been two libraries. (Uproot and Downroot?) Maybe the original problem was naming the entry function `open`, though Uproot existed for several years before it had any writing capabilities at all, or any plans to add them.\r\n\r\nOn ROOT's original TFile: I'm not convinced that a function that opens a filesystem-like database should follow the semantics of a byte- or codepoint-level `open` function; they're operating on very different levels of abstraction. Looking around at some libraries, I see that quite a few of them do, though `mode=\"a\"` is a strange thing to think about when the interface is an unsorted set of key-value pairs. Maybe it means you can't `del` objects, but can add new ones.\r\n\r\n> If that still does not convince you, then please use the signature `uproot.open(path, *, ...)` with the star so that positional arguments after `path` are an error and raise an error when the user passes `mode` as a keyword.\r\n\r\nWe should consider marking some arguments as keyword-only across the whole codebase. It wasn't done before because it was written to be compatible with Python 2, but that support was removed a long time ago. Turning arguments into keyword-only arguments is a backward-incompatible change, but that's allowed before the release of version 5.0.0 in December.\r\n\r\nAnd we should find out why extra arguments are not raising errors. I haven't forgotten that.",
  "created_at":"2022-10-19T22:11:54Z",
  "id":1284626232,
  "issue":758,
  "node_id":"IC_kwDOD6Q_ss5Mkdc4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-19T22:11:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Related: https://github.com/scikit-hep/awkward/issues/1805",
  "created_at":"2022-10-18T12:14:19Z",
  "id":1282289028,
  "issue":759,
  "node_id":"IC_kwDOD6Q_ss5Mbi2E",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-18T12:14:19Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"A workaround for my situation is this:\r\n```py\r\nwith uproot.recreate(file) as f:\r\n    for ichunk in range(chunks):\r\n        px = []\r\n        py = []\r\n        for event in generator(chunk):\r\n            px.append(event.px)\r\n            py.append(event.py)\r\n        data = {\"\": ak.zip({\"px\": ak.Array(px), \"py\": ak.Array(py)})}\r\n        if \"tree\" in f:\r\n            f[\"tree\"].extend(data)\r\n        else:\r\n            f[\"tree\"] = data\r\n```\r\n1) It would be great to add this to the docs.\r\n2) It would be great if calling f[\"tree\"].extend() would be enough here, in other words, if the tree is still empty, make `f[\"tree\"].extend(data)` do the same as `f[\"tree\"] = data` if that's technically possible.\r\n3) Thanks for supporting the special case {\"\": ak.zip(...)} correctly.",
  "created_at":"2022-10-18T12:36:12Z",
  "id":1282316566,
  "issue":759,
  "node_id":"IC_kwDOD6Q_ss5MbpkW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-18T12:37:05Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"Adopting\r\n\r\n```python\r\n{\"n\": \"int32\", \"x\": \"n * float32\", \"y\": \"n * float32\"}\r\n```\r\n\r\nwould be hard because `\"n * float32\"` doesn't parse to a valid ak.types.Type, and Uproot is just using Awkward Array here; this request cuts across the boundaries of modular code, and across the boundary between Uproot and Awkward. It could, perhaps, be implemented by adding a flag to the Awkward type-parser to allow identifiers in place of dimension sizes (as long as they're not named `\"var\"`!) and the resulting tree would contain these non-ak.types.Type placeholders, which would then have to be resolved by Uproot across the whole dict, but that makes this a much larger project than you might have been thinking.\r\n\r\nAbout adding your workaround to the docs, I'm trying to figure out how it would fit in. Uproot currently has only one tutorial, the [Getting Started Guide](https://uproot.readthedocs.io/en/latest/basic.html). I'm trying to understand your original problem well enough that this code block would be a helpful solution to the stated problem.\r\n\r\nOh! I get it: you have two Awkward Arrays with `var * float32` type and they happen to align (all of their lists have the same lengths, list by list). You want the resulting TTree to have\r\n\r\n```python\r\n{\"n\": \"int32\", \"x\": \"var * float32\", \"y\": \"var * float32\"}\r\n```\r\n\r\nwith TBranches `x` and `y` both using TBranch `n` as their shared counting branch. This _has_ come up multiple times, and my answer is to fill the TTree with exactly what you've found:\r\n\r\n```python\r\n{\"\": ak.zip({\"x\": x, \"y\": y})}\r\n```\r\n\r\n(though I usually name the outer field, but this is fine).\r\n\r\nThe question for documentation, then, is where it should go in that Getting Started Guide (because creating a new top-level tutorial for this would make people wonder why it's singled out like this). There's a [Writing TTrees to a file](https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file) section, then a [Extending TTrees with large datasets](https://uproot.readthedocs.io/en/latest/basic.html#extending-ttrees-with-large-datasets) section, then [Specifying the compression](https://uproot.readthedocs.io/en/latest/basic.html#specifying-the-compression). Probably before Specifying the compression.\r\n\r\nHow about this?\r\n\r\n------------\r\n\r\n### Ragged arrays with a shared \"counter\" TBranch\r\n\r\nOften, a computation on Awkward Arrays results in several jagged arrays that have the same list lengths, list by list. For example, suppose we have\r\n\r\n```python\r\n>>> pt = ak.Array([[0.0, 11, 22], [], [33, 44], [55], [66, 77, 88, 99]])\r\n>>> eta = ak.Array([[0.0, 1.1, 2.2], [], [3.3, 4.4], [5.5], [6.6, 7.7, 8.8, 9.9]])\r\n```\r\n\r\nThe lengths of all the lists in `pt` and `eta` are\r\n\r\n```python\r\n>>> ak.num(pt)\r\n<Array [3, 0, 2, 1, 4] type='5 * int64'>\r\n>>> ak.num(eta)\r\n<Array [3, 0, 2, 1, 4] type='5 * int64'>\r\n```\r\n\r\nwhich are the same.\r\n\r\n```python\r\n>>> ak.all(ak.num(pt) == ak.num(eta))\r\nTrue\r\n```\r\n\r\nSince dynamic-length arrays in ROOT require a \"counter\" branch (see [Writing TTrees to a file](https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file), above), simply putting these jagged arrays into a TTree results in a separate \"counter\" branch for each array:\r\n\r\n```python\r\n>>> file[\"tree6\"] = {\"Muon_pt\": pt, \"Muon_eta\": eta}\r\n>>> file[\"tree6\"].show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nnMuon_pt             | int32_t                  | AsDtype('>i4')\r\nMuon_pt              | double[]                 | AsJagged(AsDtype('>f8'))\r\nnMuon_eta            | int32_t                  | AsDtype('>i4')\r\nMuon_eta             | double[]                 | AsJagged(AsDtype('>f8'))\r\n```\r\n\r\nOnly one \"counter\" branch is needed, so this is a waste of disk space as well as hiding the fact that `pt` and `eta` have the same length lists.\r\n\r\nIn Awkward Array, jagged arrays with the same length lists can be zipped together with [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html), and Uproot recognizes zipped arrays as ones that can have a common \"counter\" branch.\r\n\r\nIf you have this case, you'll most likely want to zip your jagged arrays together at some point before writing to a file. Here's an example of zipping immediately before writing to a file:\r\n\r\n```python\r\n>>> file[\"tree7\"] = {\"Muon\": ak.zip({\"pt\": pt, \"eta\": eta})}\r\n>>> file[\"tree7\"].show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nnMuon                | int32_t                  | AsDtype('>i4')\r\nMuon_pt              | double[]                 | AsJagged(AsDtype('>f8'))\r\nMuon_eta             | double[]                 | AsJagged(AsDtype('>f8'))\r\n```\r\n\r\nIf you need to declare the TTree before filling it using [mktree](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableDirectory.html#uproot-writing-writable-writabledirectory-mktree) (see [Extending TTrees with large datasets](https://uproot.readthedocs.io/en/latest/basic.html#extending-ttrees-with-large-datasets), above), the type given by the zipped array is what you want:\r\n\r\n```python\r\n>>> muons = ak.zip({\"pt\": pt, \"eta\": eta})\r\n>>> file.mktree(\"tree8\", {\"Muon\": muons.type})\r\n<WritableTree '/tree8' at 0x00011eda67d0>\r\n>>> file[\"tree8\"].extend({\"Muon\": ak.zip({\"pt\": pt, \"eta\": eta})})\r\n>>> file[\"tree8\"].show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nnMuon                | int32_t                  | AsDtype('>i4')\r\nMuon_pt              | double[]                 | AsJagged(AsDtype('>f8'))\r\nMuon_eta             | double[]                 | AsJagged(AsDtype('>f8'))\r\n```\r\n\r\nAs a string, `muons.type` is\r\n\r\n```python\r\n>>> print(muons.type)\r\n5 * var * {pt: float64, eta: float64}\r\n```\r\n\r\nso\r\n\r\n```python\r\n>>> file.mktree(\"tree8\", {\"Muon\": \"var * {pt: float64, eta: float64}\"})\r\n```\r\n\r\nis equivalent to the above.\r\n\r\n-----------\r\n\r\nThat should cover it, right? (If so, resolving this issue would be a matter of copy-pasting the above into the Uproot documentation. I hope I don't have to convert it to reST...)",
  "created_at":"2022-10-18T17:02:49Z",
  "id":1282717751,
  "issue":759,
  "node_id":"IC_kwDOD6Q_ss5MdLg3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-18T17:02:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The link is now fixed; thanks for letting us know!",
  "created_at":"2022-10-20T20:46:40Z",
  "id":1286129919,
  "issue":760,
  "node_id":"IC_kwDOD6Q_ss5MqMj_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-20T20:46:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That's surprising; I didn't know about [TLeafG](https://root.cern.ch/doc/master/classTLeafG.html). According to the ROOT documentation (that link), it's for 64-bit integers, but [TLeafL](https://root.cern.ch/doc/master/classTLeafL.html) is also for 64-bit integers.\r\n\r\nIt is probably just a matter of copy-pasting some `TLeafL` stuff to also be named `TLeafG` (or linking/renaming?).\r\n\r\nBut that's weird: `TLeafG`! Maybe one of these is guaranteed to be 64-bit on all systems and the other isn't? (32-bit Windows?) But it's an I/O class, so it can't be platform-dependent without making the files unreadable on different platforms. Very weird.\r\n\r\nHmmm... It's [from 2020](https://github.com/root-project/root/commits/master/tree/tree/src/TLeafG.cxx).",
  "created_at":"2022-10-21T21:12:27Z",
  "id":1287440982,
  "issue":761,
  "node_id":"IC_kwDOD6Q_ss5MvMpW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-21T21:12:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"As pointed out by https://github.com/scikit-hep/uproot5/pull/763#issuecomment-1289570802 I believe the problem is due to `TList` serialization",
  "created_at":"2022-10-24T10:58:11Z",
  "id":1288858784,
  "issue":762,
  "node_id":"IC_kwDOD6Q_ss5M0myg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-24T20:56:35Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Since I saw your message in Slack, I started looking into this, but it seems that you're ahead of me: yes, the TObject will need its parameters. (Doesn't the TObject constructor make them, though?)\r\n\r\nTObjString is heavily tested in the testing suite, but not TList. I'm a little surprised that TList is even implemented, but I suppose it might be because it was needed to implement something else, such as histograms. (Writing TObjString, descendants of TH1, and simple and jagged TTrees have been implemented; nothing else has been attempted.)\r\n\r\nIt's very likely that TList's `_serialize` method was only written for _empty_ TLists. It's very likely that some histogram class can hold a list of objects that we don't attempt to write (such as fit functions) and the TList serialization was only written to allow empty TLists to be included in histograms. So when you tried to write a non-empty TList, you were in uncharted territory.\r\n\r\nEven though you've found something that works, you should remain suspicious of it because it might be working by accident. You're filling the `fUniqueID` and `fBits` of the TObject with zeros; the reader trying to deserialize a different part of the structure when it gets to those zeros, and interprets the zeros as that other thing, like size of the list, length of the TList `fName`, or something. Be sure to use much more than one test; lists of various lengths, names, etc.\r\n\r\nWhat I was looking at before seeing this PR is the fact that each item in a TList has a `fOption`. The writer does a Python `zip` through items and `self._options`:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/baa82c94eba0e7155cbacfa3728ec121037f406f/src/uproot/models/TList.py#L103-L105\r\n\r\nPython `zip` stops when it reaches the end of _either_ of the lists it is given, so filling `self._data` and not `self._options` will not get it to write the correct number of items. If that number of items disagrees with the declared `self._members[\"fSize\"]`, the reader will likely crash.\r\n\r\nIn your testing, you can test read-back first with Uproot, then (when that works) with ROOT. Uproot is generally more forgiving of wrong formats, since it reads as little as it needs to and doesn't do all of the consistency checks that ROOT does. Also, it's easier to use Uproot in a debugger or insert print-outs (my favorite method of debugging, since Python loads quickly and doesn't have a long compilation phase), so you can more easily examine which bytes the reader is seeing when it's expecting to be building a given structure. Once it works in Uproot, you still have to test it in ROOT, of course, because it might still be wrong.\r\n\r\nI hope this is useful! I'll keep tabs on your progress, and go ahead and ask questions on #awkward-uproot in Slack as you go. Good luck!",
  "created_at":"2022-10-24T13:38:02Z",
  "id":1289050121,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M1VgJ",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-10-24T13:38:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski this might be a good place to discuss; should a bare `TString` be round-trippable in Awkward? I noticed that it didn't deserialise when taking a quick look at *this* issue with `TObjString`:\r\n```python\r\nimport uproot\r\nprint(f\"{uproot.__version__=}\")\r\n\r\nfilename = \"list.root\"\r\nkey = \"listOfStrings\"\r\n\r\nwith uproot.recreate(filename) as f:\r\n    f[key] = uproot.writing.identify.to_TString(\"hello world!\")\r\n\r\nwith uproot.open(filename) as f:\r\n    print(f\"keys: {f.keys()}\")\r\n    print(f\"key: {f[key]}\")\r\n    ```",
  "created_at":"2022-10-24T13:56:12Z",
  "id":1289075940,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M1bzk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-24T13:56:41Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I also noticed this interesting behaviour that may explain why this has slipped under the radar:\r\n\r\n```\r\nimport uproot\r\n\r\nfilename = \"list.root\"\r\nkey = \"listOfStrings\"\r\n\r\ns = uproot.writing.identify.to_TObjString(\"test\")\r\nwith uproot.recreate(filename) as f:\r\n    try:\r\n        print(s.tojson())  # this does not work\r\n    except AttributeError as e:\r\n        print(e)\r\n    f[key] = s\r\n    print(f[key].tojson())  # this works\r\n```",
  "created_at":"2022-10-24T14:09:37Z",
  "id":1289096015,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M1gtP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-24T14:09:37Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"> should a bare `TString` be round-trippable in Awkward?\r\n\r\nI don't think that bare TString objects can be put in TDirectories (even in ROOT, that is). I think that's why TObjString exists: it inherits from TObject so that it has the `fProcess`, `fBits` infrastructure that objects-in-TDirectories need. (They might also need `fName` and `fTitle`, since this information is also in the TKeys of a TDirectory.) But perhaps Uproot could use some code to _prohibit_ non-TDirectory-ready objects from being written to TDirectories.\r\n\r\n> I also noticed this interesting behaviour that may explain why this has slipped under the radar:\r\n\r\nThe `tojson` method is not actually used in serialization; JSON serialization is an alternative to binary serialization, defined by ROOT, and so I used it in the early part of the project to verify that I was at least identifying the right data to serialize, before attempting the binary format. Since those early tests ([test_0010-start-streamers.py](https://github.com/scikit-hep/uproot5/blob/main/tests/test_0010-start-streamers.py) and [test_0011-generate-classes-from-streamers.py](https://github.com/scikit-hep/uproot5/blob/main/tests/test_0011-generate-classes-from-streamers.py)), the `tojson` method hasn't been used by anything and it hasn't been kept up-to-date with recent developments, such as serialization of additional data types. I've been on the fence about removing it completely: as a public interface, it should either work or not exist, and it would be easier to back off on it than to go forward with it. The reason I haven't done so is because it's a good diagnostic when someone raises a serialization issue.",
  "created_at":"2022-10-24T14:32:35Z",
  "id":1289130546,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M1pIy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-24T14:32:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I don't think that bare TString objects can be put in TDirectories (even in ROOT, that is). \r\n\r\nOh, that sounds familiar, actually.\r\n\r\n> But perhaps Uproot could use some code to prohibit non-TDirectory-ready objects from being written to TDirectories.\r\n\r\nGood to see a silly mistake motivating a new feature ;)",
  "created_at":"2022-10-24T14:33:56Z",
  "id":1289132895,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M1ptf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-24T14:33:56Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"After some work I became pretty stuck.\r\n\r\n- https://github.com/scikit-hep/uproot5/pull/763/commits/5c90b481ace60ca803bf3e4498dd3f1e1a1f5130 fixed (allegedly) serialization of `TObjString` inside the `TList`.\r\n\r\n- **The serialization of `TList` is still wrong and now I am pretty sure the problem is either in `uproot.writing.identify.to_TList` or in `TList::_serialize`**.\r\n\r\n- The deserealization (reading) of `TList` appears to work fine. An empty list is serialized / deserialized correctly too.\r\n\r\n- I tried to use the simplest `TList` I could think of (containing a single `TObject`) to test, and see that the problem is not related to `TObjString`.\r\n\r\nA simple root script to generate this basic list:\r\n\r\n```c++\r\nauto f = TFile::Open(\"list.root\", \"RECREATE\");\r\n\r\nauto l = new TList();\r\n\r\nl->Add(new TObject()); // I also tried setting a non-zero unique ID for the TObject, deserialization still works\r\n\r\nl->Write(\"myList\", TObject::kSingleKey); // if TObject::kSingleKey is not specified, each item on the list is written as a key to the root file\r\n\r\nf->Close();\r\n```\r\n\r\n- This is read correctly by `uproot`.\r\n\r\n- **Attempt to recreate this file with uproot**:\r\n\r\n```python\r\nimport uproot\r\n\r\ntobject = uproot.models.TObject.Model_TObject.empty()\r\ntobject._members[\"@fUniqueID\"] = 0\r\ntobject._members[\"@fBits\"] = 0\r\n\r\nmylist = uproot.writing.identify.to_TList([tobject])\r\n\r\nfilename = \"list.root\"\r\nkey = \"mylist\"\r\n\r\nwith uproot.recreate(filename) as f:\r\n    f[key] = mylist\r\n```\r\n\r\nwhich will produce an error like:\r\n\r\n```\r\n    raise DeserializationError(\r\nuproot.deserialization.DeserializationError: while reading\r\n    TList version 5 as uproot.models.TList.Model_TList (31 bytes)\r\n        (base): <TObject None None at 0x0122b05f10c0>\r\n        fName: ''\r\n        fSize: 1\r\nBase classes for TList: TObject?\r\nMembers for TList: fName?, fSize?\r\nexpected 31 bytes but cursor moved by 26 bytes (through TList)\r\nin file list.root\r\nin object /mylist;1\r\n```\r\n\r\n> Python zip stops when it reaches the end of either of the lists it is given, so filling self._data and not self._options will not get it to write the correct number of items. If that number of items disagrees with the declared self._members[\"fSize\"], the reader will likely crash.\r\n\r\nI added a check for this but it's not the problem in this case.\r\n\r\nI am not sure how to proceed, unless I am missing something the problem seems a bit too deep for me yet. Any tips are welcome @jpivarski @agoose77 ty.",
  "created_at":"2022-10-24T20:25:45Z",
  "id":1289570802,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M3Uny",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-24T20:26:12Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It might be helpful to write a `TList[TObject()]` from ROOT, and try to round-trip it through uproot. This also fails, but might help to narrow down where the serialisation is differing. (I assume the error lies in how we serialise the `TList` / `TObject`).",
  "created_at":"2022-10-25T06:53:57Z",
  "id":1290067706,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M5N76",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-25T06:53:57Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lobis it looks like the problem is related to the reading and writing of the list item options in `TList._serialize`. When reading in the list options in `TList.read_members`, they are parsed as pascal (byte)strings - a series of bytes with the string length prepended. Meanwhile, the `TList._serialize` writing routine directly appends the raw option bytestring to the output list. This leads to an off-by-one error.\r\n\r\nLocally, I added a simple `bytestring` helper to `uproot.serialization` that converts \"bytes\" into a length-prefixed bytestring:\r\n```python\r\ndef string(data):\r\n    \"\"\"\r\n    Converts a Python string into bytes, ready to be written to a file.\r\n\r\n    If the string's byte representation (UTF-8) has fewer than 255 bytes, it\r\n    is preceded by a 1-byte length; otherwise, it is preceded by ``b'\\xff'`` and a\r\n    4-byte length.\r\n    \"\"\"\r\n    return bytestring(\r\n        data.encode(errors=\"surrogateescape\")\r\n    )\r\n\r\n\r\ndef bytestring(data):\r\n    \"\"\"\r\n    Converts Python bytes into a length-prefixed bytestring, ready to be written to a file.\r\n\r\n    If the string's byte representation (UTF-8) has fewer than 255 bytes, it\r\n    is preceded by a 1-byte length; otherwise, it is preceded by ``b'\\xff'`` and a\r\n    4-byte length.\r\n    \"\"\"\r\n    length = len(data)\r\n    if length < 255:\r\n        return struct.pack(\">B%ds\" % length, length, data)\r\n    else:\r\n        return struct.pack(\">BI%ds\" % length, 255, length, data)\r\n```\r\n\r\nThen, changing `TList._serialize` to use this helper\r\n```python3\r\n        for datum, option in zip(self._data, self._options):\r\n            uproot.serialization._serialize_object_any(out, datum, None)\r\n            out.append(uproot.serialization.bytestring(option)) # Fixed line\r\n```\r\nfixes the reading.\r\n\r\nI think this seems fairly reasonable; if we're able to read a ROOT file correctly, then our reading logic should be correct. This implies that we want to read a length-prefixed bytestring here. And, that seems intuitive; it's the simplest means of encoding a string.",
  "created_at":"2022-10-25T08:02:02Z",
  "id":1290149135,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M5h0P",
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "total_count":1
  },
  "updated_at":"2022-10-25T08:02:52Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> fixes the reading.\r\n\r\nThanks @agoose77! this fixed the issue. I implemented your suggestion on the `uproot.writing.identity.to_TList` method instead of in the `TList` model itself because it seemed more appropiate (4da653fee0f28789fbb20abcad3a4541acd6701a).",
  "created_at":"2022-10-25T14:15:17Z",
  "id":1290629086,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M7W_e",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-25T14:15:17Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The CI here is failing because we don't upper-bound our Awkward version, and it's picking up the V2 RC candidate. This surprised me, until I realised that this happens when the dependency specifier already includes a pre-release, e.g. `1.9.0rc1`. I recommend we wait until #765 merges, and then rebase this.",
  "created_at":"2022-10-26T08:38:46Z",
  "id":1291694161,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M_bBR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-26T08:39:04Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> The CI here is failing because we don't upper-bound our Awkward version, and it's picking up the V2 RC candidate. This surprised me, until I realised that this happens when the dependency specifier already includes a pre-release, e.g. `1.9.0rc1`. I recommend we wait until #765 merges, and then rebase this.\r\n\r\nThanks, I was wondering what was wrong.",
  "created_at":"2022-10-26T08:45:00Z",
  "id":1291702008,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5M_c74",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-26T08:45:00Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski in my view, this is good to go. I would appreciate your second glance to approve.",
  "created_at":"2022-10-27T13:24:43Z",
  "id":1293523023,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5NGZhP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-27T13:24:43Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @lobis for code",
  "created_at":"2022-10-28T22:46:21Z",
  "id":1295589480,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5NOSBo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-28T22:46:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/771) to add @lobis! :tada:",
  "created_at":"2022-10-28T22:46:30Z",
  "id":1295589680,
  "issue":763,
  "node_id":"IC_kwDOD6Q_ss5NOSEw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-28T22:46:30Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"At this point serialization appears to work okay but the axis is not being drawn in the histogram (drawn using root). This is probably due to some badly set option. I will compare the uproot generated histogram with one generated with root to see the differences.\r\n\r\n```\r\nimport hist\r\nimport uproot\r\nimport numpy\r\n\r\ncat_axis = hist.axis.IntCategory([1, 2, 3], label='Category')\r\nh = hist.Hist(cat_axis)\r\nh.fill(\r\n    numpy.random.randint(1, 4, 1000)\r\n)\r\n\r\nwith uproot.recreate(\"test.root\") as f:\r\n    f['h'] = h\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/35803280/197776065-5b4a33ba-d2bb-4e97-8fb0-12c61b10dc58.png)\r\n",
  "created_at":"2022-10-25T12:43:51Z",
  "id":1290495543,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5M62Y3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-25T12:43:51Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"I have to admit that I haven't looked into the code, but are the `fLabels` that you're adding converted into strings? ROOT's `fLabels` can only be strings. If they're `\"\\x01\"`, `\"\\x02\"`, `\"\\x03\"`, then those are all invisible characters. You'll want them to be `\"1\"`, `\"2\"`, `\"3\"`, etc.\r\n\r\n(That's a shot in the dark as to why they're not showing up in the plot.)\r\n\r\nAs a diagnostic method, instead of going directly for the TCanvas plot, what are the `TAxis::GetBinLabel(bin)` values? Or the equivalent in Uproot, `Model_TAxis.member(\"fLabels\")`?",
  "created_at":"2022-10-25T15:12:33Z",
  "id":1290727389,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5M7u_d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-25T15:12:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I have to admit that I haven't looked into the code, but are the `fLabels` that you're adding converted into strings? ROOT's `fLabels` can only be strings. If they're `\"\\x01\"`, `\"\\x02\"`, `\"\\x03\"`, then those are all invisible characters. You'll want them to be `\"1\"`, `\"2\"`, `\"3\"`, etc.\r\n> \r\n> (That's a shot in the dark as to why they're not showing up in the plot.)\r\n> \r\n> As a diagnostic method, instead of going directly for the TCanvas plot, what are the `TAxis::GetBinLabel(bin)` values? Or the equivalent in Uproot, `Model_TAxis.member(\"fLabels\")`?\r\n\r\nI think they are being saved correctly as strings. `Model_TAxis.member(\"fLabels\")` returns correct values etc.\r\n\r\nThe serialization of the root histogram also appears to be correct, which why I thought there must be some style problem misplacing the labels etc.\r\n\r\n```\r\n  \"fXaxis\" : {\r\n    \"_typename\" : \"TAxis\",\r\n    \"fUniqueID\" : 0,\r\n    \"fBits\" : 8,\r\n    \"fName\" : \"\",\r\n    \"fTitle\" : \"Category\",\r\n    \"fNdivisions\" : 510,\r\n    \"fAxisColor\" : 1,\r\n    \"fLabelColor\" : 1,\r\n    \"fLabelFont\" : 42,\r\n    \"fLabelOffset\" : 0.005,\r\n    \"fLabelSize\" : 0.035,\r\n    \"fTickLength\" : 0.03,\r\n    \"fTitleOffset\" : 1,\r\n    \"fTitleSize\" : 0.035,\r\n    \"fTitleColor\" : 1,\r\n    \"fTitleFont\" : 42,\r\n    \"fNbins\" : 3,\r\n    \"fXmin\" : 0,\r\n    \"fXmax\" : 3,\r\n    \"fXbins\" : [0, 1, 2, 3],\r\n    \"fFirst\" : 0,\r\n    \"fLast\" : 0,\r\n    \"fBits2\" : 0,\r\n    \"fTimeDisplay\" : false,\r\n    \"fTimeFormat\" : \"\",\r\n    \"fLabels\" : {\r\n      \"_typename\" : \"THashList\",\r\n      \"name\" : \"THashList\",\r\n      \"arr\" : [{\r\n        \"_typename\" : \"TObjString\",\r\n        \"fUniqueID\" : 0,\r\n        \"fBits\" : 0,\r\n        \"fString\" : \"1\"\r\n      }, {\r\n        \"_typename\" : \"TObjString\",\r\n        \"fUniqueID\" : 0,\r\n        \"fBits\" : 0,\r\n        \"fString\" : \"2\"\r\n      }, {\r\n        \"_typename\" : \"TObjString\",\r\n        \"fUniqueID\" : 0,\r\n        \"fBits\" : 0,\r\n        \"fString\" : \"3\"\r\n      }],\r\n      \"opt\" : [\"\", \"\", \"\"]\r\n    },\r\n    \"fModLabs\" : null\r\n  },\r\n```\r\n\r\nHowever the `h->GetXaxis()->GetBinLabel(bin)` always returns `\"\"` so there must be a problem there. `h->GetXaxis()->GetLabels()` however appears to return the correct object, with size `3` and the correct values for entries. However when doing `h->GetXaxis()->SetBinLabel(bin, label);` an additional label is inserted into the `THashList`, instead of being replaced, so there must be something wrong.\r\n\r\nAfter looking into the code of `TAxis` (root) I think the histogram has access to the `THashList` object containing the labels but the problem is in the labels itself, since `(TObjString*)fLabels->FindObject(label);` does not find labels even if they exist, however the are printed when `Print` is invoked. So probably there is some issue with the serialization of `TList` / `THashList`. I will see where it is and update this comment.\r\n\r\nThe `TObject::fUniqueID` appears to be the bin index for each label but we set it to `0` so this may also be a problem.\r\n\r\n```c++\r\n\r\nvoid TAxis::SetBinLabel(Int_t bin, const char *label)\r\n{\r\n   if (!fLabels) fLabels = new THashList(fNbins,3);\r\n\r\n   if (bin <= 0 || bin > fNbins) {\r\n      Error(\"SetBinLabel\",\"Illegal bin number: %d\",bin);\r\n      return;\r\n   }\r\n\r\n   // Check whether this bin already has a label.\r\n   TIter next(fLabels);\r\n   TObjString *obj;\r\n   while ((obj=(TObjString*)next())) {\r\n      if ( obj->GetUniqueID()==(UInt_t)bin ) {\r\n         // It does. Overwrite it.\r\n         obj->SetString(label);\r\n         // LM need to rehash the labels list (see ROOT-5025)\r\n         fLabels->Rehash(fLabels->GetSize() );\r\n         return;\r\n      }\r\n   }\r\n   // It doesn't. Add this new label.\r\n   obj = new TObjString(label);\r\n   fLabels->Add(obj);\r\n   obj->SetUniqueID((UInt_t)bin);\r\n\r\n   // check for Alphanumeric case (labels for each bin)\r\n   if (CanBeAlphanumeric() && fLabels->GetSize() == fNbins) {\r\n      SetAlphanumeric(kTRUE);\r\n      SetCanExtend(kTRUE);\r\n   }\r\n}\r\n```\r\n\r\nAfter adding the `fUniqueID` as done by root the labels are drawn but they are not in the correct position. I have had this happen to me sometimes when setting labels for a histogram from root (without uproot). It happened the fist time I tried adding labels to a histogram as I was trying commands \"intuitively\" but now I cannot reproduce... https://root-forum.cern.ch/t/cant-change-axis-label-for-th1-frame/45104/2 is relevant.\r\n\r\n![image](https://user-images.githubusercontent.com/35803280/197965065-f912817f-5064-4969-8473-c35b67492404.png)\r\n",
  "created_at":"2022-10-26T07:12:36Z",
  "id":1291598385,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5M_Dox",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-26T09:08:24Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"After some debugging I found the problem of not placing the labels correctly is due to the `fName` of the axis, which must be `xaxis` (in the case of x axis). When the axis is created by `hist` such as in this case, the name field is empty by default.\r\n\r\nBasically you need to do:\r\n\r\n```\r\ncat_axis = hist.axis.IntCategory([0], label='Category', name=\"xaxis\")\r\n```\r\n\r\nCommit [57a5640](https://github.com/scikit-hep/uproot5/pull/764/commits/57a5640566e02339c07cf4d226807a2a86835cea) makes the axis name be the default value when the axis attribute is an empty string.\r\n\r\nHowever if the user chooses to give the axis a name which is not empty string nor the correct value (`xaxis` in this case), the drawing of labels will be broken. \r\n\r\nThis is also what happens in root if you rename the axis, so I guess we should not do further modifications. \r\n\r\nAs I see it there are three options:\r\n\r\n- Leave it as is right now (57a5640): drawing will work if axis name is not set explicitly by user.\r\n- Add logic so that if axis name is explicitly set but some condition is met (such as categorical axis), the name is overriden to the default value.\r\n- Always use the default values for the axis regardless of the `name` attribute of axis.\r\n\r\nI am inclined to choose the third option, but I don't know its possible side effects, is there any actual use for the name of axes? or is it always set to `xaxis` etc.? This would prevent the user from accidentally breaking things. However it will also prevent uproot from correctly serializing a `TAxis` as it would override the name, so I guess we should stick to the first option, as we should not try to fix root problems from uproot, right?\r\n\r\n@agoose77 @jpivarski ",
  "created_at":"2022-10-28T10:07:54Z",
  "id":1294806176,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NLSyg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-28T10:40:58Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Don't be surprised about the tests failing soon; I have to do a tricky two-package code change, explained in https://github.com/scikit-hep/uproot5/pull/770#issuecomment-1295652662.\r\n\r\nWhen Awkward 2.0.0rc2 is released and #770 is merged into `main`, you'll be able to pick up `main` and get your tests working again.\r\n\r\n(Sorry!)",
  "created_at":"2022-10-29T00:07:48Z",
  "id":1295653654,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NOhsW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-29T00:07:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lobis if ROOT does not support custom axis names for categorical histograms, then I would be in favour of overwriting the name in the `Boost Histogram \u2192 TH` conversion. I don't believe we're expecting this to be a lossless transformation, so it seems sensible to produce useable histograms. That said, @jpivarski thoughts?",
  "created_at":"2022-10-29T12:59:18Z",
  "id":1295829638,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NPMqG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-29T12:59:18Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I think that a TAxis's `fName` _has to be_ `\"xaxis\"`, `\"yaxis\"`, or `\"zaxis\"`, but the `fTitle` can be anything. Is it the `fTitle` that gets displayed when plotting on ROOT's canvas?",
  "created_at":"2022-10-29T17:51:17Z",
  "id":1295917321,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NPiEJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-29T17:51:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I think that a TAxis's `fName` _has to be_ `\"xaxis\"`, `\"yaxis\"`, or `\"zaxis\"`, but the `fTitle` can be anything. Is it the `fTitle` that gets displayed when plotting on ROOT's canvas?\r\n\r\nYes, `fTitle` is what is displayed. On commit https://github.com/scikit-hep/uproot5/pull/764/commits/43affea01ddc2baedf225e1886d45ae7f314b82e I made axis name be one of theese three values and I also updated the tests cases that checked for this on [70e7008](https://github.com/scikit-hep/uproot5/pull/764/commits/70e70083f75e30eed6a2add0b830309794b42a47).",
  "created_at":"2022-10-30T18:04:29Z",
  "id":1296316181,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NRDcV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-30T18:04:29Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think this is mostly ready to go!\r\n\r\nHowever there is a problem I could not manage to figure out. In [5370ac9](https://github.com/scikit-hep/uproot5/pull/764/commits/5370ac9e3281f6dca75c8f931dfeb978e908afb8) and d56eba3cec8b2fff22f7d661dad53905361f08d8 I introduced a dirty fix to bypass the `if` branch for weighted histogram. This needs to be properly addressed.\r\n\r\nIf this change is not introduced, the tests we failing, specifically `test_0422-hist-integration::test_issue_0659` (https://github.com/scikit-hep/uproot5/issues/659). The `view[...] =` call fails if the correct storage type is not selected? Also I cannot understand how \r\n\r\n```\r\nview.value = values\r\nview.variance = sumw2\r\n```\r\n\r\nare supposed to work. Any advice is welcome.",
  "created_at":"2022-10-31T16:08:25Z",
  "id":1297324151,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NU5h3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-31T16:08:25Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This all looks good to me, apart from the \"`and False`\" predicates called out below. I can't tell what was intended and if \"temporary\" was supposed to extend beyond this PR into a follow-up PR or not.\r\n\r\nThe changes are temporary in the sense that they should be changed before this PR is merged. I thought adding `and False` would ilustrate this since it screams to be changed before merging, I wanted to illustrate the fact the removing the `if` would fix the tests but that we should probably not remove it since it will likely break something else. In hindshight I should have removed it and then added it back.\r\n\r\nMy point is that I don't understand when (if) the `if sumw2 is not None and len(sumw2) == len(values):` and `if sumw2 is not None and len(sumw2) == self.member(\"fNcells\"):` should be entered. From my testing I found that never entering would fix all the tests this is why I added the `and False` condition, so it never enters.\r\n\r\nI think for `TH*` objects derived from `hist`/`boost` histograms, the condition `sumw2 is not None and len(sumw2) == self.member(\"fNcells\")` is always true, but this causes the storage type to be `Weight` instead of `Int` or `Double`, which is the expected behaviour for all cases I tried.\r\n\r\nBypassing `sumw2 is not None and len(sumw2) == self.member(\"fNcells\")` and `sumw2 is not None and len(sumw2) == len(values)` checks does not cause anything to fail, but still I don't want to remove the code without first checking with you or others. However if we should not remove it we should atleast add some additional test. **Can we remove this code?**",
  "created_at":"2022-10-31T17:14:52Z",
  "id":1297409505,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NVOXh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-31T17:21:49Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, then I'm pretty sure you just want to drop all of the \"`and False`\" because the consequent of this predicate handles the \"weights exist\" case:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/5370ac9e3281f6dca75c8f931dfeb978e908afb8/src/uproot/behaviors/TH1.py#L324-L333\r\n\r\n`sumw2 is None` would happen if the corresponding ROOT member were `nullptr`, but ROOT uses `len(sumw2) == 0` if the sumw2 is not defined (i.e. [TH1::Sumw2](https://root.cern.ch/doc/master/classTH1.html#aefa4ee94f053ec3d217f3223b01fa014) is never called). \"`fNcells`\" is the number of bins; there are several ways of getting that.\r\n\r\nI don't know of a way to distinguish between a zero-bin histogram with sumw2 and a zero-bin histogram without sumw2, but that case might not be possible, anyway.\r\n\r\n> However if we should not remove it we should atleast add some additional test. **Can we remove this code?**\r\n\r\nWe definitely can't remove the code, since there are histograms (somewhere) with sumw2 defined. They need to have the corresponding `sumw2` array shaped in the right way and converted to native endian (they're initially big-endian, and need to become little-endian on most platforms). Also, the corresponding boost-histograms need to have `Weight` storage for these.\r\n\r\nIsn't it possible to make example ROOT histograms with `sumw2` by either calling the [TH1::Sumw2](https://root.cern.ch/doc/master/classTH1.html#aefa4ee94f053ec3d217f3223b01fa014) method directly or [TH1::Fill](https://root.cern.ch/doc/master/classTH1.html#a894833e678f283b6849e777c635c059d) with weights?",
  "created_at":"2022-10-31T17:28:48Z",
  "id":1297429436,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NVTO8",
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "total_count":1
  },
  "updated_at":"2022-10-31T17:28:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nevermind\u2014I think I see it: `no_inherit` prevents Python classes from being applied to C++ objects in which the C++ class hierarchy has a superclass of that name, but it doesn't do anything to Python inheritance itself (which is good, because that would be confusing).\r\n\r\nhttps://github.com/lobis/uproot5/blob/c294f8f5ccd67ff1d75799074e2621736c5abaab/src/uproot/deserialization.py#L78-L83\r\n\r\nTthe Python inheritance is\r\n\r\n```mermaid\r\ngraph TB\r\n    A((Histogram))-->B((TH1))\r\n    A-->C((TH2))\r\n    A-->D((TH3))\r\n    A-->H((Profile))\r\n    H-->E((TProfile))\r\n    H-->F((TProfile2D))\r\n    H-->G((TProfile3D))\r\n```\r\n\r\nand the C++ inheritance is\r\n\r\n<img src=\"https://root.cern.ch/doc/master/classTH1__inherit__graph_org.svg\" width=\"100%\">\r\n\r\nWhen we deserialize a `TH3F`, for instance, we see that it has a C++ superclass named `TH3`, and we have a registered Python behavior for `TH3`, so its methods and properties get mixed into the histogram Model class (`Model_TH3F_v#`). Since `TH3` is set to \"`no_inherit`\" from `TH1`, we don't mix our `TH1` methods and properties into that object. However, our Python `TH3` is a Python subclass of `Histogram`, so it gets all of the `Histogram` methods, including `to_boost`.\r\n\r\nSo this PR makes complete sense to me. I'll squash-and-merge it.\r\n\r\nThanks again!",
  "created_at":"2022-11-02T18:35:53Z",
  "id":1301060064,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NjJng",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-02T18:35:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I will merge (squash!) once the tests pass.",
  "created_at":"2022-11-02T19:58:33Z",
  "id":1301160712,
  "issue":764,
  "node_id":"IC_kwDOD6Q_ss5NjiMI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-02T19:58:33Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"These tests are now failing as `dask-awkward` is an older PyPI release. Should we pin the `awkward-dask` version in our `pyproject.toml` to some Git SHA?",
  "created_at":"2022-10-25T22:04:21Z",
  "id":1291191857,
  "issue":765,
  "node_id":"IC_kwDOD6Q_ss5M9gYx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-25T22:04:21Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for checking that. We can do that as a first test, but eventually we'll want a post-2.0.0rc1 awkward-dask release. (This is all a big chicken-and-egg problem.)",
  "created_at":"2022-10-25T22:55:20Z",
  "id":1291224645,
  "issue":765,
  "node_id":"IC_kwDOD6Q_ss5M9oZF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-25T22:55:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"dask-awkward `2022.10a0` is now on PyPI \ud83d\udc4d ",
  "created_at":"2022-10-26T00:01:11Z",
  "id":1291263881,
  "issue":765,
  "node_id":"IC_kwDOD6Q_ss5M9x-J",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":2,
   "total_count":2
  },
  "updated_at":"2022-10-26T00:01:11Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski just noticed that we already have #741, shall we close #741 as this one has a higher number of changed lines?",
  "created_at":"2022-10-26T08:42:05Z",
  "id":1291698417,
  "issue":765,
  "node_id":"IC_kwDOD6Q_ss5M_cDx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-26T08:42:32Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, I completely forgot that I had already done it.\r\n\r\nI just scanned over the two diffs, and they (the diff of diffs) are essentially the same\u2014that is, I understand the differences. Not wrapping `ForthMachine64.output` in `np.asarray` is a little better (this PR), and this PR also doesn't have the hack of pulling in Awkward from `main`, unnecessary since yesterday. Also, the diff in this PR is fresher; it's on top of other improvements that have gone into Uproot in the past 26 days, so I closed the old one.",
  "created_at":"2022-10-26T13:55:54Z",
  "id":1292091692,
  "issue":765,
  "node_id":"IC_kwDOD6Q_ss5NA8Es",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-26T13:55:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This last triggering of the tests is a sanity check: the test of be23aec13d3134db67cfef3d6570275c58845cfd (which was successful) should have been enough. I just want to see it go from start to finish.\r\n\r\nOther than that, I think it's done. If you (@agoose77) give the approval, I'll merge it and that unblocks Coffea tests.",
  "created_at":"2022-10-26T14:01:17Z",
  "id":1292099767,
  "issue":765,
  "node_id":"IC_kwDOD6Q_ss5NA-C3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-26T14:01:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"CI's broken.\r\n\r\n\r\n```\r\nFAILED tests/test_0034-generic-objects-in-ttrees.py::test_general_awkward_form - ModuleNotFoundError: No module named 'awkward._v2.forms'; 'awkward._v2' is not a package\r\nFAILED tests/test_0034-generic-objects-in-ttrees.py::test_awkward_map_int_struct - ModuleNotFoundError: No module named 'awkward._v2.forms'; 'awkward._v2' is not a package\r\nFAILED tests/test_0034-generic-objects-in-ttrees.py::test_awkward_nosplit_file - ModuleNotFoundError: No module named 'awkward._v2.forms'; 'awkward._v2' is not a package\r\nFAILED tests/test_0034-generic-objects-in-ttrees.py::test_map_string_TVector3 - ModuleNotFoundError: No module named 'awkward._v2.forms'; 'awkward._v2' is not a package\r\nFAILED tests/test_0034-generic-objects-in-ttrees.py::test_gohep_output_file - AttributeError: 'awkward._ext.ForthMachine64' object has no attribute 'output_Index64'\r\nFAILED tests/test_0053-parents-should-not-be-bases.py::test_awkward_TRefArray - AttributeError: 'list' object has no attribute 'items'\r\nFAILED tests/test_0442-regular-TClonesArray.py::test_read_delphes_ak - AttributeError: 'list' object has no attribute 'items'\r\nFAILED tests/test_0637-setup-tests-for-AwkwardForth.py::test_38 - ModuleNotFoundError: No module named 'awkward._v2.forms'; 'awkward._v2' is not a package\r\nFAILED tests/test_0637-setup-tests-for-AwkwardForth.py::test_41 - ModuleNotFoundError: No module named 'awkward._v2.forms'; 'awkward._v2' is not a package\r\nFAILED tests/test_0643-reading-vector-pair-TLorentzVector-int.py::test_awkward - ModuleNotFoundError: No module named 'awkward._v2.forms'; 'awkward._v2' is not a package\r\n```\r\n\r\nCC @agoose77 ",
  "created_at":"2022-10-26T16:55:16Z",
  "id":1292334830,
  "issue":766,
  "node_id":"IC_kwDOD6Q_ss5NB3bu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-26T16:55:38Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Maybe this just needs to wait till #765. Though I don't know why the tests are failing without an update.",
  "created_at":"2022-10-26T16:56:46Z",
  "id":1292336556,
  "issue":766,
  "node_id":"IC_kwDOD6Q_ss5NB32s",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-26T16:56:46Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Uproot _did_ need code changes to get from `awkward ~ 1` (with the `_v2` submodule) to `awkward ~ 2` (without it).\r\n\r\nIn fact, the `_v2` submodule shim in 2.0.0rcX hasn't been working and we should remove it to prevent false hopes: https://github.com/scikit-hep/awkward/issues/1771 That's why #765 replaces `._v2.` \u2192 `.`.",
  "created_at":"2022-10-26T19:13:07Z",
  "id":1292505544,
  "issue":766,
  "node_id":"IC_kwDOD6Q_ss5NChHI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-26T19:13:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Fixed in scikit-hep/awkward#1843.",
  "created_at":"2022-10-31T20:49:42Z",
  "id":1297668807,
  "issue":768,
  "node_id":"IC_kwDOD6Q_ss5NWNrH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-31T20:49:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The purpose of the `\"uproot\"` parameters in the Forms was for Awkward to do the ROOT deserialization, based on the information they contained. That is, this was a pre-AwkwardForth idea, so it really must go away.",
  "created_at":"2022-10-28T22:21:04Z",
  "id":1295541907,
  "issue":770,
  "node_id":"IC_kwDOD6Q_ss5NOGaT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-28T22:21:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Note: tests against Awkward `main` still work.",
  "created_at":"2022-10-28T23:01:38Z",
  "id":1295612134,
  "issue":770,
  "node_id":"IC_kwDOD6Q_ss5NOXjm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-28T23:01:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The tests won't pass until `awkward>=2.0.0rc2` is done: https://github.com/scikit-hep/awkward/actions/runs/3349263292\r\n\r\nMaking coordinated changes across git repos is not great because there must be these in-between times when there's no combination of packages that will work. However, that's why it has to happen _now_ while these are still in pre-release.",
  "created_at":"2022-10-29T00:05:18Z",
  "id":1295652662,
  "issue":770,
  "node_id":"IC_kwDOD6Q_ss5NOhc2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-29T00:05:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"There will be no more changes to this PR, except if something is needed to get the tests to work. The tests may need to be manually triggered after Awkward 2.0.0rc2 is done.\r\n\r\nIt's the other half of scikit-hep/awkward#1845.",
  "created_at":"2022-10-29T00:11:08Z",
  "id":1295654908,
  "issue":770,
  "node_id":"IC_kwDOD6Q_ss5NOh_8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-29T00:11:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The first commit (above) makes lookup faster for Dask and non-Dask alike. The bottom line is somewhat improved:\r\n\r\n```python\r\n>>> starttime = time.time(); tree = uproot.open(\"nano_dy.root:Events\"); time.time() - starttime\r\n0.4882633686065674\r\n>>> starttime = time.time(); dask_tree = uproot.dask({\"nano_dy.root\": \"Events\"}); time.time() - starttime\r\n1.1381452083587646\r\n```\r\n\r\nSubbranch-lookup already had a cache, `HasBranches._lookup` (dict), but this was populated on demand and could include full paths with slashes. Thus, the first time we look up some end-of-list key `\"z-z-top\"` (diboson top production), it has to walk through the whole list to find it. Looking for all _n_ keys in order would be an _O(n\u00b2)_ lookup.\r\n\r\nInstead, I populated the `HasBranches._lookup` as soon as the `HasBranches` is constructed (in `HasBranches.postprocess` for all of the subclasses of `HasBranches`: `TTree` and `TBranch`). This is _slower_ if the user only ever wanted one branch, and it was at the beginning of the list (`\"a-a-axion\"`) because a full loop over all branches must now be done once. But _only ever_ once; never more than once now.\r\n\r\nConsequently, it is important that `HasBranches._lookup` is considered immutable after construction. We should not be putting new `where` strings into it anymore.",
  "created_at":"2022-10-31T18:44:25Z",
  "id":1297517674,
  "issue":772,
  "node_id":"IC_kwDOD6Q_ss5NVoxq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-31T18:44:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"A bit more time can be shaved off by not starting the machinery to read a real array (restricted to zero entries), but by building one from a Form instead. There's this One Weird Trick:\r\n\r\n```python\r\ndef _form_to_empty_array(awkward, form):\r\n    return awkward.from_buffers(\r\n        form, 0, {\"\": b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"}, buffer_key=\"\"\r\n    )\r\n```\r\n\r\n([ak.from_buffers](https://awkward-array.readthedocs.io/en/main/reference/generated/ak.from_buffers.html) starts building an array out of the Form, but all of the `buffer_keys` are empty strings, so they all try to read from the single key in the dict of buffers. That buffer is 8 bytes of zeros because the longest buffer one might need for a length-zero array is a single `int64` for `offsets` representing an empty ListOffsetArray. The trick needs to get formalized; TBD.)\r\n\r\nWith this, we can remove a few more hundred milliseconds:\r\n\r\n```python\r\n>>> import uproot, time\r\n>>> starttime = time.time(); tree = uproot.open(\"nano_dy.root:Events\"); time.time() - starttime\r\n0.4842650890350342\r\n>>> starttime = time.time(); dask_tree = uproot.dask({\"nano_dy.root\": \"Events\"}); time.time() - starttime\r\n1.036236047744751\r\n>>> starttime = time.time(); dask_tree = uproot.dask({\"nano_dy.root\": \"Events\"}, step_size=1000); time.time() - starttime\r\n0.9233171939849854\r\n```\r\n\r\nMore with a given `step_size` because that avoids `num_entries_for`, which calls `_regularize_expressions`.",
  "created_at":"2022-10-31T19:26:15Z",
  "id":1297565506,
  "issue":772,
  "node_id":"IC_kwDOD6Q_ss5NV0dC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-31T19:26:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This last edit avoids the high-level function `HasBranches.num_entries_for`. Now `uproot.dask` is not calling `_regularize_expressions` anywhere, though the final time is not noticeably better than above.\r\n\r\nNow I'm going to ask @kkothari2001 to review this PR. I've only replaced `HasBranches.arrays` and `HasBranches.num_entries_for` in the one spot that the performance test checks; there may be others elsewhere that should also be replaced. If it's a lot of places, then these solutions shouldn't be copied verbatim but wrapped up in helper functions. I've already done that with the One Weird Trick because it's so weird that it should probably be replaced by a public function from Awkward.\r\n\r\nWhen you're happy with it, @kkothari2001, you could ask for another review from @agoose77. (I shouldn't review PRs that I wrote part of.)",
  "created_at":"2022-10-31T19:44:16Z",
  "id":1297587158,
  "issue":772,
  "node_id":"IC_kwDOD6Q_ss5NV5vW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-10-31T19:44:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I know the above commit will not pass 2 tests due to change in our chunking policy (as we discussed in the meet). I only pushed to ask a question.",
  "created_at":"2022-11-01T17:56:07Z",
  "id":1298900803,
  "issue":772,
  "node_id":"IC_kwDOD6Q_ss5Na6dD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-01T17:56:07Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"I ran the profile test again, and it's still fine:\r\n\r\n```python\r\n>>> starttime = time.time(); tree = uproot.open(\"nano_dy.root:Events\"); time.time() - starttime\r\n0.4969170093536377\r\n>>> starttime = time.time(); dask_tree = uproot.dask({\"nano_dy.root\": \"Events\"}); time.time() - starttime\r\n1.1085560321807861\r\n```\r\n\r\nTwice (second time in the same process as the first, so it excludes any library-loading):\r\n\r\n```python\r\n>>> starttime = time.time(); tree = uproot.open(\"nano_dy.root:Events\"); time.time() - starttime\r\n0.5592877864837646\r\n>>> starttime = time.time(); dask_tree = uproot.dask({\"nano_dy.root\": \"Events\"}); time.time() - starttime\r\n1.124070405960083\r\n```",
  "created_at":"2022-11-09T16:59:57Z",
  "id":1309060015,
  "issue":772,
  "node_id":"IC_kwDOD6Q_ss5OBquv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-09T16:59:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I refactored `weighted` property and `to_boost` method using inheritance with the goal of reducing code replication. However perhaps there is a reason this was not done in the first place, and I can see lines such as `no_inherit = (uproot.behaviors.TH1.TH1,)` hint me to think this way.\r\n\r\nLet me know if I should remove it @jpivarski or if maybe I should have made a separate PR for this. Perhaps I should've implemented it in the `TH1` class instead of `Histogram` class.",
  "created_at":"2022-11-01T18:11:39Z",
  "id":1298919790,
  "issue":774,
  "node_id":"IC_kwDOD6Q_ss5Na_Fu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-01T18:11:39Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Looking at the code of `TH{1,2,3}.py` I see it can be refactored in a similar way to what is done in `to_boost` in this PR (implement `to_numpy`, etc in `Histogram`). This would leave only `axes` property and perhaps the `axis` method on `TH{1,2,3}.py`.\r\n\r\nI don't think the complexity of the code increases enough to discourage this and it would reduce code duplication (triplication!), but I would love to hear some thoughts @jpivarski @agoose77.",
  "created_at":"2022-11-02T08:03:24Z",
  "id":1299749401,
  "issue":774,
  "node_id":"IC_kwDOD6Q_ss5NeJoZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-02T08:03:24Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Generally, I didn't worry about code duplication up to three, particularly if it was right next to each other and formed a clear pattern. (The problem with duplicated code is that a future maintainer might fix one copy without applying the same update to the other copies, and then they get out of date until another future maintainer wonders if they were _supposed_ to be the same or not.) In the case of histograms, I knew that the code duplication/triplication would never go beyond three, since ROOT only defines 1, 2, and 3-D histograms.\r\n\r\nI'll let you decide whether you want to squash more duplication. As it is, I would accept this PR now. So if you consider yourself done, go ahead and do the honors by pressing the \"squash and merge\" button. (Make sure it's set to \"squash and merge\" mode; we don't want all of the commits in this PR to be individual items in the global history.)\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/199512497-5ac2a48c-49ec-45e0-b792-39e1a2cf4fc4.png)\r\n",
  "created_at":"2022-11-02T14:15:22Z",
  "id":1300498663,
  "issue":774,
  "node_id":"IC_kwDOD6Q_ss5NhAjn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-02T14:15:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The language used in [aliases](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#aliases) is by default [Python](https://uproot.readthedocs.io/en/latest/uproot.language.python.PythonLanguage.html), and currently that's also the only option. (To use TTreeFormula, we would need some fixes to the long-dormant [formulate](https://github.com/scikit-hep/formulate) package.) It looks like you know that because you're using the `where` function, which is part of NumPy's namespace, but not TTreeFormula.\r\n\r\nThe Python `scope` when computing this expression\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/ec80b88737055679ef86957d4ac61fab00ce682a/src/uproot/language/python.py#L429\r\n\r\ndoes not include $\\pi$, nor is it one of the functions we inject into this `scope`:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/ec80b88737055679ef86957d4ac61fab00ce682a/src/uproot/language/python.py#L257-L318\r\n\r\nAnd, anyway, you wouldn't want it to be a function: `PI()`.\r\n\r\nSo it looks like there is no $\\pi$ in this execution environment nor has there ever been one. The one example that appears to work is... probably?... a TTree with a TBranch named \"PI\".\r\n\r\nSo, _should_ we have a $\\pi$ in this language/scope? And what should it be named? And what about Euler's constant, $e$?\r\n\r\n* Python adopts C/C++ convention for constants, so maybe it ought to be `PI` and `E`, except that $\\pi$ is `np.pi` and `math.pi` and all the other functions follow NumPy naming conventions (except that both `asin` and `arcsin` are provided; different languages disagree on whether the whole word \"arc\" is needed). So maybe they should be lowercase, like `pi` and `e`?\r\n* A single-letter constant like `E` or `e` is pretty dangerous. In many physics contexts, a single-letter \"e\" would represent energy. Presumably, TTree branch names should take precedence over named constants, but there would be a danger of someone thinking they have an `E` or `e` TBranch, but get Euler's constant instead.\r\n* NumPy and Python's math module don't have Euler's constant because it's easy enough to say `exp(1)`.\r\n* By the same token, couldn't you say `2*asin(1)` for $\\pi$? I suppose, but it would make the formulas unclear.\r\n* You know, in your [arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.HasBranches.html#uproot-behaviors-tbranch-hasbranches-arrays), [iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.HasBranches.html#uproot-behaviors-tbranch-hasbranches-iterate), [uproot.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html), or [uproot.concatenate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.concatenate.html) function call, you could pass `aliases={\"PI\": \"2*asin(1)\"}` to add a new alias, which is used with the others. The others may be embedded in the file or passed explicitly: what gets computed is the union of both TTree's `fAliases` and explicit `aliases`. I think there's also no order dependence to the declarations and references of those functions (because that enables self-reference, for defining recursive functions).\r\n\r\n**What about that last suggestion: define your own `PI`?** I could convert this Issue into a Discussion and this would become a recommendation, rather than a change to the code. The good thing about define-your-own-`PI` is that you can't be surprised about its presence or absence, unless you happen to have a TTree branch named \"PI\".",
  "created_at":"2022-11-02T17:51:20Z",
  "id":1301012539,
  "issue":775,
  "node_id":"IC_kwDOD6Q_ss5Ni-A7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-02T17:51:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I found the issue. I have a branch called `phiModCalo` in the tree. So it seems that the alias `\"phiModCalo\": \"...\"` is ignored. Maybe a warning in this case?",
  "created_at":"2022-11-02T21:59:19Z",
  "id":1301368983,
  "issue":775,
  "node_id":"IC_kwDOD6Q_ss5NkVCX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-02T21:59:33Z",
  "user":"MDQ6VXNlcjE0MzM4OQ=="
 },
 {
  "author_association":"MEMBER",
  "body":"So the branch named \"phiModCalo\" took precedence over the alias defined with name \"phiModCalo\". I think it would be fine to raise an error in case of conflict, or maybe a warning because I'm thinking about cases where that behavior would be desired.\r\n\r\nBut is it really the case that when this happened, the error message was\r\n\r\n```\r\nKeyInFileError: not found: 'PI'\r\n```\r\n\r\n? Because an error message that bad is a bug.",
  "created_at":"2022-11-02T22:41:47Z",
  "id":1301456944,
  "issue":775,
  "node_id":"IC_kwDOD6Q_ss5Nkqgw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-02T22:41:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, wait a minute! It was that `PI` is not defined in either case, but the alias is ignored when it's overshadowed by a TBranch name, right? You were seeing the\r\n\r\n```\r\nKeyInFileError: not found: 'PI'\r\n```\r\n\r\nerror when it attempted to compute `DeltaPhiTG3`, but it never attempted to compute `phiModCalo` because that's a TBranch name.\r\n\r\nIn which case, we can't fix that error, but we can add a warning when an alias name conflicts with a TBranch name.",
  "created_at":"2022-11-02T22:49:30Z",
  "id":1301463895,
  "issue":775,
  "node_id":"IC_kwDOD6Q_ss5NksNX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-02T22:49:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thank you @jpivarski.\r\nAs you said, if the alias is trying to redefine an existing branch it has no effect, even if it contains an undefined symbol. And I don't have any error/warning.\r\nIf the alias is trying to define a new variable (not already existing) and it contains an undefined symbol (PI) it raises an error.\r\n\r\nThe behaviour is correct. We are talking about adding a warning in case the user tries to define an alias with the same name as a branch. Let discuss this in the MR.",
  "created_at":"2022-11-03T08:33:52Z",
  "id":1301782274,
  "issue":775,
  "node_id":"IC_kwDOD6Q_ss5Nl58C",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-03T08:34:11Z",
  "user":"MDQ6VXNlcjE0MzM4OQ=="
 },
 {
  "author_association":"MEMBER",
  "body":"I need convincing that this is a good thing to do. (I'm still thinking about it.)\r\n\r\nIn particular, let me know, @wiso, if this would have caught your problem. (Note: it's only being implemented for 5.x, since it's a new feature; I'm asking if this is the sort of thing that would have fixed it, not whether you can successfully run with this patch.)",
  "created_at":"2022-11-02T23:16:37Z",
  "id":1301481687,
  "issue":776,
  "node_id":"IC_kwDOD6Q_ss5NkwjX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-02T23:16:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I have checkout this branch and I confirm that in the example in #775 now I get the warning\r\n\r\n```\r\n/home/turra/uproot5/src/uproot/language/python.py:422: NameConflictWarning: 'phiModCalo' is both an alias and a branch name\r\n  warnings.warn(\r\n         phiModCalo\r\n0          0.005856\r\n1         -0.004650\r\n2         -0.005619\r\n3         -0.003007\r\n4         -0.004062\r\n...             ...\r\n4650957    0.002802\r\n4650958   -0.007556\r\n4650959    0.001636\r\n4650960   -0.002500\r\n4650961    0.003881\r\n\r\n[4650962 rows x 1 columns]\r\n\r\n```\r\n\r\nwhile in the other case I get the same error as before, as expected.\r\n\r\nSo, yes, this would have caught my problem.\r\n\r\nNow I am thinking if this is always the desired behavior. For example you can have an input file where you know that one branch is wrong, but you can recompute it from other branches. In this case you may want to overwrite it with an alias, with the same name of the branch (since maybe you have other codes that assume that particular branch name). By the way this is just a warning, and not an error, so I guess it is ok.\r\n\r\nAs a comparison RDataFrame is raising an error when calling `Define`, e.g.\r\n```\r\nruntime_error: RDataFrame::Define: cannot define column \"phiModCalo\". A branch with that name is already present in the input TTree/TChain. Use Redefine to force redefinition.\r\n  ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager,void> ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager,void>::Define(basic_string_view<char,char_traits<char> > name, basic_string_view<char,char_traits<char> > expression) =>\r\n    runtime_error: RDataFrame::Define: cannot define column \"phiModCalo\". A branch with that name is already present in the input TTree/TChain. Use Redefine to force redefinition.\r\n```\r\n\r\nby the way they have a method `Redefine` which works.",
  "created_at":"2022-11-03T08:48:34Z",
  "id":1301795531,
  "issue":776,
  "node_id":"IC_kwDOD6Q_ss5Nl9LL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-03T08:53:23Z",
  "user":"MDQ6VXNlcjE0MzM4OQ=="
 },
 {
  "author_association":"MEMBER",
  "body":"I made it a warning, rather than an error, to prevent undue restrictions that might not even be under a user's control: a ROOT file could contain a TBranch `fName` in a TTree that has an `fAliases` of the same name\u2014I don't want to make it impossible to read such a file.\r\n\r\nAdding the ability to choose to the API, as RDataFrame has with `Define` and `Redefine`, would expand the size of the Uproot expressions API. This is all in the name of making something easier; having more knobs to turn\u2014and knowing about them!\u2014is not necessarily easier.\r\n\r\nUltimately, I don't think physicists should be doing their analysis in the Uproot expressions API. It's not comparable to RDataFrame, in which you're supposed to put code into `Define` nodes as strings, and there's a benefit to doing so. As long as the Uproot expressions are just Python expressions in strings, there's neither an interface nor a performance benefit to doing so. In other fora (Gitter, mostly), I've been recommending against using `expressions` for computations.\r\n\r\nWhy are they there at all? Here's how it went, historically:\r\n\r\n   1. Uproot didn't support `fAliases`, and some ROOT files have significant simplifications in `fAliases` (such as `edm::Product/many/levels/deep/Muon/pt` \u2192 `Muon_pt`).\r\n   2. To support `fAliases`, Uproot needs to evaluate strings as expressions, ideally as TTreeFormula, since that's the language they were developed for.\r\n   3. If Uproot is going to evaluate expressions anyway, it might as well take user-provided expressions as well as the ones that are baked into the `fAliases` member of the TTree.[^1]\r\n   4. I didn't have time to write a TTreeFormula parser. The [formulate](https://github.com/scikit-hep/formulate) project ought to do it, but it's [broken](https://github.com/scikit-hep/formulate/issues/23) and doesn't have an active maintainer. So Uproot will interpret the `expressions` as Python \"for now.\"\r\n   5. The state of things hasn't changed.\r\n\r\nSo I'm not wanting to build up the expressions API more than it already is, which would include providing an API to express the define/redefine distinction, and raising an error on redefinition might make TTrees unreadable in ways that have little to do with the redefined expression, which users can't change.\r\n\r\nI think this is the least-bad option. I'd support anyone who wants to revamp formulate ([writing parsers is fun!](https://github.com/jpivarski/2019-05-06-adl-language-tools#how-to-build-your-own-language)), though it's too late for a PythonLanguage \u2192 TTreeFormulaLanguage switch in defaults before Uproot version 5.0.0 (early December). We could do it later with a deprecation cycle.\r\n\r\nI guess, then, I'll merge this PR.\r\n\r\n[^1]: Maybe that was the fatal error, adding an interface only because the underlying implementation exists, not because of a clear user-need.",
  "created_at":"2022-11-03T17:29:38Z",
  "id":1302445684,
  "issue":776,
  "node_id":"IC_kwDOD6Q_ss5Nob50",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-03T17:33:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thank you @jpivarski. By the way, what you are saying about aliases is worrying me. I am moving from uproot3 to 4. Before I used uproot to get a panda dataframe and then I used numexpr to compute new columns. Now, with uproot4, I am using alias to do the same. Should I go back? Probably not since I am limited by I/O and not computation.",
  "created_at":"2022-11-04T09:11:59Z",
  "id":1303152792,
  "issue":776,
  "node_id":"IC_kwDOD6Q_ss5NrIiY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-04T09:11:59Z",
  "user":"MDQ6VXNlcjE0MzM4OQ=="
 },
 {
  "author_association":"MEMBER",
  "body":"I think the problem with the `expressions` interface is that it makes it look like the expressions are calculated very early, perhaps even while the data are being read. This is not the case. It just pulls out arrays like normal, then runs the Python code in the strings. That's something that you could do as easily by running Python code not in strings.\r\n\r\nSince that Python code is primarily NumPy ufuncs, NumExpr will be more efficient because it is a single pass over the arrays, rather than one pass per mathematical expression. (It's more complicated than that now with the fact that NumPy can fuse _some_ operations, but with NumExpr, the entire expression is always fused.) This difference circumvents the potential bottleneck between RAM and CPU, which is roughly 10\u201220 Gbps. If you're I/O limited, meaning disk I/O, the fastest SSDs are 7 Gbps (and that's extreme), so it's probably not a limiting factor.\r\n\r\nTo directly answer your question: the NumExpr strings would be faster, but you're limited by I/O anyway so you probably won't see it. But more importantly, prefer doing computations in Pandas and only I/O in Uproot because Pandas is a computation library and Uproot is an I/O library. Pandas, RDataFrame, Awkward Array, NumExpr, Numba, Dask, xarray, ... should all be better at doing computations than Uproot.\r\n\r\n------------\r\n\r\n~~However, you just gave me an idea: crossing the boundary from Uproot 4.x to Uproot 5.x (this December), we're allowed to make backward-incompatible changes. As I was saying above, there isn't enough time to develop a TTreeFormula expression-handler, but we _could_ replace the default Python expression-handler with a NumExpr expression-handler. The NumExpr language is more limited but faster, which at least creates a distinction between what you can do in strings and what you can do outside of strings.~~\r\n\r\nI crossed out the above after thinking about it for a few minutes. Rushing an interface breaking change on this motivation sounds like a bad idea. We could take our time and add the NumExpr language (possibly with TTreeFormula \u2192 NumExpr translation, which would exclude the slice/loop/reducer parts of the TTreeFormula language) as a non-default, and maybe change the default through a deprecation cycle. There could be a range of version numbers in which non-trivial expressions require you to explicitly set the `language=...` argument, and after that phase, the default `language` changes.",
  "created_at":"2022-11-04T13:57:30Z",
  "id":1303590320,
  "issue":776,
  "node_id":"IC_kwDOD6Q_ss5NszWw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-04T13:57:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'm running into this error as well (in ServiceX). I'd expect this to do the same thing as `output_file['tree'] = {'a': ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]]), 'b': ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]]), 'c': ak.Array([1, 2, 3])}` (which does work).",
  "created_at":"2022-12-13T19:47:55Z",
  "id":1349600670,
  "issue":777,
  "node_id":"IC_kwDOD6Q_ss5QcUWe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-13T19:47:55Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"Noted: this is an important user-expectation. I think I've answered this question on Gitter, too.",
  "created_at":"2022-12-13T20:15:45Z",
  "id":1349630184,
  "issue":777,
  "node_id":"IC_kwDOD6Q_ss5Qcbjo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-13T20:15:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"In general, I'm all for tightening our input validation. This seems like a good policy change.",
  "created_at":"2022-11-11T12:04:20Z",
  "id":1311614383,
  "issue":779,
  "node_id":"IC_kwDOD6Q_ss5OLaWv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-11T12:04:20Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"It wasn't the Forth generation that was a problem; it was the non-Forth path. By doing Forth first and then non-Forth with the same objects, something was set such that it didn't actually run the non-Forth path.\r\n\r\nThe thing that was broken in the non-Forth path was adjusting for `\"__array__\": \"sorted_map\"` on the RecordForm instead of the ListOffsetForm that contains the RecordForm. (Remember how we changed that?) It was correct in Forth but not in non-Forth.\r\n\r\nBut thanks for the check!",
  "created_at":"2022-11-11T15:01:32Z",
  "id":1311795231,
  "issue":780,
  "node_id":"IC_kwDOD6Q_ss5OMGgf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-11T15:01:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I do believe this section is already being removed in the current awkward pandas PR.\n\n\nhttps://github.com/scikit-hep/uproot5/commit/85c8be583deb7c76ef835ca51ba9346490b44050",
  "created_at":"2022-11-11T01:42:00Z",
  "id":1311123953,
  "issue":781,
  "node_id":"IC_kwDOD6Q_ss5OJinx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-11T01:42:00Z",
  "user":"MDQ6VXNlcjUzNjUwNTM4"
 },
 {
  "author_association":"MEMBER",
  "body":"Great! In that case, let's not do it twice, so that there are no merge issues.",
  "created_at":"2022-11-11T01:43:21Z",
  "id":1311124562,
  "issue":781,
  "node_id":"IC_kwDOD6Q_ss5OJixS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-11T01:43:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I was going to approve this, but then I started to wonder; should we version the URLs to `root-project` so that this doesn't happen again? It would mean that they might become out of date, but I wonder how that stacks up against them just plain breaking due to renames etc. Also, we probably want them to be versioned as of the last edit of the code, rather than moving with ROOT's development.",
  "created_at":"2022-11-11T09:59:02Z",
  "id":1311482970,
  "issue":782,
  "node_id":"IC_kwDOD6Q_ss5OK6Ra",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-11T09:59:02Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"If you don't get a chance to look at this, @kkothari2001 (because I know you're busy), that's okay! I just wanted to give you a chance because the `interp_options` mechanism threads through your `uproot.dask` work.",
  "created_at":"2022-11-18T21:01:03Z",
  "id":1320517557,
  "issue":784,
  "node_id":"IC_kwDOD6Q_ss5OtX-1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-18T21:01:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski why _don't_ we always want to do this? My first thought was \"let's not add any options, and just make this the behaviour\" before reading your remark",
  "created_at":"2022-11-18T21:39:25Z",
  "id":1320556657,
  "issue":784,
  "node_id":"IC_kwDOD6Q_ss5Othhx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-18T21:39:25Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I had the same thought until I saw it:\r\n\r\n```python\r\nimport uproot\r\ntree = uproot.open(\"nano_dy.root:Events\")\r\narray = tree.arrays(filter_name=\"Muon*\", ak_add_doc=True)\r\narray.show(type=True)\r\n```\r\n\r\n```\r\ntype: 40 * struct[{\r\n    Muon_dxy: [var * float32, parameters={\"__doc__\": \"dxy (with sign) wrt first PV, in cm\"}],\r\n    Muon_dxyErr: [var * float32, parameters={\"__doc__\": \"dxy uncertainty, in cm\"}],\r\n    Muon_dz: [var * float32, parameters={\"__doc__\": \"dz (with sign) wrt first PV, in cm\"}],\r\n    Muon_dzErr: [var * float32, parameters={\"__doc__\": \"dz uncertainty, in cm\"}],\r\n    Muon_eta: [var * float32, parameters={\"__doc__\": \"eta\"}],\r\n    Muon_ip3d: [var * float32, parameters={\"__doc__\": \"3D impact parameter wrt first PV, in cm\"}],\r\n    Muon_jetPtRelv2: [var * float32, parameters={\"__doc__\": \"Relative momentum of the lepton with respect to the closest jet after subtracting the lepton\"}],\r\n    Muon_jetRelIso: [var * float32, parameters={\"__doc__\": \"Relative isolation in matched jet (1/ptRatio-1, pfRelIso04_all if no matched jet)\"}],\r\n    Muon_mass: [var * float32, parameters={\"__doc__\": \"mass\"}],\r\n    Muon_miniPFRelIso_all: [var * float32, parameters={\"__doc__\": \"mini PF relative isolation, total (with scaled rho*EA PU corrections)\"}],\r\n    Muon_miniPFRelIso_chg: [var * float32, parameters={\"__doc__\": \"mini PF relative isolation, charged component\"}],\r\n    Muon_pfRelIso03_all: [var * float32, parameters={\"__doc__\": \"PF relative isolation dR=0.3, total (deltaBeta corrections)\"}],\r\n    Muon_pfRelIso03_chg: [var * float32, parameters={\"__doc__\": \"PF relative isolation dR=0.3, charged component\"}],\r\n    Muon_pfRelIso04_all: [var * float32, parameters={\"__doc__\": \"PF relative isolation dR=0.4, total (deltaBeta corrections)\"}],\r\n    Muon_phi: [var * float32, parameters={\"__doc__\": \"phi\"}],\r\n    Muon_pt: [var * float32, parameters={\"__doc__\": \"pt\"}],\r\n    Muon_ptErr: [var * float32, parameters={\"__doc__\": \"ptError of the muon track\"}],\r\n    Muon_segmentComp: [var * float32, parameters={\"__doc__\": \"muon segment compatibility\"}],\r\n    Muon_sip3d: [var * float32, parameters={\"__doc__\": \"3D impact parameter significance wrt first PV\"}],\r\n    Muon_softMva: [var * float32, parameters={\"__doc__\": \"soft MVA ID score\"}],\r\n    Muon_tkRelIso: [var * float32, parameters={\"__doc__\": \"Tracker-based relative isolation dR=0.3 for highPt, trkIso/tunePpt\"}],\r\n    Muon_tunepRelPt: [var * float32, parameters={\"__doc__\": \"TuneP relative pt, tunePpt/pt\"}],\r\n    Muon_mvaLowPt: [var * float32, parameters={\"__doc__\": \"Low pt muon ID score\"}],\r\n    Muon_mvaTTH: [var * float32, parameters={\"__doc__\": \"TTH MVA lepton ID score\"}],\r\n    Muon_charge: [var * int32, parameters={\"__doc__\": \"electric charge\"}],\r\n    Muon_jetIdx: [var * int32, parameters={\"__doc__\": \"index of the associated jet (-1 if none)\"}],\r\n    Muon_nStations: [var * int32, parameters={\"__doc__\": \"number of matched stations with default arbitration (segment & track)\"}],\r\n    Muon_nTrackerLayers: [var * int32, parameters={\"__doc__\": \"number of layers in the tracker\"}],\r\n    Muon_pdgId: [var * int32, parameters={\"__doc__\": \"PDG code assigned by the event reconstruction (not by MC truth)\"}],\r\n    Muon_tightCharge: [var * int32, parameters={\"__doc__\": \"Tight charge criterion using pterr/pt of muonBestTrack (0:fail, 2:pass)\"}],\r\n    Muon_fsrPhotonIdx: [var * int32, parameters={\"__doc__\": \"Index of the associated FSR photon\"}],\r\n    Muon_highPtId: [var * uint8, parameters={\"__doc__\": \"high-pT cut-based ID (1 = tracker high pT, 2 = global high pT, which includes tracker high pT)\"}],\r\n    Muon_inTimeMuon: [var * bool, parameters={\"__doc__\": \"inTimeMuon ID\"}],\r\n    Muon_isGlobal: [var * bool, parameters={\"__doc__\": \"muon is global muon\"}],\r\n    Muon_isPFcand: [var * bool, parameters={\"__doc__\": \"muon is PF candidate\"}],\r\n    Muon_isTracker: [var * bool, parameters={\"__doc__\": \"muon is tracker muon\"}],\r\n    Muon_looseId: [var * bool, parameters={\"__doc__\": \"muon is loose muon\"}],\r\n    Muon_mediumId: [var * bool, parameters={\"__doc__\": \"cut-based ID, medium WP\"}],\r\n    Muon_mediumPromptId: [var * bool, parameters={\"__doc__\": \"cut-based ID, medium prompt WP\"}],\r\n    Muon_miniIsoId: [var * uint8, parameters={\"__doc__\": \"MiniIso ID from miniAOD selector (1=MiniIsoLoose, 2=MiniIsoMedium, 3=MiniIsoTight, 4=MiniIsoVeryTight)\"}],\r\n    Muon_multiIsoId: [var * uint8, parameters={\"__doc__\": \"MultiIsoId from miniAOD selector (1=MultiIsoLoose, 2=MultiIsoMedium)\"}],\r\n    Muon_mvaId: [var * uint8, parameters={\"__doc__\": \"Mva ID from miniAOD selector (1=MvaLoose, 2=MvaMedium, 3=MvaTight)\"}],\r\n    Muon_pfIsoId: [var * uint8, parameters={\"__doc__\": \"PFIso ID from miniAOD selector (1=PFIsoVeryLoose, 2=PFIsoLoose, 3=PFIsoMedium, 4=PFIsoTight, 5=PFIsoVeryTight, 6=PFIsoVeryVeryTight)\"}],\r\n    Muon_softId: [var * bool, parameters={\"__doc__\": \"soft cut-based ID\"}],\r\n    Muon_softMvaId: [var * bool, parameters={\"__doc__\": \"soft MVA ID\"}],\r\n    Muon_tightId: [var * bool, parameters={\"__doc__\": \"cut-based ID, tight WP\"}],\r\n    Muon_tkIsoId: [var * uint8, parameters={\"__doc__\": \"TkIso ID (1=TkIsoLoose, 2=TkIsoTight)\"}],\r\n    Muon_triggerIdLoose: [var * bool, parameters={\"__doc__\": \"TriggerIdLoose ID\"}],\r\n    Muon_genPartIdx: [var * int32, parameters={\"__doc__\": \"Index into genParticle list for MC matching to status==1 muons\"}],\r\n    Muon_genPartFlav: [var * uint8, parameters={\"__doc__\": \"Flavour of genParticle for MC matching to status==1 muons: 1 = prompt muon (including gamma*->mu mu), 15 = muon from prompt tau, 5 = muon from b, 4 = muon from c, 3 = muon from light or unknown, 0 = unmatched\"}],\r\n    Muon_cleanmask: [var * uint8, parameters={\"__doc__\": \"simple cleaning mask with priority to leptons\"}]\r\n}, parameters={\"__doc__\": \"Events\"}]\r\n[{Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [-0.000319, -0.00682], Muon_dxyErr: [...], Muon_dz: [...], ...},\r\n {Muon_dxy: [-0.00011], Muon_dxyErr: [0.00162], Muon_dz: [0.0026], ...},\r\n {Muon_dxy: [0.00324, -0.00244], Muon_dxyErr: [0.00229, ...], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n ...,\r\n {Muon_dxy: [0.000774], Muon_dxyErr: [0.00229], Muon_dz: [-0.000873], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [-0.000587], Muon_dxyErr: [0.00162], Muon_dz: [0.000254], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...}]\r\n```\r\n\r\nversus\r\n\r\n```python\r\narray = tree.arrays(filter_name=\"Muon*\", ak_add_doc=True)\r\narray.show(type=True)\r\n```\r\n\r\n```\r\ntype: 40 * {\r\n    Muon_dxy: var * float32,\r\n    Muon_dxyErr: var * float32,\r\n    Muon_dz: var * float32,\r\n    Muon_dzErr: var * float32,\r\n    Muon_eta: var * float32,\r\n    Muon_ip3d: var * float32,\r\n    Muon_jetPtRelv2: var * float32,\r\n    Muon_jetRelIso: var * float32,\r\n    Muon_mass: var * float32,\r\n    Muon_miniPFRelIso_all: var * float32,\r\n    Muon_miniPFRelIso_chg: var * float32,\r\n    Muon_pfRelIso03_all: var * float32,\r\n    Muon_pfRelIso03_chg: var * float32,\r\n    Muon_pfRelIso04_all: var * float32,\r\n    Muon_phi: var * float32,\r\n    Muon_pt: var * float32,\r\n    Muon_ptErr: var * float32,\r\n    Muon_segmentComp: var * float32,\r\n    Muon_sip3d: var * float32,\r\n    Muon_softMva: var * float32,\r\n    Muon_tkRelIso: var * float32,\r\n    Muon_tunepRelPt: var * float32,\r\n    Muon_mvaLowPt: var * float32,\r\n    Muon_mvaTTH: var * float32,\r\n    Muon_charge: var * int32,\r\n    Muon_jetIdx: var * int32,\r\n    Muon_nStations: var * int32,\r\n    Muon_nTrackerLayers: var * int32,\r\n    Muon_pdgId: var * int32,\r\n    Muon_tightCharge: var * int32,\r\n    Muon_fsrPhotonIdx: var * int32,\r\n    Muon_highPtId: var * uint8,\r\n    Muon_inTimeMuon: var * bool,\r\n    Muon_isGlobal: var * bool,\r\n    Muon_isPFcand: var * bool,\r\n    Muon_isTracker: var * bool,\r\n    Muon_looseId: var * bool,\r\n    Muon_mediumId: var * bool,\r\n    Muon_mediumPromptId: var * bool,\r\n    Muon_miniIsoId: var * uint8,\r\n    Muon_multiIsoId: var * uint8,\r\n    Muon_mvaId: var * uint8,\r\n    Muon_pfIsoId: var * uint8,\r\n    Muon_softId: var * bool,\r\n    Muon_softMvaId: var * bool,\r\n    Muon_tightId: var * bool,\r\n    Muon_tkIsoId: var * uint8,\r\n    Muon_triggerIdLoose: var * bool,\r\n    Muon_genPartIdx: var * int32,\r\n    Muon_genPartFlav: var * uint8,\r\n    Muon_cleanmask: var * uint8\r\n}\r\n[{Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [-0.000319, -0.00682], Muon_dxyErr: [...], Muon_dz: [...], ...},\r\n {Muon_dxy: [-0.00011], Muon_dxyErr: [0.00162], Muon_dz: [0.0026], ...},\r\n {Muon_dxy: [0.00324, -0.00244], Muon_dxyErr: [0.00229, ...], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n ...,\r\n {Muon_dxy: [0.000774], Muon_dxyErr: [0.00229], Muon_dz: [-0.000873], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [-0.000587], Muon_dxyErr: [0.00162], Muon_dz: [0.000254], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...},\r\n {Muon_dxy: [], Muon_dxyErr: [], Muon_dz: [], Muon_dzErr: [], ...}]\r\n```",
  "created_at":"2022-11-18T21:51:29Z",
  "id":1320566703,
  "issue":784,
  "node_id":"IC_kwDOD6Q_ss5Otj-v",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-18T21:51:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I agree with Jim it should be opt-in, we use it for some very particular user-facing features that tie into notebook use.\r\n\r\nThat gives me the idea it should be sensitive to if you're in ipython/jupyter or not and change defaultness depending on that?\r\nThat has some sense to it, since it gives useful features in situations where you can take advantage of them.",
  "created_at":"2022-11-18T21:56:12Z",
  "id":1320570575,
  "issue":784,
  "node_id":"IC_kwDOD6Q_ss5Otk7P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-18T21:56:12Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I'm generally not in favour of environment-specific behaviour; it's hard to know that it's happening without discovering it (usually accidentally), and harder still to google what's happening! \r\n\r\nWhat kind of features do you use `__doc__` for @lgray? You've piqued my interest!\r\n\r\n@jpivarski that's maybe suggesting to me that we should elide long parameters rather than we should not set them unless opt-in? Could a solution be to make parameters > N characters collapse to ellipsis, and make `__doc__` non-optional?",
  "created_at":"2022-11-18T22:06:24Z",
  "id":1320578806,
  "issue":784,
  "node_id":"IC_kwDOD6Q_ss5Otm72",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-18T22:06:24Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> What kind of features do you use `__doc__` for @lgray? You've piqued my interest!\r\n\r\nRight now it's really this very user facing documentation of what branches in TTrees do (if the designer of the TTree cares to fill it). The point is largely to have the capability there so that it can be exploited and so the data further serves as its own documentation. I could imagine people filling fairly rich descriptions of TTrees or branches or using doc strings to contain example analysis patterns for the data.",
  "created_at":"2022-11-18T22:25:00Z",
  "id":1320597689,
  "issue":784,
  "node_id":"IC_kwDOD6Q_ss5Otri5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-18T22:25:00Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks, @agoose77 and @kkothari2001!",
  "created_at":"2022-11-20T03:52:39Z",
  "id":1321028376,
  "issue":784,
  "node_id":"IC_kwDOD6Q_ss5OvUsY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-20T03:52:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The cascadetree seems to support this, btw, but that's the low-level API.\r\n\r\n**EDIT**: Nope, the low-level tree also does not allow one to set it after its creation.",
  "created_at":"2022-11-24T13:11:52Z",
  "id":1326435591,
  "issue":785,
  "node_id":"IC_kwDOD6Q_ss5PD80H",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-24T13:13:09Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"The title can be set with a high-level function, as a keyword argument of [uproot.WritableDirectory.mktree](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableDirectory.html#mktree):\r\n\r\n```python\r\n>>> import uproot\r\n>>> import numpy as np\r\n>>> with uproot.recreate(\"/tmp/tmp.root\") as file:\r\n...     file.mktree(\"name\", {\"x\": np.float32, \"y\": np.int64}, title=\"title\")\r\n...     file[\"name\"].extend({\r\n...         \"x\": np.arange(10, dtype=np.float32),\r\n...         \"y\": np.arange(10, dtype=np.int64),\r\n...     })\r\n... \r\n<WritableTree '/name' at 0x7f86b52d9280>\r\n>>> with uproot.open(\"/tmp/tmp.root\") as file:\r\n...     tree = file[\"name\"]\r\n...     print(f\"{tree.title = }\")\r\n...     tree.show()\r\n... \r\ntree.title = 'title'\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nx                    | float                    | AsDtype('>f4')\r\ny                    | int64_t                  | AsDtype('>i8')\r\n```",
  "created_at":"2022-11-24T17:39:20Z",
  "id":1326727777,
  "issue":785,
  "node_id":"IC_kwDOD6Q_ss5PFEJh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-24T17:39:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You can set the title when you use `mktree`, but then you cannot use the other feature where multiple jagged arrays share a common branch to count the varlength per event. There is no easily usable branch-specification for this in mktree. To use the shared branch, one has to use the other syntax `f[\"my_tree\"] = {\"\": ak.zip( ... )}` which does not allow one to set the title.\r\n\r\nI got this working for me only by monkey-patching `WriteableDirectory.mktree`, which is implicitly called by the above code.\r\n\r\ntl;dr: The underlying issue is that the shared varlength branch is only available through special syntax like `f[\"my_tree\"] = {\"\": ak.zip( ... )}` and not via a proper specification upfront with `mktree`.",
  "created_at":"2022-11-25T13:32:13Z",
  "id":1327483787,
  "issue":785,
  "node_id":"IC_kwDOD6Q_ss5PH8uL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-25T13:37:56Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"You can use mktree with that jagged data type. When you've created the array with `ak.zip` ask for the `type` property of the array. That's what you need to provide to mktree.\r\n\r\n```python\r\narray = ak.zip(...)\r\nf.mktree(\"my_tree\", {\"\": array.type}, title=\"my_title\")\r\n```",
  "created_at":"2022-11-25T14:22:23Z",
  "id":1327538307,
  "issue":785,
  "node_id":"IC_kwDOD6Q_ss5PIKCD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-25T14:22:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"How do we query everyone to ask if these are \"sensible\" places? Well, we can always back off\u2014move the keyword-only `*` later in the list\u2014that direction is backward compatible. Maybe we'll start here and if the first adopters of Uproot 5.0.0 complain, we'll move them back for 5.0.1 and subsequent.",
  "created_at":"2022-11-29T00:04:10Z",
  "id":1329902934,
  "issue":787,
  "node_id":"IC_kwDOD6Q_ss5PRLVW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-29T00:04:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for your comments! The reason I didn't make `expressions` keyword-only is because\r\n\r\n```python\r\narrays = tree.arrays([\"branch1\", \"branch2\", \"branch3\"])\r\n```\r\n\r\nis a pretty common pattern.\r\n\r\nAn argument for making people write\r\n\r\n```python\r\narrays = tree.arrays(expressions=[\"branch1\", \"branch2\", \"branch3\"])\r\n```\r\n\r\nwould be to avoid the confusion that these are, in fact, expressions, so `\"branch*\"` does not match all branches; it's a syntax error because `*` is multiplication and there's nothing on the right hand side. (A user who does this really wants `filter_names`.)\r\n\r\nAn argument against it is that if `expressions` were keyword-only, the error message would say \"the 'arrays' method takes 0 positional arguments\" and that would nudge newbies to write\r\n\r\n```python\r\narrays = tree.arrays()\r\n```\r\n\r\nwhich means \"read all branches,\" which has bad performance consequences. I wish the Python error message would list the names of the keyword-only arguments; maybe some new version of Python does or some future version will.\r\n\r\nIf `expressions` is positional, then `cut` should be positional as well, because they're similar sorts of things. (The first is a mapper, the second is a filter.) And there's a long tradition of expressions and cuts being the first two positional arguments. In [PAW](https://en.wikipedia.org/wiki/Physics_Analysis_Workstation), you'd write\r\n\r\n```fortran\r\nnt/plot 10.sqrt(px**2+py**2) abs(eta)<2.4.and.isolation>0.5 ! ! ! ! 100\r\n```\r\n\r\nto plot the expression `sqrt(px**2+py**2)` from an ntuple named `10`, filtered by `abs(eta)<2.4.and.isolation>0.5`, and fill a histogram named `100`. The expressions and the cuts are the first two arguments; the histogram-to-fill is the seventh argument in a language without keywords so the `!`s skip the middle four.\r\n\r\nAnyway, point being that the order\r\n\r\n   1. expression\r\n   2. cut\r\n   3. other stuff\r\n\r\nis a long-standing convention. I'm afraid that making people type these names out will start a revolt.",
  "created_at":"2022-11-29T16:15:36Z",
  "id":1330896901,
  "issue":787,
  "node_id":"IC_kwDOD6Q_ss5PU-AF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-29T16:19:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for all those additional details! I agree with your conclusion, the choice here seems very well motivated.",
  "created_at":"2022-11-29T16:28:05Z",
  "id":1330913767,
  "issue":787,
  "node_id":"IC_kwDOD6Q_ss5PVCHn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-29T16:28:05Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @jpivarski, seeing your comment above on Python error messages. Have you considered the `(*, )` argument? The following seems understandable to me:\r\n```\r\nIn [1]: def test1(x=0):\r\n   ...:     print(x)\r\n   ...:\r\nIn [2]: def test2(*, x=0):\r\n   ...:     print(x)\r\n   ...:\r\nIn [3]: test1()\r\n0\r\nIn [4]: test2()\r\n0\r\nIn [5]: test1(5)\r\n5\r\nIn [6]: test2(5)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nInput In [6], in <cell line: 1>()\r\n----> 1 test2(5)\r\n\r\nTypeError: test2() takes 0 positional arguments but 1 was given\r\n\r\nIn [7]: test1(x=5)\r\n5\r\n\r\nIn [8]: test2(x=5)\r\n5\r\n```\r\nJust a quick thought.",
  "created_at":"2022-11-29T16:30:17Z",
  "id":1330916921,
  "issue":787,
  "node_id":"IC_kwDOD6Q_ss5PVC45",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-11-29T16:30:17Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Hi @jpivarski, seeing your comment above on Python error messages. Have you considered the `(*, )` argument? The following seems understandable to me:\r\n\r\nI think Jim's argument is that \r\n```\r\nTypeError: test2() takes 0 positional arguments but 1 was given\r\n```\r\n\r\ndoesn't nudge the user to pass a _keyword_ argument, unlike if the argument were non-defaulted keyword-only. ",
  "created_at":"2022-11-29T16:37:02Z",
  "id":1330929544,
  "issue":787,
  "node_id":"IC_kwDOD6Q_ss5PVF-I",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-29T16:37:02Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"OK.",
  "created_at":"2022-11-29T16:52:52Z",
  "id":1330956422,
  "issue":787,
  "node_id":"IC_kwDOD6Q_ss5PVMiG",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-11-29T16:52:52Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah, that's what I meant. I wish the error message wasn't\r\n\r\n\r\n```\r\nTypeError: arrays() takes 2 positional arguments but 3 were given\r\n```\r\n\r\nbut\r\n\r\n```\r\nTypeError: arrays() takes 2 positional arguments but 3 were given; non-positional arguments are\r\n\r\n    'filter_name', 'filter_typename', 'filter_branch', 'aliases', 'language', 'entry_start', 'entry_stop',\r\n    'decompression_executor', 'interpretation_executor', 'array_cache', 'library', 'ak_add_doc', 'how'\r\n```\r\n\r\nwhich tells the user they weren't wrong for thinking the function might take more arguments; they just did it in the wrong way. That list of names would also act as a prompt that it's worth looking this function up in the documentation to find out what some of those other arguments do.",
  "created_at":"2022-11-29T18:01:21Z",
  "id":1331079993,
  "issue":787,
  "node_id":"IC_kwDOD6Q_ss5PVqs5",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-11-29T18:01:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@agoose77 Now that the name change has propagated to an Awkward release, this Uproot PR should follow. (And then an Uproot RC release.)",
  "created_at":"2022-12-06T22:24:42Z",
  "id":1340089023,
  "issue":788,
  "node_id":"IC_kwDOD6Q_ss5P4CK_",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2022-12-06T22:24:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I'll merge it. Thanks!",
  "created_at":"2022-12-06T22:38:43Z",
  "id":1340107628,
  "issue":788,
  "node_id":"IC_kwDOD6Q_ss5P4Gts",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-06T22:38:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@aryan26roy, if you want to take a look at this PR, please do, though I know you're busy.\r\n\r\n@agoose77, I've annotated the changes to explain them.\r\n\r\nWhen this is merged, Uproot will have all of the `next-release` updates in place, except for #788, which is just a name change adjustment to follow updates in Awkward. When this is in, I'll focus on Awkward.",
  "created_at":"2022-11-30T00:32:16Z",
  "id":1331494793,
  "issue":790,
  "node_id":"IC_kwDOD6Q_ss5PXP-J",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-30T00:32:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right that a `\"char\"` NumpyArray was missing the `\"string\"` parameter on its containing ListOffsetArray. That was another one of the corrections.",
  "created_at":"2022-11-30T00:39:45Z",
  "id":1331498882,
  "issue":790,
  "node_id":"IC_kwDOD6Q_ss5PXQ-C",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-11-30T00:39:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This seems to have issues with other operations as well:\r\n\r\n```python\r\n>>> da[(da.int_branch > 0) & (da.int_branch < 1)].compute()\r\nTraceback (most recent call last):\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/language/python.py\", line 379, in free_symbols\r\n    return list(\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/language/python.py\", line 91, in _walk_ast_yield_symbols\r\n    yield from _walk_ast_yield_symbols(x, keys, aliases, functions, getter)\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/language/python.py\", line 95, in _walk_ast_yield_symbols\r\n    yield from _walk_ast_yield_symbols(x, keys, aliases, functions, getter)\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/language/python.py\", line 91, in _walk_ast_yield_symbols\r\n    yield from _walk_ast_yield_symbols(x, keys, aliases, functions, getter)\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/language/python.py\", line 91, in _walk_ast_yield_symbols\r\n    yield from _walk_ast_yield_symbols(x, keys, aliases, functions, getter)\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/language/python.py\", line 74, in _walk_ast_yield_symbols\r\n    raise KeyError(node.id)\r\nKeyError: 'bitwise_and'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/base.py\", line 315, in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/base.py\", line 600, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/threaded.py\", line 89, in get\r\n    results = get_async(\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/local.py\", line 511, in get_async\r\n    raise_exception(exc, tb)\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/local.py\", line 319, in reraise\r\n    raise exc\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/local.py\", line 224, in execute_task\r\n    result = _execute_task(task, data)\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/core.py\", line 119, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/optimization.py\", line 990, in __call__\r\n    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/core.py\", line 149, in get\r\n    result = _execute_task(task, cache)\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/dask/core.py\", line 119, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/_dask.py\", line 592, in __call__\r\n    return self.ttrees[i].arrays(\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 808, in arrays\r\n    arrays, expression_context, branchid_interpretation = _regularize_expressions(\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 2921, in _regularize_expressions\r\n    _regularize_expression(\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 2789, in _regularize_expression\r\n    for symbol in language.free_symbols(\r\n  File \"/home/user/miniconda3/envs/func_adl_uproot_rc/lib/python3.10/site-packages/uproot/language/python.py\", line 385, in free_symbols\r\n    raise uproot.KeyInFileError(\r\nuproot.exceptions.KeyInFileError: not found: 'bitwise_and'\r\nin file tests/scalars_tree_file.root\r\nin object /tree;1\r\n```",
  "created_at":"2022-12-14T16:17:15Z",
  "id":1351714566,
  "issue":791,
  "node_id":"IC_kwDOD6Q_ss5QkYcG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-14T16:17:15Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to break a rule about never merging one's own PR without a review. In this case, the differences are uncontroversial and this is blocking the 5.0.0 release\u2014I don't want Awkward 2 to be released without Uproot 5 for a significant length of time, since the mismatch can cause problems.\r\n\r\nThis will be used as an example to clarify when we can do that. (There are other rules with exceptions, like committing directly to `main` is in fact a good thing to do when you're changing version numbers or toggling tests.)",
  "created_at":"2022-12-10T03:44:33Z",
  "id":1345041810,
  "issue":795,
  "node_id":"IC_kwDOD6Q_ss5QK7WS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-10T03:44:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The file has been added to [scikit-hep-testdata 0.4.24](https://pypi.org/project/scikit-hep-testdata/) as \"`uproot-issue-798.root`\".\r\n\r\nThere are four branches that can be read by Uproot v4/pure Python and cannot be read by Uproot v5/AwkwardForth, and they represent two different types (probably just one error).\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> uproot.__version__\r\n'5.0.0'\r\n>>> tree = uproot.open(skhep_testdata.data_path(\"uproot-issue-798.root\"))[\"CollectionTree\"]\r\n```\r\n\r\n**AnalysisJetsAuxDyn.GhostTrack:** `std::vector<std::vector<ElementLink<DataVector<xAOD::IParticle>>`\r\n\r\n```python\r\n>>> tree[\"AnalysisJetsAuxDyn.GhostTrack\"].interpretation\r\nAsObjects(AsVector(True, AsVector(False, Model_ElementLink_3c_DataVector_3c_xAOD_3a3a_IParticle_3e3e_)))\r\n>>> tree[\"AnalysisJetsAuxDyn.GhostTrack\"].array()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 1818, in array\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 3124, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 3066, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 139, in basket_array\r\n    output = self.basket_array_forth(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 254, in basket_array_forth\r\n    context[\"forth\"].vm.resume()\r\nValueError: 'skip beyond' in AwkwardForth runtime: tried to skip beyond the bounds of an input (0 or length)\r\n```\r\n\r\n**TruthBosonAuxDyn.childLinks:** `std::vector<std::vector<ElementLink<DataVector<xAOD::TruthParticle_v1>>`\r\n\r\n```python\r\n>>> tree[\"TruthBosonAuxDyn.childLinks\"].interpretation\r\nAsObjects(AsVector(True, AsVector(False, Model_ElementLink_3c_DataVector_3c_xAOD_3a3a_TruthParticle_5f_v1_3e3e_)))\r\n>>> tree[\"TruthBosonAuxDyn.childLinks\"].array()\r\nTraceback (most recent call last):\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 232, in basket_array_forth\r\n    context[\"forth\"].vm = awkward.forth.ForthMachine64(\r\nValueError: in AwkwardForth source code, line 6 col 1, input names, output names, variable names, and user-defined words must all be unique and not reserved words or integers:\r\n\r\n    output node1-offsets \r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob//src/libawkward/forth/ForthMachine.cpp#L1960)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 1818, in array\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 3124, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 3066, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 139, in basket_array\r\n    output = self.basket_array_forth(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 236, in basket_array_forth\r\n    raise type(err)(\r\nValueError: in AwkwardForth source code, line 6 col 1, input names, output names, variable names, and user-defined words must all be unique and not reserved words or integers:\r\n\r\n    output node1-offsets \r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob//src/libawkward/forth/ForthMachine.cpp#L1960)\r\n\r\nForth code generated for this data type:\r\n\r\ninput stream\r\n    input byteoffsets\r\n    input bytestops\r\n    output node0-offsets int64\r\noutput node1-offsets int64\r\noutput node1-offsets int64\r\noutput node10-data uint32\r\noutput node11-data uint32\r\n\r\n    0 node0-offsets <- stack\r\n0 node1-offsets <- stack\r\n0 node1-offsets <- stack\r\n\r\n    0 do\r\n    byteoffsets I-> stack\r\n    stream seek\r\n    6 stream skip\r\nstream !I-> stack\r\n dup node0-offsets +<- stack\r\n0 do\r\nstream !I-> stack\r\n dup node1-offsets +<- stack\r\n0 do\r\nstream !I-> stack\r\n dup node1-offsets +<- stack\r\n0 do\r\n0 stream skip \r\n6 stream skip\r\n4 stream skip\r\nstream !I-> node10-data\r\nstream !I-> node11-data\r\nloop\r\nloop\r\nloop\r\n\r\n    loop\r\n```\r\n\r\n**TruthPhotonsAuxDyn.parentLinks:** `std::vector<std::vector<ElementLink<DataVector<xAOD::TruthParticle_v1>>`\r\n\r\n```python\r\n>>> tree[\"TruthPhotonsAuxDyn.parentLinks\"].interpretation\r\nAsObjects(AsVector(True, AsVector(False, Model_ElementLink_3c_DataVector_3c_xAOD_3a3a_TruthParticle_5f_v1_3e3e_)))\r\n>>> tree[\"TruthPhotonsAuxDyn.parentLinks\"].array()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 1818, in array\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 3124, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 3066, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 139, in basket_array\r\n    output = self.basket_array_forth(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 254, in basket_array_forth\r\n    context[\"forth\"].vm.resume()\r\nValueError: 'skip beyond' in AwkwardForth runtime: tried to skip beyond the bounds of an input (0 or length)\r\n```\r\n\r\n**TruthTopAuxDyn.parentLinks:** `std::vector<std::vector<ElementLink<DataVector<xAOD::TruthParticle_v1>>`\r\n\r\n```python\r\n>>> tree[\"TruthTopAuxDyn.parentLinks\"].interpretation\r\nAsObjects(AsVector(True, AsVector(False, Model_ElementLink_3c_DataVector_3c_xAOD_3a3a_TruthParticle_5f_v1_3e3e_)))\r\n>>> tree[\"TruthTopAuxDyn.parentLinks\"].array()\r\nTraceback (most recent call last):\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 232, in basket_array_forth\r\n    context[\"forth\"].vm = awkward.forth.ForthMachine64(\r\nValueError: in AwkwardForth source code, line 6 col 1, input names, output names, variable names, and user-defined words must all be unique and not reserved words or integers:\r\n\r\n    output node1-offsets \r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob//src/libawkward/forth/ForthMachine.cpp#L1960)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 1818, in array\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 3124, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 3066, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 139, in basket_array\r\n    output = self.basket_array_forth(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/interpretation/objects.py\", line 236, in basket_array_forth\r\n    raise type(err)(\r\nValueError: in AwkwardForth source code, line 6 col 1, input names, output names, variable names, and user-defined words must all be unique and not reserved words or integers:\r\n\r\n    output node1-offsets \r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob//src/libawkward/forth/ForthMachine.cpp#L1960)\r\n\r\nForth code generated for this data type:\r\n\r\ninput stream\r\n    input byteoffsets\r\n    input bytestops\r\n    output node0-offsets int64\r\noutput node1-offsets int64\r\noutput node1-offsets int64\r\noutput node158-data uint32\r\noutput node159-data uint32\r\n\r\n    0 node0-offsets <- stack\r\n0 node1-offsets <- stack\r\n0 node1-offsets <- stack\r\n\r\n    0 do\r\n    byteoffsets I-> stack\r\n    stream seek\r\n    6 stream skip\r\nstream !I-> stack\r\n dup node0-offsets +<- stack\r\n0 do\r\nstream !I-> stack\r\n dup node1-offsets +<- stack\r\n0 do\r\nstream !I-> stack\r\n dup node1-offsets +<- stack\r\n0 do\r\n0 stream skip \r\n6 stream skip\r\n4 stream skip\r\nstream !I-> node158-data\r\nstream !I-> node159-data\r\nloop\r\nloop\r\nloop\r\n\r\n    loop\r\n```\r\n\r\nAll four of them can be read without AwkwardForth (old method). I also verified that they can be read by Uproot v4 itself, and that there aren't any other branches that can be read in v4 but not v5.\r\n\r\n```python\r\n>>> interpretation = tree[\"AnalysisJetsAuxDyn.GhostTrack\"].interpretation\r\n>>> interpretation._forth = False\r\n>>> tree[\"AnalysisJetsAuxDyn.GhostTrack\"].array(interpretation)\r\n<Array [[[{...}, {...}, ..., {...}], ...], ...] type='40 * var * var * stru...'>\r\n\r\n>>> interpretation = tree[\"TruthBosonAuxDyn.childLinks\"].interpretation\r\n>>> interpretation._forth = False\r\n>>> tree[\"TruthBosonAuxDyn.childLinks\"].array(interpretation)\r\n<Array [[[], [{...}, ..., {...}]], ..., [...]] type='40 * var * var * struc...'>\r\n\r\n>>> interpretation = tree[\"TruthPhotonsAuxDyn.parentLinks\"].interpretation\r\n>>> interpretation._forth = False\r\n>>> tree[\"TruthPhotonsAuxDyn.parentLinks\"].array(interpretation)\r\n<Array [[[{m_persKey: ..., ...}], ...], ...] type='40 * var * var * struct[...'>\r\n\r\n>>> interpretation = tree[\"TruthTopAuxDyn.parentLinks\"].interpretation\r\n>>> interpretation._forth = False\r\n>>> tree[\"TruthTopAuxDyn.parentLinks\"].array(interpretation)\r\n<Array [[[], [], [{...}], [{...}]], ..., [...]] type='40 * var * var * stru...'>\r\n```",
  "created_at":"2022-12-13T16:11:18Z",
  "id":1348915008,
  "issue":798,
  "node_id":"IC_kwDOD6Q_ss5QZs9A",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-13T16:11:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right: this is a new feature and the docs are out of date. Thanks for the heads-up!\r\n\r\nWhat's happening now is that any non-flat data uses the Awkward dtype provided by [awkward-pandas](https://github.com/intake/awkward-pandas).\r\n\r\nIf you _want_ to explode the data as before, you can get it as an Awkward Array and use [ak.to_dataframe](https://awkward-array.org/doc/main/reference/generated/ak.to_dataframe.html):\r\n\r\n```python\r\n>>> import uproot\r\n>>> import awkward as ak\r\n>>> ak.to_dataframe(events.arrays(filter_name=\"/(Jet|Muon)_P[xyz]/\", library=\"ak\"))\r\n                   Jet_Px     Jet_Py      Jet_Pz    Muon_Px    Muon_Py     Muon_Pz\r\nentry subentry                                                                    \r\n1     0        -38.874714  19.863453   -0.894942  -0.816459 -24.404259   20.199968\r\n3     0        -71.695213  93.571579  196.296432  22.088331 -85.835464  403.848450\r\n      1         36.606369  21.838793   91.666283  76.691917 -13.956494  335.094208\r\n4     0          3.880162 -75.234055 -359.601624  45.171322  67.248787  -89.695732\r\n      1          4.979580 -39.231731   68.456718  39.750957  25.403667   20.115053\r\n...                   ...        ...         ...        ...        ...         ...\r\n2414  0         33.961163  58.900467  -17.006561  -9.204197 -42.204014  -64.264900\r\n2416  0         37.071465  20.131996  225.669037 -39.285824 -14.607491   61.715790\r\n2417  0        -33.196457 -59.664749  -29.040150  35.067146 -14.150043  160.817917\r\n2418  0         -3.714818 -37.202377   41.012222 -29.756786 -15.303859  -52.663750\r\n2419  0        -36.361286  10.173571  226.429214   1.141870  63.609570  162.176315\r\n\r\n[2038 rows x 6 columns]\r\n```",
  "created_at":"2022-12-14T19:37:07Z",
  "id":1352046730,
  "issue":800,
  "node_id":"IC_kwDOD6Q_ss5QlpiK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-14T19:37:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Unfortunately, we can't change it because `scikit-hep/uproot` was the original repo name. The issue and PR numbers of `scikit-hep/uproot3` and `scikit-hep/uproot5` overlap, so if we changed `scikit-hep/uproot` to point to `scikit-hep/uproot5`, then an old URL for old Uproot issue number 123 would end up pointing to new Uproot issue number 123, which would be wrong.\r\n\r\nThose old issue (and PR) numbers probably don't matter much and Google/Bing/whatever's indexing would catch up, but it's best to not make changes to global namespaces. (The changes we made to `uproot`, `uproot3`, `uproot4` names in PyPI, another global namespace, caused troubles that I hadn't foreseen, so I'm erring on the side of being conservative now.)",
  "created_at":"2022-12-15T17:12:27Z",
  "id":1353422058,
  "issue":804,
  "node_id":"IC_kwDOD6Q_ss5Qq5Tq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-15T17:12:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I agree. I'll merge it!",
  "created_at":"2022-12-18T03:38:49Z",
  "id":1356655267,
  "issue":806,
  "node_id":"IC_kwDOD6Q_ss5Q3Oqj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-18T03:38:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I think that's the case: for the most part, I'm just having to explain myself, that these are a finite, known set and that they'll be going away eventually. I'd like to merge this now to un-block @nsmith-, though.",
  "created_at":"2022-12-22T18:05:57Z",
  "id":1363193734,
  "issue":808,
  "node_id":"IC_kwDOD6Q_ss5RQK-G",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-22T18:05:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"For now I've implemented a workaround https://github.com/CoffeaTeam/coffea/pull/736/commits/002ba7cfa28f5099fa041736db4c1ad80b65e9a0#r1055005000",
  "created_at":"2022-12-22T18:15:32Z",
  "id":1363202407,
  "issue":808,
  "node_id":"IC_kwDOD6Q_ss5RQNFn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-22T18:15:32Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Meanwhile, we can merge this and the next release will have it.",
  "created_at":"2022-12-22T18:39:27Z",
  "id":1363221195,
  "issue":808,
  "node_id":"IC_kwDOD6Q_ss5RQRrL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2022-12-22T18:39:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 }
]
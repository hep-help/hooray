[
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"From @ryuwd on https://gitter.im/Scikit-HEP/uproot:\r\n\r\n> I am using `uproot4.iterate` to read files over xrootd - I am currently seeing this error at exit after the script has completed all its tasks:\r\n> \r\n> ```\r\n>     ERROR    | Error in atexit._run_exitfuncs:\r\n>     ERROR    | Traceback (most recent call last):\r\n>     ERROR    |   File \"/XicpStToXicpPiPi/.snakemake/conda/3c5bbdd0/lib/python3.8/site-packages/uproot4/extras.py\", line 112, in cleanup_open_files\r\n>     ERROR    | if isinstance(obj, XRootD.client.file.File) and obj.is_open():\r\n>     ERROR    | ReferenceError\r\n>     ERROR    | :\r\n>     ERROR    | weakly-referenced object no longer exists\r\n>     ERROR    | Error in atexit._run_exitfuncs:\r\n> ```\r\n\r\nMy response:\r\n\r\n> Uproot has one `atexit` handler, which runs when the script is done, and apparently XRootD has weak references that are no longer valid at that time. You're not forgetting to do something (requiring users to check off formalities is not \"Pythonic\"). However, there isn't much to do about this error except ignore it. This PR does that: scikit-hep/uproot4#237 Could you check to see if it fixes your error message? (I don't have anything that reproduces it.)\r\n\r\nI should point out that this PR does a little more than just ignore the ReferenceError: it ignores it on a per-file basis, so that the exception in one file doesn't cause another file to not get closed. (The `try/except/else` is inside the `for` loop, not outside.)",
  "closed_at":"2021-01-08T14:47:22Z",
  "comments":1,
  "created_at":"2021-01-07T14:18:37Z",
  "draft":false,
  "id":781346670,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTUxMDg2NDQz",
  "number":237,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-08T14:47:22Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Hide ReferenceError in XRootD atexit.",
  "updated_at":"2021-01-08T14:47:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Dear experts,\r\n\r\nafter updating uproot, I can no longer simply turn the opened tree to panda df and get a mysterious error.\r\n\r\nMWE:\r\n```\r\nmyfile = \"input.root\"\r\nmytree = uproot.open(myfile)['DecayTree']\r\nmydf = mytree.pandas.df()\r\n```\r\n\r\nErrror:\r\n\r\nAttributeError: 'Model_TTree_v20' object has no attribute 'pandas'\r\n\r\nWorked before, is this a know issue? \r\n\r\nThank you,\r\nGedas",
  "closed_at":"2021-01-08T13:39:17Z",
  "comments":2,
  "created_at":"2021-01-08T09:36:34Z",
  "id":781983391,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU3ODE5ODMzOTE=",
  "number":238,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Can't turn ROOT tree to pandas df (Uproot 4 interface changes)",
  "updated_at":"2021-01-08T13:44:33Z",
  "user":"MDQ6VXNlcjE1ODAyMTA5"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"As soon as @giammi56/[giammi56](https://stackoverflow.com/users/14010009/giammi56) confirms that this fixes the above-mentioned StackOverflow issue, I'll merge this PR. (And if the tests pass.)\r\n\r\nThanks!",
  "closed_at":"2021-01-11T23:33:48Z",
  "comments":2,
  "created_at":"2021-01-08T17:14:17Z",
  "draft":false,
  "id":782273738,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTUxODU4Nzk4",
  "number":239,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-11T23:33:48Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fix TH3's 'to_numpy' (https://stackoverflow.com/q/65632670/1623645).",
  "updated_at":"2021-07-08T15:47:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"here's a test script to work with\r\n\r\n```python\r\nimport uproot\r\nimport os.path\r\n\r\nfname = \"HEPData-ins1755298-v3-Expected_limit_1lbb.root\"\r\n\r\nif not os.path.isfile(fname):\r\n    with open(fname, 'wb') as f:\r\n        import requests\r\n        response = requests.get(\"https://www.hepdata.net/download/table/ins1755298/Expected%20limit%201lbb/3/root\")\r\n        response.raise_for_status()\r\n        f.write(response.content)\r\n\r\nf = uproot.open(fname)\r\ngraph = f['Expected limit 1lbb/Graph1D_y1']\r\ngraph.values()\r\ngraph.errors()\r\n```",
  "closed_at":"2021-01-11T23:12:57Z",
  "comments":16,
  "created_at":"2021-01-08T17:23:10Z",
  "draft":false,
  "id":782279115,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTUxODYzMTUw",
  "number":240,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-11T23:12:57Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: Add TGraphAsymmErrors",
  "updated_at":"2021-01-11T23:31:23Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @kratsg as a contributor for code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot4/pull/240#issuecomment-758290339)",
  "closed_at":"2021-01-11T23:42:26Z",
  "comments":0,
  "created_at":"2021-01-11T23:31:20Z",
  "draft":false,
  "id":783781793,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTUzMDgzMzc3",
  "number":241,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-11T23:42:26Z"
  },
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"closed",
  "state_reason":null,
  "title":"docs: add kratsg as a contributor",
  "updated_at":"2021-01-11T23:42:29Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I'm trying to read root data onto an Nvidia GPU with uproot and cupy. I can read the data fine onto cpu but when i set the `library='cp'` I get `ValueError: non-scalar numpy.ndarray cannot be used for fill`.\r\n\r\nI've posted the code I used below (I'm also using publicly available data: `root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root` so anyone can try to replicate the error):\r\n```python\r\nPython 3.8.5 (default, Sep  4 2020, 07:30:14)\r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import awkward as ak\r\n>>> ak.__version__\r\n'1.0.1'\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'4.0.0'\r\n>>> import cupy as cp\r\n>>> cp.__version__\r\n'8.3.0'\r\n>>> tree = uproot.open('root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root')['Events']\r\n>>> cpu_array = tree.arrays(['nElectron'])\r\n>>> cpu_array\r\n<Array [{nElectron: 0}, ... {nElectron: 1}] type='29308627 * {\"nElectron\": uint32}'>\r\n>>>\r\n>>> gpu_array = tree.arrays(['nElectron'], library='cp')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/anaylor/.pyenv/versions/miniconda3-4.3.30/envs/ml_pos_reco/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 1111, in arrays\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/home/anaylor/.pyenv/versions/miniconda3-4.3.30/envs/ml_pos_reco/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 3430, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/anaylor/.pyenv/versions/miniconda3-4.3.30/envs/ml_pos_reco/lib/python3.8/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/anaylor/.pyenv/versions/miniconda3-4.3.30/envs/ml_pos_reco/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 3401, in basket_to_array\r\n    arrays[branch.cache_key] = interpretation.final_array(\r\n  File \"/home/anaylor/.pyenv/versions/miniconda3-4.3.30/envs/ml_pos_reco/lib/python3.8/site-packages/uproot/interpretation/numerical.py\", line 89, in final_array\r\n    output[: stop - entry_start] = basket_array[local_start:local_stop]\r\n  File \"cupy/core/core.pyx\", line 1299, in cupy.core.core.ndarray.__setitem__\r\n  File \"cupy/core/_routines_indexing.pyx\", line 52, in cupy.core._routines_indexing._ndarray_setitem\r\n  File \"cupy/core/_routines_indexing.pyx\", line 884, in cupy.core._routines_indexing._scatter_op\r\n  File \"cupy/core/core.pyx\", line 542, in cupy.core.core.ndarray.fill\r\nValueError: non-scalar numpy.ndarray cannot be used for fill\r\n```\r\n",
  "closed_at":"2021-02-18T15:52:11Z",
  "comments":4,
  "created_at":"2021-01-12T17:46:33Z",
  "id":784448249,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU3ODQ0NDgyNDk=",
  "number":242,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Unable to read arrays onto GPU with cupy",
  "updated_at":"2021-02-18T15:52:11Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Since i also ran into #122 i would like to help fixing it. I believe better than falling back to non-vector reads in case a single request is larger than the maximum the server allows would be chunking the request into smaller parts as i think [ROOT does it](https://root.cern/doc/master/TNetXNGFile_8cxx_source.html#l00469). The relevant code is probably that part:\r\n\r\n```cpp\r\n       // If the length is bigger than max readv size, split into smaller chunks\r\n       if (length[i] > fReadvIorMax) {\r\n          Int_t nsplit = length[i] / fReadvIorMax;\r\n          Int_t rem    = length[i] % fReadvIorMax;\r\n          Int_t j;\r\n  \r\n          // Add as many max-size chunks as are divisible\r\n          for (j = 0; j < nsplit; ++j) {\r\n             offset = position[i] + (j * fReadvIorMax);\r\n             chunks.push_back(ChunkInfo(offset, fReadvIorMax, cursor));\r\n             cursor += fReadvIorMax;\r\n          }\r\n  \r\n          // Add the remainder\r\n          offset = position[i] + (j * fReadvIorMax);\r\n          chunks.push_back(ChunkInfo(offset, rem, cursor));\r\n          cursor += rem;\r\n```\r\n\r\nI tried to implement that into `XRootDResource`, but the problem is now that the returned chunks are expected to be the same ranges that were requested. So this would need an additional kind of \"merging\" step before returning the chunks.\r\nBut i don't really know how to do that best - at that stage the chunks are not necessarily \"materialized\" yet and might just contain futures - so either one would need to wait for the result and then merge them, creating a new chunk object with merged `raw_data` or modify the task that is submitted to not only fetch the chunks, but also merge them if nescessary ...\r\n\r\nAny ideas what could be good ways to deal with this?",
  "closed_at":"2021-01-18T19:45:47Z",
  "comments":7,
  "created_at":"2021-01-14T14:27:24Z",
  "draft":false,
  "id":786032874,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTU0OTY3MzA5",
  "number":243,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-18T19:45:47Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Split vector reads into smaller chunk when single requests are larger than `max_element_size`",
  "updated_at":"2021-01-18T19:45:48Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n4.0.1\r\n```\r\n\r\nI process a lot of data sequentially (because the operations are not trivial); here is a simple function to pull out a list of batches from a test file (N.B. I would not use this in production!)\r\n\r\n```python\r\nimport itertools\r\nimport pathlib\r\nimport uproot\r\n\r\npath = pathlib.Path.cwd() / \"tracks.txt\"\r\n\r\ndef load_branches(path, branches):\r\n    with uproot.open(f\"{path}:tracks\") as tracks_file:\r\n        tracks_batches = tracks_file.iterate(\r\n            branches,\r\n            entry_start=0,\r\n            entry_stop=100,\r\n            step_size=\"50kB\",\r\n            library=\"ak\"\r\n        )\r\n        return list(tracks_batches)\r\n```\r\nWhen I read the 2D covariance matrix branch from this test file, the shape of the returned `v_y` branch is modified:\r\n```python\r\n>>> batch, = load_branches(path, ['v_y'])\r\n>>> batch['v_y'][0]\r\n147.14722347885558\r\n```\r\n\r\nIntroducing the covariance branch:\r\n```python\r\n>>> batch, = load_branches(path, ['v_y', 'covariance'])\r\n>>> batch['v_y'][0]\r\n<Array [... 126, 111, 54.3, 170, 169, 114]] type='1 * 100 * float64'\r\n```\r\n\r\nROOT file:\r\n[tracks.txt](https://github.com/scikit-hep/uproot4/files/5829933/tracks.txt)",
  "closed_at":"2021-01-18T16:12:39Z",
  "comments":2,
  "created_at":"2021-01-18T11:43:56Z",
  "id":788197930,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU3ODgxOTc5MzA=",
  "number":244,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Reading 2D array branch modifies other return branch shape",
  "updated_at":"2021-01-18T16:12:39Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-01-18T16:12:39Z",
  "comments":0,
  "created_at":"2021-01-18T14:22:37Z",
  "draft":false,
  "id":788309871,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTU2ODQzNTM2",
  "number":245,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-18T16:12:38Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Use ak.Array dict constructor instead of ak.zip with depth_limit=1.",
  "updated_at":"2021-01-18T16:12:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-01-18T18:02:18Z",
  "comments":0,
  "created_at":"2021-01-18T17:51:35Z",
  "draft":false,
  "id":788452877,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTU2OTYxNjg5",
  "number":246,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-18T18:02:18Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Black and flake8.",
  "updated_at":"2021-01-18T18:02:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-01-18T18:39:44Z",
  "comments":0,
  "created_at":"2021-01-18T18:18:07Z",
  "draft":false,
  "id":788466920,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTU2OTczMDI1",
  "number":247,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-18T18:39:44Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Include hooks in the decompression process, TBranch.compression, and TBasket.block_compression_info for more information.",
  "updated_at":"2021-01-18T18:39:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I am seeing some unexpected behavior when using uproot.concatenate. I am looking at waveform event data where each event has a single associated timestamp and then multiple features associated with the variable number of pulses possible within an event. So on a per-event basis, timestamp is zero dimensional (just a number), whereas the pulse quantities are 1D.\r\n\r\nConcatenating multiple files and just selecting the timestamp data seems to work as expected - I get a 1D array of timestamps where the length of the array is the total number of events contained across all the files. \r\n\r\nHowever, if I include an additional branch to load, the behavior changes. Instead of a 1D array of timestamps, I get a 2D array. If file 1 contains N events, and file 2 contains M events, the timestamp array is as such\r\n\r\n```\r\n[ Row 1:   [file1_timestamp1, file1_timestamp2, ..., file1_timestampN],\r\n  Row 2:   [file1_timestamp1, file1_timestamp2, ..., file1_timestampN],\r\n  ...\r\n  Row N:   [file1_timestamp1, file1_timestamp2, ..., file1_timestampN],\r\n  Row N+1: [file2_timestamp1, file2_timestamp2, ..., file2_timestampM],\r\n  Row N+2: [file2_timestamp1, file2_timestamp2, ..., file2_timestampM],\r\n  ...\r\n  Row N+M: [file2_timestamp1, file2_timestamp2, ..., file2_timestampM]]\r\n```\r\n\r\nCode\r\n\r\n```\r\n# This works as expected\r\ndata1 = uproot.concatenate(\"/path/to/rootfiles/*:events\", \"timestamp\")\r\ndata1.timestamp # a 1D array of timestamps for all events, as desired\r\n\r\n# This produces the unexpected shape for timestamps as shown above. \r\n# pulse_area, however, seems to work as expected.\r\ndata2 = uproot.concatenate(\"/path/to/rootfiles/*:events\", [\"timestamp\", \"pulse_area\"])\r\ndata2.timestamp # a 2D array with the structure shown above\r\ndata2.pulse_area # a 2D array of pulse info for each event, as expected\r\n```\r\n\r\nI unfortunately can't share the root files I am working with.\r\n\r\nIs this actually the intended behavior/is it just a problem with something I am doing?",
  "closed_at":"2021-01-19T16:37:07Z",
  "comments":5,
  "created_at":"2021-01-19T16:13:01Z",
  "id":789157411,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU3ODkxNTc0MTE=",
  "number":248,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Unexpected Results with uproot.concatenate",
  "updated_at":"2021-01-19T17:25:49Z",
  "user":"MDQ6VXNlcjg1OTM0ODE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi,\r\nSome histograms are not being opened using uproot.\r\nThe .root file is working on default root, rootbrowse and uproot3.\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/omitted/venv/lib/python3.9/site-packages/uproot/reading.py\", line 1971, in __getitem__\r\n    return self.key(where).get()\r\n  File \"/home/omitted/venv/lib/python3.9/site-packages/uproot/reading.py\", line 2356, in get\r\n    out = cls.read(chunk, cursor, context, self._file, selffile, parent)\r\n  File \"/home/omitted/venv/lib/python3.9/site-packages/uproot/model.py\", line 1164, in read\r\n    raise ValueError(\r\nValueError: Unknown version 1 for class TH1D that cannot be skipped because its number of bytes is unknown.\r\n```\r\n\r\nsteps to reproduce\r\n```python\r\n>>> import uproot\r\n>>> uproot.open(\"B4.root\")['Eabs']\r\n```\r\n\r\nIf you ignore the **raise ValueError** (line 1172 from model.py) and use **versioned_cls = cls.new_class(file, version)** the histogram works fine.\r\n\r\nI'm attaching a root file from the Geant4 example case B4a (obtained running the run2.mac)\r\n[B4.txt](https://github.com/scikit-hep/uproot4/files/5847269/B4.txt)\r\n\r\nThanks",
  "closed_at":"2021-01-21T14:30:23Z",
  "comments":1,
  "created_at":"2021-01-21T06:39:57Z",
  "id":790746554,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU3OTA3NDY1NTQ=",
  "number":250,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Can't open THD1 histogram",
  "updated_at":"2021-01-21T14:30:23Z",
  "user":"MDQ6VXNlcjgzMjEwNTE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-01-21T14:30:23Z",
  "comments":0,
  "created_at":"2021-01-21T14:05:58Z",
  "draft":false,
  "id":791126667,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTU5MjMxMDQ2",
  "number":251,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-21T14:30:23Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"DispatchByVersion doesn't need num_bytes. (Also remove unnecessary start_cursor.)",
  "updated_at":"2021-01-21T14:30:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Uproot 4.0.2rc1\r\n\r\nI'm sorry that I can't provide a root file / xrd server to reproduce this.\r\n\r\nIn\r\nhttps://github.com/scikit-hep/uproot4/blob/3e53f480475883397a3a4e2dea199cdc00fd6477/uproot/source/xrootd.py#L326-L341\r\n\r\nif the `request_ranges` list length exceeds `readv_iov_max`, the server will report the error \"Read vector is too long\" i.e.\r\n```cpp\r\n   if (rdVecNum > XrdProto::maxRvecsz)\r\n      return Response.Send(kXR_ArgTooLong, \"Read vector is too long\");\r\n``` \r\nwhere `rdVecNum = rdVecLen / sizeof(readahead_list);`,  which corresponds to the number of elements in the `request_ranges` list sent to the server.\r\n\r\nI think that the code at https://github.com/scikit-hep/uproot4/blob/3e53f480475883397a3a4e2dea199cdc00fd6477/uproot/source/xrootd.py#L294-L298\r\n\r\nmay be the cause, since it does the max element size check. Changing\r\n```\r\n     if len(all_request_ranges[-1]) > self._max_num_elements: \r\n```\r\nto\r\n```\r\n     if len(all_request_ranges[-1]) >= self._max_num_elements: \r\n```\r\nseems to fix the issue in my test. Will open a PR.\r\n\r\n\r\n",
  "closed_at":"2021-01-27T15:17:53Z",
  "comments":1,
  "created_at":"2021-01-27T13:55:44Z",
  "id":795122674,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU3OTUxMjI2NzQ=",
  "number":253,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"XRootD: The number of 'read ahead' elements requested in a readv call can exceed the server's limit",
  "updated_at":"2021-01-27T15:17:53Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Fixes #253 \r\n\r\nThis change just prevents the length of any sublist in `all_request_ranges` from exceeding the `readv_iov_max` limit.",
  "closed_at":"2021-01-27T15:17:53Z",
  "comments":13,
  "created_at":"2021-01-27T13:56:23Z",
  "draft":false,
  "id":795123200,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTYyNTIzOTc1",
  "number":254,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-27T15:17:53Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Prevent the xrootd server readv_iov_max limit from being exceeded",
  "updated_at":"2021-01-27T15:33:56Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @ryuwd as a contributor for code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot4/pull/254#issuecomment-768327490)",
  "closed_at":"2021-01-27T15:25:17Z",
  "comments":0,
  "created_at":"2021-01-27T14:36:02Z",
  "draft":false,
  "id":795155962,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTYyNTUxNTIx",
  "number":255,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-27T15:25:17Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add ryuwd as a contributor",
  "updated_at":"2021-01-27T15:29:31Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"I'm making this PR to check if conda-forge was fixed correctly I don't mind if it's merged or just closed.",
  "closed_at":"2021-01-31T17:23:10Z",
  "comments":6,
  "created_at":"2021-01-29T14:17:23Z",
  "draft":false,
  "id":796918026,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTY0MDE0NTg5",
  "number":256,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-01-31T17:23:10Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Re-enable Python 3.5 CI",
  "updated_at":"2021-01-31T21:18:39Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"We are converting an `HDF5` file to a `root` file using uproot3 with `python`.   We need to convert some data types that uproot3 does not support yet.\r\n\r\nAt this point we are thinking of using pyROOT or write a C++ program to handle \"unsigned integers\" and \"vector\". \r\n\r\nDo you plan to add functionality to uproot4 to write \"unsigned integer\" and \"vector\" to a root file?\r\nDo you think this would be available this spring release?",
  "closed_at":null,
  "comments":6,
  "created_at":"2021-02-01T20:29:04Z",
  "id":798684169,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU3OTg2ODQxNjk=",
  "number":257,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"open",
  "state_reason":null,
  "title":"writing std::vector in TTree output",
  "updated_at":"2024-01-30T16:13:23Z",
  "user":"MDQ6VXNlcjY4MzQyMzI3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"When requesting a single chunk with `uproot.source.xrootd.XRootDSource` the arguments have to be eventually casted to (python) int before giving them to `pyxrootd.client.File.read`, otherwise it will throw an exception. This happenend when requesting data via `uproot.TBranch.basket` because that passes the parameters through as `np.int64` (coming from `branch.member(\"fBasketSeek\")`:\r\n\r\n```pycon\r\n>>> import uproot\r\n>>> branch = uproot.open(\"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root:Events/Muon_pt\")\r\n>>> branch.basket(0)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/nikolai/python/uproot4/uproot/behaviors/TBranch.py\", line 2484, in basket\r\n    chunk, cursor = self.basket_chunk_cursor(basket_num)\r\n  File \"/home/nikolai/python/uproot4/uproot/behaviors/TBranch.py\", line 2508, in basket_chunk_cursor\r\n    chunk = self._file.source.chunk(start, stop)\r\n  File \"/home/nikolai/python/uproot4/uproot/source/xrootd.py\", line 277, in chunk\r\n    data = self._resource.get(start, stop)\r\n  File \"/home/nikolai/python/uproot4/uproot/source/xrootd.py\", line 152, in get\r\n    status, data = self._file.read(start, stop - start, timeout=self._xrd_timeout())\r\n  File \"/home/nikolai/.conda/envs/physlite-experiments/lib/python3.8/site-packages/XRootD/client/file.py\", line 126, in read\r\n    status, response = self.__file.read(offset, size, timeout)\r\nTypeError: integer argument expected for offset\r\n```\r\n\r\nThis should be fixed by this MR. I added a test that directly tries to pass `np.int64` to `uproot.source.xrootd.XRootDSource.chunk`.",
  "closed_at":"2021-02-08T16:11:04Z",
  "comments":5,
  "created_at":"2021-02-08T14:03:26Z",
  "draft":false,
  "id":803594766,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTY5NDk2OTUz",
  "number":258,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-08T16:11:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Cast start and length to int before passing to pyxrootd.client.File.read",
  "updated_at":"2021-02-08T20:48:30Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hello,\r\n\r\na colleague of mine ran into [this file](https://github.com/scikit-hep/uproot4/files/5945153/onepinhole.root.zip) whose tree `Hits` can be opened by uproot3, but not by uproot4.  I have to note in advance that the trees in this file are all empty.\r\n\r\nThe minimal not-working example is as follows:\r\n\r\n        import uproot\r\n        test = uproot.open(\"onepinhole.root\")\r\n        TestDf = test['Hits'].arrays(\"*\")\r\n\r\nThis will throw the following error message:\r\n\r\n        Traceback (most recent call last):\r\n        \r\n          File \"/Users/david/Library/Python-Anaconda/anaconda3/envs/phdpython/lib/python3.7/site-packages/uproot/language/python.py\", line 22, in _expression_to_node\r\n            node = ast.parse(expression)\r\n        \r\n          File \"/Users/david/Library/Python-Anaconda/anaconda3/envs/phdpython/lib/python3.7/ast.py\", line 35, in parse\r\n            return compile(source, filename, mode, PyCF_ONLY_AST)\r\n        \r\n          File \"<unknown>\", line 1\r\n            *\r\n             ^\r\n        SyntaxError: invalid syntax\r\n        \r\n        \r\n        During handling of the above exception, another exception occurred:\r\n        \r\n        Traceback (most recent call last):\r\n        \r\n          File \"/Users/david/Library/Python-Anaconda/anaconda3/envs/phdpython/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\r\n            exec(code_obj, self.user_global_ns, self.user_ns)\r\n        \r\n          File \"<ipython-input-4-a148e06a1500>\", line 1, in <module>\r\n            HitsDf = test['Hits'].arrays(\"*\")\r\n        \r\n          File \"/Users/david/Library/Python-Anaconda/anaconda3/envs/phdpython/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 1098, in arrays\r\n            get_from_cache,\r\n        \r\n          File \"/Users/david/Library/Python-Anaconda/anaconda3/envs/phdpython/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3238, in _regularize_expressions\r\n            None,\r\n        \r\n          File \"/Users/david/Library/Python-Anaconda/anaconda3/envs/phdpython/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3131, in _regularize_expression\r\n            hasbranches.object_path,\r\n        \r\n          File \"/Users/david/Library/Python-Anaconda/anaconda3/envs/phdpython/lib/python3.7/site-packages/uproot/language/python.py\", line 374, in free_symbols\r\n            node = _expression_to_node(expression, file_path, object_path)\r\n        \r\n          File \"/Users/david/Library/Python-Anaconda/anaconda3/envs/phdpython/lib/python3.7/site-packages/uproot/language/python.py\", line 26, in _expression_to_node\r\n            err.args[1],\r\n        \r\n          File \"<unknown>\", line 1\r\n            *\r\n             ^\r\n        SyntaxError: invalid syntax\r\n        in file onepinhole.root\r\n        in object /Hits;1\r\n\r\n-------------\r\n\r\nThe environment with which the example fails is as follows:\r\n\r\n      \r\n        Lawkward                  1.0.2                        \r\n        lz4                       3.1.3    \r\n        numpy                     1.16.4                         \r\n        python                    3.7.3                 \r\n        uproot                    4.0.2                        \r\n        xxhash                    2.0.0                        \r\n\r\nThe environment with with the example does not fail is as follows:\r\n\r\n        awkward                   0.14.0                   \r\n        cachetools                4.2.1                    \r\n        lz4                       3.1.3             \r\n        numpy                     1.16.4       \r\n        python                    3.7.3       \r\n        uproot3                   3.14.3                   \r\n        uproot3-methods           0.10.0                   \r\n        xxhash                    2.0.0                    \r\n\r\nUnfortunately, I don't understand the error traceback to fix this error myself. If you need further information, please let me know!\r\n\r\nBest,\r\nDavid",
  "closed_at":"2021-02-08T18:24:26Z",
  "comments":5,
  "created_at":"2021-02-08T17:02:09Z",
  "id":803749790,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MDM3NDk3OTA=",
  "number":259,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Cannot read tree from file, though working in uproot3",
  "updated_at":"2021-02-08T18:35:22Z",
  "user":"MDQ6VXNlcjYyMjYyMDgx"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Update the basic.rst documentation, in the section \"Computing expressions and cuts\",\r\nwith an example of the conditional cut using the python syntax and explicitly with the\r\nparentheses. Without the parentheses a TypeError will occur, and it is not necessarily\r\nobvious when coming from ROOT and C++ syntax.",
  "closed_at":"2021-02-09T16:10:04Z",
  "comments":8,
  "created_at":"2021-02-09T11:51:17Z",
  "draft":false,
  "id":804480956,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTcwMjQwNTMy",
  "number":260,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-09T16:10:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Update basic.rst with conditional cut example",
  "updated_at":"2021-02-11T16:35:01Z",
  "user":"MDQ6VXNlcjI4MTAxMjAx"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I am not sure whether this is a bug or not, but perhaps the error message could be made slightly clearer if not.\r\n\r\nI have a large collection of root files, some of which clearly had some issues with their creation during the processing stage. I am attempting to run\r\n\r\n```\r\ndata = uproot.concatenate(\"/path/to/rootfiles/*:events\", 'pulse_area', allow_missing=True)\r\n```\r\n\r\nbut some files have an empty `events` tree. \r\n\r\n```\r\n>>> data = uproot.open(\"/path/to/broken/file.root\")\r\n>>> data['events'].keys()\r\n[]\r\n```\r\nThese files raise a `KeyInFileError` and references the provided branch name (in the example, that is `pulse_area`) and halt the concatenation. I naively expected the `allow_missing` option to permit the case of empty trees and continue running, but perhaps that is not the goal.\r\n\r\nI am using uproot 4.0.2. I have attached an example root file. To reproduce, simply run\r\n\r\n```\r\ndata = uproot.concatenate(\"example_root.root:events\", 'pulse_area', allow_missing=True)\r\n```\r\n\r\n[example_root.txt](https://github.com/scikit-hep/uproot4/files/5953904/example_root.txt)\r\n",
  "closed_at":"2021-02-09T21:56:50Z",
  "comments":1,
  "created_at":"2021-02-09T20:01:08Z",
  "id":804876236,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MDQ4NzYyMzY=",
  "number":261,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot.concatenate with allow_missing raises KeyInFileError on files with empty trees",
  "updated_at":"2021-02-09T21:56:50Z",
  "user":"MDQ6VXNlcjg1OTM0ODE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-02-09T21:36:13Z",
  "comments":0,
  "created_at":"2021-02-09T21:13:23Z",
  "draft":false,
  "id":804928186,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTcwNjE2NDY2",
  "number":262,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-09T21:36:13Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Minor fixes for CMS AOD (Run I) listing.",
  "updated_at":"2021-02-09T21:36:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-02-09T21:56:50Z",
  "comments":0,
  "created_at":"2021-02-09T21:35:48Z",
  "draft":false,
  "id":804944275,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTcwNjMwMjMx",
  "number":263,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-09T21:56:50Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"The allow_missing parameter of uproot.concatenate and uproot.iterate should permit missing TBranches, as it already permits missing files.",
  "updated_at":"2021-02-09T21:56:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @ChristopheRappold as a contributor for doc.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot4/pull/260#issuecomment-777623877)",
  "closed_at":"2021-02-11T16:35:44Z",
  "comments":0,
  "created_at":"2021-02-11T16:33:53Z",
  "draft":false,
  "id":806552647,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTcxOTUxNDY0",
  "number":264,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-11T16:35:44Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add ChristopheRappold as a contributor",
  "updated_at":"2021-02-11T16:35:47Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"I am confused about `PlottableAxisContinuous.__getitem__`: for uproot4 it seems to give the one-indexed values where I was expecting zero-indexed, since `__len__` gives the number of bins without flow. For example:\r\n```\r\n>>> h = uproot.open('https://raw.githubusercontent.com/CoffeaTeam/coffea/master/tests/samples/testSF2d.histo.root:scalefactors_Tight_Electron')\r\n>>> axis = h.axes[0]\r\n>>> len(axis)\r\n10\r\n>>> axis.edges()\r\narray([-2.5  , -2.   , -1.566, -1.444, -0.8  ,  0.   ,  0.8  ,  1.444,\r\n        1.566,  2.   ,  2.5  ])\r\n>>> axis.edges().shape\r\n(11,)\r\n>>> axis[0]\r\n(-2.0, -1.566)\r\n```",
  "closed_at":"2021-02-16T17:46:52Z",
  "comments":4,
  "created_at":"2021-02-15T05:38:45Z",
  "id":808191828,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MDgxOTE4Mjg=",
  "number":265,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Histogram protocol issue?",
  "updated_at":"2021-02-16T17:46:52Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I have a complicated tree with branches and sub branches. Some of the branches are recognized and picked by uproot while others aren't. For exapmle the momentum distribution fPx branch. \r\n![Screenshot from 2021-02-15 18-09-58](https://user-images.githubusercontent.com/47216877/107975887-0ba61a00-6fb9-11eb-95d2-7284ab40ebaa.png)\r\n\r\nPyroot shows that they are Double32_t type branches\r\n![Screenshot from 2021-02-15 18-13-08](https://user-images.githubusercontent.com/47216877/107976268-91c26080-6fb9-11eb-87c9-86d094d5fa3d.png)\r\n\r\n\r\nCould you help me some how convert these sub branches to arrays?",
  "closed_at":"2024-01-30T16:06:09Z",
  "comments":13,
  "created_at":"2021-02-15T17:16:32Z",
  "id":808705338,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MDg3MDUzMzg=",
  "number":268,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Fields of a struct interpreted with the type of their siblings, and more cases of memberwise splitting",
  "updated_at":"2024-01-30T16:06:09Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"The TGraphAsymmErrors tests all assume that a ROOT file can be supplied by\r\n\r\nhttps://www.hepdata.net/download/table/ins1755298/Expected%20limit%201lbb/3/root\r\n\r\nHowever, this URL now returns a page saying, \"Converter error encountered.\" Perhaps the file has been deleted or moved? Could we instead put it in [scikit-hep/scikit-hep-testdata](https://github.com/scikit-hep/scikit-hep-testdata) like the others, @kratsg?\r\n\r\nAt the end of scikit-hep/uproot4#240, we were talking about the possibility of setting up something that delivers test files. This would be an instance in which relying on a third-party server is a problem, though putting everything into a GitHub repo (scikit-hep-testdata) is not a great solution, either, because of the PyPI file limits and the fact that these data are not really versioned.",
  "closed_at":"2021-02-15T19:36:52Z",
  "comments":4,
  "created_at":"2021-02-15T19:14:25Z",
  "draft":false,
  "id":808764621,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTczNzQ3NjMy",
  "number":269,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-15T19:36:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Skip some tests because HEPData isn't producing the source file anymore.",
  "updated_at":"2021-02-17T10:29:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-02-16T17:46:52Z",
  "comments":5,
  "created_at":"2021-02-15T19:42:23Z",
  "draft":false,
  "id":808778732,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTczNzU4OTcz",
  "number":270,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-16T17:46:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Probably fixes #265; please test TAxis.__getitem__ and TAxis.intervals, @nsmith-.",
  "updated_at":"2021-02-16T17:46:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hello! \r\n\r\nApologies in advance if I have posted this in the wrong location.\r\n\r\nWhen executing:\r\n`conda install -c conda-forge uproot`\r\nit appears to install uproot3 (v3.13.0) with the dependency of awkward v0.15.0.\r\n\r\n`conda install -c conda-forge uproot4` \r\noffers uproot4 v0.1.2, as opposed to the current tag 4.0.3 which shows on the conda forge website.\r\n\r\nI had understood the \"naming transition\" to have taken place, so I wanted to ask if this was expected behaviour?  \r\n\r\nCheers,\r\nPaul ",
  "closed_at":"2021-02-18T14:45:49Z",
  "comments":3,
  "created_at":"2021-02-16T14:55:47Z",
  "id":809382373,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MDkzODIzNzM=",
  "number":271,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Installing from conda installs v3.13.0",
  "updated_at":"2021-02-18T15:00:09Z",
  "user":"MDQ6VXNlcjQ1ODE2ODYw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-02-16T22:57:43Z",
  "comments":0,
  "created_at":"2021-02-16T22:44:18Z",
  "draft":false,
  "id":809702840,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTc0NTIwODIz",
  "number":272,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-16T22:57:43Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Tighten the rule that assigns streamers to branches. Issue #268 provides an example in which the name alone is not unique.",
  "updated_at":"2021-02-16T22:57:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"I was not sure whether I should post this in https://github.com/scikit-hep/uproot4/issues/38 but at least it's not directly related to memberwise splitting, so I guess a new issue is fine `;)`.\r\n\r\nI decided to spend some time today on the memberwise-mystery and discovered that `uproot` chokes on split level 0 with a simple class.\r\n\r\nA dummy project where I started to explore the memberwise splitting can be used to reproduce the ROOT files and of course it also includes the class definition and tree configuration: https://github.com/tamasgal/root_splitting\r\n\r\nSo, back on track, I attached two files, both containing the same data and one is created with split level 0, the other with split level 1. The latter works fine, but the former with split level 0 causes problems due to some misinterpretation of the number of entries.\r\nAlthough split level 0 is very uncommon, maybe this sheds light on some yet not understood aspects of the serialisation. I have not looked closer, but I wanted to dump my findings...\r\n\r\n```python\r\n>>> import uproot\r\n\r\n>>> uproot.__version__\r\n'4.0.4'\r\n\r\n>>> f = uproot.open(\"split_1.objectwise.root\")\r\n\r\n>>> f[\"T/whatever\"].show()\r\nname                 | typename                 | interpretation\r\n---------------------+--------------------------+-------------------------------\r\nwhatever             | TWhatever                | AsGroup(<TBranchElement 'whateTObject              | unknown                  | <UnknownInterpretation 'non...\r\na                    | double                   | AsDtype('>f8')\r\nb                    | int32_t                  | AsDtype('>i4')\r\n\r\n>>> f[\"T/whatever/a\"].array()[:10]\r\n<Array [0, 10.1, 20.2, ... 70.7, 80.8, 90.9] type='10 * float64'>\r\n\r\n>>> f = uproot.open(\"nosplit.objectwise.root\")\r\n\r\n>>> f[\"T/whatever\"].show()\r\nname                 | typename                 | interpretation\r\n---------------------+--------------------------+-------------------------------\r\nwhatever             | TWhatever                | AsStridedObjects(Model_TWhatev\r\n>>> f[\"T/whatever\"].array()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-7ac5bffa76fd> in <module>\r\n----> 1 f[\"T/whatever\"].array()\r\n\r\n~/Dev/km3pipe/venv/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in array(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library)\r\n   2070                         ranges_or_baskets.append((branch, basket_num, range_or_basket))\r\n   2071\r\n-> 2072         _ranges_or_baskets_to_arrays(\r\n   2073             self,\r\n   2074             ranges_or_baskets,\r\n\r\n~/Dev/km3pipe/venv/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in _ranges_or_baskets_to_arrays(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays)\r\n   3458\r\n   3459         elif isinstance(obj, tuple) and len(obj) == 3:\r\n-> 3460             uproot.source.futures.delayed_raise(*obj)\r\n   3461\r\n   3462         else:\r\n\r\n~/Dev/km3pipe/venv/lib/python3.8/site-packages/uproot/source/futures.py in delayed_raise(exception_class, exception_value, traceback)\r\n     44         exec(\"raise exception_class, exception_value, traceback\")\r\n     45     else:\r\n---> 46         raise exception_value.with_traceback(traceback)\r\n     47\r\n     48\r\n\r\n~/Dev/km3pipe/venv/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in basket_to_array(basket)\r\n   3415             )\r\n   3416             if basket.num_entries != len(basket_arrays[basket.basket_num]):\r\n-> 3417                 raise ValueError(\r\n   3418                     \"\"\"basket {0} in tree/branch {1} has the wrong number of entries \"\"\"\r\n   3419                     \"\"\"(expected {2}, obtained {3}) when interpreted as {4}\r\n\r\nValueError: basket 0 in tree/branch /T;1:whatever has the wrong number of entries (expected 1064, obtained 836) when interpreted as AsStridedObjects(Model_TWhatever_v5)\r\n    in file nosplit.objectwise.root\r\n```\r\n\r\n[files.zip](https://github.com/scikit-hep/uproot4/files/5997272/files.zip)\r\n",
  "closed_at":null,
  "comments":1,
  "created_at":"2021-02-17T17:00:19Z",
  "id":810364594,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MTAzNjQ1OTQ=",
  "number":275,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Deserialization error in AsStridedObjects but not AsObjects for an example with split level 0.",
  "updated_at":"2021-02-17T18:13:47Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This was reported in a cms-coffea-users email list:\r\n\r\n> Since last week I've encountered the error below [1] in a couple of files from the UL17/18 datasets and I haven't been able to figure out what's causing it. This error occurs even when the only thing I'm doing is counting the number of events in a file. Has anyone else seen this type of error before?\r\n> \r\n> Additional info about my setup:\r\n> coffea version: 0.7\r\n> processor: run_uproot_job\r\n> executor: futures_executor with NanoAODSchema\r\n> running on lxplus\r\n> \r\n> Thanks,\r\n> Joseph\r\n> \r\n> \r\n> [1]  Traceback (most recent call last):\r\n>   File \"/cvmfs/sft.cern.ch/lcg/views/LCG_97python3/x86_64-centos7-gcc8-opt/lib/python3.7/site-packages/XRootD/client/utils.py\", line 39, in __call__\r\n>     self.callback(self.status, self.response, self.hostlist)\r\n>   File \"/afs/cern.ch/user/j/jdulemba/.local/lib/python3.7/site-packages/uproot/source/xrootd.py\", line 220, in callback\r\n>     for chunk in response.chunks:\r\n> AttributeError: 'NoneType' object has no attribute 'chunks'\r\n\r\nThat would be this function here:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/fc32791d3cb420ed95ded64ce544a6ca1484f481/uproot/source/xrootd.py#L221-L225\r\n\r\nBut if the callback is called with `response=None`, what does that mean? If we catch this, can we do anything more meaningful than raising an error message? If so, what's the XRootD error that causes it?\r\n\r\n@kpedro88 responded with\r\n\r\n> This looks the same as https://github.com/CoffeaTeam/coffea/issues/377.\r\n> \r\n> The simplest workaround for now is:\r\n> import uproot4\r\n> uproot4.open.defaults[\"xrootd_handler\"] = uproot4.source.xrootd.MultithreadedXRootDSource\r\n> \r\n> (This may be fixed in a newer version of uproot4, but I'm not sure of the status there.)\r\n\r\nFollowing the Coffea issue, there are links to yet more, similar issues.\r\n\r\nAny follow-up here would be appreciated.",
  "closed_at":"2021-02-24T13:01:46Z",
  "comments":2,
  "created_at":"2021-02-17T19:41:15Z",
  "id":810479447,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MTA0Nzk0NDc=",
  "number":276,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"It's possible for an XRootD callback to be called with `response=None`. What does that mean?",
  "updated_at":"2021-02-24T13:01:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I have this root file which is available on Google drive at this [link](https://drive.google.com/drive/folders/1ixqk_3pDPDpnzo9iVkHtMVmbYXKLCA6n?usp=sharing), and when I used to convert it to arrays in root 3 using parallel processing, it took less time and memory. The code I was using was something like \r\n`\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nexecutor = ThreadPoolExecutor(8)\r\n\r\nbranches = pd.DataFrame.from_dict(uproot.open(''+file_with_path+'')[''+tree_name+''].arrays(namedecode='utf-8', executor = executor))`\r\n\r\nBut now it consumes all my memory in root 4, may be I am not doing it properly. Could you please have a look at it? Also it is not that speedy as it used to be.\r\n\r\n`from concurrent.futures import ThreadPoolExecutor\r\nexecutor = ThreadPoolExecutor(8)\r\n\r\ninput_tree = uproot.open('/path/10k_events_PFSimplePlainTree.root:PlainTree', decompression_executor=executor)\r\n\r\nbranches = input_tree.arrays(library='pd', decompression_executor=executor)`\r\n\r\n\r\n@jpivarski and I discussed this in the issue on this [link](https://github.com/scikit-hep/uproot4/issues/268) and he suggested that it may be just 10% more memory but it is more than 10% for me. May be 60-80% more",
  "closed_at":"2021-02-24T19:36:59Z",
  "comments":6,
  "created_at":"2021-02-18T09:24:58Z",
  "id":810918102,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MTA5MTgxMDI=",
  "number":277,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Memory problem in parallel processing while using uproot 4, it wasn't the case in uproot 3 ",
  "updated_at":"2021-02-24T19:36:59Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This was requested by @WenzDaniel on Gitter. It's good to have an example of a class with `<int>` in its name to introduce a way to generate behaviors for any numeric specialization (though I still need to move `behavior_of` out of `__init__.py`).",
  "closed_at":"2021-02-18T16:46:44Z",
  "comments":0,
  "created_at":"2021-02-18T14:44:20Z",
  "draft":false,
  "id":811163663,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTc1NzM2MTcx",
  "number":278,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-18T16:46:44Z"
  },
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"closed",
  "state_reason":null,
  "title":"Add specialized behaviors for TParameter, including generic dispatch for '<number>' templates.",
  "updated_at":"2021-02-18T16:46:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-02-18T15:52:11Z",
  "comments":0,
  "created_at":"2021-02-18T14:56:06Z",
  "draft":false,
  "id":811174114,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTc1NzQ0OTA1",
  "number":279,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-18T15:52:11Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Remove 'library=cp' (CuPy) support because we can't test it in CI. Arrays can be moved to the GPU after they've been read (it's not a big performance difference).",
  "updated_at":"2021-02-18T15:52:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-02-18T16:46:24Z",
  "comments":0,
  "created_at":"2021-02-18T15:50:57Z",
  "draft":false,
  "id":811225140,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTc1Nzg4MDI1",
  "number":280,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-18T16:46:24Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fixes #252 by interpreting leaf dimensions and only passing a length to ak.layout.RecordArray when it's ambiguous.",
  "updated_at":"2021-02-18T16:46:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-02-18T19:47:22Z",
  "comments":0,
  "created_at":"2021-02-18T19:31:29Z",
  "draft":false,
  "id":811394640,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTc1OTI5ODY4",
  "number":281,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-18T19:47:22Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Trim the memory usage, especially for the case of a flat Pandas DataFrame.",
  "updated_at":"2021-02-18T19:47:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hello, I am currently testing uproot4 functions with the Histoprint command line tool. In uproot3, one could load branches without specifying the full path. In uproot4 that does not seem to be possible.\r\n\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n4.0.4\r\n```\r\n\r\n```\r\n$ wget --quiet http://scikit-hep.org/uproot3/examples/Event.root\r\n$ python -c \"import uproot as up; up.open('Event.root')['T'].arrays(['event/fTracks.fYfirst'])\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 1133, in arrays\r\n    arrays,\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3460, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3414, in basket_to_array\r\n    library,\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/interpretation/objects.py\", line 141, in basket_array\r\n    form = self.awkward_form(branch.file, index_format=\"i64\")\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/interpretation/objects.py\", line 122, in awkward_form\r\n    self._branch.file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/containers.py\", line 645, in awkward_form\r\n    self._values, file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/_util.py\", line 426, in awkward_form\r\n    file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/containers.py\", line 369, in awkward_form\r\n    raise uproot.interpretation.objects.CannotBeAwkward(self.message)\r\nuproot.interpretation.objects.CannotBeAwkward: Double32_t in array (note: Event.root fClosestDistance has an example)\r\n```\r\n\r\nThis used to work with uproot3. It also does work with uproot4 when explicitly providing the full path to the variable:\r\n\r\n```\r\n$ python -c \"import uproot as up; up.open('Event.root')['T'].arrays(['event/fTracks/fTracks.fYfirst'])\"\r\n```\r\n\r\nNot the additional `/fTracks`.\r\n\r\nEasy to fix by providing the full paths, but I am not sure whether this is expected behaviour.",
  "closed_at":"2021-02-22T15:57:09Z",
  "comments":2,
  "created_at":"2021-02-22T14:31:27Z",
  "id":813554390,
  "labels":null,
  "locked":true,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MTM1NTQzOTA=",
  "number":282,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Error when not provifing full branch name",
  "updated_at":"2021-02-22T15:57:10Z",
  "user":"MDQ6VXNlcjU4ODQwNjU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"In\r\nhttps://github.com/scikit-hep/uproot4/blob/fc32791d3cb420ed95ded64ce544a6ca1484f481/uproot/source/xrootd.py#L221-L225\r\n\r\nIf the readv request fails, then a `NoneType` `response` can be returned to the `callback` function, which causes a cryptic traceback to be printed complaining that the `response` object has no `chunks` attribute.\r\n\r\nThis PR doesn't fix any underlying issue, but printing an error message from the xrootd server will show what is going wrong.",
  "closed_at":"2021-02-24T13:01:47Z",
  "comments":3,
  "created_at":"2021-02-24T01:03:03Z",
  "draft":false,
  "id":814971387,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTc4ODc0NTQ0",
  "number":286,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-02-24T13:01:46Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"XRootD: raise an exception if read_vector failed",
  "updated_at":"2021-02-24T13:01:47Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Our ROOT structure is as follows. Within the ROOT file there are a few TTrees, but the only one that is relevant is the Event TTree. Within that Event TTree there is a group that shows up with the typename \"std::vector<ChannelData> channels\" as each event has some # of channels. Each channel class has a member, \"pulse\". The pulse member shows up with the typename \"std::vector<Pulse> pulses\". I am looking for the information in the Pulse group, but it is showing up as a pointer (I think). The interpretation says \"AsObjects(AsArray(True,False,None\" but that is all that I can see. In ROOT the code would look like \"event->channels[0].pulses[0].nph\" and I would like to know how to convert that to python. I have also sent the Jupyter Notebook so you can better understand what I mean. Thank you for any help you can provide.\r\n\r\nAlso on the PyHEP-newcomers chat on Gitter, Mr. Pivarski had suggested writing \"pulse.array()\" which crashes my computer and \"puls.array(library-\"np\")\" which returns a \"memberwise serialization of AsArray\" NotImplementedError.\r\n\r\nhttps://files.gitter.im/5c939a19d73408ce4fbb5e66/R1CK/uproot_problem.ipynb\r\n\r\nhttps://northwestern.box.com/s/4x1w6ym8wsion6lgx49xwfuhkh7gvcdk\r\nthat is the link to the full data file (warning, it is pretty big, 3.5gb).\r\n",
  "closed_at":"2021-05-18T09:50:32Z",
  "comments":2,
  "created_at":"2021-03-02T01:02:16Z",
  "id":819479587,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MTk0Nzk1ODc=",
  "number":288,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Interpretation issue in extraction of vector of user-defined class objects",
  "updated_at":"2021-05-18T09:50:32Z",
  "user":"MDQ6VXNlcjY5NDMxMDg3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Adding interface from scikit-hep/boost-histogram#483.\n",
  "closed_at":"2021-03-03T19:32:40Z",
  "comments":2,
  "created_at":"2021-03-03T18:45:49Z",
  "draft":false,
  "id":821385378,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTg0MTg1MzU4",
  "number":289,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-03T19:32:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: to boost histogram interface support",
  "updated_at":"2021-03-03T19:32:54Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Currently I am using `uproot4.iterate` with 223 ROOT files (~400G total) over XRootD to filter through entries and only save certain columns in a parquet file. \r\n\r\nI am using the latest development version of uproot as of the time of posting at commit 7e3353cfdec91c340c7a0e26a31a695d2db3bcf1. \r\nI was using version 4.0.2, but ran into virtual memory problems (i.e. filling to max available + running into MemoryError), and figured that maybe using #281 would help.\r\n\r\nHere's the code I'm using: \r\n```python\r\n\r\n    arrays = uproot.iterate(\r\n        data.get_list(tree), # a list of xrootd paths formatted like this: \"{path}:{ROOT_tree_location}\"\r\n        filter_name=branches, # a set of 35 column / branch names\r\n        library=\"pd\", # my data is not jagged, so I'm using pandas\r\n    )\r\n    total = 0\r\n    total_saved = 0\r\n    with IncrementalPqWriter(output) as selection: # lets me write row groups to a parquet file from dataframes\r\n        for df in arrays:\r\n            mask = xicp_presel_mask(df, use_rectangular_cuts=use_rect_cuts) # returns boolean mask\r\n            df[\"foldNumber\"] = df[\"eventNumber\"] % 2\r\n            selection.write_from_df(df[mask])\r\n            \r\n            total += df.shape[0]\r\n            total_saved += np.sum(mask)\r\n            log.info(\r\n                f\" {np.sum(mask)} / {df.shape[0]} events saved. \"\r\n                f\"Total: {total_saved} saved / {total} processed\"\r\n            )\r\n\r\n```\r\n\r\n```\r\n  xicp_preselection  INFO     | Loading tree Xicp_ToPpKmPip/DecayTree from 223 ROOT file(s). 35 columns will be saved.\r\n  xicp_preselection  INFO     |  10172 / 47818 events saved. Total: 10172 saved / 47818 processed\r\n  xicp_preselection  INFO     |  7412 / 35161 events saved. Total: 17584 saved / 82979 processed\r\n  xicp_preselection  INFO     |  6339 / 30022 events saved. Total: 23923 saved / 113001 processed\r\n  xicp_preselection  INFO     |  6169 / 28699 events saved. Total: 30092 saved / 141700 processed\r\n  xicp_preselection  INFO     |  8061 / 37881 events saved. Total: 38153 saved / 179581 processed\r\n  xicp_preselection  INFO     |  5939 / 28098 events saved. Total: 44092 saved / 207679 processed\r\n  xicp_preselection  INFO     |  10060 / 47716 events saved. Total: 54152 saved / 255395 processed\r\n  xicp_preselection  INFO     |  4562 / 21916 events saved. Total: 58714 saved / 277311 processed\r\n  xicp_preselection  INFO     |  9492 / 44276 events saved. Total: 68206 saved / 321587 processed\r\n  xicp_preselection  INFO     |  6724 / 31274 events saved. Total: 74930 saved / 352861 processed\r\n  xicp_preselection  INFO     |  8489 / 39955 events saved. Total: 83419 saved / 392816 processed\r\n  xicp_preselection  INFO     |  8082 / 38127 events saved. Total: 91501 saved / 430943 processed\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\n/bin/bash: line 1: 34478 Aborted                 GIT_REVISION=\"$(git rev-parse HEAD)\" PYTHONPATH=\"/.../\" python /.../.snakemake/scripts/tmpreua4su2.wrapper.py\r\n```\r\n\r\nI think this error is maybe related to #281. In `4.0.2` I was just getting `MemoryError` - which seems to be gone now.\r\n\r\n",
  "closed_at":"2021-03-03T22:14:49Z",
  "comments":1,
  "created_at":"2021-03-03T21:54:40Z",
  "id":821517409,
  "labels":null,
  "locked":true,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MjE1MTc0MDk=",
  "number":290,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot4.iterate with XRootD - std::bad_alloc?",
  "updated_at":"2021-03-03T22:14:50Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"In https://github.com/scikit-hep/uproot4/blob/7e3353cfdec91c340c7a0e26a31a695d2db3bcf1/uproot/source/xrootd.py#L341-L343 there is no timeout argument passed to `vector_read`. The signature supports it: https://xrootd.slac.stanford.edu/doc/python/xrootd-python/modules/client/file.html#XRootD.client.File.vector_read and the regular read does use it:\r\nhttps://github.com/scikit-hep/uproot4/blob/7e3353cfdec91c340c7a0e26a31a695d2db3bcf1/uproot/source/xrootd.py#L152-L154",
  "closed_at":"2021-03-04T19:34:23Z",
  "comments":4,
  "created_at":"2021-03-04T18:24:11Z",
  "id":822386030,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MjIzODYwMzA=",
  "number":292,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Xrootd vector read has no timeout",
  "updated_at":"2021-03-04T19:34:23Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Fixes #292",
  "closed_at":"2021-03-04T19:34:24Z",
  "comments":1,
  "created_at":"2021-03-04T19:13:51Z",
  "draft":false,
  "id":822421233,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTg1MDQ5MDg2",
  "number":293,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-04T19:34:23Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Add timeout to vector_read",
  "updated_at":"2021-03-04T19:34:24Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"XRootD vector_read leaks memory and should not be the default until the issue is fixed upstream.\r\nHere is a reproducible example that demonstrates the problem:\r\n\r\n```python\r\nimport uproot\r\nimport threading\r\nfrom queue import Queue\r\nimport psutil\r\nimport time\r\nimport tqdm\r\nimport gc\r\n\r\n\r\nnotifications = Queue()\r\n\r\ndef worker():\r\n    # just eat the chunks as they come in\r\n    while True:\r\n        notifications.get().wait()\r\n        notifications.task_done()\r\n\r\n\r\nusage = []\r\ndef collect_usage():\r\n    chunktype = uproot.source.chunk.Chunk\r\n    usage.append({\r\n        \"dt\": time.time() - tstart,\r\n        \"rss\": process.memory_info().rss,\r\n        \"nchunks\": sum(isinstance(o, chunktype) for o in gc.get_objects()),\r\n    })\r\n\r\nthreading.Thread(target=worker, daemon=True).start()\r\nprocess = psutil.Process()\r\nstep = 1024 * 32  # a typical basket size\r\nstep2 = 10  # like number of baskets per chunk\r\n\r\nfn = \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\"\r\nwith uproot.source.xrootd.XRootDSource(fn, **uproot.open.defaults) as source:\r\n    maxbytes = source.num_bytes  # how much memory would you like to use? :)\r\n    ranges = [(start, start + step) for start in range(0, maxbytes, step)]\r\n\r\n    usage = []\r\n    tstart = time.time()\r\n    for i in tqdm.trange(0, len(ranges), step2):\r\n        subranges = ranges[i:i + step2]\r\n        chunks = source.chunks(subranges, notifications)\r\n        notifications.join()\r\n        del chunks  # this *should* be the only reference to the data\r\n        gc.collect()  # the chunks seem to have cycles\r\n        collect_usage()\r\n\r\ncollect_usage()\r\nimport pandas\r\nusage = pandas.DataFrame(usage).set_index(\"dt\")\r\n```\r\n\r\nIf I then plot the data:\r\n```python\r\nfrom matplotlib.ticker import EngFormatter\r\n\r\nax = usage.plot(secondary_y=\"nchunks\")\r\nax.yaxis.set_major_formatter(EngFormatter(unit='b'))\r\nax.set_title(f\"Reading {maxbytes/1e9:.1f}Gb from a file\")\r\n```\r\nI get:\r\n![image](https://user-images.githubusercontent.com/6587412/110027028-14a42480-7cf7-11eb-8a0f-dd6c8bf7325d.png)\r\n\r\nPS I reported this behavior a long time ago https://github.com/scikit-hep/uproot4/issues/2#issuecomment-629300538",
  "closed_at":"2021-03-04T20:58:40Z",
  "comments":7,
  "created_at":"2021-03-04T20:37:29Z",
  "id":822486884,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MjI0ODY4ODQ=",
  "number":294,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"XRootD vector_read leaks memory",
  "updated_at":"2021-03-05T06:48:00Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Fixes #294",
  "closed_at":"2021-03-04T20:58:40Z",
  "comments":2,
  "created_at":"2021-03-04T20:44:49Z",
  "draft":false,
  "id":822491708,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTg1MTA4Nzkx",
  "number":295,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-04T20:58:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Make MultithreadedXRootDSource the default for xrootd files",
  "updated_at":"2021-03-12T18:20:38Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This helps to break reference cycles in the future notifier, which improves the plot from https://github.com/scikit-hep/uproot4/issues/294#issuecomment-790984804 to:\r\n![image](https://user-images.githubusercontent.com/6587412/110041774-4b376a80-7d0a-11eb-9189-4e603fc7cd99.png)\r\n",
  "closed_at":"2021-03-05T00:39:47Z",
  "comments":0,
  "created_at":"2021-03-04T22:55:09Z",
  "draft":false,
  "id":822575668,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTg1MTc4NDk2",
  "number":296,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-05T00:39:47Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Remove Future attributes once done with them",
  "updated_at":"2021-03-05T00:39:48Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Targeting specifically Chunk and TBasket objects, because they are some of the heaviest intermediate results.\r\n\r\nTesting with the following:\r\n```python\r\nimport uproot\r\nimport psutil\r\nimport time\r\nimport tqdm\r\nimport gc\r\n\r\ndef getmallinfo():\r\n    import ctypes\r\n    class MallInfo(ctypes.Structure):\r\n        _fields_ = [(name, ctypes.c_int)\r\n                    for name in ('arena', 'ordblks', 'smblks', 'hblks', 'hblkhd',\r\n                                'usmblks', 'fsmblks', 'uordblks', 'fordblks',\r\n                                'keepcost')]\r\n\r\n    libc = ctypes.CDLL(\"libc.so.6\")\r\n    mallinfo = libc.mallinfo\r\n    mallinfo.argtypes = []\r\n    mallinfo.restype = MallInfo\r\n    return mallinfo()\r\n\r\n\r\nuproot.open.defaults[\"num_workers\"] = 10\r\n\r\nusage = []\r\ndef collect_usage(asize):\r\n    otype = uproot.source.chunk.Chunk\r\n    mallinfo = getmallinfo()\r\n    usage.append({\r\n        \"dt\": time.time() - tstart,\r\n        \"rss\": process.memory_info().rss,\r\n        \"arraysize\": asize,\r\n        \"ordblks\": mallinfo.ordblks,\r\n        \"uordblks\": mallinfo.uordblks,\r\n        \"fordblks\": mallinfo.fordblks,\r\n        \"nobj\": sum(isinstance(o, otype) for o in gc.get_objects()),\r\n    })\r\n\r\nprocess = psutil.Process()\r\ntstart = time.time()\r\ncollect_usage(0)\r\nfn = \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root:Events\"\r\nwith uproot.open(fn, object_cache=None, array_cache=None) as tree:\r\n    steps = tree.common_entry_offsets()\r\n    steps = [s for s in zip(steps[:-1], steps[1:]) if s[1] < (tree.num_entries // 10)]\r\n    for start, stop in tqdm.tqdm(steps):\r\n        nbytes = sum(\r\n            branch.array(entry_start=start, entry_stop=stop).nbytes\r\n            for branch in tree\r\n        )\r\n        collect_usage(nbytes)\r\n\r\ncollect_usage(0)\r\nimport pandas\r\nusage = pandas.DataFrame(usage).set_index(\"dt\")\r\n```\r\n\r\nInitially the TBasket cycles take a while to be garbage collected, so they pile up and even if gc cleans up, glibc malloc might decide to keep the arena block around (fordblocks = free ordinary blocks, I think):\r\n![image](https://user-images.githubusercontent.com/6587412/110070409-899b4c80-7d3f-11eb-9dcc-a0fabbe5c758.png)\r\n\r\n\r\nAfter refactoring `Model.concrete` we have overall lower and much less varied memory usage:\r\n![image](https://user-images.githubusercontent.com/6587412/110070348-67093380-7d3f-11eb-856f-5c8c2841c9e4.png)\r\n\r\n\r\nLastly, try to make sure the TBaskets are released sooner (not a huge change)\r\n![arrays_fixbasket_oneatatime_dropearly](https://user-images.githubusercontent.com/6587412/110070208-142f7c00-7d3f-11eb-9298-be7611df4fd5.png)\r\n",
  "closed_at":"2021-03-05T14:41:24Z",
  "comments":2,
  "created_at":"2021-03-05T05:18:25Z",
  "draft":false,
  "id":822754731,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTg1MzI3NDg2",
  "number":297,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-05T14:41:24Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Remove more cycles and release heavy stuff early",
  "updated_at":"2021-03-05T15:11:24Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Opening this to make it easier to compare but this is incredibly painful :D See #38 for lengthier details for now.\r\n\r\n```\r\n>>> [i.members for i in eff.member('fBeta_bin_params').tolist()]\r\n[{'first': -1.0, 'second': -2.0}, {'first': 2.0, 'second': 4.0}, {'first': 4.0, 'second': 8.0}, {'first': 8.0, 'second': 16.0}, {'first': 16.0, 'second': 32.0}, {'first': 32.0, 'second': 64.0}, {'first': 64.0, 'second': 128.0}, {'first': 128.0, 'second': 256.0}, {'first': 256.0, 'second': 512.0}, {'first': 512.0, 'second': 1024.0}, {'first': 1024.0, 'second': 2048.0}, {'first': -1.0, 'second': -2.0}, {'first': 1.0, 'second': 1.0}]\r\n```",
  "closed_at":"2021-03-09T18:32:53Z",
  "comments":1,
  "created_at":"2021-03-05T05:47:21Z",
  "draft":false,
  "id":822769309,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTg1MzM5OTcw",
  "number":298,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-09T18:32:53Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: Memberwise for std::pair<>",
  "updated_at":"2021-03-09T18:32:57Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Running into issues where `bcnt` is absurdly large and it might be due to missing behavior in the `uproot4` implementation of `read_object_any` compared to ROOT's: https://github.com/root-project/root/blob/c4aa801d24d0b1eeb6c1623fd18160ef2397ee54/io/io/src/TBufferFile.cxx#L2764-L2765\r\n\r\nSpecifically, I expect around 1080 bytes for this object serialized in a ROOT file, but `bcnt` is `1073742891`.\r\n\r\n```\r\n(Pdb) mycursor.fields(chunk, _numbytes_version_1, context, move=False)\r\n(1073742891, 0)\r\n(Pdb) num_bytes = numpy.int64(1073742891)\r\n(Pdb) bool(num_bytes & uproot.const.kByteCountMask)\r\nTrue\r\n(Pdb) int(num_bytes & ~uproot.const.kByteCountMask) + 4\r\n1071\r\n```\r\n\r\nso there's clearly perhaps missing behavior in masking it to get the proper bytecount out.",
  "closed_at":"2021-03-10T21:04:44Z",
  "comments":2,
  "created_at":"2021-03-10T20:10:23Z",
  "draft":false,
  "id":828275357,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTkwMTAxNDk3",
  "number":301,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-10T21:04:44Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add bytecount mask as part of read_object_any",
  "updated_at":"2021-03-10T21:04:54Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Cutting off at the resource acquisition.\r\nThe only negative I can think of is it might take people by surprise that the pickle does not completely contain the original data. But I also hope they do not expect it to behave like that.. probably worth a warning in the docs?",
  "closed_at":"2021-03-12T14:30:48Z",
  "comments":0,
  "created_at":"2021-03-12T02:37:55Z",
  "draft":false,
  "id":829700739,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTkxMzI0NTQ1",
  "number":302,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-12T14:30:48Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Allow all sources and caches to be pickled",
  "updated_at":"2021-03-12T14:42:07Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"When trying to load an array of a branch, but specifying `entry_start` to be equal or larger than `entry_stop` on a branch that is jagged, the returned object will be a `JaggedArray` instead of an array from the library specified.\r\n\r\n```python\r\nimport uproot\r\nf = uproot.open(\"https://raw.githubusercontent.com/CoffeaTeam/coffea/master/tests/samples/nano_dy.root\")\r\nbranch = f[\"Events\"][\"Jet_pt\"]\r\narray = branch.array(entry_start=1, entry_stop=1)\r\n```\r\nI would expect `array` to be of type `awkward.Array` but it has type `uproot.interpretation.jagged.JaggedArray`.\r\n\r\nI came across this while trying to search for bugs popping up in my code when I have no entries selected.\r\n\r\nTested on uproot version 4.0.6\r\n",
  "closed_at":"2021-03-12T19:11:32Z",
  "comments":1,
  "created_at":"2021-03-12T16:56:43Z",
  "id":830295139,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MzAyOTUxMzk=",
  "number":303,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"TBranch.array returns instance of JaggedArray if entry_start >= entry_stop",
  "updated_at":"2021-03-12T19:11:32Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-03-12T19:11:33Z",
  "comments":0,
  "created_at":"2021-03-12T18:53:53Z",
  "draft":false,
  "id":830365389,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTkxODg3Nzk5",
  "number":304,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-12T19:11:32Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Empty jagged arrays need to be finalized before return.",
  "updated_at":"2021-03-12T19:11:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"I need to figure out what's going on with `uproot-issue33.root` conversion into boost-histogram. If I remember right, it's a histogram with labels.\r\n\r\nOh, and @henryiii, you might be interested in this.",
  "closed_at":"2021-03-12T19:20:40Z",
  "comments":1,
  "created_at":"2021-03-12T19:01:10Z",
  "draft":false,
  "id":830370179,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NTkxODkxODgy",
  "number":305,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-12T19:20:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Histograms should be part of the regular tests.",
  "updated_at":"2021-03-12T19:23:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I'm trying to read some branch\r\n\r\n`patMETs_slimmedMETs__PAT.obj.m_state.p4Polar_.Pt()`\r\n\r\nin MiniAOD, but uproot doesn't recognize it as a branch. If I do\r\n\r\n`event['patMETs_slimmedMETs__PAT.'].show()`\r\n\r\nit shows that the branch is not interpreted properly.\r\n\r\nname                 | typename                 | interpretation    \r\n---------------------+--------------------------+-------------------------------\r\npatMETs_slimmedMETs_ | edm::Wrapper<vector<pat: | AsGroup(<TBranchElement 'patMEpatMETs_slimmedME... | bool                     | AsDtype('bool')\r\npatMETs_slimmedME... | unknown                  | <UnknownInterpretation 'non...\r\n\r\nThere are multiple other branches with the same problem, but I'm not sure what's causing it.\r\n\r\nI'm using the latest version (4.0.6). I can't find a public MiniAOD at the moment, but I think any file will reproduce the issue.\r\n\r\n",
  "closed_at":"2021-03-19T15:07:53Z",
  "comments":12,
  "created_at":"2021-03-19T12:37:03Z",
  "id":835942718,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4MzU5NDI3MTg=",
  "number":308,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Can't access some branches in MiniAOD",
  "updated_at":"2021-03-19T17:10:20Z",
  "user":"MDQ6VXNlcjY0MTA4ODUw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"While uproot4 supports `TGraphAsymmErrors`, it seems plain `TGraph` isn't supported.",
  "closed_at":"2021-06-22T13:22:57Z",
  "comments":9,
  "created_at":"2021-03-22T11:25:19Z",
  "id":837621237,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4Mzc2MjEyMzc=",
  "number":309,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"TGraph not supported",
  "updated_at":"2021-06-22T13:22:57Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"If I look at the [uproot3](https://github.com/scikit-hep/uproot3), there is a big bold text telling everyone that it is deprecated in favor of `uproot4`, so I switched and then later found out that the functionality of \"writing\" isn't yet supported. \r\n\r\nI feel that this is a bit unfortunate, considering that the writing part is quite a substantial part of an `I/O` library. \r\nSo to avoid confusion, IMHO the missing writing functionality deserves a similar large bold info instead of the small note half a page down in the readme.    ",
  "closed_at":"2021-08-30T22:19:47Z",
  "comments":2,
  "created_at":"2021-03-24T19:26:00Z",
  "id":840090770,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4NDAwOTA3NzA=",
  "number":310,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Make the fact that one can't write with uproot4 more obvious",
  "updated_at":"2021-08-30T22:19:47Z",
  "user":"MDQ6VXNlcjExMzUxNjcx"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi, \r\n\r\nI'm using uproot version 4.0.6 with uproot_methods 0.9.2. This bug affects uproot_methods TLorentzVector. I see that this is package is deprecated, but it still appears to be getting updated. Apologies if I am reporting this in the wrong place as well reporting on a deprecated package.\r\n\r\nI have a lot of very low mass TLorentzVector objects, and I've seen that summing them together can lead to an error from math.sqrt() from negative argument.\r\n\r\nmag2 returns a negative value, which then results in the above error.\r\nhttps://github.com/scikit-hep/uproot3-methods/blob/master/uproot3_methods/classes/TLorentzVector.py#L58\r\n\r\n>>> import uproot_methods\r\n>>> m=uproot_methods.TLorentzVector(x=32.07181913680199, y=-212.10064225436406, z=-126.49557185391019, t=249.0309493338232)\r\n>>> m.mass # this will crash\r\n>>> m.mag2 # this is -4e-12, which is before the float precision. Can some protection be added for float precision?\r\n\r\nBest,\r\n\r\nDoug",
  "closed_at":"2021-03-25T14:35:12Z",
  "comments":4,
  "created_at":"2021-03-25T14:03:14Z",
  "id":840963799,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4NDA5NjM3OTk=",
  "number":311,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Float precision issue with mass in TLorentzVector",
  "updated_at":"2021-03-25T15:27:30Z",
  "user":"MDQ6VXNlcjgyOTg1NTc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"I've been trying to read some CMS edm collections, and played around a bit, finding at least one memberwise interpretation successful, with a header like `>h: version` `>I: length` `>I: values_num_bytes`, e.g.\r\n```\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0  10   0   0  16   1  64  22 127 198   0   9   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- ---   @ --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n```\r\nfor a class that indeed was version 10 and had the right amount of bytes. This class had a very simple set of members (just one):\r\n```python\r\n    _stl_container0 = uproot.containers.AsVector(False, numpy.dtype(\"u1\"))\r\n    base_names_versions = []\r\n    member_names = ['data_']\r\n    class_flags = {}\r\n```\r\n@kratsg might be interested.",
  "closed_at":"2022-06-21T12:31:17Z",
  "comments":7,
  "created_at":"2021-03-28T23:18:18Z",
  "draft":true,
  "id":842855848,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjAyMzI4NDYz",
  "number":314,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Some more memberwise stuff",
  "updated_at":"2022-06-21T13:46:06Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Very small typo in the doc",
  "closed_at":"2021-03-31T12:52:08Z",
  "comments":1,
  "created_at":"2021-03-31T10:29:40Z",
  "draft":false,
  "id":846380835,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjA1NDgwODY0",
  "number":315,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-31T12:52:08Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix typo events -> uproot",
  "updated_at":"2021-03-31T12:52:08Z",
  "user":"MDQ6VXNlcjE0MzM4OQ=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"@henryiii I'm starting my work on Uproot with a clean-up. One of the major items here will be to get Uproot flake8 complaint again and another will be to set up a pre-commit hook like the one in Vector. (I'm using Vector as a model of how to do these things because it's also a pure Python project, unlike Awkward.)\r\n\r\nThings that I'm planning in this PR:\r\n\r\n   - [x] Move the requirements*.txt files into setup.py because they had been based on a misunderstanding about what they were for (which I don't entirely remember now, only that I've been meaning to do this).\r\n   - [ ] Make Uproot flake8 complaint again.\r\n   - [ ] Start using pre-commit.\r\n   - [ ] Start using isort. (I'm not planning on typing it with MyPy; that's too big of a task and remember that Uproot has to support Python 2.)\r\n   - [ ] Start using the version number-generator.\r\n   - [x] Move the `uproot` directory to `src/uproot`. (For my incremental development, I put a dummy version file in the directory and a symlink from the base repo into `src/uproot`. It's not a clean test but that's not what I need during incremental development. It was in the Vector work that I found this solution; typing `pip install .` slows down the debugging loop too much.)",
  "closed_at":"2021-03-31T19:34:32Z",
  "comments":4,
  "created_at":"2021-03-31T18:43:06Z",
  "draft":false,
  "id":847122658,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjA2MTU5NjU5",
  "number":316,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-31T19:34:32Z"
  },
  "reactions":{
   "hooray":2,
   "total_count":2
  },
  "state":"closed",
  "state_reason":null,
  "title":"Cleaning up Uproot",
  "updated_at":"2021-03-31T19:34:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This is following up on PR #316. I want to ensure that the \"change file contents\" commit is distinct from the \"move files\" commit, and I'd rather use the \"Squash and merge\" button than carefully rebase things.\r\n\r\nTasks for this PR:\r\n\r\n   - [x] Drop Python 3.5 from tests. It will not be _formally_ supported (just as Pythons 2.6 and 2.7 are not _formally_ supported, either).\r\n   - [x] Start using pre-commit.\r\n   - [x] Make Uproot flake8 complaint again.\r\n   - [x] Start using isort.\r\n   - [ ] ~~Start using the version number-generator.~~ I'm not quite ready for that yet. I have to be sure that it's going to make the correct version numbers when deployed.\r\n   - [x] Enable auto-merge.\r\n",
  "closed_at":"2021-03-31T22:16:40Z",
  "comments":0,
  "created_at":"2021-03-31T19:44:36Z",
  "draft":false,
  "id":847214325,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjA2MjQ0MDk5",
  "number":317,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-03-31T22:16:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Cleaning up Uproot 2.",
  "updated_at":"2021-04-01T01:33:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Maybe could use some extra eyes from @nsmith- . I'm using a HistFactory workspace in a ROOT file as an example. I can pass along the ROOT file for sure so you can look into it.. still trying to work out how to handle the reading correctly for this sort of file where there's a more complicated memberwise structure buried inside.",
  "closed_at":"2022-06-21T13:28:24Z",
  "comments":2,
  "created_at":"2021-03-31T20:48:54Z",
  "draft":true,
  "id":847311928,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjA2MzM0NjE1",
  "number":318,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Draft: nested memberwise",
  "updated_at":"2022-09-23T00:43:23Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-01T00:14:36Z",
  "comments":0,
  "created_at":"2021-03-31T22:45:53Z",
  "draft":false,
  "id":847474061,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjA2NDg3MTIy",
  "number":319,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-01T00:14:36Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: restore support for old Pythons",
  "updated_at":"2021-04-01T00:14:37Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"As a very first step, I add ROOT to one of the tests (Linux, Python 3.8). For the duration of this PR, I'll have the other tests commented out (since they're required, I can't forget to put them back in).",
  "closed_at":"2021-04-01T23:04:53Z",
  "comments":0,
  "created_at":"2021-04-01T15:52:57Z",
  "draft":false,
  "id":848588565,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjA3NTAyOTY2",
  "number":320,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-01T23:04:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Start working on ROOT writing.",
  "updated_at":"2021-04-01T23:04:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"The goal of this PR is to make tasks like the one in #320 more abstract, less \"rawbytes manipulation.\"\r\n\r\nSince the file-writer also has to read data (to find out where to write), some of its functionality is duplicated with Cursors and Sources. I think that's okay: writing is fundamentally different from reading\u2014not symmetric at all\u2014and so the infrastructure should be semi-independent. We may (eventually) use the same Model classes for reading and writing, so that the deserialization and serialization patterns are right next to each other and therefore easier to maintain, but the more fundamental stuff with maintaining the FreeSegments structure and cascading updates through dependencies should be kept separate from the code for read-only objects.\r\n\r\nI should also link in discussion #321, since this is a continuation of that project.",
  "closed_at":"2021-04-08T00:00:12Z",
  "comments":0,
  "created_at":"2021-04-02T18:03:41Z",
  "draft":false,
  "id":849369839,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjA4MTU1NTI1",
  "number":322,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-08T00:00:11Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Starting the infrastructure for writing files.",
  "updated_at":"2021-04-08T00:00:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"",
  "closed_at":"2021-04-09T00:22:37Z",
  "comments":0,
  "created_at":"2021-04-05T17:01:28Z",
  "draft":false,
  "id":850508790,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjA5MDcwNDc2",
  "number":323,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-09T00:22:37Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-04-09T00:22:38Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi,\r\nI have python 3.8.3 installed by an anaconda installation:\r\n(base) [bockjoo@cms ~]$ which python\r\n/opt/cms/services/anaconda3/bin/python\r\n(base) [bockjoo@cms ~]$ python -V\r\nPython 3.8.3\r\nBut I can not find the attribute recreate and some other attributes:\r\n\r\n(base) [bockjoo@cms ~]$ python \r\nPython 3.8.3 (default, Jul  2 2020, 16:21:59) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot\r\n>>> dir(uproot)\r\n['AsArray', 'AsDouble32', 'AsDtype', 'AsDtypeInPlace', 'AsDynamic', 'AsFloat16', 'AsGrouped', 'AsJagged', 'AsMap', 'AsObjects', 'AsPointer', 'AsSTLBits', 'AsSet', 'AsStridedObjects', 'AsString', 'AsStrings', 'AsVector', 'Cursor', 'DeserializationError', 'HTTPSource', 'KeyInFileError', 'LRUArrayCache', 'LRUCache', 'LZ4', 'LZMA', 'MemmapSource', 'Model', 'MultithreadedFileSource', 'MultithreadedHTTPSource', 'MultithreadedXRootDSource', 'ObjectSource', 'ReadOnlyDirectory', 'ReadOnlyFile', 'STLMap', 'STLSet', 'STLVector', 'TBranch', 'TTree', 'ThreadPoolExecutor', 'TrivialExecutor', 'XRootDSource', 'ZLIB', 'ZSTD', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_util', 'absolute_import', 'behavior_of', 'behaviors', 'cache', 'class_named', 'classes', 'classname_decode', 'classname_encode', 'compression', 'concatenate', 'const', 'containers', 'default_library', 'deserialization', 'dynamic', 'exceptions', 'extras', 'has_class_named', 'interpretation', 'iterate', 'language', 'lazy', 'model', 'models', 'no_filter', 'open', 'reading', 'reset_classes', 'source', 'unknown_classes', 'uproot', 'version']\r\n>>> quit()\r\n\r\nIs there a way to fix this?\r\nThanks,\r\nBockjoo",
  "closed_at":"2021-04-05T21:20:31Z",
  "comments":1,
  "created_at":"2021-04-05T21:08:03Z",
  "id":850677025,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4NTA2NzcwMjU=",
  "number":324,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot.recreate can't be found in Uproot 4",
  "updated_at":"2021-04-05T21:20:31Z",
  "user":"MDQ6VXNlcjQ3MTk4Nzc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Python includes a number of URL parsing utilities. One benefit is that the format for URLs in python is standard across libraries. It currently looks like the [URL parsing](https://github.com/scikit-hep/uproot4/blob/cec62a63a6639238179d622b437af9587dbc5284/src/uproot/_util.py#L171-L201) code for the `file` scheme in uproot4 is not conformant on Windows (but works fine on Linux).\r\n\r\nAn example of a valid windows URL: `file:///g:/mydir/file.root`. The above code will parse this, on windows, to `/g:/mydir/file.root` which cannot be opened by python's `open` function. However, `g:/mydir/file.root` works just fine.\r\n\r\nThere is another possible issue - in that URLs aren't unquoted during processing by the code (though I did not test this).\r\n\r\nTo work around this problem I've used some [discussion on stack overflow](https://stackoverflow.com/questions/5977576/is-there-a-convenient-way-to-map-a-file-uri-to-os-path#) to help me with the following code which works in python 3+, and is cross-platform:\r\n\r\n```\r\nfrom urllib.parse import urlparse, unquote\r\nfrom urllib.request import url2pathname\r\n\r\np = urlparse(file_url)\r\nfile_path = url2pathname(unquote(p.path))\r\n```\r\n\r\nNote that even that discussion is a bit confused: it is important to read the answer comments to interpret the answers. The above example was pulled from the last answer, and then the python2 code was stripped out.\r\n\r\nIf I might be so bold, one way to do testing on this is using `pathlib`. For example:\r\n\r\n```\r\nfrom pathlib import Path\r\n\r\ntest_file = Path('./test_data/myfile.root')   # Should work on all platforms.\r\ntest_file_url = test_file.absolute().as_uri()\r\n\r\ndata = uproot4.open(test_file_url)\r\n```\r\n\r\nOr similar. But this will make sure your file URL parsing works on all platforms and uses python \"standards\" to make sure it all works.",
  "closed_at":"2021-04-08T19:11:02Z",
  "comments":0,
  "created_at":"2021-04-05T21:14:15Z",
  "id":850683423,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4NTA2ODM0MjM=",
  "number":325,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Improper parsing of a `file://` uri on Windows",
  "updated_at":"2021-04-08T19:11:02Z",
  "user":"MDQ6VXNlcjE3NzgzNjY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-08T19:11:02Z",
  "comments":0,
  "created_at":"2021-04-08T18:48:21Z",
  "draft":false,
  "id":853752877,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjExNzkyNzg2",
  "number":328,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-08T19:11:02Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fixes #325, file:// on Windows, and handles %-encoded URIs.",
  "updated_at":"2021-04-08T19:11:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-09T00:22:05Z",
  "comments":0,
  "created_at":"2021-04-08T23:09:50Z",
  "draft":false,
  "id":854007462,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjEyMDIyNTEw",
  "number":329,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-09T00:22:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Update existing ROOT files and update ROOT -> Uproot -> ROOT.",
  "updated_at":"2021-04-09T00:22:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"In different tests that run over multiple files, using both xrootd and http access sources i'm seeing an increase in the number of threads. Also it seems sometimes (in http and also in xrootd even after the fix from https://github.com/xrootd/xrootd/pull/1440) there might be a slight memory leak. On the one hand, vmem increases proportional to the increasing number of threads (don't know if that means anything). But on the other hand i also sometimes see an increase of RSS/PSS memory.\r\n\r\nOne reproducer i could come up with is the following script:\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport os\r\nimport uproot\r\n\r\nbase_url = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/4lep/MC/\"\r\n\r\nfilenames = [\r\n    \"mc_364100.Zmumu_PTV0_70_CVetoBVeto.4lep.root\",\r\n    \"mc_364101.Zmumu_PTV0_70_CFilterBVeto.4lep.root\",\r\n    \"mc_364102.Zmumu_PTV0_70_BFilter.4lep.root\",\r\n    \"mc_364103.Zmumu_PTV70_140_CVetoBVeto.4lep.root\",\r\n    \"mc_364104.Zmumu_PTV70_140_CFilterBVeto.4lep.root\",\r\n    \"mc_364105.Zmumu_PTV70_140_BFilter.4lep.root\",\r\n    \"mc_364106.Zmumu_PTV140_280_CVetoBVeto.4lep.root\",\r\n    \"mc_364107.Zmumu_PTV140_280_CFilterBVeto.4lep.root\",\r\n    \"mc_364108.Zmumu_PTV140_280_BFilter.4lep.root\",\r\n    \"mc_364109.Zmumu_PTV280_500_CVetoBVeto.4lep.root\",\r\n    \"mc_364110.Zmumu_PTV280_500_CFilterBVeto.4lep.root\",\r\n    \"mc_364111.Zmumu_PTV280_500_BFilter.4lep.root\",\r\n    \"mc_364112.Zmumu_PTV500_1000.4lep.root\",\r\n    \"mc_364113.Zmumu_PTV1000_E_CMS.4lep.root\",\r\n    \"mc_364114.Zee_PTV0_70_CVetoBVeto.4lep.root\",\r\n    \"mc_364115.Zee_PTV0_70_CFilterBVeto.4lep.root\",\r\n    \"mc_364116.Zee_PTV0_70_BFilter.4lep.root\",\r\n    \"mc_364117.Zee_PTV70_140_CVetoBVeto.4lep.root\",\r\n    \"mc_364118.Zee_PTV70_140_CFilterBVeto.4lep.root\",\r\n    \"mc_364119.Zee_PTV70_140_BFilter.4lep.root\",\r\n    \"mc_364120.Zee_PTV140_280_CVetoBVeto.4lep.root\",\r\n    \"mc_364121.Zee_PTV140_280_CFilterBVeto.4lep.root\",\r\n    \"mc_364122.Zee_PTV140_280_BFilter.4lep.root\",\r\n    \"mc_364123.Zee_PTV280_500_CVetoBVeto.4lep.root\",\r\n    \"mc_364124.Zee_PTV280_500_CFilterBVeto.4lep.root\",\r\n    \"mc_364125.Zee_PTV280_500_BFilter.4lep.root\",\r\n    \"mc_364126.Zee_PTV500_1000.4lep.root\",\r\n    \"mc_364127.Zee_PTV1000_E_CMS.4lep.root\",\r\n    \"mc_364128.Ztautau_PTV0_70_CVetoBVeto.4lep.root\",\r\n    \"mc_364129.Ztautau_PTV0_70_CFilterBVeto.4lep.root\",\r\n    \"mc_364130.Ztautau_PTV0_70_BFilter.4lep.root\",\r\n    \"mc_364131.Ztautau_PTV70_140_CVetoBVeto.4lep.root\",\r\n    \"mc_364132.Ztautau_PTV70_140_CFilterBVeto.4lep.root\",\r\n    \"mc_364133.Ztautau_PTV70_140_BFilter.4lep.root\",\r\n    \"mc_364134.Ztautau_PTV140_280_CVetoBVeto.4lep.root\",\r\n    \"mc_364135.Ztautau_PTV140_280_CFilterBVeto.4lep.root\",\r\n    \"mc_364136.Ztautau_PTV140_280_BFilter.4lep.root\",\r\n    \"mc_364137.Ztautau_PTV280_500_CVetoBVeto.4lep.root\",\r\n    \"mc_364138.Ztautau_PTV280_500_CFilterBVeto.4lep.root\",\r\n    \"mc_364139.Ztautau_PTV280_500_BFilter.4lep.root\",\r\n    \"mc_364140.Ztautau_PTV500_1000.4lep.root\",\r\n]\r\n\r\nimport gc\r\n\r\nfor filename in filenames:\r\n    with uproot.open(f\"{os.path.join(base_url, filename)}:mini\") as tree:\r\n        print(tree.arrays(filter_name=\"lep_*\"))\r\n    gc.collect()\r\n```\r\n\r\nWhen i run this through [prmon](https://github.com/HSF/prmon) with \r\n\r\n```\r\nprmon --interval 1 -- python test_uproot_http.py\r\n```\r\n\r\ni get this graph for the number of threads:\r\n\r\n```\r\nprmon_plot.py --input prmon.txt --xvar wtime --yvar nthreads\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/3707225/115054287-5cb28b80-9ee0-11eb-89e3-a46a04699994.png)\r\n\r\nand this for RSS and PSS\r\n\r\n```\r\nprmon_plot.py --input prmon.txt --xvar wtime --yvar pss,rss --yunit GB\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/3707225/115054571-a7340800-9ee0-11eb-9316-ff169d64d60e.png)\r\n\r\nFor now i don't have a reproducer for xrootd, but i think one can see similar issues there (i'll try to come up with an example).\r\n",
  "closed_at":"2021-04-16T17:32:34Z",
  "comments":5,
  "created_at":"2021-04-16T16:25:02Z",
  "id":859974135,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4NTk5NzQxMzU=",
  "number":333,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Network access sources leak threads (and maybe also memory)",
  "updated_at":"2021-04-16T17:32:34Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-16T17:32:34Z",
  "comments":1,
  "created_at":"2021-04-16T17:13:06Z",
  "draft":false,
  "id":860009111,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjE2OTkwMjE5",
  "number":334,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-16T17:32:34Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Stop leaking threads in HTTP (through the fallback mechanism).",
  "updated_at":"2021-04-16T17:32:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I am using uproot version 4.0.7 iterate to read through several TTree objects.\r\nWhen a TTree object happens to be empty (i.e. with no entries), I get an error.\r\nI think the correct behavior should be that no error is encountered and the empty Tree is ignored.\r\nI have attached a sample ROOT file with an empty Tree inside:\r\n[empty_tree.txt](https://github.com/scikit-hep/uproot4/files/6327025/empty_tree.txt)\r\nPlease rename `empty_tree.txt` to `empty_tree.root`.\r\nThe code that fails to read it is:\r\n\r\n```python\r\nimport uproot\r\nprint(\"uproot.__version__\", uproot.__version__)\r\n\r\nfor dataframe in uproot.iterate(\"empty_tree.root:empty_tree\", ['var'], library='pd'): \r\n\tprint(\"dataframe\\n\", dataframe)\r\n```\r\n\r\nThe error message I see is:\r\n\r\n```\r\nuproot.__version__ 4.0.7\r\nTraceback (most recent call last):\r\n  File \"iterate_through_empty_tree.py\", line 5, in <module>\r\n    for dataframe in uproot.iterate(\"empty_tree.root:empty_tree\", ['var'], library='pd'):\r\n  File \"/home/name/.opt/python/miniconda3.8/envs/omega-analysis/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 205, in iterate\r\n    for item in hasbranches.iterate(\r\n  File \"/home/name/.opt/python/miniconda3.8/envs/omega-analysis/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 1316, in iterate\r\n    entry_step = _regularize_step_size(\r\n  File \"/home/name/.opt/python/miniconda3.8/envs/omega-analysis/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 3540, in _regularize_step_size\r\n    return _hasbranches_num_entries_for(\r\n  File \"/home/name/.opt/python/miniconda3.8/envs/omega-analysis/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 3523, in _hasbranches_num_entries_for\r\n    num_entries = int(round(target_num_bytes * total_entries / total_bytes))\r\nZeroDivisionError: float division by zero\r\n```",
  "closed_at":"2021-04-16T18:23:21Z",
  "comments":1,
  "created_at":"2021-04-16T17:50:44Z",
  "id":860047511,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4NjAwNDc1MTE=",
  "number":335,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot4.iterate on Empty TTree Object",
  "updated_at":"2021-04-16T18:23:21Z",
  "user":"MDQ6VXNlcjg1MTcxNjA="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-16T18:23:21Z",
  "comments":0,
  "created_at":"2021-04-16T18:15:04Z",
  "draft":false,
  "id":860062334,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjE3MDMxNTUy",
  "number":336,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-16T18:23:20Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Avoid division by zero in calculating the number of entries.",
  "updated_at":"2021-04-16T18:23:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Specifically, scikit-hep/scikit-hep-testdata#51. There were a couple of places where I used `data_path` to get the location of a file and then modified it to the name of another file I knew existed. Now they don't necessarily exist until you explicitly call `data_path` on them.",
  "closed_at":"2021-04-16T20:40:35Z",
  "comments":0,
  "created_at":"2021-04-16T19:55:53Z",
  "draft":false,
  "id":860121782,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjE3MDgwODE0",
  "number":337,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-16T20:40:35Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Adjust to the new scikit-hep-testdata.",
  "updated_at":"2021-04-16T20:40:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Looking at the changes from #334 i noticed that the `HTTPSource` calls `shutdown` on it's `ResourceThreadPoolExecutor`. I also have to do that for the `ResourceThreadPoolExecutor` i introduced in #243 because `XRootDSource` also has it's own `__exit__` method. It also has to be the `shutdown` method of the executor and not `__exit__`, because `__exit__` will try to call `__exit__` on the Resources and i set the resources in the `ResourceThreadPoolExecutor` of `XRootDSource` to `None`, because it's just there to merge chunks from vector reads that had to be split up.\r\n\r\nBefore that fix, reading with `XRootDSource` was also leaking threads:\r\n\r\n```python\r\n# test_uproot_xrd.py\r\nimport uproot\r\n\r\nfor i in range(10):\r\n    with uproot.open(\r\n        \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root:Events\",\r\n        xrootd_handler=uproot.XRootDSource,\r\n    ) as tree:\r\n        print(tree[\"Muon_pt\"].array(entry_stop=5000))\r\n```\r\n\r\n```\r\nprmon --interval 0.1 -- python test_uproot_xrd.py\r\nprmon_plot.py --input prmon.txt --xvar wtime --yvar nthreads\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/3707225/115120130-1a568080-9fac-11eb-836b-3494157fddda.png)\r\n\r\nAfter the fix it seems to be fine:\r\n\r\n![image](https://user-images.githubusercontent.com/3707225/115120147-2fcbaa80-9fac-11eb-8391-726932170b0d.png)\r\n",
  "closed_at":"2021-04-17T18:37:39Z",
  "comments":1,
  "created_at":"2021-04-17T16:39:20Z",
  "draft":false,
  "id":860450017,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjE3MzMzODMy",
  "number":338,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-17T18:37:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Shutdown the ThreadPoolExecutor in XRootDSource on exit (stop leaking threads in XRootDSource)",
  "updated_at":"2021-04-17T18:39:25Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"`ROOT`'s `TFile::Open` supports transparent access not just to HTTP, but also HTTP with basic authentication, by embedding the username and password in the URL, e.g. `http(s)://username:password@example.org/path/to/file.root`. This provides a some level of security to your precious data which you may not want to make public (yet?). In practice, hosting files this way with some credentials widely known to a collaboration is used a fair amount to expose data in academic clusters. \r\n\r\nThis pull request adds equivalent functionality to uproot. `urllib`'s parser already parses the username and password from an URL, so we can grab the credentials from the parsed_url when available and append them to the headers. Note that this is only secure at all when using HTTP, as otherwise any interloper can snoop the base64-encoded username and password. I didn't implement it this way, but it may be preferable to not allow basic auth on non-HTTPS connections. \r\n\r\nI have lightly tested this, and I assume I haven't hit all the possible corner cases (like, I assume either the normal or the fallback mode worked for me, but not both). \r\n\r\nHere is what a test might look like, although you probably shouldn't use my URL's because I can't guarantee that they'll be available for eternity:\r\n\r\n```\r\nimport uproot\r\n\r\ndef test():\r\n  # test file with no auth needed\r\n  with uproot.open('https://users.rcc.uchicago.edu/~cozzyd/uproot-test/not-so-secret-sauce/mundane.root') as f:\r\n    assert ( f['hrandom'].all_members['fTitle'] =='Random Numbers')\r\n  # test file with auth needed\r\n  with uproot.open('https://DarkHelmet:12345@users.rcc.uchicago.edu/~cozzyd/uproot-test/secret-sauce/top-secret.root') as f:\r\n    assert (f['hmeaning'].all_members['fTitle']=='Meaning of Life') \r\n```\r\n\r\nI have only tested this on Python3... it's possible it may not work on Python2 due to the differences in strings (I suspect the line that does the base64 encoding may fail on Python2, but I'm not sure how to make it compatible with both, or if what I do on that line is even sane).  ",
  "closed_at":"2021-04-20T17:31:24Z",
  "comments":6,
  "created_at":"2021-04-18T06:07:27Z",
  "draft":false,
  "id":860584240,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjE3NDI4NzI2",
  "number":339,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-20T17:31:24Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Add support for basic HTTP authentication",
  "updated_at":"2021-04-20T17:31:24Z",
  "user":"MDQ6VXNlcjkyMDY1Njk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/PyCQA/flake8: 3.9.0 \u2192 3.9.1](https://github.com/PyCQA/flake8/compare/3.9.0...3.9.1)\n",
  "closed_at":"2021-04-19T17:35:46Z",
  "comments":0,
  "created_at":"2021-04-19T17:01:44Z",
  "draft":false,
  "id":861536094,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjE4MTg1NDQw",
  "number":340,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-19T17:35:46Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-04-19T17:35:47Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-22T00:57:43Z",
  "comments":0,
  "created_at":"2021-04-20T01:13:00Z",
  "draft":false,
  "id":862233555,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjE4ODI5MjUz",
  "number":341,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-22T00:57:42Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"File writing: manipulating TStreamerInfo",
  "updated_at":"2021-04-22T00:57:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @cozzyd as a contributor for test, code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot4/pull/339#issuecomment-823468008)",
  "closed_at":"2021-04-22T16:05:48Z",
  "comments":0,
  "created_at":"2021-04-20T17:31:02Z",
  "draft":false,
  "id":863055152,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjE5NTI1MTU4",
  "number":342,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-22T16:05:48Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add cozzyd as a contributor",
  "updated_at":"2021-04-22T16:05:49Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-22T16:33:43Z",
  "comments":0,
  "created_at":"2021-04-22T16:24:38Z",
  "draft":false,
  "id":865151778,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjIxMjYyNjQ3",
  "number":343,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-22T16:33:43Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Use real datetimes in TDirectories and TKeys.",
  "updated_at":"2021-04-22T16:33:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-23T15:17:18Z",
  "comments":0,
  "created_at":"2021-04-22T16:51:24Z",
  "draft":false,
  "id":865182927,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjIxMjg5MzQw",
  "number":344,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-23T15:17:18Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Add file-reading methods to WritableDirectory, so it can be mostly interchangeable with ReadOnlyDirectory.",
  "updated_at":"2021-04-23T15:17:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-23T19:48:17Z",
  "comments":0,
  "created_at":"2021-04-23T19:32:40Z",
  "draft":false,
  "id":866377498,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjIyMjg0MDUx",
  "number":345,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-23T19:48:17Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Implemented and tested the bulk copy method.",
  "updated_at":"2021-04-23T19:48:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-04-23T20:32:03Z",
  "comments":0,
  "created_at":"2021-04-23T20:22:53Z",
  "draft":false,
  "id":866407880,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjIyMzA5NDQ2",
  "number":346,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-23T20:32:03Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Touch-ups to the bulk copy method (renaming regex syntax and exclude TTree/RNtuple).",
  "updated_at":"2021-04-23T20:32:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 20.8b1 \u2192 21.4b0](https://github.com/psf/black/compare/20.8b1...21.4b0)\n",
  "closed_at":"2021-04-26T17:34:21Z",
  "comments":0,
  "created_at":"2021-04-26T17:12:02Z",
  "draft":false,
  "id":867934381,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjIzNTI0NjE0",
  "number":347,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-04-26T17:34:21Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-04-26T17:34:22Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.4b0 \u2192 21.4b2](https://github.com/psf/black/compare/21.4b0...21.4b2)\n",
  "closed_at":"2021-05-03T17:48:23Z",
  "comments":0,
  "created_at":"2021-05-03T17:09:14Z",
  "draft":false,
  "id":874748813,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjI5MjM5Mjg2",
  "number":348,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-03T17:48:22Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-05-03T17:48:23Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"The `__setitem__` syntax,\r\n\r\n```python\r\n    with uproot.recreate(filename) as f1:\r\n        f1[\"hey\"] = \"you\"\r\n        f1[\"subdir/there\"] = \"you guys\"\r\n```\r\n\r\nis implemented through an `update` function (which handles arguments the same way as dict's `update`).",
  "closed_at":"2021-05-03T23:05:11Z",
  "comments":0,
  "created_at":"2021-05-03T22:54:53Z",
  "draft":false,
  "id":874969180,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjI5NDEzODIx",
  "number":349,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-03T23:05:11Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Added ability to write TObjStrings.",
  "updated_at":"2021-05-03T23:05:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"This adds a behaviour for `TGraph` (based on that for `TGraphAsymmErrors`), and behaviours for `RooHist` and `RooCurve` based on these. These make it easier to plot RooFit objects on a plot that's otherwise made with Python.\r\n\r\n~~Draft because I haven't added any tests yet. I thought it would be better to discuss first.~~",
  "closed_at":"2021-06-22T13:22:57Z",
  "comments":14,
  "created_at":"2021-05-04T11:13:50Z",
  "draft":false,
  "id":875357294,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjI5NzE2MzI2",
  "number":350,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-06-22T13:22:57Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Add TGraph, RooCurve, and RooHist behaviours.",
  "updated_at":"2021-06-25T11:05:29Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"When reading many files via XRootD, we see intermittent and seemingly non-deterministic \"Operation expired\" errors from XRootD. This might be an XRootD or an EOS issue, but I filed it here since we are experiencing it when using Uproot. I'd be interested to hear if you have ideas about how to troubleshoot this.\r\n\r\nMWE to reproduce follows. Unfortunately, it requires access to LHCb files on EOS.\r\n\r\n```py\r\nimport uproot\r\n\r\nsamples = [\r\n    f\"root://eoslhcb.cern.ch//eos/lhcb/grid/prod/lhcb/LHCb/Collision16/PIDCALIB.ROOT/00111823/0000/00111823_{i:08}_1.pidcalib.root\"\r\n    for i in range(1, 239)\r\n]\r\n\r\nfor file in samples:\r\n    print(f\"Reading {file}\")\r\n    tree = uproot.open(file)[\"DSt_PiMTuple\"]\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"mwe_xrootd.py\", line 12, in <module>\r\n    tree = uproot.open(file)[\"DSt_PiMTuple\"]\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/reading.py\", line 145, in open\r\n    **options  # NOTE: a comma after **options breaks Python 2\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/reading.py\", line 553, in __init__\r\n    file_path, **self._options  # NOTE: a comma after **options breaks Python 2\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 434, in __init__\r\n    self._open()\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 440, in _open\r\n    for x in uproot._util.range(self._num_workers)\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 440, in <listcomp>\r\n    for x in uproot._util.range(self._num_workers)\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 88, in __init__\r\n    self._xrd_error(status)\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 108, in _xrd_error\r\n    status.message, self._file_path\r\nOSError: XRootD error: [ERROR] Operation expired\r\nin file root://eoslhcb.cern.ch//eos/lhcb/grid/prod/lhcb/LHCb/Collision16/PIDCALIB.ROOT/00111823/0000/00111823_00000057_1.pidcalib.root\r\n```\r\n\r\nIt is not a problem with expired credentials, etc. - just rerunning the script will cause it to fail on a different file or not at all.\r\n\r\nThe issue occurs on multiple computers with very different setups.\r\n\r\nuproot 4.0.7\r\nxrootd 5.1.1",
  "closed_at":"2021-05-07T06:56:46Z",
  "comments":4,
  "created_at":"2021-05-05T14:08:33Z",
  "id":876491833,
  "labels":null,
  "locked":true,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4NzY0OTE4MzM=",
  "number":351,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"How to handle XRootD error: [ERROR] Operation expired",
  "updated_at":"2021-05-07T15:00:01Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-05-05T20:45:35Z",
  "comments":2,
  "created_at":"2021-05-05T20:35:08Z",
  "draft":false,
  "id":876812888,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjMwODgzNzg4",
  "number":352,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-05T20:45:35Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Make TH* and TGraph* models as a step toward making them writable.",
  "updated_at":"2021-05-17T18:06:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I'm fairly sure this isn't possible given the way TBranches are processed, but I figured I'd ask anyway as this is a big performance hit for me at the moment given our data structure. Sorry in advance if I don't explain what I mean clearly or if I've missed a solution in the docs.\r\n\r\nFor branches whose elements are arrays it seems it's currently impossible to select an individual element of that array at the ``arrays()`` level. \r\n\r\nAs an example, for:\r\n``vertex[4]     | float[4]                 | AsDtype(\"('>f4', (4,))\")``\r\n\r\nIf I only want the 0th ``vertex`` value, I currently have to do something like:\r\n```\r\n[0] f[\"foo\"].arrays(\"vertex[4]\", library=\"pd\")\r\n       vertex[4][0]  vertex[4][1]  vertex[4][2]  vertex[4][3]\r\n0       2168.348682    432.486840  -4035.777621   2454.722301\r\n[1] f[\"foo\"].arrays([\"vertex[4]\", library=\"pd\")[\"vertex[4][0]\"]\r\n0       2168.348682\r\n```\r\n\r\nwhich of course doesn't skip the (relatively slow) process of reading the values in and putting them into the output container. I assume there's no way around this as the entire array has to be read in when reading the branch, I was just wondering if there is any internal change to speed this up (or if there's a quicker approach).",
  "closed_at":"2021-05-08T00:10:59Z",
  "comments":6,
  "created_at":"2021-05-07T15:21:51Z",
  "id":879203482,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4NzkyMDM0ODI=",
  "number":356,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Only read single element of array in each element of TBranch to container.",
  "updated_at":"2021-05-08T00:10:59Z",
  "user":"MDQ6VXNlcjE1Njk3NjY3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.4b2 \u2192 21.5b1](https://github.com/psf/black/compare/21.4b2...21.5b1)\n- [github.com/PyCQA/flake8: 3.9.1 \u2192 3.9.2](https://github.com/PyCQA/flake8/compare/3.9.1...3.9.2)\n",
  "closed_at":"2021-05-10T20:29:25Z",
  "comments":0,
  "created_at":"2021-05-10T20:11:15Z",
  "draft":false,
  "id":885012917,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjM4MzU3MjI2",
  "number":357,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-10T20:29:25Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-05-10T20:29:26Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Now that there are versions assigned for the two XRootD workarounds I think it would be better to apply them conditionally. There are a few ways of doing this but I think using `pkg_resources` is the simplest while maintaining support for Python 2.",
  "closed_at":"2021-05-21T10:26:47Z",
  "comments":12,
  "created_at":"2021-05-13T06:17:10Z",
  "draft":false,
  "id":890751151,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjQzNzIzNzcy",
  "number":358,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-21T10:26:47Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Conditionally apply XRootD workarounds based on it's version",
  "updated_at":"2021-05-21T10:30:55Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I am trying to read a `TMatrixTSym` using uproot version 4 but the `all_members` attribute gives an empty dictionary. I have checked the root file and it is not empty. ",
  "closed_at":null,
  "comments":17,
  "created_at":"2021-05-13T12:23:17Z",
  "id":890995115,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4OTA5OTUxMTU=",
  "number":359,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Database of built-in streamers? (including TMatrixTSym)",
  "updated_at":"2024-01-30T16:13:19Z",
  "user":"MDQ6VXNlcjU2Njg5NDM4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"On the [\"Reading on demand with lazy arrays\"](https://uproot.readthedocs.io/en/latest/basic.html#reading-on-demand-with-lazy-arrays) site, there are two apparent ways of using `lazy`:\r\n\r\n# events.lazy\r\n\r\nThe first code snippet says that a file can be lazy-loaded using:\r\n````python\r\narray = events.lazy([\"dir1/*.root:events\", \"dir2/*.root:events\"])\r\n````\r\nand array should be of type `<Array>`, but it doesn't say what `events` is. I guess that section is assuming the reader has gone through all the previous sections and he knows that it is defined (under [\"Finding objects in a file\"](https://uproot.readthedocs.io/en/latest/basic.html#finding-objects-in-a-file) ) as:\r\n\r\n````python\r\nevents = uproot.open(\"https://scikit-hep.org/uproot3/examples/Zmumu.root:events\")\r\n````\r\nso `events` is of type `<TTree 'events' (20 branches) at 0x78e575394b20>`. I tried that approach with my own root file and it didn't work:\r\n\r\n````python\r\nevents = uproot.open(\"./radospet_1April21_taupair_b1s0_clean.root:tau3x1\")\r\nprint(events.lazy)\r\n\"\"\"\r\nAttributeError: 'Model_TTree_v20' object has no attribute 'lazy'\r\n\"\"\"\r\n````\r\nI guess that's what I was expecting as it wouldn't make sense to use a `TTree` to open a file, but well, that's what the docs say. Then I tried the second approach described later in that same section:\r\n\r\n# uproot.lazy\r\nNow this makes sense as I want to use the module `uproot` to lazy-open a file! The file seems to load correctly, but my array doesn't have the `cache` attribute:\r\n\r\n````python\r\nf = uproot.lazy(\"./radospet_1April21_taupair_b1s0_clean.root:tau3x1\", step_size=100)\r\nprint(f.cache)\r\n\"\"\"\r\nAttributeError: no field named 'cache'\r\n\"\"\"\r\n````\r\nAnd basically that's all the docs say about `lazy`, so I have no idea how to use it or if I should trust the rest of the document.\r\n\r\nI would like to suggest adding full examples to the repository (and/or the docs) and running them as part of the unit tests to make sure everything is working as expected.\r\n\r\nI am using uproot 4.0.7.",
  "closed_at":"2021-05-17T20:36:35Z",
  "comments":3,
  "created_at":"2021-05-14T04:05:34Z",
  "id":891578639,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU4OTE1Nzg2Mzk=",
  "number":360,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"lazy not working as documented",
  "updated_at":"2021-05-17T20:36:35Z",
  "user":"MDQ6VXNlcjc4ODk3MjY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @peguerosdc as a contributor for doc, example, test, tutorial, example, example, test, doc, doc, test, example, tutorial, doc, test, doc, doc, example, doc, doc, test.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot4/issues/360#issuecomment-841252997)",
  "closed_at":"2021-05-20T17:59:29Z",
  "comments":0,
  "created_at":"2021-05-14T13:43:29Z",
  "draft":false,
  "id":891934479,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjQ0NzIyODU2",
  "number":361,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-20T17:59:28Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add peguerosdc as a contributor",
  "updated_at":"2021-05-20T17:59:29Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/pre-commit/pre-commit-hooks: v3.4.0 \u2192 v4.0.1](https://github.com/pre-commit/pre-commit-hooks/compare/v3.4.0...v4.0.1)\n",
  "closed_at":"2021-05-17T17:29:42Z",
  "comments":0,
  "created_at":"2021-05-17T17:15:56Z",
  "draft":false,
  "id":893538663,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjQ2MDM2NjY2",
  "number":362,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-17T17:29:42Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-05-17T17:29:43Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This addresses the issue raised in https://github.com/scikit-hep/uproot4/pull/352#issuecomment-841758979\r\n\r\nI had forgotten that the mechanism that discovers behavioral mix-ins and applies them to classes only does so if the models are generated from streamers. If they're explicitly defined, then I can make the inheritance manually and statically. (It's more flexible and less magical, but I have to remember to do it!)\r\n\r\n@matthewfeickert, can you let me know if this fixes your CI?",
  "closed_at":"2021-05-17T18:05:52Z",
  "comments":2,
  "created_at":"2021-05-17T17:28:09Z",
  "draft":false,
  "id":893547100,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjQ2MDQzNzM0",
  "number":363,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-17T18:05:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Explicit models need explicit behaviors.",
  "updated_at":"2021-05-17T18:05:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Following our guidelines.\r\n\r\n@jpivarski  We should set up a scikit-hep token to upload just this package instead of using a username, which is not as safe. I can make this change (I'll need to remove the username and add the generated token as the password).",
  "closed_at":"2021-05-21T21:37:25Z",
  "comments":0,
  "created_at":"2021-05-20T12:48:56Z",
  "draft":false,
  "id":896782407,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjQ4ODUzMDYx",
  "number":366,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-21T21:37:25Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"ci: use guidelines for deploy",
  "updated_at":"2021-05-21T22:32:25Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I\u2019m encountering a strange behaviour with the zip functionality of awkward arrays.\r\nIt seems that one of the field I am trying to zip is causing a failure.\r\nThe following small test\r\n```\r\nimport uproot\r\nrfile = uproot.open('data/user.qbuat.Q_tests_pu.425200.Pythia8EvtGen_A14NNPDF23LO_Gammatautau_MassWeight_v1_output.root/user.qbuat.25693926._000001.output.root')\r\ntree = rfile['CollectionTree']\r\nfields_1 = [\r\n    'TauTracksAuxDyn.trackPt',\r\n    'TauTracksAuxDyn.trackEta',\r\n]\r\nfields_2 = [\r\n    'TauTracksAuxDyn.trackPt',\r\n    'TauTracksAuxDyn.trackEta',\r\n    'TauTracksAuxDyn.IsPileupTrackQ',\r\n]\r\narr_1 = tree.arrays(fields_1, how='zip')\r\nprint (arr_1.TauTracksAuxDyn.fields)\r\narr_2 = tree.arrays(fields_2, how='zip')\r\nprint (arr_2.TauTracksAuxDyn.fields)\r\n```\r\nreturns:\r\n```\r\n['trackPt', 'trackEta']\r\n['IsPileupTrackQ']\r\n```\r\nwhile the second printout should return the 3 fields\r\n```\r\n['trackPt', 'trackEta', 'IsPileupTrackQ']\r\n```\r\nI have looked at the root file directly but I cannot find anything suspicious with this branch. The root file is available on CERNBox: /eos/user/q/qbuat/uproot_debug/user.qbuat.25693926._000001.output.root and it is about 700MB.\r\n\r\nBest,\r\nQuentin\r\n\r\n",
  "closed_at":"2021-05-25T18:33:06Z",
  "comments":4,
  "created_at":"2021-05-25T17:48:29Z",
  "id":901152954,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5MDExNTI5NTQ=",
  "number":368,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Problem zipping fields",
  "updated_at":"2021-05-26T13:39:22Z",
  "user":"MDQ6VXNlcjY2ODYxNzk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-05-25T18:33:06Z",
  "comments":0,
  "created_at":"2021-05-25T18:21:35Z",
  "draft":false,
  "id":901188060,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjUyNjcxNDY2",
  "number":369,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-25T18:33:06Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Corner case (issue #368): jagged arrays have the same prefix but not the same number of entries.",
  "updated_at":"2021-05-25T18:33:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I'm using python concurrent futures (ProcessPoolExecutor) with uproot to fill boost histograms from many root files on one of the NERSC Cori supercomputer interactive haswell nodes (64 cores). The code i'm using run's 64 ProcessPoolExecutor workers and has worked fine running over 1000 root files (ranging from 0.5-3 GB's in size) but when i tried running it over 6000 root files I get a `zlib.error`. I thought maybe there was something up with the root file; checked the file which lead to this error and uproot opened it and pulled out the data fine. On re-running the code again over 6000 files again, this time a different file lead to the `zlib.error`. Is the error due to some i/o limitation, too many files at once?\r\n\r\nI remove all the other code from my analysis function (did the test function below) and still got the `zlib.error`; the traceback says it's coming from trying to pull out the variables from the uproot tree.\r\n\r\nCode snippet:\r\n```python\r\ndef test_function(filename, tree_name):\r\n  with uproot.open(filename) as f:\r\n    variables = ['pulseArea_phd', 'triggerTimeStamp', 'triggerType']\r\n    uproot_tree = f[tree_name]\r\n    arrays = uproot_tree.arrays(variables)\r\n```\r\n\r\nTraceback:\r\n```python\r\nFile \"analysis_script.py\", line 167, in test_function\r\n    arrays = uproot_tree.arrays(variables)\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 1094, in arrays\r\n    arrays, expression_context, branchid_interpretation = _regularize_expressions(\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 3284, in _regularize_expressio\r\nns\r\n    _regularize_expression(\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 3133, in _regularize_expressio\r\nn\r\n    branch.interpretation,\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 2194, in interpretation\r\n    self._interpretation = uproot.interpretation.identify.interpretation_of(\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/interpretation/identify.py\", line 455, in interpretation_o\r\nf\r\n    if branch.streamer is not None:\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/behaviors/TBranch.py\", line 2339, in streamer\r\n    matches = self._file.streamers.get(fParentName)\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/reading.py\", line 857, in streamers\r\n    ) = streamer_key.get_uncompressed_chunk_cursor()\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/reading.py\", line 2411, in get_uncompressed_chunk_cursor\r\n    uncompressed_chunk = uproot.compression.decompress(\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/compression.py\", line 280, in decompress\r\n    uncompressed_bytestring = cls.decompress(data, block_uncompressed_bytes)\r\n  File \"/global/homes/a/asnaylor/.conda/envs/uproot-py38/lib/python3.8/site-packages/uproot/compression.py\", line 105, in decompress\r\n    return zlib.decompress(data)\r\nzlib.error: Error -3 while decompressing data: incorrect data check\r\n```\r\n\r\nVersions:\r\n```python\r\nuproot==4.0.7\r\npython==3.8.8\r\n```\r\n",
  "closed_at":"2021-05-27T09:18:10Z",
  "comments":9,
  "created_at":"2021-05-25T19:36:04Z",
  "id":901267917,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5MDEyNjc5MTc=",
  "number":370,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"zlib.error: Error -3 while decompressing data: incorrect data check (too many root files at once?)",
  "updated_at":"2021-05-27T09:18:10Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"When calling `to_numpy()` with dd True for TH3, the returned edges for x and z axis are identical.\r\nthis is due to a typo in `uproot/behaviors/TH3.py`:\r\n```py\r\n        values = self.values(flow=flow)\r\n        xedges = self.axis(0).edges(flow=flow)\r\n        yedges = self.axis(1).edges(flow=flow)\r\n        zedges = self.axis(2).edges(flow=flow)\r\n        if dd:\r\n            return values, (xedges, yedges, xedges)\r\n        else:\r\n            return values, xedges, yedges, zedges\r\n```\r\n\r\nhere the second `xedges` in the first branch should be `zedges`",
  "closed_at":"2021-06-11T02:43:28Z",
  "comments":1,
  "created_at":"2021-05-27T18:03:48Z",
  "id":904045312,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5MDQwNDUzMTI=",
  "number":371,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"mixed up axes in to_numpy for TH3 ",
  "updated_at":"2021-06-11T02:43:28Z",
  "user":"MDQ6VXNlcjYyODkwMTEx"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-06-11T02:43:28Z",
  "comments":0,
  "created_at":"2021-05-27T18:38:52Z",
  "draft":false,
  "id":904071228,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjU1MjU0MDEz",
  "number":372,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-06-11T02:43:28Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fixed issue #371: TH3 with dd=True.",
  "updated_at":"2021-06-11T02:43:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.5b1 \u2192 21.5b2](https://github.com/psf/black/compare/21.5b1...21.5b2)\n",
  "closed_at":"2021-05-31T17:39:53Z",
  "comments":0,
  "created_at":"2021-05-31T17:18:23Z",
  "draft":false,
  "id":907622870,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjU4NDQxNTY4",
  "number":373,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-05-31T17:39:53Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-05-31T17:39:54Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-06-11T02:42:57Z",
  "comments":0,
  "created_at":"2021-06-09T17:13:25Z",
  "draft":false,
  "id":916430910,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjY2MDk3Nzgx",
  "number":376,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-06-11T02:42:57Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fixes for 2021 tutorials.",
  "updated_at":"2021-06-11T02:42:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hello,\r\n\r\nI am trying to open a root file with uproot in zip mode. I have a series of fields representing various type of particles.\r\nWhen I try to create an awkward array with all the fields at once, it seems that uproot gets confused about two containers and create a 'jagged0' array. At first I thought it was the same problem with my input file as discovered by @jpivarski in #368, but after closer look it seems different.\r\n\r\nWhen I break down my list of fields to load in sub lists and try to create an awkward array for each sub list, the zipping works just fine.\r\n\r\nI've put the root file on cernbox: https://cernbox.cern.ch/index.php/s/ybpXh90vRp8q06Q\r\nA gist to reproduce the issue is available here: https://gist.github.com/qbuat/8ac731f5cff4ceafc9faea129c89c094#file-uproot_debug-py. It is also attached to this issue as txt: [uproot_debug.txt](https://github.com/scikit-hep/uproot4/files/6633966/uproot_debug.txt)\r\n\r\nBest,\r\nQuentin\r\n\r\n```python\r\n$ python -i uproot_debug.py\r\n/Users/quentin/bbtautau_mhh/data/group.phys-hdbs.mc16_13TeV.364128.HIGG4D3.e5307_s3126_r10201_p3978.bbtt_hh_v15-01_CxAOD.root/group.phys-hdbs.21763855._000003.CxAOD.root\r\n['EventInfo___NominalAuxDyn.MCEventWeight', 'EventInfo___NominalAuxDyn.eventNumber', 'EventInfo___NominalAuxDyn.mcChannelNumber', 'jagged0', 'AntiKt4EMPFlowJets_BTagging201903___NominalAuxDyn', 'MET_Reference_AntiKt4EMPFlow___NominalAuxDyn']\r\n['EventInfo___NominalAuxDyn.MCEventWeight', 'EventInfo___NominalAuxDyn.eventNumber', 'EventInfo___NominalAuxDyn.mcChannelNumber']\r\n['TruthParticles___NominalAuxDyn']\r\n['TauJets___NominalAuxDyn']\r\n['AntiKt4EMPFlowJets_BTagging201903___NominalAuxDyn']\r\n['MET_Reference_AntiKt4EMPFlow___NominalAuxDyn']\r\n>>>\r\n```\r\n",
  "closed_at":"2021-06-22T11:29:58Z",
  "comments":9,
  "created_at":"2021-06-10T19:44:32Z",
  "id":917810229,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5MTc4MTAyMjk=",
  "number":377,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Strange zipping behaviour",
  "updated_at":"2021-06-22T11:29:58Z",
  "user":"MDQ6VXNlcjY2ODYxNzk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.5b2 \u2192 21.6b0](https://github.com/psf/black/compare/21.5b2...21.6b0)\n",
  "closed_at":"2021-06-14T21:23:33Z",
  "comments":0,
  "created_at":"2021-06-14T17:20:05Z",
  "draft":false,
  "id":920620075,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjY5NzQxOTkw",
  "number":379,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-06-14T21:23:33Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-06-14T21:23:34Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/PyCQA/isort: 5.8.0 \u2192 5.9.1](https://github.com/PyCQA/isort/compare/5.8.0...5.9.1)\n",
  "closed_at":"2021-06-21T17:36:03Z",
  "comments":0,
  "created_at":"2021-06-21T17:21:27Z",
  "draft":false,
  "id":926435758,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0Njc0NzQwNTgx",
  "number":381,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-06-21T17:36:03Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-06-21T17:36:04Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I have been trying to read a root file containing TVector and TMatrix, the idea being following https://stackoverflow.com/questions/65833805/how-do-i-read-a-tmatrixt-with-uproot-in-python\r\nI tried with the following code:\r\n\r\nimport uproot\r\nfile_u = uproot.open('efficiency.root')\r\nfile_u[file_u.keys()[0]]\r\nThe last line to get the first object in the file, but I get the following error:\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-36-875cb87b77d3> in <module>\r\n----> 1 file_u[file_u.keys()[0]]\r\n\r\n~/.local/lib/python3.9/site-packages/uproot/reading.py in __getitem__(self, where)\r\n   2043 \r\n   2044         else:\r\n-> 2045             return self.key(where).get()\r\n   2046 \r\n   2047     @property\r\n\r\n~/.local/lib/python3.9/site-packages/uproot/reading.py in get(self)\r\n   2434 \r\n   2435             try:\r\n-> 2436                 out = cls.read(chunk, cursor, context, self._file, selffile, parent)\r\n   2437 \r\n   2438             except uproot.deserialization.DeserializationError:\r\n\r\n~/.local/lib/python3.9/site-packages/uproot/model.py in read(cls, chunk, cursor, context, file, selffile, parent, concrete)\r\n   1194 \r\n   1195         elif version is not None:\r\n-> 1196             versioned_cls = cls.new_class(file, version)\r\n   1197 \r\n   1198         elif context.get(\"in_TBranch\", False):\r\n\r\n~/.local/lib/python3.9/site-packages/uproot/model.py in new_class(cls, file, version)\r\n   1130 \r\n   1131         if streamer is not None:\r\n-> 1132             versioned_cls = streamer.new_class(file)\r\n   1133             versioned_cls.class_streamer = streamer\r\n   1134             cls.known_versions[streamer.class_version] = versioned_cls\r\n\r\n~/.local/lib/python3.9/site-packages/uproot/streamers.py in new_class(self, file)\r\n    370         class_name = uproot.model.classname_encode(self.name, self.class_version)\r\n    371         classes = uproot.model.maybe_custom_classes(file.custom_classes)\r\n--> 372         return uproot.deserialization.compile_class(\r\n    373             file, classes, class_code, class_name\r\n    374         )\r\n\r\n~/.local/lib/python3.9/site-packages/uproot/deserialization.py in compile_class(file, classes, class_code, class_name)\r\n     78     setattr(uproot.dynamic, out.__name__, out)\r\n     79 \r\n---> 80     behaviors = tuple(_yield_all_behaviors(out, c))\r\n     81     exclude = tuple(\r\n     82         bad for cls in behaviors if hasattr(cls, \"no_inherit\") for bad in cls.no_inherit\r\n\r\n~/.local/lib/python3.9/site-packages/uproot/deserialization.py in _yield_all_behaviors(cls, c)\r\n     32 \r\n     33 def _yield_all_behaviors(cls, c):\r\n---> 34     behavior_cls = uproot.behavior_of(uproot.model.classname_decode(cls.__name__)[0])\r\n     35     if behavior_cls is not None:\r\n     36         yield behavior_cls\r\n\r\n~/.local/lib/python3.9/site-packages/uproot/__init__.py in behavior_of(classname)\r\n    227         return globals().get(name)\r\n    228     else:\r\n--> 229         return globals().get(name)(specialization)\r\n    230 \r\n    231 \r\n\r\nTypeError: 'NoneType' object is not callable\r\nI run uproot version '4.0.10' and Python 3.9.5\r\nIf I open the same file with pyROOT I can read the TMatrix content, and it is shown as <cppyy.gbl.TMatrixT<double> object at 0x557c36963df0>",
  "closed_at":"2021-06-30T16:33:26Z",
  "comments":7,
  "created_at":"2021-06-29T16:22:57Z",
  "id":932892955,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5MzI4OTI5NTU=",
  "number":383,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Error in reading ROOT files containing TMatrix and TVector",
  "updated_at":"2021-06-30T18:22:05Z",
  "user":"MDQ6VXNlcjEzNDQ4MTM0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-06-30T16:33:26Z",
  "comments":0,
  "created_at":"2021-06-29T17:50:22Z",
  "draft":false,
  "id":932961882,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjgwMjQyNzg2",
  "number":384,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-06-30T16:33:26Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fixed #383 and moved behavior_of to its own module.",
  "updated_at":"2021-06-30T16:33:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Thank you for submitting an issue! (I know that it takes time and effort to do so.)\r\n\r\nNote that we'll be closing the issue as soon as a solution is proposed. This is not meant to be unfriendly; it's for our own bookkeeping. If you think the first answer/solution is unsatisfactory, please do continue the thread and we'll reopen it or otherwise address it.\r\n\r\nPlease attach a small ROOT file that reproduces the issue! If small and public, you can drag-and-drop it into the issue\u2014rename the extension to \"txt\" so that GitHub allows it. If large, you can put it on some large-file service (e.g. Dropbox). In general, we can't access XRootD URLs (most are not public).\r\n\r\nInclude the version number (and update if necessary, if you're not using the [latest version](https://pypi.org/project/uproot/)).\r\n\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n```\r\n",
  "closed_at":"2021-07-05T09:12:56Z",
  "comments":0,
  "created_at":"2021-07-05T09:12:49Z",
  "id":936874087,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5MzY4NzQwODc=",
  "number":386,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"version",
  "updated_at":"2021-07-05T09:12:56Z",
  "user":"MDQ6VXNlcjEyMTQzMDEw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"uproot 4.0.11 produces a warning on every import:\r\n```\r\n[...]/reading.py:186: FutureWarning: XRootD 5.1.1 is not fully supported; either upgrade to 5.2.0+ or set\r\n\r\n    open.defaults[\"xrootd_handler\"] = uproot.MultithreadedXRootDSource\r\n\r\n  warnings.warn(message, FutureWarning)\r\n```\r\n\r\nI am not using XRootD, so this warning is meaningless to me. Please only raise this warning when a user tries to actually use xrootd.",
  "closed_at":"2021-07-07T12:59:31Z",
  "comments":1,
  "created_at":"2021-07-06T08:41:37Z",
  "id":937659110,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5Mzc2NTkxMTA=",
  "number":387,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"closed",
  "state_reason":"completed",
  "title":"Superfluous \"FutureWarning: XRootD 5.1.1 is not fully supported\" when user does not use XRootD",
  "updated_at":"2021-07-07T12:59:31Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-07-07T12:59:31Z",
  "comments":2,
  "created_at":"2021-07-06T12:38:37Z",
  "draft":false,
  "id":937853902,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0Njg0MzQwNDA5",
  "number":388,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-07-07T12:59:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Avoid warning when XRootD is installed but not used.",
  "updated_at":"2021-07-07T12:59:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/PyCQA/isort: 5.9.1 \u2192 5.9.2](https://github.com/PyCQA/isort/compare/5.9.1...5.9.2)\n",
  "closed_at":"2021-07-12T23:26:46Z",
  "comments":0,
  "created_at":"2021-07-12T23:01:59Z",
  "draft":false,
  "id":942520608,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0Njg4MjQwMTQy",
  "number":393,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-07-12T23:26:46Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-07-12T23:26:46Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"With Uproot 4.0.11 and a self-built XRootD with Python bindings, `import uproot` fails with `TypeError: '<' not supported between instances of 'str' and 'int'`. This is due to the following line: https://github.com/scikit-hep/uproot4/blob/d6f9bea0f1a9ca6806445b95da93efa37c5117ba/src/uproot/extras.py#L116\r\n\r\nWhen one builds XRootD, the version number differs from the standard `x.y.z` - it is, e.g., `v20210712-58b374f12`, which causes `LooseVersion` to raise `TypeError`. ",
  "closed_at":"2021-07-14T15:54:26Z",
  "comments":0,
  "created_at":"2021-07-14T11:03:07Z",
  "id":944307031,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5NDQzMDcwMzE=",
  "number":394,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Uproot fails with self-built XRootD",
  "updated_at":"2021-07-14T15:54:26Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"When one builds XRootD, the version number differs from the standard `x.y.z` - it is, e.g., `v20210712-58b374f12`, which causes `LooseVersion` to raise `TypeError`.\r\n\r\nFixes #394",
  "closed_at":"2021-07-14T15:54:26Z",
  "comments":3,
  "created_at":"2021-07-14T11:07:23Z",
  "draft":false,
  "id":944310698,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0Njg5ODExOTY3",
  "number":395,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-07-14T15:54:26Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Treat self-built XRootD version strings",
  "updated_at":"2021-07-14T15:54:26Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"This test is missing the pytest network mark despite trying to access a webserver at cern.ch",
  "closed_at":"2021-07-17T02:31:42Z",
  "comments":3,
  "created_at":"2021-07-16T19:50:12Z",
  "draft":false,
  "id":946556951,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjkxNzA5Nzgz",
  "number":396,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-07-17T02:31:42Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"mark test 0220 with network tag",
  "updated_at":"2024-01-18T16:58:22Z",
  "user":"MDQ6VXNlcjQ2MjM1MDQ="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.6b0 \u2192 21.7b0](https://github.com/psf/black/compare/21.6b0...21.7b0)\n",
  "closed_at":"2021-07-19T23:24:32Z",
  "comments":0,
  "created_at":"2021-07-19T21:42:10Z",
  "draft":false,
  "id":948051105,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NjkyOTQ0NDQz",
  "number":397,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-07-19T23:24:32Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-07-19T23:24:32Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi, \r\n\r\nI've tried to use uproot to read a tree created as a POD structure\r\n```\r\nimport pandas as pd\r\nimport uproot\r\nuproot.default_library=\"pd\"\r\ndf=uproot.open(\"test.root\")['orange']\r\ndf.show(interpretation_width=1500)\r\nprint( df.keys() )\r\narr = df.arrays(library=\"pd\")\r\ndf_uproot = pd.DataFrame(arr)[vars_to_save]\r\nprint((df_uproot))\r\n\r\n\r\n```\r\nBut there are some problems:\r\n```\r\npython3 ../test/1.py\r\nname                 | typename                 | interpretation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \r\n---------------------+--------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\norange               | struct {int32_t Evtak... | AsDtype(\"[('Evtake_iwant', '>i4'), ('Mc_x', '>f4'), ('Mc_y', '>f4'), ('Mc_q2', '>f4'), ('Mc_x_cr', '>f4'), ('Mc_q2_cr', '>f4'), ('Sierror', '>i4'), ('Sincand', '>i4'), ('Siprob', '>f4'), ('Sipos', '>f4'), ('Sipt', '>f4'), ('Sidca', '>f4'), ('Sitrkp', '>f4'), ('Siein', '>f4'), ('Sienin', '>f4'), ('Zvtx', '>f4'), ('Sith', '>f4'), ('Siecorr', '>f4'), ('Sizuhmom', '>f4'), ('Sicchmom', '>f4'), ('Siccempz', '>f4'), ('Sixel', '>f4'), ('Siq2el', '>f4'), ('Siyel', '>f4'), ('Sixjb', '>f4'), ('Siq2jb', '>f4'), ('Siyjb', '>f4'), ('Sixda', '>f4'), ('Siq2da', '>f4'), ('Siyda', '>f4')]\")\r\n['orange']\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/site-packages/uproot/interpretation/numerical.py\", line 328, in basket_array\r\n    output = data.view(dtype).reshape((-1,) + shape)\r\nValueError: When changing to a larger dtype, its size must be a divisor of the total size in bytes of the last axis of the array.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"../test/1.py\", line 7, in <module>\r\n    arr = df.arrays(library=\"pd\")\r\n  File \"/usr/lib/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 1130, in arrays\r\n    False,\r\n  File \"/usr/lib/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3479, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/usr/lib/python3.6/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/usr/lib/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3430, in basket_to_array\r\n    library,\r\n  File \"/usr/lib/python3.6/site-packages/uproot/interpretation/numerical.py\", line 338, in basket_array\r\n    branch.file.file_path,\r\nValueError: basket 0 in tree/branch /orange;1:orange has the wrong number of bytes (11360) for interpretation AsDtype(\"[('Evtake_iwant', '>i4'), ('Mc_x', '>f4'), ('Mc_y', '>f4'), ('Mc_q2', '>f4'), ('Mc_x_cr', '>f4'), ('Mc_q2_cr', '>f4'), ('Sierror', '>i4'), ('Sincand', '>i4'), ('Siprob', '>f4'), ('Sipos', '>f4'), ('Sipt', '>f4'), ('Sidca', '>f4'), ('Sitrkp', '>f4'), ('Siein', '>f4'), ('Sienin', '>f4'), ('Zvtx', '>f4'), ('Sith', '>f4'), ('Siecorr', '>f4'), ('Sizuhmom', '>f4'), ('Sicchmom', '>f4'), ('Siccempz', '>f4'), ('Sixel', '>f4'), ('Siq2el', '>f4'), ('Siyel', '>f4'), ('Sixjb', '>f4'), ('Siq2jb', '>f4'), ('Siyjb', '>f4'), ('Sixda', '>f4'), ('Siq2da', '>f4'), ('Siyda', '>f4')]\")\r\nin file test.root\r\n```\r\n\r\nuproot version 4.0.11, with python on stock CentOS8.\r\n\r\nBest regards,\r\n\r\nAndrii\r\n[test.root.txt](https://github.com/scikit-hep/uproot4/files/6891401/test.root.txt)\r\n\r\n",
  "closed_at":"2021-07-28T19:07:41Z",
  "comments":4,
  "created_at":"2021-07-28T08:20:57Z",
  "id":954604039,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5NTQ2MDQwMzk=",
  "number":398,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Reading POD structure",
  "updated_at":"2021-07-28T21:17:59Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-07-28T19:07:41Z",
  "comments":0,
  "created_at":"2021-07-28T18:42:35Z",
  "draft":false,
  "id":955138568,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0Njk4OTMxODQz",
  "number":399,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-07-28T19:07:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Handle dimensions in a leaf-list.",
  "updated_at":"2021-07-28T19:07:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Dear @jpivarski ,\r\n\r\nThere is another interesting class of POD structures very often used with ROOT. Those with variable length of the fields.\r\n I've created an example with the following code:\r\n```\r\n#include<string>\r\n#include \"TFile.h\"\r\n#include \"TTree.h\"\r\nconst size_t max_tracks=200;\r\nstruct TOut\r\n{\r\n\r\n    Int_t Evtake_iwant;\r\n    Float_t Mc_x;\r\n    Float_t Mc_y;\r\n    Int_t Trk_ntracks;\r\n    Float_t Trk_px[max_tracks];\r\n    Float_t Trk_py[max_tracks];\r\n    Float_t Mc_q2;\r\n};\r\n\r\nint main(int argc,char** argv) {\r\nTFile* output_file = new TFile(\"output.root\",\"recreate\");\r\noutput_file->cd();\r\nTTree* output_tree = new TTree(\"orange\",\"orange\");\r\nTOut output_struct;\r\nstd::string struct_decription = \"Evtake_iwant/I:Mc_x/F:Mc_y/F:Trk_ntracks/I:Trk_px[Trk_ntracks]/F:Trk_py[Trk_ntracks]/F:Mc_q2/F\";\r\noutput_tree->Branch(\"orange\", &output_struct, struct_decription.c_str());\r\n\r\nfor (size_t i=0;i<100;i++) {\r\n  output_struct.Trk_ntracks=i%(max_tracks-10)+10;\r\n  output_struct.Evtake_iwant=i;\r\n  output_struct.Mc_x=i+5.0;\r\n  output_struct.Mc_y=-i;\r\n  output_struct.Mc_q2=10000*i;\r\n  for (int j=0;j< output_struct.Trk_ntracks;j++) {\r\n    output_struct.Trk_px[j]=10+0.1*j;\r\n    output_struct.Trk_py[j]=-10-0.1*j;\r\n  }\r\n  output_tree->Fill();\r\n}\r\noutput_tree->Write();\r\noutput_file->Close();\r\nreturn 0;\r\n}\r\n```\r\nAnd attempted to read the output using the master branch of uproot:\r\n```\r\nimport pandas as pd\r\nimport uproot\r\nuproot.default_library=\"pd\"\r\ndf=uproot.open(\"output.root\")['orange']\r\ndf.show(interpretation_width=1500)\r\nprint( df.keys() )\r\narr = df.arrays(library=\"pd\")\r\ndf_uproot = pd.DataFrame(arr)[vars_to_save]\r\nprint((df_uproot))\r\n```\r\n\r\nThe corresponding  error was\r\n```\r\nkabel@Aloiss-Air uproot4 % python3 1.py                 \r\nname                 | typename                 | interpretation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \r\n---------------------+--------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/kabel/uproot4/1.py\", line 5, in <module>\r\n    df.show(interpretation_width=1500)\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 953, in show\r\n    typename = branch.typename\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 2211, in typename\r\n    if self.interpretation is None:\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 2195, in interpretation\r\n    self._interpretation = uproot.interpretation.identify.interpretation_of(\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/interpretation/identify.py\", line 405, in interpretation_of\r\n    _leaf_to_dtype(leaf, getdims=True).newbyteorder(\">\"),\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/interpretation/identify.py\", line 69, in _leaf_to_dtype\r\n    dims = tuple(eval(m.group(2).replace(\"][\", \", \")))\r\n  File \"<string>\", line 1, in <module>\r\nNameError: name 'Trk_ntracks' is not defined\r\nkabel@Aloiss-Air uproot4 % root -l\r\n```\r\nThe ROOT scriplett\r\n```\r\nTChain* C = new TChain(\"orange\");\r\nC->Add(\"output.root\");\r\nC->MakeClass(\"rootorange\");\r\n\r\n```\r\nwas able to deduce the structure of the tree as \r\n\r\n```\r\n\r\n   // Declaration of leaf types\r\n   Int_t           orange_Evtake_iwant;\r\n   Float_t         orange_Mc_x;\r\n   Float_t         orange_Mc_y;\r\n   Int_t           orange_Trk_ntracks;\r\n   Float_t         orange_Trk_px[109];   //[Trk_ntracks]\r\n   Float_t         orange_Trk_py[109];   //[Trk_ntracks]\r\n   Float_t         orange_Mc_q2;\r\n\r\n\r\n```\r\n\r\n\r\nBest regards,\r\n\r\nAndrii\r\n\r\nP.S. I'm adding the file I've created despite one can recreate it.\r\n[output.root.txt](https://github.com/scikit-hep/uproot4/files/6906837/output.root.txt)\r\n",
  "closed_at":null,
  "comments":5,
  "created_at":"2021-07-30T10:18:54Z",
  "id":956612399,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5NTY2MTIzOTk=",
  "number":400,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"open",
  "state_reason":null,
  "title":"Reading advanced POD",
  "updated_at":"2021-08-02T08:46:11Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi @jpivarski ,\r\n\r\nWould you consider to push the uproot4 to some large Linux distributives, e.g. RedHat EPEL + Fedora?\r\nThis would bring some advantages for the users, for instance one could have coherent installations in real systems and in containers with automatic updates. For the developers it could be interesting, as the massive rebuilds in Fedora and EPEL always test the latest versions of the software and these detect problems well ahead of time when typical users see them.\r\nIf you are interested, I would be happy to donate the .spec files I have for uproot \r\nhttps://github.com/andriish/HEPrpms/blob/master/python-uproot4/4.0.11/python-uproot4.spec\r\n\r\n\r\nBest regards,\r\n\r\nAndrii",
  "closed_at":"2021-08-01T13:31:37Z",
  "comments":6,
  "created_at":"2021-07-30T10:49:12Z",
  "id":956634094,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5NTY2MzQwOTQ=",
  "number":401,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot4 builds in EPEL/Fedora/etc",
  "updated_at":"2021-08-01T13:31:37Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"`uproot4.lazy` fails to read branches of type `std::vector<TLorentzVector>` that `uproot4.open` and `uproot4.concatenate` read successfully.\r\n\r\nI am using uproot version `4.0.11`, ROOT `6.20/00`.\r\n\r\nThe following script reproduces the problem:\r\n\r\n```python\r\nimport ROOT\r\nimport uproot4\r\nfrom importlib.metadata import version\r\n\r\n# some C++ code that generates an example file\r\nROOT.gInterpreter.GenerateDictionary(\"std::vector<TLorentzVector>\", \"vector;TLorentzVector.h\")\r\nROOT.gInterpreter.ProcessLine(\"\"\"\r\n#include <vector>\r\n#include \"TLorentzVector.h\"\r\n\r\nvoid write_file() {\r\n    TFile tfile(\"example.root\", \"RECREATE\");\r\n    TTree tree(\"tree\", \"tree\");\r\n    std::vector<TLorentzVector> fourMomentum;\r\n    tree.Branch(\"fourMomentum\", &fourMomentum);\r\n    fourMomentum.push_back({1.0, 2.0, 3.0, 4.0});\r\n    tree.Fill();\r\n    tree.AutoSave();\r\n    return;\r\n}\r\n\"\"\")\r\n\r\nif __name__ == \"__main__\":\r\n    ROOT.write_file()\r\n\r\n    # check versions\r\n    print(version(\"uproot\")) # prints: 4.0.11\r\n    print(ROOT.gROOT.GetVersion()) # prints: 6.20/00\r\n\r\n    # read with uproot\r\n    tree = uproot4.open(\"example.root:tree\")\r\n    print(tree.arrays(library=\"ak\").fourMomentum)\r\n    # prints: [[{fP: {fX: 1, fY: 2, fZ: 3}, fE: 4}]]\r\n\r\n    # read with uproot concatenate\r\n    tree = uproot4.concatenate([\"example.root:tree\"], library=\"ak\")\r\n    print(tree.fourMomentum) \r\n    # prints: [[{fP: {fX: 1, fY: 2, fZ: 3}, fE: 4}]]\r\n\r\n    # read lazily with uproot\r\n    tree = uproot4.lazy([\"example.root:tree\"], library=\"ak\")\r\n    print(tree.fourMomentum) # raises ValueError\r\n```\r\n\r\nThis produces the error message:\r\n\r\n```\r\n/home/epp/phskaj/work/t2k/ncfgd7/ncfgd/env/ncfgd/lib/python3.8/site-packages/uproot/reading.py:186: FutureWarning: XRootD 4.11.3 is not fully supported; either upgrade to 5.2.0+ or set\r\n\r\n    open.defaults[\"xrootd_handler\"] = uproot.MultithreadedXRootDSource\r\n\r\n  warnings.warn(message, FutureWarning)\r\nTTree::Bronch:0: RuntimeWarning: Using split mode on a class: TLorentzVector with a custom Streamer\r\n4.0.11\r\n6.20/00\r\n[[{fP: {fX: 1, fY: 2, fZ: 3}, fE: 4}]]\r\n[[{fP: {fX: 1, fY: 2, fZ: 3}, fE: 4}]]\r\nTraceback (most recent call last):\r\n  File \"possible_bug_in_uproot.py\", line 42, in <module>\r\n    print(tree.fourMomentum) # raises ValueError\r\n  File \"/home/epp/phskaj/work/t2k/ncfgd7/ncfgd/env/ncfgd/lib/python3.8/site-packages/awkward/highlevel.py\", line 1278, in __str__\r\n    return self._str()\r\n  File \"/home/epp/phskaj/work/t2k/ncfgd7/ncfgd/env/ncfgd/lib/python3.8/site-packages/awkward/highlevel.py\", line 1298, in _str\r\n    return ak._util.minimally_touching_string(\r\n  File \"/home/epp/phskaj/work/t2k/ncfgd7/ncfgd/env/ncfgd/lib/python3.8/site-packages/awkward/_util.py\", line 1567, in minimally_touching_string\r\n    rgt = next(rightgen)\r\n  File \"/home/epp/phskaj/work/t2k/ncfgd7/ncfgd/env/ncfgd/lib/python3.8/site-packages/awkward/_util.py\", line 1553, in forever\r\n    for token in iterable:\r\n  File \"/home/epp/phskaj/work/t2k/ncfgd7/ncfgd/env/ncfgd/lib/python3.8/site-packages/awkward/_util.py\", line 1509, in backward\r\n    for token in backward(x[i], sp):\r\n  File \"/home/epp/phskaj/work/t2k/ncfgd7/ncfgd/env/ncfgd/lib/python3.8/site-packages/awkward/partition.py\", line 231, in __getitem__\r\n    return PartitionedArray.from_ext(self._ext.getitem_at(where))\r\nValueError: generated array does not conform to expected form:\r\n\r\n{\r\n    \"class\": \"ListOffsetArray64\",\r\n    \"offsets\": \"i64\",\r\n    \"content\": {\r\n        \"class\": \"RecordArray\",\r\n        \"contents\": {\r\n            \"@fBits\": \"uint32\",\r\n            \"@fUniqueID\": \"uint32\",\r\n            \"@instance_version\": \"uint16\",\r\n            \"@num_bytes\": \"uint32\",\r\n            \"@pidf\": \"uint16\",\r\n            \"fE\": \"float64\",\r\n            \"fP\": {\r\n                \"class\": \"RecordArray\",\r\n                \"contents\": {\r\n                    \"@fBits\": \"uint32\",\r\n                    \"@fUniqueID\": \"uint32\",\r\n                    \"@instance_version\": \"uint16\",\r\n                    \"@num_bytes\": \"uint32\",\r\n                    \"@pidf\": \"uint16\",\r\n                    \"fX\": \"float64\",\r\n                    \"fY\": \"float64\",\r\n                    \"fZ\": \"float64\"\r\n                },\r\n                \"parameters\": {\r\n                    \"__record__\": \"TVector3\"\r\n                }\r\n            }\r\n        },\r\n        \"parameters\": {\r\n            \"__record__\": \"TLorentzVector\"\r\n        }\r\n    }\r\n}\r\n\r\nbut generated:\r\n\r\n{\r\n    \"class\": \"ListOffsetArray64\",\r\n    \"offsets\": \"i64\",\r\n    \"content\": {\r\n[example.zip](https://github.com/scikit-hep/uproot4/files/6907761/example.zip)\r\n\r\n        \"class\": \"RecordArray\",\r\n        \"contents\": {\r\n            \"fP\": {\r\n                \"class\": \"RecordArray\",\r\n                \"contents\": {\r\n                    \"fX\": \"float64\",\r\n                    \"fY\": \"float64\",\r\n                    \"fZ\": \"float64\"\r\n                },\r\n                \"parameters\": {\r\n                    \"__record__\": \"TVector3\"\r\n                }\r\n            },\r\n            \"fE\": \"float64\"\r\n        },\r\n        \"parameters\": {\r\n            \"__record__\": \"TLorentzVector\"\r\n        }\r\n    }\r\n}\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.2.3/src/libawkward/virtual/ArrayGenerator.cpp#L46)```\r\n\r\n```\r\n\r\nI have also attached `example.root` to this issue. \r\n\r\n[example.zip](https://github.com/scikit-hep/uproot4/files/6908136/example.zip)\r\n\r\n",
  "closed_at":"2021-07-30T17:40:47Z",
  "comments":5,
  "created_at":"2021-07-30T13:26:03Z",
  "id":956745552,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5NTY3NDU1NTI=",
  "number":402,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot4.lazy fails to read TBranch of type std::vector<TLorentzVector>",
  "updated_at":"2021-07-30T17:40:47Z",
  "user":"MDQ6VXNlcjk4NDUwMzQ="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-07-30T17:40:47Z",
  "comments":0,
  "created_at":"2021-07-30T17:13:31Z",
  "draft":false,
  "id":956930852,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzAwNDUyODEz",
  "number":403,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-07-30T17:40:46Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Remove '@' fields from lazy Forms (fixes lazy TLorentzVector).",
  "updated_at":"2021-07-30T17:40:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/PyCQA/isort: 5.9.2 \u2192 5.9.3](https://github.com/PyCQA/isort/compare/5.9.2...5.9.3)\n",
  "closed_at":"2021-08-06T03:16:39Z",
  "comments":0,
  "created_at":"2021-08-02T17:42:11Z",
  "draft":false,
  "id":958369891,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzAxNjYxNzkx",
  "number":404,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-06T03:16:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-08-06T03:16:39Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"",
  "closed_at":"2021-08-06T03:16:12Z",
  "comments":0,
  "created_at":"2021-08-02T18:26:27Z",
  "draft":false,
  "id":958401282,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzAxNjg5MjIx",
  "number":405,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-06T03:16:12Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Write a histogram.",
  "updated_at":"2021-08-06T03:16:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-08-07T01:27:07Z",
  "comments":0,
  "created_at":"2021-08-06T12:48:35Z",
  "draft":false,
  "id":962706937,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzA1NDU4MTM3",
  "number":406,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-07T01:27:07Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Write a TTree.",
  "updated_at":"2021-08-07T01:27:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":null,
  "closed_at":"2021-08-19T00:04:14Z",
  "comments":6,
  "created_at":"2021-08-07T18:15:43Z",
  "draft":false,
  "id":963278983,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzA1OTI3MzMy",
  "number":407,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-19T00:04:14Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Add support for reading TDatime",
  "updated_at":"2021-08-19T00:04:15Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"I had been committing (and pushing!) the `jpivarski/write-a-ttree` branch without realizing that it was already merged. This branch has that work.",
  "closed_at":"2021-08-10T23:56:38Z",
  "comments":0,
  "created_at":"2021-08-10T18:51:37Z",
  "draft":false,
  "id":965255998,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzA3NTg3OTEw",
  "number":408,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-10T23:56:38Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Write a TTree 2.",
  "updated_at":"2021-08-10T23:56:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"I should try to finish this off today.",
  "closed_at":"2021-08-12T00:49:08Z",
  "comments":0,
  "created_at":"2021-08-11T20:18:30Z",
  "draft":false,
  "id":967390992,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzA5NTY3NjQ3",
  "number":409,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-12T00:49:08Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Write a TTree 3.",
  "updated_at":"2021-08-12T00:49:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":null,
  "closed_at":"2021-08-12T19:26:23Z",
  "comments":1,
  "created_at":"2021-08-12T14:07:57Z",
  "draft":false,
  "id":968900035,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzEwOTg1Mjgy",
  "number":410,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-12T19:26:23Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fix links to source in sphinx",
  "updated_at":"2021-08-12T19:26:23Z",
  "user":"MDQ6VXNlcjEzNjAyNDY4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Tested on files made by https://root.cern/doc/master/float16_8C_source.html with\r\n\r\n```c++\r\n   Float16_t fI14; //[-pi,pi,14] saved as a 14 bit unsigned int\r\n```\r\n\r\nreplacing `14` with `20`, `32`, `33`, `0`, `1`, and `2`. Values outside of the legal range, `2 <= num_bits <= 32`, are _left in the file_ and have to be mapped to `32`, even for `Float16_t`.",
  "closed_at":"2021-08-16T17:20:39Z",
  "comments":0,
  "created_at":"2021-08-16T16:45:11Z",
  "draft":false,
  "id":971918919,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzEzNTkwNTE2",
  "number":411,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-16T17:20:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Float16_t with up to 32 bits is allowed (not just 16).",
  "updated_at":"2021-08-16T17:20:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-08-18T00:29:58Z",
  "comments":0,
  "created_at":"2021-08-17T15:40:21Z",
  "draft":false,
  "id":972824386,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzE0MzYzMjMw",
  "number":412,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-18T00:29:58Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Multidimensional NumPy arrays.",
  "updated_at":"2021-08-18T00:29:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":null,
  "closed_at":"2021-08-18T23:12:08Z",
  "comments":4,
  "created_at":"2021-08-18T18:13:07Z",
  "draft":false,
  "id":973933502,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzE1MzExOTMy",
  "number":413,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-18T23:12:08Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"CI: rerun tests on network errors",
  "updated_at":"2021-08-18T23:12:08Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-08-19T18:38:47Z",
  "comments":0,
  "created_at":"2021-08-18T18:18:24Z",
  "draft":false,
  "id":973937497,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzE1MzE1MzMx",
  "number":414,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-19T18:38:47Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Write a jagged array.",
  "updated_at":"2021-08-19T18:38:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Replaced `writing.py` (1944 SLOC) and `_writing.py` (2972 SLOC) with a subdirectory `writing`:\r\n\r\n   * `writing/__init__.py` (39 SLOC): gathers high-level objects\r\n   * `writing/writable.py` (798 SLOC): high-level objects like WritableFile, WritableDirectory, and WritableTree\r\n   * `writing/identify.py` (1155 SLOC): rules for converting Python objects into ROOT objects, including `to_writable`\r\n   * `writing/_cascade.py` (1910 SLOC): low-level objects for \"cascading writes\" (like a reactive framework for I/O)\r\n   * `writing/_cascadetree.py` (1071 SLOC): low-level object for writing TTrees\r\n",
  "closed_at":"2021-08-19T21:04:39Z",
  "comments":1,
  "created_at":"2021-08-19T20:38:08Z",
  "draft":false,
  "id":975030035,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzE2MjQ1Nzcw",
  "number":415,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-19T21:04:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Refactor all of the writing code.",
  "updated_at":"2021-08-19T21:04:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-08-23T22:26:26Z",
  "comments":0,
  "created_at":"2021-08-20T16:46:48Z",
  "draft":false,
  "id":975767739,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzE2ODcwNzcw",
  "number":416,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-23T22:26:26Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Start developing compression for Uproot writing.",
  "updated_at":"2021-08-23T22:26:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"TTable was a class in ROOT, until removed in 6.16. It is an \"alternative\" to TTree implementing a row-based C-structure data storage with schema migration support.",
  "closed_at":"2021-08-30T21:34:01Z",
  "comments":4,
  "created_at":"2021-08-24T05:24:15Z",
  "draft":false,
  "id":977716550,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzE4NDIwNTMx",
  "number":418,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-30T21:34:01Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Add TTable support",
  "updated_at":"2021-08-30T21:34:01Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Mainly motivated by writing files (discussion #321) because people have asked for the ability to write PyROOT objects with Uproot. But the other direction has also been requested: discussion #392.",
  "closed_at":"2021-08-26T00:39:14Z",
  "comments":5,
  "created_at":"2021-08-24T22:36:50Z",
  "draft":false,
  "id":978537663,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzE5MTA5NTM5",
  "number":420,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-26T00:39:13Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Start working on PyROOT/Uproot interoperability.",
  "updated_at":"2021-08-26T18:04:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-08-27T01:04:19Z",
  "comments":1,
  "created_at":"2021-08-26T16:52:29Z",
  "draft":false,
  "id":980454468,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzIwNjg5MDU4",
  "number":421,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-27T01:04:19Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Documenting the writing interface.",
  "updated_at":"2021-08-27T01:29:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-08-27T17:29:56Z",
  "comments":0,
  "created_at":"2021-08-27T15:49:57Z",
  "draft":false,
  "id":981378022,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzIxNDM5NDAz",
  "number":422,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-27T17:29:55Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Integration with the hist library, which completes the file-writing sprint.",
  "updated_at":"2021-08-27T17:29:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I am trying to develop code which will parse a series of branches and use uproot to read those branches. The branches could be passed as a regex pattern or as an exact name or as an expression. \r\n\r\nI am looking to combine all the requested branches into an array and passing them in one go to uproot.  The TBranch argument  ```filter_name``` fails to match the expressions and the expressions list fails to process regex patterns. Even if I group expressions together and filters together,  ```exoressions``` always overrides the ```filter_name``` argument. Is it possible to allow the ```filter_name``` argument to process an expression or for the ```expression```  argument to process regex patterns?\r\n",
  "closed_at":"2024-01-30T16:17:20Z",
  "comments":5,
  "created_at":"2021-08-27T17:19:56Z",
  "id":981444762,
  "labels":null,
  "locked":true,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5ODE0NDQ3NjI=",
  "number":423,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Allowing ```TBranch:: filter_name``` to process expressions or  ```TBranch:: expressions``` to process regex?",
  "updated_at":"2024-01-30T16:17:20Z",
  "user":"MDQ6VXNlcjg5MTQ3NDc4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Thanks, @henryiii!\r\n\r\n(There will be a 4.1.1 soon enough.)",
  "closed_at":"2021-08-27T22:52:43Z",
  "comments":0,
  "created_at":"2021-08-27T22:28:53Z",
  "draft":false,
  "id":981637553,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzIxNjQ2NDUx",
  "number":425,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-27T22:52:43Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"The 'xrootd_handler' key must be set regardless of whether there's a warning about its choice of value.",
  "updated_at":"2021-08-27T22:52:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"After the release, I made a first performance test of writing histograms. Here's the script:\r\n\r\n**scaling-histograms-uproot.py**\r\n\r\n```python\r\nimport time\r\n\r\nimport numpy as np\r\nimport uproot\r\nimport hist\r\n\r\nwith uproot.recreate(\"output.root\", compression=None) as file:\r\n    for nbins in 5, 5000, 10, 2000, 20, 1000, 50, 500, 100, 200:\r\n        starttime = time.time()\r\n\r\n        for i in range(1000):\r\n            h = hist.Hist.new.Reg(nbins, -5, 5).Weight()\r\n            file[\"h_%05d_%05d\" % (nbins, i)] = h\r\n\r\n        print(f\"{nbins = }, time = {(time.time() - starttime)}\")\r\n```\r\n\r\nThe performance was terrible.\r\n\r\n```\r\n% python scaling-histograms-uproot.py \r\nnbins = 5, time = 2.9465341567993164\r\nnbins = 5000, time = 7.75829005241394\r\nnbins = 10, time = 12.314059019088745\r\nnbins = 2000, time = 17.434439659118652\r\nnbins = 20, time = 22.311167001724243\r\nnbins = 1000, time = 27.74405813217163\r\nnbins = 50, time = 33.42725396156311\r\nnbins = 500, time = 38.9810357093811\r\nnbins = 100, time = 45.44138026237488\r\nnbins = 200, time = 51.959704875946045\r\n% rm output.root \r\nrm: remove regular file 'output.root'? y\r\n```\r\n\r\nNotice that the number of bins in each test is not monotonically increasing, but the time to complete is. I thought there might have been something wrong in the cascade-writing (in which a change in state invokes a walk over changed tree nodes, writing the changes exactly once), but that wasn't it. It was purely algorithmic: the DirectoryData maintained a single list of keys, any kind of search iterated through them all, and adding one key meant serializing the whole DirectoryData, which is now larger. Each of these operations is individually _O(n)_, so if you do them _n_ times, as in this test, the total running time is _O(n\u00b2)_. Classic algorithms stuff!\r\n\r\nSo I replaced the list with optimized data structures for searching (_O(1)_ per search, rather than _O(n)_) and I replaced the \"all keys\" serialization with \"only new keys\" if keys have only been appended at the end. Other cases (replacing or deleting a key, which are more rare) still rewrite the whole set of keys. Although I'm optimizing the code for a specific performance test, the problem of \"write a bunch of histograms in a directory\" is a very common use-case, and the extra complexity in the DirectoryData class is worth it.\r\n\r\nParticularly because these are the updated performance results with that one fix:\r\n\r\n```\r\n% python scaling-histograms-uproot.py \r\nnbins = 5, time = 0.6220302581787109\r\nnbins = 5000, time = 0.8085470199584961\r\nnbins = 10, time = 0.6134929656982422\r\nnbins = 2000, time = 0.6801645755767822\r\nnbins = 20, time = 0.6404979228973389\r\nnbins = 1000, time = 0.6516284942626953\r\nnbins = 50, time = 0.6299173831939697\r\nnbins = 500, time = 0.6578629016876221\r\nnbins = 100, time = 0.6180548667907715\r\nnbins = 200, time = 0.6521506309509277\r\n% rm output.root \r\nrm: remove regular file 'output.root'? y\r\n```\r\n\r\nIn the end, after having written 10000 histograms, it's 80\u00d7 faster, and that factor would continue to grow with the number of histograms written.\r\n\r\nIt also compares favorably with PyROOT:\r\n\r\n**scaling-histograms-pyroot.py**\r\n\r\n```python\r\nimport time\r\n\r\nimport ROOT\r\n\r\nfile = ROOT.TFile(\"output.root\", \"RECREATE\")\r\nfile.SetCompressionLevel(0)\r\n\r\nfor nbins in 5, 5000, 10, 2000, 20, 1000, 50, 500, 100, 200:\r\n    starttime = time.time()\r\n\r\n    for i in range(1000):\r\n        h = ROOT.TH1D(\"h_%05d_%05d\" % (nbins, i), \"\", nbins, -5, 5)\r\n        h.Sumw2()\r\n        file.Write()\r\n\r\n    print(f\"{nbins = }, time = {(time.time() - starttime)}\")\r\n```\r\n\r\n```\r\n% python scaling-histograms-pyroot.py \r\nnbins = 5, time = 4.220697402954102\r\nnbins = 5000, time = 5.634121656417847\r\nnbins = 10, time = 5.7256810665130615\r\nnbins = 2000, time = 6.573720455169678\r\nnbins = 20, time = 6.748169183731079\r\nnbins = 1000, time = 6.8290839195251465\r\nnbins = 50, time = 7.278932809829712\r\nnbins = 500, time = 6.5945963859558105\r\nnbins = 100, time = 6.932183027267456\r\nnbins = 200, time = 7.021673679351807\r\n% rm output.root \r\nrm: remove regular file 'output.root'? y\r\n```\r\n\r\nI don't know what's going on in PyROOT, since most of the work should be happening on the C++ side. This also grows with time, rather than with number of bins, so there might be some similar story: a registry that gets filled up with objects to manage or something.\r\n\r\nAnyway, the C++ code is much faster, so most of this is in the Python-C++ bindings:\r\n\r\n**scaling-histograms.cpp**\r\n\r\n```c++\r\n#include <stdio.h>\r\n#include <iostream>\r\n#include <chrono>\r\n\r\n#include \"TFile.h\"\r\n#include \"TH1D.h\"\r\n\r\nint main(int argc, char** argv) {\r\n  auto starttime = std::chrono::high_resolution_clock::now();\r\n\r\n  auto file = new TFile(\"output.root\", \"RECREATE\");\r\n  file->SetCompressionLevel(0);\r\n\r\n  int nbins_array[10] = {5, 5000, 10, 2000, 20, 1000, 50, 500, 100, 200};\r\n  char name[14];\r\n\r\n  for (int j = 0;  j < 10;  j++) {\r\n    int nbins = nbins_array[j];\r\n\r\n    for (int i = 0;  i < 1000;  i++) {\r\n      sprintf(name, \"h_%05d_%05d\", nbins, i);\r\n      auto h = new TH1D(name, \"\", nbins, -5, 5);\r\n      h->Sumw2();\r\n      h->Write();\r\n    }\r\n\r\n    auto stoptime = std::chrono::high_resolution_clock::now();\r\n\r\n    std::cout << \"nbins = \" << nbins << \", \"\r\n              << \"time = \"\r\n              << std::chrono::duration_cast<std::chrono::microseconds>(\r\n                   stoptime - starttime\r\n                 ).count() / 1e6\r\n              << std::endl;\r\n  }\r\n\r\n  file->Close();\r\n}\r\n```\r\n\r\n```\r\n% g++ -O3 scaling-histograms.cpp `root-config --cflags --libs` -o scaling-histograms\r\n% ./scaling-histograms \r\nnbins = 5, time = 0.170805\r\nnbins = 5000, time = 0.301064\r\nnbins = 10, time = 0.312349\r\nnbins = 2000, time = 0.361317\r\nnbins = 20, time = 0.377874\r\nnbins = 1000, time = 0.411764\r\nnbins = 50, time = 0.425719\r\nnbins = 500, time = 0.444574\r\nnbins = 100, time = 0.456827\r\nnbins = 200, time = 0.471508\r\n% rm output.root \r\nrm: remove regular file 'output.root'? y\r\n```\r\n\r\nThere is a time-dependent increase in C++, and that might be a global registry of histogram objects, but the effect here in C++ is much smaller than in PyROOT, so there's something significant in the bindings.\r\n\r\nAnyway, I got to change my code to fit the performance test, so on that subjective level, the comparison isn't fair. But this also shows that the fastest possible time is probably around 0.17 seconds for 1000 histograms, since that's ROOT's writing rate when the _O(n\u00b2)_ effect hasn't kicked in, and I'm just happy that Uproot's 0.65 seconds is something close to that.",
  "closed_at":"2021-08-28T01:51:26Z",
  "comments":0,
  "created_at":"2021-08-28T01:26:30Z",
  "draft":false,
  "id":981690053,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzIxNjg2Njk5",
  "number":426,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-28T01:51:26Z"
  },
  "reactions":{
   "rocket":2,
   "total_count":2
  },
  "state":"closed",
  "state_reason":null,
  "title":"Fixed performance bug in large directories",
  "updated_at":"2021-08-28T01:51:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.7b0 \u2192 21.8b0](https://github.com/psf/black/compare/21.7b0...21.8b0)\n",
  "closed_at":"2021-08-30T18:40:55Z",
  "comments":0,
  "created_at":"2021-08-30T18:16:11Z",
  "draft":false,
  "id":983068485,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzIyNzEwMzk0",
  "number":427,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-30T18:40:55Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-08-30T18:40:56Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Follows up on #426 with similar work for TTrees.",
  "closed_at":"2021-08-30T22:00:36Z",
  "comments":3,
  "created_at":"2021-08-30T19:39:21Z",
  "draft":false,
  "id":983135684,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzIyNzY0MTA4",
  "number":428,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-08-30T22:00:36Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Final tweaks, bug fixes, and performance checks for this stage of Uproot-writing",
  "updated_at":"2022-01-31T22:20:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"(As discussed before on Gitter)\r\nTruncate the file to 0 bytes opened with uproot.recreate before writing contents. This behavior agrees with ROOT Recreate.\r\nThe previous code lines seemed to intend to do this, however assumingly due to an erroneous 'not', this was not done.\r\nIf the file does not exist, an empty file is created. This is the same behavior as before.",
  "closed_at":"2021-09-01T15:05:16Z",
  "comments":1,
  "created_at":"2021-09-01T14:14:28Z",
  "draft":false,
  "id":985209109,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzI0NTQyNjkx",
  "number":431,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-01T15:05:15Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Recreate truncates if file exists",
  "updated_at":"2021-09-01T15:05:16Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-09-01T16:13:02Z",
  "comments":1,
  "created_at":"2021-09-01T14:16:10Z",
  "draft":false,
  "id":985211259,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzI0NTQ0NDQ2",
  "number":432,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-01T16:13:01Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fix #430, global_index for tuples of DataFrames.",
  "updated_at":"2021-09-01T16:13:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"### Discussed in https://github.com/scikit-hep/uproot4/discussions/430\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **MoAly98** September  1, 2021</sup>\r\nAssume I have 3 branches in an input file called ```file``` in a tree ```tree```:\r\n```branch1: type = AsJagged(AsDtype('>f4'))```\r\n```branch2: type = AsJagged(AsDtype('>f4'))```\r\n```branch3: type = AsDtype('>f4')```\r\nMy aim is to read in these 3 branches using uproot and save them into a dataframe. If I run \r\n```\r\nfor batch in uproot.iterate(file:tree, filter_name=[branch1, branch2, branch3], library='pd'):\r\n      print(batch)\r\n```\r\n\r\nI run into the following error from the iterate method call:\r\n```\r\n  File \"/afs/cern.ch/user/m/maly/thbb_env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 226, in iterate\r\n    arrays = library.global_index(item, global_offset)\r\n  File \"/afs/cern.ch/user/m/maly/thbb_env/lib/python3.7/site-packages/uproot/interpretation/library.py\", line 1084, in global_index\r\n    index = arrays.index.values  # pandas<0.24.0\r\nAttributeError: 'builtin_function_or_method' object has no attribute 'values'\r\n\r\n```\r\n\r\nThis problem is not present when I read the same 3 branches with \r\n```\r\nuproot.open(file:tree).arrays(filter_name=[branch1, branch2, branch3], library = 'pd' )\r\n```\r\n, which yields a tuple of dataframes with length = number of jagged arrays being read as expected (which is 2 here). \r\nI can't see anything in the ```uproot``` documentation which prohibiits reading multiple jagged arrays into pandas dataframes via the ```iterate``` method, so I'm not sure what is causing the above error. \r\n\r\nI am using version ```4.0.0``` of ```uproot```. </div>",
  "closed_at":"2021-09-01T16:13:01Z",
  "comments":7,
  "created_at":"2021-09-01T14:35:27Z",
  "id":985233932,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5ODUyMzM5MzI=",
  "number":433,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"How do I read multiple branches with jagged structure through ```uproot.iterate()```",
  "updated_at":"2021-09-03T16:43:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-09-01T17:05:36Z",
  "comments":0,
  "created_at":"2021-09-01T16:51:33Z",
  "draft":false,
  "id":985376657,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzI0NjgwNjcx",
  "number":434,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-01T17:05:36Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fix for STAR PicoDST, which has multidimensional Float16_t.",
  "updated_at":"2021-09-01T17:05:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-09-09T16:23:54Z",
  "comments":0,
  "created_at":"2021-09-09T16:03:23Z",
  "draft":false,
  "id":992391600,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDExOlB1bGxSZXF1ZXN0NzMwNzMwNTA3",
  "number":437,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-09T16:23:54Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fixes KeyError in #436.",
  "updated_at":"2021-09-09T16:23:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'4.1.1'\r\n```\r\n\r\nA simple script like the following will confirm bugs on both of these files:\r\n\r\n```python\r\nimport uproot\r\nwith uproot.open('test.root:Delphes') as tree:\r\n  tree.arrays(entry_stop=1)\r\n```\r\n\r\nwhich either results in one of two different errors (depends on Delphes versions... and what process is generated...)\r\n\r\n<details>\r\n  <summary>bug in <code>AsGroup</code> interpretation not handling <code>TCloneArrays</code></summary>\r\n  <pre>\r\nTraceback (most recent call last):\r\n  File \"inout.py\", line 5, in <module>\r\n    events = tree.arrays(entry_stop=1)\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 1130, in arrays\r\n    False,\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3494, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3470, in basket_to_array\r\n    branch,\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/interpretation/objects.py\", line 228, in final_array\r\n    output = library.finalize(output, branch, self, entry_start, entry_stop)\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/interpretation/library.py\", line 570, in finalize\r\n    (_object_to_awkward_json(form, x) for x in array), highlevel=False\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/awkward/operations/convert.py\", line 885, in from_iter\r\n    for x in iterable:\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/interpretation/library.py\", line 570, in <genexpr>\r\n    (_object_to_awkward_json(form, x) for x in array), highlevel=False\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/interpretation/library.py\", line 315, in _object_to_awkward_json\r\n    return [_object_to_awkward_json(subform, x) for x in obj]\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/interpretation/library.py\", line 315, in <listcomp>\r\n    return [_object_to_awkward_json(subform, x) for x in obj]\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/interpretation/library.py\", line 292, in _object_to_awkward_json\r\n    if obj.has_member(name):\r\nAttributeError: 'numpy.ndarray' object has no attribute 'has_member'\r\n</pre>\r\n</details>\r\n\r\n<details>\r\n  <summary>bug in <code>fBits</code> counting, but might be a header interpretation issue somehow</summary>\r\n  <pre>\r\nTraceback (most recent call last):\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/interpretation/numerical.py\", line 328, in basket_array\r\n    output = data.view(dtype).reshape((-1,) + shape)\r\nValueError: When changing to a larger dtype, its size must be a divisor of the total size in bytes of the last axis of the array.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"inout.py\", line 3, in <module>\r\n    for events in uproot.iterate(\"test.root:Delphes\", entry_stop=1):\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 218, in iterate\r\n    report=report,\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 1364, in iterate\r\n    True,\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3494, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3445, in basket_to_array\r\n    library,\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/interpretation/jagged.py\", line 176, in basket_array\r\n    data, None, basket, branch, context, cursor_offset, library\r\n  File \"/tank/scratch/kratsg/py3/lib64/python3.6/site-packages/uproot/interpretation/numerical.py\", line 338, in basket_array\r\n    branch.file.file_path,\r\nValueError: basket 2 in tree/branch /Delphes;1:Particle/Particle.fBits has the wrong number of bytes (53382) for interpretation AsDtype('>u4')\r\nin file test.root\r\n</pre>\r\n</details>\r\n\r\nTwo files are uploaded with names to clarify which bug is raised by each one.\r\n- [`Delphes_as_members_issue.root`](https://www.dropbox.com/s/egcc17rkxz6fjij/Delphes_as_members_issue.root?dl=1)\r\n- [`Delphes_fBits_issue.root`](https://www.dropbox.com/s/4k4pcgqi501orc6/Delphes_fBits_issue.root?dl=1)",
  "closed_at":"2021-09-28T22:04:23Z",
  "comments":0,
  "created_at":"2021-09-10T14:07:54Z",
  "id":993271002,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5OTMyNzEwMDI=",
  "number":438,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"bugs with uproot and Delphes interactions",
  "updated_at":"2021-09-28T22:04:23Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"4.1.2\r\n\r\n```python\r\n>>> import uproot\r\n>>> import awkward as ak\r\n>>> f = uproot.recreate(\"whatever.root\")\r\n>>> a = ak.Array([[1, 2, 3], [4, 5, 6]])\r\n>>> f[\"tree\"] = {\"branch\": a}\r\n>>> f[\"tree\"].show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nbranch               | int64_t[3]               | AsDtype(\"('>i8', (3,))\")\r\n```\r\n\r\nThat should be a branch of type `int64_t[]` with a counter `nbranch` because the array type is\r\n\r\n```\r\n2 * var * int64\r\n```\r\n\r\nnot\r\n\r\n```\r\n2 * 3 * int64\r\n```\r\n\r\nIt doesn't matter that both lists happen to have the same length. (In a subsequent batch, they might not.)",
  "closed_at":"2021-09-28T16:02:05Z",
  "comments":0,
  "created_at":"2021-09-11T00:52:15Z",
  "id":993702441,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"MDU6SXNzdWU5OTM3MDI0NDE=",
  "number":439,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Writing TTrees is getting the branch type from the pattern of values, not the Awkward type",
  "updated_at":"2021-09-28T16:02:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"If a sub-branch cannot be optimized by awkward, the parent cannot either and must have its form fixed for use with lazy arrays.",
  "closed_at":"2021-09-18T01:58:14Z",
  "comments":2,
  "created_at":"2021-09-16T22:36:39Z",
  "draft":false,
  "id":998727651,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4r3OK5",
  "number":441,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-18T01:58:14Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Recursively fix forms for objects that require ak.from_iter",
  "updated_at":"2021-09-20T19:22:25Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2021-09-20T19:09:59Z",
  "comments":7,
  "created_at":"2021-09-17T22:08:07Z",
  "draft":false,
  "id":999753594,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4r6StB",
  "number":442,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-20T19:09:59Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Implement regular array support for TClonesArray",
  "updated_at":"2021-09-20T19:21:48Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"The Electron Ion Collider (EIC) is storing large data set on storage servers that we access via the S3 protocol. Reading from these files is currently not supported by uproot since dedicated [signed authentication headers](https://docs.aws.amazon.com/general/latest/gr/sigv4-signed-request-examples.html) are required.\r\n\r\nROOT has support for the S3 (and Google Storage) protocol in [TS3WebFile](https://root.cern.ch/doc/master/TS3WebFile_8cxx_source.html) and [TS3HTTPRequest](https://root.cern.ch/doc/master/TS3HTTPRequest_8cxx_source.html). This allows for specifying files with s3http:// and s3https:// protocols (as well as gshttp:// and gshttps://).\r\n\r\nI am filing this issue as way to keep track of discussion, suggestions, and other interested parties. I will be opening a WIP PR when we have some code that others may want to comment on. Currently we are anticipating that we will be able to implement this as a modification of `src/uproot/source/http.py` rather than by adding an entirely new physical layer.",
  "closed_at":"2023-08-10T19:44:14Z",
  "comments":2,
  "created_at":"2021-09-18T19:26:13Z",
  "id":1000114268,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss47nIhc",
  "number":443,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Reading files from S3 with authenticated http headers (s3http, s3https)",
  "updated_at":"2023-08-10T19:44:15Z",
  "user":"MDQ6VXNlcjQ2NTYzOTE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"This PR addresses #443. It is a work in progress and will change over time. Before merging, it will be rebased.",
  "closed_at":"2022-06-21T13:23:43Z",
  "comments":4,
  "created_at":"2021-09-18T22:13:23Z",
  "draft":false,
  "id":1000146007,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4r7VQ_",
  "number":444,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[WIP] Read files from S3 with authenticated http headers",
  "updated_at":"2022-06-21T13:23:43Z",
  "user":"MDQ6VXNlcjQ2NTYzOTE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi,\r\n\r\nI am not sure if this is a bug in ```uproot``` or a lack of understanding of how regex commands are combined in the backend. I am trying to select all branches of a file which do not contain the words \"jets\" or \"leptons\". To do this I pass to the ```filter_name``` argument of ```uproot.iterate()``` the following list with negative-lookahead patterns:\r\n ```['/^(?!.*jets.*).*$/i', '/^(?!.*leptons.*).*$/i']```\r\n\r\nIt seems like when I do this, ```uproot``` does not apply either filters at all. If I apply just one of the two filters, I get the correct behaviour. I also get the correct behaviour if I passboth filters as one regex pattern``` /^(?!.*(jets|leptons).*).*$/i```. However, having to do this latter combination of filters into just one is not favourable for my framework. Is it expected that uproot cannot handle the list of negative look-aheads?",
  "closed_at":"2021-09-22T17:11:35Z",
  "comments":4,
  "created_at":"2021-09-20T18:07:39Z",
  "id":1001251083,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss47reEL",
  "number":445,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"```filter_name``` with a list of negative-lookahead Regex commands",
  "updated_at":"2021-09-22T17:11:35Z",
  "user":"MDQ6VXNlcjg5MTQ3NDc4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.8b0 \u2192 21.9b0](https://github.com/psf/black/compare/21.8b0...21.9b0)\n",
  "closed_at":"2021-09-27T21:03:46Z",
  "comments":0,
  "created_at":"2021-09-20T18:37:20Z",
  "draft":false,
  "id":1001275796,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4r-mui",
  "number":446,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-27T21:03:46Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-09-27T21:03:47Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Pandas turns the \"None\" values of a MultiIndex into NaN, and then they're not good keys for dict lookup. The \"filler\" to use when leaf-lists (multiple levels of column name structure) and non-leaf-lists (single level) are mixed must be empty strings, rather than \"None\".",
  "closed_at":"2021-09-22T23:51:52Z",
  "comments":0,
  "created_at":"2021-09-22T23:30:44Z",
  "draft":false,
  "id":1004861569,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4sKcsr",
  "number":449,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-22T23:51:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Leaf-lists (structs) and non-leaf-lists in the same Pandas DataFrame.",
  "updated_at":"2021-09-22T23:51:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"The title says it all, and it's clear where the problem is from the example below. I'm just not 100% sure if this should be handled in Uproot or in Awkward.\r\n\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'4.1.2'\r\n>>> with uproot.recreate('test.root') as file_for_writing:\r\n...     file_for_writing['tree'] = {'branch': []}\r\n...\r\n>>> uproot.open('test.root')['tree'].arrays()\r\n<Array [] type='0 * {\"branch\": float64}'>\r\n>>> uproot.lazy('test.root:tree')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/mproffit/miniconda3/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 683, in lazy\r\n    out = awkward.partition.IrregularlyPartitionedArray(partitions, global_offsets[1:])\r\n  File \"/home/mproffit/miniconda3/lib/python3.9/site-packages/awkward/partition.py\", line 685, in __init__\r\n    self._ext = ak._ext.IrregularlyPartitionedArray(nextpartitions, nextstops)\r\nValueError: PartitionedArray must have at least one partition\r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob/1.5.0/src/libawkward/partition/PartitionedArray.cpp#L19)\r\n```\r\n\r\nThe expected behavior would of course be for `uproot.lazy()` to return an empty `Array` like `arrays()` does.",
  "closed_at":"2021-09-27T21:23:43Z",
  "comments":1,
  "created_at":"2021-09-27T17:41:03Z",
  "id":1008418341,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss48Gz4l",
  "number":450,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"`uproot.lazy()` fails on TTrees with zero entries",
  "updated_at":"2021-09-27T21:23:43Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-09-27T21:23:43Z",
  "comments":0,
  "created_at":"2021-09-27T19:15:40Z",
  "draft":false,
  "id":1008502799,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4sVL3t",
  "number":451,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-27T21:23:43Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fixed lazy for empty TTrees (#450) and boost-histogram with no title.",
  "updated_at":"2021-09-27T21:23:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-09-27T20:36:11Z",
  "comments":0,
  "created_at":"2021-09-27T20:19:22Z",
  "draft":false,
  "id":1008559750,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4sVXRy",
  "number":452,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-27T20:36:11Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Turn off eospublic-dependent tests for now.",
  "updated_at":"2021-09-27T20:36:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-09-28T16:02:06Z",
  "comments":0,
  "created_at":"2021-09-27T21:51:38Z",
  "draft":false,
  "id":1008667403,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4sVtz3",
  "number":453,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-28T16:02:05Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fixes #439 by checking for an Awkward Array before NumPy.",
  "updated_at":"2021-09-28T16:02:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"But the second bug offers no clues as to how it can be solved.\r\n\r\nHere's the issue: according to TObject's streamers (and a lot of other things), `fBits` is 4 bytes.\r\n\r\n```python\r\n>>> import uproot\r\n>>> t = uproot.open(\"uproot-issue-438b.root:Delphes\")\r\n>>> t.file.streamer_named(\"TObject\").show()\r\nTObject (v1)\r\n    fUniqueID: unsigned int (TStreamerBasicType)\r\n    fBits: unsigned int (TStreamerBasicType)\r\n```\r\n\r\nThis is also true of _almost_ all of the `fBits` branches:\r\n\r\n```python\r\n>>> for x in t.keys(filter_name=\"*.fBits\"):\r\n...     print(f\"{len(t[x].debug_array(0)) / 4:10} {x}\")\r\n... \r\n       1.0 Event/Event.fBits\r\n       1.0 Weight/Weight.fBits\r\n    7924.5 Particle/Particle.fBits\r\n     273.0 Track/Track.fBits\r\n     819.0 Tower/Tower.fBits\r\n     273.0 EFlowTrack/EFlowTrack.fBits\r\n     477.0 EFlowPhoton/EFlowPhoton.fBits\r\n     267.0 EFlowNeutralHadron/EFlowNeutralHadron.fBits\r\n       8.0 GenJet/GenJet.fBits\r\n       1.0 GenMissingET/GenMissingET.fBits\r\n       6.0 Jet/Jet.fBits\r\n       0.0 Electron/Electron.fBits\r\n       0.0 Photon/Photon.fBits\r\n       0.0 Muon/Muon.fBits\r\n       1.0 MissingET/MissingET.fBits\r\n       1.0 ScalarHT/ScalarHT.fBits\r\n```\r\n\r\nThe exception is `Particle/Particle.fBits`, whose first event has a number of bytes that does not divide evenly by 4. Upon closer inspection, it's clearly a 6-byte value.\r\n\r\n```python\r\n>>> t[\"Particle/Particle.fBits\"].debug_array(0).view([(\"x\", \">i4\"), (\"y\", \">i2\")])\r\narray([(50331664, 0), (50331664, 0), (50331664, 0), ..., (50331664, 0),\r\n       (50331664, 0), (50331664, 0)], dtype=[('x', '>i4'), ('y', '>i2')])\r\n```\r\n\r\nHowever, there's nothing to distinguish this branch with 6 bytes:\r\n\r\n```python\r\n>>> t[\"Particle/Particle.fBits\"].all_members\r\n{\r\n    '@fUniqueID': 0,\r\n    '@fBits': 55574528,\r\n    'fName': 'Particle.fBits',\r\n    'fTitle': 'fBits[Particle_]',\r\n    'fFillColor': 0,\r\n    'fFillStyle': 1001,\r\n    'fCompress': 101,\r\n    'fBasketSize': 64000,\r\n    'fEntryOffsetLen': 10,\r\n    'fWriteBasket': 33,\r\n    'fEntryNumber': 100,\r\n    'fIOFeatures': <ROOT::TIOFeatures at 0x7fd20af3d400>,\r\n    'fOffset': 0,\r\n    'fMaxBaskets': 34,\r\n    'fSplitLevel': 0,\r\n    'fEntries': 100,\r\n    'fFirstEntry': 0,\r\n    'fTotBytes': 1710622,\r\n    'fZipBytes': 11966,\r\n    'fBranches': <TObjArray of 0 items at 0x7fd20af3d460>,\r\n    'fLeaves': <TObjArray of 1 items at 0x7fd20af3d5b0>,\r\n    'fBasketBytes': array([381, 307, 382, 348, 276, 408, 359, 400, 376, 358, 339, 320, 324,\r\n       362, 474, 310, 434, 366, 427, 282, 333, 452, 378, 378, 392, 382,\r\n       370, 349, 392, 385, 365, 316, 241,   0], dtype=int32),\r\n    'fBasketEntry': array([  0,   3,   5,   7,  11,  12,  15,  18,  21,  24,  27,  29,  31,\r\n        33,  36,  41,  43,  46,  49,  53,  54,  56,  61,  64,  68,  73,\r\n        77,  81,  83,  87,  92,  95,  97, 100]),\r\n    'fBasketSeek': array([     375,    17300,   658211,   676369,  1374256,  1410371,\r\n        2055073,  2668842,  2705428,  3223503,  3741825,  3748989,\r\n        3777650,  4534932,  5093995,  5133565,  5736716,  6572626,\r\n        7166521,  7186122,  7660358,  8287949,  8333770,  8858670,\r\n        9413125,  9435318, 10023382, 10544552, 10584617, 11540595,\r\n       12060644, 12084058, 12654014,        0]),\r\n    'fClassName': <TString 'TObject' at 0x7fd20af9bb30>,\r\n    'fParentName': <TString 'GenParticle' at 0x7fd20af9bba0>,\r\n    'fClonesName': <TString '' at 0x7fd20af9bc10>,\r\n    'fCheckSum': 2417737773,\r\n    'fClassVersion': 1,\r\n    'fID': 1,\r\n    'fType': 31,\r\n    'fStreamerType': 15,\r\n    'fMaximum': 0,\r\n    'fBranchCount': None,\r\n    'fBranchCount2': None\r\n}\r\n```\r\n\r\nfrom branches with 4 bytes:\r\n\r\n```python\r\n>>> t[\"Jet/Jet.fBits\"].all_members\r\n{\r\n    '@fUniqueID': 0,\r\n    '@fBits': 55574528,\r\n    'fName': 'Jet.fBits',\r\n    'fTitle': 'fBits[Jet_]',\r\n    'fFillColor': 0,\r\n    'fFillStyle': 1001,\r\n    'fCompress': 101,\r\n    'fBasketSize': 64000,\r\n    'fEntryOffsetLen': 400,\r\n    'fWriteBasket': 1,\r\n    'fEntryNumber': 100,\r\n    'fIOFeatures': <ROOT::TIOFeatures at 0x7fd209e72a30>,\r\n    'fOffset': 0,\r\n    'fMaxBaskets': 10,\r\n    'fSplitLevel': 0,\r\n    'fEntries': 100,\r\n    'fFirstEntry': 0,\r\n    'fTotBytes': 2131,\r\n    'fZipBytes': 352,\r\n    'fBranches': <TObjArray of 0 items at 0x7fd209e72a90>,\r\n    'fLeaves': <TObjArray of 1 items at 0x7fd209e72be0>,\r\n    'fBasketBytes': array([352,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32),\r\n    'fBasketEntry': array([  0, 100,   0,   0,   0,   0,   0,   0,   0,   0]),\r\n    'fBasketSeek': array([14605752,        0,        0,        0,        0,        0,\r\n              0,        0,        0,        0]),\r\n    'fClassName': <TString 'TObject' at 0x7fd209e6e4a0>,\r\n    'fParentName': <TString 'Jet' at 0x7fd209e6e510>,\r\n    'fClonesName': <TString '' at 0x7fd209e6e580>,\r\n    'fCheckSum': 2417737773,\r\n    'fClassVersion': 1,\r\n    'fID': 1,\r\n    'fType': 31,\r\n    'fStreamerType': 15,\r\n    'fMaximum': 0,\r\n    'fBranchCount': None,\r\n    'fBranchCount2': None\r\n}\r\n```\r\n\r\nSo they can't be given different interpretations.\r\n\r\nIn the end, I think that the 6-bytes vs 4-bytes indicator might be in the `fBits` themselves. See the first 4 bytes of the first event:\r\n\r\n```python\r\n>>> for x in t.keys(filter_name=\"*.fBits\"):\r\n...     print(f\"{str(t[x].debug_array(0)[:4]):15} {x}\")\r\n... \r\n[3 0 0 0]       Event/Event.fBits\r\n[3 0 0 0]       Weight/Weight.fBits\r\n[ 3  0  0 16]   Particle/Particle.fBits\r\n[ 3  0  0 16]   Track/Track.fBits\r\n[ 3  0  0 16]   Tower/Tower.fBits\r\n[ 3  0  0 16]   EFlowTrack/EFlowTrack.fBits\r\n[ 3  0  0 16]   EFlowPhoton/EFlowPhoton.fBits\r\n[ 3  0  0 16]   EFlowNeutralHadron/EFlowNeutralHadron.fBits\r\n[3 0 0 0]       GenJet/GenJet.fBits\r\n[3 0 0 0]       GenMissingET/GenMissingET.fBits\r\n[3 0 0 0]       Jet/Jet.fBits\r\n[]              Electron/Electron.fBits\r\n[]              Photon/Photon.fBits\r\n[]              Muon/Muon.fBits\r\n[3 0 0 0]       MissingET/MissingET.fBits\r\n[3 0 0 0]       ScalarHT/ScalarHT.fBits\r\n```\r\n\r\nThe ones that have a `16` in them also have event sizes that are divisible by 6:\r\n\r\n```python\r\n>>> for x in t.keys(filter_name=\"*.fBits\"):\r\n...     print(f\"{len(t[x].debug_array(0)) / 6:10} {x}\")\r\n... \r\n0.6666666666666666 Event/Event.fBits\r\n0.6666666666666666 Weight/Weight.fBits\r\n    5283.0 Particle/Particle.fBits\r\n     182.0 Track/Track.fBits\r\n     546.0 Tower/Tower.fBits\r\n     182.0 EFlowTrack/EFlowTrack.fBits\r\n     318.0 EFlowPhoton/EFlowPhoton.fBits\r\n     178.0 EFlowNeutralHadron/EFlowNeutralHadron.fBits\r\n5.333333333333333 GenJet/GenJet.fBits\r\n0.6666666666666666 GenMissingET/GenMissingET.fBits\r\n       4.0 Jet/Jet.fBits\r\n       0.0 Electron/Electron.fBits\r\n       0.0 Photon/Photon.fBits\r\n       0.0 Muon/Muon.fBits\r\n0.6666666666666666 MissingET/MissingET.fBits\r\n0.6666666666666666 ScalarHT/ScalarHT.fBits\r\n```\r\n\r\nThat `16` could be `kIsReferenced`...\r\n\r\n```python\r\n>>> np.array([uproot.const.kIsReferenced], \">i4\").view(\"u1\")\r\narray([ 0,  0,  0, 16], dtype=uint8)\r\n```\r\n\r\nIf this is the issue, then the `fBits` would have to become an object type ([uproot.AsObjects](https://uproot.readthedocs.io/en/latest/uproot.interpretation.objects.AsObjects.html)) that includes the extra 2 bytes if it sees a `16`. That would be slow to deserialize (before AwkwardForth) and annoying because nobody cares about `fBits`.\r\n\r\nI had another idea: why not just read them in as 1-byte values? Then at least it doesn't break.\r\n\r\n```python\r\n>>> import uproot\r\n>>> t = uproot.open(\"uproot-issue-438b.root:Delphes\")\r\n>>> for x in t.keys(filter_name=\"*.fBits\"):\r\n...     print(t[x].array())\r\n... \r\n[[3, 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0, ... 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0]]\r\n[[3, 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0, ... 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0]]\r\n[[3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0, 3, ... 3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0]]\r\n[[3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0, 3, ... 3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0]]\r\n[[3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0, 3, ... 3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0]]\r\n[[3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0, 3, ... 3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0]]\r\n[[3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0, 3, ... 3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0]]\r\n[[3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0, 3, ... 3, 0, 0, 16, 0, 0, 3, 0, 0, 16, 0, 0]]\r\n[[3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, ... [3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0]]\r\n[[3, 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0, ... 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0]]\r\n[[3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, ... [3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0]]\r\n[[], [], [], [], [], [], [], [], [], [], ... [], [], [], [], [], [], [], [], [], []]\r\n[[], [], [], [], [], [], [], [], [], [], ... [], [], [], [], [], [], [], [], [], []]\r\n[[], [], [], [], [], [], [], [], [], [], ... [], [], [], [], [], [], [], [], [], []]\r\n[[3, 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0, ... 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0]]\r\n[[3, 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0, ... 0, 0, 0], [3, 0, 0, 0], [3, 0, 0, 0]]\r\n```\r\n\r\nThat's the second commit. \r\n",
  "closed_at":"2021-09-28T22:04:23Z",
  "comments":0,
  "created_at":"2021-09-28T21:48:12Z",
  "draft":false,
  "id":1010242280,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4satJ8",
  "number":454,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-09-28T22:04:23Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Fixes both bugs in issue #438.",
  "updated_at":"2021-09-28T22:04:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"When the callback() in XRootDResource.callbacker has an exception, then an iteration never returns.  We found out this by getting socket timeout errors from xrootd.\r\n\r\nThe simplest way I found to reproduce this is to change the callback [here](https://github.com/scikit-hep/uproot4/blob/03cb08ccb1f47ce0b082117f41a112af1ab403ee/src/uproot/source/xrootd.py#L232) to immediately \"raise\" an exception, as:\r\n\r\n```python\r\n        def callback(status, response, hosts):\r\n            self._xrd_error(status)\r\n            ...rest of code\r\n```\r\nand use this example:\r\n\r\n```python\r\nimport uproot\r\n\r\nsource='root://eospublic.cern.ch//eos/opendata/lhcb/AntimatterMatters2017/data/B2HHH_MagnetDown.root'\r\nfor a in uproot.behaviors.TBranch.iterate(source, 'H3_isMuon'):\r\n    print(a)\r\n\r\nprint(\"not printed if callback fails\")\r\n```\r\n\r\nWhen there is an exception, it is printed to the console, but the script never finishes. \r\n\r\nThis was tested with uproot 4.1.3 from a pip install.\r\n\r\n\r\n",
  "closed_at":null,
  "comments":1,
  "created_at":"2021-10-01T16:32:57Z",
  "id":1013545014,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss48aXg2",
  "number":456,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"iteration blocks on exception from callback in XRootDResource",
  "updated_at":"2021-10-01T20:13:57Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"I'm writing a jagged array to a ROOT file and reading it back either in pyroot or C++ ROOT.\r\n\r\nThis mostly round trips properly:\r\n```python\r\n>>> import uproot, awkward as ak\r\n>>> print(uproot.__version__)\r\n4.1.3\r\n>>> print(ak.__version__)\r\n1.4.0\r\n>>> f = uproot.recreate(\"test.root\")\r\n>>> f[\"t\"] = {\"branch\":  ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5], [6.6]])}\r\n>>> f.close()\r\n\r\n>>> import ROOT\r\n>>> ch = ROOT.TChain(\"t\")\r\n>>> ch.Add(\"test.root\")\r\n>>> [list(evt.branch) for evt in ch] # works\r\n[[1.1, 2.2, 3.3], [], [4.4, 5.5], [6.6]]\r\n>>> ch.Draw(\"branch\", \"\", \"goff\") # this should be 6\r\n4\r\n```\r\n\r\nBut with ROOT 6.24/06 I see\r\n```cpp\r\nroot [2] t->Scan(\"*\")\r\n************************************\r\n*    Row   * nbranch.n * branch.br *\r\n************************************\r\n*        0 *         3 *       1.1 *\r\n*        1 *         0 *           *\r\n*        2 *         2 *       4.4 *\r\n*        3 *         1 *       6.6 *\r\n************************************\r\n(long long) 4\r\nroot [3] t->Scan(\"branch[0]\")\r\n************************\r\n*    Row   * branch[0] *\r\n************************\r\n*        0 *       1.1 *\r\n*        1 *           *\r\n*        2 *       4.4 *\r\n*        3 *       6.6 *\r\n************************\r\n(long long) 4\r\nroot [4] t->Scan(\"branch[1]\")\r\n************************\r\n*    Row   * branch[1] *\r\n************************\r\n*        0 *       1.1 *\r\n*        1 *           *\r\n*        2 *       4.4 *\r\n*        3 *           *\r\n************************\r\n(long long) 4\r\nroot [5] t->Scan(\"branch[2]\")\r\n************************\r\n*    Row   * branch[2] *\r\n************************\r\n*        0 *       1.1 *\r\n*        1 *           *\r\n*        2 *           *\r\n*        3 *           *\r\n************************\r\n(long long) 4\r\n```\r\nAnd `t->Draw(\"branch\")` gives me a histogram filled with `[0, 1.1, 4.4, 6.6]`",
  "closed_at":"2021-10-04T14:15:42Z",
  "comments":6,
  "created_at":"2021-10-04T00:34:48Z",
  "id":1014610803,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss48ebtz",
  "number":457,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Reading jagged array output back in C++ ROOT",
  "updated_at":"2021-10-04T21:41:37Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Although PyROOT's iteration only looks at the `fLeafCount` member to determine the number of subentries, `TTree::Scan`, `TTree::Draw`, and `TTreeReader` all seem to be parsing the text of the leaf to determine how to count the subentries.\r\n\r\nSee discussion in #457.",
  "closed_at":"2021-10-04T14:15:42Z",
  "comments":0,
  "created_at":"2021-10-04T13:51:01Z",
  "draft":false,
  "id":1015210204,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4spD5E",
  "number":458,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-04T14:15:42Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Set leaf title to 'branchname[countername]' for jagged arrays.",
  "updated_at":"2021-10-04T14:15:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi, \r\nI am using uproot 4.0.4 and when I use the command uproot.recreate the following error appears:\r\n\"\"\r\nmodule 'uproot' has no attribute 'recreate'\r\n\r\n\"\r\n",
  "closed_at":"2021-10-05T17:19:36Z",
  "comments":1,
  "created_at":"2021-10-05T17:12:02Z",
  "id":1016570497,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss48l6KB",
  "number":459,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot.recreate doesn't exist",
  "updated_at":"2021-10-05T17:19:36Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"In my pandas dataframe, I have columns/variable names with whitespace and parantheses, which I had heard ROOT does not support. Just out of curiosity, I tried writing it to a ROOT TTree with uproot 4.1.3 and it worked without warnings. With uproot I could read the file back in again and got the column names back in the correct form. So in this aspect it seemed to me more advanced than ROOT and this is why I didn't file it as a bug.\r\n\r\nHowever, when opening the file in the TBrowser (ROOT 6.24), I couldn't see most leaves/variables. I have a replacement dictionary to make variable names ROOT-compatible, and when writing the resulting root-compatible dataframe to a ROOT file, I could see all branches in TBrowser again.\r\n\r\nI would suggest to give at least a warning when writing data to ROOT with ROOT-incompatible keys/variablenames. At least if the resulting file will be unusuable from ROOT. \r\n\r\nExcept you think that the TTrees are valid and them not being visible in TBrowser is an implementation issue of ROOT/TBrowser, then feel free to close this.",
  "closed_at":"2021-10-06T17:13:56Z",
  "comments":5,
  "created_at":"2021-10-06T14:12:07Z",
  "id":1018413224,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss48s8Co",
  "number":460,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Add warning/error or handle ROOT-incompatible variable/column names",
  "updated_at":"2021-10-06T21:56:54Z",
  "user":"MDQ6VXNlcjUxMjE4MjQ="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-10-07T17:35:28Z",
  "comments":1,
  "created_at":"2021-10-07T17:18:51Z",
  "draft":false,
  "id":1020281116,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4s5734",
  "number":463,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-07T17:35:28Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"RNTuple needs to be in the must_be_attached list.",
  "updated_at":"2021-10-07T17:35:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-10-08T21:24:05Z",
  "comments":0,
  "created_at":"2021-10-08T21:04:53Z",
  "draft":false,
  "id":1021461150,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4s9jjJ",
  "number":467,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-08T21:24:05Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Add an implementation of non-split TClonesArray.",
  "updated_at":"2021-10-08T21:24:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/asottile/setup-cfg-fmt: v1.17.0 \u2192 v1.18.0](https://github.com/asottile/setup-cfg-fmt/compare/v1.17.0...v1.18.0)\n- [github.com/PyCQA/flake8: 3.9.2 \u2192 4.0.1](https://github.com/PyCQA/flake8/compare/3.9.2...4.0.1)\n",
  "closed_at":"2021-10-11T19:34:00Z",
  "comments":0,
  "created_at":"2021-10-11T18:27:37Z",
  "draft":false,
  "id":1022999401,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tCHh_",
  "number":468,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-11T19:34:00Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-10-11T19:34:00Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"### Discussed in https://github.com/scikit-hep/uproot4/discussions/470\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **denehoffman** October 12, 2021</sup>\r\nI'm trying to read in data from an existing TTree, reformat/skim it down, and write it to a new TTree. The only problem is that my entire collaboration uses an outdated version of ROOT which apparently can't read version-20 TTrees:\r\n```\r\nError in <TBufferFile::ReadClassBuffer>: Could not find the StreamerInfo for version 20 of the class TTree, object skipped at offset 51\r\nError in <TBufferFile::CheckByteCount>: object of class TTree read too few bytes: 2 instead of 5528\r\n```\r\nThe tree can be opened (in ROOT) just fine on my local computer running the latest ROOT. When I look at the input tree versions, I see that\r\n```\r\n>>> uproot.open(\"tree_sum_AMO_30274_31057.root\")['ksks__B4_Tree'].__class__\r\n<class 'uproot.models.TTree.Model_TTree_v19'>\r\n```\r\nso version-19 trees are fine, but the output trees have\r\n```\r\n>>> uproot.open(\"flattree_AMO.root\")['kin'].__class__\r\n<class 'uproot.models.TTree.Model_TTree_v20'>\r\n```\r\nand these ones can't be read by the collaboration's version of ROOT. Is there some way to specify using the previous version when writing?</div>\r\n\r\n---------------\r\n\r\nSubsequent discussion showed that no streamer info was written at all.\r\n\r\n```\r\n% python\r\nPython 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> import uproot\r\n>>> with uproot.recreate(\"demo.root\") as output_file:\r\n...     output_file.mktree(\"kin\", {\"Weight\": \"f4\", \"Energy\": (\"f4\", (3,))})\r\n...     for i in range(10):\r\n...         weights = np.random.rand(10000)\r\n...         energies = np.random.rand(10000, 3)\r\n...         output_file[\"kin\"].extend({\"Weight\": weights, \"Energy\": energies})\r\n... \r\n<WritableTree '/kin' at 0x7f3c5bfa2d90>\r\n>>> \r\n% python\r\nPython 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> input_file = uproot.open(\"demo.root\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nNameError: name 'uproot' is not defined\r\n>>> import uproot\r\n>>> input_file = uproot.open(\"demo.root\")\r\n>>> input_file.file.show_streamers()\r\n```\r\n\r\n(no output).",
  "closed_at":"2021-10-13T19:44:07Z",
  "comments":3,
  "created_at":"2021-10-13T17:01:18Z",
  "id":1025490708,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss49H78U",
  "number":471,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Writing a TTree doesn't put the TTree TStreamerInfo into the file.",
  "updated_at":"2021-10-13T20:48:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-10-13T19:44:07Z",
  "comments":0,
  "created_at":"2021-10-13T19:11:10Z",
  "draft":false,
  "id":1025604414,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tKYqX",
  "number":472,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-13T19:44:07Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Add TTree TStreamerInfo when writing a TTree.",
  "updated_at":"2021-10-13T19:44:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Attempting to `update()` a newly-created file with another TTree raises an `AssertionError`.\r\n\r\n```python\r\n>>> import numpy\r\n>>> import uproot\r\n>>> print(uproot.__version__)\r\n4.1.3\r\n>>> b1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n>>> b2 = [0.0, 1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9]\r\n>>> with uproot.recreate(\"test.root\", compression=None) as fout:\r\n...     tree = fout.mktree(\"t\", {\"b1\": numpy.int32, \"b2\": numpy.float64}, \"title\")\r\n...     tree.extend({\"b1\": b1, \"b2\": b2})\r\n...\r\n>>> with uproot.update(\"test.root\") as fout:\r\n...     tree = fout.mktree(\"t2\", {\"b1\": numpy.int32, \"b2\": numpy.float64}, \"title\")\r\n...     tree.extend({\"b1\": b1, \"b2\": b2})\r\n...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/duncan/opt/miniconda3/envs/py39/lib/python3.9/site-packages/uproot/writing/writable.py\", line 171, in update\r\n    cascading = uproot.writing._cascade.update_existing(\r\n  File \"/home/duncan/opt/miniconda3/envs/py39/lib/python3.9/site-packages/uproot/writing/_cascade.py\", line 2435, in update_existing\r\n    freesegments_key = Key.deserialize(\r\n  File \"/home/duncan/opt/miniconda3/envs/py39/lib/python3.9/site-packages/uproot/writing/_cascade.py\", line 442, in deserialize\r\n    assert fCycle > 0\r\nAssertionError\r\n```\r\n\r\nI presume this is user error, so can someone please point me in the right direction to be able to append new trees to an existing file?",
  "closed_at":"2021-10-14T14:13:19Z",
  "comments":3,
  "created_at":"2021-10-14T09:46:48Z",
  "id":1026190493,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss49Kmyd",
  "number":474,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"TBaskets from one TTree get overwritten by another TTree",
  "updated_at":"2021-10-14T14:45:45Z",
  "user":"MDQ6VXNlcjE2MTg1MzA="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-10-14T14:13:19Z",
  "comments":0,
  "created_at":"2021-10-14T13:37:33Z",
  "draft":false,
  "id":1026411992,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tM6nH",
  "number":475,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-14T14:13:19Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Remember to update FreeSegments when writing TBaskets.",
  "updated_at":"2021-10-14T14:13:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"This PR updates the `MANIFEST.in` to include the `tests/` directory in source distributions (tarballs). This should enable downstream package builders (e.g. conda-forge) to run the test suite to validate packages.",
  "closed_at":"2021-10-14T16:41:45Z",
  "comments":2,
  "created_at":"2021-10-14T15:39:47Z",
  "draft":false,
  "id":1026549459,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tNWdD",
  "number":477,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-14T16:41:45Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Include the tests suite in source distributions",
  "updated_at":"2021-10-27T21:46:47Z",
  "user":"MDQ6VXNlcjE2MTg1MzA="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Awkward 2.x will require an Uproot version bump to 5.x, but it won't be changing much. (Mostly, `uproot.lazy` will have to use Dask, rather than VirtualArray.)",
  "closed_at":"2021-10-18T17:34:22Z",
  "comments":2,
  "created_at":"2021-10-18T17:16:04Z",
  "draft":false,
  "id":1029388422,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tVzrf",
  "number":478,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-18T17:34:22Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Restrict Uproot 4.x to Awkward 1.x.",
  "updated_at":"2021-10-28T11:31:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Sketch of a solution for issue #456. Simply catch the exception on the\r\ncallback and update the futures accordingly (as HTTP source does).",
  "closed_at":"2021-10-20T18:21:52Z",
  "comments":7,
  "created_at":"2021-10-20T15:51:12Z",
  "draft":false,
  "id":1031570805,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tcqx0",
  "number":480,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-20T18:21:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Pass exceptions to main thread in xrootd",
  "updated_at":"2021-10-20T19:01:13Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-10-20T19:31:07Z",
  "comments":0,
  "created_at":"2021-10-20T19:04:23Z",
  "draft":false,
  "id":1031738414,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tdNnu",
  "number":481,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-20T19:31:07Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Add @btovar manually because the allcontributors bot is broken.",
  "updated_at":"2021-10-20T19:31:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi,\r\n\r\nI'm trying to read in a leaf (fX) of a branch in uproot that has type 'TNonSplitBrowsable', stored within the branch 'hitPosExtrap', which itself is stored in a Tree. Previously, the TNonSplitBrowsable (fX) used to be stored as a TBranchElement and I could read it in by writing the leaf as: 'hitPosExtrap/fX'. This now fails as it's not listed as an 'available key'. Is there an alternative way to read a TNonSplitBrowsable using uproot? Here is a screenshot of the ROOT file:\r\n![138164234-36dc9c8d-66e2-4ea7-ae6a-880c6f97d658](https://user-images.githubusercontent.com/73175126/138164315-ff66d128-f98e-4998-8583-860369a3237e.png)\r\n\r\n\r\nCheers,\r\nTom",
  "closed_at":"2021-10-20T20:41:58Z",
  "comments":4,
  "created_at":"2021-10-20T20:08:16Z",
  "id":1031787226,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss49f9La",
  "number":482,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Reading in a TNonSplitBrowsable using uproot",
  "updated_at":"2021-10-20T20:41:58Z",
  "user":"MDQ6VXNlcjczMTc1MTI2"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Dear experts,\r\n\r\nFrom https://github.com/scikit-hep/uproot4/discussions/321 I noticed that uproot4 has supported writing ROOT files (many thanks for this very useful feature!)\r\n\r\nI met an error trying to write ROOT file with uproot4 then read the file with uproot3. Here is a minimal script.\r\n\r\n```python\r\nimport uproot\r\nimport awkward as ak\r\n\r\na = ak.Array([1,2,3])\r\nb = ak.ones_like(a)\r\nwith uproot.recreate('test.root', compression=None) as f:\r\n    f['Events'] = {'a':a, 'b':b}\r\n    f['Events'].title = 'Events'\r\n\r\nimport uproot3\r\nuproot3.open('test.root')['Events']\r\n```\r\n\r\nIt returns `<Undefined_TTree at 0x7f3005b9f2e0>` instead of a normal `TTree` class.\r\nWriting jagged arrays with uproot4 also reproduces this error.\r\n\r\nP.S. I know uproot3 is nearly deprecated and the described workflow is actually not recommended. But it is still helpful though, if it can be fixed while not taking too much time.\r\nThank you very much for this.",
  "closed_at":"2021-10-21T12:33:53Z",
  "comments":2,
  "created_at":"2021-10-21T10:29:12Z",
  "id":1032324953,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss49iAdZ",
  "number":483,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Undefined_TTree reported in uproot3 when reading uproot4-writed ROOT files",
  "updated_at":"2021-10-21T12:33:53Z",
  "user":"MDQ6VXNlcjQ0ODg1NDAw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2022-06-21T12:42:03Z",
  "comments":0,
  "created_at":"2021-10-22T15:58:07Z",
  "draft":false,
  "id":1033731086,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tjq4f",
  "number":484,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2022-06-21T12:42:02Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Manually add a Model for TMatrixTSym<double>.",
  "updated_at":"2022-06-21T12:42:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"### Discussed in https://github.com/scikit-hep/uproot4/discussions/479\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **niamh-h** October 20, 2021</sup>\r\nHi! I'm trying to use a SciKit classifier on some data by reading it in from multiple ROOT TTrees to Pandas DataFrames (so I use uproot.open()). I then want to update the existing files with the classification data (e.g. confidence scores/probabilities etc) using uproot.update(). I'm having trouble with updating the files as they don't seem to be closing correctly despite being opened in a with statement. I've tried different ways of updating the files (below) but they all lead to the following error when I open the file in ROOT to check: \r\n\r\n```\r\nError in <TBufferFile::CheckByteCount>: object of class TList read too few bytes: 17391 instead of 17773\r\nError in <TBufferFile::CheckByteCount>: Byte count probably corrupted around buffer position 64:\r\n\t17773 for a possible maximum of -9\r\n```\r\n\r\nI think this means it's not closing correctly? If I then retry the python code with the same ROOT file I get an error when trying to open it in a writable form using uproot.update() (but no error when using uproot.open()), I'm assuming because it's been corrupted: (FYI I have updated to uproot 4.1.5)\r\n\r\n`ValueError: Uproot can't read TKey version -1 for writing, only version 4`\r\n\r\nThis is the code for two ways I've tried updating the file: \r\n```\r\nwith uproot.update('Datafile.root') as file:\r\n  file.mktree('t', {'branch1': np.int64, 'branch2':np.int64}, 'title')\r\n  file['title'].extend({'branch1:df.loc[:,'branch1'], 'branch2':df.loc[:,'branch2']})\r\n```\r\n```\r\nfile = uproot.update('Datafile.root)\r\nfile['df'] = df\r\nfile.close()\r\n```\r\n(I've also tried this way inside a with statement). \r\nHope I've explained this ok, any help would be appreciated! </div>\r\n\r\nBelow is the code I'm using to open the file and an example of extracting the data into pandas. Then in a new with statement I want to add some data to the file using update. For this I'm using an example TTree of data from the ROOTSYS tutorials. \r\n```\r\nwith uproot.open('cernstaff.root:T') as file1:\r\n  df1 = file1.arrays(['Age'], library='pd')\r\n  df1['label'] = 0\r\n  df1['source'] = 1\r\nwith uproot.update('cernstaff.root') as file2:\r\n  file2.mktree('ML_data', {'label':np.int64, 'source':np.int64}, 'ML_data')\r\n  file2['new_data'].extend({'label':df1.loc[:,'label'], 'source':df1.loc[:,'source']}) \r\n\r\n```\r\nI've also tried the following:\r\n```\r\nwith uproot.open('cernstaff.root:T') as file1:\r\n  df1 = file1.arrays(['Age'], library='pd')\r\n  df1['label'] = 0\r\n  df1['source'] = 1\r\n  print (df1)\r\nwith uproot.update('cernstaff.root') as file2:\r\n  file2['new_data'] = df\r\n```\r\nBoth methods give the same errors I described above. \r\n",
  "closed_at":"2021-10-27T21:36:39Z",
  "comments":6,
  "created_at":"2021-10-26T10:14:34Z",
  "id":1036103739,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss49wbA7",
  "number":485,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Updating ROOT file with Pandas DataFrame error",
  "updated_at":"2021-10-27T21:36:39Z",
  "user":"MDQ6VXNlcjc0NDIyMzc3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"There are two independent places where the XRootD version is detected:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/9607057b07b5936d6f2925e33ad6386a7dfc7705/src/uproot/extras.py#L121 and https://github.com/scikit-hep/uproot4/blob/9607057b07b5936d6f2925e33ad6386a7dfc7705/src/uproot/reading.py#L170\r\n\r\nThese both rely on `pkg_resources.get_distribution(\"XRootD\")`, but the name of the Python module [changed from `pyxrootd` to `xrootd`](https://github.com/xrootd/xrootd/commit/dfa0806f02775dc04e7835e8fc871b81a2bd786f) in [v4.11.1](https://github.com/xrootd/xrootd/blob/v4.11.1/docs/ReleaseNotes.txt#L26). Critically, this means that with XRootD v4.11.0 and earlier, the workaround here (for the deadlock condition described in https://github.com/scikit-hep/uproot3/issues/504):\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/9607057b07b5936d6f2925e33ad6386a7dfc7705/src/uproot/extras.py#L87-L107\r\n\r\ndoesn't get applied, and you don't get the warning in:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/9607057b07b5936d6f2925e33ad6386a7dfc7705/src/uproot/reading.py#L169-L178",
  "closed_at":"2021-11-07T12:31:42Z",
  "comments":5,
  "created_at":"2021-10-26T16:08:14Z",
  "id":1036490952,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss49x5jI",
  "number":486,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"XRootD version detection doesn't work for versions before 4.11.1",
  "updated_at":"2021-11-07T12:31:42Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"A simple implementation of `AsDtypeInplace` interpretation.\r\n\r\nThis has been tested on simple input types only (float and int).\r\nThis allows this kind of code to work : \r\n```\r\n        var = np.zeros(N, dtype=np.float32)\r\n        b = uproot.openn('afile.root')['treename']['varname']\r\n        b.array(library='np', interpretation=b.interpretation.fill_inplace(var) )\r\n``` \r\n\r\nIf this is useful, I'm happy to follow any suggestion to get it integrated in the main branc",
  "closed_at":"2021-10-28T17:02:15Z",
  "comments":7,
  "created_at":"2021-10-27T12:17:31Z",
  "draft":false,
  "id":1037350264,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tvOwM",
  "number":487,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-28T17:02:15Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Implement asdtypeinplace",
  "updated_at":"2021-10-28T19:02:07Z",
  "user":"MDQ6VXNlcjEyMTk4Njg="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-10-27T21:36:39Z",
  "comments":0,
  "created_at":"2021-10-27T18:30:05Z",
  "draft":false,
  "id":1037731455,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4twdly",
  "number":488,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-27T21:36:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Bug-fixes in writing TTrees to preexisting files (\"update\" mode).",
  "updated_at":"2021-10-27T21:36:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Add @duncanmmacleod as a contributor for infra.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot4/pull/477#issuecomment-953335567)",
  "closed_at":"2021-10-28T22:02:55Z",
  "comments":0,
  "created_at":"2021-10-27T21:46:44Z",
  "draft":false,
  "id":1037889349,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tw-is",
  "number":489,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-28T22:02:55Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add duncanmmacleod as a contributor for infra",
  "updated_at":"2021-10-28T22:02:56Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Add @mpad as a contributor for code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot4/pull/487#issuecomment-954033707)",
  "closed_at":"2021-10-28T21:59:25Z",
  "comments":0,
  "created_at":"2021-10-28T17:03:08Z",
  "draft":false,
  "id":1038740236,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4tzvfr",
  "number":491,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-10-28T21:59:25Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add mpad as a contributor for code",
  "updated_at":"2021-10-28T21:59:26Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.9b0 \u2192 21.10b0](https://github.com/psf/black/compare/21.9b0...21.10b0)\n- [github.com/asottile/setup-cfg-fmt: v1.18.0 \u2192 v1.19.0](https://github.com/asottile/setup-cfg-fmt/compare/v1.18.0...v1.19.0)\n",
  "closed_at":"2021-11-01T20:45:39Z",
  "comments":0,
  "created_at":"2021-11-01T19:09:21Z",
  "draft":false,
  "id":1041500110,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4t8Yxw",
  "number":493,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-11-01T20:45:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-11-01T20:45:39Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Functions like `uproot.lazy` normally work without specifying the tree name if there is only one TTree in the file, but this doesn't work if there are multiple cycle numbers:\r\n\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'4.1.7'\r\n>>> uproot.lazy('root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/user/miniconda3/envs/iris-hep/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 551, in lazy\r\n    obj = _regularize_object_path(\r\n  File \"/home/user/miniconda3/envs/iris-hep/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 2982, in _regularize_object_path\r\n    raise ValueError(\r\nValueError: TTree object paths must be specified in the 'files' as {\"filenames*.root\": \"path\"} if any files have more than one TTree\r\n\r\n    TTrees: 'Events;75', 'Events;74'\r\n\r\nin file root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root\r\n```\r\n\r\nThis is confusing since you otherwise never have to worry about cycle numbers. For example: `uproot.open('root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root:Events')` doesn't require a cycle number.",
  "closed_at":"2021-11-05T19:19:05Z",
  "comments":3,
  "created_at":"2021-11-03T08:44:42Z",
  "id":1043216065,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4-LjbB",
  "number":494,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Multiple cycle numbers break single TTree detection",
  "updated_at":"2021-11-05T19:19:05Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"```python\r\nimport uproot\r\nimport numpy as np\r\n\r\nhist = np.histogram([], bins=22, range=(0, 22))\r\n# Using to_writable directly to skip writing and reading hist to/from a file\r\nuproot_hist = uproot.writing.to_writable(hist)\r\nhist_hist = uproot_hist.to_hist()\r\n```\r\n\r\nNow doing `np.all(uproot_hist.axis(0).edges() == hist[1])` will give `True`. However `np.all(hist_hist.axes[0].edges == hist[1])` will give `False`. Somehow the edge at index 15 got moved slightly. I also observe this when `uproot_hist` is coming from a file ROOT produced. The same happens also with `to_boost`. If this binning is used for other histograms, it may yield very different results after being filled.\r\nI think `to_hist` and `to_boost` should only change the representation of the histogram, they should not modify the edges or contents.",
  "closed_at":"2021-11-04T14:55:08Z",
  "comments":2,
  "created_at":"2021-11-04T14:32:19Z",
  "id":1044827848,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4-Rs7I",
  "number":495,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Wrong binning obtained from to_hist()/to_boost()",
  "updated_at":"2021-11-04T14:55:09Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hello,\r\nI think I've found an issue with jagged arrays when creating TTrees in version 4.17 of uproot (with version 1.21.0 of Numpy).\r\n\r\nI found this issue when trying to cut and rewrite numpy \"object\" arrays containing arrays of different lengths which were loaded from JaggedArrays initially. I've recreated it below with a quick write, read and re-write example and the output and error message below that (in this example I've created my own version of the tuple for quicker bug testing).\r\n\r\nEssentially, the TTree creates these new \"nVariable\" branches containing information about the length of each array in a jagged array. I didn't expect this behaviour for numpy arrays, as in the [documentation](https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file) it only discusses this for Awkward arrays (but this may not be a bug). When trying to read in and rewrite this example TTree to a new file uproot gives a \"missing branch\" error even though the branch is clearly there. Looking at the error message I guess the \"metadata\" used in mktree is missing these newly created branches, making me wonder whether they are intentional?\r\n\r\nI tried the same example but reading the ntuple in with library=\"ak\" and this produces a new error with `fields of a record must be NumPy types, though the record itself may be in a jagged array` \r\n\r\nCode:\r\n```python\r\nimport numpy as np\r\nimport uproot as up\r\nprint(np.__version__) #1.21\r\nprint(up.__version__) #4.17\r\n\r\n#first going to make a tuple with jagged arrays\r\nwith up.recreate(\"jaggedTest.root\") as w:\r\n    w[\"testTree\"]={\"arr1\":np.array([[4,2,1],[6,432,89,54],[3,2],[88]],dtype=object),\r\n                  \"arr2\":np.array([[21,53,1,45],[34,42,39],[9,1,37,98],[83]],dtype=object),\r\n                  \"arr3\":np.array([34,1012,38,1040]),\r\n                  \"arr4\":np.array([3.4,3.8,10.12,10.40])}\r\n    \r\n    \r\nwith up.open(\"jaggedTest.root:testTree\",library=\"np\") as f:\r\n    print(f.show())\r\n    \r\n#now read it in and write it to new tuple with cuts\r\nwith up.recreate(\"jaggedTestCut.root\") as w:\r\n    intialCheck=True #check to instruct uproot to create the new tree first before extending it\r\n    \r\n    #using iterate as this is where issue was found\r\n    #step size of two so that the above is split into sections\r\n    #cut made as thought this could be issue, it isn't.\r\n    for batch in up.iterate([a+\":testTree\" for a in [\"jaggedTest.root\"]],\r\n        cut=\"arr3>50\",library=\"np\",step_size=2):\r\n        print(\"Printing Batch to Load \",batch)\r\n        #can see that the new \"nVar\" arrays have been created when making the above ntuple\r\n\r\n        if initialCheck:\r\n            #but it's as if it's not even stored in the metadata as can't load the batch in.\r\n            w[\"testTree\"]=batch\r\n            print(w[\"testTree\"])\r\n            initialCheck=False\r\n        else:\r\n            #usually fails here, but was using a ROOT made tuple so no \"nVariables\"\r\n            #until after loading first batch into \"w\"\r\n            w[\"testTree\"].extend(batch)\r\n        \r\n```\r\n\r\nOutput:\r\n\r\n```text\r\n1.21.0\r\n4.1.7\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nnarr1                | int32_t                  | AsDtype('>i4')\r\narr1                 | int64_t[]                | AsJagged(AsDtype('>i8'))\r\nnarr2                | int32_t                  | AsDtype('>i4')\r\narr2                 | int64_t[]                | AsJagged(AsDtype('>i8'))\r\narr3                 | int64_t                  | AsDtype('>i8')\r\narr4                 | double                   | AsDtype('>f8')\r\nNone\r\nPrinting Batch to Load  {'narr1': array([4], dtype=int32), 'arr1': array([array([  6, 432,  89,  54])], dtype=object), 'narr2': array([3], dtype=int32), 'arr2': array([array([34, 42, 39])], dtype=object), 'arr3': array([1012]), 'arr4': array([3.8])}\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-26-d722a337c359> in <module>\r\n     30         if initialCheck:\r\n     31             #but it's as if it's not even stored in the metadata as can't load the batch in.\r\n---> 32             w[\"testTree\"]=batch\r\n     33             print(w[\"testTree\"])\r\n     34         else:\r\n\r\n/.../lib/python3.9/site-packages/uproot/writing/writable.py in __setitem__(self, where, what)\r\n    965         if self._file.sink.closed:\r\n    966             raise ValueError(\"cannot write data to a closed file\")\r\n--> 967         self.update({where: what})\r\n    968 \r\n    969     def __delitem__(self, where):\r\n\r\n/.../lib/python3.9/site-packages/uproot/writing/writable.py in update(self, pairs, **more_pairs)\r\n   1466                 directory = directory[item]\r\n   1467 \r\n-> 1468             uproot.writing.identify.add_to_directory(v, name, directory, streamers)\r\n   1469 \r\n   1470         self._file._cascading.streamers.update_streamers(self._file.sink, streamers)\r\n\r\n/.../lib/python3.9/site-packages/uproot/writing/identify.py in add_to_directory(obj, name, directory, streamers)\r\n    148     if is_ttree:\r\n    149         tree = directory.mktree(name, metadata)\r\n--> 150         tree.extend(data)\r\n    151 \r\n    152     else:\r\n\r\n/.../lib/python3.9/site-packages/uproot/writing/writable.py in extend(self, data)\r\n   1745             **As a word of warning,** be sure that each call to :ref:`uproot.writing.writable.WritableTree.extend` includes at least 100 kB per branch/array. (NumPy and Awkward Arrays have an `nbytes <https://numpy.org/doc/stable/reference/generated/numpy.ndarray.nbytes.html>`__ property; you want at least ``100000`` per array.) If you ask Uproot to write very small TBaskets, it will spend more time working on TBasket overhead than actually writing data. The absolute worst case is one-entry-per-:ref:`uproot.writing.writable.WritableTree.extend`. See `#428 (comment) <https://github.com/scikit-hep/uproot4/pull/428#issuecomment-908703486>`__.\r\n   1746         \"\"\"\r\n-> 1747         self._cascading.extend(self._file, self._file.sink, data)\r\n   1748 \r\n   1749     def show(\r\n\r\n/.../lib/python3.9/site-packages/uproot/writing/_cascadetree.py in extend(self, file, sink, data)\r\n    586                     actual_branches[datum[\"fName\"]] = provided.pop(datum[\"fName\"])\r\n    587                 else:\r\n--> 588                     raise ValueError(\r\n    589                         \"'extend' must be given an array for every branch; missing {0}\".format(\r\n    590                             repr(datum[\"fName\"])\r\n\r\nValueError: 'extend' must be given an array for every branch; missing 'narr1'       \r\n```\r\n",
  "closed_at":"2021-11-05T17:51:03Z",
  "comments":1,
  "created_at":"2021-11-05T12:52:23Z",
  "id":1045819321,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4-Ve-5",
  "number":498,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Writing, Reading then Rewriting Numpy \"Object\"/Jagged arrays produces missing branch error",
  "updated_at":"2021-11-05T17:51:03Z",
  "user":"MDQ6VXNlcjU2MjMyMDA4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-11-05T17:51:04Z",
  "comments":0,
  "created_at":"2021-11-05T16:35:46Z",
  "draft":false,
  "id":1046036390,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4uKIkt",
  "number":499,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-11-05T17:51:03Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Handle arrays passed both explicitly and generated as counters.",
  "updated_at":"2021-11-05T17:51:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-11-05T19:19:05Z",
  "comments":0,
  "created_at":"2021-11-05T18:14:27Z",
  "draft":false,
  "id":1046115127,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4uKY_Y",
  "number":500,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-11-05T19:19:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"uproot.lazy with no TTree specification should pick the one with max cycle.",
  "updated_at":"2021-11-05T19:19:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Resolves #486. Checks for the package name `pyxrootd` if searching for `XRootD` fails.",
  "closed_at":"2021-11-07T12:31:42Z",
  "comments":4,
  "created_at":"2021-11-05T21:18:45Z",
  "draft":false,
  "id":1046251601,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4uK1t_",
  "number":501,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-11-07T12:31:42Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Detect XRootD versions before 4.11.1",
  "updated_at":"2021-11-07T12:31:42Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/PyCQA/isort: 5.9.3 \u2192 5.10.0](https://github.com/PyCQA/isort/compare/5.9.3...5.10.0)\n",
  "closed_at":"2021-11-08T20:15:35Z",
  "comments":0,
  "created_at":"2021-11-08T19:03:04Z",
  "draft":false,
  "id":1047816005,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4uPtgY",
  "number":503,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-11-08T20:15:35Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-11-08T20:15:36Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"There is a `damerau_levenshtein` function being called when we hit a missing key that takes a long time when the number of keys in a file is very large (https://github.com/scikit-hep/uproot4/blob/85f219a36e76dffc18da4756227a7beb760657a0/src/uproot/_util.py#L810-L858).\r\n\r\nSimilarly, `potential_name in directory` is also not the best: \r\nhttps://github.com/scikit-hep/uproot4/blob/85f219a36e76dffc18da4756227a7beb760657a0/src/uproot/reading.py#L1913-L1919\r\n\r\nFor now, we'll have to call `directory.keys()` and store that in a set for valid key lookups.\r\n\r\n_Originally posted by @kratsg in https://github.com/scikit-hep/pyhf/discussions/1687#discussioncomment-1621078_",
  "closed_at":"2024-02-15T15:16:59Z",
  "comments":5,
  "created_at":"2021-11-10T16:41:53Z",
  "id":1050063388,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4-lrIc",
  "number":504,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Slow \"contains\" lookup and error message reporting for files with large amounts of keys",
  "updated_at":"2024-02-15T15:16:59Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'4.1.5'\r\n```\r\n\r\nROOT file in question: https://github.com/scikit-hep/pyhf/blob/master/validation/xmlimport_input/data/example.root\r\n\r\n```python\r\n>>> import uproot\r\n>>> f = uproot.update('data/example.root')\r\n>>> f.keys()\r\n['data;1', 'signal;1', 'background1;1', 'background2;1', 'background1_statUncert;1']\r\n>>> del f['signal']\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/kratsg/.pyenv/versions/pyhf-dev/lib/python3.8/site-packages/uproot/writing/writable.py\", line 972, in __delitem__\r\n    return self._get_del_search(where, False)\r\n  File \"/Users/kratsg/.pyenv/versions/pyhf-dev/lib/python3.8/site-packages/uproot/writing/writable.py\", line 957, in _get_del_search\r\n    return self._del(item, cycle)\r\n  File \"/Users/kratsg/.pyenv/versions/pyhf-dev/lib/python3.8/site-packages/uproot/writing/writable.py\", line 1029, in _del\r\n    self._cascading.freesegments.release(start, stop)\r\n  File \"/Users/kratsg/.pyenv/versions/pyhf-dev/lib/python3.8/site-packages/uproot/writing/_cascade.py\", line 757, in release\r\n    new_slices = self._another_slice(self._data.slices, start, stop)\r\n  File \"/Users/kratsg/.pyenv/versions/pyhf-dev/lib/python3.8/site-packages/uproot/writing/_cascade.py\", line 714, in _another_slice\r\n    raise RuntimeError(\r\nRuntimeError: segment of data to release overlaps one already marked as free: releasing [1778, 2070) but [2045, 5045) is free\r\n```\r\n\r\nWhen re-running a second time, it's ok. Probably just an expectation issue somewhere.",
  "closed_at":"2023-03-09T20:51:10Z",
  "comments":1,
  "created_at":"2021-11-10T21:10:26Z",
  "id":1050312643,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4-mn_D",
  "number":505,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"closed",
  "state_reason":"completed",
  "title":"Unable to delete histogram from a ROOT file",
  "updated_at":"2023-03-09T20:51:10Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"My original plan to \"publish\" them was to do so by adding `@property` accessors without the underscore, but to simply rename might be even better.",
  "closed_at":"2021-12-03T00:19:12Z",
  "comments":4,
  "created_at":"2021-11-12T00:26:12Z",
  "draft":false,
  "id":1051468082,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4ubn44",
  "number":506,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-12-03T00:19:12Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Remove underscore prefixes from data members uproot.model.TTable",
  "updated_at":"2021-12-03T00:19:12Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"We use [this file](https://rebrand.ly/00vvyzg) in our python training (~50 MB, ROOT file).\r\n\r\nWith `uproot3`:\r\n\r\n```python\r\nimport uproot3\r\nuproot3.open(\"https://rebrand.ly/00vvyzg\")[\"b0phiKs\"].pandas.df()\r\n```\r\n\r\nreturns a dataframe in ~10s.\r\n\r\nWith `uproot4`:\r\n\r\n```python\r\nimport uproot\r\nuproot.open(\"https://rebrand.ly/00vvyzg\")[\"b0phiKs\"].arrays(library=\"pd\")\r\n```\r\n\r\ntimes out unsuccessfully after about a minute.\r\n\r\nLoading the file from disk after `wget`ing it works fine in both cases. \r\n\r\nYou can also use the link `https://syncandshare.desy.de/index.php/s/TkCSBQq3QHprFxR/download` (that's what the shortened URL points to) instead to get the same results.\r\n\r\n`uproot3` version: `3.14.4`\r\n`uproot4` version: `4.0.7`\r\nOS: Ubuntu 18.04 bionic\r\n\r\nUnfortunately I don't have time to investigate this further on my own right now, but it would already be interesting if this can be reproduced by others (I tested it with two independent installations).",
  "closed_at":"2022-05-11T20:58:34Z",
  "comments":3,
  "created_at":"2021-11-15T16:03:37Z",
  "id":1053833468,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4-0Dj8",
  "number":507,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"HTTPSource doesn't always switch over to fallback mode.",
  "updated_at":"2022-05-11T20:58:34Z",
  "user":"MDQ6VXNlcjEzNjAyNDY4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n>>>'4.1.8'\r\n```\r\nI get different results from uproot and root_numpy. See below:\r\n\r\n```\r\nimport ROOT as R\r\nimport numpy as np\r\nimport uproot\r\n\r\ntree = uproot.open('EH1.root:IBD;1')\r\nCut1=\"AD==2 & (PromptZ > 0)\"\r\ndata1=tree.arrays([\"EPrompt\"],cut=Cut1, library=\"np\")\r\nprint(data1)\r\nprint(len(data1['EPrompt']))\r\nprint(\"###\")\r\nimport root_numpy \r\nF=R.TFile(\"EH1.root\")\r\ntree=F.Get(\"IBD;1\")\r\nCut2=\"AD==2 && (PromptZ > 0)\"\r\ndata2=root_numpy.tree2array(tree,\r\n        branches=['EPrompt'],\r\n        selection=Cut2,\r\n        )\r\nprint(data2)\r\nprint(len(data2))\r\n```\r\n\r\nresults:\r\n```\r\n{'EPrompt': array([], dtype=float64)}\r\n0\r\n###\r\n[(0.4171901 ,) (0.52765977,) (2.66819382,) ... (0.83053756,) (0.84282863,)\r\n (0.35761279,)]\r\n18052763\r\n```\r\n\r\nWhy uproot can not get the right array? Cut1 is for uproot and Cut2 is for root_numpy.\r\n",
  "closed_at":"2021-11-16T14:46:36Z",
  "comments":3,
  "created_at":"2021-11-15T16:40:21Z",
  "id":1053873151,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4-0NP_",
  "number":508,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Get Wrong Branches from a tree",
  "updated_at":"2021-11-16T14:46:37Z",
  "user":"MDQ6VXNlcjQwODc1NDc0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/PyCQA/isort: 5.10.0 \u2192 5.10.1](https://github.com/PyCQA/isort/compare/5.10.0...5.10.1)\n",
  "closed_at":"2021-11-15T20:55:31Z",
  "comments":0,
  "created_at":"2021-11-15T19:13:57Z",
  "draft":false,
  "id":1054012560,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4ujOdP",
  "number":509,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-11-15T20:55:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-11-15T20:55:32Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi there. I am attempting to pull all branches of a tree in a root dataset of about 20 files of ~6GB total into a pandas dataframe. The machine I am using has 64G of memory so I assumed it would be able to handle this no problem. However I am about 30 minutes into the concatenate command and currently sat around ~55GB of memory used. Do you have any advice as to making handling large datasets in uproot more performant? I am using a chunksize of 1024.\r\n\r\nPlatform: Arch linux\r\nPython version: 3.9.7\r\nUproot version; 4.1.8\r\n\r\nEdit: after around 45 minutes the script exited with sigkill 9 so I assume it hit the memory limit and crashed",
  "closed_at":"2023-10-05T14:19:24Z",
  "comments":4,
  "created_at":"2021-11-20T11:04:55Z",
  "id":1059120295,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4_IOSn",
  "number":511,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Performance issues with concatenate() on large datasets with multiple files",
  "updated_at":"2023-10-05T14:19:24Z",
  "user":"MDQ6VXNlcjIwOTExOTg3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.10b0 \u2192 21.11b1](https://github.com/psf/black/compare/21.10b0...21.11b1)\n- [github.com/asottile/setup-cfg-fmt: v1.19.0 \u2192 v1.20.0](https://github.com/asottile/setup-cfg-fmt/compare/v1.19.0...v1.20.0)\n",
  "closed_at":"2021-11-22T21:09:05Z",
  "comments":0,
  "created_at":"2021-11-22T19:24:15Z",
  "draft":false,
  "id":1060513696,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4u3s5w",
  "number":512,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-11-22T21:09:05Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-11-22T21:09:06Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hello!\r\n\r\nI am having problems trying to read a Math::XYZVector object stored in this TTree [TAtest2.root.txt](https://github.com/scikit-hep/uproot4/files/7583947/TAtest2.root.txt). \r\n\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'4.0.0'\r\n>>> file = uproot.open(\"TAtest2.root\")\r\n>>> trkana = file['TrkAnaNeg/trkana']\r\n>>> trkana['demcent/_mom'].array()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/interpretation/numerical.py in basket_array(self, data, byte_offsets, basket, branch, context, cursor_offset, library)\r\n    327         try:\r\n--> 328             output = data.view(dtype).reshape((-1,) + shape)\r\n    329         except ValueError:\r\n\r\nValueError: When changing to a larger dtype, its size must be a divisor of the total size in bytes of the last axis of the array.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-fc2b20effd04> in <module>\r\n----> 1 trkana['demcent/_mom'].array()\r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in array(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library)\r\n   2057                         ranges_or_baskets.append((branch, basket_num, range_or_basket))\r\n   2058 \r\n-> 2059         _ranges_or_baskets_to_arrays(\r\n   2060             self,\r\n   2061             ranges_or_baskets,\r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in _ranges_or_baskets_to_arrays(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays)\r\n   3428 \r\n   3429         elif isinstance(obj, tuple) and len(obj) == 3:\r\n-> 3430             uproot.source.futures.delayed_raise(*obj)\r\n   3431 \r\n   3432         else:\r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/source/futures.py in delayed_raise(exception_class, exception_value, traceback)\r\n     44         exec(\"raise exception_class, exception_value, traceback\")\r\n     45     else:\r\n---> 46         raise exception_value.with_traceback(traceback)\r\n     47 \r\n     48 \r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in basket_to_array(basket)\r\n   3375             basket_arrays = branchid_arrays[branch.cache_key]\r\n   3376 \r\n-> 3377             basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n   3378                 basket.data,\r\n   3379                 basket.byte_offsets,\r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/interpretation/numerical.py in basket_array(self, data, byte_offsets, basket, branch, context, cursor_offset, library)\r\n    328             output = data.view(dtype).reshape((-1,) + shape)\r\n    329         except ValueError:\r\n--> 330             raise ValueError(\r\n    331                 \"\"\"basket {0} in tree/branch {1} has the wrong number of bytes ({2}) \"\"\"\r\n    332                 \"\"\"for interpretation {3}\r\n\r\nValueError: basket 0 in tree/branch /TrkAnaNeg/trkana;1:demcent/_mom has the wrong number of bytes (3232) for interpretation AsStridedObjects(Model_ROOT_3a3a_Math_3a3a_DisplacementVector3D_3c_ROOT_3a3a_Math_3a3a_Cartesian3D_3c_float_3e2c_ROOT_3a3a_Math_3a3a_DefaultCoordinateSystemTag_3e__v1)\r\nin file TAtest2.root.txt\r\n```\r\n\r\nDo you know if there is a way to interpret this kind of object correctly, since it is now the recommended alternative to TVector3 (https://root.cern.ch/doc/master/classTVector3.html)? Thanks!",
  "closed_at":"2023-03-27T19:18:09Z",
  "comments":6,
  "created_at":"2021-11-22T19:52:46Z",
  "id":1060534864,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4_NnpQ",
  "number":513,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Problem with interpreting XYZVector",
  "updated_at":"2023-03-27T19:18:09Z",
  "user":"MDQ6VXNlcjY1MzUyNTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Writing a dataframe containing a `Categorical` column to a ROOT TTree raises an `AttributeError` (see MWE below). I guess this is not supported. I don't know if ROOT TTrees even have an equivalent datatype. Of course I can still as a user convert the columns into integer or something and then write the resulting dataframe.\r\n\r\nI saw that with a string object columns, we get:\r\n\r\n``` traceback\r\nNotImplementedError: array of strings\r\n```\r\n\r\nWith this issue I ask to maybe raise a more helpful `NotImplementedError` for Categorical axes. It would also be ideal to support writing those datatypes, though I assume that's not trivial and probably would also require implementing string axes etc.\r\n\r\nWhile playing around with examples for this issue, I also tried the experimental `StringArray` datatype (`dtype=\"string\"`) and is also raises the same `AttributeError` instead of a `NotImplementedError`, so maybe I can include that in this issue as well. \r\n\r\nHere is my MWE to reproduce the error when writing TTrees from Categorical columns:\r\n\r\n``` python\r\nimport uproot\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({0: pd.Categorical([\"a\", \"a\", \"b\", \"c\", \"c\", \"c\"])})\r\nf = uproot.recreate(\"/tmp/cat.root\")\r\nf[\"tree\"] = df\r\n```\r\nwhich results in the traceback:\r\n``` traceback\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 <ipython-input-10-bc1c7ac44da4>:1 in <module>                                                    \u2502\r\n\u2502 /home/michael/.local/lib/python3.9/site-packages/uproot/writing/writable.py:967 in __setitem__   \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    964 \u2502   def __setitem__(self, where, what):                                                   \u2502\r\n\u2502    965 \u2502   \u2502   if self._file.sink.closed:                                                        \u2502\r\n\u2502    966 \u2502   \u2502   \u2502   raise ValueError(\"cannot write data to a closed file\")                        \u2502\r\n\u2502 \u2771  967 \u2502   \u2502   self.update({where: what})                                                        \u2502\r\n\u2502    968 \u2502                                                                                         \u2502\r\n\u2502    969 \u2502   def __delitem__(self, where):                                                         \u2502\r\n\u2502    970 \u2502   \u2502   if self._file.sink.closed:                                                        \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /home/michael/.local/lib/python3.9/site-packages/uproot/writing/writable.py:1468 in update       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1465 \u2502   \u2502   \u2502   for item in path:                                                             \u2502\r\n\u2502   1466 \u2502   \u2502   \u2502   \u2502   directory = directory[item]                                               \u2502\r\n\u2502   1467 \u2502   \u2502   \u2502                                                                                 \u2502\r\n\u2502 \u2771 1468 \u2502   \u2502   \u2502   uproot.writing.identify.add_to_directory(v, name, directory, streamers)       \u2502\r\n\u2502   1469 \u2502   \u2502                                                                                     \u2502\r\n\u2502   1470 \u2502   \u2502   self._file._cascading.streamers.update_streamers(self._file.sink, streamers)      \u2502\r\n\u2502   1471                                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /home/michael/.local/lib/python3.9/site-packages/uproot/writing/identify.py:79 in                \u2502\r\n\u2502 add_to_directory                                                                                 \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502     76 \u2502   \u2502   \u2502   module_name = type(branch_array).__module__                                   \u2502\r\n\u2502     77 \u2502   \u2502   \u2502                                                                                 \u2502\r\n\u2502     78 \u2502   \u2502   \u2502   if module_name == \"pandas\" or module_name.startswith(\"pandas.\"):              \u2502\r\n\u2502 \u2771   79 \u2502   \u2502   \u2502   \u2502   branch_array = uproot.writing._cascadetree.dataframe_to_dict(             \u2502\r\n\u2502     80 \u2502   \u2502   \u2502   \u2502   \u2502   branch_array                                                          \u2502\r\n\u2502     81 \u2502   \u2502   \u2502   \u2502   )                                                                         \u2502\r\n\u2502     82                                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /home/michael/.local/lib/python3.9/site-packages/uproot/writing/_cascadetree.py:1477 in          \u2502\r\n\u2502 dataframe_to_dict                                                                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1474 \u2502   \"\"\"                                                                                   \u2502\r\n\u2502   1475 \u2502   Converts a Pandas DataFrame into a dict of NumPy arrays for writing.                  \u2502\r\n\u2502   1476 \u2502   \"\"\"                                                                                   \u2502\r\n\u2502 \u2771 1477 \u2502   out = {\"index\": df.index.values}                                                      \u2502\r\n\u2502   1478 \u2502   for column_name in df.columns:                                                        \u2502\r\n\u2502   1479 \u2502   \u2502   out[str(column_name)] = df[column_name].values                                    \u2502\r\n\u2502   1480 \u2502   return out                                                                            \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nAttributeError: 'Categorical' object has no attribute 'index'\r\n\r\n```",
  "closed_at":"2023-10-03T13:18:10Z",
  "comments":3,
  "created_at":"2021-11-30T12:55:00Z",
  "id":1067203348,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4_nDsU",
  "number":516,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Support writing string data (TLeafC) to TTrees",
  "updated_at":"2023-10-03T13:18:11Z",
  "user":"MDQ6VXNlcjUxMjE4MjQ="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"At least we now get NotImplementedError in #516.",
  "closed_at":"2021-11-30T17:30:29Z",
  "comments":0,
  "created_at":"2021-11-30T15:40:08Z",
  "draft":false,
  "id":1067383165,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4vMjFF",
  "number":517,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-11-30T17:30:29Z"
  },
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"closed",
  "state_reason":null,
  "title":"Be more careful about identifying pd.DataFrame.",
  "updated_at":"2021-11-30T17:30:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This one: https://pypi.org/project/deflate/\r\n\r\n@Moelf and @aminnj have shown that using this third-party library gives a 30% performance improvement. Uproot and UnROOT used to have about the same speed, but replacing standard \"zlib\" with \"deflate\" in UnROOT sped it up by 30%. Compare Uproot:\r\n\r\n![image](https://user-images.githubusercontent.com/1852447/144336866-cb7caa42-bf08-4606-8dfc-733c1446f0aa.png)\r\n\r\nwith UnROOT:\r\n\r\n![image3](https://user-images.githubusercontent.com/1852447/144338067-92365ec8-b80d-4505-ba4f-a930524b1ec6.png)\r\n\r\n(Cross-reference to an UnROOT issue?)",
  "closed_at":null,
  "comments":4,
  "created_at":"2021-12-02T00:45:10Z",
  "id":1069009529,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4_t8p5",
  "number":518,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Include an option to use the \"deflate\" library instead of Python's built-in \"zlib\"",
  "updated_at":"2024-01-30T15:42:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Dear `uproot4` developers,\r\n\r\nthis PR removes unnecessary(?) copies of the `numpy.memmap`. This should be safe, as `self._file` is only opened in `r`-mode, see: https://github.com/scikit-hep/uproot4/blob/8ae497a89ec92f06724980fccafa1c4286ce6040/src/uproot/source/file.py#L113 `uproot3` also did not do copies on the `numpy.memmaps`: https://github.com/scikit-hep/uproot3/blob/master/uproot3/source/memmap.py#L54-L57\r\n\r\nSince this is a very fundamental piece of code I would really appreciate your feedback here!\r\n(I hope I did not overlook a trivial reason why now a copy is needed.)\r\n\r\nThank you very much for reviewing!\r\n\r\nBest, Peter",
  "closed_at":"2021-12-02T21:07:00Z",
  "comments":5,
  "created_at":"2021-12-02T16:55:16Z",
  "draft":false,
  "id":1069774795,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4vUdZ2",
  "number":519,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-12-02T21:07:00Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[MemmapSource] Remove unnecessary(?) copies",
  "updated_at":"2021-12-08T16:29:32Z",
  "user":"MDQ6VXNlcjE4NDYzNTgy"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hello,\r\n\r\nI have been encountering an error when trying to pickle/unpickle some histograms. As an example, I have two scripts: one which creates a histogram and pickles it and the other which opens the file and reads it in. I know I could store these in root files, but since I was manipulating hist objects originally I used pickle for convenience. \r\n\r\nThe odd thing is that opening the pickle file in the same script in which is was created works as expected, but opening it in a different script does not.\r\n\r\nThe first script is:\r\n```\r\nimport ROOT as r\r\nimport uproot\r\nimport pickle\r\n\r\nprint(f\"{uproot.__version__=}\")\r\nh = r.TH1D(\"h\", 'Example histogram', 100,0,1)\r\n\r\nh2 = uproot.from_pyroot(h) #.to_hist() # this was the original version, and the same error occurs when trying to pickle a hist object\r\nprint(h2, type(h2))\r\n\r\nwith open(\"out.pickle\", 'wb') as f:\r\n    pickle.dump(h2, f)\r\n\r\nprint(\"Opening the pickled file in the same script works\")\r\n\r\nwith open(\"out.pickle\", 'rb') as f:\r\n    ding = pickle.load(f)\r\n\r\nprint(ding, type(ding))\r\n```\r\nAnd this works as expected:\r\n```\r\n(vscode) jlab@SB3:~/uproot_issue$ python --version\r\nPython 3.9.7\r\n(vscode) jlab@SB3:~/uproot_issue$ python create_file.py \r\nuproot.__version__='4.1.2'\r\n<TH1D (version 3) at 0x7ffa4eefff10> <class 'uproot.dynamic.Model_TH1D_v3'>\r\nOpening the pickled file in the same script works\r\n<TH1D (version 3) at 0x7ffa4d4f2af0> <class 'uproot.dynamic.Model_TH1D_v3'>\r\n(vscode) jlab@SB3:~/uproot_issue$ \r\n```\r\n\r\nWhen I try to open the same output file in the other script, however:\r\n```\r\nimport pickle\r\nimport uproot\r\n\r\nprint(\"Opening the pickled file in another script does not work\")\r\nwith open(\"out.pickle\", 'rb') as f:\r\n    ding = pickle.load(f)\r\n\r\nprint(ding, type(ding))\r\n```\r\nyields the error:\r\n```\r\n(vscode) jlab@SB3:~/uproot_issue$ python open_file.py \r\nOpening the pickled file in another script does not work\r\nTraceback (most recent call last):\r\n  File \"/home/jlab/uproot_issue/open_file.py\", line 6, in <module>\r\n    ding = pickle.load(f)\r\n  File \"/home/jlab/miniconda3/envs/vscode/lib/python3.9/site-packages/uproot/model.py\", line 1637, in __setstate__\r\n    cls.__bases__ = (\r\nTypeError: __bases__ assignment: 'TAxis' object layout differs from 'DynamicModel'\r\n(vscode) jlab@SB3:~/uproot_issue$ \r\n```\r\n\r\nI have reproduced this on python 3.8 and 3.9, and uproot versions 4.1.2 and 4.1.8. Thank you!\r\n\r\n[example.zip](https://github.com/scikit-hep/uproot4/files/7644580/example.zip)",
  "closed_at":"2021-12-02T23:25:31Z",
  "comments":3,
  "created_at":"2021-12-02T19:33:00Z",
  "id":1069911304,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss4_xY0I",
  "number":520,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"\"'TAxis' object layout differs from 'DynamicModel'\" when unpickleing histograms created using from_pyroot",
  "updated_at":"2021-12-03T19:12:57Z",
  "user":"MDQ6VXNlcjEyNjE2NDU4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-12-02T23:25:31Z",
  "comments":0,
  "created_at":"2021-12-02T21:02:12Z",
  "draft":false,
  "id":1069978330,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4vVIeH",
  "number":521,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-12-02T23:25:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Dynamic classes can't be ABC subclasses, such as Sequence.",
  "updated_at":"2021-12-02T23:25:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/psf/black: 21.11b1 \u2192 21.12b0](https://github.com/psf/black/compare/21.11b1...21.12b0)\n",
  "closed_at":"2021-12-06T21:00:47Z",
  "comments":0,
  "created_at":"2021-12-06T19:32:27Z",
  "draft":false,
  "id":1072528346,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4vdTOD",
  "number":522,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-12-06T21:00:47Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-12-06T21:00:47Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Add @pfackeldey as a contributor for code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot4/pull/519#issuecomment-988970526)",
  "closed_at":"2021-12-08T16:30:15Z",
  "comments":0,
  "created_at":"2021-12-08T16:29:29Z",
  "draft":false,
  "id":1074596371,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4vkGiJ",
  "number":523,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-12-08T16:30:15Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add pfackeldey as a contributor for code",
  "updated_at":"2021-12-08T16:30:16Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"I only updated CI tests, deployments, setup requirements and classifiers, not the code itself.",
  "closed_at":"2021-12-08T19:15:35Z",
  "comments":0,
  "created_at":"2021-12-08T18:19:06Z",
  "draft":false,
  "id":1074690008,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4vkaQf",
  "number":525,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-12-08T19:15:35Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Remove Python 2 and 3.5 support.",
  "updated_at":"2021-12-08T19:15:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"First commit is mostly automated, second commend is hand cleanup for things the first one didn't catch. Many `.format`'s are left, this is just the easy ones that clearly are simpler as f-strings. Manual comparisons with sys.version_info are always preferred, and can be automatically cleaned up, so I had to look around for the try/catch ones. I'll note some cleanups inline.",
  "closed_at":"2021-12-10T17:49:45Z",
  "comments":6,
  "created_at":"2021-12-10T16:34:39Z",
  "draft":false,
  "id":1077027193,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4vsBxO",
  "number":526,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-12-10T17:49:45Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: drop Python 2 syntax",
  "updated_at":"2021-12-10T20:19:44Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This cleans up some flake8 things.\n",
  "closed_at":"2022-07-09T04:07:59Z",
  "comments":6,
  "created_at":"2021-12-10T19:54:20Z",
  "draft":false,
  "id":1077180330,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4vshzH",
  "number":527,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2022-07-09T04:07:59Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: cleanup flake8",
  "updated_at":"2022-07-09T04:08:00Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"You could switch to the standard badge, it's shorter by a bit with 2.7 gone. (![](https://img.shields.io/pypi/pyversions/uproot) will look more like ![](https://img.shields.io/pypi/pyversions/hist)) when the next release happens. Then you don't have to maintain this.",
  "closed_at":"2022-01-04T18:27:29Z",
  "comments":1,
  "created_at":"2021-12-10T20:30:32Z",
  "draft":false,
  "id":1077205840,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4vsnLQ",
  "number":528,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: correct version support badge",
  "updated_at":"2022-01-04T18:27:41Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"_Uproot version: 4.1.8_\r\n\r\nWhen reading a ```TTree``` while using the ```cut``` argument I observe the below crash. \r\n\r\n**MWE**\r\n\r\n```python\r\nimport uproot\r\n\r\nbranches_gen = [ 'event', 'genpart_exphi', 'genpart_exeta', 'genpart_energy' ]\r\ngen_cut = 'genpart_pid == 22'\r\nwith uproot.open('skim.root:HGCalTriggerNtuple') as data:\r\n    for batch in data.iterate(branches_gen, gen_cut, step_size=\"1 kB\", library='pd'):\r\n        print(batch)\r\n```\r\n\r\n**Input data** \r\n\r\n[skim.txt](https://github.com/scikit-hep/uproot4/files/7718745/skim.txt) (```ROOT``` file)\r\n\r\n**Output**\r\n\r\n```bash\r\n$ python ~/test.py \r\nTraceback (most recent call last):\r\n  File \"/home/bruno/test.py\", line 6, in <module>\r\n    for batch in data.iterate(branches_gen, gen_cut, step_size=\"1 kB\", library='pd'):\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 1387, in iterate\r\n    output = language.compute_expressions(\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/uproot/language/python.py\", line 480, in compute_expressions\r\n    output[name] = output[name][cut]\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/pandas/core/series.py\", line 962, in __getitem__\r\n    key = check_bool_indexer(self.index, key)\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/pandas/core/indexing.py\", line 2385, in check_bool_indexer\r\n    result = result.reindex(index)\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/pandas/core/series.py\", line 4580, in reindex\r\n    return super().reindex(index=index, **kwargs)\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/pandas/core/generic.py\", line 4818, in reindex\r\n    return self._reindex_axes(\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/pandas/core/generic.py\", line 4834, in _reindex_axes\r\n    new_index, indexer = ax.reindex(\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/pandas/core/indexes/multi.py\", line 2533, in reindex\r\n    target = MultiIndex.from_tuples(target)\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/pandas/core/indexes/multi.py\", line 202, in new_meth\r\n    return meth(self_or_cls, *args, **kwargs)\r\n  File \"/home/bruno/miniconda3/envs/FPGA/lib/python3.9/site-packages/pandas/core/indexes/multi.py\", line 553, in from_tuples\r\n    arrays = list(lib.tuples_to_object_array(tuples).T)\r\n  File \"pandas/_libs/lib.pyx\", line 2919, in pandas._libs.lib.tuples_to_object_array\r\nValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'\r\n```\r\n\r\nPerforming a standard ```pandas``` selection works:\r\n\r\n```python\r\nimport uproot\r\n\r\nbranches_gen = [ 'event', 'genpart_pid', 'genpart_exphi', 'genpart_exeta', 'genpart_energy' ]\r\nwith uproot.open('~/Downloads/skim.root:HGCalTriggerNtuple') as data:\r\n    for batch in data.iterate(branches_gen, step_size=\"500 kB\", library='pd'):\r\n        print(batch[ batch['genpart_pid']==22 ])\r\n```\r\n\r\n**Note**\r\n\r\nAlthough [the documentation](https://uproot.readthedocs.io/en/latest/basic.html#computing-expressions-and-cuts) clearly states that we should not expect both snippets above to have different performances, I could find no comment regarding memory usage. When using ```cut```, are the non-matching data loaded anyways?",
  "closed_at":"2021-12-15T17:16:40Z",
  "comments":1,
  "created_at":"2021-12-15T11:16:17Z",
  "id":1080912287,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5AbWmf",
  "number":529,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Filtering TTrees with the ```cut``` argument",
  "updated_at":"2021-12-15T17:19:00Z",
  "user":"MDQ6VXNlcjIwNzAzOTQ3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2021-12-15T17:16:41Z",
  "comments":0,
  "created_at":"2021-12-15T17:07:55Z",
  "draft":false,
  "id":1081280789,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4v5xuY",
  "number":530,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-12-15T17:16:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"Corner case: cut string is a Pandas MultiIndex but column to cut isn't.",
  "updated_at":"2021-12-15T17:16:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"I was wondering if there is a way to figure out that a ROOT file was not closed properly, or even some ways to recover the data inside. This was already discussed before, e.g. here: https://github.com/scikit-hep/uproot3/issues/472 (funnily the issue was raised by a member of our collaboration, apparently we have some issues with improperly closed files `;)`) and of course, data recovery might be very painful (regex search for `TKey` etc.) and probably not worth the effort to implement it in `uproot` but I thought I revive this quickly with an example file, which actually **is** readable with our ROOT-based frameworks.\r\n\r\nI think it would be helpful if at least some kind of an error was shown that the file is in a dangerous (non-closed) state, instead of silently opening it and not listing any keys.\r\n\r\nhttp://131.188.161.12:30002/not_properly_closed_file.root\r\n\r\n```\r\n$ root.exe not_properly_closed_file.root\r\n   ------------------------------------------------------------------\r\n  | Welcome to ROOT 6.22/06                        https://root.cern |\r\n  | (c) 1995-2020, The ROOT Team; conception: R. Brun, F. Rademakers |\r\n  | Built for linuxx8664gcc on Mar 01 2021, 11:20:55                 |\r\n  | From tags/v6-22-06@v6-22-06                                      |\r\n  | Try '.help', '.demo', '.license', '.credits', '.quit'/'.q'       |\r\n   ------------------------------------------------------------------\r\n\r\nroot [0]\r\nAttaching file not_properly_closed_file.root as _file0...\r\nWarning in <TFile::Init>: file not_properly_closed_file.root probably not closed, trying to recover\r\nInfo in <TFile::Recover>: not_properly_closed_file.root, recovered key TTree:E at address 59282385\r\nWarning in <TFile::Init>: successfully recovered 1 keys\r\n\r\nroot [1] _file0->TestBit(TFile::kRecovered)\r\n(bool) true\r\n```\r\n\r\nAs seen above, the `...->TestBit(TFile::kRecovered)` method can be used to check if the file was recovered (the bit `kRecovered` is defined here:\r\n\r\nhttps://github.com/root-project/root/blob/47f66c57ca0a657942a9075030f89d9da885b83a/io/io/inc/TFile.h#L183)\r\n\r\nand the corresponding `Recover` method here:\r\n\r\nhttps://github.com/root-project/root/blob/47f66c57ca0a657942a9075030f89d9da885b83a/io/io/src/TFile.cxx#L1939\r\n\r\n`uproot` v4.1.9 is not crashing but also does not seem to get any data out of it, nor does it report the erroneous state:\r\n\r\n```\r\n>>> import uproot\r\n\r\n>>> uproot.__version__\r\n'4.1.9'\r\n\r\n>>> f = uproot.open(\"/sps/km3net/users/heijboer/issue_6/v6.3/mcv6.3.gsg_numu-CCHEDIS_1e2-1e8GeV.1.root\")\r\n\r\n>>> f.keys()\r\n[]\r\n\r\n>>> f.classnames()\r\n{}\r\n\r\n>>> f._fSeekKeys\r\n0\r\n\r\n>>> f._keys\r\n[]\r\n\r\n>>> f._fSeekParent\r\n0\r\n```\r\n\r\nBtw. with `UnROOT.jl` we get a runaway cursor ;)\r\n```\r\njulia> using UnROOT\r\n\r\njulia> f = UnROOT.ROOTFile(\"not_properly_closed_file.root\")\r\nERROR: EOFError: read end of file\r\n```",
  "closed_at":null,
  "comments":2,
  "created_at":"2021-12-21T09:40:21Z",
  "id":1085629613,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5AtWSt",
  "number":531,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"open",
  "state_reason":null,
  "title":"Recovering or signalling improperly closed files",
  "updated_at":"2021-12-22T02:10:31Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"updates:\n- [github.com/pre-commit/pre-commit-hooks: v4.0.1 \u2192 v4.1.0](https://github.com/pre-commit/pre-commit-hooks/compare/v4.0.1...v4.1.0)\n",
  "closed_at":"2021-12-27T23:13:05Z",
  "comments":0,
  "created_at":"2021-12-27T19:28:03Z",
  "draft":false,
  "id":1089432876,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss4wUEWp",
  "number":532,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2021-12-27T23:13:05Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"[pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2021-12-27T23:13:06Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 }
]
[
 {
  "author_association":"NONE",
  "body":"Finding this issue from [here](https://stackoverflow.com/questions/66497060/reading-a-branch-of-th1ds).\r\nIs it still the case that uproot is unable to read a branch of TH1Ds?\r\nAre there any workarounds that do not involve reading the files with ROOT to extract the TH1D information (which defeats the point of using uproot in the first place)?",
  "created_at":"2023-07-14T13:26:00Z",
  "id":1635864586,
  "issue":38,
  "node_id":"IC_kwDOD6Q_ss5hgVAK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-14T13:26:00Z",
  "user":"MDQ6VXNlcjk1NzE1NDQ="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't think it's `TH1D` specifically; one of our unit tests reads a TTree TBranch of `TH1F`.\r\n\r\nI don't know what determines whether ROOT writes the objects with memberwise splitting or not, but we can support one and not the other. (Memberwise splitting is very different and will require another deep dive into reverse-engineering the binary format.)",
  "created_at":"2023-07-14T15:29:38Z",
  "id":1636026629,
  "issue":38,
  "node_id":"IC_kwDOD6Q_ss5hg8kF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-14T15:29:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Okay, so I guess for some unknown reason the branch of TH1D is being written with memberwise splitting in the files I'm using.\r\nFor now will have to work with ROOT directly then, at least as far as converting / rewriting into a format uproot can read.",
  "created_at":"2023-07-15T11:34:41Z",
  "id":1636742508,
  "issue":38,
  "node_id":"IC_kwDOD6Q_ss5hjrVs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-15T11:34:41Z",
  "user":"MDQ6VXNlcjk1NzE1NDQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Re-reading this two years later, it looks like it's a solved problem, so I'm closing it. Thanks for reporting!",
  "created_at":"2023-10-05T14:19:24Z",
  "id":1749006181,
  "issue":511,
  "node_id":"IC_kwDOD6Q_ss5oP7dl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T14:19:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi! \r\n\r\nhttps://github.com/scikit-hep/uproot5/pull/940 added support for writing string data (TLeafC).\r\n\r\nYour use case is now working as expected: \r\n\r\n```\r\nimport uproot\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({0: pd.Categorical([\"a\", \"a\", \"b\", \"c\", \"c\", \"c\"])})\r\nf = uproot.recreate(\"cat.root\")\r\nf[\"tree\"] = df\r\n\r\n\r\n>>> nf = uproot.open(\"cat.root\")\r\n>>> f[\"tree\"].keys()\r\n['index', '0']\r\n>>> f[\"tree\"][\"index\"].arrays().show()\r\n[{index: 0},\r\n {index: 1},\r\n {index: 2},\r\n {index: 3},\r\n {index: 4},\r\n {index: 5}]\r\n>>> f[\"tree\"][\"0\"].arrays().show()\r\n[{'0': 'a'},\r\n {'0': 'a'},\r\n {'0': 'b'},\r\n {'0': 'c'},\r\n {'0': 'c'},\r\n {'0': 'c'}]\r\n```",
  "created_at":"2023-10-03T13:18:10Z",
  "id":1744962360,
  "issue":516,
  "node_id":"IC_kwDOD6Q_ss5oAgM4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-03T13:18:10Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Closed in favor of #967 ",
  "created_at":"2023-10-04T19:35:44Z",
  "id":1747517535,
  "issue":692,
  "node_id":"IC_kwDOD6Q_ss5oKQBf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-04T19:35:44Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"To add to the above, it only touches a few files used by the rest of Uproot:\r\n\r\n   * src/uproot/const.py: adds a few new constants and a dependency on the `struct` Python standard library module\r\n   * src/uproot/writing/_cascade.py: adds a new function, `add_ntuple` (maybe the name should be `add_rntuple`? There's already a `TNTuple`, and we don't mean that.)\r\n   * src/uproot/writing/writable.py: adds new methods, a new class, a new (hidden) attribute, and an `elif` case to `WritableDirectory._get`.\r\n\r\nNone of this should cause any problems for current users of Uproot.",
  "created_at":"2023-01-05T21:36:58Z",
  "id":1372807265,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5R02Bh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-05T21:36:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"seems weird\r\n```\r\ntests/test_0791-protect-uproot-project_columns-from-dask-node-names.py::test - AssertionError: assert \r\n```\r\n\r\nonly happens on >3.7 ",
  "created_at":"2023-01-06T01:03:40Z",
  "id":1372999162,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5R1k36",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-06T01:03:40Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"dask-awkward only runs in Python 3.7+. However, it shouldn't fail. I'm going to run the tests in `main`: https://github.com/scikit-hep/uproot5/actions/runs/3851990477\r\n\r\nWe'll see if it's broken overall or only in this branch.",
  "created_at":"2023-01-06T02:11:38Z",
  "id":1373049402,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5R1xI6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-06T02:11:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"looks like manually triggered CI also failed on `main`",
  "created_at":"2023-01-06T17:45:46Z",
  "id":1373949694,
  "issue":705,
  "node_id":"IC_kwDOD6Q_ss5R5M7-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-06T17:45:46Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"NONE",
  "body":"Are there any updates about that bug?",
  "created_at":"2023-07-26T11:55:39Z",
  "id":1651661362,
  "issue":709,
  "node_id":"IC_kwDOD6Q_ss5icloy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-26T11:55:39Z",
  "user":"MDQ6VXNlcjE1OTc4ODIz"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"HI @JohanWulff !\r\n\r\nWere the two files provided as input written with `uproot`? If yes, could you please elaborate on how these files were generated. \r\n\r\nWhen reading the second file with ROOT, the data comes out as 0.0, which suggests that the input files are not correct. \r\n\r\n```\r\n>>> g = uproot.open(\"FDF4838A-7644-014B-B2CD-1B2747CC43C3_MA.root\")\r\n>>> g['tout']['Muon_pt'].array()\r\n<Array [32.5, 37.4, 34.1] type='3 * float64'>\r\n>>> \r\n>>> file = ROOT.TFile(\"FDF4838A-7644-014B-B2CD-1B2747CC43C3_MA.root\")\r\n>>> tree = file.Get(\"tout\")\r\n>>> for x in tree:\r\n...     print(x.Muon_pt)\r\n... \r\n0.0\r\n0.0\r\n0.0\r\n>>> \r\n```",
  "created_at":"2023-04-06T16:34:26Z",
  "id":1499334014,
  "issue":756,
  "node_id":"IC_kwDOD6Q_ss5ZXgV-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-06T16:34:26Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"thanks for investigating this! Since i had some manual implementations for awkward-forth for exactly these branches i tried to compare the generated code with what i had. There are a couple of issues:\r\n\r\nFor `TruthBosonAuxDyn.childLinks` and also `TruthTopAuxDyn.parentLinks` the Forth Machine isn't even created since it tries to define `output node1-offsets` twice - which also seems to be described in the error message \"... user-defined words must all be unique ...\"\r\n\r\nFor the other two branches it seems to fail only later when running the machine. Here i could identify 2 issues - which i'll comment in the following generated code (i got this by printing `context[\"forth\"].vm.decompiled` in pdb):\r\n\r\n```\r\ninput stream\r\ninput byteoffsets\r\ninput bytestops\r\noutput node0-offsets int64\r\noutput node1-offsets int64\r\noutput node6-data uint32\r\noutput node7-data uint32\r\n\r\n0\r\nnode0-offsets <- stack\r\n0\r\nnode1-offsets <- stack\r\n// there is a stop missing here - one could do `byteoffsets len 4 /`\r\n0 \r\ndo\r\n  byteoffsets I-> stack\r\n  stream seek\r\n  6\r\n  stream skip\r\n  stream !I-> stack\r\n  dup\r\n  node0-offsets +<- stack\r\n  0\r\n  do\r\n    stream !I-> stack\r\n    dup\r\n    node1-offsets +<- stack\r\n    0\r\n    do\r\n      // in the following block it skips 6 + 4 bytes, but it should skip 20 bytes (seems the header of these objects is that large)\r\n      0\r\n      stream skip\r\n      0\r\n      stream skip\r\n      6\r\n      stream skip\r\n      4\r\n      stream skip\r\n      stream !I-> node6-data\r\n      stream !I-> node7-data\r\n    loop\r\n  loop\r\nloop\r\n```\r\n\r\nRunning it with these fixes it can deserialize all 4 branches. In fact, i expect this to work for all `vector<vector<ElementLink<something>>` :\r\n\r\n```pycon\r\n>>> import numpy as np\r\n>>> import uproot, skhep_testdata\r\n>>> uproot.__version__\r\n'5.0.2'\r\n>>> import awkward as ak\r\n>>> ak.__version__\r\n'2.0.6'\r\n>>> fm_code = \"\"\"\r\ninput stream\r\ninput byteoffsets\r\ninput bytestops\r\noutput node0-offsets int64\r\noutput node1-offsets int64\r\noutput node6-data uint32\r\noutput node7-data uint32\r\n\r\n0\r\nnode0-offsets <- stack\r\n0\r\nnode1-offsets <- stack\r\nbyteoffsets len 4 /\r\n0\r\ndo\r\n  byteoffsets I-> stack\r\n  stream seek\r\n  6\r\n  stream skip\r\n  stream !I-> stack\r\n  dup\r\n  node0-offsets +<- stack\r\n  0\r\n  do\r\n    stream !I-> stack\r\n    dup\r\n    node1-offsets +<- stack\r\n    0\r\n    do\r\n      20\r\n      stream skip\r\n      stream !I-> node6-data\r\n      stream !I-> node7-data\r\n    loop\r\n  loop\r\nloop\r\n\"\"\"\r\n>>> tree = uproot.open(skhep_testdata.data_path(\"uproot-issue-798.root\"))[\"CollectionTree\"]\r\n>>> basket = tree[\"TruthBosonAuxDyn.childLinks\"].basket(0)\r\n>>> inputs = {\r\n    \"stream\": basket.data,\r\n    \"byteoffsets\": basket.byte_offsets[:-1],\r\n    \"bytestops\": basket.byte_offsets[1:]\r\n}\r\n>>> machine = ak.forth.ForthMachine32(fm_code)\r\n>>> machine.run(inputs)\r\n```\r\nThe outputs of this seem to match what the non-forth version spits out\r\n```python\r\ninterpretation = tree[\"TruthBosonAuxDyn.childLinks\"].interpretation\r\ninterpretation._forth = False\r\narray = tree[\"TruthBosonAuxDyn.childLinks\"].array(interpretation)\r\nassert ak.all(array.layout.offsets == machine.outputs[\"node0-offsets\"])\r\nassert ak.all(array.layout.content.offsets == machine.outputs[\"node1-offsets\"])\r\nassert ak.all(np.asarray(array.layout.content.content.content(0)) == machine.outputs[\"node6-data\"])\r\nassert ak.all(np.asarray(array.layout.content.content.content(1)) == machine.outputs[\"node7-data\"])\r\n```",
  "created_at":"2023-02-15T17:38:57Z",
  "id":1431755614,
  "issue":798,
  "node_id":"IC_kwDOD6Q_ss5VVtte",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-15T17:38:57Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi @oksuzian! \r\n\r\nCould you please provide a test file for this issue? I could not replicate it when testing with a 3D array. ",
  "created_at":"2023-02-23T15:15:38Z",
  "id":1441960335,
  "issue":811,
  "node_id":"IC_kwDOD6Q_ss5V8pGP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-23T15:15:38Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"yeah, the `array<>` was not handled before, so need #705",
  "created_at":"2023-01-06T19:23:38Z",
  "id":1374032649,
  "issue":812,
  "node_id":"IC_kwDOD6Q_ss5R5hMJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-06T19:23:38Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Following the page list Envelope link of the same content (single field with int_32 `5,4,3,2,1`)\r\n\r\nROOT reference:\r\n```\r\n# length 60\r\n[1, 0, 1, 0, #Versin, MinVersion of the envelope\r\n#------------------------------- 52 bytes --------------------------\r\n204, 255, 255, 255, 1, 0, 0, 0, #Size = -52, NumElement = 1\r\n#================= 44 bytes =====================\r\n  212, 255, 255, 255, 1, 0, 0, 0, #Size = -44, NumElement = 1\r\n############## 36 bytes ######################\r\n    220, 255, 255, 255, 1, 0, 0, 0, #Size = -36, NumElement = 1\r\n      5, 0, 0, 0, #Page Description Num Elements 0x00000005\r\n      20, 0, 0, 0, #Locator num_bytes\r\n      209, 1, 0, 0, 0, 0, 0, 0, #Locator Offset\r\n      0, 0, 0, 0, 0, 0, 0, 0, # the offset we're missing\r\n      148, 1, 0, 0,  # the compression setting we're missing\r\n############################################\r\n#=========================================\r\n#------------------------------------------------------------------------\r\n142, 11, 229, 80 # CRC32 of the envelope\r\n]\r\n```\r\n\r\nthis PR right now:\r\n```\r\n# length 48\r\n[1, 0, 1, 0, \r\n216, 255, 255, 255, 1, 0, 0, 0, \r\n224, 255, 255, 255, 1, 0, 0, 0, \r\n232, 255, 255, 255, 1, 0, 0, 0, \r\n5, 0, 0, 0, \r\n20, 0, 0, 0, \r\n25, 8, 0, 0, 0, 0, 0, 0, \r\n231, 8, 58, 176]\r\n```\r\n\r\nwe can see that we're missing exactly those 8+4=12 bytes here, although, if we have more than one column group, we'd have more",
  "created_at":"2023-01-07T07:16:06Z",
  "id":1374401167,
  "issue":813,
  "node_id":"IC_kwDOD6Q_ss5R67KP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-07T07:17:24Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"works now, from uproot:\r\n```\r\n[1, 0, 1, 0, \r\n204, 255, 255, 255, 1, 0, 0, 0, \r\n212, 255, 255, 255, 1, 0, 0, 0, \r\n220, 255, 255, 255, 1, 0, 0, 0, \r\n5, 0, 0, 0, \r\n20, 0, 0, 0, \r\n25, 8, 0, 0, 0, 0, 0, 0, \r\n0, 0, 0, 0, 0, 0, 0, 0, \r\n0, 0, 0, 0, \r\n31, 188, 148, 126]\r\n```\r\n\r\n```python\r\n>>> rn = ROOT.Experimental.RNTupleReader.Open(\"ntuple\", \"./test.root\")\r\n[ROOT.NTuple] Warning /code/root_src/tree/ntuple/v7/src/RNTupleSerialize.cxx:1148 in static ROOT::Experimental::RResult<void> ROOT::Experimental::Internal::RNTupleSerializer::DeserializeHeaderV1(const void*, uint32_t, ROOT::Experimental::RNTupleDescriptorBuilder&):0: RuntimeWarning: Pre-release format version: RC 1\r\n>>> rn.PrintInfo()\r\n************************************ NTUPLE ************************************\r\n* N-Tuple : ntuple                                                             *\r\n* Entries : 5                                                                  *\r\n********************************************************************************\r\n* Field 1   : one (std::int32_t)                                               *\r\n********************************************************************************\r\n```\r\n\r\nbut data doesn't come back yet\r\n```python\r\n>>> df = ROOT.RDataFrame(\"ntuple\", \"test.root\")\r\n>>> df.AsNumpy([\"one\"])\r\nError in <TBufferFile::ReadVersion>: Could not find the StreamerInfo with a checksum of 0x1365788d for the class \"ROOT::Experimental::Internal::RFileNTupleAnchor\" in test.root.\r\nError in <TBufferFile::CheckByteCount>: object of class ROOT::Experimental::Internal::RFileNTupleAnchor read too few bytes: 6 instead of 54\r\n *** Break *** segmentation violation\r\n```",
  "created_at":"2023-01-09T16:05:35Z",
  "id":1375866727,
  "issue":813,
  "node_id":"IC_kwDOD6Q_ss5SAg9n",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-09T16:11:23Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"now it works, need to use the special `MakeNTupleDataFrame`\r\n```python\r\n>>> df = ROOT.Experimental.MakeNTupleDataFrame(\"ntuple\", \"./test.root\")\r\n[ROOT.NTuple] Warning /code/root_src/tree/ntuple/v7/src/RNTupleSerialize.cxx:1148 in static ROOT::Experimental::RResult<void> ROOT::Experimental::Internal::RNTupleSerializer::DeserializeHeaderV1(const void*, uint32_t, ROOT::Experimental::RNTupleDescriptorBuilder&):0: RuntimeWarning: Pre-release format version: RC 1\r\n>>> df.AsNumpy([\"one\"])\r\n{'one': ndarray([5, 4, 3, 2, 1], dtype=int32)}\r\n```",
  "created_at":"2023-01-09T16:23:36Z",
  "id":1375902692,
  "issue":813,
  "node_id":"IC_kwDOD6Q_ss5SApvk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-09T16:23:42Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"NONE",
  "body":"I have tested the minor ROOT versions available on `conda-forge` (`6.20.6`, `6.22.8`, `6.24.6`, `6.26.10`). The example above works with the following versions:\r\n```\r\npython==3.8\r\nroot==6.20.10\r\nuproot==5.0.2\r\n```\r\n\r\nIf you are hit by this problem, and downgrading your ROOT is an option, this is a possible work around.\r\n",
  "created_at":"2023-01-10T19:10:49Z",
  "id":1377724854,
  "issue":814,
  "node_id":"IC_kwDOD6Q_ss5SHmm2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-10T19:10:49Z",
  "user":"MDQ6VXNlcjk4NDUwMzQ="
 },
 {
  "author_association":"NONE",
  "body":"Hii mentors, I would like to contribute  on your project .\r\ncan you please sugest some resources  that can help me solve this problem .\r\n",
  "created_at":"2023-01-15T09:06:45Z",
  "id":1383096008,
  "issue":816,
  "node_id":"IC_kwDOD6Q_ss5ScF7I",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-15T09:06:45Z",
  "user":"U_kgDOBogzqg"
 },
 {
  "author_association":"MEMBER",
  "body":"Hi, @GaganMeshram123! We'd be glad to have help. Also, thanks, @TJKhoo, for reporting this bug.\r\n\r\nThe current state of things is that I've been unable to give Uproot as much attention as it deserves (to fix bugs like the above), but @ioanaif is getting up to speed on the codebase and is looking at a few issues, including this one, to figure out how to start contributing. (FYI: @ioanaif developed most of Awkward Array version 2, and is switching to Uproot now that Awkward 2 is done.)\r\n\r\nTo help @ioanaif, but it might also help @GaganMeshram123, I've been writing down everything about the ROOT file format and the Uproot codebase here:\r\n\r\nhttps://github.com/scikit-hep/uproot5/wiki/Guide-to-the-codebase\r\n\r\nand I'm not done. In particular, the AwkwardForth part, which this issue concerns, is not written at the time of this comment.\r\n\r\nI've also pointed @aryan26roy, the author of the AwkwardForth part, to this issue, but he started a new job last week. `:)`\r\n\r\n@TJKhoo is absolutely right that the generation of AwkwardForth for one TBranch should not be affecting any other TBranch. The generated AwkwardForth is saved on the TBranch so that it doesn't need to be generated again, but that ought to be per-TBranch\u2014specifically, the AwkwardForth source code and VM are supposed to be attached to the `AsObjects` interpretation, which is tied to the TBranch through its `interpretation` property. There shouldn't be any interference between the `interpretation` of one TBranch and the `interpretation` of another. Even if two TBranches have the same data type, the AwkwardForth should be regenerated because one might be from one file (with one version of a class streamer) and another might be from a different file (with a different version of the class, and hence different Forth code might be needed to deserialize it). Whatever this problem is, it doesn't get deep into the specifics of how AwkwardForth works; it gets into how generated code gets attached to TBranches/their Interpretations.\r\n\r\n@GaganMeshram123, if you would like to learn about Uproot by trying to solve problems like this one, I can talk you through getting started. (That's what I'm doing a lot of these days: teaching.) Send me an email (https://github.com/jpivarski) and we can Zoom-chat. But we should be sure that we're not duplicating effort. @ioanaif, are you planning on taking this issue? If so, let us know!\r\n\r\nI would rate this one at medium difficulty. It's not a \"shallow\" bug like a missing `\\n` in a print-out or something. The difficulty level would be \"high\" if you had to understand the Forth code, but I'm sure this bug does not depend on the Forth itself, just the Python objects that it's attached to.",
  "created_at":"2023-01-16T16:27:01Z",
  "id":1384291427,
  "issue":816,
  "node_id":"IC_kwDOD6Q_ss5Sgpxj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-16T16:27:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi! I made a note to leave this issue for you guys to look at :) ",
  "created_at":"2023-01-17T07:10:31Z",
  "id":1384927863,
  "issue":816,
  "node_id":"IC_kwDOD6Q_ss5SjFJ3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-17T07:10:31Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the confirmation, @ioanaif!\r\n\r\n@GaganMeshram123, are you interested in looking into this? If so, I'll help you through it. If you want to talk about it \"in person,\" email me to arrange a time and I'll send a Zoom link.",
  "created_at":"2023-01-17T14:16:35Z",
  "id":1385495152,
  "issue":816,
  "node_id":"IC_kwDOD6Q_ss5SlPpw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-17T14:16:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"yes mentor,  I am intrested to solve this problem.",
  "created_at":"2023-01-19T15:20:11Z",
  "id":1397141002,
  "issue":816,
  "node_id":"IC_kwDOD6Q_ss5TRq4K",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-19T15:20:11Z",
  "user":"U_kgDOBogzqg"
 },
 {
  "author_association":"MEMBER",
  "body":"Send me an email at `pivarski@princeton.edu` and we'll arrange a time for a conversation on Zoom. We'll need a back-and-forth conversation to help me understand what you know, what information I can give you about the problem, and we can start the first steps toward figuring it out. (It helps to share screen for something like that.) Also, I'd like to know what your longer-term goals are for working in this area, so that I can help you toward them.",
  "created_at":"2023-01-19T15:56:16Z",
  "id":1397204308,
  "issue":816,
  "node_id":"IC_kwDOD6Q_ss5TR6VU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-19T15:56:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@GaganMeshram123, let me know if you're still interested in working on this. It's okay if you're not, or if you're too busy, or something else is preventing you from getting to it now. I just need to know if someone else (myself, @ioanaif, ...) should plan to take this issue instead. If now is a bad time, I can help you get started on another issue in a few weeks or so when you do have time. (There will _always_ be more issues!)",
  "created_at":"2023-01-23T17:21:28Z",
  "id":1400707435,
  "issue":816,
  "node_id":"IC_kwDOD6Q_ss5TfRlr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-23T17:21:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"BTW, @jpivarski, thought it might be worth mentioning that `uproot.iterate` doesn't run into the issue described here, just in case that helps narrow things down.",
  "created_at":"2023-01-23T17:23:21Z",
  "id":1400710031,
  "issue":816,
  "node_id":"IC_kwDOD6Q_ss5TfSOP",
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "total_count":1
  },
  "updated_at":"2023-01-23T17:23:21Z",
  "user":"MDQ6VXNlcjE1MDk2NDk1"
 },
 {
  "author_association":"NONE",
  "body":"Oh no, I was wrong above :( On closer inspection, exactly the same thing happens with iterate. Sorry for the red herring.",
  "created_at":"2023-01-24T13:59:35Z",
  "id":1401994987,
  "issue":816,
  "node_id":"IC_kwDOD6Q_ss5TkL7r",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-24T13:59:35Z",
  "user":"MDQ6VXNlcjE1MDk2NDk1"
 },
 {
  "author_association":"MEMBER",
  "body":"I can't finish this now, but here's a way to get exactly the data that needs to be written to the TLeafC-type TBasket. Suppose you have an array of strings\r\n\r\n```python\r\narray = ak.Array([\"zero\", \"one\" * 100, \"two\", \"three\" * 100, \"four\", \"five\"])\r\n```\r\n\r\nTwo of these strings have more than 254 bytes and will have a 5-byte size marker: `b\"\\xff\" + int32_to_bigendian_bytes(size)`.\r\n\r\nThe others have fewer than 255, and so they have a 1-byte size marker.\r\n\r\nHere's a way to do it:\r\n\r\n```python\r\nlengths = np.asarray(ak.num(array))\r\nwhich_big = lengths >= 255\r\n\r\nlengths_extension_offsets = np.empty(len(array) + 1, np.int64)\r\nlengths_extension_offsets[0] = 0\r\nnp.cumsum(which_big * 4, out=lengths_extension_offsets[1:])\r\n\r\nlengths_extension = ak.contents.ListOffsetArray(\r\n    ak.index.Index64(lengths_extension_offsets),\r\n    ak.contents.NumpyArray(lengths[which_big].astype(\">u4\").view(\"u1\"))\r\n)\r\n\r\nlengths[which_big] = 255\r\n\r\nleafc_data_awkward = ak.concatenate([\r\n    lengths.reshape(-1, 1).astype(\"u1\"),\r\n    lengths_extension,\r\n    ak.without_parameters(array),\r\n], axis=1)\r\n\r\nleafc_data = np.asarray(ak.flatten(leafc_data_awkward))\r\n```\r\n\r\nNow `leafc_data` has exactly the data one needs for the TLeafC TBasket.\r\n\r\n```\r\n\\x04zero\\xff\\x00\\x00\\x01,oneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneone\r\noneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneo\r\nneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneoneon\r\neoneone\\x03two\\xff\\x00\\x00\\x01\\xf4threethreethreethreethreethreethreethreethreethreethreethreethreethreeth\r\nreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethr\r\neethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethre\r\nethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethree\r\nthreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreethreet\r\nhree\\x04four\\x04five\r\n```\r\n\r\nThat TBasket also has ragged array-style offsets\u2014I think\u2014so there's a little more to do with `lengths`, but not much.\r\n\r\nThis would need to get inserted as a third method like `write_np_basket` and `write_jagged_basket`, but for this string case, and it would have to get called by the appropriate code.\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/d6d311aef4e22cc73ae056665dbb41cc18b2d6ac/src/uproot/writing/_cascadetree.py#L1265-L1402",
  "created_at":"2023-01-26T20:43:19Z",
  "id":1405626000,
  "issue":818,
  "node_id":"IC_kwDOD6Q_ss5TyCaQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-26T20:43:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Implemented in #940 ",
  "created_at":"2023-09-25T16:01:34Z",
  "id":1734042232,
  "issue":818,
  "node_id":"IC_kwDOD6Q_ss5nW2J4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-25T16:01:34Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I just looked through it and the changes are exactly what we talked about in the meeting. Just to be sure I tried to reproduce the error locally and it ran without any errors.",
  "created_at":"2023-01-27T09:47:47Z",
  "id":1406264198,
  "issue":819,
  "node_id":"IC_kwDOD6Q_ss5T0eOG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-01-27T09:47:47Z",
  "user":"MDQ6VXNlcjUwNTc3ODA5"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I should have checked this sooner, but I get now\r\n```\r\n>>> events.MET.pt.compute(scheduler=\"synchronous\")\r\n<Array [78, 24.6, 5.05, 58.4, ..., 35.7, 29.9, 54] type='203939 * float32'>\r\n>>> events.MET.pt.compute(scheduler=\"threads\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/base.py\", line 315, in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/base.py\", line 600, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/threaded.py\", line 89, in get\r\n    results = get_async(\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/local.py\", line 511, in get_async\r\n    raise_exception(exc, tb)\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/local.py\", line 319, in reraise\r\n    raise exc\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/local.py\", line 224, in execute_task\r\n    result = _execute_task(task, data)\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/core.py\", line 119, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/optimization.py\", line 990, in __call__\r\n    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/core.py\", line 149, in get\r\n    result = _execute_task(task, cache)\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/dask/core.py\", line 119, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/_dask.py\", line 592, in __call__\r\n    return self.ttrees[i].arrays(\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 834, in arrays\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3124, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3037, in chunk_to_basket\r\n    basket = uproot.models.TBasket.Model_TBasket.read(\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/model.py\", line 867, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/models/TBasket.py\", line 291, in read_members\r\n    uncompressed = uproot.compression.decompress(\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/compression.py\", line 415, in decompress\r\n    uncompressed_bytestring = decompressor.decompress(\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/compression.py\", line 249, in decompress\r\n    return self.decompressor.decompress(data)\r\n  File \"/Users/ncsmith/src/coffea/.env/lib/python3.10/site-packages/uproot/compression.py\", line 243, in decompressor\r\n    if self._decompressor.obj is None:\r\nAttributeError: '_thread._local' object has no attribute 'obj'\r\n```\r\n\r\nThe issue is that, apparently, each thread has an independent attribute dictionary:\r\n```python\r\n>>> import threading\r\n>>> x = threading.local()\r\n>>> x.obj = None\r\n>>> print(hasattr(x, \"obj\"))\r\nTrue\r\n>>> t = threading.Thread(target=lambda x: print(hasattr(x, \"obj\")), args=(x,))\r\n>>> t.start()\r\nFalse\r\n```",
  "created_at":"2023-02-16T18:28:40Z",
  "id":1433529651,
  "issue":820,
  "node_id":"IC_kwDOD6Q_ss5Vce0z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-16T18:28:40Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I'll prepare a new PR",
  "created_at":"2023-02-16T18:37:57Z",
  "id":1433545894,
  "issue":820,
  "node_id":"IC_kwDOD6Q_ss5Vciym",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-16T18:37:57Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"More generally, how can I specify a type for my array of zero length?\r\n```python\r\nb = ak.ArrayBuilder()\r\nb.begin_list()\r\nfor _ in []:\r\n    b.begin_record()\r\n    b.field(\"x\")\r\n    b.begin_list()\r\n    b.null()\r\n    b.end_list()\r\n    b.end_record()\r\n    raise RuntimeException(\"should not be reached\")\r\nb.end_list()\r\n# b.snapshot()[\"x\"] # ValueError: cannot slice EmptyArray by field name\r\nb.snapshot().type\r\n```\r\n```output\r\n1 * var * unknown\r\n```\r\nThe same principle of building a \"full\" element and slicing it away applies, but would be nice to have a better way. The docs don't seem to mention anything about manual construction/specification of types.",
  "created_at":"2023-03-03T19:26:06Z",
  "id":1454020488,
  "issue":822,
  "node_id":"IC_kwDOD6Q_ss5WqpeI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-03T19:26:06Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"ArrayBuilder doesn't build pre-defined types. LayoutBuilder does, although that's in C++. I think there will someday be a LayoutBuilder implemented in Numba, with a pure-Python implementation for testing workflows in Numba.\r\n\r\nWe've talked about having a general-purpose function for making any array conform to any specified type, but in the meantime, there are specialized functions for it, like [ak.arrays_astype](https://awkward-array.org/doc/main/reference/generated/ak.values_astype.html).\r\n\r\n```python\r\n>>> array = ak.Array([[]])\r\n>>> array\r\n<Array [[]] type='1 * var * unknown'>\r\n\r\n>>> ak.values_astype(array, np.float32, including_unknown=True)\r\n<Array [[]] type='1 * var * float32'>\r\n```\r\n\r\nThe above would turn _all_ leaf-type nodes (unknown and numerical) to `float32`. There isn't a good way to do it for one record branch and not another\u2014you'd have to deconstruct it and reconstruct it with [ak.unzip](https://awkward-array.org/doc/main/reference/generated/ak.unzip.html) and [ak.zip](https://awkward-array.org/doc/main/reference/generated/ak.zip.html).",
  "created_at":"2023-03-03T19:38:14Z",
  "id":1454038477,
  "issue":822,
  "node_id":"IC_kwDOD6Q_ss5Wqt3N",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-03-03T19:38:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"There's also a brief discussion of this in [the user-guide](https://awkward-array.org/doc/main/user-guide/how-to-create-arraybuilder.html#setting-the-type-of-empty-lists), with a suggestion to append a dummy element of the correct type. ",
  "created_at":"2023-03-04T13:26:43Z",
  "id":1454739878,
  "issue":822,
  "node_id":"IC_kwDOD6Q_ss5WtZGm",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-03-04T13:26:43Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Link to LayoutBuilder API docs: https://github.com/scikit-hep/awkward/blob/main/docs/user-guide/how-to-use-header-only-layoutbuilder.md",
  "created_at":"2023-03-04T22:00:21Z",
  "id":1454900705,
  "issue":822,
  "node_id":"IC_kwDOD6Q_ss5WuAXh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-04T22:00:21Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"So it does not cover the dynamic type case?",
  "created_at":"2023-03-04T22:00:50Z",
  "id":1454900836,
  "issue":822,
  "node_id":"IC_kwDOD6Q_ss5WuAZk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-04T22:00:50Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Once this goes in I'll try turning on the test folder and seeing what the flake8-pytest plugin finds.",
  "created_at":"2023-02-16T15:54:08Z",
  "id":1433309964,
  "issue":825,
  "node_id":"IC_kwDOD6Q_ss5VbpMM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-16T15:54:08Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @renyhp for code\r\n\r\nfor finding and providing a fix of this bug: https://github.com/scikit-hep/uproot5/discussions/826#discussioncomment-5010116.",
  "created_at":"2023-02-17T16:41:26Z",
  "id":1434897341,
  "issue":827,
  "node_id":"IC_kwDOD6Q_ss5Vhsu9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-17T16:41:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/829) to add @renyhp! :tada:",
  "created_at":"2023-02-17T16:41:35Z",
  "id":1434897503,
  "issue":827,
  "node_id":"IC_kwDOD6Q_ss5Vhsxf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-17T16:41:35Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to merge this PR because these are two correct fixes\u2014improvements\u2014even if the bug reported in #826 is not completely fixed yet.",
  "created_at":"2023-02-17T19:27:13Z",
  "id":1435134984,
  "issue":827,
  "node_id":"IC_kwDOD6Q_ss5VimwI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-17T19:27:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, with this change my test works.",
  "created_at":"2023-02-16T21:05:27Z",
  "id":1433713436,
  "issue":828,
  "node_id":"IC_kwDOD6Q_ss5VdLsc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-16T21:05:27Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"For eager mode I don't think there's a need for encapsulating this concept. It is already largely done by `ak.from_buffers`, this PR makes it so the correct projection of columns and optimization around it in dask can occur without some really baroque `map_partition` calls.",
  "created_at":"2023-02-17T19:39:28Z",
  "id":1435146387,
  "issue":830,
  "node_id":"IC_kwDOD6Q_ss5VipiT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-17T19:41:25Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"I was on the fence about whether this should be considered a fix or a feature. I guess you're right that it's a feature. It's not something that we had promised before.",
  "created_at":"2023-02-17T19:39:52Z",
  "id":1435146766,
  "issue":830,
  "node_id":"IC_kwDOD6Q_ss5VipoO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-17T19:39:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @lgray for code\r\n",
  "created_at":"2023-03-01T22:48:54Z",
  "id":1450970600,
  "issue":830,
  "node_id":"IC_kwDOD6Q_ss5WfA3o",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-01T22:48:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/847) to add @lgray! :tada:",
  "created_at":"2023-03-01T22:49:03Z",
  "id":1450970751,
  "issue":830,
  "node_id":"IC_kwDOD6Q_ss5WfA5_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-01T22:49:03Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, it's better to reuse `interp_options`, to have fewer \"moving parts\" (i.e. multiple representations of the same data that could, in principle, become inconsistent with each other). So thanks for catching that!",
  "created_at":"2023-02-18T16:02:29Z",
  "id":1435706387,
  "issue":832,
  "node_id":"IC_kwDOD6Q_ss5VkyQT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-18T16:02:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah - sorry this one requires things to get merged in dask-awkward, it is presently not possible in uproot to do it.",
  "created_at":"2023-02-18T15:58:10Z",
  "id":1435705536,
  "issue":834,
  "node_id":"IC_kwDOD6Q_ss5VkyDA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-18T15:58:10Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Looks like the new release of dask-awkward broke some existing tests.",
  "created_at":"2023-03-01T01:45:13Z",
  "id":1449191415,
  "issue":834,
  "node_id":"IC_kwDOD6Q_ss5WYOf3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-01T01:45:13Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski could you re-run the tests on this branch? It should pass now.",
  "created_at":"2023-03-01T20:39:16Z",
  "id":1450814299,
  "issue":834,
  "node_id":"IC_kwDOD6Q_ss5Weatb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-01T20:39:31Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK - was able to remove all the extra bits of paranoia, it works fine without them after a rebase locally.",
  "created_at":"2023-02-21T14:33:25Z",
  "id":1438593340,
  "issue":836,
  "node_id":"IC_kwDOD6Q_ss5VvzE8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-21T14:33:25Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"The file that revealed the issue is private\u2014otherwise, that would have been used as a test.\r\n\r\nHowever, every existing test with `library=\"ak\"` goes through this new code path, so it is being tested in the sense of having full coverage. In fact, the unrelated bug (highlighted above) was discovered because it went through this code path, and in the old system, it somehow managed to avoid the old (late) code path. I'd take that as an indication that coverage has increased.",
  "created_at":"2023-02-21T15:50:02Z",
  "id":1438710469,
  "issue":838,
  "node_id":"IC_kwDOD6Q_ss5VwPrF",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-02-21T15:50:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Looks great! Feel free to merge when you're ready.",
  "created_at":"2023-02-23T18:43:54Z",
  "id":1442259230,
  "issue":840,
  "node_id":"IC_kwDOD6Q_ss5V9yEe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-02-23T18:43:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @ioanaif for code",
  "created_at":"2023-03-01T22:50:52Z",
  "id":1450972289,
  "issue":841,
  "node_id":"IC_kwDOD6Q_ss5WfBSB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-01T22:50:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/848) to add @ioanaif! :tada:",
  "created_at":"2023-03-01T22:51:01Z",
  "id":1450972384,
  "issue":841,
  "node_id":"IC_kwDOD6Q_ss5WfBTg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-01T22:51:01Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"But something happened in the tests. I'm going to see if `main` is affected: https://github.com/scikit-hep/uproot5/actions/runs/4307341032",
  "created_at":"2023-03-01T19:45:54Z",
  "id":1450753540,
  "issue":843,
  "node_id":"IC_kwDOD6Q_ss5WeL4E",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-03-01T19:45:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This issue shows up when the file has `FreeSegmentsData` after the histogram which needs to be deleted. The computation of `stop` might be at fault here (`stop = start + key.num_bytes + key.compressed_bytes`)\r\n\r\nFor the test written in this PR with file `f`:\r\n\r\n{'_path': (), '_file': <WritableFile './hist_del_test.root' at 0x7f9e338ca310>, '_cascading': RootDirectory(Key(100, 80, 80, String(126, 'TFile'), String(132, 'py-fillrandom.root'), String(151, ''), 1, 0, None), String(152, 'py-fillrandom.root'), String(171, ''), DirectoryHeader(172, 100, 72, 7748, 225, 0, UUID('a9f54b5c-b82a-11ed-a3b7-1701a8c0beef')), Key(7748, 173, 173, String(7774, 'TFile'), String(7780, 'py-fillrandom.root'), String(7799, ''), 1, 100, None), DirectoryData(7800, 173, [Key(7804, 92, 92, String(7830, 'TFormula'), String(7839, 'form1'), String(7845, 'abs(sin(x)/x)'), 1, 100, 232), **Key(7859, 538, 269, String(7885, 'TF1'), String(7889, 'sqroot'), String(7896, 'x*gaus(0) + [3]*form1'), 1, 100, 379), Key(7918, 2103, 947, String(7944, 'TH1F'), String(7949, 'h1f'), String(7953, 'Test random numbers'), 1, 100, 707)])**, FreeSegments(Key(28137, 20, 20, String(7999, 'TFile'), String(8005, 'py-fillrandom.root'), String(8024, ''), 1, 100, None), FreeSegmentsData(28189, ((1709, 7748),), 28209), FileHeader(28209, 28137, 72, 1, 72, ZLIB(1), 7973, 20164, UUID('a9f54b5c-b82a-11ed-a3b7-1701a8c0beef')))), '_subdirs': {}}\r\n\r\nIf we want to do  `del f[\"sqroot\"]`:\r\n\r\n`Key(7859, 538, 269, String(7885, 'TF1'), String(7889, 'sqroot'), String(7896, 'x*gaus(0) + [3]*form1'), 1, 100, 379)`\r\n \r\n`uproot.reading._key_format_small.size  = 26`\r\n`self._classname.allocation  = 4`\r\n`self._name.allocation  = 7`\r\n`self._title.allocation  = 22`\r\n \r\n`key.num_bytes = 59 `( = uproot.reading._key_format_small.size  + self._classname.allocation + self._title.allocation)\r\n`key.compressed_bytes  = 269`\r\n\r\n`start  = 7859`\r\n`stop  = 8186` ( = start + key.num_bytes + key.compressed_bytes)\r\n\r\nHowever, after deleting `h1f`, (7918, 7973) is also marked as empty. Thus, we cannot delete (7859, 8186). ",
  "created_at":"2023-03-01T14:45:51Z",
  "id":1450269384,
  "issue":844,
  "node_id":"IC_kwDOD6Q_ss5WcVrI",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-03-02T14:13:45Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"It's ready to be merged!",
  "created_at":"2023-03-09T20:11:59Z",
  "id":1462705243,
  "issue":844,
  "node_id":"IC_kwDOD6Q_ss5XLxxb",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-03-09T20:11:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"can you try 5.0.4?\r\n```python\r\nIn [1]: import uproot\r\n\r\nIn [2]: t= uproot.open(\"test.root:gnnjet\")\r\n\r\nIn [5]: %timeit t.arrays(['nnTruthReco' ])\r\n508 \u00b5s \u00b1 55.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [6]: %timeit t.arrays(['nnRecoReco' ])\r\n13.3 ms \u00b1 39.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [7]: uproot.__version__\r\nOut[7]: '5.0.4'\r\n```\r\n\r\nthe 2x slow down is expected, `nnRecoReco` has 2 baskets instead of 1",
  "created_at":"2023-03-02T15:55:44Z",
  "id":1452099979,
  "issue":850,
  "node_id":"IC_kwDOD6Q_ss5WjUmL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-02T15:56:11Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi,\r\n\r\nThanks for the fast feedback !\r\nI don't have ipython but could update to 5.0.4 : \r\n```python\r\n\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'5.0.4'\r\n>>> t=uproot.open(\"test.root:gnnjet\")\r\n>>> timeit.timeit(\"a=t.arrays(['nnTruthReco' ])\", globals=globals(), number=1)\r\n6.966970829991624\r\n>>> timeit.timeit(\"a=t.arrays(['nnRecoReco' ])\", globals=globals(), number=1)\r\n0.05337693099863827\r\n```  \r\nI see the same behaviour. \r\n\r\nBut I'm not sure I understand your tests : there are many runs (aren't you re-using the cache? ) and a different number of loops... \r\n",
  "created_at":"2023-03-02T16:08:09Z",
  "id":1452124492,
  "issue":850,
  "node_id":"IC_kwDOD6Q_ss5WjalM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-02T16:08:09Z",
  "user":"MDQ6VXNlcjEyMTk4Njg="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"you're right, I think cache is involved:\r\n```python\r\nIn [8]: %timeit uproot.open(\"test.root:gnnjet\").arrays(['nnRecoReco'])\r\n39.5 ms \u00b1 102 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nIn [9]: %timeit uproot.open(\"test.root:gnnjet\").arrays(['nnTruthReco'])\r\n5.11 s \u00b1 66.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nthe different number of loops is just heuristics of `%timeit`, the final number is always correctly averaged.\r\n\r\nbtw I don't see it from a different package just for sanity check:\r\n```julia\r\n# the `[:]` materialize everything\r\njulia> @btime LazyTree(\"/tmp/test.root\", \"gnnjet\", \"nnTruthReco\")[:];\r\n  38.639 ms (441377 allocations: 50.66 MiB)\r\n\r\njulia> @btime LazyTree(\"/tmp/test.root\", \"gnnjet\", \"nnRecoReco\")[:];\r\n  24.760 ms (329552 allocations: 35.29 MiB)\r\n```",
  "created_at":"2023-03-02T16:17:42Z",
  "id":1452143438,
  "issue":850,
  "node_id":"IC_kwDOD6Q_ss5WjfNO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-02T16:21:47Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"The fact that the data types are `std::vector<std::vector<int>>` and the performance differences you're seeing are factors of ~100 makes me suspicious that the fast ones are using the new AwkwardForth optimization and the slow ones are not. I'm looking into it.",
  "created_at":"2023-03-02T16:39:41Z",
  "id":1452176247,
  "issue":850,
  "node_id":"IC_kwDOD6Q_ss5WjnN3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-02T16:39:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm glad the differences in performance were large enough for you to have noticed!\r\n\r\nAlthough the consequences of this are only in performance, not correctness of the output, it resulted in taking a completely different code-path for very similar TBranches, which is not something we want. (It could _lead_ to a subtle bug.)",
  "created_at":"2023-03-02T20:22:34Z",
  "id":1452496682,
  "issue":850,
  "node_id":"IC_kwDOD6Q_ss5Wk1cq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-02T20:22:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hey, glad it could help a little and thanks for the super fast reaction !  ",
  "created_at":"2023-03-02T21:42:21Z",
  "id":1452587519,
  "issue":850,
  "node_id":"IC_kwDOD6Q_ss5WlLn_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-02T21:42:21Z",
  "user":"MDQ6VXNlcjEyMTk4Njg="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"pre-commit.ci autofix",
  "created_at":"2023-03-20T15:02:27Z",
  "id":1476395904,
  "issue":852,
  "node_id":"IC_kwDOD6Q_ss5YAAOA",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2023-03-20T15:02:27Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I compared the last issue of the non-primitive dtypes being passed on to awkward in the test added by this PR with `tests/test_0034-generic-objects-in-ttrees.py::test_jagged_strided_awkward`\r\n\r\nI tracked where the byte order changes. This happens in `interpretation.jagged.final_array()`:\u2028\u2028 \r\n\r\n```\r\nprint(\"content dtype \", cnt['fP/fX'].dtype) => content dtype  >f8\r\ncontent = numpy.empty((before,), self.content.to_dtype)\r\nbefore = 0\r\nfor cnt in contents:\r\n    content[before : before + len(cnt)] = cnt\r\n    before += len(cnt)\r\n\r\nprint(\u201ccontent dtype after \u201c, content['fP/fX'].dtype) => content dtype after  float64\r\n\r\n```\r\n\r\nFor the test I added, in `interpretation.numerical.final_array()` this transformation doesn\u2019t take place so the byteorder is preserved and thus awkward complains about the fact that it gets a non-native dtype. This is solved by adding `output = output.newbyteorder('=')` before `self.hook_before_library_finalize`. \r\n",
  "created_at":"2023-03-21T15:27:10Z",
  "id":1478041140,
  "issue":852,
  "node_id":"IC_kwDOD6Q_ss5YGR40",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-21T15:27:10Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think we've heard before that our documentation on how to write histograms is not as clear / comprehensive as it could be. On `uproot`'s end, I think we need to be clearer that for histograms with weights, third-party libraries are needed, and perhaps be slightly more explicit about the `histogramdd` form.\r\n\r\nThe other part of this improvement is probably to explain how to add weights in `hist`. I'll open another issue there based upon this one?",
  "created_at":"2023-03-09T11:37:00Z",
  "id":1461864750,
  "issue":853,
  "node_id":"IC_kwDOD6Q_ss5XIkku",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-09T11:37:00Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Maybe this Uproot Getting Started Guide could be expanded by a few sentences, with more pointers to the hist documentation. (NumPy histograms can't have uncertainties, so if you're arriving with pre-filled bin values and uncertainties, then you'll need boost-histogram or hist.)\r\n\r\nHowever, an introduction to the task of histogramming with ROOT-file serialization would cross package boundaries significantly. If the Uproot Getting Started Guide had a lot of details about this, it would\r\n\r\n  * get too long: users interested in Uproot topics _other_ than histograms would have more difficulty finding what they need\r\n  * obscure which parts are Uproot and which parts are boost-histogram/hist or other packages\r\n  * easily get out of date, since hist authors will think of updating the hist documentation when they change an interface, but might not even realize there are Uproot docs talking about it.\r\n\r\nI can think of technical solutions to the first (break down \"Getting Started\" into many pages) and third (set up automated testing of the documentation, with good alerting to the authors), but I think a better solution is to rely more on centralized tutorials that cut across package boundaries. That is, instead of an $n^2$ maintenance problem of $n$ package authors documenting the overlaps of all $n$ packages, students interested in a cross-cutting task like histogramming should be directed to a tutorial on histogramming, which uses any packages that are helpful for that task.\r\n\r\nAt our last group meeting, @klieret described efforts on developing a centralized portal for these tutorials, which used to be developed in disparate places but is now being pulled together into one place. I wonder: how is that coming?\r\n\r\nThe easiest documentation for package authors to write is the one thing that has to be on the package-specific site: API references. These task-oriented tutorials are much harder when you're in package-author mode, and most of the ones I've seen are pretty thin. (Uproot's has only one Getting Started Guide and Awkward's has a lot of blank pages.) Rather than swim against that tide of making the package authors do it[^1], let's try to point users toward the centralized tutorial-portal.\r\n\r\n[^1]: This isn't passing off on responsibility. The same people can write good tutorials when they take off their package-author hats and know that what they're writing isn't going to go on their package's documentation site. I wrote a Scikit-HEP tutorial for the HSF/Carpentries because the existing tutorial focused _too much_ on Uproot.",
  "created_at":"2023-03-09T13:14:33Z",
  "id":1462048360,
  "issue":853,
  "node_id":"IC_kwDOD6Q_ss5XJRZo",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-03-09T13:14:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi @jpivarski: I submitted a [GSoC project](https://hepsoftwarefoundation.org/gsoc/2023/proposal_HSFTraining_central_entry_point.html) for that. Honestly completely overwhelmed by the number of applicants, but there are some great ones, so I think this will work out great :)\r\n\r\nThe main idea so far was to develop something to replace the [current HSF training enter](https://hepsoftwarefoundation.org/training/curriculum.html) with something that looks more like [learn.astropy](https://learn.astropy.org/) (but is simply configured with a yaml file, [like the current one](https://github.com/HSF/hsf.github.io/blob/main/_data/training-modules.yaml)). This would give us a lot of space and flexibility to list everything we have in the community.\r\n\r\nHowever, just the other day I was thinking about whether we should also have something _very_ similar to learn.astropy (i.e., a training center generated from a collection of stand-alone notebooks that are each listed individually) just for various scikit-hep howto-guides (!= tutorial) that cover more ground.\r\nBased on the fact that the current best applicants were basically coding a rough version of my first idea within 48h from scratch, I think that both might be in the scope of the project (though I designated it as 'short' = 175h). \r\n\r\nAlternatively, we could simply add a 'scikit-hep howtos' tab (or another button) to the training center (similarly configured with yaml) and list all the howto guides there. I think this might work just as well (and is considerably less work for sure).\r\n\r\nLet me know what you think! (though this might be a discussion that touches more projects and might be better at another place)",
  "created_at":"2023-03-09T15:43:11Z",
  "id":1462280647,
  "issue":853,
  "node_id":"IC_kwDOD6Q_ss5XKKHH",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":2,
   "total_count":2
  },
  "updated_at":"2023-03-09T15:47:56Z",
  "user":"MDQ6VXNlcjEzNjAyNDY4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This is really exciting! In principle, there's not a lot stopping us from using `uproot` in the browser. At this point, it probably makes sense for the upstream dependency (`awkward`) to be distributed on both emscripten-forge and pyodide. We already build pyodide-wheels, but we build the package ourselves. \r\n\r\nUproot itself should be fairly trivial; we don't have any other _required_ binary dependencies. The assumption about various packages that do not exist in pyodide being available is fairly easy to resolve; we can add some guards for emscripten. The challenge will probably be in our optional dependencies used for compression. These are not _really_ optional; most ROOT files are compressed in one form or another. \r\n\r\nI can see uproot-in-browser as a useful thing; a big selling point of pyodide (for me) is that it's very easy to spin up, and is highly portable. If we start packaging uproot and awkward in the aforementioned ecosystems, we can provide a use case for packaging these other wheels (xxhash, lz4, etc).\r\n\r\n",
  "created_at":"2023-03-10T17:25:09Z",
  "id":1464129178,
  "issue":854,
  "node_id":"IC_kwDOD6Q_ss5XRNaa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-10T17:25:09Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"It should work, and I'm all for it, except\u2014how do you end up with ROOT files in the browser? They have to be explicitly copied in or something. Maybe the HTTP backend? They couldn't be very large...",
  "created_at":"2023-03-10T21:03:36Z",
  "id":1464469182,
  "issue":854,
  "node_id":"IC_kwDOD6Q_ss5XSga-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-10T21:03:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If the issue is threading, maybe switching things to `TrivialExecutor` would work.",
  "created_at":"2023-03-10T21:05:52Z",
  "id":1464471577,
  "issue":854,
  "node_id":"IC_kwDOD6Q_ss5XShAZ",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-03-10T21:05:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> If the issue is threading, maybe switching things to TrivialExecutor would work.\r\n\r\nHere's all the code I used in my test jupyterlite notebook:\r\n```py\r\nimport piplite\r\n\r\n%pip install -q numpy pandas pyodide-http\r\n\r\nimport pyodide_http\r\n\r\npyodide_http.patch_all()\r\nfrom http import client\r\nclass HTTPSConnection: pass\r\nclient.HTTPSConnection = HTTPSConnection\r\n\r\n%pip install --no-deps uproot\r\n\r\nimport uproot\r\nuproot.decompression_executor = uproot.source.futures.TrivialExecutor\r\nuproot.interpretation_executor = uproot.source.futures.TrivialExecutor\r\n\r\n\r\nf=uproot.open(\"https://github.com/scikit-hep/scikit-hep-testdata/raw/main/src/skhep_testdata/data/uproot-hepdata-example.root\")\r\n```\r\nand the traceback:\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[3], line 1\r\n----> 1 f=uproot.open(\"https://github.com/scikit-hep/scikit-hep-testdata/raw/main/src/skhep_testdata/data/uproot-hepdata-example.root\")\r\n\r\nFile /lib/python3.10/site-packages/uproot/reading.py:141, in open(path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\r\n    132 if not uproot._util.isstr(file_path) and not (\r\n    133     hasattr(file_path, \"read\") and hasattr(file_path, \"seek\")\r\n    134 ):\r\n    135     raise ValueError(\r\n    136         \"'path' must be a string, pathlib.Path, an object with 'read' and \"\r\n    137         \"'seek' methods, or a length-1 dict of {file_path: object_path}, \"\r\n    138         f\"not {path!r}\"\r\n    139     )\r\n--> 141 file = ReadOnlyFile(\r\n    142     file_path,\r\n    143     object_cache=object_cache,\r\n    144     array_cache=array_cache,\r\n    145     custom_classes=custom_classes,\r\n    146     decompression_executor=decompression_executor,\r\n    147     interpretation_executor=interpretation_executor,\r\n    148     **options,  # NOTE: a comma after **options breaks Python 2\r\n    149 )\r\n    151 if object_path is None:\r\n    152     return file.root_directory\r\n\r\nFile /lib/python3.10/site-packages/uproot/reading.py:581, in ReadOnlyFile.__init__(self, file_path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\r\n    576 self.hook_before_create_source()\r\n    578 Source, file_path = uproot._util.file_path_to_source_class(\r\n    579     file_path, self._options\r\n    580 )\r\n--> 581 self._source = Source(\r\n    582     file_path, **self._options  # NOTE: a comma after **options breaks Python 2\r\n    583 )\r\n    585 self.hook_before_get_chunks()\r\n    587 if self._options[\"begin_chunk_size\"] < _file_header_fields_big.size:\r\n\r\nFile /lib/python3.10/site-packages/uproot/source/http.py:564, in HTTPSource.__init__(self, file_path, **options)\r\n    562 self._fallback_options = options.copy()\r\n    563 self._fallback_options[\"num_workers\"] = self._num_fallback_workers\r\n--> 564 self._open()\r\n\r\nFile /lib/python3.10/site-packages/uproot/source/http.py:567, in HTTPSource._open(self)\r\n    566 def _open(self):\r\n--> 567     self._executor = uproot.source.futures.ResourceThreadPoolExecutor(\r\n    568         [HTTPResource(self._file_path, self._timeout)]\r\n    569     )\r\n\r\nFile /lib/python3.10/site-packages/uproot/source/futures.py:351, in ResourceThreadPoolExecutor.__init__(self, resources)\r\n    349     self._workers.append(ResourceWorker(self._work_queue, resource))\r\n    350 for worker in self._workers:\r\n--> 351     worker.start()\r\n\r\nFile /lib/python3.10/threading.py:928, in Thread.start(self)\r\n    926     _limbo[self] = self\r\n    927 try:\r\n--> 928     _start_new_thread(self._bootstrap, ())\r\n    929 except Exception:\r\n    930     with _active_limbo_lock:\r\n\r\nRuntimeError: can't start new thread\r\n```\r\n\r\nThe thread issue points to `HTTPSource` using `ThreadPoolExecutor`\r\n\r\n> It should work, and I'm all for it, except\u2014how do you end up with ROOT files in the browser? They have to be explicitly copied in or something. Maybe the HTTP backend? They couldn't be very large...\r\n\r\nThere is an interface to fetching content with the browser:\r\nhttps://jupyterlite.readthedocs.io/en/0.1.0-beta/howto/content/python.html#fetching-remote-content\r\n\r\nWe wanted to see if we could give people a quick way to setup a notebook in their browser, serve the ROOT files over HTTP, enable very basic histogramming, plotting and exploration. If the user wanted to do more complicated stuff, they could download and run the notebook somewhere with more power and continue that way, or re-use the template code to build something a bit more substantial.\r\n\r\nThe more obvious solution would be to generate a notebook, have the user download and run it locally, but running in the browser would be interesting to try - even if only just to see how feasible it is.",
  "created_at":"2023-03-10T22:43:46Z",
  "id":1464585846,
  "issue":854,
  "node_id":"IC_kwDOD6Q_ss5XS852",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-03-10T22:47:18Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "author_association":"MEMBER",
  "body":"> It should work, and I'm all for it, except\u2014how do you end up with ROOT files in the browser? They have to be explicitly copied in or something. Maybe the HTTP backend? They couldn't be very large...\r\n\r\nThe HTTP backend is what we're interested in for accessing ntuples that are on grid storage and we already have a service in LHCb which provides short lived EOS tokens as URL parameters to provide authenticiation. I think it's potentially interesting for quick browsing without needing to go the the hassle of setting up anything locally or have dedicated compute resources assigned to a jupyterhub instance.",
  "created_at":"2023-03-10T22:44:54Z",
  "id":1464587650,
  "issue":854,
  "node_id":"IC_kwDOD6Q_ss5XS9WC",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2023-03-10T22:44:54Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ioanaif I'm going to be tinkering with some of this for `awkward`. Does getting `uproot` working on Pyodide/emscripten sound like something you'd be interested in working on too? If not, I can add it to my own to-do list (I'm hoping it won't take too much work!) :)",
  "created_at":"2023-03-11T12:33:36Z",
  "id":1464902478,
  "issue":854,
  "node_id":"IC_kwDOD6Q_ss5XUKNO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-11T12:33:36Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@agoose77 yes, I'll take on the uproot side! ",
  "created_at":"2023-03-13T07:08:50Z",
  "id":1465619937,
  "issue":854,
  "node_id":"IC_kwDOD6Q_ss5XW5Xh",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-03-13T07:08:50Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif did you get this working or should this be re-opened? As far as I can tell there are still more issues to resolve for pyodide.",
  "created_at":"2023-07-27T03:58:36Z",
  "id":1652873342,
  "issue":854,
  "node_id":"IC_kwDOD6Q_ss5ihNh-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-27T03:58:36Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi! \r\n\r\nThe issue reported here `RuntimeError: can't start new thread` was fixed in #868. \r\n\r\nHowever, `uproot` does not work at the moment in jupyterlite because we cannot import 'HTTPSConnection' which seems to be caused by the fact that `hhtp-client` is not available for pyodide yet. \r\n\r\n`ImportError: cannot import name 'HTTPSConnection' from 'http.client' (/lib/python311.zip/http/client.py)`",
  "created_at":"2023-07-27T13:08:31Z",
  "id":1653594665,
  "issue":854,
  "node_id":"IC_kwDOD6Q_ss5ij9op",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-27T13:08:31Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"It seems to be the file itself that's at fault. As a first step, I tried to open it in ROOT:\r\n\r\n```python\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"uproot-issue-858.root\")\r\n```\r\n\r\n```\r\nError in <TFile::Init>: file uproot-issue-858.root is truncated at 25736752 bytes: should be 2186062826, trying to recover\r\nTFile::Init:0: RuntimeWarning: no keys recovered, file has been made a Zombie\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/mambaforge/lib/python3.9/site-packages/ROOT/_pythonization/_tfile.py\", line 55, in _TFileConstructor\r\n    raise OSError('Failed to open file {}'.format(args[0]))\r\nOSError: Failed to open file uproot-issue-858.root\r\n```\r\n\r\nFrom the stack trace that you provided above, it looks like the TFile header is intact, as is the first half of the root TDirectory, but not its second half.\r\n\r\nThe `fEND` field of the TFile header, which says how large ROOT thinks the file is, says that the file is 2186062826 bytes (2084 MB), but the actual size of the file is 25736752 bytes (24 MB).\r\n\r\nI think you just got the first 1.1% of a much larger file, and I think it got this way by being chopped off. Maybe it was a download that failed early, leaving you with only the very beginning of the file. Just enough to be confusing!\r\n\r\nI'm going to close this because it's not indicative of an Uproot bug. If you get the whole file and it still doesn't work, go ahead and open a new issue (because it would be a _different_ issue).",
  "created_at":"2023-03-15T16:22:16Z",
  "id":1470347218,
  "issue":858,
  "node_id":"IC_kwDOD6Q_ss5Xo7fS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-15T16:22:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Cc @mohankarthikg, if you can post a small (< 3 MB), non-private file here, we can add it to our testing suite and ensure that this case will always work in the future. Thanks!",
  "created_at":"2023-03-15T16:08:59Z",
  "id":1470323915,
  "issue":860,
  "node_id":"IC_kwDOD6Q_ss5Xo1zL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-15T16:08:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Not so fast: maybe we just need a list of classes that have this exceptional header behavior? https://github.com/scikit-hep/uproot5/discussions/859#discussioncomment-5325233\r\n\r\n> It seems that this is not one of those examples... or maybe the reason this is a recurring problem is because there are a lot of \"old\" classes? If so, then maybe issue #861 is not the best solution, as it would lead to a lot of duplicated code; maybe we need a list of classes that have headers, despite the general rule that new-style classes do not...",
  "created_at":"2023-03-15T18:18:14Z",
  "id":1470532199,
  "issue":861,
  "node_id":"IC_kwDOD6Q_ss5Xpopn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-15T18:18:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"After a conversation with Philippe, I learned that _anything_ that is unsplit should have these 6-byte headers. TTime is not an exception. For some reason, we had concluded that the existence of these headers is variable (so we have an argument, in some places two for inner/outer, to configure it) with a default of expecting no headers. The existence of any unsplit branches without 6-byte headers should not be expected.\r\n\r\nSo I wonder what would happen if we toggle the default. If we have any failing tests with that, those test failures are counterexamples to the expectation that unsplit branches must have 6-byte headers. One of us should see how many test failures we have and drill down on a representative example file to see if it really is lacking `num_bytes (4), class_version (2)` before each object, or if we're\u2014I don't know\u2014making two mistakes that somehow cancel.",
  "created_at":"2023-03-24T15:33:16Z",
  "id":1483002637,
  "issue":861,
  "node_id":"IC_kwDOD6Q_ss5YZNMN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-03-24T15:33:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@GiovanniVolta encountered this issue and reported it on Gitter:\r\n\r\n> Hi All,\r\n> I received a root file for a Ge spectroscopy with this structure:\r\n> \r\n> ```\r\n> ['Energy;1',\r\n>  'Energy/_F_EnergyCH0@DT5725S_14295;1',\r\n>  'Energy/Calibration_0;1',\r\n>  'Time;1',\r\n>  'Time/_F_TimeCH0@DT5725S_14295;1',\r\n>  'RealTime_0;1',\r\n>  'LiveTime_0;1']\r\n> ```\r\n> \r\n> I would like to get access via uproot to `Energy/Calibration_0`, `RealTime` and `LiveTime` but they have a strange formant and I don't understand how to open them. See here below:\r\n> \r\n> ```\r\n> print(file_HcomappsF['Energy/Calibration_0;1'], file_HcomappsF['RealTime_0;1'], file_HcomappsF['LiveTime_0;1'])\r\n> (<Unknown CalibrationCoefficient at 0x016bda34d250>,\r\n>  <Unknown TTime at 0x016bda328b90>,\r\n>  <Unknown TTime at 0x016bdf24cb90>)\r\n> ```\r\n> \r\n> Any suggestions ? Thank you in advance\r\n\r\nHere is the file (gzipped):\r\n\r\n[HcompassF_226Ra_run_2_20231117_085722.root.gz](https://github.com/scikit-hep/uproot5/files/13478358/HcompassF_226Ra_run_2_20231117_085722.root.gz)",
  "created_at":"2023-11-27T17:29:55Z",
  "id":1828303038,
  "issue":861,
  "node_id":"IC_kwDOD6Q_ss5s-bC-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-27T17:29:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Superseded by #869.",
  "created_at":"2023-04-03T10:00:26Z",
  "id":1494031389,
  "issue":866,
  "node_id":"IC_kwDOD6Q_ss5ZDRwd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-03T10:00:26Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "author_association":"MEMBER",
  "body":"You have my approval here; this only needs the update from #871 to get merged in so that the test passes.",
  "created_at":"2023-04-06T15:58:43Z",
  "id":1499291846,
  "issue":870,
  "node_id":"IC_kwDOD6Q_ss5ZXWDG",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-04-06T15:58:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I made the change to return the correct function based on the version, so that it is less restrictive.",
  "created_at":"2023-04-06T15:51:01Z",
  "id":1499281609,
  "issue":871,
  "node_id":"IC_kwDOD6Q_ss5ZXTjJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-06T15:51:01Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Just these two PRs #871  and #870. ",
  "created_at":"2023-04-06T15:57:16Z",
  "id":1499289980,
  "issue":871,
  "node_id":"IC_kwDOD6Q_ss5ZXVl8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-06T15:57:16Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry, but having a special rule for a string as specific as \"`zteos64`\" is a bad idea: it will fix this case and break others in a way that's hard to guess.\r\n\r\n(I _wish_, I _wish_, I _wish_ we never introduced that colon-parsing rule!)\r\n\r\nThere are two ways around it:\r\n\r\n  * use a `pathlib.Path` instead of a bare string, since a `pathlib.Path` is assumed to be entirely a file-path (although using a URL as a `pathlib.Path` is a little unusual)\r\n  * use the `{\"FULL-PATH\": None}` syntax.\r\n\r\nIn your case, the latter would be to replace\r\n\r\n```python\r\n\"root://eosserver//eos/path/file.root?xrd.wantprot=unix&authz=zteos64:BASE64TOKEN\"\r\n```\r\n\r\nwith\r\n\r\n```python\r\n{\"root://eosserver//eos/path/file.root?xrd.wantprot=unix&authz=zteos64:BASE64TOKEN\": None}\r\n```\r\n\r\nThat's how we solidified all of the paths in Coffea: https://github.com/CoffeaTeam/coffea/pull/771\r\n\r\nIn that pull request, you can see a lovely list of 10 Issues, PRs, and Discussions related to the colon parsing. This would be the twelveth, counting the Coffea PR.\r\n\r\n(I _wish_, I _wish_, I _wish_ we never introduced that colon-parsing rule! Let this be a lesson about adding high-level user conveniences that complicate interfaces for library developers. I should have stuck with this opinion: https://github.com/scikit-hep/uproot5/issues/79#issuecomment-680861697.)\r\n\r\nAdding the new rule in this PR would be going further in that direction.",
  "created_at":"2023-04-12T18:48:59Z",
  "id":1505764177,
  "issue":875,
  "node_id":"IC_kwDOD6Q_ss5ZwCNR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-12T18:48:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim, fair enough! We can use the {\"FULL-PATH\": None} syntax indeed, that will be a lot cleaner.",
  "created_at":"2023-04-12T20:58:24Z",
  "id":1505931575,
  "issue":875,
  "node_id":"IC_kwDOD6Q_ss5ZwrE3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-12T20:58:24Z",
  "user":"MDQ6VXNlcjcyMDgyODg="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Some embellishments upon this might be to allow the chunks in lazy-open mode to be specified per file somehow, perhaps the `files` argument to `uproot.dask` could allow a tuple as the key like`(file, start, stop)` so you'd have:\r\n```python3\r\n{(\"/path/to/file.root\", 10, 20): \"ObjectPath\", ...}\r\n```\r\n\r\nThis would let client libraries like coffea be very exact in their ability to distribute work, and also do so efficiently (e.g. aligning to clusters in root files, etc).",
  "created_at":"2023-04-12T20:39:13Z",
  "id":1505905488,
  "issue":876,
  "node_id":"IC_kwDOD6Q_ss5ZwktQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-12T20:39:13Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Moving a conversation from Slack: @lgray reminds me that `chunks_per_file` only _needs_ to be applied to the two `open_files=False` cases (NumPy and Awkward), since `step_size` can be used in the `open_files=True` cases.\r\n\r\nThat's right, but we want interfaces to be as consistent as possible. We now have two ways of specifying the chunking within a file, and that's bad because if there's a fight between them, who wins? The interface should be designed to (1) highlight the fact that they're both addressing the same thing and (2) be very clear about which one is active.\r\n\r\nFor (1), let's start by giving them the same names. `step_size` is used all of Uproot; we should keep that name, but make the new argument `steps_per_file` instead of `chunks_per_file`.\r\n\r\nFor (2), we can remove confusion by making the two arguments explicitly incompatible. That is, the defaults of both `step_size` and `steps_per_file` would be `unset`, where `unset` is defined as it is in Awkward:\r\n\r\nhttps://github.com/scikit-hep/awkward/blob/ee387cbe4ad41fb7407a965f5a91cc39f9e28cc3/src/awkward/_util.py#L83-L89\r\n\r\nThe first thing to check when entering the `uproot.dask` function is the state of the two arguments:\r\n\r\n  * If both are not `unset` (i.e. if both have been set to some values, even their defaults), then raise a `TypeError` with a message saying that you can only set one or the other, not both.\r\n  * If `step_size` is set to some value,\r\n    * if `open_files=True`, use it;\r\n    * if `open_files=False`, raise a `TypeError` saying that `step_size` is inconsistent with `open_files=False`.\r\n  * If `steps_per_file` is set to some value, use it. For `open_files=True`, the `steps_per_file` value can immediately be converted into an equivalent `step_size`, and that's what can get passed down into the machinery. That is, the `ichunk` and `nchunk` only need to be propagated to the `_NumpyOpenAndRead` and `_UprootOpenAndRead` classes.\r\n  * If both are `unset`, use `steps_per_file=1` regardless of whether `open_files` is `True` or `False`. This is a change, but it makes the behavior consistent between the two `open_files` cases. If this increases memory use for any current users, the recourse is to set one of these arguments.\r\n\r\nThat should do it. And then there needs to be tests for each case, and you can test both NumPy and Awkward by using a `pytest.parameterize` to switch between them.",
  "created_at":"2023-04-13T14:57:42Z",
  "id":1507123256,
  "issue":876,
  "node_id":"IC_kwDOD6Q_ss5Z1OA4",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-04-13T14:57:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We do want to make it as correct as it can be, but I've been bewildered by all of the variation in files that I've come across.\r\n\r\nIt sounds like you have a firm handle on all of the \"if/then\" logic. Do you see how the code in this part of identify.py,\r\n\r\nhttps://github.com/scikit-hep/uproot5/pull/570/files\r\n\r\ncan be modified to get it right? A backward-incompatible change that converts a raises-an-exception case into a working-properly case is not a backward-incompatibility that we're concerned about.\r\n\r\n-----------------------\r\n\r\nOh, when you say that it depends on `kIsReferenced`, you mean that it can't be predicted before reading data, right? In that case, any data containing an `fBits` can't be `AsDtype`; it would have to be `AsObjects`. At the point referenced above in identify.py,\r\n\r\n```python\r\nraise NotNumerical\r\n```\r\n\r\nwill get out of the part of the code that is trying to make it `AsDtype` and fall back to `AsObjects`. Then the `Model_TObject` can't have an `AsStridedObjects` interpretation because the byte-width of `fBits` can't be variable. This code:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/b53b16f3a47a8ef90374b54f34dae6d3e2bbc48b/src/uproot/models/TObject.py#L74-L87\r\n\r\nwould have to\r\n\r\n```python\r\nraise CannotBeStrided\r\n```\r\n\r\nto get out of the attempt to interpret it as a fixed-bytesize (strided) object.\r\n\r\nAnd then the final fallback of stepping a different number of bytes depending on whether `kIsReferenced` is set seems to already be implemented in the full object-interpretation:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/b53b16f3a47a8ef90374b54f34dae6d3e2bbc48b/src/uproot/models/TObject.py#L46-L51\r\n\r\nSo maybe as a first question, before digging too deeply into this: if you construct an `AsObjects` interpretation for some data that has `fBits`, does it get the number of bytes right? If so, then the project becomes one of making sure that it always falls back to the full object interpretation, as I've outlined above. Now that we have AwkwardForth, this is less of a performance degradation than it once was.\r\n\r\n---------\r\n\r\nA lot of classes descend from `TObject`, which has this potentially variable-sized `fBits`. Even `TLorentzVector`, which was the original motivation for `AsStridedObjects`, would be changed by this. Such a modification would affect a lot of data types, but presumably, some of those are failing now because of variable-width `fBits`...",
  "created_at":"2023-04-14T17:05:41Z",
  "id":1508971694,
  "issue":878,
  "node_id":"IC_kwDOD6Q_ss5Z8RSu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-14T17:05:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Oh, when you say that it depends on kIsReferenced, you mean that it can't be predicted before reading data, right?\r\n\r\nYes, that is exactly what I meant.\r\n\r\n> if you construct an AsObjects interpretation for some data that has fBits, does it get the number of bytes right?\r\n\r\nActually, I have never used `AsObjects` (to be honest, I am a novice user of uproot and didn't even know it) and rather use `arrays` or `iterate`, but I agree with this and also your sketch of changes to be done.\r\n\r\n> Now that we have AwkwardForth, this is less of a performance degradation than it once was.\r\n\r\nSounds great. To be honest, I think that effectively it doesn't matter even in pure python. The complexity changes from O(1) to O(N) though.\r\n\r\n> A lot of classes descend from TObject, which has this potentially variable-sized fBits. Even TLorentzVector, which was the original motivation for AsStridedObjects, would be changed by this. Such a modification would affect a lot of data types, but presumably, some of those are failing now because of variable-width fBits...\r\n\r\nI completely agree with this, unfortunately. \r\n\r\n",
  "created_at":"2023-04-14T17:55:59Z",
  "id":1509024977,
  "issue":878,
  "node_id":"IC_kwDOD6Q_ss5Z8eTR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-14T17:55:59Z",
  "user":"MDQ6VXNlcjEyNzkwNTA5"
 },
 {
  "author_association":"MEMBER",
  "body":"@mattiasoldani\r\n\r\nSomething odd is happening here. This `zCluster` class is not too complex for Uproot: apart from some inheritance, it could be a simple struct:\r\n\r\n```python\r\n>>> f.file.show_streamers(\"zCluster\")\r\nTObject (v1)\r\n    fUniqueID: unsigned int (TStreamerBasicType)\r\n    fBits: unsigned int (TStreamerBasicType)\r\n\r\nTVector3 (v3): TObject (v1)\r\n    fX: double (TStreamerBasicType)\r\n    fY: double (TStreamerBasicType)\r\n    fZ: double (TStreamerBasicType)\r\n\r\nzHit (v2)\r\n    Status: int (TStreamerBasicType)\r\n    Bin: int (TStreamerBasicType)\r\n    iSubdet: int (TStreamerBasicType)\r\n    Detected: int (TStreamerBasicType)\r\n    Primary: int (TStreamerBasicType)\r\n    Used: int (TStreamerBasicType)\r\n    Charge: int (TStreamerBasicType)\r\n    Eff: double (TStreamerBasicType)\r\n    xTrue: TVector3 (TStreamerObject)\r\n    xRec: TVector3 (TStreamerObject)\r\n    PosRes: double (TStreamerBasicType)\r\n    xTrue1: TVector3 (TStreamerObject)\r\n    xTrue2: TVector3 (TStreamerObject)\r\n    xPre1: TVector3 (TStreamerObject)\r\n    xPre2: TVector3 (TStreamerObject)\r\n    PreRes: double (TStreamerBasicType)\r\n    ETrue: double (TStreamerBasicType)\r\n    ERec: double (TStreamerBasicType)\r\n    ERes: double (TStreamerBasicType)\r\n    Converted: int (TStreamerBasicType)\r\n    dXdZTrue: double (TStreamerBasicType)\r\n    dYdZTrue: double (TStreamerBasicType)\r\n\r\nzCluster (v2): zHit (v2)\r\n    nHits: int (TStreamerBasicType)\r\n    HitList: int (TStreamerBasicType)\r\n    Primary: int (TStreamerBasicType)\r\n    nConverted: int (TStreamerBasicType)\r\n```\r\n\r\nIt doesn't even have any variable-length objects (lists), so it could even be an `AsStridedObjects`, let alone an Awkward Array.\r\n\r\nThe error message happens here:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/b53b16f3a47a8ef90374b54f34dae6d3e2bbc48b/src/uproot/streamers.py#L243-L244\r\n\r\nIt's recursing over the streamer info (printed above) in dynamically generated code (the Python code inside of the quoted strings), and the current class (`cls`) finds itself in the `breadcrumbs`, which is a tuple of classes we've recursed through so far. In fact, I printed them out\u2014the exception gets raised in `Model_TVector3_v3.awkward_form` with breadcrumbs\r\n\r\n  1. `Model_zCluster_v2`\r\n  2. `Model_zHit_v2`\r\n  3. `Model_TVector3_v3`\r\n\r\nThese first three steps of recursion are correct (`TVector3` is nested in `zHit`, which is nested in `zCluster`, in one case because it's member data, in the other because it's C++ class inheritance), but in no way does `TVector3` get nested within `TVector3`. If it did, it couldn't be an Awkward Array (arbitrary depth trees are not allowed), but `TVector3` is a very simple class containing only three floating point numbers.\r\n\r\nI can't see what's going on here, how it got to this point, but this is an earlier error than the reading error, so it's best to start at the first bug. We read `TVector3` many times in the testing suite, so there must be something peculiar about this file.\r\n\r\nLooking at the code that gets dynamically generated for `TVector3`,\r\n\r\n```python\r\n>>> print(f.file.class_named(\"TVector3\", 3).class_code)\r\n```\r\n\r\nI don't see anything amiss. (But I also just ran out of time that I can spend looking at it.)\r\n\r\n@ioanaif, can you look at this?",
  "created_at":"2023-04-17T14:01:40Z",
  "id":1511413366,
  "issue":880,
  "node_id":"IC_kwDOD6Q_ss5aFlZ2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-17T14:01:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski yes, will have a look.",
  "created_at":"2023-04-17T14:02:12Z",
  "id":1511414317,
  "issue":880,
  "node_id":"IC_kwDOD6Q_ss5aFlot",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-17T14:02:12Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"NONE",
  "body":"Thank you very much for looking into this! Do you have any updates, or perhaps does any workaround to access the data come to your mind?",
  "created_at":"2023-04-24T10:00:05Z",
  "id":1519813526,
  "issue":880,
  "node_id":"IC_kwDOD6Q_ss5aloOW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-24T10:00:05Z",
  "user":"MDQ6VXNlcjYyNTE3NjYx"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi! \u2028@mattiasoldani \r\n\r\nI investigated this and found out that the issue comes from the fact that when we build the breadcrumbs for keeping a record of the recursion, in this particular case, at the level of the `zHit` it tries to add `TVector3` multiple times to the breadcrumbs (for `xTrue`, then for `xTrue1`, `xTrue2` etc) which raises the error you see. \u2028\u2028\r\n\r\nI am working towards fixing this issue such that the recursion breadcrumbs are built correctly.\u2028\r\n\r\nOn the second part of the issue, regarding the number of clusters that get outputted, I opened the file with the jsroot browser and it shows only 116 entries, the same as what uproot shows:\r\n\r\n<img width=\"1438\" alt=\"image\" src=\"https://user-images.githubusercontent.com/9751871/235666542-5732a56f-559b-40dc-ab78-e5e6ac51d0b1.png\">\r\n\r\n",
  "created_at":"2023-05-02T12:31:35Z",
  "id":1531397106,
  "issue":880,
  "node_id":"IC_kwDOD6Q_ss5bR0Py",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-02T12:31:35Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"NONE",
  "body":"Dear @ioanaif,\r\n\r\nthank you very much for the update!\r\n\r\nI'd only like to point out that it is correct that \"nCluster\" has 116 entries, i.e., as many as the tree entries. That is just a counter of, say, the clusters of interest in each event. On the other hand, \"Cluster[6]\" should contain six clusters per event, corresponding to a total of 116*6 = 696 entries in the whole array. All this is confirmed by inspecting the file with plain ROOT and the TBrowser, and I think it should be the same using JSROOT. On the other hand, opening the file with uproot leads to an array of 116 clusters, only 1 per event.",
  "created_at":"2023-05-07T10:13:07Z",
  "id":1537377858,
  "issue":880,
  "node_id":"IC_kwDOD6Q_ss5booZC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-07T10:13:07Z",
  "user":"MDQ6VXNlcjYyNTE3NjYx"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Dear @mattiasoldani \r\n\r\nIndeed, \"Cluster[6]\" should contain six clusters per event, I was mistaken in my previous comment. \r\n\r\nThus, you uncovered 2 issues here: the first one (recursion issue) was fixed in #886, and for the second one (six cluster vs one cluster contained) I am working towards a solution to have uproot output the correct number of clusters. \r\n\r\n",
  "created_at":"2023-05-07T20:24:59Z",
  "id":1537533228,
  "issue":880,
  "node_id":"IC_kwDOD6Q_ss5bpOUs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-07T20:24:59Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"> * What causes the `http.client.RemoteDisconnected`?\r\n\r\nWe just _use_ the standard library's `http.client` library; the error is coming from there, and presumably it means that the server decided to disconnect before the data could be fully downloaded. The last Uproot line in the stack trace is\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/b53b16f3a47a8ef90374b54f34dae6d3e2bbc48b/src/uproot/source/http.py#L304\r\n\r\nwhere we're just asking an `HTTPSConnection` object to `getresponse()` ([docs for that function](https://docs.python.org/3/library/http.client.html#http.client.HTTPConnection.getresponse)).\r\n\r\n> * Why is the reading of all columns at once (batched or not batched) so much slower than downloading the full file, or than reading one column at a time?\r\n\r\nI wonder if the following is happening: the file has a large number of TBaskets and Uproot is asking for them in concurrent threads, effectively attacking the server with many requests (one for each TBasket, from each of the threads) and the server gives up and starts closing connections. This could explain why it's slow and the slowness is proportionate to the number of TBranches requested (and therefore the number of TBaskets), and it could explain why the server is more likely to give up in such a case.\r\n\r\nIdeally, Uproot batches all of the file byte ranges that it needs to read (one range of bytes for each TBasket) in a single request (using the [HTTPSource.chunks](https://github.com/scikit-hep/uproot5/blob/b53b16f3a47a8ef90374b54f34dae6d3e2bbc48b/src/uproot/source/http.py#L607-L632) method). Then the server responds with a [multipart range](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests#multipart_ranges), which [we parse](https://github.com/scikit-hep/uproot5/blob/b53b16f3a47a8ef90374b54f34dae6d3e2bbc48b/src/uproot/source/http.py#L389-L480) as a stream, putting the raw bytes for each TBasket onto a notification queue as it comes, and decompression/interpretation threads take them off the queue and work on them.\r\n\r\nHowever, it's valid for an HTTP server to just not support multipart ranges. [When this happens](https://github.com/scikit-hep/uproot5/blob/b53b16f3a47a8ef90374b54f34dae6d3e2bbc48b/src/uproot/source/http.py#L357-L372), Uproot goes into a fallback mode ([MultithreadedHTTPSource](https://github.com/scikit-hep/uproot5/blob/b53b16f3a47a8ef90374b54f34dae6d3e2bbc48b/src/uproot/source/http.py#L707)), which launches [some number](https://github.com/scikit-hep/uproot5/blob/b53b16f3a47a8ef90374b54f34dae6d3e2bbc48b/src/uproot/reading.py#L187) of worker threads that each request one byte range/TBasket at a time, but keep the server busy by asking for them concurrently. However, as I said above, this could overwhelm the server. How many requests overwhelms the server depends entirely on the server\u2014it's configured with its own thresholds. (If you can control the server, there's probably an \"allow multipart GET\" option, which might be turned off just because it was a default for the HTTP server software. It's not a very familiar/widely used part of the HTTP protocol; I've seen [some questions](https://stackoverflow.com/q/55665466/1623645) on StackOverflow just asking why it even exists.)\r\n\r\nTo determine if the scenario I described above is happening, there are a few things you can do:\r\n\r\n  * Use [uproot.TBranch.num_baskets](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#num-baskets) to find out how many baskets each requested TBranch has: this is the number of byte ranges Uproot will need to request.\r\n  * Check the `f.file.source.fallback` (after attempting to read TBasket data) to see if the `HTTPSource` has switched over into fallback mode.\r\n  * Check the server configuration to see if there's an easy way to turn on [multipart ranges](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests#multipart_ranges). (That link goes to the Mozilla documentation, which uses standard terminology/words to search for.)\r\n  * Reduce the `num_fallback_workers` in [uproot.open](https://uproot.readthedocs.io/en/latest/uproot.reading.open.html), though that won't help your performance problem.\r\n  * Use XRootD? That shouldn't be the necessary solution, since HTTP (the protocol) can do this multipart-GET, but the equivalent in XRootD (called \"vector_read\") is much more common/familiar to servers.",
  "created_at":"2023-04-24T14:42:46Z",
  "id":1520302606,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5anfoO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-24T14:42:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for all the info Jim! I will follow up with experts at UNL to understand whether anything can be changed on the side of the server. This sounds like `uproot` might just be behaving as expected.\r\n\r\nThe `num_baskets` per branch is around 200:\r\n```\r\nJet_mass 212\r\nnJet 194\r\nMuon_pt 274\r\nJet_phi 212\r\nJet_btagCSVV2 212\r\nJet_pt 212\r\nJet_eta 212\r\n```\r\nI have no reference for that number, is this a lot? Over 1000 threads making a simultaneous request naively sounds like it could be a problem.\r\n\r\n`f.file.source.fallback` is `None` after reading some data, assuming I am testing this correctly:\r\n```python\r\nimport uproot\r\n\r\nfname = \"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV\"\\\r\n    \"-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic\"\\\r\n    \"_v12_ext3-v1_00000_0000.root\"\r\n\r\nwith uproot.open(fname) as f:\r\n    f[\"Events\"][\"nJet\"]\r\n    assert f.file.source.fallback is None\r\n```\r\nDoes that imply that multipart ranges are actually supported? Perhaps expected compared with the previous test, but `num_fallback_workers=1` also does not change the behavior of `http.client.RemoteDisconnected` after a few seconds.\r\n\r\nSwitching to `root://` for reading can be a partial solution, it requires any valid certificate to read that way (while https requires nothing), I do not remember the reason but that is specific to the server and not easy to change from what I understand.",
  "created_at":"2023-04-24T18:29:35Z",
  "id":1520638630,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5aoxqm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-24T18:29:35Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"NONE",
  "body":"The sample code above seems to produce requests with over 1500 items, and a `Range` header length of ~34k characters.\r\n\r\nI did some testing on the XRootD v5.5.3 service at UNL:\r\n\r\nA `Range` request with over 1024 items fails. The server side logs `Xrootd_Response: sending err 3002: Read vector is too long`. The client side unfortunately receives two HTTP replies, which looks like a bug.\r\n\r\n```\r\nHTTP/1.1 206 Partial Content\r\nConnection: Keep-Alive\r\nServer: XrootD/v5.5.3\r\nContent-Length: 102342\r\nContent-Type: multipart/byteranges; boundary=123456\r\n\r\nHTTP/1.1 500 Internal Server Error\r\nConnection: Close\r\nServer: XrootD/v5.5.3\r\nContent-Length: 24\r\n\r\nRead vector is too long\r\n```\r\n\r\nAlso, a `Range` request over ~19k characters fails with the server side immediately closing the connection. The failures were intermittent as the header approached the 19k boundary.\r\n\r\nThere might be XRootD knobs to raise the limits, but I'd guess that some sort of chunking will be necessary in any case. Limiting range requests to 1024 items and 16k chars each may be a reasonable start.",
  "created_at":"2023-04-24T20:22:47Z",
  "id":1520779028,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5apT8U",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-24T20:22:47Z",
  "user":"MDQ6VXNlcjE5NTExMTU="
 },
 {
  "author_association":"MEMBER",
  "body":"> I have no reference for that number, is this a lot? Over 1000 threads making a simultaneous request naively sounds like it could be a problem.\r\n\r\n200 TBaskets per TBranch in a reasonably large file, like 2 GB, sounds about right.\r\n\r\nIt would be 10 threads (`num_fallback_workers`) making over 1000 requests. Up to 10 requests can be in flight at any given time.\r\n\r\n> `f.file.source.fallback` is `None` after reading some data, assuming I am testing this correctly:\r\n\r\nThat looks like a correct test to me, so I guess it's not going into the fallback. Therefore, you actually have 1 thread making a multi-part GET with thousands of byte ranges in it.\r\n\r\n> A `Range` request with over 1024 items fails.\r\n\r\n> Also, a `Range` request over ~19k characters fails with the server side immediately closing the connection.\r\n\r\nI didn't know about these possible failure modes. In principle, Uproot ought to catch this and retry the request in smaller batches, but that will require significant new infrastructure: a fallback before the complete-fallback. Maybe if the HTTPSource (which attempts multi-part GET) can recognize that the `Range` requests are too long, it could split itself into two HTTPSources on different threads? Or would two requests for 1024 items each have to be sequential (i.e. don't request the second until the first has fully responded)? The concurrent one would be easier to write...\r\n\r\nIs there a way to recognize this situation of the request being too long as distinct from the server just not accepting multi-part GET at all (and therefore know _which_ fallback to fall back to)?",
  "created_at":"2023-04-24T21:19:39Z",
  "id":1520840755,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5apjAz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-24T21:19:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Maybe if the HTTPSource (which attempts multi-part GET) can recognize that the Range requests are too long, it could split itself into two HTTPSources on different threads? Or would two requests for 1024 items each have to be sequential (i.e. don't request the second until the first has fully responded)?\r\n\r\nThat sounds reasonable to me. I'm not an XRootD expert, but would believe multiple simultaneous requests will be fine. I'm guessing we're just running into guardrails around its parsing of HTTP headers.\r\n\r\n> Is there a way to recognize this situation of the request being too long as distinct from the server just not accepting multi-part GET at all (and therefore know which fallback to fall back to)?\r\n\r\nWould limiting the request size and count to a reasonable default be enough? If the request fails, assume multi-part is not supported.",
  "created_at":"2023-04-24T22:35:08Z",
  "id":1520912078,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5ap0bO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-24T22:35:08Z",
  "user":"MDQ6VXNlcjE5NTExMTU="
 },
 {
  "author_association":"MEMBER",
  "body":"@alexander-held can quickly work around it by putting fewer TBranches in his `arrays` request, and then joining them with `ak.zip` if need be. I think he raised this issue because it's something that's not _supposed_ to happen.",
  "created_at":"2023-04-25T13:22:35Z",
  "id":1521783404,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5atJJs",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-04-25T13:22:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Indeed, this is not blocking my work. I wanted to understand if everything is working as expected or if there is an action that should be taken somewhere. I imagine that the behavior might look confusing to other people as well (just as it was confusing for me).\r\n\r\nIf the server configuration is \"normal\" and other people may run into similar issues with other files at other sites, it might be useful to have some extra context in the exception. Not clear to me if that is potentially more confusing than helpful though if there are other likely sources of it being raised. At least now if someone faces this, makes it to the issue tracker and searches for `RemoteDisconnected`, they will find this, so perhaps that can already help a bit.",
  "created_at":"2023-04-25T13:36:04Z",
  "id":1521803755,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5atOHr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-25T13:42:07Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"We _do_ often get bug reports about remote server issues, though more often with XRootD. Usually, I think we can't solve them because it's outside of Uproot's control, but this is an example of something that Uproot can do something about.\r\n\r\nThe main thing that we'll need, though, is a clear way to determine that the list of byte ranges was too long (and that's why it failed) or is going to be too long (and we shouldn't send it, because it will fail), so that Uproot can break down one thread sending a long request into N threads, each sending a reasonably long request. All the infrastructure is ready to accept the responses in parallel.",
  "created_at":"2023-04-25T15:36:24Z",
  "id":1522012686,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5auBIO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-25T15:36:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"There are certainly issues that XRootD HTTP needs to fix: returning a TCP RST, rather than an HTTP error. And returning a 206, then a spurious 500 failure immediately after. I'll try to get that into an issue on the XRootD side.\r\n\r\nOn the Uproot side of things, I'd guess it might be challenging to detect the error and adjust. Might get a 413, 431, or even a generic 400 or 500 error.\r\n\r\nFrom searching, it looks like the [Apache default](https://httpd.apache.org/docs/2.4/mod/core.html#limitrequestfieldsize) for header fields is 8190 bytes. And it seems widespread support for lengths beyond 16k is unusual. Would breaking requests into ~7.5k chunks be a good-enough solution?",
  "created_at":"2023-04-25T20:15:26Z",
  "id":1522359947,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5avV6L",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-04-25T20:15:26Z",
  "user":"MDQ6VXNlcjE5NTExMTU="
 },
 {
  "author_association":"MEMBER",
  "body":"We can adapt Uproot to the way it should be, rather than the way it is now, since the course of action would be to upgrade software in either case. (Though I suppose that upgrading server software is more difficult and may be out of an individual's control.)\r\n\r\nA configurable limit on the number of request bytes, default at 8190, before splitting into multiple requests is something we can do. I like this solution because it means we don't need to recognize a new type of server response. If there are any servers with this limit set below the default, it would be hard to debug, but an Uproot user who knows what the problem is can adjust the limit.\r\n\r\nI'll turn this issue into a feature request.",
  "created_at":"2023-04-25T21:08:49Z",
  "id":1522419846,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5avkiG",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2023-04-25T21:08:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"One interesting note here: I can reproduce the original error here with `https://xrootd-local.unl.edu:1094`, but not with a locally running installation of the same XRootD version (5.5.4). The local version is slower than it should be due to https://github.com/xrootd/xrootd/issues/1976 and https://github.com/xrootd/xrootd/issues/2003, but it still works and doesn't raise the `RemoteDisconnected` exception. I wonder if there's a timeout or some XRootD config setting that's different on the UNL server.",
  "created_at":"2023-05-01T09:34:44Z",
  "id":1529521955,
  "issue":881,
  "node_id":"IC_kwDOD6Q_ss5bKqcj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-01T09:34:44Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"> I think the probe is `axis.member(\"fLabels\")` throws an error, not returns `None`, when it's not present, but this code assumes it returns None. This should probably have `none_if_missing=True`.\r\n\r\nYour assessment is entirely correct. The `none_if_missing` argument was added to make it easier to write code for ROOT classes of different versions, which may be missing some attributes. (Geant4 has its own ROOT writer, so its class versions may be very old.) This is such a case.\r\n\r\n> (Just curious, why is this not `default=None`? That seems more pythonic, would look like `dict.get`).\r\n\r\nThat would have been a much better name. This was developed as part of a coding sprint, not carefully crafting a public API. I can see reasons why downstream developers would need to use this function (to write frameworks for their custom C++ classes), so deprecating `none_if_missing` (boolean) in favor of `default` (any value) may be a good idea. First, I'd like to scan all those GitHub repos I have on my computer at home to see how often it is actually used. We don't have a deprecation policy in place for Uproot as we do for Awkward, since the API is not rapidly changing. (Most changes are bug-fixes that keep the intended API unchanged.)",
  "created_at":"2023-05-04T03:12:56Z",
  "id":1534029085,
  "issue":883,
  "node_id":"IC_kwDOD6Q_ss5bb20d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-04T03:12:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Just a note: we'll need to check this again\u2014maybe the Uproot issue is solved by https://github.com/scikit-hep/uproot5/pull/884.",
  "created_at":"2023-05-25T14:42:36Z",
  "id":1563034195,
  "issue":883,
  "node_id":"IC_kwDOD6Q_ss5dKgJT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-25T14:42:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Tested with \r\n\r\n```\r\n>>> import uproot\r\n>>> import skhep_testdata\r\n>>> uproot_file = uproot.open(skhep_testdata.data_path(\"uproot-from-geant4.root\"))\r\n>>> uproot_file[\"cot_diff\"].to_hist()\r\nHist(Regular(200, -0.266667, 0.266667, name='xaxis', label='#delta cot #theta'), storage=Weight()) # Sum: WeightedSum(value=220, variance=220) (WeightedSum(value=224, variance=224) with flow)\r\n```\r\n\r\nSeems to be solved by #884  ",
  "created_at":"2023-06-01T07:47:18Z",
  "id":1571530384,
  "issue":883,
  "node_id":"IC_kwDOD6Q_ss5dq6aQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-01T07:47:18Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, indeed we discussed all these changes last week. I applied all the changes, for: \r\n\r\n- [x]     src/uproot/models/TH.py\r\n- [x]     src/uproot/models/TGraph.py\r\n- [x]     src/uproot/models/TAtt.py\r\n\r\nIn  `src/uproot/behaviors/TBranch.py`, a copy of the context is made prior to resetting the breadcrumbs:\r\n\r\n```\r\n        self._context = dict(context)\r\n        self._context[\"breadcrumbs\"] = ()\r\n```\r\n   ",
  "created_at":"2023-05-08T17:10:56Z",
  "id":1538741449,
  "issue":886,
  "node_id":"IC_kwDOD6Q_ss5bt1TJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-08T17:10:56Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"This looks done. If it is done, please do merge it. (I've approved it.)\r\n\r\nYou might want to update branch and enable auto-merge (squash).",
  "created_at":"2023-05-15T20:39:43Z",
  "id":1548542741,
  "issue":886,
  "node_id":"IC_kwDOD6Q_ss5cTOMV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-15T20:39:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I can also set up trusted publisher deployment if you'd like.",
  "created_at":"2023-05-15T20:42:16Z",
  "id":1548545905,
  "issue":887,
  "node_id":"IC_kwDOD6Q_ss5cTO9x",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-15T20:42:16Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Three tests are failing with:\r\n\r\n```\r\nFAILED tests/test_0755-dask-awkward-column-projection.py::test_column_projection_sanity_check - DeprecationWarning: In version 2.3.0, this will be an error.\r\nFAILED tests/test_0791-protect-uproot-project_columns-from-dask-node-names.py::test - ...\r\nFAILED tests/test_0832-ak_add_doc-should-also-add-to-typetracer.py::test - ...\r\n```\r\n\r\n```\r\nIssue: The `highlevel=True` variant of `Form.length_zero_array` is now deprecated. Please use `ak.Array(form.length_zero_array(...), behavior=...)` if an `ak.Array` is required..\r\n```\r\n\r\nFWIW, it would really help if the name of the library was included in the warning! Also, the warning talks a great deal about how to turn it into an error (which could be googled easily), but ~~doesn't tell a user how to fix the problem it's warning about.~~ Ahh, it's at the bottom. Hmm, maybe the instructions could be shortened? You can just run Python with `-Werror` to make all warnings errors, for example. A blank line at the end (and one period instead of two) might help as well.\r\n\r\nAlso, matching this via `ignore:<stuff>...`, say in pytest or with Python's `-W` is harder because normally warnings start with the unique parts. Usually you start with the custom, relevant info and them match on that.\r\n\r\nAre there really tests called \"test\"?",
  "created_at":"2023-05-15T21:13:15Z",
  "id":1548588305,
  "issue":887,
  "node_id":"IC_kwDOD6Q_ss5cTZUR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-15T21:26:36Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"These will be fixed by https://github.com/scikit-hep/uproot5/pull/892.\r\n\r\nThe message about converting warnings into errors is in Awkward's deprecation machinery:\r\n\r\nhttps://github.com/scikit-hep/awkward/blob/e3e844c95cc02f15e2b4962f036fc871c16890e7/src/awkward/_errors.py#L359-L367\r\n\r\nI know that I would have a hard time figuring out how to get a stack trace from a warning\u2014because I've had to do that! (On the second, third, and fourth time, I didn't remember how I did it on the first, second, and third time, or what I had to Google to find out.) It is especially for a deprecation warning that you'd want to get a stack trace, because the one and only thing you want to do with it is to update your code to be future-proof, unlike, say, a NumPy \"invalid value encountered in sqrt\" warning. I think it would be even better if the warning could give me something to copy-paste into a terminal. (There are git operations that I deliberately do wrong because I know that git will give me what I need to copy-paste into the terminal. Much better than Googling!)\r\n\r\nThis is the only warning in Awkward Array, so if there's something we can do to make the message easier to work with, let's do it. (Pulling @agoose77 into this.)\r\n\r\n> Are there really tests called \"test\"?\r\n\r\nIf it's the only test in its file, it doesn't need to be disambiguated from anything else. (The filename already provides its coordinates.)",
  "created_at":"2023-05-15T23:20:49Z",
  "id":1548745865,
  "issue":887,
  "node_id":"IC_kwDOD6Q_ss5cT_yJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-15T23:20:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Are you using an old version? Transposed axes (or something more complex if it was not square) was an issue early on, but it was caught and fixed in #198. It's hard to see how that could happen again.\r\n\r\nI looked into this, and I don't see a problem in the most recent version of the code. We have a test file with a 2D histogram, [uproot-hepdata-example.root](https://github.com/scikit-hep/scikit-hep-testdata/blob/main/src/skhep_testdata/data/uproot-hepdata-example.root), with two unfortunate features: it's square, and it's an old version of the histogram class that can't be serialized through `to_pyroot`. It also has `values` and `variances` that are equal to each other, but that helps us to see that we're not treating `values` and `variances` differently. We can use this file as a test of your issue by concentrating on the bins in the center of the randomly filled blob\u2014we would be able to tell the difference between one matrix and its transpose\u2014and we can make it `to_pyroot`-serializable by changing the version numbers of its classes and filling the missing member data with default values.\r\n\r\nSo first, this is how `to_hist().plot2d()` plots it:\r\n\r\n```python\r\nimport uproot\r\nimport skhep_testdata\r\nimport ROOT\r\nimport mplhep as hep\r\n\r\nf = uproot.open(skhep_testdata.data_path(\"uproot-hepdata-example.root\"))\r\n\r\nf[\"hpxpy\"].to_hist().plot2d()\r\n```\r\n\r\n![download](https://github.com/scikit-hep/uproot5/assets/1852447/c240ffa9-d7c1-459d-aab5-9e25cee5fec6)\r\n\r\nLook at the yellowest bins near the center: their pattern is easy to distinguish from the transpose. (There's a line of three bright ones in a horizontal line, with one below and to the left and another above and to the right. If this were to be transposed, that would change.)\r\n\r\nNow let's use mplhep's `hist2dplot` directly to see if it has a different interpretation of horizontal and vertical.\r\n\r\n```python\r\nhep.hist2dplot(f[\"hpxpy\"].to_hist().variances())\r\n```\r\n\r\n![download](https://github.com/scikit-hep/uproot5/assets/1852447/6de234d4-417b-4e26-b0a5-71883ef61e7b)\r\n\r\nNo it doesn't. (Those central yellow bins are showing the same pattern.)\r\n\r\nNow what about ROOT's interpretation of the same histogram?\r\n\r\n```python\r\ntfile = ROOT.TFile(\"~/irishep/OLD/uproot3/tests/samples/hepdata-example.root\")\r\nhistogram = tfile.Get(\"hpxpy\")\r\n\r\nc1 = ROOT.TCanvas()\r\nhistogram.Draw(\"COLZ\")\r\nc1.Draw()   # necessary because I tested this in a Jupyter notebook...\r\n```\r\n\r\n![download](https://github.com/scikit-hep/uproot5/assets/1852447/c08a5a63-b850-4d0f-a395-afc150adfa6e)\r\n\r\nAlthough the colors are different, the maximal bins have the same pattern in the center.\r\n\r\nSuppose we do what you did and collected `GetBinError` in a list comprehension (which would be another opportunity to interpret horizontal and vertical differently, depending on the order of list comprehension and the interpretation of the two arguments of `GetBinError`).\r\n\r\n```python\r\nerrors = [[histogram.GetBinError(i+1, j+1) for j in range(40)] for i in range(40)]\r\n\r\nhep.hist2dplot(errors)\r\n```\r\n\r\n![download](https://github.com/scikit-hep/uproot5/assets/1852447/5dcbca77-f240-458a-8ebd-1d9ed578ee7f)\r\n\r\nSame picture: the yellow bins in the center are still in a horizontal line.\r\n\r\nOkay, what about converting the Uproot histogram into a PyROOT histogram through `to_pyroot`, rather than reading it directly with ROOT? In principle, the `to_pyroot` serialization might be getting the wrong axis ordering (C contiguous instead of Fortran contiguous or something similar). The issue here is that our file has an old histogram version and needs to be updated manually.\r\n\r\n```python\r\nh = f[\"hpxpy\"]\r\n\r\nh.__class__ = uproot.models.TH.Model_TH2F_v4\r\n\r\nh.bases[0].__class__ = uproot.models.TH.Model_TH2_v5\r\n\r\nh.bases[0].bases[0].__class__ = uproot.models.TH.Model_TH1_v8\r\n\r\nh.bases[0].bases[0]._speedbump1 = None\r\nh.bases[0].bases[0].members[\"fStatOverflows\"] = 0\r\n\r\nh.bases[0].bases[0].bases[1].__class__ = uproot.models.TAtt.Model_TAttLine_v2\r\n\r\nh.bases[0].bases[0].bases[2].__class__ = uproot.models.TAtt.Model_TAttFill_v2\r\n\r\nh.bases[0].bases[0].members[\"fXaxis\"].__class__ = uproot.models.TH.Model_TAxis_v10\r\nh.bases[0].bases[0].members[\"fYaxis\"].__class__ = uproot.models.TH.Model_TAxis_v10\r\nh.bases[0].bases[0].members[\"fZaxis\"].__class__ = uproot.models.TH.Model_TAxis_v10\r\n\r\nh.bases[0].bases[0].members[\"fXaxis\"].members[\"fModLabs\"] = None\r\nh.bases[0].bases[0].members[\"fYaxis\"].members[\"fModLabs\"] = None\r\nh.bases[0].bases[0].members[\"fZaxis\"].members[\"fModLabs\"] = None\r\n```\r\n\r\nAlthough this is some invasive surgery, it definitely would not change how the NumPy array is serialized (which happens in the `TArrayF` class, whose version didn't even need to be changed).\r\n\r\n```python\r\npyroot_hist = h.to_pyroot()\r\n\r\npyroot_hist.Draw(\"COLZ\")\r\nc1.Draw()\r\n```\r\n\r\n![download](https://github.com/scikit-hep/uproot5/assets/1852447/64ae25b5-d2a4-4f43-abcb-d9c6113d6b60)\r\n\r\nIt's still the same picture. I don't see any issue. We could do this test with a non-square and more recent histogram class version, but it wouldn't change the outcome, based on what I've seen above.\r\n\r\nI think the most likely thing is that you have a version of Uproot from before #198 was fixed. If you update to the most recent version of Uproot and still have this problem, you can reopen this issue (or comment here asking us to, if the permissions don't let you reopen), but then you should also include the file so that we can reproduce it.",
  "created_at":"2023-06-01T16:15:14Z",
  "id":1572348214,
  "issue":888,
  "node_id":"IC_kwDOD6Q_ss5duCE2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-01T16:15:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for taking the time to look into this.\r\n\r\nI am using uproot 5.0.7 which seems to be almost the newest stable release (5.0.8 as of this writing). I am creating these histograms from TUnfoldBinning objects within a C++ framework using TUnfoldBinning::CreateHistogramOfMigrations. I have produced 833 histograms and every single histogram shows this feature.\r\n\r\nHere is an example ROOT file containing histograms with these features: https://drive.google.com/file/d/1etIvX7jxyhhkZi3NjFKIQofxgXwS3gao/view?usp=sharing\r\n\r\nThe below code can be used to replicate what I am seeing:\r\n\r\n```\r\nimport uproot\r\nimport matplotlib\r\nfrom matplotlib import pyplot as plt\r\nimport mplhep as hep\r\n\r\nfile = uproot.open('histosTUnfold_combined_ttbarsignalplustau.root')\r\n\r\nfig, ax = plt.subplots(1, 3, figsize=(30, 10))\r\n\r\nhep.hist2dplot(file['hrecoVsgen_ll_cHel_400mttbar'].variances(), norm=matplotlib.colors.LogNorm(), ax=ax[0])\r\nmyhist = file['hrecoVsgen_ll_cHel_400mttbar'].to_pyroot()\r\n\r\nvariances = [[myhist.GetBinError(i+1, j+1)**2 for j in range(768)] for i in range(192)]\r\nhep.hist2dplot(variances, norm=matplotlib.colors.LogNorm(), ax=ax[1])\r\nhep.hist2dplot(file['hrecoVsgen_ll_cHel_400mttbar'].to_hist().variances(), norm=matplotlib.colors.LogNorm(), ax=ax[2])\r\n```\r\n\r\n@jpivarski Could you please re-open this issue?",
  "created_at":"2023-06-16T05:20:32Z",
  "id":1594120505,
  "issue":888,
  "node_id":"IC_kwDOD6Q_ss5fBFk5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-16T05:24:30Z",
  "user":"MDQ6VXNlcjY1MTYyMjY="
 },
 {
  "author_association":"MEMBER",
  "body":"It's reopened so that we can reproduce it and find out why the histogram axes seem to be backward for your case and not for other cases. (There's some mixup going on at some level; we need to get to the bottom of this.)",
  "created_at":"2023-06-16T21:40:33Z",
  "id":1595345903,
  "issue":888,
  "node_id":"IC_kwDOD6Q_ss5fFwvv",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-06-16T21:40:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"~~The job failures are coming from promoted awkward deprecation warnings, I will not fix those.~~\r\n~~The warning is coming from https://github.com/scikit-hep/awkward/pull/2437~~\r\n\r\nEasy fix!",
  "created_at":"2023-05-11T22:48:37Z",
  "id":1544789379,
  "issue":890,
  "node_id":"IC_kwDOD6Q_ss5cE52D",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-11T22:53:29Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"I just tested this with [sample-6.18.00-zlib.root](https://github.com/scikit-hep/scikit-hep-testdata/blob/main/src/skhep_testdata/data/uproot-sample-6.18.00-zlib.root) and [sample-5.30.00-zlib.root](https://github.com/scikit-hep/scikit-hep-testdata/blob/main/src/skhep_testdata/data/uproot-sample-5.30.00-zlib.root), which have a \"str\" TBranch of unique values from `\"hey-0\"` through `\"hey-29\"`.\r\n\r\nI also added a print-out for `uproot.__version__` and the initial values of `entry_start` and `entry_stop` in `uproot.dask` to make sure that I'm running what I think I'm running.\r\n\r\n```\r\n% python -c 'import uproot; print(uproot.__version__); print(uproot.dask([\"sample-6.18.00-zlib.root\", \"sample-5.30.00-zlib.root\"], open_files=False, entry_start=0, entry_stop=5)[\"str\"].compute().to_list())'\r\n5.0.9\r\nentry_start = 0 entry_stop = 5\r\n['hey-0', 'hey-1', 'hey-2', 'hey-3', 'hey-4', 'hey-0', 'hey-1', 'hey-2', 'hey-3', 'hey-4']\r\n```\r\n\r\nIt sliced the first 5 of each file.\r\n\r\n```\r\n% python -c 'import uproot; print(uproot.__version__); print(uproot.dask([\"sample-6.18.00-zlib.root\", \"sample-5.30.00-zlib.root\"], open_files=False, entry_start=20, entry_stop=40)[\"str\"].compute().to_list())'\r\n5.0.9\r\nentry_start = 20 entry_stop = 40\r\n['hey-20', 'hey-21', 'hey-22', 'hey-23', 'hey-24', 'hey-25', 'hey-26', 'hey-27', 'hey-28', 'hey-29', 'hey-20', 'hey-21', 'hey-22', 'hey-23', 'hey-24', 'hey-25', 'hey-26', 'hey-27', 'hey-28', 'hey-29']\r\n```\r\n\r\nIt sliced entries 20 through the end (30, not 40) of each file.\r\n\r\nAs a user of this feature, I was expect the entry numbers to be global. There isn't a precedent for that because all of the other multi-file array-fetching functions, [uproot.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html) and [uproot.concatenate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.concatenate.html) do not have `entry_start` and `entry_stop`. ([uproot.concatenate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.concatenate.html) only exists to be an eager version of [uproot.dask](https://uproot.readthedocs.io/en/latest/uproot._dask.dask.html), so the new arguments would need to be ported to it with the same behavior, but that's a secondary thing.)\r\n\r\nAlso, I noticed that this only applies in `open_files=False` mode:\r\n\r\n```\r\n% python -c 'import uproot; print(uproot.__version__); print(uproot.dask([\"sample-6.18.00-zlib.root\", \"sample-5.30.00-zlib.root\"], open_files=True, entry_start=0, entry_stop=5)[\"str\"].compute().to_list())'\r\n5.0.9\r\nentry_start = 0 entry_stop = 5\r\n['hey-0', 'hey-1', 'hey-2', 'hey-3', 'hey-4', 'hey-5', 'hey-6', 'hey-7', 'hey-8', 'hey-9', 'hey-10', 'hey-11', 'hey-12', 'hey-13', 'hey-14', 'hey-15', 'hey-16', 'hey-17', 'hey-18', 'hey-19', 'hey-20', 'hey-21', 'hey-22', 'hey-23', 'hey-24', 'hey-25', 'hey-26', 'hey-27', 'hey-28', 'hey-29', 'hey-0', 'hey-1', 'hey-2', 'hey-3', 'hey-4', 'hey-5', 'hey-6', 'hey-7', 'hey-8', 'hey-9', 'hey-10', 'hey-11', 'hey-12', 'hey-13', 'hey-14', 'hey-15', 'hey-16', 'hey-17', 'hey-18', 'hey-19', 'hey-20', 'hey-21', 'hey-22', 'hey-23', 'hey-24', 'hey-25', 'hey-26', 'hey-27', 'hey-28', 'hey-29']\r\n```\r\n\r\nwhich would also be confusing for a user. (It confused me for the first hour!)\r\n\r\nBut anyway, I think that this is the wrong place to put a work-around. If we want to lazily load a range of entries, the don't-have-to-look-anything-up syntax for that would be\r\n\r\n```python\r\ndelayed_array = uproot.dask([\"all\", \"the\", \"files\"])\r\nresult = delayed_array[entry_start:entry_stop].compute()\r\n```\r\n\r\nThen it wouldn't matter whether the `delayed_array` came from Uproot/a ROOT file or not. It would apply equally to the row-groups of Dask arrays from Parquet.\r\n\r\nFor that reason, I'd like to _close_ this issue _in favor_ of https://github.com/dask-contrib/dask-awkward/issues/265, which would provide this functionality in a more natural and source-independent way.",
  "created_at":"2023-05-15T22:41:43Z",
  "id":1548711090,
  "issue":890,
  "node_id":"IC_kwDOD6Q_ss5cT3Sy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-15T22:41:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Gah, I had forgotten that we use this in `uproot`. Nice catch team.",
  "created_at":"2023-05-16T08:07:32Z",
  "id":1549196600,
  "issue":892,
  "node_id":"IC_kwDOD6Q_ss5cVt04",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-16T08:07:32Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Oops; I had independently fixed this with #957, but didn't see that this one already fixed it. Well, I resolved the conflict.",
  "created_at":"2023-09-14T16:55:01Z",
  "id":1719809686,
  "issue":894,
  "node_id":"IC_kwDOD6Q_ss5mgjaW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-14T16:55:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I want this feature ON for pandas df so my life becomes lot easier. Thanks.\r\n\r\n![image](https://github.com/scikit-hep/uproot5/assets/58833553/7de364e8-a776-46d4-8378-c72b63f1ec8e)\r\n",
  "created_at":"2023-05-23T18:52:40Z",
  "id":1559969777,
  "issue":895,
  "node_id":"IC_kwDOD6Q_ss5c-z_x",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-23T18:52:40Z",
  "user":"MDQ6VXNlcjU4ODMzNTUz"
 },
 {
  "author_association":"MEMBER",
  "body":"@douglasdavis, should I move this issue to https://github.com/intake/awkward-pandas?",
  "created_at":"2023-05-23T18:55:57Z",
  "id":1559973381,
  "issue":895,
  "node_id":"IC_kwDOD6Q_ss5c-04F",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-23T18:55:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes I think that would make sense",
  "created_at":"2023-05-23T19:28:28Z",
  "id":1560011274,
  "issue":895,
  "node_id":"IC_kwDOD6Q_ss5c--IK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-23T19:28:28Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll need a higher level of access (\"Write\" access).",
  "created_at":"2023-05-23T19:35:08Z",
  "id":1560018603,
  "issue":895,
  "node_id":"IC_kwDOD6Q_ss5c-_6r",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-23T19:35:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah, should be taken care of now",
  "created_at":"2023-05-23T19:46:56Z",
  "id":1560032644,
  "issue":895,
  "node_id":"IC_kwDOD6Q_ss5c_DWE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-23T19:46:56Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know why, but GitHub isn't letting me transfer this. I manually transferred it as https://github.com/intake/awkward-pandas/issues/31.",
  "created_at":"2023-05-23T20:00:28Z",
  "id":1560047476,
  "issue":895,
  "node_id":"IC_kwDOD6Q_ss5c_G90",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-23T20:00:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Ah, my mistake was that I had to accept an invitation, first!",
  "created_at":"2023-05-23T20:01:11Z",
  "id":1560048372,
  "issue":895,
  "node_id":"IC_kwDOD6Q_ss5c_HL0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-05-23T20:01:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi! \u2028\u2028\r\n\r\nThe crash happens when the file is hosted on `cernbox`, however it does work when hosted elsewhere (I alos tested this from my own cernbox). Thus, it seems as though this is a `cernbox` issue and not an `uproot` issue. ",
  "created_at":"2023-07-06T17:17:28Z",
  "id":1624035170,
  "issue":896,
  "node_id":"IC_kwDOD6Q_ss5gzM9i",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-06T17:17:28Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"It's sounding like it's not an Uproot issue. A good additional check would be to ask if the same file can be read in ROOT (also using HTTP protocol). If ROOT can read it but Uproot can't... maybe Uproot's `HTTPSource` is not recognizing a redirect or something? (No, that's not it, because then it would never start the file; here the issue is that it's failing in the middle of the file... But maybe something like that.)",
  "created_at":"2023-07-06T18:03:19Z",
  "id":1624106177,
  "issue":896,
  "node_id":"IC_kwDOD6Q_ss5gzeTB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-06T18:03:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hello, as written originally this problem started on the ROOT forum, since the reading is very slow. The thread is very long and it seems related to an issue of the http server serving the file from cernbox. There is also an issue on the root project https://github.com/root-project/root/issues/13018.\r\n\r\nBy the way, with ROOT it is slow, but it works.",
  "created_at":"2023-07-06T19:29:41Z",
  "id":1624205167,
  "issue":896,
  "node_id":"IC_kwDOD6Q_ss5gz2dv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-06T19:31:01Z",
  "user":"MDQ6VXNlcjE0MzM4OQ=="
 },
 {
  "author_association":"MEMBER",
  "body":"I was able to pull all of the TBasket data (individually) with\r\n\r\n```python\r\n>>> import uproot\r\n>>> f = uproot.open(\"https://cernbox.cern.ch/remote.php/dav/public-files/1Cy1HIf03Ca76Dm/test_ntuples_200123.root\")\r\n>>> t = f[\"Electrons_All\"]\r\n>>> baskets = []\r\n>>> for i in range(t[\"pt__NOSYS\"].num_baskets):\r\n...     basket = t[\"pt__NOSYS\"].basket(i)\r\n...     baskets.append(basket)\r\n...     print(i, basket)\r\n... \r\n0 <TBasket 0 of 'pt__NOSYS' at 0x7f953b4b2b50>\r\n1 <TBasket 1 of 'pt__NOSYS' at 0x7f953b4b2af0>\r\n2 <TBasket 2 of 'pt__NOSYS' at 0x7f953b4b2a90>\r\n3 <TBasket 3 of 'pt__NOSYS' at 0x7f953b4b2a30>\r\n4 <TBasket 4 of 'pt__NOSYS' at 0x7f953b4b29d0>\r\n5 <TBasket 5 of 'pt__NOSYS' at 0x7f953b4b28e0>\r\n6 <TBasket 6 of 'pt__NOSYS' at 0x7f953b4b2850>\r\n...\r\n# restarted it when there was a socket timeout at basket 271\r\n...\r\n297 <TBasket 297 of 'pt__NOSYS' at 0x7f953b3bcca0>\r\n298 <TBasket 298 of 'pt__NOSYS' at 0x7f953b3bcd30>\r\n299 <TBasket 299 of 'pt__NOSYS' at 0x7f953b3bcee0>\r\n```\r\n\r\nBecause it's jagged data, I can't do `basket.array()` to test interpretations. None of them are oddly small:\r\n\r\n```python\r\n>>> [len(x.raw_data) for x in baskets]\r\n[4472, 4576, 4640, 4440, 4496, 4816, 4600, 4592, 4688, 4600, 4528, 4672, 4688, 4680, 4536, 4616, 4536, 4456, 4648, 4528, 4656, 4768, 4368, 4640, 4720, 4784, 4824, 4760, 4656, 4560, 4800, 4640, 4560, 4632, 4776, 4848, 4648, 4640, 4576, 4544, 4664, 4472, 4520, 4472, 4728, 4800, 4536, 4712, 4704, 4408, 4720, 4512, 4528, 4624, 4760, 4560, 4480, 4640, 4776, 4752, 4616, 4600, 4576, 4600, 4784, 4568, 4808, 4608, 4632, 4624, 4680, 4624, 4728, 4664, 4568, 4544, 4688, 4680, 4728, 4712, 4768, 4480, 4688, 4680, 4568, 4744, 4744, 4464, 4712, 4512, 4696, 4712, 4632, 4688, 4672, 4600, 4840, 4520, 4712, 4480, 4632, 4592, 4560, 4616, 4680, 4656, 4744, 4528, 4624, 4760, 4680, 4552, 4688, 4632, 4664, 4624, 4632, 4512, 4728, 4616, 4712, 4712, 4760, 4536, 4776, 4632, 4688, 4560, 4656, 4800, 4632, 4600, 4648, 4608, 4744, 4496, 4464, 4744, 4776, 4640, 4608, 4440, 4568, 4576, 4816, 4512, 4576, 4760, 4728, 4584, 4480, 4720, 4608, 4672, 4640, 4832, 4576, 4664, 4600, 4688, 4752, 4808, 4656, 4760, 4664, 4720, 4424, 4696, 4648, 4656, 4632, 4624, 4584, 4792, 4552, 4576, 4728, 4672, 4640, 4576, 4512, 4680, 4728, 4720, 4792, 4672, 4680, 4720, 4712, 4688, 4584, 4528, 4672, 4624, 4712, 4736, 4696, 4672, 4456, 4456, 4688, 4648, 4536, 4672, 4688, 4552, 4632, 4592, 4568, 4664, 4544, 4552, 4696, 4600, 4488, 4592, 4640, 4592, 4576, 4648, 4408, 4544, 4800, 4656, 4584, 4688, 4536, 4800, 4792, 4632, 4792, 4768, 4592, 4728, 4648, 4736, 4744, 4528, 4592, 4824, 4688, 4472, 4640, 4576, 4480, 4576, 4640, 4704, 4768, 4584, 4680, 4752, 4560, 4832, 4456, 4736, 4616, 4768, 4592, 4568, 4504, 4584, 4544, 4760, 4648, 4912, 4544, 4664, 4616, 4632, 4696, 4680, 4488, 4480, 4800, 4712, 4808, 4544, 4632, 4560, 4808, 4696, 4472, 4704, 4560, 4640, 4704, 4632, 4656, 4640, 4808, 4480, 4648, 4568, 4776, 4664, 4640, 4728, 4576, 4672]\r\n```\r\n\r\nMaybe a smarter thing would be if I had run `TBranch.array` with `entry_start` and `entry_stop` set by `t[\"pt__NOSYS\"].member(\"fBasketEntry\")`. I suspect it's one bad TBasket.\r\n\r\nReading the first TBasket is fine:\r\n\r\n```python\r\n>>> t[\"pt__NOSYS\"].array(entry_start=0, entry_stop=10)\r\n<Array [[33.6, 17.1], [], ..., [50.3, 42.3], []] type='10 * var * float64'>\r\n```",
  "created_at":"2023-07-06T19:43:44Z",
  "id":1624220329,
  "issue":896,
  "node_id":"IC_kwDOD6Q_ss5gz6Kp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-06T19:43:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I just tried reading the file locally, and there are no problems there, either:\r\n\r\n```python\r\n>>> import uproot\r\n>>> f = uproot.open(\"test_ntuples_200123.root\")\r\n>>> f[\"Electrons_All\"][\"pt__NOSYS\"].array()\r\n<Array [[33.6, 17.1], [], ..., [], [63.4, 23.7]] type='60000 * var * float64'>\r\n>>> f[\"Electrons_All\"].arrays(\"pt__NOSYS\")\r\n<Array [{pt__NOSYS: [33.6, ...]}, ..., {...}] type='60000 * {pt__NOSYS: var...'>\r\n```\r\n\r\nSo it works locally, it works through HTTP on the rgw.fisica.unimi.it server, but it just doesn't work on CERNBox. That seems to narrow it down to CERNBox, a server problem. It manifests itself as a deserialization error, but that's what would happen if a server returned wrong bytes.\r\n\r\nCould this be taken up with CERNBox? (I'm wondering how we could provide them with a minimal reproducer. We'd want to show them how the bytes that come from one of the chunks is different when it's going through CERNBox than when it's going through rgw.fisica.unimi.it or local files.)\r\n\r\nI'll be closing this here, though.",
  "created_at":"2023-08-03T13:58:06Z",
  "id":1664034549,
  "issue":896,
  "node_id":"IC_kwDOD6Q_ss5jLyb1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-03T13:58:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Same exact problem! Thanks for noting that importing it in the main scope solves the issue!",
  "created_at":"2023-08-07T09:47:25Z",
  "id":1667543801,
  "issue":897,
  "node_id":"IC_kwDOD6Q_ss5jZLL5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-07T09:47:25Z",
  "user":"MDQ6VXNlcjQ5MjA0NTE0"
 },
 {
  "author_association":"MEMBER",
  "body":"This is an interesting puzzle! I can provide some information about what it's doing, though I'm glad that you found a workaround.\r\n\r\n`uproot.from_pyroot` and `uproot.to_pyroot` were implemented ([here](https://github.com/scikit-hep/uproot5/blob/main/src/uproot/pyroot.py)) as side-features, taking advantage of the fact that Uproot can deserialize most ROOT objects (`from_pyroot`) and serialize a few types of ROOT objects (`to_pyroot`). Instead of reading and writing to disk, we read from and write to a [TMessage](https://root.cern/doc/master/classTMessage.html), which comes from or is sent to ROOT via PyROOT. If Uproot does not recognize the class (`from_pyroot` only, and shouldn't apply to recent versions of `TProfile3D`), then we also get the [TStreamerInfo](https://root.cern.ch/doc/master/classTStreamerInfo.html) through a [TMemFile](https://root.cern.ch/doc/master/classTMemFile.html), which models an entire file in memory, rather than just a single object, so that we can decode the streamers and learn how to deserialize that class.\r\n\r\nI don't see what any of this has to do with importing early or importing late. Perhaps there's some danger in the fact that we use ROOT to compile some C++ code to do some things we can't do on the Python side (defining a subclass of TMessage [here](https://github.com/scikit-hep/uproot5/blob/a8644df7a96ee2d788adb1e35f326ba160bedf48/src/uproot/pyroot.py#L45-L52) and using it to reallocate the TMessage [here](https://github.com/scikit-hep/uproot5/blob/a8644df7a96ee2d788adb1e35f326ba160bedf48/src/uproot/pyroot.py#L92-L119)). Those class definitions are compiled exactly once, protected by module-level Python variables that indicate whether we're in the pre-compilation state or the post-compilation state.\r\n\r\nWhen Python imports a module in a restricted scope (like `function`), access to the names go out of scope, but the module definitions continue to exist; they are not deallocated. It won't forget that it's already compiled these C++ functions, for instance.\r\n\r\nI don't see anything, but I'll keep thinking about it in the back of my mind.",
  "created_at":"2023-08-07T14:36:52Z",
  "id":1667989398,
  "issue":897,
  "node_id":"IC_kwDOD6Q_ss5ja3-W",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-07T14:36:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This nerdsniped me.\r\n\r\nIt turns out we expect `uproot` to be a global in `pyroot.py`, e.g.\r\n```cpp\r\n    _Uproot_buffer_sizer* ptr = reinterpret_cast<_Uproot_buffer_sizer*>(\r\n        (void*)TPython::Eval(\"uproot.pyroot.pyroot_to_buffer.sizer\")\r\n    );\r\n```\r\n\r\nWe can \"fix\" this with an import expression:\r\n\r\n```cpp\r\n    _Uproot_buffer_sizer* ptr = reinterpret_cast<_Uproot_buffer_sizer*>(\r\n        (void*)TPython::Eval(\"__import__('uproot').pyroot.pyroot_to_buffer.sizer\")\r\n    );\r\n```\r\n\r\nThere are several places this must be done in this C++ string.",
  "created_at":"2023-08-07T15:31:34Z",
  "id":1668096971,
  "issue":897,
  "node_id":"IC_kwDOD6Q_ss5jbSPL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-07T15:31:34Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh yeah! That's right! It exists in `sys.modules`, but these `TPython::Eval` are being called in some Python scope (in ROOT.py?), and the Uproot global variable is not in that scope. It makes me wonder now why this ever worked, since I would have thought that the `TPython::Eval` scope would be some private-ROOT thing. But maybe not. Anyway, this should be an easy fix, and I'll try it. Thanks!",
  "created_at":"2023-08-07T15:43:39Z",
  "id":1668117373,
  "issue":897,
  "node_id":"IC_kwDOD6Q_ss5jbXN9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-07T15:43:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Also, I think we'll need to rename \"chunk\" \u2192 \"partition.\" Uproot has a specific meaning for [Chunk](https://github.com/scikit-hep/uproot5/blob/2d24309dffcb8eccd15687c37bb11108a4490f9f/src/uproot/source/chunk.py#L217-L470) that we don't want to confuse this with.\r\n\r\nAlthough dask-array uses the word \"chunk,\" I'm not seeing that in dask-awkward, which has `npartitions` and `divisions`.\r\n\r\nI can do that, since I've got the files open.\r\n\r\n--------------\r\n\r\nUnrelated: if you specify chunks/partitions in the `files`, it will have to complain about using `steps_per_file` or `step_size`. We can't have people guessing about which takes precedence.",
  "created_at":"2023-06-08T20:42:08Z",
  "id":1583313547,
  "issue":898,
  "node_id":"IC_kwDOD6Q_ss5eX3KL",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-06-08T20:42:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> \"chunk\" \u2192 \"partition\"\r\n\r\nAck! Everywhere else in `uproot.dask`, we call these things \"steps.\" That's to align with the convention set up `uproot.iterate`. Since we want it to be clear that these things are mutually exclusive with `step_size` and `steps_per_file`, I guess a better replacement is \"chunk\" \u2192 \"step\".",
  "created_at":"2023-06-08T20:54:36Z",
  "id":1583329617,
  "issue":898,
  "node_id":"IC_kwDOD6Q_ss5eX7FR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-08T20:54:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif, I asked for your review on this PR because I contributed to it. (Reviewers shouldn't be authors.)",
  "created_at":"2023-06-08T23:37:10Z",
  "id":1583605663,
  "issue":898,
  "node_id":"IC_kwDOD6Q_ss5eY-ef",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-08T23:37:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"An optional TODO would be to re-implement also the alignment along cluster boundaries as done in https://github.com/CoffeaTeam/coffea/blob/f8a4eb97137e84dd52474d26b8100174da196b57/src/coffea/processor/executor.py#L115",
  "created_at":"2023-06-09T15:44:00Z",
  "id":1584792213,
  "issue":898,
  "node_id":"IC_kwDOD6Q_ss5edgKV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-09T15:44:00Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@nsmith- IMO that kind of logic should go in user code or be provided by another library. I don't think uproot should be making those kinds of decisions.\r\n\r\nBetter to have some code that does it and makes the cluster-aligned list of chunks.\r\n\r\nOTOH implemented as a free function it kinda makes sense here so people can apply it or have a reference implementation of a chunk calculator.",
  "created_at":"2023-06-09T15:57:36Z",
  "id":1584808629,
  "issue":898,
  "node_id":"IC_kwDOD6Q_ss5edkK1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-09T15:57:36Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"at @agoose77 I opened #901 which is based on the branch in this PR; I think I covered all 4 cases (the 2 callable classes and also the 2 possible full forms- the remapped one or the raw one)",
  "created_at":"2023-06-09T16:37:30Z",
  "id":1584856270,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5edvzO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-09T16:37:30Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"do we need a fairly immediate uproot 5.0.10?",
  "created_at":"2023-06-27T21:49:55Z",
  "id":1610266145,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5f-rYh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-27T21:49:55Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"If this is working I would think it warrants a release. But I just tested the branch on @iasonkrom's example:\r\n\r\n```python\r\nfrom coffea.nanoevents import NanoAODSchema, NanoEventsFactory\r\nimport os\r\n\r\nfname = \"coffea/tests/samples/nano_dy.root\"\r\nevents = NanoEventsFactory.from_root(\r\n    {os.path.abspath(fname): \"Events\"},\r\n    schemaclass=NanoAODSchema,\r\n    metadata={\"dataset\": \"DYJets\"},\r\n    permit_dask=True,\r\n).events()\r\n\r\nprint(len(events.Electron))\r\n```\r\n\r\nAnd we have a problem in either in unproject_layout or in the way we're calling it here in uproot. I haven't dug into this yet.\r\n\r\n\r\n```python\r\n>>> from coffea.nanoevents import NanoAODSchema, NanoEventsFactory\r\n>>> import os\r\n>>> \r\n>>> fname = \"coffea/tests/samples/nano_dy.root\"\r\n>>> events = NanoEventsFactory.from_root(\r\n...     {os.path.abspath(fname): \"Events\"},\r\n...     schemaclass=NanoAODSchema,\r\n...     metadata={\"dataset\": \"DYJets\"},\r\n...     permit_dask=True,\r\n... ).events()\r\n/Users/ddavis/software/repos/coffea/src/coffea/nanoevents/schemas/nanoaod.py:215: RuntimeWarning: Missing cross-reference index for FatJet_genJetAK8Idx => GenJetAK8\r\n  warnings.warn(\r\n>>> \r\n>>> print(len(events.Electron))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask_awkward/lib/core.py\", line 565, in __len__\r\n    self.eager_compute_divisions()\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask_awkward/lib/core.py\", line 1203, in eager_compute_divisions\r\n    self._divisions = calculate_known_divisions(self)\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask_awkward/lib/core.py\", line 1838, in calculate_known_divisions\r\n    return (0, num.compute())\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/base.py\", line 310, in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/base.py\", line 595, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/threaded.py\", line 89, in get\r\n    results = get_async(\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/local.py\", line 511, in get_async\r\n    raise_exception(exc, tb)\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/local.py\", line 319, in reraise\r\n    raise exc\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/local.py\", line 224, in execute_task\r\n    result = _execute_task(task, data)\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/core.py\", line 121, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/optimization.py\", line 992, in __call__\r\n    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/core.py\", line 151, in get\r\n    result = _execute_task(task, cache)\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/core.py\", line 121, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/core.py\", line 121, in <genexpr>\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/core.py\", line 121, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/core.py\", line 121, in <genexpr>\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask/core.py\", line 121, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/ddavis/software/repos/uproot5/src/uproot/_dask.py\", line 932, in __call__\r\n    dask_awkward.lib.unproject_layout.unproject_layout(\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask_awkward/lib/unproject_layout.py\", line 406, in unproject_layout\r\n    return _unproject_layout(form, layout, layout.length, layout.backend)\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/dask_awkward/lib/unproject_layout.py\", line 267, in _unproject_layout\r\n    form.content(field),\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/awkward/forms/recordform.py\", line 143, in content\r\n    index = self.field_to_index(index_or_field)\r\n  File \"/Users/ddavis/.pyenv/versions/cafe/lib/python3.10/site-packages/awkward/forms/recordform.py\", line 117, in field_to_index\r\n    i = self._fields.index(field)\r\nAttributeError: 'dict_keys' object has no attribute 'index'\r\n```",
  "created_at":"2023-06-28T16:02:09Z",
  "id":1611702361,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5gEKBZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-28T16:02:09Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This is an Awkward bug that's fixed in #scikit-hep/awkward#2548!",
  "created_at":"2023-06-28T16:08:03Z",
  "id":1611711878,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5gEMWG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-28T16:08:03Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"oh, awesome!",
  "created_at":"2023-06-28T16:08:33Z",
  "id":1611712640,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5gEMiA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-28T16:08:33Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"and @iasonkrom's snippet now works (on `awkward@main`) :+1:",
  "created_at":"2023-06-28T16:10:21Z",
  "id":1611715698,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5gENRy",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":2,
   "total_count":2
  },
  "updated_at":"2023-06-28T16:10:34Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Is this done? More to the point, is it needed to fix a dask-awkward bug? I didn't get a notification about it because I wasn't pinged for review.",
  "created_at":"2023-06-29T21:56:43Z",
  "id":1613855429,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5gMXrF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-29T21:56:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"iiuc, at least for the downstream issues in nanoevents it depends on first the next awkward release?",
  "created_at":"2023-06-29T21:58:12Z",
  "id":1613856690,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5gMX-y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-29T21:58:12Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"though I don't see why we cannot merge it now (aside from perhaps a pinning issue, which may more appropriately be taken care of in coffea).",
  "created_at":"2023-06-29T21:58:48Z",
  "id":1613857286,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5gMYIG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-29T21:58:48Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"This PR can wait for the next Awkward release, which is coming up pretty soon anyway.",
  "created_at":"2023-06-29T22:27:01Z",
  "id":1613878487,
  "issue":900,
  "node_id":"IC_kwDOD6Q_ss5gMdTX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-29T22:27:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"After looking at all the sites where `RUF012` was failing, I realized that it's trying to enforce particular type annotations on a codebase that largely or entirely lacks type annotations. It would be weird to have only these type annotations and no others.\r\n\r\nI know that `typing.Any` can be put to good use, but Uproot is a library that extracts data from files and the type of the data depends on the values in the file. There are a few other libraries and functions like this, such as `json.loads`, which can't have a useful return type. In Uproot, the problem function is `ReadOnlyDirectory.__getitem__`. Even if all of the other Uproot functions are well typed, a type-check of user code using Uproot would have to make an unverifyable type assertion on `ReadOnlyDirectory.__getitem__` for any of the subsequent type-checks to be constraining.\r\n\r\nNot that we should give up on typing libraries like Uproot and `json`\u2014I'm just saying that they're in a special category, in which more work is needed to make type annotations useful than usual.",
  "created_at":"2023-06-29T22:25:29Z",
  "id":1613877418,
  "issue":902,
  "node_id":"IC_kwDOD6Q_ss5gMdCq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-29T22:25:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@iasonkrom can you give this PR a spin and see if it fixes your tests without column-opt turned on?",
  "created_at":"2023-06-19T00:41:50Z",
  "id":1596330781,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fJhMd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T00:41:50Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"NONE",
  "body":"Yup, they do pass! The only problem is that they take way too long without column optimization. Testing the analysis tools now went up from 1 min to 1 hr.  Can this be sped up by maybe reading way less with `NanoEventsFactory`? ",
  "created_at":"2023-06-19T09:28:49Z",
  "id":1596836793,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fLcu5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T09:28:49Z",
  "user":"MDQ6VXNlcjgyMTU1NDA0"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"How big is the file you're testing on?",
  "created_at":"2023-06-19T11:39:07Z",
  "id":1597030075,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fML67",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T11:39:07Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"NONE",
  "body":"It's the `nano_dy` sample. It's crazy slow when `NanoEventsFactory` is in the way. With just `uproot.dask` it's fast",
  "created_at":"2023-06-19T11:40:28Z",
  "id":1597031573,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fMMSV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T11:40:28Z",
  "user":"MDQ6VXNlcjgyMTU1NDA0"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hmm it's not that slow in eager mode either. I'll take a closer look tomorrow, this seems to be some funny code path.\r\n\r\n@jpivarski @ioanaif please don't merge this until we get decent performance out of it.",
  "created_at":"2023-06-19T11:45:03Z",
  "id":1597036829,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fMNkd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T11:45:03Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"NONE",
  "body":"To put this into perspective, running the tests without this PR (where dak nanoevents tests fail) gives `2 failed, 26 passed, 1 warning in 117.57s (0:01:57)`.\r\nWith this PR, it took slightly over an hour for all 28 tests to pass which means nanoevents with column-opt set to false is the bottlebeck",
  "created_at":"2023-06-19T14:07:38Z",
  "id":1597257992,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fNDkI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T14:07:38Z",
  "user":"MDQ6VXNlcjgyMTU1NDA0"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"If you can run a profile (with pyinstrument) to see where it's spending all its time, it'll help.",
  "created_at":"2023-06-19T14:12:05Z",
  "id":1597266489,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fNFo5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T14:12:05Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"NONE",
  "body":"Letting it run and I'll send the output html file over.",
  "created_at":"2023-06-19T14:20:41Z",
  "id":1597280378,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fNJB6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T14:20:41Z",
  "user":"MDQ6VXNlcjgyMTU1NDA0"
 },
 {
  "author_association":"NONE",
  "body":"Does this help at all?\r\nhttps://www.dropbox.com/scl/fi/ujr8a7azbloai8vcj7dye/profile_analysis_tools.html?dl=0&rlkey=fmjkxkr9h3s1cubb2s6esl7sm",
  "created_at":"2023-06-19T15:18:23Z",
  "id":1597370306,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fNe_C",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T15:18:23Z",
  "user":"MDQ6VXNlcjgyMTU1NDA0"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"no, not really. I'll take a look tomorrow then. Thanks!",
  "created_at":"2023-06-19T15:21:30Z",
  "id":1597374878,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fNgGe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-19T15:21:41Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@iasonkrom I found the fix! `form.select_columns` seems to have quadratic or worse scaling. Now I just check if the list of request columns is the same as the full set of columns and use the un-projected form if it is the case.\r\n\r\n```\r\nx = events.Muon.pt #.calculate(scheduler=\"sync\")\r\n```\r\ntakes about 5 minutes in either eager mode or delayed mode nanoevents, the latter with column optimization off.\r\n\r\nCan you give this a try on your tests?",
  "created_at":"2023-06-20T19:24:40Z",
  "id":1599379487,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fVJgf",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2023-06-20T19:25:54Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"NONE",
  "body":"<img width=\"1023\" alt=\"image\" src=\"https://github.com/scikit-hep/uproot5/assets/82155404/9a358de6-0412-4f33-9666-6936e6557ede\">\r\n\r\nYup, looks good!",
  "created_at":"2023-06-20T19:32:47Z",
  "id":1599389676,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fVL_s",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-06-20T19:32:47Z",
  "user":"MDQ6VXNlcjgyMTU1NDA0"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski @ioanaif this is ready to merge",
  "created_at":"2023-06-20T19:40:29Z",
  "id":1599399275,
  "issue":905,
  "node_id":"IC_kwDOD6Q_ss5fVOVr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-20T19:40:29Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"This is another high-priority item, like the one I mentioned in https://github.com/scikit-hep/uproot5/issues/908#issuecomment-1608423979. (And, similarly, it has been staring at me in my email inbox while I've been thinking about how to devote some time to it.)",
  "created_at":"2023-06-26T22:53:27Z",
  "id":1608425019,
  "issue":906,
  "node_id":"IC_kwDOD6Q_ss5f3p47",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-26T22:53:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've had this in my email inbox for a week in order to say something intelligent about it, but all I can say is, yes: that would be a very good feature to have. Hopefully, we'll be able to get more effort on Uproot because there are a few important requests like this one.\r\n\r\nI'll just note that this is a high-priority item.",
  "created_at":"2023-06-26T22:51:49Z",
  "id":1608423979,
  "issue":908,
  "node_id":"IC_kwDOD6Q_ss5f3por",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-26T22:51:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for your response! I know that I'm making a request for a new feature without offering to actually help on the implementation, so I have no expectations about the timeline. (From what I can tell, it needs some expertise on PlottableHistogram that I don't have, but if you think it's something that could be completed in a couple of hours for someone new to the uproot internals, I'm willing to give it a shot).\r\n\r\nAs always, thanks for all of your efforts!",
  "created_at":"2023-06-27T03:46:25Z",
  "id":1608742047,
  "issue":908,
  "node_id":"IC_kwDOD6Q_ss5f43Sf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-06-27T03:46:25Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"I should explicitly let you know that I've been keeping this PR in my email inbox because I think it's important, but I just realized that I haven't said anything about it yet.\r\n\r\n(I'm in a talk about open source practices and I just guiltily remembered that this is still open.)",
  "created_at":"2023-07-12T19:19:30Z",
  "id":1633082507,
  "issue":916,
  "node_id":"IC_kwDOD6Q_ss5hVtyL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-12T19:19:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Another approach to setting up a CI test for S3 is to spin up a Minio server as part of the job, write an S3 file to it with their client, then connect to it with uproot. Something like https://github.com/fclairamb/docker-minio-github-actions.",
  "created_at":"2023-07-13T02:04:35Z",
  "id":1633426843,
  "issue":916,
  "node_id":"IC_kwDOD6Q_ss5hXB2b",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-13T02:04:35Z",
  "user":"MDQ6VXNlcjQ2NTYzOTE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi! \r\nThis looks ready to go, it only misses testing. Once the tests are added (and they pass) we can merge this.\r\n\r\n*The S3 files will be persist so the test suite will be reliable ",
  "created_at":"2023-07-18T14:45:02Z",
  "id":1640369201,
  "issue":916,
  "node_id":"IC_kwDOD6Q_ss5hxgwx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-18T14:45:02Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Can I point out that if we were to take fsspec dependency as in #692 we would have S3 for free? That said, it was not finished.",
  "created_at":"2023-07-21T14:18:49Z",
  "id":1645667331,
  "issue":916,
  "node_id":"IC_kwDOD6Q_ss5iFuQD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-21T14:18:49Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks, @nsmith-, for clarifying. It's what I meant by\r\n\r\n> I should also let you know that we plan to revive #692, which should eventually run all backends through fsspec, and that would include S3. But it's better to add this feature now and have it be useful before the long-term plan becomes a reality.\r\n\r\nAt that point in the future, we would roll back this explicit implementation (and probably release a new minor version of Uproot). But this will be useful in the interim.",
  "created_at":"2023-07-21T14:25:18Z",
  "id":1645677489,
  "issue":916,
  "node_id":"IC_kwDOD6Q_ss5iFwux",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-21T14:25:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Apologies! I failed to read your whole comment. Indeed this is useful in interim.",
  "created_at":"2023-07-21T14:41:01Z",
  "id":1645700960,
  "issue":916,
  "node_id":"IC_kwDOD6Q_ss5iF2dg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-21T14:41:01Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi! It looks like this is done\u2014is it? (I wasn't sure, because we weren't requested for a new review.)",
  "created_at":"2023-08-10T13:19:50Z",
  "id":1673208608,
  "issue":916,
  "node_id":"IC_kwDOD6Q_ss5juyMg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-10T13:19:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes. I think this is ready.",
  "created_at":"2023-08-10T18:54:13Z",
  "id":1673742457,
  "issue":916,
  "node_id":"IC_kwDOD6Q_ss5jw0h5",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-08-10T18:54:13Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a major version mismatch (that is, the `X` in `X.Y.Z`). Uproot v5 does not have this function (removed in #770), and these libraries need to be matched up by major version number:\r\n\r\n| | old | new |\r\n|:-|:-:|:-:|\r\n| Uproot | 4.* | 5.* |\r\n| Awkward Array | 1.* | 2.* |\r\n| Coffea | 0.7.* | 2023.* |\r\n\r\nA package manager is supposed to maintain these version constraints, but maybe you installed differently? Or you have multiple installations and they're not fully isolated?",
  "created_at":"2023-07-11T19:54:26Z",
  "id":1631427390,
  "issue":919,
  "node_id":"IC_kwDOD6Q_ss5hPZs-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-11T19:54:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Just had one of the HATs students run into this yesterday evening, from trying to install coffea in conda. I didn't have them check the awkward version, but coffea 0.7.* pulled in uproot 5.*",
  "created_at":"2023-07-11T19:58:28Z",
  "id":1631432210,
  "issue":919,
  "node_id":"IC_kwDOD6Q_ss5hPa4S",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-11T19:58:28Z",
  "user":"MDQ6VXNlcjM4MjE3Mjc0"
 },
 {
  "author_association":"MEMBER",
  "body":"If conda is installing an incompatible combination, then this is a bug in [conda-forge/coffea-feedstock](https://github.com/conda-forge/coffea-feedstock). If you're still seeing it, raise that issue there. For now, though, I'll be closing this issue here since it's not Uproot-related.",
  "created_at":"2023-08-03T13:36:11Z",
  "id":1663998707,
  "issue":919,
  "node_id":"IC_kwDOD6Q_ss5jLprz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-03T13:36:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think it's not the length of the filename, but the fact that it has colons in it. If so, we can add this issue to https://github.com/CoffeaTeam/coffea/pull/771#issue-1616039711...\r\n\r\n  * https://github.com/scikit-hep/uproot5/issues/47\r\n  * https://github.com/scikit-hep/uproot5/issues/79\r\n  * https://github.com/scikit-hep/uproot5/pull/80\r\n  * https://github.com/scikit-hep/uproot5/pull/81\r\n  * https://github.com/scikit-hep/uproot5/issues/129\r\n  * https://github.com/scikit-hep/uproot5/discussions/365\r\n  * https://github.com/scikit-hep/uproot5/discussions/541\r\n  * https://github.com/scikit-hep/uproot5/discussions/543\r\n  * https://github.com/scikit-hep/uproot5/issues/669\r\n  * https://github.com/scikit-hep/uproot5/pull/670\r\n\r\nIf this is it, there are two workarounds (either one works):\r\n\r\n  1. Make the filename a `pathlib.Path`, rather than a `str`.\r\n  2. Replace filename string `x` with dict `{x: None}`. This one works because the colon is splitting between the filesystem-path and the object-inside-ROOT-file-path, and the dict syntax is another way of doing that split, with `None` for no object-inside-ROOT-file-path.\r\n\r\nI wish, I wish, I wish I could remove this colon feature, but when it was absent in Uproot 4.0 (because 4.0 was allowed to make backward-breaking changes from 3.x), users missed it (and thought that Uproot 4.0 was incapable of reading files that were readable in 3.x). In my [investigations into API usage](https://indico.jlab.org/event/459/contributions/11547/), I found that it's not an uncommonly used feature; people would still miss it if it disappears again.",
  "created_at":"2023-07-12T18:56:23Z",
  "id":1633050022,
  "issue":920,
  "node_id":"IC_kwDOD6Q_ss5hVl2m",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-07-12T18:56:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Great, thanks a lot for the fix!",
  "created_at":"2023-08-17T08:44:38Z",
  "id":1681880404,
  "issue":922,
  "node_id":"IC_kwDOD6Q_ss5kP3VU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-17T08:44:38Z",
  "user":"MDQ6VXNlcjE3NDU0ODQ4"
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know, @mramospe and @franciscosili, if this PR fixes the issue in your context. As you can see above, I wasn't able to reproduce it.",
  "created_at":"2023-08-07T16:01:47Z",
  "id":1668150692,
  "issue":927,
  "node_id":"IC_kwDOD6Q_ss5jbfWk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-07T16:01:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I can't reproduce with your test, but this one works for me\r\n```python\r\n# BSD 3-Clause License; see https://github.com/scikit-hep/uproot5/blob/main/LICENSE\r\n\r\n\r\nimport ROOT\r\n\r\n\r\ndef test_import():\r\n    h = ROOT.TProfile3D()\r\n    __import__(\"uproot\").from_pyroot(h)\r\n```",
  "created_at":"2023-08-07T16:39:55Z",
  "id":1668235357,
  "issue":927,
  "node_id":"IC_kwDOD6Q_ss5jb0Bd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-07T16:39:55Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I verified that TProfile3D raises a segfault in `main` (whereas TH1F does not), but it's fixed in this PR. I wonder what's special about TProfile3D\u2014maybe there's a new class version for it? Or maybe we don't have predefined streamers for it?\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/a8644df7a96ee2d788adb1e35f326ba160bedf48/src/uproot/behaviors/TH3.py#L4-L5\r\n\r\nWell, this does verify that there's a segfault without the fix and no segfault with the fix, so I'll ask @ioanaif to review this PR and merge it if everything's good. (I'm not planning on making any more changes.)",
  "created_at":"2023-08-07T16:57:24Z",
  "id":1668258751,
  "issue":927,
  "node_id":"IC_kwDOD6Q_ss5jb5u_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-07T16:57:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Let me know, @mramospe and @franciscosili, if this PR fixes the issue in your context. As you can see above, I wasn't able to reproduce it.\r\n\r\nWorks for me now. Thanks a lot!",
  "created_at":"2023-08-21T15:57:04Z",
  "id":1686596465,
  "issue":927,
  "node_id":"IC_kwDOD6Q_ss5kh2tx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-21T15:57:04Z",
  "user":"MDQ6VXNlcjE0ODcyMDY2"
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif has started to look into this, and found that some of the changes are headers/metadata, while another is that [variable-length integers](https://lucene.apache.org/core/3_5_0/fileformats.html#VInt) and [zig-zag encoding](https://protobuf.dev/programming-guides/encoding/#signed-ints) are now included in RNTuple.\r\n\r\nConversion to and from a variable-length integer format is not NumPy-vectorizable: it is necessary to write for loops to do this conversion. Here's what that looks like in Python:\r\n\r\n```python\r\ndef to_varint(data):\r\n    assert issubclass(data.dtype.type, np.uint64)\r\n\r\n    output = []\r\n    for value in data:\r\n        mask = np.uint64(0x7f)\r\n        more = np.uint64(np.iinfo(np.uint64).max)\r\n        for shift in np.arange(0, 7 * 9, 7, dtype=np.uint64):\r\n            byte = ((value & mask) >> shift).astype(np.uint8)\r\n            mask <<= np.uint64(7)\r\n            more <<= np.uint64(7)\r\n\r\n            if not (value & more):\r\n                output.append(byte)\r\n                break\r\n            else:\r\n                output.append(byte | np.uint8(0x80))\r\n\r\n    return b\"\".join(output)\r\n\r\ndef from_varint(buffer):\r\n    data = []\r\n    pos = 0\r\n    while pos < len(buffer):\r\n        shift = np.uint64(0)\r\n        result = np.uint64(0)\r\n        while True:\r\n            byte = np.uint64(buffer[pos])\r\n            pos += 1\r\n\r\n            if shift == 7 * 9:\r\n                raise Exception(\"number is too big for uint64\")\r\n\r\n            result |= (byte & np.uint64(0x7f)) << shift\r\n            shift += np.uint64(7)\r\n\r\n            if not (byte & np.uint64(0x80)):\r\n                break\r\n\r\n        data.append(result)\r\n\r\n    return np.array(data)\r\n```\r\n\r\n```python\r\n>>> data = np.array([0, 1, 2, 127, 128, 129, 130, 16383, 16384, 16385], np.uint64)\r\n>>> data.tolist()\r\n[0, 1, 2, 127, 128, 129, 130, 16383, 16384, 16385]\r\n\r\n>>> buffer = to_varint(data)\r\n>>> buffer\r\nb'\\x00\\x01\\x02\\x7f\\x80\\x01\\x81\\x01\\x82\\x01\\xff\\x7f\\x80\\x80\\x01\\x81\\x80\\x01'\r\n\r\n>>> from_varint(buffer).tolist()\r\n[0, 1, 2, 127, 128, 129, 130, 16383, 16384, 16385]\r\n```\r\n\r\nBut we want to avoid Python for loops. AwkwardForth provides a way to do that when converting _from_ a variable-length encoding into integers. Here's how it can be done:\r\n\r\n```python\r\n>>> from awkward.forth import ForthMachine64\r\n>>> vm = ForthMachine64(\"\"\"\r\n... input buffer\r\n... output data uint64\r\n... \r\n... begin\r\n...     buffer varint-> data\r\n... again\r\n... \"\"\")\r\n\r\n>>> buffer = to_varint(np.array([0, 1, 2, 127, 128, 129, 130, 16383, 16384, 16385], np.uint64))\r\n\r\n>>> vm.run({\"buffer\": buffer}, raise_read_beyond=False)\r\n'read beyond'\r\n>>> vm.outputs[\"data\"]\r\narray([    0,     1,     2,   127,   128,   129,   130, 16383, 16384,\r\n       16385], dtype=uint64)\r\n```\r\n\r\nWhat the above does:\r\n\r\n* declares [input and output buffers](https://awkward-array.org/doc/main/reference/awkwardforth.html#variables-inputs-and-outputs) (inputs are given; outputs are created and grow as needed)\r\n* uses the [`varint->`](https://awkward-array.org/doc/main/reference/awkwardforth.html#variable-length-integers) word to decode one variable-length integer from the input `buffer` to the output `data`\r\n* uses the [`begin .. again` construct](https://awkward-array.org/doc/main/reference/awkwardforth.html#begin-again) to do an infinite loop (like `while True`)\r\n* uses `raise_read_beyond=False` to catch a \"read beyond length of input buffer\" exception and return it as a string instead of raising a Python exception.\r\n\r\nBut since AwkwardForth is compiled code, it's a lot faster than pure Python:\r\n\r\n```python\r\n%%timeit\r\nfrom_varint(buffer);\r\n# 5.11 s \u00b1 15.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n%%timeit\r\nvm.run({\"buffer\": buffer}, raise_read_beyond=False);\r\n# 21.3 ms \u00b1 114 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\n(240\u00d7 faster, in this case).\r\n\r\nThe value of this encoding is that integers that are close to zero use the fewest bytes, but any integer can be encoded (including integers larger than `uint64`, though that's all we care about when NumPy is involved).\r\n\r\nYou could do the same thing with signed integers, but small signed integers like `-1` would be encoded with the most bytes, because `-1` is `0xffffffffffffffff` in `int64`.\r\n\r\n```python\r\n>>> hex(np.int64(-1).view(np.uint64))\r\n'0xffffffffffffffff'\r\n```\r\n\r\nSo, on top of the variable-length encoding, signed integers are zig-zag encoded, mapping e.g. `[0, -1, 1, -2, ...]` as `[0, 1, 2, 3, ...]`. That step can be vectorized:\r\n\r\nhttps://github.com/JuliaHEP/UnROOT.jl/blob/d50081090d95b44138098e25b4102e0d01f270a6/src/RNTuple/fieldcolumn_reading.jl#L87-L88\r\n\r\nor\r\n\r\n```python\r\nfrom_zigzag = lambda n: (n >> 1) ^ -(n & 1)\r\nto_zigzag = lambda n: (n << 1) ^ (n >> 63)\r\n```\r\n\r\nin Python. However, AwkwardForth has a built-in word for it, [`zigzag->`](https://awkward-array.org/doc/main/reference/awkwardforth.html#variable-length-integers) (which does both the zig-zag and the variable-length decoding), so you can just use that.\r\n\r\n```python\r\n>>> vm = ForthMachine64(\"\"\"\r\n... input buffer\r\n... output data int64\r\n... \r\n... begin\r\n...     buffer zigzag-> data\r\n... again\r\n... \"\"\")\r\n\r\n>>> buffer = to_varint(\r\n...     to_zigzag(np.array([0, -1, 1, -2, 2, 100, -100, 1000, -1000], np.int64)).astype(np.uint64)\r\n... )\r\n\r\n>>> vm.run({\"buffer\": buffer}, raise_read_beyond=False)\r\n'read beyond'\r\n>>> vm.outputs[\"data\"]\r\narray([    0,    -1,     1,    -2,     2,   100,  -100,  1000, -1000])\r\n```",
  "created_at":"2023-08-24T16:15:09Z",
  "id":1692001404,
  "issue":928,
  "node_id":"IC_kwDOD6Q_ss5k2eR8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-24T16:15:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> @ioanaif has started to look into this, and found that some of the changes are headers/metadata, while another is that [variable-length integers](https://lucene.apache.org/core/3_5_0/fileformats.html#VInt) and [zig-zag encoding](https://protobuf.dev/programming-guides/encoding/#signed-ints) are now included in RNTuple.\r\n\r\nZig-zag encoding is now part of RNTuple but varints aren't.\r\n",
  "created_at":"2023-09-05T14:08:57Z",
  "id":1706698318,
  "issue":928,
  "node_id":"IC_kwDOD6Q_ss5luiZO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-05T14:08:57Z",
  "user":"MDQ6VXNlcjE1NTczNjA="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @natsukium for test",
  "created_at":"2023-08-14T13:02:49Z",
  "id":1677275776,
  "issue":934,
  "node_id":"IC_kwDOD6Q_ss5j-TKA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-14T13:02:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/937) to add @natsukium! :tada:",
  "created_at":"2023-08-14T13:02:57Z",
  "id":1677276027,
  "issue":934,
  "node_id":"IC_kwDOD6Q_ss5j-TN7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-14T13:02:57Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for your review!",
  "created_at":"2023-08-14T14:48:11Z",
  "id":1677466063,
  "issue":934,
  "node_id":"IC_kwDOD6Q_ss5j_BnP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-14T14:48:11Z",
  "user":"MDQ6VXNlcjI1MDgzNzkw"
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @JostMigenda for doc",
  "created_at":"2023-08-14T16:25:10Z",
  "id":1677658055,
  "issue":935,
  "node_id":"IC_kwDOD6Q_ss5j_wfH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-14T16:25:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/938) to add @JostMigenda! :tada:",
  "created_at":"2023-08-14T16:25:19Z",
  "id":1677658266,
  "issue":935,
  "node_id":"IC_kwDOD6Q_ss5j_wia",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-14T16:25:19Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> > Note: Releases in the v4.3.x series are still ignored by this script, since they are on the separate main-v4 branch, not on main. Do you think it\u2019s worth adding something like\r\n> > ```python\r\n> > outfile.write(\"Note: Releases in the 4.3.x series were developed in parallel with v5.0 on a separate branch and are not included here. See https://github.com/scikit-hep/uproot5/releases for details on those releases.\\n\")\r\n> > ```\r\n> > in line 78, after the \u201cRelease History\u201d header, to avoid confusion?\r\n> \r\n> This is also a very good idea. You can tell that we haven't looked at this release notes page in a while and these things slipped by.\r\n> \r\n> Where would the message about the 4.x legacy releases go? At the top of the page, where it would be noticable?\r\n\r\nProbably best, yes.\r\n\r\n> Here's a URL that shows all of the 4.x legacy releases:\r\n> \r\n> https://github.com/scikit-hep/uproot5/releases?q=%22v4.3.%22&expanded=true\r\n> \r\n> It works because regular (pre-legacy) 4.x used tags without the \"v\" prefix (a standard we learned about later and switched on the major version boundary). I don't know how to make this URL only target the tag field, or if it's possible, but even as a free-text search it finds only legacy releases because it's unusual to see the string `\"v4.3.\"` in any other context.\r\n\r\nUnfortunately, it looks like that search only shows `v4.3.5`\u2013`v4.3.7`, but not `4.3.0`\u2013`4.3.4` (which didn\u2019t have the \u201cv\u201d prefix). The best I can come up with is https://github.com/scikit-hep/uproot5/releases?q=%224.3%22+OR+%22v4.3%22&expanded=true which, bewilderingly, includes _all_ 4.x releases; but since 4.3.x are at the top of the page, I think that\u2019s okay.\r\n\r\nCommit with that link coming up in a bit.",
  "created_at":"2023-08-14T16:53:25Z",
  "id":1677701356,
  "issue":935,
  "node_id":"IC_kwDOD6Q_ss5j_7Ds",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-14T16:53:25Z",
  "user":"MDQ6VXNlcjE2MTg5NzQ3"
 },
 {
  "author_association":"MEMBER",
  "body":"I see. The [main-v4 branch was introduced](https://github.com/scikit-hep/uproot5/compare/main...main-v4) just before 4.3.1 and the \"v\" tag syntax was supposed to be just for 5.x, but after a few months of using it on 5.x pre-releases, I must have forgotten and applied it to the remaining 4.x releases as well.\r\n\r\nI guess \"legacy release\" could be defined as any 4.x release (a) after the first 5.0 pre-release or (b) after the 5.0 release itself. Definition (b) makes more sense to me, and using this definition, there haven't been any legacy releases of Uproot. (I got confused because we have had a few legacy releases of Awkward Array.) But what matters for releases not showing up in the documentation is not whether or not they're out of order, but whether they're based on the `main` branch or the `main-v4` branch.\r\n\r\nHow about this URL:\r\n\r\nhttps://github.com/scikit-hep/uproot5/releases?q=%224.3.%22&expanded=true\r\n\r\nIt includes too many releases, but the ones that are based on `main-v4` are at the top of the list. If someone's looking for a specific release and they don't see it on the main release notes page, this URL would be the most helpful in finding the relevant GitHub release page. Maybe the text could say,\r\n\r\n> If you're looking for a 4.x release that isn't in this list, try [here](https://github.com/scikit-hep/uproot5/releases?q=%224.3.%22&expanded=true).",
  "created_at":"2023-08-14T17:44:06Z",
  "id":1677800115,
  "issue":935,
  "node_id":"IC_kwDOD6Q_ss5kATKz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-14T17:44:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"That URL seems to skip v4.3.6 and v4.3.7 (though I don\u2019t understand why; this search functionality is very confusing), so let\u2019s stick with the URL I suggested above.\r\n\r\nI agree, the \u201clegacy release\u201d language is not quite clear; especially for users who are unfamiliar with the development history. I tried to include a little context [in my commit](https://github.com/scikit-hep/uproot5/pull/935/commits/03e604e5904a6f216e87fe6e66e4c9a5113799bc) without being too verbose. If that wording looks fine to you, I think this PR is ready to merge now.",
  "created_at":"2023-08-14T17:59:34Z",
  "id":1677821519,
  "issue":935,
  "node_id":"IC_kwDOD6Q_ss5kAYZP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-14T17:59:34Z",
  "user":"MDQ6VXNlcjE2MTg5NzQ3"
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, this is the best that it can be, I believe. Thank you!",
  "created_at":"2023-08-14T18:40:35Z",
  "id":1677877035,
  "issue":935,
  "node_id":"IC_kwDOD6Q_ss5kAl8r",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-14T18:40:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"```\r\n                                          4 bytes          4 bytes\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510\r\n\u2502 TKey    \u2502 content                        \u2502 X \u2502 offsets    \u2502 x \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\r\n          \u2502\u2190        fLast - fKeylen       \u2192\u2502                    \u2502\r\n          \u2502                                                     \u2502\r\n          \u2502\u2190                       fObjlen                     \u2192\u2502\r\n```\r\n\r\nThese offsets are measured in bytes, starting from the beginning of the TKey.\r\n\r\nThey do not include the last, so we have to get the last offset from `fLast` (in the TKey header).\r\n\r\nAs for what the two \"4 bytes\" X values are, see the jagged array writer.\r\n\r\n**Example to follow:**\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> uproot.open(skhep_testdata.data_path(\"uproot-simple.root\"))[\"tree\"][\"three\"].array()\r\n<Array ['uno', 'dos', 'tres', 'quatro'] type='4 * string'>\r\n```",
  "created_at":"2023-08-17T13:32:51Z",
  "id":1682295341,
  "issue":940,
  "node_id":"IC_kwDOD6Q_ss5kRcot",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-17T13:46:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This failure\r\n\r\n```\r\nFAILED tests/test_0912-fix-pandas-and-double-nested-vectors-issue-885.py::test_pandas_and_double_nested_vectors_issue_885 - AssertionError: assert False\r\n +  where False = isinstance(<STLVector [[0.0, 1.0, 2.0], [0.0, 1.0, 2.0, 3.0, 4.0]] at 0x7f3c92b49ac0>, <class 'awkward.highlevel.Array'>)\r\n +    where <class 'awkward.highlevel.Array'> = <module 'awkward.highlevel' from '/usr/share/miniconda3/envs/test/lib/python3.8/site-packages/awkward/highlevel.py'>.Array\r\n +      where <module 'awkward.highlevel' from '/usr/share/miniconda3/envs/test/lib/python3.8/site-packages/awkward/highlevel.py'> = ak.highlevel\r\n============ 1 failed, 515 passed, 184 skipped in 94.05s (0:01:34) =============\r\n```\r\n\r\nis because it's expecting AwkwardForth code to run and it isn't. Just insert a test skip, like the others.",
  "created_at":"2023-08-17T15:10:17Z",
  "id":1682457042,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5kSEHS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-17T15:10:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Things that will need to be done:\r\n\r\n- [x] append_code (and everything surrounding it) can be dropped\r\n- [x] remove old form generation\r\n- [x] replace ForthStashes with Nodes, sets name and model at construction\r\n- [x] remove ForthStash class\r\n- [x] `uproot._awkward_forth.forth_stash(context)` should instead be returning `forth_obj` (`ForthGenerator`), and therefore every `if forth_stash is not None` \u2192 `if forth_obj is not None`\r\n- [x] streamers cases\r\n- [x] hand-written Models like TObject (they're all in the models/ directory)\r\n- [x] AsArray?\r\n- [x] FIXMEs, like removing now-dead variables, such as `content` in streamers.py",
  "created_at":"2023-11-12T00:43:16Z",
  "id":1806960891,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5rtAj7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-03T03:33:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Last batch of to-dos:\r\n\r\n- [x] UnwindProtect context manager is detrimental; undo it\r\n- [x] AsRVec and AsSet are almost exactly like AsVector; rederive them from AsVector (by copy-paste and change)\r\n- [x] Look over AsArray code; it has gotten messy\r\n- [x] Merge `main` into this branch\r\n- [x] Little things like `Forth_Generator` name; maybe import symbols into generated code in streamers.py to make it less verbose\r\n- [ ] ~~Fix `test_open_fsspec_xrootd[FSSpecSource-True]`; it's probably a threading issue~~ No such issue (anymore?)\r\n\r\nThen it's done and should go into Uproot 5.2.0!",
  "created_at":"2023-12-03T03:36:01Z",
  "id":1837329308,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5tg2uc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-05T22:13:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@SethBendigo, in case you're interested, stack push-pop is a relatively good replacement for UnwindProtect: 222759e13d0ab684d9bce4290c44159b4c644155. The cost is having to remember to pair up a `pop` with each `push`, but the benefit is that the code between the `push` and `pop` doesn't have to be duplicated, and we don't have to keep track of a `hold_previous_model` variable.",
  "created_at":"2023-12-05T19:37:06Z",
  "id":1841496820,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5twwL0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-05T19:37:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Look over AsArray code; it has gotten messy\r\n\r\nIt's fine.",
  "created_at":"2023-12-05T20:13:29Z",
  "id":1841545454,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5tw8Du",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-05T20:13:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The test failures were failing in `main`, too: https://github.com/scikit-hep/uproot5/actions/runs/7074482533/job/19255559117\r\n\r\nI manually triggered the tests on `main` to check that and forgot to follow up on it.\r\n\r\nThe test itself was literally saying that this should happen:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/cb36ef5ca789e5889ef967e2b6c4aea9c8a6d1e6/tests/test_0692_fsspec_writing.py#L133-L142\r\n\r\nSo I'll fix it now, in this PR, and it will carry through to `main` when I merge it.",
  "created_at":"2023-12-05T20:29:21Z",
  "id":1841565055,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5txA1_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-05T20:29:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@lobis, is 05ab1f3a2aa569645a508b73610961f722a1bc6f the right way to turn off the \"waiting for fsspec/filesystem_spec#1426\" error? If not, I'll fix it in this PR. It will enter `main` when this PR merges (maybe today).",
  "created_at":"2023-12-05T20:34:51Z",
  "id":1841572209,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5txClx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-05T20:34:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @lobis, is [05ab1f3](https://github.com/scikit-hep/uproot5/commit/05ab1f3a2aa569645a508b73610961f722a1bc6f) the right way to turn off the \"waiting for [fsspec/filesystem_spec#1426](https://github.com/fsspec/filesystem_spec/pull/1426)\" error? If not, I'll fix it in this PR. It will enter `main` when this PR merges (maybe today).\r\n\r\nYes, this is what was expected.",
  "created_at":"2023-12-05T20:39:18Z",
  "id":1841577591,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5txD53",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-12-05T20:39:18Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Well, @SethBendigo, it's done! It's finally and fully done. Thank you very much for all of your work on this!!!\r\n\r\nWhen the tests pass one last time, I'll merge it.",
  "created_at":"2023-12-05T22:23:24Z",
  "id":1841720177,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5txmtx",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-12-05T22:23:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I didn't see that this got in!? How exciting @SethBendigo excellent work, this was a long one.",
  "created_at":"2023-12-12T18:04:25Z",
  "id":1852552374,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5ua7S2",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-12-12T18:04:25Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks @agoose77! Thanks @jpivarski, I really really appreciated your help throughout. I had a lot of fun and learned a lot!",
  "created_at":"2023-12-12T22:36:47Z",
  "id":1852922866,
  "issue":943,
  "node_id":"IC_kwDOD6Q_ss5ucVvy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-12T22:36:47Z",
  "user":"MDQ6VXNlcjQ5MTY5MDM1"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! It's being fixed in #947.",
  "created_at":"2023-08-23T12:20:49Z",
  "id":1689863608,
  "issue":946,
  "node_id":"IC_kwDOD6Q_ss5kuUW4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-08-23T12:20:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! The tests are failing in all branches, not just this one, but #957 will fix it when that's done. I'll merge it into this PR.",
  "created_at":"2023-09-14T16:32:01Z",
  "id":1719779132,
  "issue":955,
  "node_id":"IC_kwDOD6Q_ss5mgb88",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-14T16:32:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Nice! \r\nFor me the file opening part is still not working in the demo. However, I am not sure if it is a problem caused by my browser (firefox - happened in the past for CoDaS) ",
  "created_at":"2023-09-14T12:00:17Z",
  "id":1719314306,
  "issue":956,
  "node_id":"IC_kwDOD6Q_ss5meqeC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-14T12:00:17Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ioanaif I use Firefox too! Does it give a useful traceback e.g. `FileNotFoundError`? If so, it might help to force-refresh the page.",
  "created_at":"2023-09-14T12:07:30Z",
  "id":1719324731,
  "issue":956,
  "node_id":"IC_kwDOD6Q_ss5metA7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-14T12:07:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes. Last time this happened for my browser because the file was too large",
  "created_at":"2023-09-14T12:13:12Z",
  "id":1719333145,
  "issue":956,
  "node_id":"IC_kwDOD6Q_ss5mevEZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-14T12:13:12Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"> > Actually, I don't think no_threads is documented at all, including the fact that its default depends on sys.platform.\r\n> \r\n> Hmm, you're right. At one point I was making this an implicit thing, but there's sense in exposing it.\r\n\r\nIf `no_threads` is in the list of `**options`, then it's already exposed. A user could override that manually.",
  "created_at":"2023-09-14T18:04:06Z",
  "id":1719908337,
  "issue":956,
  "node_id":"IC_kwDOD6Q_ss5mg7fx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-14T18:04:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Let me hold this until I finish some last changes :)",
  "created_at":"2023-09-14T18:13:14Z",
  "id":1719922159,
  "issue":956,
  "node_id":"IC_kwDOD6Q_ss5mg-3v",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-14T18:13:14Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ioanaif and @jpivarski, what do you think about testing this PR? We could just parameterise the `use_threads` argument, but that will more than double our test time. ",
  "created_at":"2023-09-15T12:10:23Z",
  "id":1721171973,
  "issue":956,
  "node_id":"IC_kwDOD6Q_ss5mlwAF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-15T12:10:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"We could double our test time on a subset of tests. Pick one that seems to be focusing on backend features, one that tests HTTP and XRootD with various options, non-existent files, etc., and double those with `use_threads=True/False`.",
  "created_at":"2023-09-15T15:02:15Z",
  "id":1721427318,
  "issue":956,
  "node_id":"IC_kwDOD6Q_ss5mmuV2",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-09-15T15:02:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I've updated the tests. The documentation is probably not perfect; we talk about the executor being a threaded executor, but sometimes it's a trivial executor. I'm happy to not worry about that for now, as I think we should drop our executor abstraction in the near future in favour of the existing solution in Python 3's `concurrent.futures`.",
  "created_at":"2023-09-18T15:31:20Z",
  "id":1723719172,
  "issue":956,
  "node_id":"IC_kwDOD6Q_ss5mvd4E",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-18T15:31:28Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif, I'm going to fast-track this (self-approve and merge it) because it's needed to get our tests to run.",
  "created_at":"2023-09-14T16:28:33Z",
  "id":1719774194,
  "issue":957,
  "node_id":"IC_kwDOD6Q_ss5mgavy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-14T16:28:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think this is ~about right but I'm probably missing something. This PR will (and should) fail until https://github.com/dask-contrib/dask-awkward/pull/368 is merged.",
  "created_at":"2023-09-18T21:53:18Z",
  "id":1724508880,
  "issue":960,
  "node_id":"IC_kwDOD6Q_ss5myerQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-09-18T21:53:53Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Closed in favour of #966",
  "created_at":"2023-10-04T10:51:24Z",
  "id":1746626678,
  "issue":960,
  "node_id":"IC_kwDOD6Q_ss5oG2h2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-04T10:51:24Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"In TH2, `_values_variances` does perform the axis swap, thus all is good in the computation of the variances.\r\n\r\nI removed both the previous computation and my fix to the computation from `to_boost` and replaced it with simply calling `.variances(flow=True)` since variances are already computed at that stage. \r\n\r\nThis should fix any problem with the variances' computation and remove duplication for easier debug in the future. \r\n\r\nIt's ready to go imo. \u263a\ufe0f",
  "created_at":"2023-10-03T16:09:45Z",
  "id":1745295076,
  "issue":965,
  "node_id":"IC_kwDOD6Q_ss5oBxbk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-03T16:09:45Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hey team (@jpivarski and @ioanaif), would you mind casting your eyes over this PR and see whether anything stands out. Particular with respect to the abstractions being built here. \r\n\r\nI chose to avoid branching, and instead have form remapping be the _base_ case, and treat non-remapping as a trivial remapper.",
  "created_at":"2023-10-03T16:39:23Z",
  "id":1745347527,
  "issue":966,
  "node_id":"IC_kwDOD6Q_ss5oB-PH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-03T16:39:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I take it there isn't an order in which we can apply these changes in successive releases of awkward, dask-awkward, uproot, and coffea to not break things while releasing one new version of each at a time? \r\n\r\nWe can\r\n1. release `coffea` and bump the minimum version of `dask-awkward` \r\n1. release `dask-awkward` and bump the minimum version of `awkward`\r\n1. release `awkward` and `uproot`.\r\n\r\nThat should minimise the pain, I hope, as `pip` shouldn't give users newer coffea or dask-awkward until these dependencies exist. That is, assuming that our various release mechanisms don't e.g. require these packages to be on PyPI already.\r\n\r\n",
  "created_at":"2023-10-03T20:26:20Z",
  "id":1745673283,
  "issue":966,
  "node_id":"IC_kwDOD6Q_ss5oDNxD",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2023-10-03T20:26:20Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski and @ioanaif  I've bumped the awkward version to 2.4.5+, which is strict but necessary to ensure that uproot.dask works. My view is awkward 2 users should not have any upper caps at this stage, do you feel OK with this change? If so, let's merge!",
  "created_at":"2023-10-06T22:11:59Z",
  "id":1751454496,
  "issue":966,
  "node_id":"IC_kwDOD6Q_ss5oZRMg",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-06T22:21:35Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski could we make another uproot pre-release at some point?",
  "created_at":"2023-10-08T11:08:23Z",
  "id":1751999738,
  "issue":966,
  "node_id":"IC_kwDOD6Q_ss5obWT6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-08T11:08:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"[5.1.0rc2](https://github.com/scikit-hep/uproot5/compare/v5.1.0rc1...v5.1.0rc2) is another pre-release: https://pypi.org/project/uproot/#history",
  "created_at":"2023-10-09T14:04:31Z",
  "id":1753079652,
  "issue":966,
  "node_id":"IC_kwDOD6Q_ss5ofd9k",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-10-09T14:04:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Great to see this starting! (We'll talk on Zoom in an hour.) I'm watching it, but I removed myself as a reviewer for now because you can use the \"request review\" button as a way of reminding me about it when it's done. I don't think I get emails about a PR going out of draft mode, but I do get emails about requests-to-review, so we use that as a last step in our process.",
  "created_at":"2023-10-03T18:08:46Z",
  "id":1745477388,
  "issue":967,
  "node_id":"IC_kwDOD6Q_ss5oCd8M",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-03T18:08:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski @nsmith- I think this is ready to go.\r\n\r\nCurrently the `fsspec` source works in place of the http, file, s3 and xrootd sources. At this point fsspec is only present as a test dependency and this can be merged without causing any problems as it's fully optional.\r\n\r\nOne of my next steps (for a dedicated PR) would be to refactor the `reading.open` options for handlers (`http_handler`, `file_handler`, etc.) in order to use a single handler option (`handler`) that can be set to the currenty existing sources or the new fsspec source with the goal of setting the fsspec source as the default once the integration is complete.\r\n\r\nI think this would introduce some limitations such as the ability to open some files with one protocol and some with another in the same function call, but as we discussed this is an acceptable compromise. The ability to use the previous sources (`http`, `s3`, etc.) will remain and the corresponding source will be assigned from parsing the uri string.\r\n\r\nThe CI starting to fail when I added `s3fs` as a dependency (required by `fsspec` to read `s3` files. I have no idea why, it looks like it has something to do with `ssl` and only fails the CI on some python versions, not all. Any idea what might be going on? (https://github.com/scikit-hep/uproot5/actions/runs/6408524082 for the logs).\r\n\r\nI saw that `minio` is used to add support for s3 in the current implementation of the s3 source, currently being an optional dependency. When this project is done `fsspec` will become a core dependency and users trying to open s3 files will recieve an error (from fsspec) that will direct them to install `s3fs`. `s3fs` should still be present on the `test` dependencies to validate against s3 files though.",
  "created_at":"2023-10-04T15:57:31Z",
  "id":1747201743,
  "issue":967,
  "node_id":"IC_kwDOD6Q_ss5oJC7P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-04T16:35:00Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"This is something that would disrupt current use of Uproot (or provide two ways of doing something? Would `handler` take precedence over `*_handler`?). Therefore, we might need to schedule when it gets merged to avoid the upcoming 5.1.0 release.",
  "created_at":"2023-10-04T18:45:09Z",
  "id":1747448684,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5oJ_Ns",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-04T18:45:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This is something that would disrupt current use of Uproot (or provide two ways of doing something? Would `handler` take precedence over `*_handler`?). Therefore, we might need to schedule when it gets merged to avoid the upcoming 5.1.0 release.\r\n\r\nYes this would be somewhat disruptive so it needs to be merged carefully (update docs, etc.). I was planning on removing `*_handler` option entirely and replace it with `handler` (which in practise would never be overriden by the user as the `fsspec` handler should handle everything). Maybe this PR can keep the old `*_handler` options available but the new `handler` takes precendence in a way that it mantains backwards compatibility, I'll think about it.\r\n\r\n\r\n",
  "created_at":"2023-10-04T18:57:07Z",
  "id":1747465274,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5oKDQ6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-04T18:59:04Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"What you're describing is an API change, and those can only happen with a deprecation. For the entire 5.1.* series, we'd have to identify when users are explicitly passing a `*_handler` and raise a warning, telling them that it will be gone in 5.2.0 and how they should update their code. After a sufficiently long time (e.g. 2 months), we'd release 5.2.0 with the arguments removed.\r\n\r\nTo do this, `*_handler` would have to take a default like\r\n\r\n```python\r\nUNSET = object()\r\n\r\ndef open(..., file_handler=UNSET, http_handler=UNSET, ...):\r\n    if file_handler is not UNSET:\r\n        warnings.warn(\r\n            f\"\"\"In version 5.2.0, the 'file_handler' argument will be removed from 'uproot.open'. Use\r\n    uproot.open(..., handler={repr(file_handler)})\r\ninstead.\r\n\r\nTo raise these warnings as errors (and get stack traces to find out where they're called), run\r\n    import warnings\r\n    warnings.filterwarnings(\"error\", module=\"uproot.*\")\r\nafter the first `import uproot` or use `@pytest.mark.filterwarnings(\"error:::uproot.*\")` in pytest.\"\"\",\r\n            DeprecationWarning,\r\n        )\r\n        if handler is UNSET and the protocol is \"file://\":  # but there can be multiple files\r\n            handler = file_handler\r\n```\r\n\r\nor similar.\r\n\r\nWe do interface-breaking changes more frequently in Awkward Array, and therefore have a helper function for it:\r\n\r\nhttps://github.com/scikit-hep/awkward/blob/626ff08fa73e6f84de85147ffaf8acf1df7a2776/src/awkward/_errors.py#L400-L420",
  "created_at":"2023-10-04T19:11:51Z",
  "id":1747485893,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5oKITF",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2023-10-04T19:11:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Not sure if this has been spotted yet: I saw the CI fail when accessing the `s3` bucket (https://github.com/scikit-hep/uproot5/actions/runs/6426306777/job/17450310571?pr=971#step:7:5667) returning a 503 code (Service Unavailable). Shouldn't S3 be able to handle virtually any load? (was fixed after rerun).",
  "created_at":"2023-10-06T01:33:19Z",
  "id":1749868793,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5oTOD5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-06T01:40:20Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This PR is ready for review. The deprecation warnings make the `file_path_to_source_class`method complicated to read but it basically does:\r\n\r\n- If specified use the handler in the corresponding `*_handler`option as before. These options are now `None` by default.\r\n- Use the new `handler` otherwise.\r\n\r\nIn https://github.com/scikit-hep/uproot5/pull/984 I remove these for good and declare the fsspec source as default which should handle all cases (discussions related to this can be done in the PR).\r\n\r\nIn principle it should be ready to merge but I'm not sure if I should wait for the ci to pass (failing due to unrelated issues).",
  "created_at":"2023-10-11T17:04:49Z",
  "id":1758123107,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5oytRj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-11T17:04:49Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"That's a good update. This will be ready to squash-and-merge once the tests pass. I'm going to look at one of those to see what's happening.",
  "created_at":"2023-10-12T16:32:30Z",
  "id":1759965854,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5o5vKe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T16:32:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> That's a good update. This will be ready to squash-and-merge once the tests pass. I'm going to look at one of those to see what's happening.\r\n\r\nI actually forgot to add that `*_handler` options take preference over `handler`. I'll update and merge. Tests are passing now (server issues I guess).",
  "created_at":"2023-10-12T16:34:11Z",
  "id":1759969406,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5o5wB-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T16:34:11Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"The dalitzdata.root is being opened; the `TimeoutErrors` are happening much later, during the read. We can even see some successfully read TBaskets before the socket breaks.\r\n\r\nIt's only used in two tests,\r\n\r\n* `tests/test_0088-read-with-http.py::test_issue176`\r\n* `tests/test_0088-read-with-http.py::test_issue176_again`\r\n\r\n...",
  "created_at":"2023-10-12T16:37:20Z",
  "id":1759974195,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5o5xMz",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-12T16:37:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"On my computer (not CI), I can run these tests in `main`, `main-v510`, and `open-handlers-unify`. It seems like the HTTP server just decided that it doesn't like GitHub Actions anymore.\r\n\r\nWhat happens if you skip these tests (`reason=\"starterkit.web.cern.ch is breaking HTTP connections in GitHub Actions\"`)? Does the rest of the suite run?",
  "created_at":"2023-10-12T16:40:00Z",
  "id":1759978199,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5o5yLX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T16:40:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That last commit did not address the starterkit.web.cern.ch server connection failure, but it started working. Maybe the transient issue with the server is over? (If it is, we'll just ignore it for now. Angus's idea of hosting our own in-CI mini-servers could solve it more generally.)",
  "created_at":"2023-10-12T16:55:27Z",
  "id":1760001745,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5o537R",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T16:55:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith-, I'm in favor of merging this. If you want to give it a look-over before that happens, let me know. Note that this is going into `main-v510`.",
  "created_at":"2023-10-12T16:56:08Z",
  "id":1760002910,
  "issue":971,
  "node_id":"IC_kwDOD6Q_ss5o54Ne",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T16:56:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Is the `github://` test using `api.github.com` with its rate limit? (This is continued from a conversation on Slack.)\r\n> \r\n> If it is, let's just drop the `github://` test. We can assume that fsspec does its job correctly (managing exotic backends like the GitHub one) and just verify that our connection to fsspec is doing what it's supposed to. We should continue testing local files, HTTP, XRootD, and maybe S3 because Uproot users are known to use them, but I'd say that the GitHub one is just \"nice to have.\"\r\n> \r\n> Other than that, I think this PR is ready to merge.\r\n\r\nI think I will keep the pytest skip instead of just removing the test, I think it's nice to have an example of some of the exotic fsspec sources. I think there is some value in testing it explicitly though, for example the github schema url has two `:` in it which made the old \"split path and object\" helper method fail. I guess we could explicitly test the helper method instead through.",
  "created_at":"2023-10-05T20:45:23Z",
  "id":1749616229,
  "issue":973,
  "node_id":"IC_kwDOD6Q_ss5oSQZl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T20:45:23Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay. If we're ever working on something unrelated and see failures in this test, we'll unceremoniously drop it. You'll notice that some of the `@pytest.mark.skip` reasons are \"such-and-such a server is flaky.\"",
  "created_at":"2023-10-05T21:27:42Z",
  "id":1749670312,
  "issue":973,
  "node_id":"IC_kwDOD6Q_ss5oSdmo",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-05T21:27:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm sorry\u2014I intended all of the chatter on #975 about colons to go here. I didn't notice that I was posting to the wrong issue.\r\n\r\nAny further discussion about colons should be on this issue thread.",
  "created_at":"2023-10-05T21:31:10Z",
  "id":1749674653,
  "issue":974,
  "node_id":"IC_kwDOD6Q_ss5oSeqd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T21:31:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"tagging @Moelf ",
  "created_at":"2023-10-05T15:23:00Z",
  "id":1749125344,
  "issue":975,
  "node_id":"IC_kwDOD6Q_ss5oQYjg",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2023-10-05T15:23:00Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"@ioanaif, @agoose77, and I were talking about it this morning. First thing: Uproot needs to be able to address names with colons in them\u2014I don't know how I thought that `:` between a TTree and its TBranches was a good idea. (You're also allowed to use `/` in that context, even though the nesting of TBranches within TBranches within a TTree is not the same as the nesting of TDirectories within TDirectories, at least that couldn't conflict with valid names.)\r\n\r\nColons in names is valid in ROOT:\r\n\r\n```python\r\n% python\r\nPython 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"/tmp/whatever.root\", \"RECREATE\")\r\n>>> h = ROOT.TH1D(\"has:colon\", \"\", 5, 0, 1)\r\n>>> h.Write()\r\n249\r\n>>> f.Close()\r\n>>> \r\n% python\r\nPython 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"/tmp/whatever.root\")\r\n>>> h = f.Get(\"has:colon\")\r\n>>> h\r\n<cppyy.gbl.TH1D object at 0x55aafcee03c0>\r\n```\r\n\r\nSo Uproot needs to be able to read and write them.\r\n\r\nWe can't just change [uproot.ReadOnlyDirectory](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyDirectory.html)'s and [uproot.WritableDirectory](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableDirectory.html)'s `__getitem__` and `__setitem__` methods, and we can't introduce a function argument because they're square brackets, not function calls with arguments. Assignment can't have a function call on the left-hand-side because of Python syntax.\r\n\r\n[uproot.ReadOnlyDirectory](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyDirectory.html) has a more low-level `get` method (which is oddly not in the documentation). That function does nested directories with `/` and `:` parsing, though it could take a new argument to prevent this.\r\n\r\nAs an ugly solution that experts would need to be \"in the know\" to use, we could add an argument preventing `/` and `:` parsing to `__getitem__` and `__setitem__` themselves, so users would have to literally call `__getitem__` and `__setitem__` by name instead of using square brackets and assignment.\r\n\r\nAs a less-ugly solution, we could add a proxy like Awkward's [ak.mask](https://awkward-array.org/doc/main/reference/generated/ak.mask.html) (see \"Another syntax\" at the end of the page). It would be implemented a bit like this:\r\n\r\n```python\r\nclass WritableDirectory:\r\n  @property\r\n  def noparse(self):\r\n    return ReadOnlyDirectory._NoParse(self)\r\n  class _NoParse:\r\n    def __init__(self, directory):\r\n      self._directory = directory\r\n    def __getitem__(self, where):\r\n      pass  # get an item without parsing directories\r\n    def __setitem__(self, where, what):\r\n      pass  # set an item without parsing directories\r\n```\r\n\r\nbut with a better name than `noparse`. That would allow\r\n\r\n```python\r\n>>> directory.noparse[\"has:column\"]\r\n>>> directory.noparse[\"has:column\"] = \"blub\"\r\n```\r\n\r\nThoughts?",
  "created_at":"2023-10-05T16:54:56Z",
  "id":1749309078,
  "issue":975,
  "node_id":"IC_kwDOD6Q_ss5oRFaW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T16:54:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski how do you feel about the `__getitem__` treating keys as higher precedence over \"paths\"? e.g.\r\n```python\r\n\r\ndef parse_first(keys, where, allow_colon=True):\r\n    longest_keys = sorted(keys, key=len, reverse=True)\r\n\r\n    for key in longest_keys:\r\n        # Prefer to match a key vs a pattern\r\n        if where.startswith(key):\r\n            remainder = where[len(key) :]\r\n            if remainder.startswith(\"/\") or (remainder.startswith(\":\") and allow_colon):\r\n                next_where = remainder[1:]\r\n            else:\r\n                next_where = None\r\n            return key, next_where\r\n    else:\r\n        pattern = \":/\" if allow_colon else \"/\"\r\n        # parse the key\r\n        head_tail = re.split(pattern, where, maxsplit=1)\r\n\r\n        # Does the string not contain this pattern?\r\n        if len(head_tail) == 1:\r\n            raise ValueError(f\"Encountered non-key {where}\")\r\n        else:\r\n            return tuple(head_tail)\r\n\r\n\r\nassert parse_first({\"bla:bla\"}, \"bla:bla\") == ('bla:bla', None)\r\nassert parse_first({\"bla\"}, \"bla/bla\") == ('bla', \"bla\")\r\nassert parse_first({\"bla\"}, \"bla:bla\") == ('bla', \"bla\")\r\nassert parse_first({\"bla:bla\", \"bla\"}, \"bla:bla\") == ('bla:bla', None)\r\n```\r\n\r\nWe'll still want an unambiguous lookup, but we should also fix the default case where we end up with a recursion.",
  "created_at":"2023-10-05T17:13:35Z",
  "id":1749334769,
  "issue":975,
  "node_id":"IC_kwDOD6Q_ss5oRLrx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T17:25:16Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"ok yeah PHYSLITE is wild but I think it's kinda just working out of the box! https://gist.github.com/Moelf/63308270b7a8143465b39f2d8fa3f98b\r\n\r\n(kinda just works in Julia is indirect evidence that I had some correct-ish understanding of RNTuple's new spec)\r\n\r\nbut also there are top level columns that look suspecious at first glance:\r\n\r\n```julia\r\n\u251c\u2500 :AnalysisJets \u21d2 Vector\r\n\u2502                  \u251c\u2500 :offset \u21d2 Leaf{UnROOT.Index64}(col=181)\r\n\u2502                  \u2514\u2500 :content \u21d2 Struct\r\n\u2502                                \u2514\u2500 Symbol(\":_0\") \u21d2 Struct\r\n\u2502                                                   \u2514\u2500 Symbol(\":_0\") \u21d2 Struct\r\n\u2502                                                                      \u2514\u2500 Symbol(\":_0\") \u21d2 Struct\r\n\r\njulia> df[:, :AnalysisJets]\r\nERROR: ArgumentError: only eltypes with fields are supported\r\nStacktrace:\r\n  [1] (StructArrays.StructVector{@NamedTuple{}, @NamedTuple{}})(c::@NamedTuple{})\r\n    @ StructArrays ~/.julia/packages/StructArrays/0h2SD/src/structarray.jl:17\r\n  [2] (StructArrays.StructArray{@NamedTuple{}})(c::@NamedTuple{})\r\n    @ StructArrays ~/.julia/packages/StructArrays/0h2SD/src/structarray.jl:94\r\n```",
  "created_at":"2023-10-05T19:58:24Z",
  "id":1749560697,
  "issue":975,
  "node_id":"IC_kwDOD6Q_ss5oSC15",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":2,
   "total_count":2
  },
  "updated_at":"2023-10-05T19:59:14Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski how do you feel about the `__getitem__` treating keys as higher precedence over \"paths\"?\r\n\r\nAt first, I thought that's a great idea, but what if a file contains\r\n\r\n```\r\n\u2514\u2500 has\r\n    \u2514\u2500 colon\r\n```\r\n\r\n(assuming `has` is a TTree and `colon` is a TBranch). At this point in time, someone can access `colon` by `file[\"has:colon\"]`.\r\n\r\nLater, another object is added (any type).\r\n\r\n```\r\n\u251c\u2500 has\r\n\u2502   \u2514\u2500 colon\r\n\u2514\u2500 has:colon\r\n```\r\n\r\nNow the same `file[\"has:colon\"]` would return a different object. How a path gets resolved depends on what other objects exist, a kind of action at a distance.\r\n\r\nIt's not terrible because the original object is still addressable, as `file[\"has\"][\"colon\"]`. It's also a corner case that we can recognize and explain if it ever comes up.\r\n\r\nThis only applies to colons, not slashes. ROOT doesn't allow slashes in names:\r\n\r\n```python\r\n% python\r\nPython 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"/tmp/whatever.root\", \"RECREATE\")\r\n>>> h = ROOT.TH1D(\"has/slash\", \"\", 5, 0, 1)\r\n>>> h.Write()\r\n249\r\n>>> f.Close()\r\n>>> \r\n% python\r\nPython 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"/tmp/whatever.root\")\r\n>>> h = f.Get(\"has/slash\")\r\n>>> h\r\n<cppyy.gbl.TH1D object at 0x(nil)>\r\n```\r\n\r\n(Note the `0x(nil)`.)\r\n\r\nAh, but there's another problem with the precedence solution: when using it to assign a new object, like\r\n\r\n```python\r\n>>> file[\"new:object\"] = whatever\r\n```\r\n\r\nshould it make an object with that whole name or make a directory named `new` containing an object named `object`?\r\n\r\nAh\u00b2, it can't: colons are only for TTree-TBranch combinations. `file[\"new:object\"]` can't be a TTree named `new` and a TBranch named `object`; that's not how the interface works. So the above code should always make a new object named `new:object`. And the same thing with `new/object` does not apply: you can never make a ROOT object with a `/` in its name.\r\n\r\nSo I think this is a good solution, particularly since it doesn't add new features that users are supposed to use instead of the obvious ones. The only subtleties (action at a distance) are rare, can be explained, and can be worked around. I'm in favor of the new precedence rule.",
  "created_at":"2023-10-05T21:24:12Z",
  "id":1749665956,
  "issue":975,
  "node_id":"IC_kwDOD6Q_ss5oScik",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T21:24:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm sorry\u2014I intended all of the discussion about colons to be on #974. I didn't notice that there were two issues. Anything else about colons should go to $974 instead of here.\r\n\r\nThis issue is about PHYSLITE. The problems reading it ~~may be~~ are likely related to #928. Uproot is not up-to-date on RNTuple reading. (The format changed.)",
  "created_at":"2023-10-05T21:33:35Z",
  "id":1749681191,
  "issue":975,
  "node_id":"IC_kwDOD6Q_ss5oSgQn",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2023-10-05T21:33:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Perhaps it would make sense to add `fsspec` as a dependency for this PR after the next major release? This would avoid having potentially different behaviours for users having `fsspec` installed or not, and I guess it would also make the code easier to write and read we wouldn't have to handle both cases.",
  "created_at":"2023-10-05T15:47:18Z",
  "id":1749170480,
  "issue":976,
  "node_id":"IC_kwDOD6Q_ss5oQjkw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T15:47:18Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> When this is done, fsspec will become a strict dependency.\r\n> \r\n> > This would avoid having potentially different behaviours for users having fsspec installed or not\r\n> \r\n> If that's possible, then you would be introducing a behavior change now for those users who happen to have fsspec installed for other reasons. We don't want the path-handling to change at all. Could you get more certainty about that?\r\n> \r\n> Meanwhile, this is not a change that has to happen now (or ever, technically). The original splitting was using a Python standard library function. Even when we start relying on fsspec for all file backends, there's no reason we couldn't still use the standard `urlparse` for URL parsing. Does `fsspec.utils.urlsplit` do something special?\r\n> \r\n> I'm on the fence about this one.\r\n\r\nYes I think you are right we should not use the fsspec url parsing when we can just use urllib. At the end I ended up refactoring the helper method to use `urllib` and added an explicit test for it. The old method got confused with things such as double `:` in the url, etc.\r\n\r\nI feel that this method is more concise but I am worried it does not fully reproduce the old one (everything looks good so far, all the tests pass).",
  "created_at":"2023-10-05T21:24:40Z",
  "id":1749666506,
  "issue":976,
  "node_id":"IC_kwDOD6Q_ss5oScrK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T21:24:40Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Might be worth reviewing all the supported syntaxes of `files` argument in https://uproot.readthedocs.io/en/latest/uproot._dask.dask.html as it is a bit more than `uproot.open` and thinking about what would make the most sense to normalize paths to something uniform.",
  "created_at":"2023-10-05T21:41:50Z",
  "id":1749691182,
  "issue":976,
  "node_id":"IC_kwDOD6Q_ss5oSisu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T21:41:50Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"The colon-splitting between filename and object (not related to #974\u2014that's a colon in an object path) has been a lot of trouble. There's a list of problems it's caused on https://github.com/scikit-hep/uproot5/issues/920#issuecomment-1633050022, including platform-dependent issues with `C:\\` on Windows.\r\n\r\nOn page 19 of [this talk](https://indico.jlab.org/event/459/contributions/11547/) I analyzed a large dataset of user code to see how many people are using it, to see if we could ever get rid of it. The result was 10% of `uproot.open` calls explicitly use the colon and 64% are unknown because they pass in a variable as the filename.\r\n\r\nSo the path interpretation is unfortunately complex and we should leave it untouched if possible. It seems to me that it should be possible, since replacing the backend doesn't change the path-or-URL that we send to the backend.",
  "created_at":"2023-10-05T21:42:32Z",
  "id":1749692475,
  "issue":976,
  "node_id":"IC_kwDOD6Q_ss5oSjA7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T21:42:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Personally I'd be in favor of dropping `url:filepath` in favor of `{\"url\": \"filepath\"}` but I believe there was some discussion in the past on this.",
  "created_at":"2023-10-05T21:42:57Z",
  "id":1749693089,
  "issue":976,
  "node_id":"IC_kwDOD6Q_ss5oSjKh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T21:42:57Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Might be worth reviewing all the supported syntaxes of `files` argument in https://uproot.readthedocs.io/en/latest/uproot._dask.dask.html as it is a bit more than `uproot.open` and thinking about what would make the most sense to normalize paths to something uniform.\r\n\r\nI think we have consistency across the file-opening functions under control. File syntax is never interpreted differently by different file-opening functions, but some functions have more options than others.\r\n\r\n  * `uproot.open` can only take one file, so it has\r\n    * `str`/`bytes` (filepath-colon-objectpath)\r\n    * `pathlib.Path` (filepath only)\r\n    * object with `read` and `seek` methods (file-like object)\r\n    * length-1 dict of filepath, objectpath (control where the split happens)\r\n  * `uproot.iterate` can take multiple files, so it has\r\n    * `str`/`bytes` (filepath-colon-objectpath)\r\n    * `pathlib.Path` (filepath only)\r\n    * glob syntax in `str`/`bytes` or `pathlib.Path`, including bash extensions (multiple filepaths, but only one objectpath)\r\n    * any-length dict of filepath, objectpath (control where the split happens and get multiple filepaths, multiple objectpaths)\r\n    * already-open TTree objects (to chain them)\r\n    * iterables of the above (to chain them)\r\n  * `uproot.concatenate` can also take multiple files, so it has\r\n    * all the same options as `uproot.iterate`\r\n  * `uproot.dask` can take multiple files and needs to partition them somehow, so it has\r\n    * all the same options as `uproot.iterate`\r\n    * any-length dict of filepath, dict of `{\"object_path\": OBJECTPATH, \"steps\": STEPS}` where `OBJECTPATH` is the objectpath and `STEPS` is either a list of offsets or a list of start-stop pairs (entry numbers for the partitions).\r\n\r\n(`uproot.dask` has three ways to partition, but each one is mutually exclusive of the other two. If you try to use more than one, it will raise an error.)\r\n\r\nThe above is complex, but I believe that it is under control. Each one of these methods was motivated by a request (a long history...).",
  "created_at":"2023-10-05T21:58:14Z",
  "id":1749710120,
  "issue":976,
  "node_id":"IC_kwDOD6Q_ss5oSnUo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-05T21:58:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"To clarify (after an in-person discussion with @jpivarski I think there was some confusion):\r\n\r\n- This was motivated by the `fsspec` integration (https://github.com/scikit-hep/uproot5/issues/972).\r\n- I tried using the newly added `fsspec` source with some exotic urls such as `github://scikit-hep:scikit-hep-testdata@v0.4.33/src/skhep_testdata/data/uproot-issue121.root` but it didn't work due to the helper function not working correctly, this is the reason for this PR: make a more robust url / object split helper function.\r\n- I thought it made sense to refactor this function to use `urllib` as it's being used elsewhere in the code.\r\n- The new implementation is more concise and it looks like it handles all previous cases (atleast all the tests pass, if this is not the case we should add them).\r\n- I added a new test just to explicitly test this (as the github api url test had to be skipped due to api rate limits).\r\n- This is not just a refactoring, it should make this method correclty handle more cases and hopefully 100% cover previous ones.",
  "created_at":"2023-10-06T19:18:31Z",
  "id":1751297770,
  "issue":976,
  "node_id":"IC_kwDOD6Q_ss5oYq7q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-06T19:18:31Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The PR is again ready to be reviewed.\r\n\r\nNow it should work as intended, the `chunks` method should be non-blocking.\r\n\r\nI have some questions regarding the management of the executor. Currently I am creating a local executor in the `chunks` method but perhaps I should do proper lifecycle management in a similar fashion to what is being done in the http source.\r\n\r\nLooks like doing the lifecycle management would mean refactoring the current code though, since my `ResourceThreadPoolExecutor` is constructed from the requested byte ranges...",
  "created_at":"2023-10-10T16:17:58Z",
  "id":1755781051,
  "issue":979,
  "node_id":"IC_kwDOD6Q_ss5opxe7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-10T16:22:30Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> If so, then FSSpecSource doesn't need to have an executor\r\n\r\nI don't understand this, don't you always need some kind of executor to run futures?",
  "created_at":"2023-10-10T19:43:47Z",
  "id":1756114817,
  "issue":979,
  "node_id":"IC_kwDOD6Q_ss5orC-B",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-10T19:43:47Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"The executor might be inside of the fsspec library. I don't know that\u2014I'm just thinking it might be, and if it is, we should take advantage of it.\r\n\r\nAlso, it might use `async` instead of an executor, but the effect would be the same: we might be able to get it to do the concurrent waiting-for-I/O for us.",
  "created_at":"2023-10-10T19:52:15Z",
  "id":1756126197,
  "issue":979,
  "node_id":"IC_kwDOD6Q_ss5orFv1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-10T19:52:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This comes down to a fundamental question: what is fsspec's scope? How much does it do? There's this `cat_file` interface, which does the whole process from URL to file bytes, but it's a blocking interface. Is there an async interface? If so, does it have a way to attach a function to call when the async function is done?\r\n\r\nFrom my understanding `fsspec` supports an async interface only for the http file system. This is enabled by passing the option `asynchronous=True` on creation. If this option is set to true, some methods of the filesystem will become asynchronous (such as `cat_file`). Running in asynchronous mode would need an event loop.\r\n\r\nI imagine it would be possible to work with the sync filesystem but use an async one inside the `chunks` method so all the `asyncio` logic will be contained, otherwise it can get complicated. I will look into this.\r\n\r\nRegarding this PR: I think it's done (unless there is some mistake) as currently the chunks interface is non-blocking. Depending on the complexity of leveraging the fsspec asyncio interface I will include it into this PR, a separate PR, or do nothing.\r\n\r\n",
  "created_at":"2023-10-10T20:01:17Z",
  "id":1756138646,
  "issue":979,
  "node_id":"IC_kwDOD6Q_ss5orIyW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-10T20:01:17Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> This is looking good. Does it depend on #983?\r\n\r\nIt does not.",
  "created_at":"2023-10-11T14:48:47Z",
  "id":1757855631,
  "issue":979,
  "node_id":"IC_kwDOD6Q_ss5oxr-P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-11T14:48:47Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"`test_0692_fsspec.py::test_fsspec_chunks` needs\r\n\r\n```python\r\npytest.importorskip(\"aiohttp\")\r\n```\r\n\r\nin the failing function. That's a non-strict dependency, so any of our tests that use it have to be skipped when the package isn't available.\r\n\r\nThis one failed because it's Python 3.12 and presumably the package isn't available _yet_, so I'm glad we got this chance to check.\r\n\r\nIf there are any tests that need fsspec or another non-strict dependency to run, those tests need to have `pytest.importorskip`, either in the individual function or at the top of the Python file for module-level.",
  "created_at":"2023-10-12T16:52:35Z",
  "id":1759996301,
  "issue":979,
  "node_id":"IC_kwDOD6Q_ss5o52mN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T16:52:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think file.py needs `from __future__ import annotations`.",
  "created_at":"2023-10-12T17:38:51Z",
  "id":1760068570,
  "issue":979,
  "node_id":"IC_kwDOD6Q_ss5o6IPa",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-10-12T17:38:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I guess we should also remove python 3.7 from the GitHub Actions workflow file right?",
  "created_at":"2023-10-06T23:28:36Z",
  "id":1751498516,
  "issue":980,
  "node_id":"IC_kwDOD6Q_ss5oZb8U",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-06T23:28:36Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"> I guess we should also remove python 3.7 from the GitHub Actions workflow file right?\r\n\r\nGood point! That would go with this PR. (Go ahead and make the modification directly; we won't merge it until everyone's satisfied with it.)",
  "created_at":"2023-10-06T23:43:50Z",
  "id":1751505294,
  "issue":980,
  "node_id":"IC_kwDOD6Q_ss5oZdmO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-06T23:43:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lobis that's weird \u2014 I thought I had done that here ... I think I'm working on too many PRs at once :laughing: Thanks!",
  "created_at":"2023-10-07T00:15:32Z",
  "id":1751520258,
  "issue":980,
  "node_id":"IC_kwDOD6Q_ss5oZhQC",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2023-10-07T00:15:32Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Should we also add python 3.12 to the CI? (I tried doing so but found some problems, it's a bit hard to debug with the current status of the pipeline, but I think it should be an easy fix).",
  "created_at":"2023-10-07T19:33:10Z",
  "id":1751801420,
  "issue":980,
  "node_id":"IC_kwDOD6Q_ss5oal5M",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-07T19:33:10Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Should we also add python 3.12 to the CI? \r\n\r\nDefinitely!",
  "created_at":"2023-10-08T10:55:33Z",
  "id":1751997164,
  "issue":980,
  "node_id":"IC_kwDOD6Q_ss5obVrs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-08T10:55:33Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski this is ready to merge. It updates our Python min, max versions (>=3.8, <=3.12), and fixes a use of `np.asarray(content)` that is broken by the latest awkward.\r\n\r\nUpon merging, you'd need to update the required branch protection rules.",
  "created_at":"2023-10-11T10:26:54Z",
  "id":1757349053,
  "issue":980,
  "node_id":"IC_kwDOD6Q_ss5ovwS9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-11T10:27:08Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> (edit: updating to boost_histogram 1.4.0 fixed this, was previously on 1.3.1)\r\n\r\nI think that was it: the boost-histogram version. I just tried it (with boost-histogram 1.4.0) and didn't encounter any errors.\r\n\r\nSo this is resolved, right? I'll close it, and if you disagree, just follow up on this thread with what isn't working.",
  "created_at":"2023-10-09T13:35:00Z",
  "id":1753027726,
  "issue":981,
  "node_id":"IC_kwDOD6Q_ss5ofRSO",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-09T13:35:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I started looking at this in #956, before realising it would be a lot more work than I wanted for that PR. I'm glad you're looking at this now!\r\n\r\nI felt at the time that it would be helpful for @jpivarski and @ioanaif to weigh in on how important is it to provide backwards compatibility in uproot's executors. Can we break them in a minor version release? If we _can't_ break them, then I think we would want to introduce an adaptor to make the uproot executors _look_ like `concurrent.futures`\r\n   \r\nRegarding the `ResourceThreadPoolExecutor`s, I didn't get as far as figuring out the best model to align with `concurrent.futures`. Upon reflection, I think we could try something that effectively `functools.partial`s the func, but with a callable argument factory e.g.:\r\n```python\r\nimport concurrent.futures\r\nimport threading\r\nimport functools\r\nimport urllib.request\r\n\r\n\r\ndef execute_with_resource(func, args, kwargs, local, create_resource):\r\n    try:\r\n        resource = local.value\r\n    except AttributeError:\r\n        resource = local.value = create_resource()\r\n\r\n    return func(*args, **kwargs, resource=resource)\r\n\r\n\r\ndef submit_with_resource(executor, func, local, create_resource, /, *args, **kwargs):\r\n    return executor.submit(\r\n        execute_with_resource, func, args, kwargs, local, create_resource\r\n    )\r\n\r\n\r\ndef read_n_bytes(resource, n_bytes):\r\n    return resource.read(n_bytes)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    url = \"https://github.com/scikit-hep/scikit-hep-testdata/raw/v0.4.33/src/skhep_testdata/data/uproot-issue121.root\"\r\n\r\n    executor = concurrent.futures.ThreadPoolExecutor()\r\n\r\n    locals = [\r\n        threading.local() for _ in range(executor._max_workers)\r\n    ]  # TODO: abstract this!\r\n\r\n    open_url = functools.partial(urllib.request.urlopen, url)\r\n\r\n    tasks = [\r\n        submit_with_resource(executor, read_n_bytes, local, open_url, n_bytes=512)\r\n        for local in locals\r\n    ]\r\n    for future in concurrent.futures.as_completed(tasks):\r\n        print(future.result())\r\n```\r\n\r\nThis wouldn't work with `ProcessPoolExecutor`, because the `threading.local` isn't pickleable. But, in any case, I think that kind of executor-agnostic abstraction would be good.",
  "created_at":"2023-10-11T12:29:43Z",
  "id":1757582946,
  "issue":983,
  "node_id":"IC_kwDOD6Q_ss5owpZi",
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "total_count":1
  },
  "updated_at":"2023-10-11T12:29:43Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Closing but saved for later if we remember.",
  "created_at":"2023-10-12T15:55:56Z",
  "id":1759903257,
  "issue":984,
  "node_id":"IC_kwDOD6Q_ss5o5f4Z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T15:55:56Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"starterkit.web.cern.ch just started working again in #971, so I'm going to just try re-running the failed tests to see if they work here, too.",
  "created_at":"2023-10-12T16:57:35Z",
  "id":1760004922,
  "issue":985,
  "node_id":"IC_kwDOD6Q_ss5o54s6",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-10-12T16:57:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In the results of `fgrep -rl '.asarray(' src` (on `main`):\r\n\r\n* src/uproot/_util.py:\r\n  * used in `ensure_numpy`\r\n* src/uproot/containers.py: nothing that could be a `Content`\r\n* src/uproot/writing/_cascadetree.py:\r\n  * used on the output of `ak.num` and `ak.flatten`, but those are all implicitly `highlevel=True`\r\n  * used on `branch_array.layout.offsets` and `layout.offsets` (`Index`)\r\n  * used on `branch_array` and `content`, which might be `Content`\r\n* src/uproot/behaviors/TProfile2D.py: nothing that could be a `Content`\r\n* src/uproot/behaviors/TH3.py: nothing that could be a `Content`\r\n* src/uproot/behaviors/TGraphAsymmErrors.py: nothing that could be a `Content`\r\n* src/uproot/behaviors/TProfile3D.py: nothing that could be a `Content`\r\n* src/uproot/behaviors/TH2.py: nothing that could be a `Content`\r\n* src/uproot/behaviors/TGraph.py: nothing that could be a `Content`\r\n* src/uproot/behaviors/TAxis.py: nothing that could be a `Content`\r\n* src/uproot/behaviors/TProfile.py: nothing that could be a `Content`\r\n* src/uproot/behaviors/TH1.py: nothing that could be a `Content`\r\n* src/uproot/interpretation/library.py:\r\n  * used on `array.offsets` (`Index`)\r\n  * used on `array.data` where `array` is a `NumpyArray` (good)\r\n\r\nIn the results of `fgrep -rl 'ensure_numpy' src`:\r\n\r\n* src/uproot/writing/identify.py:\r\n  * used in cases that might be `Content`\r\n\r\n`np.array` is rarely used to cast data (almost all cases are `np.asarray`) and those that are casting data all look safe to me: I don't think any of them could be `Content`.",
  "created_at":"2023-10-12T17:12:47Z",
  "id":1760030828,
  "issue":985,
  "node_id":"IC_kwDOD6Q_ss5o5_Bs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T17:12:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I *think* this PR catches all cases, now.",
  "created_at":"2023-10-12T17:46:05Z",
  "id":1760085513,
  "issue":985,
  "node_id":"IC_kwDOD6Q_ss5o6MYJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T17:46:05Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a forward-port of #985, right? It's missing `content` \u2192 `content.data` in\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/4a977f65d112310f3607ac7b679d43df46cdc8fa/src/uproot/writing/_cascadetree.py#L785",
  "created_at":"2023-10-12T18:39:29Z",
  "id":1760182554,
  "issue":986,
  "node_id":"IC_kwDOD6Q_ss5o6kEa",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-12T18:39:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, but that change is already in this branch :)",
  "created_at":"2023-10-12T18:44:56Z",
  "id":1760189997,
  "issue":986,
  "node_id":"IC_kwDOD6Q_ss5o6l4t",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T18:45:14Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Indeed it is!",
  "created_at":"2023-10-12T18:54:16Z",
  "id":1760203731,
  "issue":986,
  "node_id":"IC_kwDOD6Q_ss5o6pPT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-12T18:54:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski I added a test for it. `uproot.open` was not being tested against `xrootd` files (the source was being used directly) that is why this typo had been passing the tests. (Perhaps this test should be in another file though, I made a quick fix using pytest fixtures).",
  "created_at":"2023-10-13T20:10:14Z",
  "id":1762141897,
  "issue":990,
  "node_id":"IC_kwDOD6Q_ss5pCCbJ",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-13T20:10:31Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This is the log when running:\r\n\r\n`pytest tests/test_0692_fsspec.py::test_fsspec_chunks`\r\n\r\nThis only fails on ubuntu. Looks like it's an OS thing (not enough resources?).\r\n\r\n```\r\n========================================================================= test session starts =========================================================================\r\nplatform linux -- Python 3.10.12, pytest-7.4.2, pluggy-1.3.0\r\nrootdir: /uproot\r\nconfigfile: pyproject.toml\r\nplugins: timeout-2.2.0, rerunfailures-12.0\r\ntimeout: 600.0s\r\ntimeout method: signal\r\ntimeout func_only: False\r\ncollected 1 item                                                                                                                                                      \r\n\r\ntests/test_0692_fsspec.py .                                                                                                                                     [100%]\r\n\r\n========================================================================== 1 passed in 2.71s ==========================================================================\r\nFatal error on SSL transport\r\nprotocol: <asyncio.sslproto.SSLProtocol object at 0xffff79695f30>\r\ntransport: <_SelectorSocketTransport closing fd=18>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 924, in write\r\n    n = self._sock.send(data)\r\nOSError: [Errno 9] Bad file descriptor\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/sslproto.py\", line 690, in _process_write_backlog\r\n    self._transport.write(chunk)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 930, in write\r\n    self._fatal_error(exc, 'Fatal write error on socket transport')\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 725, in _fatal_error\r\n    self._force_close(exc)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 737, in _force_close\r\n    self._loop.call_soon(self._call_connection_lost, exc)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 753, in call_soon\r\n    self._check_closed()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\r\n    raise RuntimeError('Event loop is closed')\r\nRuntimeError: Event loop is closed\r\nFatal error on SSL transport\r\nprotocol: <asyncio.sslproto.SSLProtocol object at 0xffff796955d0>\r\ntransport: <_SelectorSocketTransport closing fd=19>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 924, in write\r\n    n = self._sock.send(data)\r\nOSError: [Errno 9] Bad file descriptor\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/sslproto.py\", line 690, in _process_write_backlog\r\n    self._transport.write(chunk)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 930, in write\r\n    self._fatal_error(exc, 'Fatal write error on socket transport')\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 725, in _fatal_error\r\n    self._force_close(exc)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 737, in _force_close\r\n    self._loop.call_soon(self._call_connection_lost, exc)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 753, in call_soon\r\n    self._check_closed()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\r\n    raise RuntimeError('Event loop is closed')\r\nRuntimeError: Event loop is closed\r\nFatal error on SSL transport\r\nprotocol: <asyncio.sslproto.SSLProtocol object at 0xffff79696cb0>\r\ntransport: <_SelectorSocketTransport closing fd=20>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 924, in write\r\n    n = self._sock.send(data)\r\nOSError: [Errno 9] Bad file descriptor\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/sslproto.py\", line 690, in _process_write_backlog\r\n    self._transport.write(chunk)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 930, in write\r\n    self._fatal_error(exc, 'Fatal write error on socket transport')\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 725, in _fatal_error\r\n    self._force_close(exc)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 737, in _force_close\r\n    self._loop.call_soon(self._call_connection_lost, exc)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 753, in call_soon\r\n    self._check_closed()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\r\n    raise RuntimeError('Event loop is closed')\r\nRuntimeError: Event loop is closed\r\nFatal error on SSL transport\r\nprotocol: <asyncio.sslproto.SSLProtocol object at 0xffff796961d0>\r\ntransport: <_SelectorSocketTransport closing fd=21>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 924, in write\r\n    n = self._sock.send(data)\r\nOSError: [Errno 9] Bad file descriptor\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/sslproto.py\", line 690, in _process_write_backlog\r\n    self._transport.write(chunk)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 930, in write\r\n    self._fatal_error(exc, 'Fatal write error on socket transport')\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 725, in _fatal_error\r\n    self._force_close(exc)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 737, in _force_close\r\n    self._loop.call_soon(self._call_connection_lost, exc)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 753, in call_soon\r\n    self._check_closed()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\r\n    raise RuntimeError('Event loop is closed')\r\nRuntimeError: Event loop is closed\r\nFatal error on SSL transport\r\nprotocol: <asyncio.sslproto.SSLProtocol object at 0xffff79697a00>\r\ntransport: <_SelectorSocketTransport closing fd=22>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 924, in write\r\n    n = self._sock.send(data)\r\nOSError: [Errno 9] Bad file descriptor\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/sslproto.py\", line 690, in _process_write_backlog\r\n    self._transport.write(chunk)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 930, in write\r\n    self._fatal_error(exc, 'Fatal write error on socket transport')\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 725, in _fatal_error\r\n    self._force_close(exc)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 737, in _force_close\r\n    self._loop.call_soon(self._call_connection_lost, exc)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 753, in call_soon\r\n    self._check_closed()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\r\n    raise RuntimeError('Event loop is closed')\r\nRuntimeError: Event loop is closed\r\nFatal error on SSL transport\r\nprotocol: <asyncio.sslproto.SSLProtocol object at 0xffff796953c0>\r\ntransport: <_SelectorSocketTransport closing fd=17>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 924, in write\r\n    n = self._sock.send(data)\r\nOSError: [Errno 9] Bad file descriptor\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/sslproto.py\", line 690, in _process_write_backlog\r\n    self._transport.write(chunk)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 930, in write\r\n    self._fatal_error(exc, 'Fatal write error on socket transport')\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 725, in _fatal_error\r\n    self._force_close(exc)\r\n  File \"/usr/lib/python3.10/asyncio/selector_events.py\", line 737, in _force_close\r\n    self._loop.call_soon(self._call_connection_lost, exc)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 753, in call_soon\r\n    self._check_closed()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\r\n    raise RuntimeError('Event loop is closed')\r\nRuntimeError: Event loop is closed\r\n\r\n```",
  "created_at":"2023-10-17T15:52:54Z",
  "id":1766704145,
  "issue":992,
  "node_id":"IC_kwDOD6Q_ss5pTcQR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-17T15:54:48Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"[c615eca](https://github.com/scikit-hep/uproot5/pull/992/commits/c615eca1c30c7b9bd1a01ff16a3626df6a3c66d3) fixes the issue with the `fsspec` chunks test (but now `xrootd` `fsspec` tests are failing...).\r\n\r\nI think the problem has something to do with the shutting down of the loop executor. It's lifetime should in principle be bound to the source, but there is something wrong when doing the shutdown.",
  "created_at":"2023-10-17T16:57:28Z",
  "id":1766815402,
  "issue":992,
  "node_id":"IC_kwDOD6Q_ss5pT3aq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-17T16:57:28Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"~~@lobis I've only scanned over this PR, but my current impression is that the context manager stops the loop as soon as the `__exit__` pathway is executed. I think you'll want to actually wait until the loop is finished, otherwise `asyncio` will cancel the running task.~~\r\n\r\nNo, it's OK; we're blocking on the loop result in the usage.",
  "created_at":"2023-10-17T17:00:03Z",
  "id":1766819203,
  "issue":992,
  "node_id":"IC_kwDOD6Q_ss5pT4WD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-17T17:07:44Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Looks like something \"out of our control\" (fsspec?) is submitting tasks to the loop after it has finished processing the requested tasks (get the chunks). This fails if the loop is not running (we shut it down because we don't expect to do any more work).\r\n\r\nWhy this happens (and why it only happens in some OS...) is a mistery to me.\r\n\r\nMaybe the pattern of spawning a thread bound to a source has some fundamental flaw? Maybe we should use a single loop for everything async-related, so making the `LoopExecutor` a singleton? (but how do we shut it down? making it a daemon thread?). IF (big if) we want to transition uproot into an async code with sync wrappers this pattern would be more suited for it, as it's one step closer to having a single loop blocking the main thread.",
  "created_at":"2023-10-17T19:01:47Z",
  "id":1766994515,
  "issue":992,
  "node_id":"IC_kwDOD6Q_ss5pUjJT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-17T19:02:35Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"For anyone following up: I think I know what the problem is: something is trying to shutdown the executor while the intended tasks are running, this should not happen. Now I only need to find it.",
  "created_at":"2023-10-17T20:37:14Z",
  "id":1767133724,
  "issue":992,
  "node_id":"IC_kwDOD6Q_ss5pVFIc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-17T20:37:14Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> For anyone following up: I think I know what the problem is: something is trying to shutdown the executor while the intended tasks are running, this should not happen. Now I only need to find it.\n\nThis makes sense - I was curious as to whether this was happening in https://github.com/scikit-hep/uproot5/pull/992#issuecomment-1766819203\n\nHowever, we should be blocking on the future that corresponds with the loop result, so I am surprised. I can take a look at that today! \n",
  "created_at":"2023-10-18T07:00:13Z",
  "id":1767796039,
  "issue":992,
  "node_id":"IC_kwDOD6Q_ss5pXm1H",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-18T07:00:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I realised that fsspec provides and manages it's own loop - exactly what we want, so I trashed the previous implementation of the `LoopExecutor` and just used this one instead. I updated the PR description (https://github.com/scikit-hep/uproot5/pull/992#issue-1943634061).\r\n\r\nIt should probably be reviewed again since it's gone under significant changes since the last review.",
  "created_at":"2023-10-19T04:13:32Z",
  "id":1769861735,
  "issue":992,
  "node_id":"IC_kwDOD6Q_ss5pffJn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-19T13:29:59Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I removed the import completely.",
  "created_at":"2023-10-18T14:06:53Z",
  "id":1768539749,
  "issue":994,
  "node_id":"IC_kwDOD6Q_ss5pacZl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-18T14:06:53Z",
  "user":"MDQ6VXNlcjMzMDU4NzQ3"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, now it's in a good state. The lines where there's a hidden dependency on hist (Uproot would raise `ModuleNotFoundError` suggesting that you install it) are\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/e4b4b80cea06537108cf49555b4bb631b64bad15/tests/test_0965-inverted-axes-variances-hist-888.py#L25\r\n\r\nand\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/e4b4b80cea06537108cf49555b4bb631b64bad15/tests/test_0965-inverted-axes-variances-hist-888.py#L46\r\n\r\nNow that the file has\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/e4b4b80cea06537108cf49555b4bb631b64bad15/tests/test_0965-inverted-axes-variances-hist-888.py#L8\r\n\r\nI'll enable \"auto-merge (squash).\"",
  "created_at":"2023-10-18T14:12:42Z",
  "id":1768552642,
  "issue":994,
  "node_id":"IC_kwDOD6Q_ss5pafjC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-18T14:12:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @GaetanLepage for test\r\n\r\nThanks!\r\n",
  "created_at":"2023-10-18T14:14:10Z",
  "id":1768555613,
  "issue":994,
  "node_id":"IC_kwDOD6Q_ss5pagRd",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-18T14:14:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/995) to add @GaetanLepage! :tada:",
  "created_at":"2023-10-18T14:14:19Z",
  "id":1768555941,
  "issue":994,
  "node_id":"IC_kwDOD6Q_ss5pagWl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-18T14:14:19Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"For posterity, I don't think mypy understands the integral type. Also, there's the os.PathLike interface for non-str/bytes types, so we could define that in a local types module. ",
  "created_at":"2023-10-18T19:35:02Z",
  "id":1769196448,
  "issue":996,
  "node_id":"IC_kwDOD6Q_ss5pc8ug",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-18T19:35:02Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This is mostly stemming from how nanoevents is implemented. The schemas that eventually create the rendered forms are provided to the machinery as classes, not instances of classes, so any client-side configuration does not propagate to the worker side (without significant re-engineering of nanoevents itself or nasty pickling hacks). \r\n\r\nThere may be a minor performance gain from not re-executing the same code to build the form thousands of times when on a realistic dataset. The main benefit I see is that this sets a firebreak in the interface so that it is clear to people developing schemas and data formats that anything stateful is client-side *only* and needs to be provided to dask workers via the form and behaviors *alone*.",
  "created_at":"2023-10-18T17:56:24Z",
  "id":1769057668,
  "issue":997,
  "node_id":"IC_kwDOD6Q_ss5pca2E",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-18T18:00:38Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"So, there are indeed a couple of related but distinct considerations to be made here:\r\n1. Performance; form remapping is performed _for each chunk_ for each file.\r\n2. Correctness; schemas are not pickle-aware, so anything performed on the client w.r.t schemas doesn't behave properly\r\n\r\nI am guessing that we could customise the schema pickler using a metaclass, but that's probably not the right solution here given (1).\r\n\r\nI wasn't worried about the performance of remapping, and expect it to be idempotent, but perhaps that first assumption was too strong.\r\n\r\nI will flesh out a PR to fix this. Although we're in \"this is very niche\" land, I'll try not to break existing code.",
  "created_at":"2023-10-18T19:00:29Z",
  "id":1769144862,
  "issue":997,
  "node_id":"IC_kwDOD6Q_ss5pcwIe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-18T19:00:29Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"FWIW this appears to break no existing code at least for what it's concerned (already tested it out on coffea), I'm sure there are some other odds and ends to deal with.",
  "created_at":"2023-10-18T19:12:21Z",
  "id":1769164045,
  "issue":997,
  "node_id":"IC_kwDOD6Q_ss5pc00N",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-18T19:12:21Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Superseded by #998 ",
  "created_at":"2023-10-19T15:45:22Z",
  "id":1771257762,
  "issue":997,
  "node_id":"IC_kwDOD6Q_ss5pkz-i",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-19T15:45:22Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lgray can you check coffea against this PR to confirm no regressions?",
  "created_at":"2023-10-18T21:24:30Z",
  "id":1769343931,
  "issue":998,
  "node_id":"IC_kwDOD6Q_ss5pdgu7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-18T21:24:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@agoose77 yep looks good on my side.",
  "created_at":"2023-10-19T00:51:27Z",
  "id":1769720184,
  "issue":998,
  "node_id":"IC_kwDOD6Q_ss5pe8l4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-19T00:51:27Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Last touched 12 hours ago, I guess it's ready. I'll update to main to re-run the tests and enable auto-merge.",
  "created_at":"2023-10-19T17:51:36Z",
  "id":1771454373,
  "issue":998,
  "node_id":"IC_kwDOD6Q_ss5plj-l",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-10-19T17:51:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Trace for the error:\r\n\r\n```/Users/lobis/miniconda3/envs/uproot-38/bin/python /Users/lobis/Applications/PyCharm Professional Edition.app/Contents/plugins/python/helpers/pycharm/_jb_pytest_runner.py --target test_0692_fsspec.py::test_open_fsspec_s3_issue \r\nTesting started at 13:05 ...\r\nLaunching pytest with arguments test_0692_fsspec.py::test_open_fsspec_s3_issue --no-header --no-summary -q in /Users/lobis/git/uproot/tests\r\n\r\n============================= test session starts ==============================\r\ncollecting ... collected 1 item\r\n\r\ntest_0692_fsspec.py::test_open_fsspec_s3_issue \r\n\r\n============================== 1 failed in 17.62s ==============================\r\nFAILED                    [100%]\r\ntests/test_0692_fsspec.py:98 (test_open_fsspec_s3_issue)\r\ncls = <class '_pytest.runner.CallInfo'>\r\nfunc = <function call_runtest_hook.<locals>.<lambda> at 0x10774faf0>\r\nwhen = 'call'\r\nreraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)\r\n\r\n    @classmethod\r\n    def from_call(\r\n        cls,\r\n        func: \"Callable[[], TResult]\",\r\n        when: \"Literal['collect', 'setup', 'call', 'teardown']\",\r\n        reraise: Optional[\r\n            Union[Type[BaseException], Tuple[Type[BaseException], ...]]\r\n        ] = None,\r\n    ) -> \"CallInfo[TResult]\":\r\n        \"\"\"Call func, wrapping the result in a CallInfo.\r\n    \r\n        :param func:\r\n            The function to call. Called without arguments.\r\n        :param when:\r\n            The phase in which the function is called.\r\n        :param reraise:\r\n            Exception or exceptions that shall propagate if raised by the\r\n            function, instead of being wrapped in the CallInfo.\r\n        \"\"\"\r\n        excinfo = None\r\n        start = timing.time()\r\n        precise_start = timing.perf_counter()\r\n        try:\r\n>           result: Optional[TResult] = func()\r\n\r\ncls        = <class '_pytest.runner.CallInfo'>\r\nduration   = 17.538951792\r\nexcinfo    = <ExceptionInfo PytestUnraisableExceptionWarning('Exception ignored in: <function _SSLProtocolTransport.__del__ at 0x1021ab9d0>\\n\\nTra...g, source=self)\\nResourceWarning: unclosed transport <asyncio.sslproto._SSLProtocolTransport object at 0x122382be0>\\n') tblen=7>\r\nfunc       = <function call_runtest_hook.<locals>.<lambda> at 0x10774faf0>\r\nprecise_start = 0.873500208\r\nprecise_stop = 18.412452\r\nreraise    = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)\r\nresult     = None\r\nstart      = 1698685524.7358022\r\nstop       = 1698685542.2744918\r\nwhen       = 'call'\r\n\r\n../../../miniconda3/envs/uproot-38/lib/python3.8/site-packages/_pytest/runner.py:341: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../../miniconda3/envs/uproot-38/lib/python3.8/site-packages/_pytest/runner.py:262: in <lambda>\r\n    lambda: ihook(item=item, **kwds), when=when, reraise=reraise\r\n        ihook      = <HookCaller 'pytest_runtest_call'>\r\n        item       = <Function test_open_fsspec_s3_issue>\r\n        kwds       = {}\r\n../../../miniconda3/envs/uproot-38/lib/python3.8/site-packages/pluggy/_hooks.py:493: in __call__\r\n    return self._hookexec(self.name, self._hookimpls, kwargs, firstresult)\r\n        firstresult = False\r\n        kwargs     = {'item': <Function test_open_fsspec_s3_issue>}\r\n        self       = <HookCaller 'pytest_runtest_call'>\r\n../../../miniconda3/envs/uproot-38/lib/python3.8/site-packages/pluggy/_manager.py:115: in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n        firstresult = False\r\n        hook_name  = 'pytest_runtest_call'\r\n        kwargs     = {'item': <Function test_open_fsspec_s3_issue>}\r\n        methods    = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/Users/lobis/miniconda3/envs/uproot-38/lib/python3.8/site-packages/_pytest/runner.py'>>,\r\n <HookImpl plugin_name='skipping', plugin=<module '_pytest.skipping' from '/Users/lobis/miniconda3/envs/uproot-38/lib/python3.8/site-packages/_pytest/skipping.py'>>,\r\n <HookImpl plugin_name='timeout', plugin=<module 'pytest_timeout' from '/Users/lobis/miniconda3/envs/uproot-38/lib/python3.8/site-packages/pytest_timeout.py'>>,\r\n <HookImpl plugin_name='capturemanager', plugin=<CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=6 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=7 _state='suspended' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=8 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> in_=<FDCapture 0 oldfd=3 _state='started' tmpfile=<_io.TextIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>>,\r\n <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x1076b9eb0>>,\r\n <HookImpl plugin_name='unraisableexception', plugin=<module '_pytest.unraisableexception' from '/Users/lobis/miniconda3/envs/uproot-38/lib/python3.8/site-packages/_pytest/unraisableexception.py'>>,\r\n <HookImpl plugin_name='threadexception', plugin=<module '_pytest.threadexception' from '/Users/lobis/miniconda3/envs/uproot-38/lib/python3.8/site-packages/_pytest/threadexception.py'>>]\r\n        self       = <_pytest.config.PytestPluginManager object at 0x10350bd90>\r\n../../../miniconda3/envs/uproot-38/lib/python3.8/site-packages/_pytest/unraisableexception.py:88: in pytest_runtest_call\r\n    yield from unraisable_exception_runtest_hook()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n    def unraisable_exception_runtest_hook() -> Generator[None, None, None]:\r\n        with catch_unraisable_exception() as cm:\r\n            yield\r\n            if cm.unraisable:\r\n                if cm.unraisable.err_msg is not None:\r\n                    err_msg = cm.unraisable.err_msg\r\n                else:\r\n                    err_msg = \"Exception ignored in\"\r\n                msg = f\"{err_msg}: {cm.unraisable.object!r}\\n\\n\"\r\n                msg += \"\".join(\r\n                    traceback.format_exception(\r\n                        cm.unraisable.exc_type,\r\n                        cm.unraisable.exc_value,\r\n                        cm.unraisable.exc_traceback,\r\n                    )\r\n                )\r\n>               warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))\r\nE               pytest.PytestUnraisableExceptionWarning: Exception ignored in: <function _SSLProtocolTransport.__del__ at 0x1021ab9d0>\r\nE               \r\nE               Traceback (most recent call last):\r\nE                 File \"/Users/lobis/miniconda3/envs/uproot-38/lib/python3.8/asyncio/sslproto.py\", line 321, in __del__\r\nE                   _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\r\nE               ResourceWarning: unclosed transport <asyncio.sslproto._SSLProtocolTransport object at 0x122382be0>\r\n\r\ncm         = <_pytest.unraisableexception.catch_unraisable_exception object at 0x107742b20>\r\nerr_msg    = 'Exception ignored in'\r\nmsg        = ('Exception ignored in: <function _SSLProtocolTransport.__del__ at '\r\n '0x1021ab9d0>\\n'\r\n '\\n'\r\n 'Traceback (most recent call last):\\n'\r\n '  File '\r\n '\"/Users/lobis/miniconda3/envs/uproot-38/lib/python3.8/asyncio/sslproto.py\", '\r\n 'line 321, in __del__\\n'\r\n '    _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\\n'\r\n 'ResourceWarning: unclosed transport <asyncio.sslproto._SSLProtocolTransport '\r\n 'object at 0x122382be0>\\n')\r\n\r\n../../../miniconda3/envs/uproot-38/lib/python3.8/site-packages/_pytest/unraisableexception.py:78: PytestUnraisableExceptionWarning\r\n\r\nProcess finished with exit code 1```",
  "created_at":"2023-10-30T17:06:33Z",
  "id":1785679509,
  "issue":999,
  "node_id":"IC_kwDOD6Q_ss5qb06V",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-30T17:06:33Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"\r\n> Although `sshfs` has been added to the test dependencies, I think the only ssh test is disabled. I wonder if running sshd on the test-runner and connecting to\r\n> \r\n> ```shell\r\n> ssh `whoami`@localhost\r\n> ```\r\n> \r\n> would be an option? It's not a big deal.\r\n\r\nGood idea I can try this in a different PR, I can use the cache directory for `skhep_testdata` after pulling the file in the same test.\r\n\r\n> Isolating the glitchiness of the test to S3 is good\u2014we can provide the functionality without testing it because it's one of the things fsspec is supposed to do on its own. (We should only be responsible for using the fsspec API correctly in Uproot\u2014there's a \"separation of concerns.\") It's too bad that it can't be reproduced outside of Uproot, but I know you put a lot of time into trying to do that.\r\n\r\nI'm 90% sure it's `s3fs` but I cannot say for sure. I created https://github.com/scikit-hep/uproot5/pull/1012 to continue debugging it. I would say the problem lies here: https://github.com/fsspec/s3fs/blob/main/s3fs/core.py#L541-L560, I don't think the socket is properly closed.\r\n",
  "created_at":"2023-10-31T14:09:26Z",
  "id":1787295563,
  "issue":999,
  "node_id":"IC_kwDOD6Q_ss5qh_dL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-31T14:09:26Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":">>> h2 = uproot.open(\"/tmp/whatever.root:h1\")\r\n>>> h2.values()\r\narray([25.,  0.])                    # good\r\n>>> h2.errors()\r\narray([5.59016994, 0.        ])      # good\r\n\r\n>>> import numpy as np\r\n>>> h3 = h2.to_hist()\r\n>>> h3.values()\r\narray([100.,   0.])                  # this is very different\r\n>>> np.sqrt(h3.variances())\r\narray([2.79508497,        nan])      # this is very different\u2028\u2028\r\n\r\nThe above happens because internally, in `to_hist` we define values to be equal to data; which means than values in hist is set to be the sum of [10,20,30,40] = [100].\r\n\r\n\u2028\u2028The variances should be correct, we set them as the square of errors, but as I understood from Henry, they are internally recomputed using the \"_sum_of_weighted_deltas_squared\u201d, which we set as : \r\n\r\n```\r\nview[\"_sum_of_weighted_deltas_squared\"] = variances * (\r\n            sum_of_bin_weights - sum_of_bin_weights_squared / sum_of_bin_weights\r\n        )\r\n```\r\n\r\nWhen writing them to a file and checking from ROOT, it is all good because we made the `values` `data` swap (the same that created the .values() mismatch above) \r\n\r\n\u2028>>> h2.GetBinContent(1)              # good\r\n25.0\r\n>>> h2.GetBinError(1)                # good\r\n5.5901699437494745\r\n\r\n>>> h3.GetBinContent(1)              # good\r\n25.0\r\n>>> h3.GetBinError(1)                # good\r\n5.5901699437494745\r\n",
  "created_at":"2023-10-27T09:12:37Z",
  "id":1782573663,
  "issue":1000,
  "node_id":"IC_kwDOD6Q_ss5qP-pf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-27T09:13:20Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"I just checked this, and it's not an Uproot error. ROOT isn't adding the nested TBranch to the ROOT file. I think it has more to do with the fact that ROOT has a built-in dictionary (class definition for streamers) for `std::vector<int>` but not for `std::vector<std::vector<int>>` than the fact that there are zero entries. I think the zero entries is a red herring.\r\n\r\nWhen I make the ROOT file using your procedure (adding `f.Close()` at the end), I get this warning message:\r\n\r\n```\r\nError in <TTree::Branch>: The class requested (vector<vector<int> >) for the branch \"nested\" is an instance of an stl collection and does not have a compiled CollectionProxy. Please generate the dictionary for this collection (vector<vector<int> >) to avoid to write corrupted data.\r\n```\r\n\r\nThen when I try to read it back in Uproot, I do get the KeyError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/reading.py\", line 2098, in __getitem__\r\n    return step[\"/\".join(items[i:])]\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/behaviors/TBranch.py\", line 1655, in __getitem__\r\n    raise uproot.KeyInFileError(\r\nuproot.exceptions.KeyInFileError: not found: 'nested'\r\n\r\n    Available keys: 'simple'\r\n\r\nin file empty-tree.root\r\nin object /tree;1\r\n```\r\n\r\nbecause it can't find a TBranch named `nested`; the full set of TBranch names is only `simple`. Looking at this directly,\r\n\r\n```python\r\n>>> t = uproot.open(\"empty-tree.root\")[\"tree\"]\r\n>>> t.show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nsimple               | int32_t                  | AsDtype('>i4')\r\n```\r\n\r\nI can check this in ROOT as well. The TTree has only one TBranch:\r\n\r\n```python\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"empty-tree.root\")\r\n>>> t = f.Get(\"tree\")\r\n>>> t.Print()\r\n******************************************************************************\r\n*Tree    :tree      : Tree                                                   *\r\n*Entries :        0 : Total =             826 bytes  File  Size =        329 *\r\n*        :          : Tree compression factor =   1.00                       *\r\n******************************************************************************\r\n*Br    0 :simple    : simple/I                                               *\r\n*Entries :        0 : Total  Size=        489 bytes  One basket in memory    *\r\n*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n```\r\n\r\nSo the writing process (in ROOT) never wrote the `nested` TBranch to the TTree, and it's very likely because there's no dictionary for `std::vector<std::vector<int>>`, not anything to do with it being empty.",
  "created_at":"2023-10-26T14:06:38Z",
  "id":1781203264,
  "issue":1008,
  "node_id":"IC_kwDOD6Q_ss5qKwFA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-26T14:06:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I suppose there's no legitimate reason to use `://` in a file path or an object path (both of which use `/+` as a delimiter, and the initial `/` in an object path is optional). Therefore, if someone gets the \"Invalid URI scheme\" error message, they'll likely recognize what it is and they'll have a recourse to work around it.\r\n> \r\n> Is it possible to get the `_remote_schemes` from fsspec? Just wondering.\r\n> \r\n> Anyway, this PR is good and may be merged.\r\n\r\nYes, we can get all schemes from `fsspec`. I left the previous code there because `fsspec` is not yet a dependency. Once it is this will be further simplified.",
  "created_at":"2023-10-25T20:37:29Z",
  "id":1780016955,
  "issue":1009,
  "node_id":"IC_kwDOD6Q_ss5qGOc7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-25T20:37:29Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"> I left the previous code there because `fsspec` is not yet a dependency.\r\n\r\nOf course! That makes a lot of sense.",
  "created_at":"2023-10-25T20:43:28Z",
  "id":1780024674,
  "issue":1009,
  "node_id":"IC_kwDOD6Q_ss5qGQVi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-25T20:43:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This particular package isn't super well-maintained (it has a single maintainer who's expressed an interest in handing off to someone else). However, it's also a very simple, self-contained part of the test system, and there _is_ interest from another developer in maintaining. We could probably just vendor it in future, if we need to. So, on the balance of cost-tradeoffs, I think this is a sensible inclusion. It gets my approval \u2014 but definitely wait for Jim's review, too.",
  "created_at":"2023-10-26T06:49:33Z",
  "id":1780513783,
  "issue":1010,
  "node_id":"IC_kwDOD6Q_ss5qIHv3",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-10-26T06:50:39Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> The diff is quite reasonable, but the list of commits is very long! Were you debugging in CI? (Not that I haven't done it myself...)\r\n\r\nYes, before I gave up on fixing the underlying issue I tried many different things.\r\n\r\nI'm curious, is there a problem I'm missing with the long list of commits? I'll get squashed, it'll just be a very long commit message, am I right?\r\n\r\n",
  "created_at":"2023-11-16T20:39:31Z",
  "id":1815278223,
  "issue":1012,
  "node_id":"IC_kwDOD6Q_ss5sMvKP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T20:39:31Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"That's right. It will be a long description; the title of the commit will be the title of the PR plus \" (#1012)\". It's only a problem for mouseover in the GitHub web interface, which shows the long description of the commit you're hovering over.",
  "created_at":"2023-11-16T20:45:19Z",
  "id":1815285264,
  "issue":1012,
  "node_id":"IC_kwDOD6Q_ss5sMw4Q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T20:45:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> If you've tested this on your computer, you have sshd, and it passes, that's good enough for me. Even if you haven't, this is fine to merge as-is. If there's a problem with it that we discover later, then we'll deal with it then.\r\n\r\nNo, I actually never tested this. After running the test I saw that `sshfs` does not implement `cat_file` interface (required to request a chunk of the file), so it's not possible to use uproot to read ssh files with fsspec at this time.",
  "created_at":"2023-10-31T15:49:23Z",
  "id":1787493441,
  "issue":1013,
  "node_id":"IC_kwDOD6Q_ss5qivxB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-31T15:49:23Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"So it will always fail with NotImplementedError until they (or we) add that feature. That's okay, it's a placeholder, and it will alert us to when they do add it (if it fails).",
  "created_at":"2023-10-31T15:52:35Z",
  "id":1787499625,
  "issue":1013,
  "node_id":"IC_kwDOD6Q_ss5qixRp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-10-31T15:52:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"\r\n> I'll need to know more about what a \"chain of filesystems\" would do. Is it something we'd even be able to use, given our API?\r\n\r\nI guess it has some niche uses, for example, opening a root file inside a zip file available via http. It should also work for writing. But in general I think only the local caching for writing will be used \"frequently\".",
  "created_at":"2023-11-15T20:44:01Z",
  "id":1813235544,
  "issue":1016,
  "node_id":"IC_kwDOD6Q_ss5sE8dY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-15T20:44:01Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":":wave: I just wanted to raise that this PR breaks `pyhf`'s nightly HEAD of dependency tests. For example, installing `uproot` from 28b0f7bdbeb18b54ae2af4740607d982eea51b35 causes `pyhf`'s `test_export_root_histogram` test in `tests/test_export.py` to fail. Here's the [relevant bit](https://github.com/scikit-hep/pyhf/blob/43a84a477a93df640faea5c0526dc3abeead9759/tests/test_export.py#L411-L421)\r\n\r\n```python\r\ndef test_export_root_histogram(mocker, tmp_path):\r\n    \"\"\"\r\n    Test that pyhf.writexml._export_root_histogram writes out a histogram\r\n    in the manner that uproot is expecting and verifies this by reading\r\n    the serialized file\r\n    \"\"\"\r\n    mocker.patch(\"pyhf.writexml._ROOT_DATA_FILE\", {})\r\n    pyhf.writexml._export_root_histogram(\"hist\", [0, 1, 2, 3, 4, 5, 6, 7, 8])\r\n\r\n    with uproot.recreate(tmp_path.joinpath(\"test_export_root_histogram.root\")) as file:\r\n        file[\"hist\"] = pyhf.writexml._ROOT_DATA_FILE[\"hist\"]\r\n```\r\n\r\nwhich then fails with a `TypeError` and the following traceback\r\n\r\n<details>\r\n<summary>Collapsed for space:</summary>\r\n\r\n```pytb\r\n========================================================================================== FAILURES ==========================================================================================\r\n_________________________________________________________________________________ test_export_root_histogram _________________________________________________________________________________\r\n\r\nmocker = <pytest_mock.plugin.MockerFixture object at 0x7ff50e8d4cd0>, tmp_path = PosixPath('/tmp/pytest-of-feickert/pytest-11/test_export_root_histogram0')\r\n\r\n    def test_export_root_histogram(mocker, tmp_path):\r\n        \"\"\"\r\n        Test that pyhf.writexml._export_root_histogram writes out a histogram\r\n        in the manner that uproot is expecting and verifies this by reading\r\n        the serialized file\r\n        \"\"\"\r\n        mocker.patch(\"pyhf.writexml._ROOT_DATA_FILE\", {})\r\n        pyhf.writexml._export_root_histogram(\"hist\", [0, 1, 2, 3, 4, 5, 6, 7, 8])\r\n    \r\n>       with uproot.recreate(tmp_path.joinpath(\"test_export_root_histogram.root\")) as file:\r\n\r\nmocker     = <pytest_mock.plugin.MockerFixture object at 0x7ff50e8d4cd0>\r\ntmp_path   = PosixPath('/tmp/pytest-of-feickert/pytest-11/test_export_root_histogram0')\r\n\r\ntests/test_export.py:420: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../../.pyenv/versions/pyhf-dev-cpu/lib/python3.11/site-packages/uproot/writing/writable.py:148: in recreate\r\n    sink = _sink_from_path(file_path, **storage_options)\r\n        file_path  = PosixPath('/tmp/pytest-of-feickert/pytest-11/test_export_root_histogram0/test_export_root_histogram.root')\r\n        options    = {}\r\n        storage_options = {}\r\n../../../.pyenv/versions/pyhf-dev-cpu/lib/python3.11/site-packages/uproot/writing/writable.py:80: in _sink_from_path\r\n    return uproot.sink.file.FileSink.from_object(file_path_or_object)\r\n        file_path_or_object = PosixPath('/tmp/pytest-of-feickert/pytest-11/test_export_root_histogram0/test_export_root_histogram.root')\r\n        storage_options = {}\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ncls = <class 'uproot.sink.file.FileSink'>, obj = PosixPath('/tmp/pytest-of-feickert/pytest-11/test_export_root_histogram0/test_export_root_histogram.root')\r\n\r\n    @classmethod\r\n    def from_object(cls, obj) -> FileSink:\r\n        \"\"\"\r\n        Args:\r\n            obj (file-like object): An object with ``read``, ``write``, ``seek``,\r\n                ``tell``, and ``flush`` methods.\r\n    \r\n        Creates a :doc:`uproot.sink.file.FileSink` from a file-like object, such\r\n        as ``io.BytesIO``. The object must be readable, writable, and seekable\r\n        with ``\"r+b\"`` mode semantics.\r\n        \"\"\"\r\n        if (\r\n            callable(getattr(obj, \"read\", None))\r\n            and callable(getattr(obj, \"write\", None))\r\n            and callable(getattr(obj, \"seek\", None))\r\n            and callable(getattr(obj, \"tell\", None))\r\n            and callable(getattr(obj, \"flush\", None))\r\n            and (not hasattr(obj, \"readable\") or obj.readable())\r\n            and (not hasattr(obj, \"writable\") or obj.writable())\r\n            and (not hasattr(obj, \"seekable\") or obj.seekable())\r\n        ):\r\n            self = cls(None)\r\n            self._file = obj\r\n        else:\r\n>           raise TypeError(\r\n                \"\"\"writable file can only be created from a file path or an object\r\n    \r\n    * that has 'read', 'write', 'seek', and 'tell' methods\r\n    * is 'readable() and writable() and seekable()'\"\"\"\r\n            )\r\nE           TypeError: writable file can only be created from a file path or an object\r\nE           \r\nE               * that has 'read', 'write', 'seek', and 'tell' methods\r\nE               * is 'readable() and writable() and seekable()'\r\n\r\ncls        = <class 'uproot.sink.file.FileSink'>\r\nobj        = PosixPath('/tmp/pytest-of-feickert/pytest-11/test_export_root_histogram0/test_export_root_histogram.root')\r\n\r\n../../../.pyenv/versions/pyhf-dev-cpu/lib/python3.11/site-packages/uproot/sink/file.py:55: TypeError\r\n================================================================================== short test summary info ===================================================================================\r\nFAILED tests/test_export.py::test_export_root_histogram - TypeError: writable file can only be created from a file path or an object\r\n```\r\n\r\n</details>\r\n\r\nI can raise an Issue if that's helpful, but I thought I'd start here in case this is relevant to ongoing discussion.",
  "created_at":"2023-11-16T07:13:15Z",
  "id":1813902187,
  "issue":1016,
  "node_id":"IC_kwDOD6Q_ss5sHfNr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T07:13:57Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi, I ran into the same thing in `cabinetry`, here is a minimal reproducer:\r\n```python\r\nimport pathlib\r\nimport uproot\r\n\r\nuproot.recreate(pathlib.Path(\"f.root\"))\r\n```\r\nI will open an issue for reference. -> #1029",
  "created_at":"2023-11-16T07:39:21Z",
  "id":1813932427,
  "issue":1016,
  "node_id":"IC_kwDOD6Q_ss5sHmmL",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-16T07:49:10Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"Looking at the stack trace, it must be because pyhf was relying on passing a file-like object `obj` that satisfies\r\n\r\n```python\r\n    callable(getattr(obj, \"read\", None))\r\nand callable(getattr(obj, \"write\", None))\r\nand callable(getattr(obj, \"seek\", None))\r\nand callable(getattr(obj, \"tell\", None))\r\nand callable(getattr(obj, \"flush\", None))\r\nand (not hasattr(obj, \"readable\") or obj.readable())\r\nand (not hasattr(obj, \"writable\") or obj.writable())\r\nand (not hasattr(obj, \"seekable\") or obj.seekable())\r\n```\r\n\r\nand this has somehow changed. (It wasn't supposed to.) @lobis?\r\n\r\n----------------\r\n\r\nI just tried a test and I don't see what's wrong with it.\r\n\r\n```python\r\nimport os\r\nimport numpy as np\r\nimport uproot\r\n\r\nclass FileLikeObject:\r\n    def __init__(self):\r\n        self.data = np.zeros(0, dtype=np.uint8)\r\n        self.pos = 0\r\n    def tobytes(self) -> bytes:\r\n        return self.data.tobytes()\r\n    def ensure(self, size: int) -> None:\r\n        if size > len(self.data):\r\n            data = np.zeros(size, dtype=np.uint8)\r\n            data[:len(self.data)] = self.data\r\n            self.data = data\r\n    def read(self, num_bytes: int) -> bytes:\r\n        self.pos += num_bytes\r\n        self.ensure(self.pos)\r\n        return self.data[self.pos - num_bytes : self.pos].tobytes()\r\n    def write(self, data: bytes) -> None:\r\n        self.pos += len(data)\r\n        self.ensure(self.pos)\r\n        self.data[self.pos - len(data) : self.pos] = np.frombuffer(data, np.uint8)\r\n    def seek(self, offset: int, whence: int = os.SEEK_SET) -> None:\r\n        assert whence in (os.SEEK_SET, os.SEEK_CUR, os.SEEK_END)\r\n        if whence == os.SEEK_SET:\r\n            self.pos = offset\r\n        elif whence == os.SEEK_CUR:\r\n            self.pos += offset\r\n        elif whence == os.SEEK_END:\r\n            self.pos = len(self.data) + offset\r\n        self.ensure(self.pos)\r\n    def tell(self) -> int:\r\n        return self.pos\r\n    def flush(self) -> None:\r\n        pass\r\n    # def readable(self) -> bool:\r\n    #     return True\r\n    # def writable(self) -> bool:\r\n    #     return True\r\n    # def seekable(self) -> bool:\r\n    #     return True\r\n\r\nfile = FileLikeObject()\r\nwritable_file = uproot.recreate(file)\r\nwritable_file[\"hist\"] = np.histogram(np.random.normal(0, 1, 1000), bins=100, range=(-5, 5))\r\n\r\nfile.tobytes()   # lots of bytes\r\n\r\nreadable_file = uproot.open(file)\r\nreadable_file[\"hist\"].to_hist()\r\n```\r\n\r\n![image](https://github.com/scikit-hep/uproot5/assets/1852447/29992132-4b7e-4802-bab5-68597d34fcdc)\r\n\r\nIt doesn't matter whether the `readable`, `writable`, `seekable` methods exist or not.",
  "created_at":"2023-11-16T19:27:49Z",
  "id":1815178081,
  "issue":1016,
  "node_id":"IC_kwDOD6Q_ss5sMWth",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T19:27:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Looking at the stack trace, it must be because pyhf was relying on passing a file-like object obj that satisfies\r\n\r\nI think this happened via the `tmp_path` fixture in `pytest` creating a `pathlib.Path` (-like?) object (#1029).",
  "created_at":"2023-11-16T19:30:09Z",
  "id":1815181326,
  "issue":1016,
  "node_id":"IC_kwDOD6Q_ss5sMXgO",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-16T19:30:55Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"I need to read all of my email before responding to anything...",
  "created_at":"2023-11-16T19:31:40Z",
  "id":1815183564,
  "issue":1016,
  "node_id":"IC_kwDOD6Q_ss5sMYDM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T19:31:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"(sorry I didn't realise there was activity here after closing the issue).\r\n\r\nThere is a check to check if the argument is a path or an object. It was doing the check poorly (checking if it's a string) and failed when a path object was being passed. Now it tests for object (with a new helper function), otherwise assumes path-like object (and transforms to string).\r\n\r\nhttps://github.com/scikit-hep/uproot5/pull/1031 is ready to be merged with the fix.",
  "created_at":"2023-11-16T19:49:42Z",
  "id":1815211471,
  "issue":1016,
  "node_id":"IC_kwDOD6Q_ss5sMe3P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T19:50:41Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"I'm willing to help you implement that. The first step is to get a file with a `std::list` in it, preferably a small file, a simple file, or a file for which the values in the list are known. The process of looking at the bytes and recognizing how a `std::list` is to be interpreted will be much easier if you can recognize the values in the list as bytes. Thus, it's easier if the values in the list are integers smaller than 256 or floating point numbers that are powers of 2\u2014those are much easier to recognize by eye.\r\n\r\nThe next step will likely be in uproot/interpretation/identify.py, to recognize `std::list` in the C++ type string. It would likely be very similar to `std::vector`, so probably just another stanza after this one:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/ccb56b25e44ac94e26da1cabcda172854118ce9e/src/uproot/interpretation/identify.py#L958-L971\r\n\r\nIn the above, you can see that we use `uproot.containers.AsVector` to represent the `std::vector` interpretation. This is in uproot/containers.py. This file defines both `AsVector` the interpretation of the _type_ and `STLVector` the instantiation of a _value_ if not Awkward Array (e.g. if read with `library=\"np\"`). Here is the definition of `AsVector`:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/ccb56b25e44ac94e26da1cabcda172854118ce9e/src/uproot/containers.py#L1009-L1209\r\n\r\nand here is `STLVector`:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/ccb56b25e44ac94e26da1cabcda172854118ce9e/src/uproot/containers.py#L1729-L1782\r\n\r\nThe `STLVector` is very straightforward; just a class with `__getitem__` and `__iter__` and such, so that it acts like a Sequence in Python. The `AsVector` is more complex because it's handling several cases:\r\n\r\n* Content as a value type, like `int32` or `float64`, versus content as a more complex kind of record.\r\n* Reading the data from an entry of a TTree versus reading the data as an object that has been saved directly in a TDirectory.\r\n* Filling an Awkward Array (only TTree with `library=\"ak\"` or `library=\"pd\"` through [awkward-pandas](https://github.com/intake/awkward-pandas)) versus filling a `STLVector`.\r\n* Using AwkwardForth if it is available (subset of Awkward case).\r\n* Reading memberwise or non-memberwise data (only non-memberwise has been implemented, but the other case needs to raise an error).\r\n\r\nThe first question I should have asked you is whether your `std::list` and `CalibEvent` are inside of a TTree or on their own in a TDirectory, since that cuts out some of the cases.\r\n\r\nEven if it is inside of a TTree, which has more subcases, there is a minimal implementation that you can do to avoid the complex cases:\r\n\r\n* Do implement the value type versus complex record, even if you only have one kind of data, because this switch doesn't add much complexity and it would be confusing to future users if it handles `std::list` for one type of content but not another.\r\n* The things you need to worry about for inside-of-TTree are a strict superset of outside-of-TTree, so if your data are in a TTree, we'll get the outside-of-TTree case for free.\r\n* Don't worry about special-casing for Awkward Arrays or AwkwardForth. AwkwardForth is especially complicated and is being deeply refactored right now (#943), so it would not pay to solve that problem in the `main` branch. You can do\r\n\r\n```python\r\nif forth_stash is not None:\r\n    context[\"cancel_forth\"] = True\r\n```\r\n\r\n* We have not been implementing memberwise deserialization anywhere, except in `std::map`, for which our only examples are memberwise (so for that one, we don't implement non-memberwise). You can check for memberwise (or non-memberwise, whichever your case isn't) and raise an error for the unhandled case.\r\n\r\nAs test-driven development, you can stub the `read_members` method of `AsList` with `cursor.debug(chunk)` ([docs](https://uproot.readthedocs.io/en/latest/uproot.source.cursor.Cursor.html#debug)) followed by an exception just to stop the program flow. The debugging output looks like\r\n\r\n```\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n123 123 123  63 140 204 205  64  12 204 205  64  83  51  51  64 140 204 205  64\r\n  {   {   {   ? --- --- ---   @ --- --- ---   @   S   3   3   @ --- --- ---   @\r\n                        1.1             2.2             3.3             4.4\r\n    --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n    176   0   0  64 211  51  51  64 246 102 102  65  12 204 205  65  30 102 102  66\r\n    --- --- ---   @ ---   3   3   @ ---   f   f   A --- --- ---   A ---   f   f   B\r\n            5.5             6.6             7.7             8.8             9.9\r\n    --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n    202   0   0  67  74   0   0  67 151 128   0 123 123\r\n    --- --- ---   C   J --- ---   C --- --- ---   {   {\r\n          101.0           202.0           303.0\r\n```\r\n\r\n(with some of the options turned on, `dtype=\">f4\"` and `offset=3`). The three rows that are always present are the `--+---+---+---+---` separators, the decimal-valued bytes, and the interpretation as printable characters. With a given `dtype`, the debugging output will also show you the values interpreted as a numeric type, but you have to get the `offset` correct for this to be useful. Since not all of the data belong to a given `dtype`, it's usually easier to put data in the file that correspond to easy-to-read bytes. For example, big-endian (ROOT is big-endian) int32 values look like this as bytes:\r\n\r\n```python\r\n>>> np.array([1, 2, 3, 4, 5], dtype=\">i4\").view(\"u1\")\r\narray([0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 5],\r\n      dtype=uint8)\r\n```\r\n\r\nThat's very readable, even if it's embedded among some headers and strings. (Strings are easy to identify from the character interpretation lines.) Keep in mind that you want the numbers you're using as anchors to be distinguishable from the surrounding headers, which often have a lot of zeros, so pick numbers that are not zero (or one). `123` is a great one to use; it's easy to pick out by eye and it's small enough to fit in one byte.\r\n\r\nAfter having said all of that, I _highly suspect_ that the byte-serialization of `std::list` will be just like that of `std::vector`. I _highly suspect_ that there will be a 6 byte header that you can ignore, but it will start with a decimal `64` (that's a high-bit flag in a 4-byte integer part of the 6 byte header), followed by a 4 byte \"number of items in the `std::list`,\" followed by that many data values.\r\n\r\nFor example:\r\n\r\n```\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64   0   0  22   0   3   0   0   0   5   0   0   0   1   0   0   0   2   0   0\r\n  @ --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   3   0   0   0   4   0   0   0   5\r\n--- --- --- --- --- --- --- --- --- ---\r\n```\r\n\r\nwhere the total number of bytes in the object (which you don't need) is 22, the `std::list` serialization version (that I just made up) is 3, there are 5 elements in the list, followed by the values for 1, 2, 3, 4, and 5.\r\n\r\n\r\nThat's a guess, but the reason I guessed that is because it's how `std::vector` is serialized, how `std::set` is serialized, how `std::map` would be serialized except that it's memberwise and the data come in key-value pairs, and it's how ROOT's `RVec` is serialized. I'd be surprised if they break pattern for `std::list`. (How these STL objects are implemented in memory in C++ doesn't matter for how they are serialized to disk.)\r\n\r\nGood luck, and I'm available for help if you have any questions!",
  "created_at":"2023-11-10T17:26:51Z",
  "id":1806137212,
  "issue":1017,
  "node_id":"IC_kwDOD6Q_ss5rp3d8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-10T17:26:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lobis this sniped me!\r\n\r\nI looked at the RFCs, and found [RFC9110](https://www.rfc-editor.org/rfc/rfc9110.html#section-5.6) which defines the semantics of HTTP. In that document, it appears that the grammar for a list is given as:\r\n```ABNF\r\n1#element => element *( OWS \",\" OWS element )\r\n```\r\n\r\nthus the spacing is arbitrary. It seems that the recipients are making too strong assumptions about the grammar if they can't parse this.\r\n\r\nOn a second read, it's not 100% clear to me whether this refers to the protocol or to the ABNF grammar used to define it, so let me check back once I'm back from running an errand.",
  "created_at":"2023-11-13T12:08:57Z",
  "id":1808039553,
  "issue":1018,
  "node_id":"IC_kwDOD6Q_ss5rxH6B",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2023-11-13T12:10:23Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, I took another look and `Range` header satisifies `ranges-specifier`\r\n```\r\nrange-resp = incl-range \"/\" ( complete-length / \"*\" )\r\nrange-set = range-spec *( OWS \",\" OWS range-spec )\r\nrange-spec = int-range / suffix-range / other-range\r\nrange-unit = token\r\nranges-specifier = range-unit \"=\" range-set\r\n```",
  "created_at":"2023-11-13T14:52:35Z",
  "id":1808312640,
  "issue":1018,
  "node_id":"IC_kwDOD6Q_ss5ryKlA",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2023-11-13T14:52:35Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"In conclusion:\r\n\r\nwhitespaces (on both sides of the comma) are supported and optional. The server should be able to process the requests with spaces but it isn't (server's fault). We can safely drop the spaces since they are optional and we would produce a smaller request in doing so, which be a good thing even if this issue didn't exist.",
  "created_at":"2023-11-13T15:42:43Z",
  "id":1808408873,
  "issue":1018,
  "node_id":"IC_kwDOD6Q_ss5ryiEp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-13T15:43:24Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"That same RFC defines `OWS` as optional whitespace, so the specification is explicitly allowing spaces before and after the commas in a ranges-specifier. HTTP servers that enable multi-part GET only when there are no spaces are just wrong\u2014it's a bug.\r\n\r\nHowever, those servers are out there and it costs us nothing to drop the space. (I don't know how we would report this, either.)",
  "created_at":"2023-11-13T15:43:47Z",
  "id":1808410814,
  "issue":1018,
  "node_id":"IC_kwDOD6Q_ss5ryii-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-13T15:43:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski and I discussed this the other day, and a concern that I had regarding making `fsspec` a required dependency is whether it's available on Emscripten. At one point, I didn't think that this was the case, and was therefore against this idea.\r\n\r\nHowever, I've revisited the question today, and it seems to be fine. I'm not sure whether the package changed, or my memory is failing. As long as we still support an fsspec-less file-reader, then it will be possible to have some kind of integration on emscripten. Perhaps down the road we can look into support for things like multipart which I think will fail trying to use the built-in HTTP packages.",
  "created_at":"2023-11-14T16:49:26Z",
  "id":1810678333,
  "issue":1021,
  "node_id":"IC_kwDOD6Q_ss5r7MI9",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2023-11-14T16:49:26Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"That's great to hear. fsspec _should_ be okay on Emscripten because it's just doing pure Python dispatching things in itself; the networking functionality are all in optional dependencies. The aiohttp package (optional dependency) might not work, but that's a secondary question. It wouldn't be strictly required by Uproot the way that fsspec would.",
  "created_at":"2023-11-14T16:58:59Z",
  "id":1810695163,
  "issue":1021,
  "node_id":"IC_kwDOD6Q_ss5r7QP7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-14T16:58:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @jpivarski and I discussed this the other day, and a concern that I had regarding making `fsspec` a required dependency is whether it's available on Emscripten. At one point, I didn't think that this was the case, and was therefore against this idea.\r\n>\r\n> However, I've revisited the question today, and it seems to be fine. I'm not sure whether the package changed, or my memory is failing. As long as we still support an fsspec-less file-reader, then it will be possible to have some kind of integration on emscripten. Perhaps down the road we can look into support for things like multipart which I think will fail trying to use the built-in HTTP packages.\r\n\r\n\r\nYes, I think it wouldn't be a problem. I don't know much about the topic but this is how I tested it:\r\n\r\nhttps://pyodide.org/en/stable/console.html\r\n\r\n```\r\nimport micropip\r\nawait micropip.install(\"fsspec\")\r\nimport fsspec\r\nimport fsspec.asyn # I thought this would make it crash but it doesn't\r\nfsspec.asyn.get_loop() # this causes an exception as expected since it cannot start a new thread, some sources won't work (but it's expected)\r\n```\r\n\r\nRegarding the http source in the browser, we should be able to use fsspec (https://github.com/fsspec/filesystem_spec/pull/1177). It wouldn't support multipart but it should work. We can always implement this feature in fsspec if we deem it necessary.\r\n\r\nI am willing to work on this since but I may need to ask you a few questions @agoose77 since looks like installing `awkward` is not straightforward in pyodide (and awkward is a strict dependency of uproot).\r\n",
  "created_at":"2023-11-14T17:03:31Z",
  "id":1810702917,
  "issue":1021,
  "node_id":"IC_kwDOD6Q_ss5r7SJF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-14T22:02:27Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Do you have to `await micropip.install(\"fsspec\")`?\r\n\r\nI just tried installing Awkward in this terminal (it should work), and I found that the latest `awkward-cpp` is just a bit old.\r\n\r\n```\r\n>>> micropip.list()\r\nName               | Version | Source\r\n------------------ | ------- | -------\r\nawkward-cpp        | 22      | pyodide\r\n```\r\n\r\n([awkward 2.3.3](https://github.com/scikit-hep/awkward/blob/v2.3.3/awkward-cpp/pyproject.toml) was bound to awkward-cpp 22.)",
  "created_at":"2023-11-14T17:54:03Z",
  "id":1810827049,
  "issue":1021,
  "node_id":"IC_kwDOD6Q_ss5r7wcp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-14T17:54:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, nevermind: this is going into the `main-fsspec` branch, which isn't what anyone would get from a naive installation.\r\n\r\nSo yes, please go ahead and merge into the `main-fsspec` branch!",
  "created_at":"2023-11-14T17:56:48Z",
  "id":1810831079,
  "issue":1021,
  "node_id":"IC_kwDOD6Q_ss5r7xbn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-14T17:56:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Do you have to `await micropip.install(\"fsspec\")`?\r\n\r\nYes, sorry, I already updated the comment.\r\n\r\n",
  "created_at":"2023-11-14T22:03:02Z",
  "id":1811433914,
  "issue":1021,
  "node_id":"IC_kwDOD6Q_ss5r-Em6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-14T22:03:02Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I'm in favor of this and the implementation as represented by this PR. Maybe we should wait until the Awkward installation is fixed, though. Awkward 2.5.0 will be ready soon.\r\n\r\nI don't follow. I don't think it's going to be possible to install awkward (and by extension uproot) using micropip because awkward does not provide a pure Python wheel, right? It's my understanding that this is because of the underlaying cpp code. However it's possible to install numpy (which I assume is similar to awkward in it's dependancy to cpp) so you are saying that in the next release of awkward it will be possible to install from micropip?",
  "created_at":"2023-11-14T22:08:40Z",
  "id":1811440463,
  "issue":1021,
  "node_id":"IC_kwDOD6Q_ss5r-GNP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-14T22:08:40Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I don't follow. I don't think it's going to be possible to install awkward (and by extension uproot) using micropip because awkward does not provide a pure Python wheel, right? It's my understanding that this is because of the underlaying cpp code. However it's possible to install numpy (which I assume is similar to awkward in it's dependancy to cpp) so you are saying that in the next release of awkward it will be possible to install from micropip?\r\n\r\n`awkward` is a pure-Python wheel, and `awkward-cpp` is built with the pyodide distribution (although its not up-to-date: I'd thought there was a PR bot there, but I was mistaken).\r\n\r\nIt's already possible to install the latest version of Awkward in a particular version of pyodide; we do this in our documentation such that the branch previews always have the custom (branch) version of Awkward installed in pyodide.",
  "created_at":"2023-11-14T22:14:03Z",
  "id":1811446752,
  "issue":1021,
  "node_id":"IC_kwDOD6Q_ss5r-Hvg",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-14T22:14:03Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Well, I guess this is into `main-fsspec`, so what is the path to merge `main-fsspec` into `main`?",
  "created_at":"2023-11-15T21:56:52Z",
  "id":1813325473,
  "issue":1023,
  "node_id":"IC_kwDOD6Q_ss5sFSah",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-15T21:56:52Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Well, I guess this is into `main-fsspec`, so what is the path to merge `main-fsspec` into `main`?\r\n\r\n- https://github.com/scikit-hep/uproot5/pull/1022\r\n\r\nIt'll be merged when 5.2.0 is released (decemeber?) but in the meantime we will publish rc from this branch",
  "created_at":"2023-11-15T22:00:48Z",
  "id":1813329815,
  "issue":1023,
  "node_id":"IC_kwDOD6Q_ss5sFTeX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-15T22:00:48Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"> I suppose we should ask @eduardo-rodrigues based on his input to #692 For me it seems to deserve a minor version bump, otherwise OK.\r\n\r\nHello. Sorry to come late to the party. Sounds good, @nsmith- what you propose anyway.",
  "created_at":"2023-11-16T09:47:34Z",
  "id":1814107257,
  "issue":1023,
  "node_id":"IC_kwDOD6Q_ss5sIRR5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T09:47:34Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Fixed in https://github.com/scikit-hep/uproot5/pull/1028 (will take some time to go into main).",
  "created_at":"2023-11-16T18:53:00Z",
  "id":1815042528,
  "issue":1025,
  "node_id":"IC_kwDOD6Q_ss5sL1ng",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T18:53:00Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lobis you can use the WASM artefact for awkward-cpp from our release workflow to test the CI: https://github.com/scikit-hep/awkward/actions/runs/6894740990\r\n\r\nThis won't last for ever (I suspect 30 days is the limit on our artefact caching atm).",
  "created_at":"2023-11-20T20:58:17Z",
  "id":1819793585,
  "issue":1026,
  "node_id":"IC_kwDOD6Q_ss5sd9ix",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-20T20:58:17Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Fixes https://github.com/scikit-hep/uproot5/issues/1038",
  "created_at":"2023-11-21T20:21:42Z",
  "id":1821622216,
  "issue":1028,
  "node_id":"IC_kwDOD6Q_ss5sk7_I",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-21T20:21:42Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the fast fix. :+1: Everything seems good with 81750f8d13495cb18828efee291480638c524ac1.",
  "created_at":"2023-11-16T21:21:17Z",
  "id":1815333563,
  "issue":1031,
  "node_id":"IC_kwDOD6Q_ss5sM8q7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T21:22:22Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"https://github.com/scikit-hep/uproot5/actions/runs/6883516363/job/18724245334#step:8:782\r\n\r\nIs this okay @nsmith- ? The test passes with the old xrootd handler but fails with the new one. I'm not quite sure what is going on. Is it okay to skip it, or is this a fundamental issue that needs to be fixed (probably not here though)?",
  "created_at":"2023-11-16T22:19:20Z",
  "id":1815405866,
  "issue":1032,
  "node_id":"IC_kwDOD6Q_ss5sNOUq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T22:19:20Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The pickle-ability of the sources is important. It looks like this is a deficiency of `fsspec-xrootd`. It appears that, although filesystem objects work (https://github.com/CoffeaTeam/fsspec-xrootd/blob/main/tests/test_basicio.py#L99-L108) the `File` objects do not.",
  "created_at":"2023-11-16T22:38:14Z",
  "id":1815426054,
  "issue":1032,
  "node_id":"IC_kwDOD6Q_ss5sNTQG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T22:38:14Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Do I recall correctly that holding an actual file handle is somewhat optional now? Perhaps the fsspec source should not bother with that?",
  "created_at":"2023-11-16T22:38:56Z",
  "id":1815426728,
  "issue":1032,
  "node_id":"IC_kwDOD6Q_ss5sNTao",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T22:38:56Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I'll merge this without waiting for approval given it's been implicitly approved in https://github.com/scikit-hep/uproot5/pull/1032#pullrequestreview-1735708927",
  "created_at":"2023-11-16T23:34:52Z",
  "id":1815497187,
  "issue":1033,
  "node_id":"IC_kwDOD6Q_ss5sNknj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-16T23:34:52Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Are you sure this covers all the same cases as tests/test_0325_fix_windows_file_uris.py?\r\n\r\nI also wanted to ask about the %-encoded uris. As of this PR the decoding uproot does is removed. I'm not sure I understand the motivation behind this (decoding conditionally if the `file://` scheme is present or not). I would be in favour of not doing the decoding and passing it as it stands to fsspec. Would there be any problem with this?",
  "created_at":"2023-11-20T18:08:07Z",
  "id":1819566115,
  "issue":1034,
  "node_id":"IC_kwDOD6Q_ss5sdGAj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-20T18:08:07Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"%-encoding is another thing that web browser location bars accept. I don't remember if that was the specific motivation here. It certainly would have come from a user request\u2014I wouldn't have thought to add something like that without someone asking for it. Do you see, tracing back from the point when it was added, which issue prompted it, if any?",
  "created_at":"2023-11-20T18:10:33Z",
  "id":1819569699,
  "issue":1034,
  "node_id":"IC_kwDOD6Q_ss5sdG4j",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-20T18:10:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> %-encoding is another thing that web browser location bars accept. I don't remember if that was the specific motivation here. It certainly would have come from a user request\u2014I wouldn't have thought to add something like that without someone asking for it. Do you see, tracing back from the point when it was added, which issue prompted it, if any?\r\n\r\nIt appears to be triggered by the same issue https://github.com/scikit-hep/uproot5/issues/325 which was resolved by https://github.com/scikit-hep/uproot5/pull/328. I appears @jpivarski added the support for %-encoded as a \"bonus\" to https://github.com/scikit-hep/uproot5/issues/325. I think it may come from https://stackoverflow.com/questions/5977576/is-there-a-convenient-way-to-map-a-file-uri-to-os-path# (linked in the issue post).",
  "created_at":"2023-11-20T19:27:47Z",
  "id":1819670789,
  "issue":1034,
  "node_id":"IC_kwDOD6Q_ss5sdfkF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-20T19:34:50Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski it appears that the previous windows path test cases are still covered by fsspec (I updated the tests to cover them in https://github.com/scikit-hep/uproot5/pull/1034/files#diff-67e65d3e7efb19f80946512ff5b8aedad9c5dcc887febe2e0968348cfe8f14a5R34-R50).\r\n\r\nThe only breaking change is that we no longer decode %-encoded urls. I think this change makes sense since we should not distinguish between urlpaths having `file://` or not (since this is handled by fsspec).",
  "created_at":"2023-11-20T20:19:32Z",
  "id":1819744271,
  "issue":1034,
  "node_id":"IC_kwDOD6Q_ss5sdxgP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-20T20:19:32Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"It seems reasonable to not decode %-encoded URLs. Go ahead with this change.",
  "created_at":"2023-11-20T20:23:48Z",
  "id":1819751418,
  "issue":1034,
  "node_id":"IC_kwDOD6Q_ss5sdzP6",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-20T20:23:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"https://github.com/scikit-hep/uproot5/issues/1040 may be related to this.",
  "created_at":"2023-11-22T01:39:00Z",
  "id":1821954372,
  "issue":1035,
  "node_id":"IC_kwDOD6Q_ss5smNFE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-22T01:39:00Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I added a fix but I'm not sure of the possible consequences of it.",
  "created_at":"2023-11-21T21:52:02Z",
  "id":1821756687,
  "issue":1036,
  "node_id":"IC_kwDOD6Q_ss5slc0P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-21T21:52:02Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"https://github.com/scikit-hep/uproot5/issues/1040 may be related (both use atlas file)",
  "created_at":"2023-11-22T01:39:43Z",
  "id":1821954889,
  "issue":1036,
  "node_id":"IC_kwDOD6Q_ss5smNNJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-22T01:39:43Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hello @matthewfeickert ,\r\n\r\nWe have been trying to simplify how paths are handled by uproot with the goal of delegating as much responsibility as possible to fsspec. We tried to maintain support for all previous usages but it appears we didn't cover all cases. **I'll add a test with this particular url to make sure it keeps working in the future**. (sorry for the troubles!)\r\n\r\nThis particular path you are posting will be correctly processed after [0ace10af972ba825d98c28fa7df97cd3ef0b480f wh](https://github.com/scikit-hep/uproot5/pull/1028). However this PR is not yet available in the `main` branch as it introduces some breaking changes. We will soon produce a `v5.2.0rc2` pre-release which will also include this PR. In the meantime you can also use the `main-fsspec` branch which should correctly process this url.\r\n\r\nThe new naming scheme is pretty simple:\r\n- Only files ending in exactly `.root` will support the `path:object` scheme (when you want to specify which object to read from the file url). Uproot is responsible for correctly splitting the object if present.\r\n- The file (without object) url is fed into fsspec which will attempt to resolve the path after applying protocol chaining. `::` and `://` are the special sequences that fsspec uses for protocol chaining. The resolution of this url is now a responsibility of fsspec or the particular package that implements the filesystem in question (such as `fsspec-xrootd`).\r\n\r\nIn this particular case it looks like there are two scheme (`://root`) sequences so I'm not 100% sure it would work (I'm not familiar with what an xrootd url can look like). In the case it doesn't work, the issue should be raised with https://github.com/CoffeaTeam/fsspec-xrootd. But please let us know in any case and I will raise the issue in case it doesn't work.\r\n\r\nIf it doesn't work you could also try to use the `handler` option as in: `uproot.open(urlpath, handler=uproot.source.xrootd.XRootDSource)` which should revert to the previous behaviour. (But this should work with the default handler, if it doesn't it's a bug).\r\n\r\n- https://github.com/scikit-hep/uproot5/pull/1039",
  "created_at":"2023-11-21T07:37:56Z",
  "id":1820382236,
  "issue":1038,
  "node_id":"IC_kwDOD6Q_ss5sgNQc",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-21T20:20:56Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks @lobis. Yeah, after installing from 0ace10af972ba825d98c28fa7df97cd3ef0b480f and installing `fsspec-xrootd` things indeed work. :+1: \r\n\r\n```console\r\n$ docker run --rm -ti sslhep/analysis-dask-base:latest /bin/bash\r\nConfigured GCC from: /opt/lcg/gcc/11.2.0-8a51a/x86_64-centos7/bin/gcc\r\nConfigured AnalysisBase from: /usr/AnalysisBase/24.2.26/InstallArea/x86_64-centos7-gcc11-opt\r\nConfigured PyColumnarPrototype from: /usr/tools/PyColumnarPrototypeDemo/1.0.0/InstallArea/x86_64-centos7-gcc11-opt\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > python -m pip --quiet uninstall --yes uproot\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > python -m pip --quiet install --upgrade git+https://github.com/scikit-hep/uproot5.git@0ace10af972ba825d98c28fa7df97cd3ef0b480f\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > python -m pip show uproot\r\nName: uproot\r\nVersion: 5.2.0rc1\r\nSummary: ROOT I/O in pure Python and NumPy.\r\nHome-page: \r\nAuthor: \r\nAuthor-email: Jim Pivarski <pivarski@princeton.edu>\r\nLicense: \r\nLocation: /venv/lib/python3.9/site-packages\r\nRequires: awkward, fsspec, numpy, packaging, typing-extensions\r\nRequired-by: coffea\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > python -m pip install --upgrade fsspec-xrootd\r\nCollecting fsspec-xrootd\r\n  Downloading fsspec_xrootd-0.2.2-py3-none-any.whl.metadata (4.1 kB)\r\nRequirement already satisfied: fsspec in /venv/lib/python3.9/site-packages (from fsspec-xrootd) (2023.10.0)\r\nDownloading fsspec_xrootd-0.2.2-py3-none-any.whl (11 kB)\r\nInstalling collected packages: fsspec-xrootd\r\nSuccessfully installed fsspec-xrootd-0.2.2\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > kinit feickert@CERN.CH\r\nPassword for feickert@CERN.CH: \r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > klist\r\nTicket cache: FILE:/tmp/krb5cc_1000\r\nDefault principal: feickert@CERN.CH\r\n\r\nValid starting     Expires            Service principal\r\n11/21/23 09:02:28  11/22/23 10:02:19  krbtgt/CERN.CH@CERN.CH\r\n\trenew until 11/26/23 09:02:19\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > vi test.py\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > python -i test.py \r\nuproot version: 5.2.0rc1\r\nXRootD Python bindings version: 5.4.3\r\n>>> file\r\n<ReadOnlyDirectory '/' at 0x7f3c1c2bba30>\r\n>>> len(file[\"CollectionTree\"].keys())\r\n941\r\n>>> \r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > \r\n```\r\n\r\nSo we'll be on the lookout for `v5.2.0rc2`!",
  "created_at":"2023-11-21T08:10:05Z",
  "id":1820422646,
  "issue":1038,
  "node_id":"IC_kwDOD6Q_ss5sgXH2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-21T08:10:05Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Out of curiosity, would `uproot.open({xrootd_uri: None})` also work pre-patch and/or is that not (no longer?) recommended? That previously was the generic workaround (see e.g. #669).",
  "created_at":"2023-11-21T10:38:38Z",
  "id":1820659071,
  "issue":1038,
  "node_id":"IC_kwDOD6Q_ss5shQ1_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-21T10:38:38Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Out of curiosity, would `uproot.open({xrootd_uri: None})` also work pre-patch and/or is that not (no longer?) recommended? That previously was the generic workaround (see e.g. #669).\n\nYes, this will also work and it's the most robust way to specify the object inside the file. In this case there are no restrictions that the file name needs to end in .root",
  "created_at":"2023-11-21T14:13:08Z",
  "id":1821002470,
  "issue":1038,
  "node_id":"IC_kwDOD6Q_ss5sikrm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-21T14:13:08Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"NONE",
  "body":"So about possible paths...\r\nIn ATLAS very often files end with \".root.x\" where x is an integer. \r\nPaths that use xcache can look like:\r\n* `root[s]://xcacheserver[:port]//root[s]://originserver:[port]/path/file`\r\n* `http[s]://xcacheserver[:port]//root[s]://originserver:[port]/path/file`\r\n* `root[s]://xcacheserver[:port]//http[s]://originserver:[port]/path/file`\r\n* `http[s]://xcacheserver[:port]//http[s]://originserver:[port]/path/file`\r\n\r\nBest,\r\nIlija\r\n ",
  "created_at":"2023-11-21T15:30:06Z",
  "id":1821150370,
  "issue":1038,
  "node_id":"IC_kwDOD6Q_ss5sjIyi",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-21T17:06:55Z",
  "user":"MDQ6VXNlcjE3MzY5ODQ="
 },
 {
  "author_association":"NONE",
  "body":"Another issue is opening more than one file:\r\n\r\n```python\r\nimport uproot\r\n\r\nxc='root://xcache.af.uchicago.edu:1094//'\r\nfname_data = xc+\"root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1\"\r\nfname_dat1 = xc+\"root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/6c/67/DAOD_PHYSLITE.34858087._000002.pool.root.1\"\r\n\r\ntree_data = uproot.iterate(\r\n    {fname_data: \"CollectionTree\"}, {fname_dat1: \"CollectionTree\"}\r\n)\r\nnext(tree_data)  # trigger error\r\n```\r\n\r\n```pytb\r\nValueError: cannot produce Awkward Arrays for interpretation AsObjects(Unknown_xAOD_3a3a_MissingETAssociationMap_5f_v1) because\r\n\r\n    xAOD::MissingETAssociationMap_v1\r\n\r\ninstead, try library=\"np\" rather than library=\"ak\" or globally set uproot.default_library\r\n\r\nin file root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1\r\nin object /CollectionTree;1:METAssoc_AnalysisMET\r\n```\r\n\r\nThe same happens even if I try to open single file using iterator...\r\n",
  "created_at":"2023-11-21T15:51:59Z",
  "id":1821190003,
  "issue":1038,
  "node_id":"IC_kwDOD6Q_ss5sjSdz",
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "total_count":1
  },
  "updated_at":"2023-11-21T17:20:45Z",
  "user":"MDQ6VXNlcjE3MzY5ODQ="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I will leave this issue open in case anyone runs into the same error. It will be fixed by https://github.com/scikit-hep/uproot5/pull/1022 which will be available in the next release (`5.2.0`). This issue will be automatically closed when this is merged into `main`.",
  "created_at":"2023-11-22T19:17:59Z",
  "id":1823362668,
  "issue":1038,
  "node_id":"IC_kwDOD6Q_ss5srk5s",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-22T19:18:28Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I'm not sure if we need to be using the xrootd_handler option here, but that seems like it shouldn't be needed for the single file case(?).\r\n\r\nthe `*_handler` options are under deprecation and will be removed in the 5.2.0 release. Now only `handler` should be used (can take the same values as the other `*_handlers` did previously).",
  "created_at":"2023-11-21T21:55:06Z",
  "id":1821760331,
  "issue":1040,
  "node_id":"IC_kwDOD6Q_ss5sldtL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-21T21:55:06Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@matthewfeickert I added some tests in https://github.com/scikit-hep/uproot5/pull/1039 that iterate over files/trees in xrootd files. They work fine so I think it's a problem of this particular file. I think I can test this but not over xrootd, but I think this is not xrootd related. Does this problem appear when accessing the file over http or local filesystem?",
  "created_at":"2023-11-21T23:11:43Z",
  "id":1821842404,
  "issue":1040,
  "node_id":"IC_kwDOD6Q_ss5slxvk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-21T23:11:43Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"> Does this problem appear when accessing the file over http or local filesystem?\r\n\r\nYes, the following\r\n\r\n```python\r\n# test.py\r\nfrom pathlib import Path\r\n\r\nimport uproot\r\n\r\nfile_path = Path().cwd() / \"DAOD_PHYSLITE.34858087._000001.pool.root.1\"\r\ntree_name = \"CollectionTree\"\r\nbranch_name = \"AnalysisTrigMatch_HLT_e20_lhvloose\"\r\n\r\nwith uproot.open(file_path) as read_file:\r\n    print(read_file[tree_name])\r\n    print(read_file[tree_name][branch_name])\r\n\r\n    _tree = read_file[tree_name]\r\n    tree_data = _tree.iterate([branch_name], step_size=100)\r\n    next(tree_data)  # trigger error\r\n\r\n```\r\n\r\nalso breaks for the local filesystem:\r\n\r\n```console\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > kinit feickert@CERN.CH\r\nPassword for feickert@CERN.CH: \r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > xrdcp -f root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1 .\r\n[252.2MB/252.2MB][100%][==================================================][3.455MB/s]\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > python test.py \r\n<TTree 'CollectionTree' (864 branches) at 0x7fef5a2c6d30>\r\n<TBranchElement 'AnalysisTrigMatch_HLT_e20_lhvloose' at 0x7fef59b458e0>\r\nTraceback (most recent call last):\r\n  File \"/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 2478, in _awkward_check\r\n    interpretation.awkward_form(self.file)\r\n  File \"/venv/lib/python3.9/site-packages/uproot/interpretation/objects.py\", line 111, in awkward_form\r\n    return self._model.awkward_form(self._branch.file, context)\r\n  File \"/venv/lib/python3.9/site-packages/uproot/model.py\", line 684, in awkward_form\r\n    raise uproot.interpretation.objects.CannotBeAwkward(\r\nuproot.interpretation.objects.CannotBeAwkward: DataVector<xAOD::TrigComposite_v1>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/analysis/test.py\", line 15, in <module>\r\n    next(tree_data)  # trigger error\r\n  File \"/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 1076, in iterate\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 3041, in _ranges_or_baskets_to_arrays\r\n    branchid_to_branch[cache_key]._awkward_check(interpretation)\r\n  File \"/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 2480, in _awkward_check\r\n    raise ValueError(\r\nValueError: cannot produce Awkward Arrays for interpretation AsObjects(Unknown_DataVector_3c_xAOD_3a3a_TrigComposite_5f_v1_3e_) because\r\n\r\n    DataVector<xAOD::TrigComposite_v1>\r\n\r\ninstead, try library=\"np\" rather than library=\"ak\" or globally set uproot.default_library\r\n\r\nin file /analysis/DAOD_PHYSLITE.34858087._000001.pool.root.1\r\nin object /CollectionTree;1:AnalysisTrigMatch_HLT_e20_lhvloose\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis >\r\n```\r\n\r\nso it seems that we've chosen just a tree and branch that can't uniformly use Awkward as the array library, but we have branches like `AnalysisElectronsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVALooseCone_pt1000` that work\r\n\r\n```python\r\n>>> import uproot\r\n>>> file = uproot.open(\"DAOD_PHYSLITE.34858087._000001.pool.root.1\")\r\n>>> tree = file[\"CollectionTree\"]\r\n>>> branch = tree[\"AnalysisTrigMatch_HLT_e20_lhvloose\"]  # requires numpy as array library\r\n>>> branch.array(library=\"np\")\r\narray([<Unknown DataVector<xAOD::TrigComposite_v1> at 0x7fe97b2f3550>,\r\n       <Unknown DataVector<xAOD::TrigComposite_v1> at 0x7fe97b2f35b0>,\r\n       <Unknown DataVector<xAOD::TrigComposite_v1> at 0x7fe97b2bd730>,\r\n       ...,\r\n       <Unknown DataVector<xAOD::TrigComposite_v1> at 0x7fe979d7ad90>,\r\n       <Unknown DataVector<xAOD::TrigComposite_v1> at 0x7fe979d7ae20>,\r\n       <Unknown DataVector<xAOD::TrigComposite_v1> at 0x7fe979d7aeb0>],\r\n      dtype=object)\r\n>>> branch = tree[\"AnalysisElectronsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVALooseCone_pt1000\"]  # works with awkward\r\n>>> branch.array()\r\n<Array [[], [], [], ..., [2.24e+04], []] type='29442 * var * float32'>\r\n>>> \r\n```\r\n\r\nso \r\n\r\n```python\r\nfrom pathlib import Path\r\n\r\nimport uproot\r\n\r\nfile_path = Path().cwd() / \"DAOD_PHYSLITE.34858087._000001.pool.root.1\"\r\ntree_name = \"CollectionTree\"\r\nbranch_name = \"AnalysisElectronsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVALooseCone_pt1000\"\r\n\r\nwith uproot.open(file_path) as read_file:\r\n    print(read_file[tree_name])\r\n    print(read_file[tree_name][branch_name])\r\n\r\n    _tree = read_file[tree_name]\r\n    tree_data = _tree.iterate([branch_name], step_size=100)\r\n    next(tree_data)  # trigger error\r\n\r\n```\r\n\r\nruns fine.\r\n\r\nSo, @ivukotic then as expected, looking back to https://github.com/usatlas/analysisbase-dask/issues/4#issuecomment-1811308328, the following works\r\n\r\n```python\r\nimport uproot\r\n\r\nxc = \"root://xcache.af.uchicago.edu:1094//\"\r\nfname_data = (\r\n    xc\r\n    + \"root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1\"\r\n)\r\nfname_dat1 = (\r\n    xc\r\n    + \"root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/6c/67/DAOD_PHYSLITE.34858087._000002.pool.root.1\"\r\n)\r\n\r\ntree_name = \"CollectionTree\"\r\nbranches = [\r\n    \"AnalysisElectronsAuxDyn.pt\",\r\n    \"AnalysisElectronsAuxDyn.eta\",\r\n    \"AnalysisElectronsAuxDyn.phi\",\r\n    \"AnalysisElectronsAuxDyn.charge\",\r\n]\r\n\r\ntree_data = uproot.iterate(\r\n    {fname_data: tree_name, fname_dat1: tree_name},\r\n    expressions=branches,\r\n    step_size=1000,\r\n)\r\nprint(next(tree_data))\r\n\r\n```\r\n\r\n```console\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > python -i test.py \r\n[{'AnalysisElectronsAuxDyn.pt': [], ...}, {...}, {...}, ..., {...}, {...}]\r\n>>> next(tree_data)\r\n<Array [{...}, {...}, {...}, ..., {...}, {...}] type='1000 * {\"AnalysisElec...'>\r\n>>> \r\n```\r\n\r\nGiven this I'm closing this Issue. Thanks @lobis. :+1: ",
  "created_at":"2023-11-22T06:16:27Z",
  "id":1822175748,
  "issue":1040,
  "node_id":"IC_kwDOD6Q_ss5snDIE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-22T06:31:18Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@agoose77 do you know any rule to enforce a single line jump before imports, to avoid https://github.com/scikit-hep/uproot5/pull/1042/files#diff-7bf9f23c1473db1a176a044ec01de0b429c4f8c6c3998aa7496f0d834a8a1b9cR3-R4?",
  "created_at":"2023-11-22T20:47:42Z",
  "id":1823483863,
  "issue":1042,
  "node_id":"IC_kwDOD6Q_ss5ssCfX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-22T20:47:42Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lobis I actually prefer that style. If you don't have strong feelings, I'd just leave it.\r\n\r\nI'm not aware of any rule / configuration option to enforce this, but it doesn't mean that one doesn't exist. I had another look, and couldn't see anything that stands out to me.",
  "created_at":"2023-11-22T23:26:34Z",
  "id":1823622808,
  "issue":1042,
  "node_id":"IC_kwDOD6Q_ss5sskaY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-22T23:26:34Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> @lobis I actually prefer that style. If you don't have strong feelings, I'd just leave it.\r\n> \r\n> I'm not aware of any rule / configuration option to enforce this, but it doesn't mean that one doesn't exist. I had another look, and couldn't see anything that stands out to me.\r\n\r\nSure, we can leave it as it is. I couldn't find anything either so it's probably something we shouldn't care about anyway.",
  "created_at":"2023-11-23T02:22:14Z",
  "id":1823748353,
  "issue":1042,
  "node_id":"IC_kwDOD6Q_ss5stDEB",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-11-23T02:22:14Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi,\r\n\r\nhere a small file created from within root via:\r\n\r\n```cpp\r\nstruct Foo { const std::string bar; };\r\nFoo f;\r\nTFile* outfile = TFile::Open(\"foobar.root\", \"recreate\");\r\nTTree* tree = new TTree(\"FooBar\", \"FooBar\");\r\ntree->Branch(\"Foo\", &f);\r\ntree->Fill();\r\noutfile->Write();\r\noutfile->Close();\r\n```\r\n\r\n[foobar.zip](https://github.com/scikit-hep/uproot5/files/13473045/foobar.zip)\r\n",
  "created_at":"2023-11-27T10:32:05Z",
  "id":1827565694,
  "issue":1043,
  "node_id":"IC_kwDOD6Q_ss5s7nB-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-27T10:32:05Z",
  "user":"MDQ6VXNlcjgwNTAyOTI="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @HaarigerHarald for code",
  "created_at":"2023-11-28T21:22:17Z",
  "id":1830768616,
  "issue":1043,
  "node_id":"IC_kwDOD6Q_ss5tH0_o",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-28T21:22:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/1049) to add @HaarigerHarald! :tada:",
  "created_at":"2023-11-28T21:22:26Z",
  "id":1830768851,
  "issue":1043,
  "node_id":"IC_kwDOD6Q_ss5tH1DT",
  "performed_via_github_app":"MDM6QXBwMjMxODY=",
  "reactions":{},
  "updated_at":"2023-11-28T21:22:26Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi @ekourlit ,\n\nThe latest version of uproot (5.1.2) is on conda-forge. It's just a noarch package, so it's hard to se amongst the no-longer needed arch packages. Prefix.dev is now my preferred browser for conda-forge:\n\nhttps://prefix.dev/channels/conda-forge/packages/uproot",
  "created_at":"2023-11-25T08:34:45Z",
  "id":1826254503,
  "issue":1044,
  "node_id":"IC_kwDOD6Q_ss5s2m6n",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-25T08:34:45Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"maybe I see what you say... The main issue is that when one executes the command of the instructions will get v4.3.7.\r\n`conda install -c conda-forge uproot`",
  "created_at":"2023-11-27T14:07:12Z",
  "id":1827900756,
  "issue":1044,
  "node_id":"IC_kwDOD6Q_ss5s841U",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-27T14:07:12Z",
  "user":"MDQ6VXNlcjE5NTU2OTM0"
 },
 {
  "author_association":"NONE",
  "body":"I get similar behaviour with pip as well\r\n```shell\r\n(uproot5) [ekourlit@lxplus738 ~]$ pip install uproot==5.1.2\r\nERROR: Could not find a version that satisfies the requirement uproot==5.1.2 (from versions: 1.0.0, 1.0.1, 1.2.0, 1.2.1, 1.3.0, 1.3.1, 1.4.2, 1.5.0, 1.5.3, 1.6.0, 1.6.1, 1.6.2, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.1.1, 2.1.2, 2.1.4, 2.1.5, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.4.0, 2.4.1, 2.5.0, 2.5.2, 2.5.3, 2.5.4, 2.5.5, 2.5.8, 2.5.9, 2.5.10, 2.5.11, 2.5.12, 2.5.13, 2.5.14, 2.5.15, 2.5.16, 2.5.17, 2.5.18, 2.5.20, 2.5.21, 2.5.22, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.6.6, 2.6.7, 2.6.8, 2.6.9, 2.6.10, 2.6.11, 2.6.12, 2.6.13, 2.6.14, 2.6.15, 2.6.16, 2.6.17, 2.6.18, 2.6.19, 2.7.0, 2.7.1, 2.7.3, 2.7.4, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.8.5, 2.8.6, 2.8.7, 2.8.8, 2.8.9, 2.8.10, 2.8.11, 2.8.12, 2.8.13, 2.8.14, 2.8.15, 2.8.16, 2.8.17, 2.8.18, 2.8.19, 2.8.20, 2.8.21, 2.8.23, 2.8.24, 2.8.25, 2.8.26, 2.8.27, 2.8.28, 2.8.29, 2.8.30, 2.8.31, 2.8.32, 2.8.33, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.9.4, 2.9.5, 2.9.6, 2.9.7, 2.9.8, 2.9.9, 2.9.10, 2.9.11, 3.0.0b1, 3.0.0b2, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.2.5, 3.2.6, 3.2.7, 3.2.8, 3.2.9, 3.2.10, 3.2.11, 3.2.12, 3.2.13, 3.2.14, 3.2.15, 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.3.5, 3.3.6, 3.4.0, 3.4.1, 3.4.2, 3.4.3, 3.4.4, 3.4.5, 3.4.6, 3.4.7, 3.4.8, 3.4.9, 3.4.10, 3.4.11, 3.4.12, 3.4.13, 3.4.14, 3.4.15, 3.4.16, 3.4.17, 3.4.18, 3.4.19, 3.4.21, 3.5.0rc1, 3.5.0rc2, 3.5.0, 3.5.1, 3.5.2, 3.6.0, 3.6.1, 3.6.2, 3.6.3, 3.6.4, 3.6.5, 3.7.0rc1, 3.7.0, 3.7.1, 3.7.2, 3.8.0, 3.8.1, 3.8.2, 3.9.0, 3.9.1, 3.9.2, 3.9.3, 3.10.0, 3.10.1, 3.10.2, 3.10.3, 3.10.4, 3.10.5, 3.10.6, 3.10.7, 3.10.8, 3.10.9, 3.10.10, 3.10.11, 3.10.12, 3.11.0, 3.11.1, 3.11.2, 3.11.3, 3.11.4, 3.11.5, 3.11.6, 3.11.7, 3.12.0, 3.13.0, 3.13.1, 4.0.0rc2, 4.0.0rc3, 4.0.0rc4, 4.0.0, 4.0.1rc1, 4.0.1rc2, 4.0.1rc3, 4.0.1, 4.0.2rc1, 4.0.2rc2, 4.0.2, 4.0.3, 4.0.4, 4.0.5, 4.0.6, 4.0.7, 4.0.8, 4.0.9, 4.0.10, 4.0.11, 4.1.0, 4.1.1, 4.1.2, 4.1.3, 4.1.4, 4.1.5, 4.1.6, 4.1.7, 4.1.8, 4.1.9, 4.2.0, 4.2.1, 4.2.2, 4.2.3, 4.2.4, 4.3.0, 4.3.1, 4.3.2, 4.3.3, 4.3.4, 4.3.5, 4.3.6, 4.3.7, 5.0.0rc1, 5.0.0rc2, 5.0.0rc3)\r\n```\r\nmaybe it's a problem of my setup?\r\n",
  "created_at":"2023-11-27T14:25:28Z",
  "id":1827937165,
  "issue":1044,
  "node_id":"IC_kwDOD6Q_ss5s9BuN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-27T14:25:28Z",
  "user":"MDQ6VXNlcjE5NTU2OTM0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"What Python version are you using? Is it 3.6 by any chance?",
  "created_at":"2023-11-27T14:27:07Z",
  "id":1827940489,
  "issue":1044,
  "node_id":"IC_kwDOD6Q_ss5s9CiJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-27T14:27:07Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"yes, 3.6.8",
  "created_at":"2023-11-27T14:27:49Z",
  "id":1827941881,
  "issue":1044,
  "node_id":"IC_kwDOD6Q_ss5s9C35",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-27T14:27:49Z",
  "user":"MDQ6VXNlcjE5NTU2OTM0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This version of Python is no longer supported by uproot (or indeed much of the scikit-hep / Scientific Python ecosystem). Is it possible for you to upgrade to at least 3.8?",
  "created_at":"2023-11-27T14:37:05Z",
  "id":1827959979,
  "issue":1044,
  "node_id":"IC_kwDOD6Q_ss5s9HSr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-27T14:37:05Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"yes! That worked. OK. I understand `pip` behaviour.\r\n\r\nI still don't understand `conda install -c conda-forge uproot` though which even with python 3.9 gives me uproot v4. Should the `conda install` not used anymore but rather `micromamba install`?\r\n",
  "created_at":"2023-11-27T14:42:03Z",
  "id":1827968717,
  "issue":1044,
  "node_id":"IC_kwDOD6Q_ss5s9JbN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-27T14:42:03Z",
  "user":"MDQ6VXNlcjE5NTU2OTM0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ekourlit it may be that you have other packages in your Conda environment that are holding `uproot` back. If you created a fresh environment with Python 3.9, I'd expect it to find the latest `uproot`. E.g., if you had `coffea`, which requires `awkward<2`, you'd only be able to get `uproot<5` (I think).",
  "created_at":"2023-11-27T16:12:58Z",
  "id":1828151450,
  "issue":1044,
  "node_id":"IC_kwDOD6Q_ss5s92Ca",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-27T16:14:43Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"We should allow the user to configure the kinds of exceptions that cause empty outputs for a partition rather than hardcoding Exception. Probably defaulted to (OSError, FileNotFoundError)?",
  "created_at":"2023-11-29T21:30:02Z",
  "id":1832729428,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tPTtU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-29T21:30:02Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"How should we pass that in, as the `report=` argument or an additional argument?",
  "created_at":"2023-11-29T21:50:41Z",
  "id":1832756134,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tPaOm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-29T21:50:41Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"`uproot.iterate` has report=True/False to take care of toggling having a report or not, I'd have that here for consistency.\r\nI would add a separate argument to uproot.dask to specify the exceptions.\r\n\r\nI suppose there should be an argument for custom report fillers and such too?\r\n\r\n@jpivarski?",
  "created_at":"2023-11-29T21:59:29Z",
  "id":1832767900,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tPdGc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-29T21:59:44Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"That or we have `report:bool | ReportSpec = False` where ReportSpec is something that specifies the four steering arguments of `from_map`. If report is a bool and True we use a well-informed default ReportSpec?",
  "created_at":"2023-11-29T22:02:59Z",
  "id":1832772461,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tPeNt",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-29T22:03:21Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"`report: bool` or `report: bool | ReportSpec` is a good idea (the latter can be a later extension).\r\n\r\nIt's also worthwhile to get the report on the same side\u2014left or right\u2014as it is in [uproot.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html):\r\n\r\n> **report** _(bool)_ \u2013 If True, this generator yields (arrays, [uproot.behaviors.TBranch.Report](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.Report.html)) pairs; if False, it only yields arrays. The report has data about the `TFile`, `TTree`, and global and local entry ranges.\r\n\r\nSo, report goes on the right if it's a 2-tuple.",
  "created_at":"2023-11-29T22:09:12Z",
  "id":1832780183,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tPgGX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-29T22:09:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"We should have `uproot.dask` error if you ask for the report when not in dask-awkward mode.",
  "created_at":"2023-11-30T19:30:03Z",
  "id":1834420953,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tVwrZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-11-30T19:30:03Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Giving this a try and right now I get (this is using dak on the failure-report branch rebased to main, this branch uproot):\r\n```\r\n>>> events, report = uproot.dask(\"tests/samples/nano_dy.root:Events\", report=True)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/_dask.py\", line 249, in dask\r\n    return _get_dak_array(\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/_dask.py\", line 1323, in _get_dak_array\r\n    return dask_awkward.from_map(\r\n  File \"/Users/lgray/coffea-dev/dask-awkward/src/dask_awkward/lib/io/io.py\", line 775, in from_map\r\n    raise ValueError(\"io_func must implement mock_empty method.\")\r\nValueError: io_func must implement mock_empty method.\r\n```",
  "created_at":"2023-12-01T00:43:18Z",
  "id":1835152414,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tYjQe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-01T00:43:18Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah yes the function classes need a mock_empty along with mock. I'll tinker with it tomorrow",
  "created_at":"2023-12-01T04:23:54Z",
  "id":1835435626,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tZoZq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-01T04:23:54Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Tried out the latest branch with the following code:\r\n```python3\r\nfrom coffea.nanoevents import NanoEventsFactory, NanoAODSchema\r\n#from distributed import Client                                                                                                                                                                                                                      \r\nimport dask\r\n#import dask_awkward as dak                                                                                                                                                                                                                          \r\n\r\n\r\nif __name__ == \"__main__\":\r\n    #client = Client()                                                                                                                                                                                                                               \r\n\r\n\r\n    #dask.config.set({\"awkward.optimization.enabled\": True, \"awkward.raise-failed-meta\": True, \"awkward.optimization.on-fail\": \"raise\"})                                                                                                             \r\n    events, report = NanoEventsFactory.from_root(\r\n        [\"tests/samples/nano_dy.root:Events\", \"/not/actually/a/root/file.root:Events\"],\r\n        metadata={\"dataset\": \"nano_dy\"},\r\n        schemaclass=NanoAODSchema,\r\n        uproot_options={\"report\": True}\r\n    ).events()\r\n\r\n    pt, creport = dask.compute(events.Muon.pt, report)\r\n```\r\n\r\nthe result was:\r\n```\r\n(coffea-dev) lgray@Lindseys-MacBook-Pro coffea % python -i report_play.py\r\n/Users/lgray/coffea-dev/coffea/src/coffea/nanoevents/schemas/nanoaod.py:243: RuntimeWarning: Missing cross-reference index for FatJet_genJetAK8Idx => GenJetAK8\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 114, in _open\r\n    self._file = numpy.memmap(self._file_path, dtype=self._dtype, mode=\"r\")\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/numpy/core/memmap.py\", line 228, in __new__\r\n    f_ctx = open(os_fspath(filename), ('r' if mode == 'c' else mode)+'b')\r\nFileNotFoundError: [Errno 2] No such file or directory: '/not/actually/a/root/file.root'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 38, in __init__\r\n    self._file = open(self._file_path, \"rb\")\r\nFileNotFoundError: [Errno 2] No such file or directory: '/not/actually/a/root/file.root'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/lgray/coffea-dev/dask-awkward/src/dask_awkward/lib/io/io.py\", line 586, in __call__\r\n    result = self.fn(*args, **kwargs)\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/_dask.py\", line 1086, in __call__\r\n    ttree = uproot._util.regularize_object_path(\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/_util.py\", line 1153, in regularize_object_path\r\n    file = ReadOnlyFile(\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/reading.py\", line 588, in __init__\r\n    self._source = Source(file_path, **self._options)\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 110, in __init__\r\n    self._open()\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 120, in _open\r\n    self._fallback = uproot.source.file.MultithreadedFileSource(\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 253, in __init__\r\n    self._open()\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 258, in _open\r\n    [FileResource(self._file_path) for x in range(self._num_workers)]\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 258, in <listcomp>\r\n    [FileResource(self._file_path) for x in range(self._num_workers)]\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 40, in __init__\r\n    raise uproot._util._file_not_found(file_path) from err\r\nFileNotFoundError: file not found\r\n\r\n    '/not/actually/a/root/file.root'\r\n\r\nFiles may be specified as:\r\n   * str/bytes: relative or absolute filesystem path or URL, without any colons\r\n         other than Windows drive letter or URL schema.\r\n         Examples: \"rel/file.root\", \"C:\\abs\\file.root\", \"http://where/what.root\"\r\n   * str/bytes: same with an object-within-ROOT path, separated by a colon.\r\n         Example: \"rel/file.root:tdirectory/ttree\"\r\n   * pathlib.Path: always interpreted as a filesystem path or URL only (no\r\n         object-within-ROOT path), regardless of whether there are any colons.\r\n         Examples: Path(\"rel:/file.root\"), Path(\"/abs/path:stuff.root\")\r\n\r\nFunctions that accept many files (uproot.iterate, etc.) also allow:\r\n   * glob syntax in str/bytes and pathlib.Path.\r\n         Examples: Path(\"rel/*.root\"), \"/abs/*.root:tdirectory/ttree\"\r\n   * dict: keys are filesystem paths, values are objects-within-ROOT paths.\r\n         Example: {\"/data_v1/*.root\": \"ttree_v1\", \"/data_v2/*.root\": \"ttree_v2\"}\r\n   * already-open TTree objects.\r\n   * iterables of the above.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"report_play.py\", line 19, in <module>\r\n    pt, creport = dask.compute(events.Muon.pt, report)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/base.py\", line 599, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/threaded.py\", line 89, in get\r\n    results = get_async(\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/local.py\", line 511, in get_async\r\n    raise_exception(exc, tb)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/local.py\", line 319, in reraise\r\n    raise exc\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/local.py\", line 224, in execute_task\r\n    result = _execute_task(task, data)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/core.py\", line 119, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/optimization.py\", line 990, in __call__\r\n    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/core.py\", line 149, in get\r\n    result = _execute_task(task, cache)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/core.py\", line 119, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/lgray/coffea-dev/dask-awkward/src/dask_awkward/lib/io/io.py\", line 589, in __call__\r\n    result = self.fn.mock_empty(self.backend)\r\nTypeError: mock_empty() takes 1 positional argument but 2 were given\r\n```",
  "created_at":"2023-12-01T19:14:40Z",
  "id":1836641099,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5teOtL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-01T19:14:40Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Using uproot alone:\r\n\r\n```python3\r\nimport uproot\r\nimport dask\r\n\r\nif __name__ == \"__main__\":\r\n    events, report = uproot.dask(\r\n        [\"tests/samples/nano_dy.root:Events\", \"/not/actually/a/root/file.root:Events\"],\r\n        report=True,\r\n        open_files=False,\r\n    )\r\n\r\n    pt, creport = dask.compute(events.Muon_pt, report)\r\n```\r\nyields the exact same error.",
  "created_at":"2023-12-01T19:17:05Z",
  "id":1836643790,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tePXO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-01T19:17:05Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK `mock_empty` just needs to accept `backend` to fix that.\r\n\r\nAfter doing that I'm able to get:\r\n```\r\n>>> creport.to_list()\r\n[{'args': [], 'kwargs': [], 'exception': '', 'message': ''}, {'args': [\"('/not/actually/a/root/file.root', 'Events', 0, 1, False)\"], 'kwargs': [], 'exception': 'FileNotFoundError', 'message': 'file not found\\n\\n    \\'/not/actually/a/root/file.root\\'\\n\\nFiles may be specified as:\\n   * str/bytes: relative or absolute filesystem path or URL, without any colons\\n         other than Windows drive letter or URL schema.\\n         Examples: \"rel/file.root\", \"C:\\\\abs\\\\file.root\", \"http://where/what.root\"\\n   * str/bytes: same with an object-within-ROOT path, separated by a colon.\\n         Example: \"rel/file.root:tdirectory/ttree\"\\n   * pathlib.Path: always interpreted as a filesystem path or URL only (no\\n         object-within-ROOT path), regardless of whether there are any colons.\\n         Examples: Path(\"rel:/file.root\"), Path(\"/abs/path:stuff.root\")\\n\\nFunctions that accept many files (uproot.iterate, etc.) also allow:\\n   * glob syntax in str/bytes and pathlib.Path.\\n         Examples: Path(\"rel/*.root\"), \"/abs/*.root:tdirectory/ttree\"\\n   * dict: keys are filesystem paths, values are objects-within-ROOT paths.\\n         Example: {\"/data_v1/*.root\": \"ttree_v1\", \"/data_v2/*.root\": \"ttree_v2\"}\\n   * already-open TTree objects.\\n   * iterables of the above.\\n'}]\r\n```\r\n\r\ntadaa!",
  "created_at":"2023-12-01T20:40:05Z",
  "id":1836747443,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5teoqz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-01T20:40:05Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"So I can see the utility already of ReportSpec.\r\n\r\nI think it makes sense that the default report for uproot doesn't fill any information for successful file opening/reading, and in fact we should fill with `None` rather than an empty report so it is clear to the user there was literally no problem in readiing.\r\n\r\nHowever, for debugging system-level performance it would be useful to fill every report, regardless of success or failure, with step info (args I guess), bytes read, time taken to read. Adding the exception information when it happens.\r\n\r\nSo I think we should come out of the box with ReportSpec so that folks can dive into debugging/optimizing their clusters if they find performance issues. Possibly even providing a report spec that does what I describe above so it is at hand?\r\n\r\n@douglasdavis @jpivarski ",
  "created_at":"2023-12-01T20:44:56Z",
  "id":1836752578,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tep7C",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-01T20:47:28Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"whoops on missing the backend argument! Fixed in the PR now.\r\n\r\nOn the topic of a report spec: I still need to think about how to make it possible to get a fully customize-able report record that isn't limited in the information it can use to build itself out. We'll likely need users to implement this stuff in their on `__call__` implementation instead of us providing a more blackbox-ish wrapper. (which is exists as things stand -- in this PR and the dask-awkward PR the only information the report can possible have access to are the *args and **kwargs passed to the read function, and the exception that was raised). I've been tinkering with moving the code that accomplishes this into a more OOP approach but I'm still ironing it out.",
  "created_at":"2023-12-01T21:59:21Z",
  "id":1836834335,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5te94f",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-01T21:59:21Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"did a test using the full nanoevents machinery and it looks like we're good! report works as expected, no strange side effects.",
  "created_at":"2023-12-02T18:08:56Z",
  "id":1837218779,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5tgbvb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-02T18:08:56Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@douglasdavis I think the direction you're going is fine in this case, the \"user\" I believe can be uproot since we have to implement our own thing anyway. We  can then present a more black-boxy interface to folks using the library. I think that's OK especially since it's a not-typically user-facing part of dask-awkward.",
  "created_at":"2023-12-03T17:30:13Z",
  "id":1837546020,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5throk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-03T17:30:13Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Closed in favor of #1058.",
  "created_at":"2023-12-08T15:26:51Z",
  "id":1847372854,
  "issue":1050,
  "node_id":"IC_kwDOD6Q_ss5uHKw2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T15:26:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This was fixed by https://github.com/fsspec/filesystem_spec/releases/tag/2023.12.2.",
  "created_at":"2023-12-13T18:39:23Z",
  "id":1854513486,
  "issue":1054,
  "node_id":"IC_kwDOD6Q_ss5uiaFO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-13T18:39:23Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The fsspec problem will be fixed by:\r\n\r\n- https://github.com/fsspec/filesystem_spec/pull/1452\r\n",
  "created_at":"2023-12-07T15:56:27Z",
  "id":1845601613,
  "issue":1055,
  "node_id":"IC_kwDOD6Q_ss5uAaVN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T17:44:07Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"\r\n> I think this is a good PR, but we want to be sure we can meet the release deadline and we can't include a failing test. If we're still waiting on fsspec by the end of the week, we'll need a work-around that works for both old and new fsspec.\r\n\r\nThe `main-fsspec` ci is also failing because of this (because of new defaults) so I guess there is no harm in merging it.\r\n\r\n> Will the new fsspec be out before we need to release Uproot 5.2.0?\r\n\r\nThe fix has already been submitted and should be merged soon. @martindurant would it be possible to have a `2023.12.2` fsspec release by the end of this week?\r\n",
  "created_at":"2023-12-07T17:50:00Z",
  "id":1845833778,
  "issue":1055,
  "node_id":"IC_kwDOD6Q_ss5uBTAy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T17:50:00Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Should be fixed now in https://pypi.org/project/fsspec/2023.12.2/ @lobis ",
  "created_at":"2023-12-11T21:40:40Z",
  "id":1850933043,
  "issue":1055,
  "node_id":"IC_kwDOD6Q_ss5uUv8z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-11T21:40:40Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"In this PR the failure and success reports are completely defined and used here in uproot, specifically in the `__call__` implementations. There is no default provided by dask-awkward. dask-awkward just needs to know if the input layer is going to return two arrays- that is controlled with the `return_report` attribute on the `io_func` that gets passed to `from_map`. (on the dask-awkward side if `from_map` detects that the `io_func` it receives has `io_func.return_report == True`, it builds the layer as needed\r\n\r\nAs a POC I've made the fields\r\n- duration\r\n- exception\r\n- error message\r\n- args\r\n- kwargs",
  "created_at":"2023-12-07T18:15:31Z",
  "id":1845875394,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uBdLC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T18:20:42Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Here's a quick script that I've been using to check things!\r\n\r\n```python3\r\nfrom coffea.nanoevents import NanoEventsFactory, NanoAODSchema\r\n#from distributed import Client\r\nimport dask\r\n#import dask_awkward as dak\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    #client = Client()\r\n\r\n\r\n    #dask.config.set({\"awkward.optimization.enabled\": True, \"awkward.raise-failed-meta\": True, \"awkward.optimization.on-fail\": \"raise\"})\r\n    events, report = NanoEventsFactory.from_root(\r\n        [\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root:Events\", \"/not/actually/a/root/file.root:Events\"],\r\n        metadata={\"dataset\": \"nano_dy\"},\r\n        schemaclass=NanoAODSchema,\r\n        uproot_options={\"allow_read_errors_with_report\": True}\r\n    ).events()\r\n\r\n    pt, creport = dask.compute(events.Muon.pt, report)\r\n```\r\n(edited to follow @jpivarski's review comments)",
  "created_at":"2023-12-07T18:52:33Z",
  "id":1845927386,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uBp3a",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T20:21:48Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Though with the latest report and the above script I get:\r\n```\r\n(coffea-dev) lgray@Lindseys-MacBook-Pro coffea % python -i report_play.py\r\n/Users/lgray/coffea-dev/coffea/src/coffea/nanoevents/schemas/nanoaod.py:243: RuntimeWarning: Missing cross-reference index for FatJet_genJetAK8Idx => GenJetAK8\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"report_play.py\", line 12, in <module>\r\n    events, report = NanoEventsFactory.from_root(\r\n  File \"/Users/lgray/coffea-dev/dask-awkward/src/dask_awkward/lib/core.py\", line 1006, in __iter__\r\n    raise NotImplementedError(\r\nNotImplementedError: Iteration over a Dask Awkward collection is not supported.\r\nA suggested alternative: define a function which iterates over\r\nan awkward array and use that function with map_partitions.\r\n```\r\n\r\nWhereas the last report version completed just fine.",
  "created_at":"2023-12-07T18:57:58Z",
  "id":1845934613,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uBroV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T18:57:58Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The latest whoops commits gives me:\r\n```\r\n(coffea-dev) lgray@Lindseys-MacBook-Pro coffea % python -i report_play.py                                 \r\n/Users/lgray/coffea-dev/coffea/src/coffea/nanoevents/schemas/nanoaod.py:243: RuntimeWarning: Missing cross-reference index for FatJet_genJetAK8Idx => GenJetAK8\r\n  warnings.warn(\r\nTraceback (most recent call last):\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 114, in _open\r\n    self._file = numpy.memmap(self._file_path, dtype=self._dtype, mode=\"r\")\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/numpy/core/memmap.py\", line 228, in __new__\r\n    f_ctx = open(os_fspath(filename), ('r' if mode == 'c' else mode)+'b')\r\nFileNotFoundError: [Errno 2] No such file or directory: '/not/actually/a/root/file.root'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 38, in __init__\r\n    self._file = open(self._file_path, \"rb\")\r\nFileNotFoundError: [Errno 2] No such file or directory: '/not/actually/a/root/file.root'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"report_play.py\", line 19, in <module>\r\n    pt, creport = dask.compute(events.Muon.pt, report)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/base.py\", line 599, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/threaded.py\", line 89, in get\r\n    results = get_async(\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/local.py\", line 511, in get_async\r\n    raise_exception(exc, tb)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/local.py\", line 319, in reraise\r\n    raise exc\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/local.py\", line 224, in execute_task\r\n    result = _execute_task(task, data)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/core.py\", line 119, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/optimization.py\", line 990, in __call__\r\n    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/core.py\", line 149, in get\r\n    result = _execute_task(task, cache)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-dev/lib/python3.8/site-packages/dask/core.py\", line 119, in _execute_task\r\n    return func(*(_execute_task(a, cache) for a in args))\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/_dask.py\", line 1173, in __call__\r\n    ttree = uproot._util.regularize_object_path(\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/_util.py\", line 1153, in regularize_object_path\r\n    file = ReadOnlyFile(\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/reading.py\", line 588, in __init__\r\n    self._source = Source(file_path, **self._options)\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 110, in __init__\r\n    self._open()\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 120, in _open\r\n    self._fallback = uproot.source.file.MultithreadedFileSource(\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 253, in __init__\r\n    self._open()\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 258, in _open\r\n    [FileResource(self._file_path) for x in range(self._num_workers)]\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 258, in <listcomp>\r\n    [FileResource(self._file_path) for x in range(self._num_workers)]\r\n  File \"/Users/lgray/coffea-dev/uproot5/src/uproot/source/file.py\", line 40, in __init__\r\n    raise uproot._util._file_not_found(file_path) from err\r\nFileNotFoundError: file not found\r\n\r\n    '/not/actually/a/root/file.root'\r\n\r\nFiles may be specified as:\r\n   * str/bytes: relative or absolute filesystem path or URL, without any colons\r\n         other than Windows drive letter or URL schema.\r\n         Examples: \"rel/file.root\", \"C:\\abs\\file.root\", \"http://where/what.root\"\r\n   * str/bytes: same with an object-within-ROOT path, separated by a colon.\r\n         Example: \"rel/file.root:tdirectory/ttree\"\r\n   * pathlib.Path: always interpreted as a filesystem path or URL only (no\r\n         object-within-ROOT path), regardless of whether there are any colons.\r\n         Examples: Path(\"rel:/file.root\"), Path(\"/abs/path:stuff.root\")\r\n\r\nFunctions that accept many files (uproot.iterate, etc.) also allow:\r\n   * glob syntax in str/bytes and pathlib.Path.\r\n         Examples: Path(\"rel/*.root\"), \"/abs/*.root:tdirectory/ttree\"\r\n   * dict: keys are filesystem paths, values are objects-within-ROOT paths.\r\n         Example: {\"/data_v1/*.root\": \"ttree_v1\", \"/data_v2/*.root\": \"ttree_v2\"}\r\n   * already-open TTree objects.\r\n   * iterables of the above.\r\n ```\r\n \r\n So somehow it's not catching the FileNotFound/OSError correctly?",
  "created_at":"2023-12-07T19:47:37Z",
  "id":1845999985,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uB7lx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T19:47:37Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think the exception is getting raised _before_ dask-awkward gets to the place where it allows exceptions (at the `__call__`) I think uproot is checking for a file existence before asking for any data (so before compute)?\r\n\r\nI'm still digging/working on this!\r\n",
  "created_at":"2023-12-07T19:50:32Z",
  "id":1846003968,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uB8kA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T19:50:32Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Some good news here though:\r\n```python\r\nIn [11]: import uproot\r\n    ...: import dask\r\n    ...: files = [\"/Users/ddavis/software/repos/coffea/tests/samples/nano_dy.root:Events\"]\r\n    ...: thing, report = uproot.dask(files, report=True)\r\n    ...: a, b = dask.compute(thing, report)\r\n    ...: \r\n    ...: \r\n\r\nIn [12]: a\r\nOut[12]: <Array [{run: 1, ...}, ..., {run: 1, ...}] type='40 * {run: uint32, luminos...'>\r\n\r\nIn [13]: b.tolist()\r\nOut[13]: \r\n[{'duration': -1701769030.9317346,\r\n  'args': [\"<TTree 'Events' (1499 branches) at 0x000176803c10>\", '0', '40'],\r\n  'kwargs': [],\r\n  'exception': None,\r\n  'message': None,\r\n  'fqdn': None,\r\n  'hostname': None}]\r\n```",
  "created_at":"2023-12-07T19:51:15Z",
  "id":1846004991,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uB8z_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T19:51:15Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I use open_files=False in uproot.dask so it shouldn't be trying to open anything!",
  "created_at":"2023-12-07T19:54:14Z",
  "id":1846011021,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uB-SN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T19:54:14Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah you're right. I didnt catch the whole traceback :) there is some uproot code getting used in `__call__` before the exception check, just need to reorder some things",
  "created_at":"2023-12-07T19:55:22Z",
  "id":1846013622,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uB-62",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T19:57:39Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"```python3\r\nimport uproot\r\nimport dask\r\n\r\nif __name__ == \"__main__\":\r\n    events, report = uproot.dask(\r\n        [\"tests/samples/nano_dy.root:Events\", \"/not/actually/a/root/file.root:Events\"],\r\n        allow_read_errors_with_report=True,\r\n        open_files=False,\r\n    )\r\n\r\n    pt, creport = dask.compute(events.Muon_pt, report)\r\n```\r\n\r\nHere's a non-coffea script. (edit for comments from @jpivarski's review)",
  "created_at":"2023-12-07T20:00:59Z",
  "id":1846026073,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uCB9Z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T20:21:00Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK just got the coffea-using script to work. ~Only other Q I had is above -- about how the duration should be implemented~ I see you answered it! GH is appearing flaky for me \ud83e\udd14 ",
  "created_at":"2023-12-07T20:08:56Z",
  "id":1846036409,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uCEe5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T20:11:43Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"commented in that review thread but putting it here:\r\n```python3\r\ncall_time = time.time_ns() # integer time since epoch, convertible to a datetime\r\nstart = time.monotonic() # since start of program for calibration purposes\r\ncall(stuff)\r\nstop = time.monotonic()\r\nreturn call_time, stop - start\r\n```",
  "created_at":"2023-12-07T20:11:40Z",
  "id":1846039389,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uCFNd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T20:11:40Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"and, more or less, we care about call time if it fails and duration if it succeeds!",
  "created_at":"2023-12-07T20:12:42Z",
  "id":1846041089,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uCFoB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T20:12:57Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"very nice:\r\n```\r\n(coffea-dev) lgray@Lindseys-MacBook-Pro coffea % python -i report_play.py                                 \r\n/Users/lgray/coffea-dev/coffea/src/coffea/nanoevents/schemas/nanoaod.py:243: RuntimeWarning: Missing cross-reference index for FatJet_genJetAK8Idx => GenJetAK8\r\n  warnings.warn(\r\n>>> import pprint\r\n>>> pprint.pprint(creport.to_list())\r\n[{'args': [\"'https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root'\",\r\n           \"'Events'\",\r\n           '0',\r\n           '1',\r\n           'False'],\r\n  'call_time': None,\r\n  'duration': 1.389443333,\r\n  'exception': None,\r\n  'fqdn': None,\r\n  'hostname': None,\r\n  'kwargs': [],\r\n  'message': None},\r\n {'args': [\"'/not/actually/a/root/file.root'\", \"'Events'\", '0', '1', 'False'],\r\n  'call_time': 1701980886668361000,\r\n  'duration': None,\r\n  'exception': 'FileNotFoundError',\r\n  'fqdn': '1.0.0.127.in-addr.arpa',\r\n  'hostname': 'Lindseys-MacBook-Pro.local',\r\n  'kwargs': [],\r\n  'message': 'file not found\\n'\r\n             '\\n'\r\n             \"    '/not/actually/a/root/file.root'\\n\"\r\n             '\\n'\r\n             'Files may be specified as:\\n'\r\n             '   * str/bytes: relative or absolute filesystem path or URL, '\r\n             'without any colons\\n'\r\n             '         other than Windows drive letter or URL schema.\\n'\r\n             '         Examples: \"rel/file.root\", \"C:\\\\abs\\\\file.root\", '\r\n             '\"http://where/what.root\"\\n'\r\n             '   * str/bytes: same with an object-within-ROOT path, separated '\r\n             'by a colon.\\n'\r\n             '         Example: \"rel/file.root:tdirectory/ttree\"\\n'\r\n             '   * pathlib.Path: always interpreted as a filesystem path or '\r\n             'URL only (no\\n'\r\n             '         object-within-ROOT path), regardless of whether there '\r\n             'are any colons.\\n'\r\n             '         Examples: Path(\"rel:/file.root\"), '\r\n             'Path(\"/abs/path:stuff.root\")\\n'\r\n             '\\n'\r\n             'Functions that accept many files (uproot.iterate, etc.) also '\r\n             'allow:\\n'\r\n             '   * glob syntax in str/bytes and pathlib.Path.\\n'\r\n             '         Examples: Path(\"rel/*.root\"), '\r\n             '\"/abs/*.root:tdirectory/ttree\"\\n'\r\n             '   * dict: keys are filesystem paths, values are '\r\n             'objects-within-ROOT paths.\\n'\r\n             '         Example: {\"/data_v1/*.root\": \"ttree_v1\", '\r\n             '\"/data_v2/*.root\": \"ttree_v2\"}\\n'\r\n             '   * already-open TTree objects.\\n'\r\n             '   * iterables of the above.\\n'}]\r\n>>> from datetime import datetime\r\n>>> datetime.utcfromtimestamp(creport.to_list()[1][\"call_time\"]/1e9).strftime('%Y-%m-%d %H:%M:%S')\r\n'2023-12-07 20:28:06'\r\n```",
  "created_at":"2023-12-07T20:31:49Z",
  "id":1846067369,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uCMCp",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2023-12-07T20:31:49Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK I think this is in a pretty good place now",
  "created_at":"2023-12-07T20:31:51Z",
  "id":1846067424,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uCMDg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T20:31:51Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yep - this seems to be in the right place!",
  "created_at":"2023-12-07T20:43:57Z",
  "id":1846085607,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uCQfn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T20:43:57Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Need to polish the PR in dask-awkward and that'll be in the next dask-awkward release",
  "created_at":"2023-12-07T20:57:21Z",
  "id":1846101714,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uCUbS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-07T20:57:21Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@btovar could you try this with taskvine to make sure the report works with very different backends?",
  "created_at":"2023-12-08T13:22:43Z",
  "id":1847159143,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uGWln",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T13:22:43Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@lgray will do!",
  "created_at":"2023-12-08T13:24:51Z",
  "id":1847161619,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uGXMT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T13:24:51Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Currently I'm getting:\r\n\r\n```\r\nNotImplementedError: Iteration over a Dask Awkward collection is not supported.\r\nA suggested alternative: define a function which iterates over\r\nan awkward array and use that function with map_partitions.\r\n```\r\n\r\nversions: awkward-2.5.1rc1 coffea-2023.12.0rc0 dask-awkward-2023.12.0\r\nuproot commit:  cbad36e1d433144a8918e5dc2317a88cc1598aaa\r\n\r\nI get that error even with the example above without coffea. Let me try with an env just with this uproot commit.\r\n\r\n```python\r\nimport uproot\r\nimport dask\r\n\r\nif __name__ == \"__main__\":\r\n    events, report = uproot.dask(\r\n        [\"tests/samples/nano_dy.root:Events\", \"/not/actually/a/root/file.root:Events\"],\r\n        report=True,\r\n        open_files=False,\r\n    )\r\n\r\n    pt, creport = dask.compute(events.Muon_pt, report)\r\n```",
  "created_at":"2023-12-08T18:41:16Z",
  "id":1847655922,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uIP3y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T18:47:31Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"You'll need to checkout the branch @ https://github.com/dask-contrib/dask-awkward/pull/433",
  "created_at":"2023-12-08T18:47:30Z",
  "id":1847662632,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uIRgo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T18:47:30Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'm still getting the same error from a clean env.  I'm checking out the given commits for uproot5 (cbad36e1d433144a8918e5dc2317a88cc1598aaa) and dask-awkward (d64f8a5aacf1f995c7aad5a380b9b7e406fd0194), and doing a `pip install -e .` in a clean env.   This is the output of `pip list`:\r\n\r\n```\r\nPackage            Version\r\n------------------ -----------------------\r\nawkward            2.5.1rc1\r\nawkward-cpp        26\r\nclick              8.1.7\r\ncloudpickle        3.0.0\r\ndask               2023.12.0\r\ndask-awkward       2023.12.1.dev4+gd64f8a5\r\nfsspec             2023.12.1\r\nimportlib-metadata 7.0.0\r\nlocket             1.0.0\r\nnumpy              1.26.2\r\npackaging          23.2\r\npartd              1.4.1\r\npip                23.3.1\r\nPyYAML             6.0.1\r\nsetuptools         68.2.2\r\ntoolz              0.12.0\r\ntyping_extensions  4.8.0\r\nuproot             5.1.2\r\nwheel              0.42.0\r\nzipp               3.17.0\r\n```",
  "created_at":"2023-12-08T19:27:09Z",
  "id":1847704797,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uIbzd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T19:27:09Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah, latest uproot commit requires\r\n\r\n```diff\r\n- report=True,\r\n+ allow_read_errors_with_report=True,\r\n```\r\nin `uproot.dask`.\r\n\r\nSo coffea would need `from_root(..., uproot_options={\"allow_read_errors_with_report\": True})`",
  "created_at":"2023-12-08T19:55:48Z",
  "id":1847767619,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uIrJD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T19:57:33Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yeah my bad for not updating the stuff in the comments above, have been traveling today - done now.",
  "created_at":"2023-12-08T20:59:31Z",
  "id":1847836640,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uI7_g",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-08T20:59:31Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for your help, it seems to be working now. I tried both with local and remote taskvine workers.",
  "created_at":"2023-12-09T18:16:42Z",
  "id":1848604969,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uL3kp",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "rocket":2,
   "total_count":3
  },
  "updated_at":"2023-12-09T18:16:42Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I've added a simple test and the upstream feature is now available in the latest version of dask-awkward (`2023.12.1`), so I don't plan to add anything unless something needs polishing or is missing upon review",
  "created_at":"2023-12-11T16:43:09Z",
  "id":1850456013,
  "issue":1058,
  "node_id":"IC_kwDOD6Q_ss5uS7fN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-11T16:43:09Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"fsspec-xrootd 0.2.3 that includes https://github.com/CoffeaTeam/fsspec-xrootd/pull/40 is now on pypi",
  "created_at":"2023-12-12T12:29:51Z",
  "id":1851944772,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5uYm9E",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-12-12T12:29:51Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"The Windows tests are failing because it's thinking that the drive letter is a URI scheme:\r\n\r\n```\r\n        if isinstance(files, str):\r\n            if parse_colon:\r\n                file_path, object_path = file_object_path_split(files)\r\n            else:\r\n                file_path, object_path = files, None\r\n    \r\n            parsed_url = urlparse(file_path)\r\n            scheme = parsed_url.scheme\r\n            if scheme in fsspec.available_protocols():\r\n                # user specified a protocol, so we use fsspec to expand the glob and return the full paths\r\n                file_names_full = [file.full_name for file in fsspec.open_files(files)]\r\n                # https://github.com/fsspec/filesystem_spec/issues/1459\r\n                # Not all protocols return the full_name attribute correctly (if they have url parameters)\r\n                for file_name_full in file_names_full:\r\n                    yield file_name_full, object_path, maybe_steps\r\n            elif scheme != \"\":\r\n                # user specified a protocol, but it's not supported by fsspec (e.g. user does not have s3fs installed)\r\n>               raise ValueError(\r\n                    f\"Protocol {scheme} is not supported by fsspec. Please install the corresponding package.\"\r\n                )\r\nE               ValueError: Protocol c is not supported by fsspec. Please install the corresponding package.\r\n\r\nHasBranches = <class 'uproot.behaviors.TBranch.HasBranches'>\r\ncounter    = [1]\r\nfile_path  = 'C:\\\\Users\\\\runneradmin\\\\.local\\\\skhepdata\\\\uproot-Zmumu.root'\r\nfiles      = 'C:\\\\Users\\\\runneradmin\\\\.local\\\\skhepdata\\\\uproot-Zmumu.root:events'\r\nfiles2     = 'C:\\\\Users\\\\runneradmin\\\\.local\\\\skhepdata\\\\uproot-Zmumu.root:events'\r\nmaybe_steps = None\r\nobject_path = 'events'\r\nparse_colon = True\r\nparsed_url = ParseResult(scheme='c', netloc='', path='\\\\Users\\\\runneradmin\\\\.local\\\\skhepdata\\\\uproot-Zmumu.root', params='', query='', fragment='')\r\nscheme     = 'c'\r\nsteps_allowed = True\r\n```\r\n\r\nThe same preprocessing that protects \"open file\" should be applied to \"expand glob.\"",
  "created_at":"2023-12-12T16:03:21Z",
  "id":1852334616,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5uaGIY",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-12-12T16:03:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like you just got it in 50835ae9b1b9fe189ab0031eea568a5ce5429d09.",
  "created_at":"2023-12-12T16:03:49Z",
  "id":1852335506,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5uaGWS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-12T16:03:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> [lobis](https://github.com/lobis) changed the title ~~feat: globbing with fsspec (xrootd only)~~ feat: globbing with fsspec\r\n\r\nDoes that mean that the new fsspec is out? No, fsspec/filesystem_spec#1459 hasn't been merged, and Martin has a response about it.",
  "created_at":"2023-12-12T16:05:30Z",
  "id":1852338679,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5uaHH3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-12T16:08:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like it only tests globbing in XRootD, so I'll assume that's what it does.",
  "created_at":"2023-12-12T16:06:10Z",
  "id":1852340008,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5uaHco",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-12T16:06:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Looks like this one needs its failed tests rerun as well. Failures appear unrelated.",
  "created_at":"2023-12-12T20:24:16Z",
  "id":1852749273,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5ubrXZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-12T20:24:16Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"I wonder if this is one of the tests that had previously been called \"super-flaky\"? @lobis reinstated quite a few of these and they hadn't been acting up, maybe until now. The issue is an XRootD connection failure.\r\n\r\nMaybe we want to install this: https://github.com/marketplace/actions/retry-step\r\n\r\n(Maybe also here: https://github.com/CoffeaTeam/integration-test/blob/main/.github/workflows/test.yml)",
  "created_at":"2023-12-12T20:54:33Z",
  "id":1852796867,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5ub2_D",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-12-12T20:54:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Since this one is approved - if it can go in 5.2.0 please add it :-)",
  "created_at":"2023-12-13T01:44:25Z",
  "id":1853136180,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5udJ00",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-12-13T10:55:48Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> I wonder if this is one of the tests that had previously been called \"super-flaky\"? @lobis reinstated quite a few of these and they hadn't been acting up, maybe until now. The issue is an XRootD connection failure.\r\n> \r\n> Maybe we want to install this: https://github.com/marketplace/actions/retry-step\r\n> \r\n> (Maybe also here: https://github.com/CoffeaTeam/integration-test/blob/main/.github/workflows/test.yml)\r\n\r\nYes the xrootd tests are the most problematic and likely to fail due to server issues.\r\n\r\nWe are already doing reruns for the xrootd tests, with the following settings: ` --reruns 3 --reruns-delay 30`, so I'm not sure about adding this `retry-step` action, it feels redundant (maybe I didn't understand what it does). Perhaps we should increase the number of reruns and the time between them.",
  "created_at":"2023-12-13T16:26:12Z",
  "id":1854258849,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5uhb6h",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-13T16:26:12Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> It looks like it only tests globbing in XRootD, so I'll assume that's what it does.\r\n\r\nActually this should work for all fsspec filesystems that support globbing. I added some tests for s3 to show this. http will not work (I added a test that should fail if fsspec ever implements globbing for http, so we are noticied).\r\n\r\nSince these tests expand a glob expression they may fail in the future if some other files are added that match the glob. I set the tests in such a way that this is hopefully easy to spot and fix.",
  "created_at":"2023-12-13T16:28:16Z",
  "id":1854262404,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5uhcyE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-13T16:28:16Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Right, let's not add multiple _ways_ to retry. But if the error rate is high enough that with 3 retries we're still seeing it, then perhaps the retry parameters should get increased: `--reruns 5 --reruns-delay 60` (I assume that's 60 seconds).\r\n\r\nThat doesn't need to be done for this PR, though, which has nothing to do with the failing tests. For this PR, we can \"rerun failed tests\" manually go get all the green checkmarks and then merge the PR when everything is verified-okay.",
  "created_at":"2023-12-13T17:46:59Z",
  "id":1854441288,
  "issue":1061,
  "node_id":"IC_kwDOD6Q_ss5uiIdI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-13T17:46:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski some of these failures seem ~persistent but are obviously not related. \r\n\r\nAnyway - this one isn't super high priority. Decided to propose the option to y'all!",
  "created_at":"2023-12-12T20:12:00Z",
  "id":1852733415,
  "issue":1062,
  "node_id":"IC_kwDOD6Q_ss5ubnfn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-12T20:12:00Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"We should also add some tests with `distributed` to catch stuff like this in the future!",
  "created_at":"2023-12-13T12:49:48Z",
  "id":1853859746,
  "issue":1063,
  "node_id":"IC_kwDOD6Q_ss5uf6ei",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2023-12-13T12:49:48Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This will be closed by https://github.com/scikit-hep/uproot5/pull/1022 once it's merged into main",
  "created_at":"2023-12-13T21:36:24Z",
  "id":1854737266,
  "issue":1063,
  "node_id":"IC_kwDOD6Q_ss5ujQty",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-13T21:36:24Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, I missed the `--only-rerun` part of that. You're already doing the refinement. It's good, then!",
  "created_at":"2023-12-13T17:51:29Z",
  "id":1854447164,
  "issue":1064,
  "node_id":"IC_kwDOD6Q_ss5uiJ48",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-13T17:51:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@lgray I think this is done, let me know of it works fine!",
  "created_at":"2023-12-13T19:38:05Z",
  "id":1854591716,
  "issue":1065,
  "node_id":"IC_kwDOD6Q_ss5uitLk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-13T19:38:05Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"looks good to me! please merge :-)",
  "created_at":"2023-12-13T20:55:24Z",
  "id":1854687511,
  "issue":1065,
  "node_id":"IC_kwDOD6Q_ss5ujEkX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-13T20:55:24Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@lobis @jpivarski this should go in for 5.2.0",
  "created_at":"2023-12-14T12:43:11Z",
  "id":1855784074,
  "issue":1067,
  "node_id":"IC_kwDOD6Q_ss5unQSK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-14T12:43:11Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"NONE",
  "body":"The issue only appears in ROOT files with many branches.\r\nHere are code snippets to reproduce the problem.\r\n1. Produce a ROOT file with many branches:\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport uproot\r\n\r\ndf_dict = {}\r\n\r\ncolumn_names = []\r\n\r\nfor i in range(100):\r\n    column_names.append(\"column_{}\".format(i))\r\n\r\nfor i in column_names:\r\n    df_dict[i] = np.random.rand(1000)\r\n\r\ndf = pd.DataFrame.from_dict(df_dict)\r\n\r\nwith uproot.recreate(\"test.root\", compression=uproot.ZLIB(4)) as file:\r\n    file[\"tree\"] = df\r\n```\r\n2. Open the ROOT file with `uproot.iterate`:\r\n```\r\nimport uproot\r\n\r\nfor array in uproot.iterate(\"./test.root:tree\", step_size=100, library=\"pandas\"):\r\n    pass\r\n```\r\n",
  "created_at":"2023-12-15T12:31:35Z",
  "id":1857810452,
  "issue":1070,
  "node_id":"IC_kwDOD6Q_ss5uu_AU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-15T12:32:40Z",
  "user":"U_kgDOBtvh2Q"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Some additional information:\r\n\r\n- This warning also appears for uproot v5.1.0\r\n- It's not related to fsspec, passing `handler=uproot.source.file.MemmapSource` does not make a difference",
  "created_at":"2023-12-15T15:33:26Z",
  "id":1858069771,
  "issue":1070,
  "node_id":"IC_kwDOD6Q_ss5uv-UL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-15T15:33:26Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Cool! I looked through the implementation of the local XRootD server. I'm not sure why it's an iterator with exactly one yield: I don't think that allows it to run asynchronously.\r\n\r\nI've writen a couple of fixtures and they all follow the same pattern (this one is actually based on what's used in fsspec-xrootd). My understanding is that fixtures have this structure so that setup / cleanup can be defined in a single function. First the fixture method is run until the yield (setup), then the test code is ran, then the code after the yield (cleanup) is ran.",
  "created_at":"2023-12-21T14:51:15Z",
  "id":1866411791,
  "issue":1076,
  "node_id":"IC_kwDOD6Q_ss5vPy8P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2023-12-21T14:51:15Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 }
]
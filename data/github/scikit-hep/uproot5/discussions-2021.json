[
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"From @johnmcgowan1 in scikit-hep/uproot3#532:\r\n\r\n> Just a question, so maybe an Issue is not the right place to ask. Will uproot support the writing of ndarrays in the future? If so is there a timescale on which we can expect this?\r\n\r\nAnd also @Skagamaster in [Gitter](https://gitter.im/Scikit-HEP/uproot):\r\n\r\n> Sorry for posting int he incorrect forum, and thanks for the input! My arrays aren't rectangular, so I'll have to go with the ListOffsetArray method (or pad or flatten, as suggested).\r\n\r\nUproot 3's ability to write NumPy arrays and simple Awkward Arrays (lists of booleans/numbers only) will be ported into Uproot 4 this spring. This will also include writing histograms (and TObjString, though that's primarily because it's a simple type for debugging). I can see that it's a frequently requested feature, but it will take a few months to complete. Everything in Uproot 4 is a rewrite of the corresponding part of Uproot 3 to use the new Awkward Array API and make Uproot more maintainable (e.g. what used to be two large files in Uproot 3 has been split into specialized classes, one per file, with improved debugging, ROOT class version handling, and all that). I'm going to do the same thing for @reikdas's file-writing classes. He put a lot of effort into understanding how to write correct ROOT files\u2014which is a different problem from merely reading them\u2014and that needs to be translated into more future-proof code, like all the rest of Uproot 3.\r\n\r\nYou can follow progress on its [project board](https://github.com/scikit-hep/uproot4/projects/3), though work won't begin in earnest until late February or early March (juggling with other deadlines). Originally, this was one of the items that I wanted to include before renaming `uproot4` \u2192 `uproot`, but considering how long file-writing would take, I didn't want it to delay the name change. (The new version had already been in this opt-in state for about 5 months by that point.)\r\n\r\nWhat will be included:\r\n\r\n   * Creating ROOT files that can be read and updated in ROOT, but probably not updating files made by ROOT for reasons described here: https://github.com/scikit-hep/uproot3/issues/381#issuecomment-605375173.\r\n   * Writing histograms (descendants of TH1) and TObjStrings.\r\n   * Writing TTrees with arrays that don't have to be in memory all at once\u2014i.e. progressively extending the file in chunks. (Those chunks becomes TBaskets).\r\n   * \"Arrays\" includes all NumPy arrays, including multidimensional and numeric dtypes (booleans and numbers), and maybe [structured arrays](https://numpy.org/doc/stable/user/basics.rec.html) \u2192 leaf-lists. It also includes Awkward lists of numeric types; that is, [ListArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListArray.html), [ListOffsetArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.ListOffsetArray.html), [RegularArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.RegularArray.html), and [NumpyArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.NumpyArray.html) from Awkward Array (and [EmptyArray](https://awkward-array.readthedocs.io/en/latest/ak.layout.EmptyArray.html) as a non-useful trivial case, for completeness).\r\n\r\nFor the time being, if you need to write arrays to ROOT files, you have a few options:\r\n\r\n   * Import both `uproot` and `uproot3` in the same Python process* (_this_ is why they have different names, so you can do that) and follow [these instructions to write histograms](https://github.com/scikit-hep/uproot3#writing-histograms) and follow [these instructions to write TTrees](https://github.com/scikit-hep/uproot3#writing-ttrees) from NumPy arrays.\r\n   * If you have an Awkward Array of lists (\"jagged array\"), this is supported in Uproot 3 but not documented. [Here is the pull request](https://github.com/scikit-hep/uproot3/pull/477), which has some discussion of the API and points to [the tests that demonstrate it](https://github.com/scikit-hep/uproot3/blob/b4eaf90e0af85003ff656286c8adf1c6f9a08fe6/tests/test_write.py#L1862-L2136). If your arrays are in Awkward 1.x format, convert them into Awkward 0.x using [ak.to_awkward0](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_awkward0.html).\r\n   * [Read NumPy arrays into RDataFrame](https://github.com/root-project/root/blob/master/bindings/pyroot/pythonizations/test/rdataframe_makenumpy.py) and use RDataFrame to write files (syntax is `ROOT.RDF.MakeNumpyDataFrame(dict_of_numpy_arrays)`).\r\n   * [I'm planning to add round-trip transformations](https://github.com/scikit-hep/awkward-1.0/issues/588) between arbitrary Awkward Arrays and RDataFrame, so this will be a long-term option (not just for writing files, but also for mixing Awkward/Coffea and C++ analysis in general).\r\n\r\n* To use `uproot` and `uproot3` in the same Python process, as well as `awkward` and `awkward0`, make sure to update all relevant packages: everything should be to the right of the transition line:\r\n\r\n![transition-version-numbers-coffea-methods](https://user-images.githubusercontent.com/1852447/105192214-7c166400-5afd-11eb-94be-0ae9df79e611.png)",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-01-20T15:17:52Z",
  "number":249,
  "title":"Writing arrays to ROOT files in Uproot 4",
  "url":"https://github.com/scikit-hep/uproot5/discussions/249"
 },
 {
  "author":{
   "login":"DebajyotiS"
  },
  "body":"I have a root file with the following structure (image attached)\r\n![Screenshot_20210215_114723](https://user-images.githubusercontent.com/27846585/107937132-ce736500-6f83-11eb-8290-42ea9814490c.png)\r\n\r\nI load the root file with uproot, in order to access the \"m_event_number\", however when I try file[\"CollectionTree\"][\"Eventnfo_p4_McEventInfo\"][\"m_event_ID\"] - I get keyInFileError - Available keys: None.\r\n\r\nAm I doing something wrong?\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"tamasgal"
     },
     "body":"Do you have the possibility to upload the file? That would make thing a bit easier.\r\n\r\nMeanwhile: what is the output of:\r\n\r\n```python\r\nfile[\"CollectionTree\"][\"Eventnfo_p4_McEventInfo\"].show()\r\n```",
     "createdAt":"2021-02-15T11:08:50Z",
     "number":370153,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"DebajyotiS"
        },
        "body":"![image](https://user-images.githubusercontent.com/27846585/107941784-84da4880-6f8a-11eb-8f20-9bdb273c7b3d.png)\r\nI will check if I have the rights to share the file, until then I can share the result for this",
        "createdAt":"2021-02-15T11:37:41Z",
        "number":370227
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"tamasgal"
     },
     "body":"OK, it seems you have a custom streamer there. Do you have any luck with `file[\"CollectionTree\"][\"Eventnfo_p4_McEventInfo\"].array()`?\r\n\r\nI fear you need to provide the interpretations on your own, but let's see \ud83e\udd14 ",
     "createdAt":"2021-02-15T11:39:19Z",
     "number":370233,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"DebajyotiS"
        },
        "body":"I had tried .array(), but I get an error : CannotBeAwkward: TriggerInfo_p2::StreamTag_p2",
        "createdAt":"2021-02-15T12:00:50Z",
        "number":370281
       },
       {
        "author":{
         "login":"tamasgal"
        },
        "body":"Yes that seems to be a more complicated structure and you will have to provide the interpretation details since uproot is unable to figure it out automatically. If you have access to the C++ code, that might be already a good starting point, but the question is of course also if you have time and energy to dive into this as it is not a trivial task (depending on your experience with ROOT/C++ and serialisation in general).",
        "createdAt":"2021-02-15T12:03:37Z",
        "number":370287
       },
       {
        "author":{
         "login":"DebajyotiS"
        },
        "body":"Unfortunately, I only have a little experience with ROOT and C++. When you say provide interpretation details, do you mean passing it to uproot while trying to access the branch? ",
        "createdAt":"2021-02-15T12:12:18Z",
        "number":370305
       },
       {
        "author":{
         "login":"tamasgal"
        },
        "body":"Yes, this means that you need to tell uproot how a specific class is serialised into the ROOT format. This can be quite complicated as you can imagine since you need to provide the field types and the types/structure of sub-structures etc. uproot is usually able to figure out a lot but sometimes it chokes on structures which still needs to be understood and implemented. For this, a sample file would be very helpful to see where it fails (obviously the `TriggerInfo` stuff is one of them) and maybe is an easy fix but it can also be much more difficult, especially without access to the C++ code or an actual dump of the original data to be able to compare.",
        "createdAt":"2021-02-15T12:18:14Z",
        "number":370319
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Actually, \"cannot be Awkward\" is weaker than \"cannot be read.\" If the error message doesn't already say it, it should say \"try `library=\"np\"`. Very complex objects, such as histograms, can't be represented as Awkward Arrays, but they can be read into Python objects, and `library=\"np\"` makes NumPy arrays of Python objects.\r\n\r\nAccording to `.show()`, it looks like Uproot identified an interpretation for this type. (We don't know, until you successfully read it, that the interpretation is correct.) So start with `library=\"np\"` because there's a good chance that the streamers that Uproot found and interpreted might be correct.",
        "createdAt":"2021-02-15T16:23:31Z",
        "number":371039
       },
       {
        "author":{
         "login":"tamasgal"
        },
        "body":"Ah interesting, I always thought that \"cannot be awkward\" is kind of a dead-end, since my gut feeling is that if something cannot be awkward, it cannot be numpy either (like numpy being a subset of awkard), but indeed I completely forgot that numpy is able to hold Python objects \ud83d\ude09 \r\n\r\nEdit: that being said, yes, I think there should be a hint to try `library=\"np\"` and additionally  maybe a warning that the performance might not be optimal.",
        "createdAt":"2021-02-15T16:27:37Z",
        "number":371052
       },
       {
        "author":{
         "login":"DebajyotiS"
        },
        "body":"with ```HITS_1[\"CollectionTree\"][\"EventInfo_p4_McEventInfo\"].array(library=\"np\")```, I get a deserialisation error (looks like there is a version clash?):\r\n\r\n![image](https://user-images.githubusercontent.com/27846585/107972802-a6e8c080-6fb4-11eb-92a7-5c9e6e07d301.png)\r\n \r\n\r\n",
        "createdAt":"2021-02-15T16:38:51Z",
        "number":371094
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"It wasn't interpreted correctly. From this error message, it looks like the interpretation thought there would be a header indicating the number of bytes in `EventType_p3` (version 1), but that header doesn't exist and it's picking something else up as the number of bytes, thereby thinking that there ought to be 391380996 bytes in this object, which is contained within `EventInfo_p4` (version 1), which has 54 bytes. Very likely, only the 54 bytes is correct. The 4-byte number \"391380996\" is `[23, 84, 0, 4]` as single bytes, and that `[0, 4]` looks a lot like a version number.\r\n\r\nMore fundamentally, I think the main error is, as you've pointed out, related to the \"version 0\" versus \"version 1.\" These are version numbers of the classes, not ROOT. If Uproot encounters an object with a specific version number but doesn't find a class with that version number in the ROOT file's streamers, then it takes the highest version and hopes that's correct. It's not correct in this case. Sometimes, the class streamers that are absolutely required for knowing how to deserialize an object just aren't in the ROOT streamers, and then we can't do anything. This might be such a case.\r\n\r\nIf it's possible to recreate these files with a high \"split-level,\" like [splitLevel=99](https://root-forum.cern.ch/t/what-is-the-meaning-of-splitlevel-99/20593), doing so would alleviate all of these problems. This `m_event_number` is just a bunch of integers, the `m_bit_mask` is just `std::vector<bool>`, etc. With a high split-level, they would each be separate TBranches, separate arrays, and not only avoid the difficulties of deserializing complex objects, but would also be much faster. (This deserialization code is Python for loops\u2014interpreting an array of integers or `std::vector<bool>` is NumPy.)",
        "createdAt":"2021-02-15T17:39:51Z",
        "number":371251
       }
      ],
      "totalCount":8
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-02-15T10:53:43Z",
  "number":267,
  "title":"Unable to read Branches",
  "url":"https://github.com/scikit-hep/uproot5/discussions/267"
 },
 {
  "author":{
   "login":"wctaylor"
  },
  "body":"I'm curious to see what the \"pros\" do for handling large datasets. In my case, I have ~1k datasets, and each dataset is ~20k rootfiles. Each dataset is 4-5 GB on-disk. In other words, there's 4-5 TB distributed across ~20 million files. \r\n\r\nMy current strategy is not very sophisticated, though I guess it seems to work. I work on a HPC cluster, and I generally have a SLURM job run over each dataset and do something, then try to pull it all together at the end into something manageable. \r\n\r\nThat said, in other areas, it seems that tools like Dask allow for decently interactive handling of 1 TB+ inside Jupyter notebooks, so in principle it might be possible to interactively explore the whole set relatively quickly, but it doesn't seem like Dask is well suited for physics data/Awkward Arrays. \r\n\r\nSo how might one interactively explore this much data in a Jupyter notebook?\r\n\r\nIs it possible or feasible?\r\nShould I first try to concatenate everything into one massive rootfile, and then use either uproot.iterate or uproot.lazy to work with that file? What is the best way to utilize the HPC environment through uproot/awkward? \r\nWhat other tips/tricks should I be aware of?\r\n\r\nI'd love to get some insights on how others would handle this situation. And if others find it valuable, too, maybe it could make it into the docs. \r\n\r\nThanks for any input.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"tamasgal"
     },
     "body":"I do not really do interactive work with large datasets; to me interactivity is mostly important for discovering high-level data which is already condensed and fit into a single computer's resource limitations.\r\n\r\nI'll rather answer in a more generic way: to me it sounds like you could make benefit of a workflow system which supports HPC environments and offers a few other crucial features like provenance, reproducibility and shareability. Such a workflow system is usually combined with containerisation since you need to be able to distribute the software including all the dependencies in a unified way also to heterogenous HPC systems.\r\n\r\nI highly recommend two frameworks: [nextflow](https://nextflow.io) if you want an all-in-one package (supports SLURM, PBS, SGE, etc.) and the [Common Workflow Language](https://www.commonwl.org) if you want to do something more modular. The former offers workflow descriptions and a job-runner/manager in one single command line application (Java 8+) the latter is just a standards definition to describe tools and workflows with e.g. YAML files and the runner is then free of choice (e.g. [Toil](https://toil.readthedocs.io/en/latest/)), as long as it implements one of the CWL standards.\r\n\r\nAgain, this is nothing about interactive play with terabytes of data spread over multiple nodes, but a more sophisticated workflow management, which in my opinion essential in science for several good reasons.",
     "createdAt":"2021-02-17T14:09:08Z",
     "number":377684,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"wctaylor"
        },
        "body":"Thanks! That is still valuable input, and indeed, it looks like something that I should become familiar with",
        "createdAt":"2021-02-17T14:15:29Z",
        "number":377707
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Awkward Arrays need to be able to work better with Dask. That's a to-do item. I have tried to just interpret lazy Awkward Arrays as Dask delayed arrays, but you quickly run into assumptions that the array has a `shape` and `dtype`, which is exactly what Awkward Arrays don't have\u2014that's what's awkward about them. I think we might need to make a new collection type for it, alongside Dask's array, bag, and frame abstractions. We've talked with the Dask team about that and scoped out the work.\r\n\r\nOn the other hand, you can also use Dask by decorating _functions_, and those functions can contain any operations to compute. This might be the best option for using Dask at the moment.\r\n\r\nIn particular, this is how [Coffea](https://coffeateam.github.io/coffea/) uses Dask with Awkward Arrays. Coffea has a \"Processor\" abstraction on top of Dask that does the map (work with arrays) + reduce (combine final histograms) process. That's how a lot of physicists scale up their work.\r\n\r\nI should point out that [uproot.lazy](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.lazy.html) allows you to treat a collection of ROOT files as a single lazy array\u2014you don't need to combine them for that. However, this function does need to _open_ all the files immediately, to determine the lengths of all the TTrees, to index them all into a single abstract array. If you have enough files, just opening them all is prohibitive, even without reading their contents.\r\n\r\nCoffea has a \"NanoEvents\" framework that goes a step further in laziness, allowing you to address a collection of files without opening them all. It has to find out the number of events in each file, first, though. (I don't remember how it does that.)\r\n\r\nI've been learning more about the Parquet file format recently, and this has a nice \"dataset\" abstraction that pulls all of the metadata out of each file in a collection, putting it all in a single metadata-only file. That way, you don't have to open the individual files to find out such things as how many entries are in each. I've added the ability to read and write these things from Awkward Array (scikit-hep/awkward-1.0#706 and https://github.com/scikit-hep/awkward-1.0/issues/368#issuecomment-774333546). If it's interesting, I'll write some documentation for Parquet datasets.",
     "createdAt":"2021-02-17T16:30:33Z",
     "number":378283,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"wctaylor"
        },
        "body":"Thanks! Maybe I need to get more familiar with Coffea, then. I have been making use of Parquet to save some intermediate results, since uproot4 doesn't currently support writing ROOT files. So if there's a more effective way I could make use of it, I'd be happy to learn that as well. ",
        "createdAt":"2021-02-17T16:40:52Z",
        "number":378335
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-02-17T13:40:23Z",
  "number":274,
  "title":"Best Practices/How-tos for Handling Large Amounts of Data?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/274"
 },
 {
  "author":{
   "login":"ast0815"
  },
  "body":"Hello, I am currently testing uproot4 functions with the Histoprint command line tool. In uproot3, one could load branches without specifying the full path. In uproot4 that does not seem to be possible.\r\n\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n4.0.4\r\n```\r\n\r\n```\r\n$ wget --quiet http://scikit-hep.org/uproot3/examples/Event.root\r\n$ python -c \"import uproot as up; up.open('Event.root')['T'].arrays(['event/fTracks.fYfirst'])\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 1133, in arrays\r\n    arrays,\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3460, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3414, in basket_to_array\r\n    library,\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/interpretation/objects.py\", line 141, in basket_array\r\n    form = self.awkward_form(branch.file, index_format=\"i64\")\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/interpretation/objects.py\", line 122, in awkward_form\r\n    self._branch.file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/containers.py\", line 645, in awkward_form\r\n    self._values, file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/_util.py\", line 426, in awkward_form\r\n    file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/containers.py\", line 369, in awkward_form\r\n    raise uproot.interpretation.objects.CannotBeAwkward(self.message)\r\nuproot.interpretation.objects.CannotBeAwkward: Double32_t in array (note: Event.root fClosestDistance has an example)\r\n```\r\n\r\nThis used to work with uproot3. It also does work with uproot4 when explicitly providing the full path to the variable:\r\n\r\n```\r\n$ python -c \"import uproot as up; up.open('Event.root')['T'].arrays(['event/fTracks/fTracks.fYfirst'])\"\r\n```\r\n\r\nNot the additional `/fTracks`.\r\n\r\nEasy to fix by providing the full paths, but I am not sure whether this is expected behaviour.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"ast0815"
     },
     "body":"I spoke too soon. When trying to address specific indices, it fails again:\r\n\r\n```\r\n$ python -c \"import uproot as up; up.open('Event.root')['T'].arrays(['event/fTracks/fTracks.fYfirst[...:0]'])\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 1133, in arrays\r\n    arrays,\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3460, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3414, in basket_to_array\r\n    library,\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/interpretation/objects.py\", line 141, in basket_array\r\n    form = self.awkward_form(branch.file, index_format=\"i64\")\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/interpretation/objects.py\", line 122, in awkward_form\r\n    self._branch.file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/containers.py\", line 645, in awkward_form\r\n    self._values, file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/_util.py\", line 426, in awkward_form\r\n    file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/containers.py\", line 369, in awkward_form\r\n    raise uproot.interpretation.objects.CannotBeAwkward(self.message)\r\nuproot.interpretation.objects.CannotBeAwkward: Double32_t in array (note: Event.root fClosestDistance has an example)\r\n```\r\n\r\nSlicing does work for another file I have though: [test.root.zip](https://github.com/scikit-hep/uproot4/files/6022713/test.root.zip)\r\n\r\n```\r\n$ python -c \"import uproot as up; up.open('test.root')['truth'].arrays(['truelepton_dir[...,0]'])\"\r\n```",
     "createdAt":"2021-02-22T14:49:00Z",
     "number":394081,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Full paths or relative paths are not the issue: what it's trying to do here is load the whole `event` array as a single object to _divide it_ by `fTracks`, and _divide that_ by `fTracks.fYfirst[...:0]`. The first argument of [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) is \"`expressions`\", quantities that are computed by Python. (You are using the fact that this is a computed expression by slicing `fTracks.fYfirst` with `..., :0`, which couldn't be done in Uproot 3.)\r\n\r\nThe nested path names (with `/` not meaning \"divide\") and pattern matching (with `*` not meaning \"multiply\") are in the `filter_name` parameter. What you meant to say in your first example is:\r\n\r\n```python\r\n>>> uproot.open(\"/mnt/storage/data/misc/Event.root\")[\"T\"].arrays(\r\n...     [\"fTracks.fYfirst[..., :0]\"],\r\n...     filter_name=\"event/fTracks/fTracks.fYfirst\")\r\n<Array [{'fTracks.fYfirst[..., :0]': [, ... ] type='1000 * {\"fTracks.fYfirst[......'>\r\n```\r\n\r\nor you could hide that unwieldy field name as an alias (which work just like ROOT's built-in fAliases):\r\n\r\n```python\r\n>>> uproot.open(\"/mnt/storage/data/misc/Event.root\")[\"T\"].arrays(\r\n...     [\"first\"],\r\n...     filter_name=\"event/fTracks/fTracks.fYfirst\",\r\n...     aliases={\"first\": \"fTracks.fYfirst[..., :0]\"})\r\n<Array [{first: []}, ... {first: []}] type='1000 * {\"first\": var * float32}'>\r\n```\r\n\r\nAlso, if you're after just one branch, you can navigate to that branch explicitly and avoid all this searching and renaming.\r\n\r\n```python\r\n>>> uproot.open(\"/mnt/storage/data/misc/Event.root\")[\"T/event/fTracks/fTracks.fYfirst\"].array()[..., :0]\r\n<Array [[], [], [], [], ... [], [], [], []] type='1000 * var * float32'>\r\n```\r\n\r\nNow I'm wondering if you really meant to slice it with \"`..., :0`\", since this would give you all empty lists, but the slicing is independent of the distinction between `expressions` (computable expressions, possibly aliased), `filter_name` (picking out the branches you want), and paths (navigating through TDirectories and TBranches).\r\n\r\nI don't see a bug here, and the above would be good for everybody to know, so I'm going to make this a Discussion. If I'm wrong and there's still a bug, open a new Issue. (Unfortunately, we can't turn Discussions back into Issues, but a second iteration on your problem would likely be more specific, anyway, justifying a new Issue.)",
     "createdAt":"2021-02-22T15:56:52Z",
     "number":394082,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ast0815"
        },
        "body":"Right, there was a typo there. It should have been `[:,0]`, i.e. selecting only the first element for all events, like in the second example.\r\n\r\nLet me explain my use case, and maybe you have a suggestion how to bes implement this:\r\n\r\nThe `histoprint` CLI program can read in ROOT files and plot histograms from TTree branches. It used to get the data of the branches one-by-one, like you suggested. I would like to extend that functionality by allowing users to provide a \"cut\" so that the data can be filtered before plotting.\r\n\r\nNow when reading the uprrot3->4 cheat sheet, I saw that the `arrays` method accepts a `cut` argument, while the `array` (singular) method does not. I assumed that the `arrays` method would thus be more efficient by doing the cutting while reading the TTree rather than me loading branches and then calculating a boolean index to do the cutting. If you say that the evaluation of the first two arguments happens on Python objects, is there no performance gain from using `arrays` rather than `array`?\r\n\r\nWhat would you suggest is the best way to load multiple branches (potentially with indices, e.g. to plot the 2nd component of a direction vector) with some sort of filtering based on those (or other) branches?\r\n\r\nSo far I have kept the \"provide full path\" approach, because some ROOT files might have sub-branches with identical names.",
        "createdAt":"2021-02-22T17:21:25Z",
        "number":394370
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"There is no performance gain from using strings (e.g. non-trivial `expressions` or `cut`), rather than working with Python objects. In fact, that's how the string-evaluation is implemented.\r\n\r\nThe reason it exists at all is to implement `aliases`. Those `aliases` are usually expressed in TTreeFormula syntax, which is not the same as Awkward Arrays in Python, so the job isn't entirely done: we'll need a parser for TTreeFormula so that this can be passed as the `language` argument (probably by default, after a switchover period). But when that's complete, there will still be this issue about `/` meaning division and `*` meaning multiplication that we need to get used to now.\r\n\r\nThere isn't a way to improve performance beyond reading the arrays used in the cut and the arrays they are meant to cut, then performing that calculation array-wise. TBaskets can't be partially read because they're compressed as a single block, in order to read any byte from a TBasket, you have to read the whole TBasket. There is one caveat: you could do a little better than \"read TBaskets into whole arrays, apply cut\" by \"read individual TBaskets, apply cut to each TBasket-array, then combine TBasket-arrays into whole arrays,\" which saves a memory copy, but most calculations you might want to do in order to implement the cut are more expensive than a memory-copy.\r\n\r\nYou can still provide the whole path, but it has to be in `filter_name` (which can be a list, if you're loading several), not `expressions`.",
        "createdAt":"2021-02-22T18:33:02Z",
        "number":394647
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-02-22T14:31:27Z",
  "number":283,
  "title":"TTree.arrays() first argument with paths (\"/\" character meant path in Uproot 3, division in Uproot 4)",
  "url":"https://github.com/scikit-hep/uproot5/discussions/283"
 },
 {
  "author":{
   "login":"nikoladze"
  },
  "body":"I'm trying to understand what the overhead for reading a \"flat\" TTree (meaning no sub-branches and only fundamental types and arrays thereof) is when reading via `TTree.arrays` as compared to looping over the branches and calling `TBranch.array`. This can be illustrated with the NanoAOD sample from the tests:\r\n\r\n```python\r\nfrom io import BytesIO\r\nimport requests\r\nimport uproot\r\nimport awkward as ak\r\n\r\nurl = \"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\"\r\n\r\nwith requests.get(url) as res:\r\n    raw_data = res.content\r\n\r\ndef test_iteritems(raw_data):\r\n    with uproot.open(BytesIO(raw_data)) as f:\r\n        return ak.zip(\r\n            {k : v.array() for k, v in f[\"Events\"].iteritems()},\r\n            depth_limit=1\r\n        )\r\n\r\ndef test_tree_arrays(raw_data):\r\n    with uproot.open(BytesIO(raw_data)) as f:\r\n        return f[\"Events\"].arrays()\r\n```\r\n```pycon\r\nIn [1]: %time test_iteritems(raw_data)\r\nCPU times: user 1.54 s, sys: 20.6 ms, total: 1.56 s\r\nWall time: 1.54 s\r\nOut[1]: <Array [{run: 1, ... L1_ZeroBias_copy: True}] type='40 * {\"run\": uint32, \"lumino...'>\r\nIn [2]: %time test_tree_arrays(raw_data)\r\nCPU times: user 16.8 s, sys: 36.1 ms, total: 16.8 s\r\nWall time: 16.8 s\r\nOut[2]: <Array [{run: 1, ... L1_ZeroBias_copy: True}] type='40 * {\"run\": uint32, \"lumino...'>\r\n```\r\n\r\nThe first noticeable thing when running a profiler is that there seem to be significantly more calls to `member`\r\n```\r\nIn [1]: %prun test_tree_arrays(raw_data)\r\n...\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n16930772/11294002    8.996    0.000   12.563    0.000 model.py:498(member)\r\n...\r\nIn [2]: %prun test_iteritems(raw_data)\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n...\r\n61023/48501    0.031    0.000    0.040    0.000 model.py:498(member)\r\n...\r\n```\r\n\r\nWhy is that?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"There shouldn't be any scaling (i.e. _O(n)_) performance differences between calling `.array` on each branch versus calling `.arrays` on the whole tree. They differ only in the \"bookkeeping\" of discovering what branches need to be collected and collecting them, though it's not as simple as your `test_iteritems` since the code path has to deal with potentially evaluating expressions, filtering branches, and such. As \"bookkeeping code,\" the distinction should only be visible for small arrays. When the array size is large (MB, at least), all that running around and string manipulation for each branch should become subdominant to reading and decompressing the arrays.\r\n\r\nThat's where the optimization effort went: ensuring that there's no Python code in the _O(n)_ loop, releasing the GIL when possible, minimizing the number of remote call-and-response, etc. No special effort was paid to minimizing the _O(1)_ bookkeeping stuff.\r\n\r\nIf you have a TTree with a very large number of branches and a small number of entries, then this \"bookkeeping stuff\" could become relevant, and maybe if you have a lot of these short, wide files, it would add up to human time (seconds or more).\r\n\r\nThe \"`member`\" calls are getting member data of a ROOT class, and if I had to guess, I'd guess it's `member(\"fBranches\")` on the TBranch object. Why this is called many more times would be deep in that bookkeeping code, but a simple fix could be to cache it.\r\n\r\nFor instance, _if_ this is the offending call:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/c719f858e775d8bd04b05a69552610e51cdb0047/uproot/behaviors/TBranch.py#L857-L864\r\n\r\n_then_ we could speed it up like this:\r\n\r\n```python\r\n    # on the HasBranches class...\r\n    _branches = None\r\n\r\n    @property\r\n    def branches(self):\r\n        \"\"\"\r\n        The list of :doc:`uproot.behaviors.TBranch.TBranch` directly under\r\n        this :doc:`uproot.behaviors.TTree.TTree` or\r\n        :doc:`uproot.behaviors.TBranch.TBranch` (i.e. not recursive).\r\n        \"\"\"\r\n        if self._branches is None:\r\n            # on this instance...\r\n            self._branches = self.member(\"fBranches\")\r\n        return self._branches\r\n```\r\n\r\nThe \"`member`\" abstraction is nice to have for most objects (emulating the C++ class hierarchy without reproducing it on the Python side, because of the problems this caused in Uproot 3), but TBranch is special and we can turn the guarantee that members don't change (because they're accessed with a function) into an assumption that this member doesn't change.\r\n\r\nIf, on the other hand, I'm guessing wrong about which member is being called so much, then that other member should be cached instead.",
     "createdAt":"2021-02-22T20:17:32Z",
     "number":394972,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-02-22T19:43:34Z",
  "number":285,
  "title":"Overhead in `TTree.arrays` for \"flat\" ntuples",
  "url":"https://github.com/scikit-hep/uproot5/discussions/285"
 },
 {
  "author":{
   "login":"calebmiller"
  },
  "body":"I'd like to load a TTree from a rootfile but with a prescale applied. ie. Instead of loading each event into the array I only load events where event_id(mod)10=0 or event_id(mod)100=0. \r\n\r\nI imagine this is fairly easy to do if the event_id is already in the TTree as a branch, by just requiring the event meets the mod 0 requirement. But what about if there is no explicit event number branch?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Any array in Python can use a slice syntax like this:\r\n\r\n```python\r\narray[::10]\r\n```\r\n\r\nThe above will take event `0`, event `10`, event `20`, etc. (If you wanted event `9`, `19`, `29`, etc., then you can do `array[9::10]`. That would agree better with `eventId % 10` if the `eventId` started with `1` instead of `0`.)\r\n\r\nThere's no way to avoid _reading_ all the events before applying this prescale because contiguous events have to be read together (due to the columnar structure of a ROOT TBranch), but this will get you every 10th event after they've been read.",
     "createdAt":"2021-02-27T21:42:12Z",
     "number":413474,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"calebmiller"
        },
        "body":"Ah that makes sense. Thanks for your help",
        "createdAt":"2021-02-27T21:46:39Z",
        "number":413498
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-02-27T21:18:54Z",
  "number":287,
  "title":"reading in every Nth event from a file",
  "url":"https://github.com/scikit-hep/uproot5/discussions/287"
 },
 {
  "author":{
   "login":"ryuwd"
  },
  "body":"Currently I am using `uproot4.iterate` with 223 ROOT files (~400G total) over XRootD to filter through entries and only save certain columns in a parquet file. \r\n\r\nI am using the latest development version of uproot as of the time of posting at commit 7e3353cfdec91c340c7a0e26a31a695d2db3bcf1. \r\nI was using version 4.0.2, but ran into virtual memory problems (i.e. filling to max available + running into MemoryError), and figured that maybe using #281 would help.\r\n\r\nHere's the code I'm using: \r\n```python\r\n\r\n    arrays = uproot.iterate(\r\n        data.get_list(tree), # a list of xrootd paths formatted like this: \"{path}:{ROOT_tree_location}\"\r\n        filter_name=branches, # a set of 35 column / branch names\r\n        library=\"pd\", # my data is not jagged, so I'm using pandas\r\n    )\r\n    total = 0\r\n    total_saved = 0\r\n    with IncrementalPqWriter(output) as selection: # lets me write row groups to a parquet file from dataframes\r\n        for df in arrays:\r\n            mask = xicp_presel_mask(df, use_rectangular_cuts=use_rect_cuts) # returns boolean mask\r\n            df[\"foldNumber\"] = df[\"eventNumber\"] % 2\r\n            selection.write_from_df(df[mask])\r\n            \r\n            total += df.shape[0]\r\n            total_saved += np.sum(mask)\r\n            log.info(\r\n                f\" {np.sum(mask)} / {df.shape[0]} events saved. \"\r\n                f\"Total: {total_saved} saved / {total} processed\"\r\n            )\r\n\r\n```\r\n\r\n```\r\n  xicp_preselection  INFO     | Loading tree Xicp_ToPpKmPip/DecayTree from 223 ROOT file(s). 35 columns will be saved.\r\n  xicp_preselection  INFO     |  10172 / 47818 events saved. Total: 10172 saved / 47818 processed\r\n  xicp_preselection  INFO     |  7412 / 35161 events saved. Total: 17584 saved / 82979 processed\r\n  xicp_preselection  INFO     |  6339 / 30022 events saved. Total: 23923 saved / 113001 processed\r\n  xicp_preselection  INFO     |  6169 / 28699 events saved. Total: 30092 saved / 141700 processed\r\n  xicp_preselection  INFO     |  8061 / 37881 events saved. Total: 38153 saved / 179581 processed\r\n  xicp_preselection  INFO     |  5939 / 28098 events saved. Total: 44092 saved / 207679 processed\r\n  xicp_preselection  INFO     |  10060 / 47716 events saved. Total: 54152 saved / 255395 processed\r\n  xicp_preselection  INFO     |  4562 / 21916 events saved. Total: 58714 saved / 277311 processed\r\n  xicp_preselection  INFO     |  9492 / 44276 events saved. Total: 68206 saved / 321587 processed\r\n  xicp_preselection  INFO     |  6724 / 31274 events saved. Total: 74930 saved / 352861 processed\r\n  xicp_preselection  INFO     |  8489 / 39955 events saved. Total: 83419 saved / 392816 processed\r\n  xicp_preselection  INFO     |  8082 / 38127 events saved. Total: 91501 saved / 430943 processed\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\n/bin/bash: line 1: 34478 Aborted                 GIT_REVISION=\"$(git rev-parse HEAD)\" PYTHONPATH=\"/.../\" python /.../.snakemake/scripts/tmpreua4su2.wrapper.py\r\n```\r\n\r\nI think this error is maybe related to #281. In `4.0.2` I was just getting `MemoryError` - which seems to be gone now.\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"MemoryError and `std::bad_alloc` are pretty similar: the first is Python's exception, the second is C++'s exception. The allocation that actually fails to allocate memory, whether it's in Python or C++, is not where the real problem lies. It's a matter of other things using up the memory.\r\n\r\nIf it's running out of memory \"legitimately,\" which is to say, you just don't have enough memory to do what you're trying to do, then it is difficult to diagnose on a different computer with a different amount of memory available. If it's a memory leak (which #281 was not), then it happens on any computer if you wait long enough.\r\n\r\nWhat you're doing in your code doesn't involve Pandas, just some masking, so you can remove a big performance and memory hog by replacing `library=\"pd\"` with `library=\"np\"` and do everything with plain NumPy arrays. (What you've named \"`df`\" would come out as a dict of NumPy arrays, which can be selected with square brackets and strings, just as you have it.)\r\n\r\nThe Python garbage collector is _supposed to_ get invoked when you start running low on memory, though it could only do that if the limit is approached while the Python code is active (i.e. a MemoryError, not `std::bad_alloc`). In #281, however, it wasn't getting invoked before a Pandas DataFrame construction, but that could be because Pandas has its own C++ code to hide allocations from the garbage collector. So removing Pandas might help that, or you could `import gc` and `gc.collect()` at the end of your loop.\r\n\r\nActually, thinking about this more deeply, the arrays you make in one step of iteration (`mask`, `df`, etc.) are still in scope when the next iteration starts, so either put all the logic in the body of the loop or explicitly `del mask`, `del df`, etc. when done, maybe in addition to `gc.collect()`. (The garbage collector can only remove things that are out of scope.) As it is, there has to be two iterations' worth of arrays in memory at the time that the next iteration makes arrays\u2014the old and the new ones. So that can also help.\r\n\r\nBeyond that, the resource utilization of XRootD are a bit of a mystery to me. @nsmith- recommends\r\n\r\n```python\r\nuproot.open.defaults[\"xrootd_handler\"] = uproot.source.xrootd.MultithreadedXRootDSource\r\n```\r\n\r\nand this should probably become the default. The current default does a vector-read, which has caused a number of problems.\r\n\r\nBeyond that, maybe break your work up into smaller processes. (That's what I'd do if I run out of all other options.)\r\n\r\nI'm going to convert this into a Discussion because it's not really a bug, that I know of.",
     "createdAt":"2021-03-03T22:14:35Z",
     "number":427452,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ryuwd"
        },
        "body":"Hi @jpivarski,\r\n\r\nThanks for the detailed explanation and your recommendations, it's really appreciated.\r\n\r\n> MemoryError and std::bad_alloc are pretty similar: the first is Python's exception, the second is C++'s exception.\r\n\r\nAh, I naively thought that if I was still dealing with an OOM problem that python would throw another `MemoryError`. It didn't occur to me that it could also present as std::bad_alloc. Thanks!\r\n\r\nFor some context: I was running on Scientific Linux 7 in an interactive job on a cluster, the virtual memory constraint in this case was 16GB, with 8 cores available. I guess I didn't notice this problem when I was running this on my local PC, as there were a lot of allocations but no complaints from the OS causing an allocation error, since there was plenty of swap (in contrast to the hard 16GB Vmem limit on the cluster).\r\n\r\n> Actually, thinking about this more deeply, the arrays you make in one step of iteration (mask, df, etc.) are still in scope when the next iteration starts, so either put all the logic in the body of the loop or explicitly del mask, del df, etc. when done, maybe in addition to gc.collect().\r\n\r\nThanks for pointing this out! I moved the operations into a function. I also implemented all the other suggestions (`library=\"np\"`, etc) and now the virtual memory doesn't exceed the Vmem upper-limit.\r\n\r\nI realised that a previous call earlier on in this code to `uproot.lazy` was filling up close to my 16GB available Vmem, although I specified 500 MB as the array cache size. I was running `ak.sum` over 223 doubles in the tree across the 223 files (i.e. 1 entry per file, but these are the same files holding the tree with 35+ columns), so I guess that something is going on with XRootD there.\r\nWhen I changed this code to use `uproot.iterate(..., library='np')` with `gc.collect` the usage went right down. Now my Vmem usage doesn't exceed 1.5 GB over the whole process. \r\n\r\nAnyway - thanks! The suggestions helped a lot.\r\n",
        "createdAt":"2021-03-04T00:04:57Z",
        "number":427685
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-03-03T21:54:40Z",
  "number":291,
  "title":"uproot4.iterate with XRootD - std::bad_alloc?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/291"
 },
 {
  "author":{
   "login":"andriish"
  },
  "body":"Dear uproot authors,\r\n\r\nwould you consider adding a simple C++ interface to uproot, so it would be possible to read  ROOT files from C++ using only python  (CPython)+ uproot? This could be useful for very simple plain trees.\r\n\r\nBest regards,\r\n\r\nAndrii",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"The right solution depends on your motivation:\r\n\r\n   * Is it about ease of use? If so, RDataFrame makes it easy to access simple TTrees (and more complex ones, but especially the simple ones).\r\n   * Are you looking for a way to get the data in arrays, rather than one entry at a time? As arrays, what would be your preference: `std::vector`, `std::shared_ptr<void>`, `ROOT::RVec`, ...?\r\n   * Is it about speed of deserialization? If so, this would argue for ROOT to provide an interface for the BulkIO deserialization methods that are already implemented.\r\n   * is it about dependencies? Is there an advantage to having CPython as a dependency, rather than ROOT? If so, why? (CPython is pretty big, too.) Is it about being able to embed it in an executable or library statically, to avoid version conflicts? Or are you planning to dynamically link to libpython, with the assumption that it's available everywhere?\r\n\r\nJust trying to understand the request. Thanks!",
     "createdAt":"2021-03-05T13:52:41Z",
     "number":433312,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"DraTeots"
        },
        "body":"> Is there an advantage to having CPython as a dependency, rather than ROOT? If so, why? \r\n\r\nActually there are advantages: \r\n\r\n1. Python is provided for every system. Most of the time it is just as easy as `apt install python3-devel` to have it ready (and conda comes with headers). **Root is not**. \r\n\r\n2. Even if you need to build your own python, my experience as a package maintainer with python is very good - it just builds and installs most of the time. **Root is not**. Countless hours where spent by HENP community on fixing failing ROOT builds\r\n\r\n3. Python has thorough unit testing and pretty careful API migration, so when you build your app against python the API behaves sane most of the time. **Root is not**. Spectacular C++17 headers behaviour in recent version (where you have to build your application with the C++ standard as root). And countless number errors and crashes (e.g. classics - crash on app exit because of some destructor, because weird memory cache being dumped into the file. Or crashes because improper termination of QT (!) stuff). \r\n\r\n4. Python installation is relatively small and there are projects like micro-python. **Root is not**. You have to install all or nothing. \"Slim\" root installation is still 600mb minimum. For certain devices it is crucial. For container creation it might be relevant. \r\n\r\nSo I would agree that having a c++ that runs python to read/write root files via uproot looks bizarre. But we live in a world where this way actually have advantages over just using ROOT.\r\n",
        "createdAt":"2021-04-23T02:41:39Z",
        "number":647709
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"andriish"
     },
     "body":"Hi @jpivarski ,\r\n\r\nI think that might be useful because of the dependencies. ROOT has too many dependencies which can be properly satisfied only on some systems (RH). That makes some people (not me) allergic to ROOT. Having an option to do simple I/O with a much smaller Python runtime could make things easier.\r\n\r\n>Is it about being able to embed it in an executable or library statically, to avoid version conflicts? Or are you planning to dynamically link to libpython, with the assumption that it's available everywhere?\r\n\r\nOh, that is an interesting option. I haven't thought about it, but it could be interesting.\r\nIn any case, this is not a feature request. I just looked at the option of I/O via uproot into C++ for one specific case and that worked quite straightforward for me. So I thought it could be interesting for other people. \r\n\r\nBest regards and thanks a lot for uproot,\r\n\r\nAndrii\r\n\r\nOther questions:\r\n\r\n>  Is it about ease of use? \r\n\r\nROOT works well for me.\r\n\r\n> Are you looking for a way \r\n\r\nI've tried to do things completely w/o ROOT so used just std::vector.\r\n\r\n> Is it about speed of deserialization \r\n\r\nNo, just reading ROOT files w/o ROOT.\r\n\r\n\r\n\r\n\r\n",
     "createdAt":"2021-03-06T12:55:42Z",
     "number":435821,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"andriish"
     },
     "body":"Hi @jpivarski ,\r\nas that is \"idea\" not a \"feature request\"/\"question\"/etc, I think that makes sense to close it as soon as you find it appropriate.\r\n\r\nBest regards,\r\n\r\nAndrii\r\n",
     "createdAt":"2021-03-08T15:15:21Z",
     "number":450494,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I think this can remain open, since \"Discussions\" are not to-do items like \"Issues.\"\r\n\r\nActually, it looks like the GitHub interface doesn't have a \"close Discussion,\" so it's a moot point. That's good anyway. I want the Discussions to remain open for future reference and updates. (When these things are discussed in Issues, it's somewhat harder to find them after they've been closed, but I need to close completed Issues or I'll have trouble keeping track of what's left to do.)",
        "createdAt":"2021-03-08T16:49:32Z",
        "number":450794
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":3
  },
  "createdAt":"2021-03-05T11:46:22Z",
  "number":299,
  "title":"C++ interface to read simple TTrees",
  "url":"https://github.com/scikit-hep/uproot5/discussions/299"
 },
 {
  "author":{
   "login":"kondratyevd"
  },
  "body":"In `uproot3` I could do something like this:\r\n```python\r\nx = graph._fX\r\ny = graph._fY\r\n```\r\n\r\nBut now that doesn't work, and I'm getting an error:\r\n```\r\n'Model_TGraphErrors_v3' object has no attribute '_fX'\r\n```\r\nWhat is the correct way to do it in `uproot4`? Thanks!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"```python\r\nx = graph.member(\"fX\")\r\ny = graph.member(\"fY\")\r\n```",
     "createdAt":"2021-03-10T03:11:54Z",
     "number":459464,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"kondratyevd"
        },
        "body":"Works, thanks!",
        "createdAt":"2021-03-10T03:23:24Z",
        "number":459520
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-03-10T02:08:21Z",
  "number":300,
  "title":"How to access axis values of TGraphErrors?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/300"
 },
 {
  "author":{
   "login":"HDembinski"
  },
  "body":"I am wondering how smart ArrayBuilder is implemented. Consider the following typical analysis code, where we read a chunk of events, process them with numba and the fill the result in some (boost-)histograms. ArrayBuilder is needed for this, because in the analysis the size of the output array is not predictable.\r\n\r\n```py\r\nfrom numba import njit\r\n\r\n@njit\r\ndef process(builder, a, b, c):\r\n   ...\r\n\r\n\r\n# tree is some tree read with uproot, hist is some histogram\r\nfor a, b, c in tree.iterate([\"a\", \"b\", \"c\"], how=tuple):\r\n    builder = ArrayBuilder()\r\n    process(builder, a, b, c)\r\n    hist.fill(builder.snapshot())\r\n```\r\n\r\nThis should work, but it is not efficient, since ArrayBuilders are created and destroyed in each iteration of the outer loop and allocating memory from the OS is an expensive operation. ArrayBuilder should reuse its accumulated memory similarly to a std::vector on `clear` for the next iteration.\r\n\r\nI read in the docs that it is safe to reuse the builder after calling snapshot. So I was wondering if the following code is smart enough to not accumulate an ever increasing amount of memory:\r\n\r\n```py\r\nbuilder = ArrayBuilder()\r\nfor a, b, c in tree.iterate([\"a\", \"b\", \"c\"], how=tuple):\r\n    process(builder, a, b, c)\r\n    hist.fill(builder.snapshot())\r\n```\r\nI read in the docs that the array returned by builder.snapshot is a memory view. So at least in theory this array once destroyed could call back into its parent to signal the viewed memory is now free again to be used for building the next array.\r\n\r\nSo does it work this way or could it be implemented to work in this way?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"ArrayBuilder code is completely contained within the [src/libawkward/builder](https://github.com/scikit-hep/awkward-1.0/tree/main/src/libawkward/builder) directory (interface in [include/awkward/builder](https://github.com/scikit-hep/awkward-1.0/tree/main/include/awkward/builder)).\r\n\r\nEach ArrayBuilder in Python (or Numba) is an instance of [ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1ArrayBuilder.html) in C++, which is an interface class that holds a tree of [Builder](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1Builder.html) nodes. The first Builder is an [UnknownBuilder](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1UnknownBuilder.html) which represents data of unknown type. If you only call ArrayBuilder's `null` method, the UnknownBuilder just counts nulls, but if you call any other method, such as `integer`, then the UnknownBuilder gets replaced by an [Int64Builder](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1Int64Builder.html) (or whichever). If you had accumulated many nulls, the Int64Builder would be wrapped in an [OptionBuilder](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1OptionBuilder.html). As long as the methods that you call correspond to the Builder tree that's already there, it only appends data to the buffers associated with the existing tree, but if you ever call a surprising method, it _adds to_ the tree to reflect the more complex type. In the most generic situations, you'd end up with a lot of [UnionBuilders](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1UnionBuilder.html), but if you stick to lists, records, and numeric types, the tree-building finishes early in the process and you're just growing buffers. (Some types are considered subtypes of each other: appending a floating point number to integers retroactively turns the integers into floating point, appending a complex number turns them into complex, and adding a field to a record retroactively adds the field to previous instances of the record with missing values up to now. [More detail here.](https://awkward-array.org/how-to-create-arraybuilder.html#records-and-unions))\r\n\r\nThe reason for all this generality is to make `ak.Array([any data goes here])` work. It makes the construction of small examples very easy. But as you can imagine, dynamically checking types and growing trees that consist of vtable-heavy subclasses is not the fastest way to accumulate data.\r\n\r\nEach Builder node in this tree can have zero or more [GrowableBuffers](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1GrowableBuffer.html) associated with it. A GrowableBuffer grows in a similar way to `std::vector` (originally, I was just going to _use_ `std::vector`, but there's a technical issue with that). These are templated by the type of data they can fill (primitive type `T`), so in a different context, they might be fast. A GrowableBuffer holds one `std::shared_ptr<T>` pointing to data, and the `std::shared_ptr` is constructed with a deleter that deletes the array ([array_deleter](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1kernel_1_1array__deleter.html)) when there are no more references to the `std::shared_ptr`. After an initial allocation (default is 1024 items), filling the GrowableBuffer inserts items into the buffer until reaching its maximum, and then it allocates a new, larger `std::shared_ptr<T>` (by default, 1.5 times larger: see [initial and resize arguments](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html)), copies the old data to the new, and lets the old `std::shared_ptr<T>` go out of scope.\r\n\r\nWhen you perform a `snapshot`, the data are not copied: the ArrayBuilder just passes a reference to the new tree of [Content](https://awkward-array.readthedocs.io/en/latest/_static/classawkward_1_1Content.html) nodes that is created. If growing the ArrayBuilder means putting more items into the shared buffers, that's okay because the Content doesn't \"see\" them. (The Content has a fixed `length`, so what happens in its buffers beyond that `length` is invisible.) If the ArrayBuilder appends enough that its GrowableBuffers allocate new buffers, that's okay too, since the Content created by the `snapshot` is an owner of the buffers: they stay in scope if and only if _somebody_ is using them.\r\n\r\nBut this policy has some bad consequences. For instance, if you wanted to use an ArrayBuilder in a way that clears the existing data and starts appending from zero, such an action would have to replace, not reuse, the `std::shared_ptrs` associated with each GrowableBuffer. Reusing the same `std::shared_ptr` could invalidate data that is in some Content somewhere. Also, @ianna discovered that replacing `std::shared_ptrs` with `std::unique_ptrs` would have a noticeable impact on performance. (I don't remember the exact number, but it was something like 10%.) Also also, the buffers given to Content nodes can't be trimmed to their actual size\u2014they're some 1.5\u00d7 multiple of 1024.\r\n\r\nThis choice of policy favors frequent `snapshots` of a long-growing array. That doesn't seem to be the most popular use, so @ianna and I have been talking about changes in policy. Here are all the possibilities we've considered:\r\n\r\n   1. GrowableBuffers own `std::shared_ptrs`, extending them by making exponentially larger ones, and share them with Content at `snapshot` time _(current policy)_.\r\n   2. GrowableBuffers own `std::unique_ptrs`, extending them by making exponentially larger ones, and copy the data into new buffers at `snapshot` time. This makes the `snapshot` more costly, but the ArrayBuilders can now have a `clear` operation that does not need to allocate data (so once a high-water mark is reached, it probably won't have to allocate any more in a long-running process that frequently clears the buffers). It also means that the data owned by the Content nodes can be trimmed to the minimal necessary size, which makes sense because in most uses, the Content are more long-lived than the ArrayBuilder that made them, as the ArrayBuilder is usually just implicit in the [ak.Array](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html)) constructor.\r\n   3. Instead of extending GrowableBuffers by replacing their data with exponentially larger allocations, the way `std::vector` does, perhaps it should allocate equal-size, non-contiguous chunks, pushing to a `std::vector<std::unique_ptr<T>>`. With this policy, a `snapshot` operation _has to_ copy, since the `snapshot` would have to concatenate them for the Content nodes, which need contiguous data.\r\n\r\nThe second policy would be good for frequent clearing and reuse, which I didn't know was a common use, but it appears to be in your case. The third policy might be better for memory management, since a field full of equal-size allocations that get deleted when each ArrayBuilder is done would be less fragmented: new ArrayBuilders can fill in those equal-size slots. Thus, the third policy might be best for situations with many ArrayBuilders making arrays, which I think might be common (It certainly is in our unit tests!)\r\n\r\nIndependent of the problem of finding the best GrowableBuffer policy, there's another performance bottleneck in the way that ArrayBuilder has to discover the type of its data while accumulating. For months, we've been thinking about adding a TypedArrayBuilder that would fix this problem by requiring the type up-front. It could then build the one and only tree for that type, and most commands, such as `begin_list` where an integer is expected, would raise errors. @ianna is implementing this in PR #769, and it looks nearly done.\r\n\r\nAlthough the TypedArrayBuilder knows the type before filling, it doesn't know the type at compile-time, since we have to distribute a wheel for Python that can't instantiate the infinitely many types, so we need something that \"almost compiles\" at runtime. @ianna is implementing this in C++ using AwkwardForth ([arxiv:2102.13516](https://arxiv.org/abs/2102.13516)), which is also intended for communication between Uproot and Awkward Array: it's a low-level language to generate code for (i.e. not writing it by hand) that is optimized for filling buffers quickly. Instructions in AwkwardForth are not quite as fast as compiled C++ (they're about 5 ns per instruction with no ability to do compiler-optimizations to reduce the number of instructions), but some AwkwardForth programs are faster than fetching data from memory, and certainly from disk. Most importantly, they are written at runtime. @ianna's implementation of TypedArrayBuilder has fixed C++ code for the `begin_list`, append `integer`, etc. commands that call into an AwkwardForth VM whose code is generated at runtime, from the type of the array one wishes to build.\r\n\r\nThis talk about generating code at runtime begs the question, \"Why not use a JIT-compiler?\" so here is where I will talk about Numba. AwkwardForth is an alternative to a full JIT-compiler without dependencies\u2014its purpose is to ensure that Awkward Array doesn't strictly require LLVM for all users. But if you _have_ LLVM because you're using JAX or Numba, that's a different story.\r\n\r\nThe non-typed ArrayBuilder is implemented in Numba by having Numba call ArrayBuilder's C++ through function pointers. (The Numba lowering is in [src/awkward/_connect/_numba/builder](https://github.com/scikit-hep/awkward-1.0/blob/main/src/awkward/_connect/_numba/builder.py), ArrayBuilder's function pointer interface is [here](https://github.com/scikit-hep/awkward-1.0/blob/596c05fad785a958a4d8bbcabb86968f091b0808/src/libawkward/builder/ArrayBuilder.cpp#L244-L528).) From Numba's perspective, the ArrayBuilder is a single opaque type, since the types of Numba objects can't change at runtime the way that ArrayBuilder does. From a performance perspective, this means that we gain nothing from Numba's ability to JIT-compile to a specific type, and LLVM can't optimize code that is called through external function pointers.\r\n\r\nThe TypedArrayBuilder concept could help here, but not the C++ implementation linked through function pointers. To really gain a performance advantage from a TypedArrayBuilder in Numba, we would have to use the given type to generate specialized Numba code, which unfortunately means a complete reimplementation. We're going forward with TypedArrayBuilder in C++ now because (a) it has non-Numba applications and (b) it sets the interface for how a TypedArrayBuilder should work, so that implementing a Numba version would be adhering to an already-developed standard. (This \"design work\" is relevant because the TypedArrayBuilder interface is not exactly the same as the ArrayBuilder interface\u2014some commands, like `begin_record`, `end_record`, and `field` become unnecessary in a context where the type is already known.)\r\n\r\nAs a very first step, we should think about Numba-native GrowableBuffers, which would be useful even without the (Typed)ArrayBuilder. You've brought that up before, @HDembinski. As described above, there are at least three ways to do the buffer-growing: (1) replace with exponentially larger buffers and share with `snapshot`, (2) replace with exponentially larger buffers and copy to `snapshot`, and (3) accumulate a list of equal-size buffers and concatenate + copy to `snapshot`. We should probably figure out what is the fastest kind of GrowableBuffer (for the right set of use-cases) before deciding on one to implement in Numba.\r\n\r\nI don't see how a \"changing size array\" concept fits into JAX's view of JIT-compilation, but if you have any ideas there, let me know!\r\n\r\nMost of what I've talked about above presupposes that we change how ArrayBuilder (or at least GrowableBuffer) works to improve performance. In its current state, you have to\r\n\r\n```python\r\nfor a, b, c in tree.iterate([\"a\", \"b\", \"c\"], how=tuple):\r\n    builder = ArrayBuilder()\r\n    process(builder, a, b, c)\r\n    hist.fill(builder.snapshot())\r\n```\r\n\r\nand can't\r\n\r\n```python\r\nbuilder = ArrayBuilder()\r\nfor a, b, c in tree.iterate([\"a\", \"b\", \"c\"], how=tuple):\r\n    process(builder, a, b, c)\r\n    hist.fill(builder.snapshot())\r\n```\r\n\r\nbecause the latter would fill histograms with more and more data (refilling it with what was filled on previous iterations). There's no `clear` operation, and it certainly isn't implicit. If _this_ is the use-case we're optimizing for, I think that GrowableBuffer policy (2) would be the best one, since that policy would make a `clear` operation inexpensive (no new allocations) and reuse memory buffers that have reached their high-water mark, at the expense of making the `snapshot` operation more expensive than it is right now.\r\n\r\nAlso implicit in this example: it looks like your `process` only ever makes unstructured, one-dimensional arrays? If you're histogramming it, then it must be flat. In that case, you really want the GrowableBuffer _without_ any ArrayBuilder, typed or otherwise. If `\"a\"`, `\"b\"`, and `\"c\"` are also flat or rectilinear, then bonus: you can use `library=\"np\"` and get plain NumPy arrays. It all depends on what your `process` does and what the inputs are.\r\n\r\nI think it should be possible to implement a GrowableBuffer in Numba, even without [extending Numba](https://numba.pydata.org/numba-doc/dev/extending/index.html). Each buffer would be a NumPy array in Numba. I don't know if Numba's type system allows you to replace a variable representing an array, but it's possible to make typed lists, and you can replace an element of a list or append to the list. (By replacing a length-1 list, you could implement policy (1) or (2); by appending, you could implement policy (3).) I searched for an example of this, and [this StackOverflow answer](https://stackoverflow.com/a/59393997/1623645) shows how to do it in a `@jitclass`, which is yet another way to make mutable non-arrays in Numba.\r\n\r\nLooking even further into the future, your problem would be more easily solved if histogram objects were recognized by Numba (by writing a [Numba extension](https://numba.pydata.org/numba-doc/dev/extending/index.html) for it). @henryiii and I have talked about that\u2014that would be great. It would be a matter of replacing the boost-histogram with a Numba model consisting of NumPy arrays viewing the histogram's counts and then implementing the `fill` operation as a lowered Numba function. Then you wouldn't even need to fill a large array with data that you're just going to be reducing into a histogram anyway. Writing Numba extensions requires some insider knowledge\u2014it's not fully documented\u2014that I accumulated through successive versions of Awkward Array. I'd be willing to share what I've learned for the purposes of lowering histograms. @henryiii and I have already talked about this and looked at code examples for a day, but that was over a year ago now. If you're ever interested in lowering histograms as a Numba extension, I'm here to help!",
     "createdAt":"2021-03-13T18:28:59Z",
     "number":478711,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-03-13T11:29:30Z",
  "number":307,
  "title":"How to use ArrayBuilder most efficiently?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/307"
 },
 {
  "author":{
   "login":"lkildetoft"
  },
  "body":"Hi! I'm having trouble reading leaves of branches with uproot, where the branch names contain dots.\r\nFor example in my ROOT file, I have the branch \"EventInfoAux.\", which contains a leaf \"eventNumber\". \r\nNormally I would access this with something like:\r\nfile[\"tree/EventInfoAux.eventNumber\"]\r\n\r\nHowever, this is not possible as it results in \r\nfile[\"tree/EventInfoAux..eventNumber\"]\r\n\r\nwhich uproot doesn't understand. Is there an easy solution to this? \r\nThanks!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"tamasgal"
     },
     "body":"Can you show more output from that file and also the error message? What is e.g. `file[\"tree\"].keys()`?",
     "createdAt":"2021-03-26T14:29:18Z",
     "number":534574,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Uproot doesn't do any special interpretation of the dots (`.`), though it does parse by slashes (`/`). ROOT inserts dots in branch names according to its C++ object-splitting rules and Uproot takes a hands-off approach to them.\r\n\r\nYou can check to see if this string is exactly what you get from [TTree.keys](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#keys), possibly with `recursive=False` to narrow in.\r\n\r\nIt's also possible for ROOT to make a file with non-unique branch names. Uproot will pick the _first_ that matches, at each level of hierarchy, if that's relevant. In this case, you need to select them by integer index. You can reverse-index them with [TBranch.index](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#index) (to find out which index to pass).",
     "createdAt":"2021-03-26T14:31:09Z",
     "number":534580,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-03-26T13:59:45Z",
  "number":313,
  "title":"Reading values of branches with dots in names",
  "url":"https://github.com/scikit-hep/uproot5/discussions/313"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"This thread is about writing files with Uproot 4. I've included here everyone who has asked about this, talked about this, or provided feedback on it in Uproot 3. You can \"unwatch\" this thread if you're not interested.\r\n\r\n@reikdas @pcanal @sbinet @oshadura @nsmith- @goi42 @eduardo-rodrigues @chrisburr @hassec @dnadeau-lanl @lindsaybolz @atasattari @lkorley @bfis @masonproffitt @jonr667 @xaratustrah @HDembinski @kratsg @adamdddave @bixel @andrzejnovak @BrutalPosedon @VukanJ @marinang @douglasdavis @kgizdov @matthewfeickert @beojan @Justin-Tan @burneyy @anerokhi @lukasheinrich @henryiii \r\n\r\nPR #320 was a first step\u2014it only adds one test file:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/main/tests/test_0320-start-working-on-ROOT-writing.py\r\n\r\nwhich doesn't actually use Uproot; it reads and writes to a file with basic Python commands (`struct.pack` and `struct.unpack`). It moves the root directory of the file from one seek position to another such that the file contents are effectively the same, but it has to update pointers and the list of free segments to do so (for three different files that started with different numbers of free segments). To be thorough, I put `\"X\"` chars over the original data, to be sure it can't be read anymore.\r\n\r\nThen the same file can not only be read by ROOT, it can be updated by ROOT and the output of that second operation is still valid. Thus, it's a full cycle: the original file was created by ROOT \u2192 updated by Python code \u2192 read back and updated again by ROOT \u2192 read back again by ROOT and Uproot. The original contents and the newly added contents (a TObjString) are both readable in the final state.\r\n\r\nMy conclusion from this is that it will be possible to have an \"update\" function, not just \"recreate\", despite what I've said in the past about that being out of scope (e.g. scikit-hep/uproot3#381, scikit-hep/uproot3#460, scikit-hep/uproot3#530). I haven't written any of that new code, just determined that it will be possible.\r\n\r\nI have all of @reikdas's Uproot 3 work to draw on, and I just came across this:\r\n\r\nhttps://github.com/root-project/root/tree/master/io/doc/TFile\r\n\r\nMany of the fundamental serialization types of ROOT _are_ specified, including the [free segments](https://github.com/root-project/root/blob/master/io/doc/TFile/freesegments.txt) used above. I've gone through and verified these against what we use in Uproot\u2014this is a fantastic resource I wish I'd known about before!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Just an update: we can now create ROOT files with nested subdirectories (and no other kinds of objects!).\r\n\r\nMost importantly, ROOT can open these files and put objects in the subdirectories Uproot made, with Uproot \u2192 ROOT \u2192 Uproot round trips and no corruption. I'm fairly confident that the seek positions and map of free segments are being updated correctly because even adding a subdirectory requires significant movement of data as directory key-lists outgrow their original allocation. Also, ROOT has thankfully been verbose about any mistakes, even recoverable ones.\r\n\r\nA sample test from PR #322:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/2b518fa7f01fa94f539710bfc73fead455ea2877/tests/test_0322-writablefile-infrastructure.py#L65-L107\r\n\r\nNext, I'll work on defining streamers, which will make it possible to store actual objects (starting with `TObjString` for simplicity).\r\n",
     "createdAt":"2021-04-08T00:07:00Z",
     "number":582341,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"As of PR #329, updating files can go full circle: files made by ROOT can be updated by Uproot, the result of which can be updated by ROOT, etc.\r\n\r\nHere, ROOT made the initial file containing a string, Uproot added a subdirectory, ROOT put another string inside that subdirectory, and Uproot-reading shows that everything's there in the end.\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/95bd056afebf33a4b6cf8f01f023ee53d85ed87f/tests/test_0329-update-existing-root-files.py#L17-L43",
     "createdAt":"2021-04-09T00:27:07Z",
     "number":587274,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"liaolibo-jay"
        },
        "body":"Hello, dear jpivarski,\r\nI tried to input some information from NumPy to ROOT by uproot, but it always failed.\r\nAnd in this file, uproot4/tests/test_0329-update-existing-root-files.py, has a module named uproot.writing, but I can't find this module anywhere.  Do you have any complete tutorials?\r\nthanks a lot.",
        "createdAt":"2021-04-25T07:50:08Z",
        "number":655019
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I don't think there has been a deployed release with `uproot.writing` in it yet, but also check this thread for progress: as I've said here, you can't write new objects in a ROOT file yet. Next, I'll be working on TObjString, then histograms after that, then TTrees after that (the case that you need).",
        "createdAt":"2021-04-25T14:37:57Z",
        "number":655792
       },
       {
        "author":{
         "login":"henryiii"
        },
        "body":"And you can always use `uproot3` in the meantime for the writing step.",
        "createdAt":"2021-04-25T14:43:05Z",
        "number":655808
       },
       {
        "author":{
         "login":"liaolibo-jay"
        },
        "body":"ok, thanks a lot",
        "createdAt":"2021-04-27T09:10:47Z",
        "number":663514
       }
      ],
      "totalCount":4
     }
    },
    {
     "author":{
      "login":"agoose77"
     },
     "body":"> This thread is about writing files with Uproot 4. I've included here everyone who has asked about this, talked about this, or provided feedback on it in Uproot 3. You can \"unwatch\" this thread if you're not interested.\r\n> \r\n> @reikdas @pcanal @sbinet @oshadura @nsmith- @goi42 @eduardo-rodrigues @chrisburr @hassec @dnadeau-lanl @lindsaybolz @atasattari @lkorley @bfis @masonproffitt @jonr667 @xaratustrah @HDembinski @kratsg @adamdddave @bixel @andrzejnovak @BrutalPosedon @VukanJ @marinang @douglasdavis @kgizdov @matthewfeickert @beojan @Justin-Tan @burneyy @anerokhi @lukasheinrich @henryiii\r\n> \r\n> PR #320 was a first step\u2014it only adds one test file:\r\n> \r\n> https://github.com/scikit-hep/uproot4/blob/main/tests/test_0320-start-working-on-ROOT-writing.py\r\n> \r\n> which doesn't actually use Uproot; it reads and writes to a file with basic Python commands (`struct.pack` and `struct.unpack`). It moves the root directory of the file from one seek position to another such that the file contents are effectively the same, but it has to update pointers and the list of free segments to do so (for three different files that started with different numbers of free segments). To be thorough, I put `\"X\"` chars over the original data, to be sure it can't be read anymore.\r\n> \r\n> Then the same file can not only be read by ROOT, it can be updated by ROOT and the output of that second operation is still valid. Thus, it's a full cycle: the original file was created by ROOT \u2192 updated by Python code \u2192 read back and updated again by ROOT \u2192 read back again by ROOT and Uproot. The original contents and the newly added contents (a TObjString) are both readable in the final state.\r\n> \r\n> My conclusion from this is that it will be possible to have an \"update\" function, not just \"recreate\", despite what I've said in the past about that being out of scope (e.g. [scikit-hep/uproot3#381](https://github.com/scikit-hep/uproot3/issues/381), [scikit-hep/uproot3#460](https://github.com/scikit-hep/uproot3/issues/460), [scikit-hep/uproot3#530](https://github.com/scikit-hep/uproot3/issues/530)). I haven't written any of that new code, just determined that it will be possible.\r\n> \r\n> I have all of @reikdas's Uproot 3 work to draw on, and I just came across this:\r\n> \r\n> https://github.com/root-project/root/tree/master/io/doc/TFile\r\n> \r\n> Many of the fundamental serialization types of ROOT _are_ specified, including the [free segments](https://github.com/root-project/root/blob/master/io/doc/TFile/freesegments.txt) used above. I've gone through and verified these against what we use in Uproot\u2014this is a fantastic resource I wish I'd known about before!\r\n\r\nIt's very interesting to see such low level interaction with the root format. Thanks for sharing this!",
     "createdAt":"2021-04-13T14:01:52Z",
     "number":604991,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Just to keep this thread updated, I managed to get back to Uproot file-writing and added TStreamerInfo handling in PR #341. It is still the case that you can't write any objects to the file (other than subdirectories of subdirectories), but TStreamerInfo is a prerequisite for objects because a type's serialization must be described for it to be readable.\r\n\r\nAs usual, everything is being tested in \"volleys\"\u2014a file is updated by ROOT, then by Uproot, then by ROOT again, etc. to ensure that Uproot is giving ROOT data that it can not only understand, but manipulate.",
     "createdAt":"2021-04-22T00:43:48Z",
     "number":642710,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Today's update: Uproot can now _copy_ objects from one ROOT file into another, taking all the TStreamerInfos that would be necessary to interpret those objects.\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/191274763de98ed9cf9ac1eb9474be53b087d1b5/tests/test_0345-bulk-copy-method.py#L34-L36\r\n\r\nThis was the first step because it's a literal byte-for-byte copy of the original data, without interpretation (other than getting its classname to make a dependency tree of all the TStreamerInfos, but the classname is in the TKey, so the object data do not need to be interpreted). In ROOT-speak, this is a \"fast copy,\" because it doesn't require any decompression and recompression.\r\n\r\nThe\r\n\r\n```python\r\ndestination_file[\"name\"] = object_to_write\r\n```\r\n\r\nsyntax of Uproot 3 will be reimplemented, but it requires knowledge of how to serialize the `object_to_write`. The first of these will be TObjString, for simplicity, followed by histograms. (I'm heavily relying on @reikdas's code for that!)\r\n\r\nThe `copy_from` method will remain a better way to copy many objects, though, because it knows the full set of objects you want to copy. Its second argument is a name filter (like the one you can pass to [ReadOnlyDirectory.keys](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyDirectory.html#keys)), so you can pass a list of paths, paths with wildcards, regular expressions, or a function that returns True or False for each name. Therefore, it knows the full set of TStreamerInfos to copy, the full set of TDirectories to make, and how many items will go in each, so that the TDirectories are allocated with a large enough size to not need to be reallocated. Also, the data-reading is collected into a single HTTP/XRootD request, if the server supports multi-part GET or vector_read.\r\n\r\nWe'll probably want something similar for the categorical-axis histograms in Coffea and Boost-Histogram/Hist, so that they can be translated into a directory of conventional histograms without reallocating the directory as it grows in size.",
     "createdAt":"2021-04-23T19:58:57Z",
     "number":651724,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"archiron"
        },
        "body":"Hi, I tried those lines to make a copy but tell if I'm wrong :\r\nit only copy the name of the directory ?\r\nI tried with : `dest.copy_from(source, \"DQMData/Run 1/EgammaV\")` to have a copy of EgammaV but there is only the name of the directory which was created.\r\n",
        "createdAt":"2021-11-22T13:09:57Z",
        "number":1681392
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"It should be recursively copying all of the objects. I'm on the fence about whether to support `copy_from` or not: it was an early stepping-stone to writing data because it doesn't require Uproot to understand the contents of what it's copying, yet it exercised some of the machinery of writing. It might be useful for copying a lot of data quickly (because it doesn't interpret), but it's another code path that needs to be kept alive and answer questions about usage.\r\n\r\nThe tests that I put in when `copy_from` was first implemented still work\u2014have you found a case that doesn't? If so, how does it differ from the tests? Sorry, I mean \"test\" (singular):\r\n\r\n```bash\r\n% fgrep copy_from tests/*.py\r\ntests/test_0345-bulk-copy-method.py:            dest.copy_from(source, \"subdir/hist\")\r\n```\r\n\r\nIs this an operation that you need to do\u2014fast bulk copying? If it's not something you actively need and the `copy_from` function is indeed broken, my quick fix would be to hide the function and only bring it back (with enough care to ensure that all the cases work) if someone says they need it.\r\n\r\nWriting new data to a file\u2014with examples farther down in this thread or in the [official documentation](https://uproot.readthedocs.io/en/latest/basic.html#opening-a-file-for-writing), are fully supported.",
        "createdAt":"2021-11-22T13:55:31Z",
        "number":1681632
       },
       {
        "author":{
         "login":"archiron"
        },
        "body":"I only want to copy all the branch (here DQMData/Run 1/EgammaV) into a new file. \r\n\r\nPerhaps it's me that does not use uproot correctly. I use :\r\n\r\n```\r\nfileName1 = rootFilesPath + '/' + \"DQM_V0001_R000000001__RelValZEE_13__CMSSW_10_6_20-106X_mc2017_realistic_v9-v1__DQMIO.root\"\r\nfileName2 = rootFilesPath + '/' + \"result.root\"\r\n\r\nwith up4.open(fileName1) as source:\r\n    with up4.writing.recreate(fileName2) as dest:\r\n        dest.copy_from(source, \"DQMData/Run 1/EgammaV\")\r\n```\r\nbut the only result I have into the new file is empty directories : DQMData/Run 1/EgammaV.\r\n\r\n",
        "createdAt":"2021-11-23T09:54:53Z",
        "number":1686475
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Here's a reason why `copy_from` should perhaps be retired: it _won't_ copy the data associated with a TTree. What it does is recursively copy self-contained objects, and TTrees refer to data outside of themselves (the TBaskets), so by its name, it gave a misleading impression that it would copy the TTrees when it can't.\r\n\r\nTo copy the TTrees, you'll need to iterate over the first file and write to the new file. Enough people have asked about copying TTrees that we ought to have more tools for doing that (exact copying, copying and renaming branches, dropping branches, adding branches...).",
        "createdAt":"2021-11-23T15:35:28Z",
        "number":1688428
       }
      ],
      "totalCount":4
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Today's update: PR #349 adds the ability to write TObjStrings from scratch (they don't need to come from another file), assigning the TStreamerInfo appropriately. With this update, ROOT files can be used as persistent dicts of strings:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/1ce55e34285be232f5255730e090b83189e03061/tests/test_0349-write-TObjString.py#L17-L29\r\n\r\nNext is histograms.",
     "createdAt":"2021-05-03T23:01:04Z",
     "number":690691,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"After 3 months of not being able to get to this, I managed to start working on ROOT file writing in Uproot again. PR #405 writes a histogram. It will have to be generalized beyond TH1F version 3, but now I have a method for filling in the missing serialization code.\r\n\r\n```python\r\ndef test(tmp_path):\r\n    original = os.path.join(tmp_path, \"original.root\")\r\n    newfile = os.path.join(tmp_path, \"newfile.root\")\r\n\r\n    f1 = ROOT.TFile(original, \"recreate\")\r\n    h1 = ROOT.TH1F(\"h1\", \"title\", 8, -3.14, 2.71)\r\n    h1.SetBinContent(0, 0.0)\r\n    h1.SetBinContent(1, 1.1)\r\n    h1.SetBinContent(2, 2.2)\r\n    h1.SetBinContent(3, 3.3)\r\n    h1.SetBinContent(4, 4.4)\r\n    h1.SetBinContent(5, 5.5)\r\n    h1.SetBinContent(6, 6.6)\r\n    h1.SetBinContent(7, 7.7)\r\n    h1.SetBinContent(8, 8.8)\r\n    h1.SetBinContent(9, 9.9)\r\n    h1.Write()\r\n    f1.Close()\r\n\r\n    with uproot.open(original) as fin:\r\n        h2 = fin[\"h1\"]\r\n\r\n        with uproot.recreate(newfile) as fout:\r\n            fout[\"h1\"] = h2\r\n\r\n    f3 = ROOT.TFile(newfile)\r\n    h3 = f3.Get(\"h1\")\r\n    assert h3.GetBinContent(0) == pytest.approx(0.0)\r\n    assert h3.GetBinContent(1) == pytest.approx(1.1)\r\n    assert h3.GetBinContent(2) == pytest.approx(2.2)\r\n    assert h3.GetBinContent(3) == pytest.approx(3.3)\r\n    assert h3.GetBinContent(4) == pytest.approx(4.4)\r\n    assert h3.GetBinContent(5) == pytest.approx(5.5)\r\n    assert h3.GetBinContent(6) == pytest.approx(6.6)\r\n    assert h3.GetBinContent(7) == pytest.approx(7.7)\r\n    assert h3.GetBinContent(8) == pytest.approx(8.8)\r\n    assert h3.GetBinContent(9) == pytest.approx(9.9)\r\n```",
     "createdAt":"2021-08-04T21:53:48Z",
     "number":1132135,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"And now any histogram (descendants of TH1, excluding TH1K, TH2Poly, TProfile2Poly, and TGLTH3Composition) can be written to a new file (`uproot.recreate`) or added to an existing file (`uproot.update`): PR #405.\r\n\r\nAs a reminder, that's all of these:\r\n\r\n<img src=\"https://root.cern.ch/doc/master/classTH1__inherit__graph_org.svg\" width=\"100%\" >\r\n\r\nSo, all the 1, 2, and 3D histograms with profiles, but no TEfficiency. There are also functions like [uproot.writing.to_th1x](https://github.com/scikit-hep/uproot4/blob/73b5c473c6b1cf8428ed8ef4f43a5e95cac207ca/src/uproot/writing.py#L1237-L1271) to create these writable objects from data, rather than from another file. The idea is that [uproot.writing.to_writable](https://github.com/scikit-hep/uproot4/blob/73b5c473c6b1cf8428ed8ef4f43a5e95cac207ca/src/uproot/writing.py#L1001-L1018), which is called every time this happens:\r\n\r\n```python\r\noutfile = uproot.recreate(\"outfile.root\")\r\noutfile[\"name_of_histogram\"] = xyz\r\n```\r\n\r\nwill use these methods to attempt to convert `xyz` into something writable. The first `xyz` types that will be recognized are tuples of NumPy arrays, so that they can be auto-converted into Uproot histograms for writing. The second types that will be recognized are hist and boost-histogram objects (@henryiii and @amangoel185).\r\n\r\nFor PyROOT objects (conversion in both directions), I think we should do something more general, serializing and deserializing `uproot.Model` objects as TMessages, which PyROOT objects can also serialize and deserialize, such that any PyROOT object can become an Uproot object, and any _writable_ Uproot object can become a PyROOT object (\"writable\" is a subset that only includes TObjString and histograms right now).",
        "createdAt":"2021-08-06T03:12:08Z",
        "number":1137406
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Moving along, PR #406 creates an empty TTree (with a TBranch, but no TBasket data). This has to follow a different code path from histograms and other \"normal\" objects because TTrees can create and manage other objects (TBaskets), and they have to be replaceable, if the number of TBaskets exceeds the originally allocated number. (A TTree has to communicate with the TDirectory that contains it to replace the TTree metadata with larger metadata. The TBaskets themselves never get copied or moved.)\r\n\r\n```python\r\n    with uproot.recreate(newfile, compression=None) as fout:\r\n        # internal interface; not wrapped up in a high-level function yet\r\n        tree = fout._cascading.add_tree(\r\n            fout._file.sink, \"t1\", \"title\", {\"branch1\": np.float64}\r\n        )\r\n\r\n    f2 = ROOT.TFile(newfile)\r\n    t2 = f2.Get(\"t1\")\r\n\r\n    assert t2.GetName() == \"t1\"\r\n    assert t2.GetTitle() == \"title\"\r\n\r\n    assert t2.GetBranch(\"branch1\").GetName() == \"branch1\"\r\n    assert t2.GetBranch(\"branch1\").GetTitle() == \"branch1/D\"\r\n\r\n    assert t2.GetBranch(\"branch1\").GetLeaf(\"branch1\").GetName() == \"branch1\"\r\n    assert t2.GetBranch(\"branch1\").GetLeaf(\"branch1\").GetTitle() == \"branch1\"\r\n\r\n    assert t2.GetLeaf(\"branch1\").GetName() == \"branch1\"\r\n    assert t2.GetLeaf(\"branch1\").GetTitle() == \"branch1\"\r\n```\r\n\r\nThe TTree above is byte-for-byte identical with one made this way:\r\n\r\n```python\r\n    f1 = ROOT.TFile(original, \"recreate\")\r\n    f1.SetCompressionLevel(0)\r\n\r\n    t1 = ROOT.TTree(\"t1\", \"title\")\r\n    d1 = array.array(\"d\", [0.0])\r\n    d2 = array.array(\"d\", [0.0])\r\n    t1.Branch(\"branch1\", d1, \"branch1/D\")\r\n\r\n    t1.Write()\r\n    f1.Close()\r\n```",
     "createdAt":"2021-08-07T01:13:00Z",
     "number":1141354,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Today's update (PR #408): we can now write NumPy arrays to a TTree in Uproot and read them back in ROOT.\r\n\r\n```python\r\n    newfile = os.path.join(tmp_path, \"newfile.root\")\r\n\r\n    b1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n    b2 = [0.0, 1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9]\r\n\r\n    with uproot.recreate(newfile, compression=None) as fout:\r\n        tree = fout.mktree(\"t\", \"title\", {\"b1\": np.int32, \"b2\": np.float64})   # analogy with mkdir\r\n        tree.extend({\"b1\": b1, \"b2\": b2})   # iterables are cast to arrays of the appropriate type\r\n        tree.extend({\"b1\": b1, \"b2\": b2})\r\n\r\n    # read back in ROOT\r\n    f1 = ROOT.TFile(newfile)\r\n    t1 = f1.Get(\"t\")\r\n    assert t1.GetEntries() == len(b1) + len(b1)\r\n    assert [x.b1 for x in t1] == b1 + b1\r\n    assert [x.b2 for x in t1] == b2 + b2\r\n\r\n    # read back in Uproot\r\n    with uproot.open(newfile + \":t\") as t2:\r\n        assert t2.num_entries == len(b1) + len(b1)\r\n        assert t2[\"b1\"].array().tolist() == b1 + b1\r\n        assert t2[\"b2\"].array().tolist() == b2 + b2\r\n```\r\n\r\nStill, there are limitations:\r\n\r\n  * Each `extend` makes a TBasket in every TBranch, and that number can't exceed 10. When a TTree exceeds its initial capacity for baskets (10), it will have to rewrite itself to become larger, and that's a TODO.\r\n  * Baskets are not compressed.\r\n  * We can't add data to TTrees from preexiting files (`uproot.update`) yet.\r\n  * No jagged arrays yet. Eventually, the syntax will be just\r\n\r\n```python\r\nfout[\"treename\"] = awkward_array\r\nfout[\"treename\"].extend(another_awkward_array)\r\n```\r\n\r\nbut that's to come.",
        "createdAt":"2021-08-10T23:39:09Z",
        "number":1156374
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"## Uproot file-writing is in a usable state!\r\n\r\nAs of PR #409 (which is now merged into main), Uproot's file-writing code can be considered alpha quality: ready for testers, albeit with no documentation and you'd be the _first_ to test it. Fortunately for documentation, there's not much to explain.\r\n\r\nThe interface for writing a TTree is\r\n\r\n```python\r\nwith uproot.recreate(\"filename.root\") as output_file:\r\n    output_file[\"tree_name\"] = {\"branch1\": array1, \"branch2\": array2, \"branch3\": array3}\r\n```\r\n\r\nwhere `array1`, `array2`, `array3`, etc. are NumPy arrays of the same length (maybe different dtypes). This will create a TTree named `\"tree_name\"` with the array data as branches, the opposite of `input_file[\"tree_name/branch1\"].array()`.\r\n\r\nOnce you've created a TTree, you can add more data to it with\r\n\r\n```python\r\n    output_file[\"tree_name\"].extend({\"branch1\": array1, \"branch2\": array2, \"branch3\": array3})\r\n```\r\n\r\nThe set of branch names needs to be the same (dtypes will be converted if they don't match the first batch), and again, the lengths of all of these arrays must be equal. The file must still be open. Each call to `extend` adds another TBasket to every TBranch, so be sure to extend the TTree in large batches, like MB at least, to ensure that the file has big enough TBaskets for good performance. (Appending length-1 arrays would lead to the worst possible read performance.)\r\n\r\nThe interface for writing a histogram is\r\n\r\n```python\r\n    output_file[\"hist_name\"] = np.histogram(some_data)\r\n```\r\n\r\nwhere the right-hand-side is some \"recognized type,\" a type that can be converted into a histogram. The first such type is a 2-tuple of NumPy arrays\u2014what `np.histogram` creates.\r\n\r\n**Fancy stuff:**\r\n\r\n   * The name that you assign to can be a `\"deeply/nested/directory\"`. If you want to make a directory without putting any objects in it, you can use `output_file.mkdir(\"name\")` (and `output_file.mktree(\"name\", \"title\", {\"branch\": dtype})` for empty TTrees).\r\n   * `uproot.recreate` creates a new ROOT file (overwriting anything that previously existed with that name), and `uproot.update` opens a previously existing ROOT file, so that you can add more TTrees and histograms or delete objects. The original ROOT file could have been made by ROOT or by Uproot.\r\n   * You can delete objects (`uproot.recreate` or `uproot.update` mode) with `del output_file[\"object_name\"]`.\r\n\r\n**Temporary limitations:**\r\n\r\n   * Input arrays for making TTrees have to be one-dimensional NumPy arrays. Multidimensional NumPy, single-jagged Awkward, and Pandas DataFrames are TODO items.\r\n   * Only 2-tuples of NumPy arrays are currently recognized as histograms. 3-tuples (TH2) and 4-tuples (TH3) are TODO items, as well as boost-histogram and hist objects.\r\n   * PyROOT objects are not yet recognized entities, but they will be. (They'll be serialized and deserialized through a TMessage.) The same for reading: all Uproot Models will get a `to_pyroot()` method to convert themselves into PyROOT objects (through a TMessage).\r\n   * Nothing is compressed yet, but in principle, the same compression libraries (zlib, lz4, lzma, zstd) can be used to compress as to decompress.\r\n\r\n**Permanent limitations:**\r\n\r\n   * Once a histogram is written to the file, there is no modifying it. You can `del output_file[\"hist_name\"]` and assign it again, or just assign it again and get a new cycle number (which does not delete the first with that name). If you try to look at `output_file[\"hist_name\"]` after writing it, you'll get a read-only histogram object.\r\n   * When you add a TTree to a file, it is always a WritableTree; looking at it again with `output_file[\"tree_name\"]` gives you the same writable object back. This writable object does not have all the methods and properties that a read-only TTree has.\r\n   * You cannot extract a read-only TTree from a file opened with `uproot.recreate` or `uproot.update`. TTrees are either write-only or read-only, depending on how the file was opened.\r\n\r\n**Probably permanent limitation:**\r\n\r\n   * Can only write to local files. I don't know what it would take to write through XRootD, and I don't know if it batches data for performance. If it does any batching, that batching would have limited performance if you interleave reads and writes of a `uproot.update` file.",
     "createdAt":"2021-08-12T00:49:42Z",
     "number":1161308,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Update from PR #412: I fixed some of the \"temporary limitations\" above, including usability feedback from @alexander-held (on https://gitter.im/Scikit-HEP/uproot):\r\n\r\n   - [x] `rootfile[\"tree\"] = {\"branch\": [ ... simple list ... ]}` now converts the list to NumPy and writes it into the ROOT TTree as a branch. Any non-Mapping Iterable that _can_ be converted into a NumPy array with integer or floating point type (i.e. no `dtype(\"O\")`) _will be_ converted into a NumPy array, and from there into a branch of a ROOT TTree. This includes multidimensional arrays.\r\n   - [x] multidimensional NumPy arrays (i.e. _not jagged_ arrays, since those aren't NumPy or have `dtype(\"O\")`, which we strongly avoid)\r\n   - [x] NumPy [structured arrays](https://numpy.org/doc/stable/user/basics.rec.html). These aren't written to ROOT as leaf-lists but as simple branches; Uproot will only write fully split data.\r\n   - [x] Pandas DataFrames; you can say `rootfile[\"df\"] = df` and save your Pandas DataFrames in a ROOT file. The index has to be numeric (`df.index.is_numeric()`). That leaves open the possibility of identifying a DataFrame with an interval index as a histogram and a multiindex as a jagged array.\r\n   - [x] jagged arrays (that will be a PR on its own)\r\n   - [x] the NumPy forms (tuples of arrays) of 1D, 2D, and 3D histograms are all recognized, both the [np.histogram2d](https://numpy.org/doc/stable/reference/generated/numpy.histogram2d.html) format (flat tuple of entries and bin edges) and the [np.histogramdd](https://numpy.org/doc/stable/reference/generated/numpy.histogramdd.html) format (nested tuple for the bin edges). In particular, I tested all of the quantities for getting the stats box right.\r\n   - [x] writing boost-histogram and hist objects to ROOT files. @henryiii and @amangoel185, I'd like to talk sometime to share knowledge on that; as a preview from my side, [this is what's involved in recognizing the 1D NumPy histogram case](https://github.com/scikit-hep/uproot4/blob/3d265262a9760e916bd5f7c2789f9aa8b862c005/src/uproot/writing.py#L2515-L2568). I don't think boost-histogram/hist would be much more complicated.\r\n   - [x] writing generic PyROOT objects\r\n   - [x] compression\r\n\r\n**Now just for showing off, behold:**\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame({\"x\": [1, 2, 3, 4, 5], \"y\": [1.1, 2.2, 3.3, 4.4, 5.5]})\r\n>>> df\r\n   x    y\r\n0  1  1.1\r\n1  2  2.2\r\n2  3  3.3\r\n3  4  4.4\r\n4  5  5.5\r\n>>>\r\n>>> import uproot\r\n>>> with uproot.recreate(\"output.root\") as rootfile:\r\n...     rootfile[\"tree\"] = df\r\n... \r\n>>>\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"output.root\")\r\n>>> t = f.Get(\"tree\")\r\n>>> t.Scan()\r\n************************************************\r\n*    Row   * index.ind *       x.x *       y.y *\r\n************************************************\r\n*        0 *         0 *         1 *       1.1 *\r\n*        1 *         1 *         2 *       2.2 *\r\n*        2 *         2 *         3 *       3.3 *\r\n*        3 *         3 *         4 *       4.4 *\r\n*        4 *         4 *         5 *       5.5 *\r\n************************************************\r\n5\r\n>>> t.Print()\r\n******************************************************************************\r\n*Tree    :tree      :                                                        *\r\n*Entries :        5 : Total =            2133 bytes  File  Size =       2144 *\r\n*        :          : Tree compression factor =   1.00                       *\r\n******************************************************************************\r\n*Br    0 :index     : index/L                                                *\r\n*Entries :        5 : Total  Size=        609 bytes  File Size  =        112 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    1 :x         : x/L                                                    *\r\n*Entries :        5 : Total  Size=        589 bytes  File Size  =        108 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    2 :y         : y/D                                                    *\r\n*Entries :        5 : Total  Size=        589 bytes  File Size  =        108 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n```\r\n\r\n(The DataFrame index becomes a TBranch named \"index,\" unless overwritten by a DataFrame column named \"index.\")\r\n\r\n**Thus, a ROOT file is a handy way to store Pandas DataFrames.**",
     "createdAt":"2021-08-18T00:21:53Z",
     "number":1197840,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"PR #414 adds jagged array writing.\r\n\r\nJagged arrays present a problem of interface. Although we have a choice between writing them as dynamic length arrays (e.g. `\"Muon_pt[NMuon]/F\"`) and writing them as `std::vectors`, dynamic length arrays are both more efficient on disk and faster for Uproot to read and write because each entry does not have a header, making it possible to just dump or load numerical data without inserting or removing `std::vector` headers at non-uniform places in the byte stream. So we want dynamic length arrays, but these require some other branch to contain a counter (the `\"NMuon/I\"` that goes with the `\"Muon_pt[NMuon]/F\"`). One array becomes two branches.\r\n\r\nWhat do we name these extra branches? How do we keep them from colliding? How do we avoid creating a lot of them?\r\n\r\nFor the first two problems, I added `counter_name` and `field_name` arguments to `WritableDirectory.mktree`, which are functions that determine how branch names will be built. Their defaults are the NanoAOD conventions:\r\n\r\n   * `counter_name = lambda counted: \"N\" + counted`: i.e. an array named `Muon` gets a counter named `NMuon`\r\n   * `field_name = lambda outer, inner: inner if outer == \"\" else outer + \"_\" + inner`: i.e. a record array named `Muon` with fields `pt`, `eta`, phi` becomes branches `Muon_pt`, `Muon_eta`, `Muon_phi`.\r\n\r\nIf you need different names or you need to avoid a name collision, you can pass your own lambda functions.\r\n\r\nFor the last problem of avoiding many of them, I made sure that we can write Awkward record arrays as well as numerical or jagged-numerical arrays. This is a direct extension of the `rootfile[\"tree\"] = df` DataFrame writing described in the comment above. If you have a lot of jagged arrays with the same number of values per entry each, you can [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html) them into a single array.\r\n\r\n```python\r\n>>> import uproot\r\n>>> import awkward as ak\r\n>>> one = ak.Array([[1, 2, 3], [], [4, 5]])\r\n>>> two = ak.Array([[1.1, 2.2, 3.3], [], [4.4, 5.5]])\r\n>>> three = ak.Array([[100], [200, 200], [300, 300, 300]])\r\n>>> with uproot.recreate(\"output.root\", compression=None) as rootfile:\r\n...     rootfile[\"tree1\"] = ak.zip({\"one\": one, \"two\": two})\r\n...     rootfile[\"tree2\"] = {\"A\": ak.zip({\"one\": one, \"two\": two}), \"B\": three}\r\n... \r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"output.root\")\r\n>>> tree1 = f.Get(\"tree1\")\r\n>>> tree2 = f.Get(\"tree2\")\r\n```\r\n\r\nThe first TTree, `\"tree1\"`, takes the record array to be the complete set of branches, adding only a counter branch named `\"N\"`.\r\n\r\n```python\r\n>>> tree1.Print()\r\n******************************************************************************\r\n*Tree    :tree1     :                                                        *\r\n*Entries :        3 : Total =            2145 bytes  File  Size =       2161 *\r\n*        :          : Tree compression factor =   1.00                       *\r\n******************************************************************************\r\n*Br    0 :N         : N/I                                                    *\r\n*Entries :        3 : Total  Size=        550 bytes  File Size  =         81 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    1 :one       : one/L                                                  *\r\n*Entries :        3 : Total  Size=        685 bytes  File Size  =        131 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    2 :two       : two/D                                                  *\r\n*Entries :        3 : Total  Size=        685 bytes  File Size  =        131 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n>>> [x.N for x in tree1]\r\n[3, 0, 2]\r\n>>> [list(x.one) for x in tree1]\r\n[[1, 2, 3], [], [4, 5]]\r\n>>> [list(x.two) for x in tree1]\r\n[[1.1, 2.2, 3.3], [], [4.4, 5.5]]\r\n```\r\n\r\nThe second TTree, `\"tree2\"`, takes the record array to be under the names `\"A\"` and `\"B\"`. The counters named `\"NA\"` and `\"NB\"` distinguish the arrays with different numbers of items per entry.\r\n\r\n```python\r\n>>> tree2.Print()\r\n******************************************************************************\r\n*Tree    :tree2     :                                                        *\r\n*Entries :        3 : Total =            3349 bytes  File  Size =       3395 *\r\n*        :          : Tree compression factor =   1.00                       *\r\n******************************************************************************\r\n*Br    0 :NA        : NA/I                                                   *\r\n*Entries :        3 : Total  Size=        555 bytes  File Size  =         82 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    1 :A_one     : A_one/L                                                *\r\n*Entries :        3 : Total  Size=        697 bytes  File Size  =        133 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    2 :A_two     : A_two/D                                                *\r\n*Entries :        3 : Total  Size=        697 bytes  File Size  =        133 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    3 :NB        : NB/I                                                   *\r\n*Entries :        3 : Total  Size=        555 bytes  File Size  =         82 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    4 :B         : B/L                                                    *\r\n*Entries :        3 : Total  Size=        685 bytes  File Size  =        137 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n>>> [x.NA for x in tree2]\r\n[3, 0, 2]\r\n>>> [list(x.A_one) for x in tree2]\r\n[[1, 2, 3], [], [4, 5]]\r\n>>> [list(x.A_two) for x in tree2]\r\n[[1.1, 2.2, 3.3], [], [4.4, 5.5]]\r\n>>> \r\n>>> [x.NB for x in tree2]\r\n[1, 2, 3]\r\n>>> [list(x.B) for x in tree2]\r\n[[100], [200, 200], [300, 300, 300]]\r\n```\r\n\r\nI know that TTrees can have nested branches and this could be a good application of nested branches (i.e. distinguish them as `\"A/N\"`, `\"A/one\"`, `\"A/two\"`, `\"B/N\"`, and `\"B/three\"`), but users of TTrees, particularly at the level of type-complexity that Uproot-writing will ever support, have been opting for naming conventions in non-nested branches. Also, that's much easier for me.\r\n\r\nNow that this is done, all that's left is\r\n\r\n  - [x] and (one more) refactor the writing.py and _writing.py modules into submodules; they're too large\r\n  - [ ] writing boost-histogram and hist objects to ROOT files\r\n  - [x] writing generic PyROOT objects (see below)\r\n  - [x] compression (see below)\r\n  - [x] documentation",
     "createdAt":"2021-08-19T18:22:48Z",
     "number":1208199,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"(The refactoring task is done in #415.)",
        "createdAt":"2021-08-19T20:39:07Z",
        "number":1208775
       },
       {
        "author":{
         "login":"jrueb"
        },
        "body":"Really nice!\r\nYou wrote that the `counter_name` defaults to the NanoAOD convention, but in NanoAOD the prefix of the counter is always a lower case 'n', while it is upper case in uproot right now if I understand correctly. Where does the difference come from?",
        "createdAt":"2021-08-20T08:51:25Z",
        "number":1210609
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"You're right! I don't know where I got that from. I'll change the default `counter_name` to `lambda counted: \"n\" + counted`.",
        "createdAt":"2021-08-20T14:30:30Z",
        "number":1211993
       },
       {
        "author":{
         "login":"masonproffitt"
        },
        "body":"Is it possible to put an option in somewhere to write `std::vector`s instead of dynamic length arrays? Most existing ROOT analysis code that I've seen expects `std::vector`s, so having this as an option would be a huge benefit for compatibility with other code.",
        "createdAt":"2021-08-27T17:13:17Z",
        "number":1245414
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Ach, no, this was not added. And now I remember that it was requested, in #257!\r\n\r\nWhile implementing, I was thinking about how writing `std::vector` would simplify the interface\u2014you wouldn't need counter branches\u2014but it would complicate the implementation and definitely slow down the execution time. I'll keep thinking about how to add this as an extra feature for when you're willing to make that trade-off, but it did not make it into this sprint, sorry.",
        "createdAt":"2021-08-27T17:57:39Z",
        "number":1245595
       }
      ],
      "totalCount":5
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"PR #416 adds the ability to compress data written to ROOT files: objects like histograms and also TTree data. The compression setting can be specified when creating the file:\r\n\r\n```python\r\nwith uproot.recreate(\"filename.root\", compression=uproot.ZLIB(5)) as file:\r\n    ...\r\n```\r\n\r\nor it can be assigned to a file later:\r\n\r\n```python\r\nfile.compression = uproot.LZMA(9)\r\n```\r\n\r\nor it can be assigned to a tree (and therefore be different from the rest of the file):\r\n\r\n```python\r\ntree = file.mktree(\"tree\", {\"branch1\": numpy_dtype1, \"branch2\": awkward_type2})\r\ntree.compression = uproot.LZ4(1)\r\n```\r\n\r\nor it can be set on a per-branch level:\r\n\r\n```python\r\ntree[\"branch1\"].compression = uproot.ZSTD(4)\r\n```\r\n\r\nor\r\n\r\n```python\r\ntree.compression = {\"branch1\": uproot.ZSTD(4), \"branch2\": uproot.LZ4(1)}\r\n```\r\n\r\nSince WritableFile, WritableDirectory (which just passes it to its file), and WritableTree are all mutable, you can set the compression to one value, write some data, set it to another value, and write some more data. The compression that gets used is the configuration at the time of writing. Thus, it's even possible for different baskets of the same branch to be compressed differently, though I can't see how that's a good idea. As far as I can tell, it is allowed by ROOT, since ROOT uses a specialized header at the beginning of a TObject or TBasket to determine how to decompress some data; it doesn't seem to ever use the file-level or tree-level `fCompress` value to decide how to _decompress_. That's good.",
     "createdAt":"2021-08-23T21:50:02Z",
     "number":1224599,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"pcanal"
        },
        "body":">  Thus, it's even possible for different baskets of the same branch to be compressed differently,\r\n\r\nIRL this happens when (fast) merging TTrees from files written some time apart with different compression request.",
        "createdAt":"2021-08-25T18:36:51Z",
        "number":1234831
       },
       {
        "author":{
         "login":"pcanal"
        },
        "body":"> ; it doesn't seem to ever use the file-level or tree-level fCompress value to decide how to decompress. That's good.\r\n\r\nIndeed, each compressed buffer carries the compression algorithm used with it.\r\n\r\nAnother practical use is to compression different branches with different compression alogithm (for example, one for indices and one for data).",
        "createdAt":"2021-08-25T18:38:34Z",
        "number":1234836
       }
      ],
      "totalCount":2
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Hi @pcanal!\r\n\r\nPR #420 adds PyROOT-Uproot interoperability:\r\n\r\n   * any PyROOT object can be converted to its Uproot equivalent through `uproot.from_pyroot(obj)`, _if_ Uproot would have been able to read instances of that class from a file (a rather broad set);\r\n   * any [uproot.Model](https://uproot.readthedocs.io/en/latest/uproot.model.Model.html) can be converted to its PyROOT equivalent through `obj.to_pyroot()`, _if_ Uproot would have been able to write that class to a file (just TObjString and histograms\u2014quite limited).\r\n\r\nThe reason for these caveats is because the data conversion goes through TMessages: they're serialized to bytes and back, though that all happens in memory (no disk). The main motivation for this is to be able to write PyROOT objects to the same files you're writing hist histograms as TH* or Awkward Arrays as TTrees (without having to reopen the file with PyROOT and doing it the conventional way\u2014this is a convenience).\r\n\r\n```python\r\nwith uproot.recreate(\"filename.root\") as file:\r\n    file[\"hist\"] = hist_object\r\n    file[\"tree\"] = {\"branch1\": jagged_array, \"branch2\": numpy_array}\r\n    file[\"workspace\"] = roofit_workspace\r\n```\r\n\r\n(When PyROOT objects are being written to files, they do not need to be readable by Uproot because the raw bytes from the TMessage are directly copied into the file, albeit with compression if you ask for it. Also, all of the appropriate TStreamerInfo is included in the output file, which we get from PyROOT by making a temporary TMemFile, once per class type. If you have to write a thousand histograms, only one TMemFile will be made for the TStreamerInfo, but a thousand TMessages will be made.)\r\n\r\nThe only things left on my to-do list are interpreting hist objects and documentation. @henryiii and @amangoel185, we should talk about the hist objects; I'll ping you on Slack. With the documentation finished, this will be released as 4.1.0.",
     "createdAt":"2021-08-26T00:38:27Z",
     "number":1236624,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"As of PR #421, it is now documented. See the [last 1/3rd of the Getting Started Guide](https://uproot.readthedocs.io/en/latest/basic.html#opening-a-file-for-writing).",
     "createdAt":"2021-08-27T01:14:19Z",
     "number":1242162,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"allenxiangxin"
        },
        "body":"Thanks for adding the write feature. Useful stuff.",
        "createdAt":"2021-08-27T17:17:11Z",
        "number":1245430
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"goi42"
     },
     "body":"As I accidentally stated [here](https://github.com/scikit-hep/uproot4/pull/421#issuecomment-906852526),\r\n\r\nThank you, @jpivarski, for putting this together. This was a really essential project for the interoperability of ROOT, and we\u2019re all better off for your effort.",
     "createdAt":"2021-08-27T01:31:38Z",
     "number":1242199,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"As a last update (in this sprint, at least), I've released [Uproot 4.1.1](https://github.com/scikit-hep/uproot4/releases/tag/4.1.1) ([PyPI](https://pypi.org/project/uproot/)) with several writing-related performance bugs and one bug-bug fixed, all in PRs #426 and #428. The GitHub pages have some measurements with the code used to make them.\r\n\r\nA rule of thumb that comes out of this is that you want to call [TTree.extend](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableTree.html#uproot-writing-writable-writabletree-extend) with no less than about 100 kB per array branch. At that point, the Pythonic overhead for writing TBasket metadata is in the noise:\r\n\r\n<img src=\"https://user-images.githubusercontent.com/1852447/131406058-5bdeba69-e743-4472-aeaa-2df60dcbcadd.png\" width=\"100%\">\r\n\r\nThis is also the scale at which _reading_ data becomes efficient, too, but in writing, you get to control it.\r\n\r\n@masonproffitt's question is buried in a thread above, but writing of `std::vector` types (as opposed to dynamically sized arrays, both of which are jagged) has not been implemented yet. This is also issue #257, which remains open.",
     "createdAt":"2021-08-30T22:23:03Z",
     "number":1256527,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"eguiraud"
     },
     "body":"Hi,\r\n\r\nI was wondering that the status of uproot writing is w.r.t. inferring the right counters from the input tree.\r\n\r\nFor example here I'm reading `Muon_pt` and its counter `nMuon` from an input NanoAOD and I try to write them in a new tree with uproot, but it looks like uproot things `Muon_pt` should have a counter called `nMuon_pt` rather than just `nMuon`:\r\n\r\n```python\r\nimport uproot\r\nfname = \"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root\"\r\nin_tree, out_file = uproot.open(fname + \":Events\"), uproot.recreate(\"out.root\")\r\n\r\ndef get_uproot_type(interpretation):\r\n    try:\r\n        return interpretation.to_dtype\r\n    except:\r\n        return interpretation.content.to_dtype\r\n\r\nbranch_dict = {name: get_uproot_type(branch.interpretation) for name, branch in in_tree.items() if name == \"Muon_pt\" or name == \"nMuon\"}\r\nout_tree = out_file.mktree(\"Events\", branch_dict)\r\niterator = in_tree.iterate(branch_dict.keys(), step_size=\"1 MB\")\r\nbatch = next(iterator)\r\ncontents = {name: batch[name] for name in branch_dict.keys()}\r\nout_tree.extend(contents)\r\n```\r\n\r\nyields `ValueError: 'extend' was given data that do not correspond to any branch: 'nMuon_pt'`.\r\n\r\nQuestion 2. is whether there is a better, built-in way to convert the input branch types to the dtypes that works for ~all branches, I realize my `get_uproot_type` function is hacky at best.\r\n\r\n(Question 3. is whether I'm completely misusing uproot here :smile: )\r\n\r\nEDIT:\r\nFor the counter issue an ancillary question is why does uproot not get that information from TBranch directly?",
     "createdAt":"2023-05-19T16:59:45Z",
     "number":5950831,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Each TBranch is independently read from a ROOT file, producing an Awkward Array with independent offsets for each output array (i.e. each field of the output array of records):\r\n\r\n```python\r\n>>> import uproot\r\n>>> import awkward as ak\r\n>>> in_tree = uproot.open(\"Run2012B_DoubleMuParked.root\")[\"Events\"]\r\n>>> muon_pt_eta = in_tree.arrays([\"Muon_pt\", \"Muon_eta\"])\r\n>>> muon_pt_eta.type.show(type=True)\r\ntype: 1000000 * {\r\n    Muon_pt: var * float32,\r\n    Muon_eta: var * float32\r\n}\r\n```\r\n\r\nThe key thing is that the type has `Muon_pt: var * float32` and separately `Muon_eta: var * float32`. There's no assumption that they share the same \"`var`\". This _could_ be discovered by checking to see if both TBranches reference the same counter TLeaf, but reading treats each input TBranch independently\u2014it doesn't even look at the counters or require there to be counters (because all of the offsets can be found in the TBaskets).\r\n\r\nWriting is different. We can't make variable array type data without also making their counters because there are ROOT functions that use these counters. So, given a jagged array, Uproot has to make _two_ TBranches, one for the entire jagged array (including offsets) and another for the counter. The [uproot.WritableDirectory.mktree](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableDirectory.html#mktree) function has some handles to control what the counter TBranch will be named,\r\n\r\n  * **counter_name** (callable of str \u2192 str) \u2013 Function to generate counter-TBranch names for Awkward Arrays of variable-length lists.\r\n\r\nand the default is to put \"`n`\" at the beginning of the name (like NanoAOD).\r\n\r\nThis becomes problematic if you have a lot of arrays that should share the same counter TBranch, such as `Muon_pt`, `Muon_eta`, `Muon_phi`, `Muon_...`. Uproot has to somehow be informed that these arrays happen to have the same number of items in each list. The way this can be done is to make the separate arrays for each particle attribute into a single array of particles with attributes.\r\n\r\n```python\r\n>>> muons = ak.zip({\"pt\": muon_pt_eta[\"Muon_pt\"], \"eta\": muon_pt_eta[\"Muon_eta\"]})\r\n>>> muons.type.show()\r\n1000000 * var * {\r\n    pt: float32,\r\n    eta: float32\r\n}\r\n```\r\n\r\nNow there's only one \"`var`\", outside of the record-type. If the separate arrays didn't have the same number of items in each list, the [ak.zip](https://awkward-array.org/doc/main/reference/generated/ak.zip.html) would fail.\r\n\r\nThis is an Awkward type that Uproot recognizes, and upon seeing it, Uproot will make all of the output TBranches share a single counter TBranch because it knows that it can do that. Now, though, it also needs to generate names for TBranches from fields of a record array, which is another [uproot.WritableDirectory.mktree](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableDirectory.html#mktree) argument,\r\n\r\n   * **field_name** (callable of str \u2192 str) \u2013 Function to generate TBranch names for columns of an Awkward record array or a Pandas DataFrame.\r\n\r\nand the default is to use \"`_`\" as a separator (like NanoAOD).\r\n\r\n```python\r\n>>> out_file = uproot.recreate(\"/tmp/output.root\")\r\n>>> out_file[\"out_tree\"] = {\"Muon\": muons}\r\n>>> out_file[\"out_tree\"].show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nnMuon                | int32_t                  | AsDtype('>i4')\r\nMuon_pt              | float[]                  | AsJagged(AsDtype('>f4'))\r\nMuon_eta             | float[]                  | AsJagged(AsDtype('>f4'))\r\n```\r\n\r\nThis asymmetry between reading and writing is because the data model of Awkward Arrays doesn't perfectly match the data model of TTree. To make them match, reading could get more aggressive about zipping TBranches that share a counter, but that would surprise long-time users of Uproot, who expect 1 TBranch \u2192 1 array. (Also, that's a nice simplifying assumption.) Writing can't get less aggressive about unzipping records into TBranches with a shared counter because somehow the knowledge of which TBranches should share a counter has to be communicated. It could have been done with arguments to the function, but then if somebody had data like `muons`, above, they'd have to do even more work to express information that's already latent in the structure.\r\n\r\nAnd anyway, data structures like `muons`, above, are more convenient for doing analysis, since you can slice all fields of an array of particles with one line, instead of doing it individually for each attribute.",
     "createdAt":"2023-05-19T17:36:30Z",
     "number":5951111,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"eguiraud"
        },
        "body":"Thank you for the explanation, Jim!\r\n\r\n> the default is to put \"n\" at the beginning of the name (like NanoAOD).\r\n\r\nThat's not the NanoAOD convention it, I think? E.g. `Muon_pt`'s counter is not `nMuon_pt`, but `nMuon` (hence the error in my snippet above).\r\nI guess you are implicitly assuming that when reading `Muon_{pt,eta,...}` they always get zipped into a `Muon`?\r\nBased on your explanation, however, I would think that if you used the actual NanoAOD naming convention (something like `lambda name: 'n' + name.rsplit('_')[0]`), then the writing logic would automatically use the same counter for several separate branches (and I guess that if the same counter is requested multiple times it is still only written once).\r\n\r\nSo I _think_ things could work out of the box for NanoAODs thanks to their regular naming conventions? In general I understand that I should explicitly inform `mktree` of how the counters are called.",
        "createdAt":"2023-05-19T18:46:58Z",
        "number":5951653
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"See the last example: since the key of the dict in\r\n\r\n```python\r\n>>> out_file[\"out_tree\"] = {\"Muon\": muons}\r\n```\r\n\r\nis `\"Muon\"`, the name of the counter branch becomes `\"nMuon\"`, which is NanoAOD convention.\r\n\r\nWhat you can't do is read data out of one file and write it into another file without reformatting with `ak.zip`. The information that the writing step would need (the fact that all of the arrays suggestively named \"`Muon_`\" ought to share a counter) is lost.",
        "createdAt":"2023-05-19T19:00:04Z",
        "number":5951739
       },
       {
        "author":{
         "login":"eguiraud"
        },
        "body":"Yes I understand, as well as the will to prioritize the case of \"zipped\" SoAs like `\"Muon\"` over single columns like `\"Muon_pt\"`.\r\n\r\n> The information that the writing step would need (the fact that all of the arrays suggestively named \"Muon_\" ought to share a counter) is lost.\r\n\r\nAbout this specific point, what I wanted to say is that the writing side of uproot could easily recover that information from the column names (since uproot already bases its default `counter_name` on NanoAOD naming conventions). It could even support both SoAs like `Muon` and individual columns like `Muon_pt` if `counter_name` were able to distinguish the two cases, e.g. with a second argument that contains the type of the column:\r\n\r\n```\r\ndef counter_name(name, type_ = None):\r\n  if is_zip(type): # SoA\r\n    return 'n' + name\r\n  else:\r\n    return 'n' + name.rsplit('_')[0]\r\n```",
        "createdAt":"2023-05-19T20:15:58Z",
        "number":5952180
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"If `counter_name` is the function above, then each `Muon_pt`, `Muon_eta`, `Muon_...` would each get a counter named `nMuon`. I'm not really sure what would happen if multiple input arrays generate counter branches with the same name, though in this case, they would also have the same value (unchecked, unlike the `ak.zip` method). It's possible that, the way it's implemented now, only one counter would get written.\r\n\r\nI'd like to avoid, as much as possible, CMS-specific details getting into Uproot. The default values of the `counter_name` and `field_name` functions match NanoAOD only because there isn't a competing convention that I'm aware of and this is a style that's sometimes even adopted by non-CMS analyses. Splitting strings (i.e. assuming that they _have_ an underscore) is a little further down that road I wanted to avoid than just appending \"`n`\" and joining by \"`_`\".\r\n\r\nBut there's plenty of room for specialization by building on top of Uproot. It was a design goal for Uproot to just be a base layer that other packages build on top of. This name-manipulation is a good candidate for that.",
        "createdAt":"2023-05-19T20:46:09Z",
        "number":5952356
       },
       {
        "author":{
         "login":"eguiraud"
        },
        "body":"Fair enough, thank you for your patience!\r\n\r\nAt the end of the day my goal was to read some columns from a NanoAOD-like TTree and write them out in another -- based on this conversation my understanding is that it's not possible without re-injecting, between the reading and the writing part, some information about objects (\"`Muon` is made of `Muon_pt`, `Muon_eta`, `Muon_phi`, ...\") and their counters (i.e. some domain knowledge about the actual schema). Good to know!",
        "createdAt":"2023-05-19T21:28:45Z",
        "number":5952586
       }
      ],
      "totalCount":5
     }
    },
    {
     "author":{
      "login":"xaratustrah"
     },
     "body":"Hi @jpivarski, and everyone, thanks for updating on this! I was just wondering, is there an example of how to use `uproot.behaviors.TH1.TH1`? I have been using TH1 with `uproot3` and `uproot3_methods` successfully [here](https://github.com/xaratustrah/iqtools/blob/475fbc1fb86ede66830ac4950c4f7e88ab2c6252/iqtools/tools.py#L627), btw. same with writing a tree, like [here](https://github.com/xaratustrah/iqtools/blob/475fbc1fb86ede66830ac4950c4f7e88ab2c6252/iqtools/tools.py#LL595C53-L595C53). Wondering if it would be possible to move on to `uproot5`?\r\n\r\n\r\n",
     "createdAt":"2023-06-14T16:09:01Z",
     "number":6176854,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"The `TH1` interface is [documented here](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TH1.TH1.html), though the most useful methods are the ones that take you out of Uproot into [boost-histogram](https://github.com/scikit-hep/boost-histogram) ([TH1.to_boost](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TH1.TH1.html#to-boost)) or [hist](https://github.com/scikit-hep/hist) ([TH1.to_hist](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TH1.TH1.html#to-hist)). The design strategy for Uproot v4 (and v5) is to get out of the business of providing interfaces for all the ROOT data types and instead pass them to libraries that specialize in each type of functionality.\r\n\r\nThese histogram objects are also recognized by Uproot's file-writing, as [documented here](https://uproot.readthedocs.io/en/latest/basic.html#writing-objects-to-a-file). The blue box lists all of the types that are recognized on the right-hand side of file-writing assignment.\r\n\r\nIt should be possible to modify anything that worked in Uproot v3 to use the new interfaces in v4/v5. I can't think of anything that v3 could do that v4/v5 can't do\u2014it should be a strict superset.",
        "createdAt":"2023-06-14T17:56:13Z",
        "number":6177869
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":19
  },
  "createdAt":"2021-04-01T23:50:56Z",
  "number":321,
  "title":"Writing files with Uproot 4",
  "url":"https://github.com/scikit-hep/uproot5/discussions/321"
 },
 {
  "author":{
   "login":"lwpiotr"
  },
  "body":"Hello,\r\n\r\nI'm new to uproot and either I found that I don't know how to use it optimally, or I found a significant inefficiency.\r\nIn my TTree there are many branches, but almost all of the data is concentrated in a vector<trace>, where trace is a class with 6 vector<float> inside. ROOT TTree essentially splits it into 6 branches with vector<vector<float>>. This is not what TTree was designed for and ROOT needs to read the whole vector<float> at once, unfortunately (see https://root-forum.cern.ch/t/the-optimal-way-to-store-variable-tracks-count-in-a-ttree if interested), but with uproot I've stumbled upon a very significant slowdown. \r\n\r\nThe TTree has 1001 events, vectors are variable, but in every entry their dimension is roughly 180*1000. The TTree is 4.5 GB large and almost all of it are those vectors. Compression is off.\r\n\r\nReading just 1 of those 6 vector<vector<float>> in a single entry takes ROOT roughly 0.07s. Uproot3 takes 3 seconds, Uproot4 takes 1s.\r\n\r\nThe code is (where event=500):\r\nroot: `entries = t.Draw(\"traces.SimSignal_X\", \"\", \"goff\", 1, event-1)`\r\nuproot3: `energy_root = t.array(\"traces.SimSignal_X\")[event-1]`\r\nuproot4: `energy_root = t[\"traces.SimSignal_X\"].array(entry_start=event-1, entry_stop=event)`\r\n\r\nHowever, when I try to read the 3 of the vector<vector<float>> and do some multiplication, it takes ROOT around 2 s, uproot3 9 s, and with uproot4 it is so long that I gave up on waiting. The code is:\r\nroot: `entries = t.Draw(\"traces[100].SimSignal_X[150]*traces[100].SimSignal_Y[150]*traces[100].SimSignal_Z[150]*traces[10].SimSignal_X[150]*traces[50].SimSignal_Z[150]\", \"\", \"goff\")`\r\nuproot3: \r\n```\r\na = t.arrays([\"traces.SimSignal_X\", \"traces.SimSignal_Y\", \"traces.SimSignal_Z\"])\r\nenergy_root = np.array(a[b\"traces.SimSignal_X\"][:][150][100],copy=False)*np.array(a[b\"traces.SimSignal_Y\"][:][150][100],copy=False)*np.array(a[b\"traces.SimSignal_Z\"][:][150][100],copy=False)*np.array(a[b\"traces.SimSignal_X\"][:][150][10],copy=False)*np.array(a[b\"traces.SimSignal_Z\"][:][150][50], copy=False)\r\n```\r\nuproot4:\r\nI actually finally gave up on a less demanding example, which was also running endlessly:\r\n`energy_root1 = t[\"traces.SimSignal_X\"].array()[:,0,150]`\r\n\r\nThe TTree printout for the relevant part is:\r\n```\r\n*............................................................................*\r\n*Br   95 :traces    : Int_t traces_                                          *\r\n*Entries :     1001 : Total  Size=      97909 bytes  File Size  =      20020 *\r\n*Baskets :      143 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br   96 :traces.SimEfield_X : vector<float> SimEfield_X[traces_]            *\r\n*Entries :     1001 : Total  Size=  726715213 bytes  File Size  =  726703274 *\r\n*Baskets :      575 : Basket Size=    1783808 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br   97 :traces.SimEfield_Y : vector<float> SimEfield_Y[traces_]            *\r\n*Entries :     1001 : Total  Size=  726715213 bytes  File Size  =  726703274 *\r\n*Baskets :      575 : Basket Size=    1783808 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br   98 :traces.SimEfield_Z : vector<float> SimEfield_Z[traces_]            *\r\n*Entries :     1001 : Total  Size=  726715213 bytes  File Size  =  726703274 *\r\n*Baskets :      575 : Basket Size=    1783808 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br   99 :traces.SimSignal_X : vector<float> SimSignal_X[traces_]            *\r\n*Entries :     1001 : Total  Size=  726714509 bytes  File Size  =  726702570 *\r\n*Baskets :      575 : Basket Size=    1783808 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br  100 :traces.SimSignal_Y : vector<float> SimSignal_Y[traces_]            *\r\n*Entries :     1001 : Total  Size=  726714509 bytes  File Size  =  726702570 *\r\n*Baskets :      575 : Basket Size=    1783808 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br  101 :traces.SimSignal_Z : vector<float> SimSignal_Z[traces_]            *\r\n*Entries :     1001 : Total  Size=  726714509 bytes  File Size  =  726702570 *\r\n*Baskets :      575 : Basket Size=    1783808 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n\r\n```\r\n\r\nAm I doing something wrong, especially comparing uproot3 to uproot4, or did I stumble upon some uproot problem? I will be grateful for any advice.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"The slowness of more-than-one nesting of variable-length lists is a known issue. The problem is that it's serialized in a different way (non-columnar), so the NumPy tricks that work for `std::vector<float>` don't work for `std::vector<std::vector<float>>`. The latter needs to fall back to pure Python code, which is a huge bottleneck.\r\n\r\nHere's where I talked about this problem at 2019 CHEP (Figure 3 is based on a hacked performance study): https://arxiv.org/abs/2001.06307 and this year: https://arxiv.org/abs/2102.13516 (Figure 1 is using AwkwardForth, a new formalism that is yet to be integrated into Uproot).\r\n\r\nUproot 3 could read these data faster because it delayed deserialization: it made Awkward 0 ObjectArrays, breaking the Awkward Array formalism so that you couldn't slice it like other arrays. Having a different public interface because of an internal difference in ROOT serialization was itself a problem: I ended up having to explain/apologize for this interface difference a lot. In Awkward 1, it was a goal to have all arrays behave the same way, regardless of where they came from. However, that means that Uproot 4 must deserialize the `std::vector<std::vector<...>>` up-front, not delayed until they're used in a calculation.\r\n\r\nSince your code selects one element, it only deserializes that one in Uproot 3, whereas Uproot 4 has to deserialize everything to give you that one. That's why the speed is different.\r\n\r\nI'm working on file-writing at the moment, but integrating AwkwardForth so that Uproot 4 will be able to deserialize these objects as fast as ROOT (Figure 1 of that new paper). AwkwardForth is a few times slower than compiled, optimized C++, but many times faster than pure Python and about as fast as the data transfer from RAM to CPU (so computation is not the primary bottleneck, especially if there's any decompression involved).",
     "createdAt":"2021-04-07T15:52:57Z",
     "number":580583,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"lwpiotr"
        },
        "body":"Thanks! I don't need this solution now, especially that due to very small performance gain when using ROOT compared to HDF5 in our case we decided to go with HDF5. However, some time ago when I tried only uproot3 I stumbled somewhere upon the explanation that you gave about and a suggestion that the looping will be done in C++ in uproot4. Thus I was surprised to see a worse performance in uproot4 and decided to report. But now I understand. Thanks again.\r\n\r\nBtw. I understand that uproot does not do any magic that would allow it to read just a part of vector<vector<float>> instead of the whole vector?",
        "createdAt":"2021-04-07T16:14:10Z",
        "number":580717
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"> Btw. I understand that uproot does not do any magic that would allow it to read just a part of vector<vector> instead of the whole vector?\r\n\r\nYou can select only one entry, but you've already done that.\r\n\r\nThe thing I was talking about with someday compiling these loops is what has evolved into AwkwardForth, but still there's that last step of integrating AwkwardForth into Uproot. (Uproot has to generate Forth code for each type of data to deserialize, instead of generating Python code.)\r\n",
        "createdAt":"2021-04-07T16:26:20Z",
        "number":580790
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-04-07T15:23:11Z",
  "number":327,
  "title":"Inefficiency when reading vector<vector<float>>",
  "url":"https://github.com/scikit-hep/uproot5/discussions/327"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@kratsg implemented memberwise serialization for `std::pair`: PR #298, #301.\r\n\r\n@nsmith- made sources and caches pickleable: PR #302.\r\n\r\n@wiso fixed a typo in the documentation: PR #315.\r\n\r\n@henryiii cleaned up the build procedure and got Python 2.7 added to the regular tests: PR #316, #317, #319.\r\n\r\n@jpivarski fixed a bug that exposed internal JaggedArray objects (if they're empty): PR #304. Added histograms to the regular tests: PR #305. Fixed \"file://\" handling on Windows: PR #328.\r\n\r\n@jpivarski also started developing file-writing (though it's not ready yet): PR #320, #322, #329, and fea04add86aa6efff4f48724ef5f940af427aeee.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.0.7'>4.0.7</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-04-12T13:50:30Z",
  "number":330,
  "title":"4.0.7",
  "url":"https://github.com/scikit-hep/uproot5/discussions/330"
 },
 {
  "author":{
   "login":"shallmann"
  },
  "body":"Hi, I am trying to find out how to use `uproot.iterate` on existing root files containing branches for which the correct interpretation is not discovered automatically.\r\n\r\nOn a single file, I can read the data doing something like:\r\n```\r\nmy_interpretation = uproot.interpretation.jagged.AsJagged(uproot.interpretation.numerical.AsDtype('>i4'), header_bytes=6)\r\nfile = uproot.open(\"testfile.root\")\r\nbranch = file['some_tree']['some_branch']\r\nbranch.array(interpretation=my_interpretation)\r\n```\r\n\r\nIs there a straightforward possibility to pass the interpretation information for the branches throwing errors (*) also to `uproot.iterate`?\r\n\r\nThanks a lot for suggestions!\r\n\r\n(*) I am getting errors like: ValueError: basket 0 in tree/branch /some_tree:some_branch has the wrong number of bytes (266) for interpretation AsStridedObjects(Model_TTimeStamp_v1)",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I checked to see if it might just be undocumented, but no, there currently isn't a way to pass custom interpretations to `uproot.iterate`. The [TBranch.array](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#array) method takes an `interpretation` as its first argument, but the interface to `TTree.arrays`, `TTree.iterate`, and `uproot.iterate` is complicated by the fact that the strings might not be branch names directly, but might be expressions to compute from branches (to enable `aliases`). At best, we could add a dict argument that maps branch name (not an expression) to custom interpretations. There are a few places in the code that would have to change to pass that down.\r\n\r\nBut still, it would be a complicated interface. If a branch name is misspelled in that dict, it just wouldn't take effect. Uproot 3 combined the list of branches to read with custom interpretations by accepting it as a dict, rather than a list, but those strings were real branch names, not expressions, in Uproot 3, so such an interface made more sense. Here, it would _have to_ be a separate dict, which increases the possibility of error. Also, passing custom interpretations is uncommon. I see why you're doing it for this timestamp\u2014you happen to know that timestamps are just 4-byte integers, but the streamer is being interpreted otherwise.\r\n\r\nActually, can you print out the\r\n\r\n```python\r\nfile.file.show_streamers(\"TTimeStamp\")\r\n```\r\n\r\nto see how it's being interpreted? Looking at its header file,\r\n\r\nhttps://github.com/root-project/root/blob/87a998d48803bc207288d90038e60ff148827664/core/base/inc/TTimeStamp.h#L81-L82\r\n\r\nit ought to be a pair of 32-bit integers, not just one 32-bit integer.\r\n\r\n266 bytes doesn't evenly divide into either 4 or 8. Its prime factorization is 2 \u00d7 7 \u00d7 19. I doubt the struct size is 2 or 7 (or anything larger), so maybe the basket this is obtaining is wrong somehow? The timestamp is in a branch by itself, right?",
     "createdAt":"2021-05-07T15:17:12Z",
     "number":709911,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"shallmann"
        },
        "body":"Thanks for your reply. Then at least I did not miss anything obvious.\r\n\r\nI hope indeed that custom interpretations are needed only in rare use cases in general - it is very cumbersome..., for me the timestamp branch was not the only one causing these kind of errors after all, I got something similar with the trigger mask... I think I can live with not using the `uproot.iterate` for the reader of these files. \r\n\r\nFor the 266 bytes interpretation, I just tried until I got out no errors and the correct time values. Anyway, the `file.file.show_streamers(\"TTimeStamp\")` gives me\r\n```\r\nTTimeStamp (v1)\r\n    fSec: int (TStreamerBasicType)\r\n    fNanoSec: int (TStreamerBasicType)\r\n```\r\nand the times are in a separate branch `file['EventHeader./EventHeader.fTime']`\r\n```\r\n<TBranchElement 'EventHeader.fTime' at 0x7ff989ea0dc0>\r\n```\r\nIf as you say the basket is wrong somehow, I have no means of fixing that when I just want to use externally provided files, right?\r\n\r\n",
        "createdAt":"2021-05-11T11:58:46Z",
        "number":723305
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"That TTimeStamp streamer agrees with the C++ header file; if you're interpreting the branch as `\">i4\"`, then you ought to be getting `fSec` and `fNanoSec` interleaved. Are you sure you don't want\r\n\r\n```python\r\nuproot.interpretation.jagged.AsJagged(uproot.interpretation.numerical.AsDtype(\r\n    [(\"fSec\", \">i4\"), (\"fNanoSec\", \">i4\")]\r\n), header_bytes=6)\r\n```\r\n\r\n?",
        "createdAt":"2021-05-11T13:19:03Z",
        "number":723667
       },
       {
        "author":{
         "login":"shallmann"
        },
        "body":"Oh that is indeed much more convenient to use, yes. Thanks a lot!\r\n\r\nThat leaves me with only one more problematic one to solve...\r\n\r\n``` \r\n     TSnTriggerSetup version 3 as uproot.dynamic.Model_TSnTriggerSetup_v3 (184 bytes)\r\n        (base): <TObject None None at 0x7ff98e86e850>\r\n        TMap version 3 as uproot.dynamic.Model_TMap_v3 (139 bytes)\r\n            (base): <TCollection (version 3) at 0x7ff98e86e9a0>\r\n            fTable: None\r\n            TSnTriggerSetup version 3 as uproot.dynamic.Model_TSnTriggerSetup_v3 (184 bytes)\r\n                (base): <TObject None None at 0x7ff98e86edc0>\r\n                TMap version 3 as uproot.dynamic.Model_TMap_v3 (139 bytes)\r\n                    (base): <TCollection (version 3) at 0x7ff98e86efd0>\r\n                    fTable: None\r\nBase classes for TMap: (TCollection)\r\nMembers for TMap: (fTable)\r\n\r\nexpected 139 bytes but cursor moved by 12 bytes (through TMap)\r\n```",
        "createdAt":"2021-05-11T14:08:30Z",
        "number":723994
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"There's something very peculiar about the fact that the TSnTriggerSetup and TMap deserializations are nested within themselves, and the sizes of the nested ones are not smaller. The number of bytes should include all constituents, which have non-zero size headers, so nested objects should have strictly fewer bytes than the outer ones. This might have accidentally gone into some recursive loop, which wasn't an infinite loop because it stopped at the first number of bytes check that failed. (We might be looking at the same TSnTriggerSetup and TMap objects\u2014the reading position in the file might have jumped back.) This is likely a bug that I'll need to see a copy of the original file to diagnose (GitHub Issue, minimally reproducing example, and a small or one-event file that can be public).",
        "createdAt":"2021-05-11T15:32:10Z",
        "number":724483
       }
      ],
      "totalCount":4
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-05-07T14:24:52Z",
  "number":354,
  "title":"passing interpretation info to uproot.iterate",
  "url":"https://github.com/scikit-hep/uproot5/discussions/354"
 },
 {
  "author":{
   "login":"dcervenkov"
  },
  "body":"When reading many files via XRootD, we see intermittent and seemingly non-deterministic \"Operation expired\" errors from XRootD. This might be an XRootD or an EOS issue, but I filed it here since we are experiencing it when using Uproot. I'd be interested to hear if you have ideas about how to troubleshoot this.\r\n\r\nMWE to reproduce follows. Unfortunately, it requires access to LHCb files on EOS.\r\n\r\n```py\r\nimport uproot\r\n\r\nsamples = [\r\n    f\"root://eoslhcb.cern.ch//eos/lhcb/grid/prod/lhcb/LHCb/Collision16/PIDCALIB.ROOT/00111823/0000/00111823_{i:08}_1.pidcalib.root\"\r\n    for i in range(1, 239)\r\n]\r\n\r\nfor file in samples:\r\n    print(f\"Reading {file}\")\r\n    tree = uproot.open(file)[\"DSt_PiMTuple\"]\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"mwe_xrootd.py\", line 12, in <module>\r\n    tree = uproot.open(file)[\"DSt_PiMTuple\"]\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/reading.py\", line 145, in open\r\n    **options  # NOTE: a comma after **options breaks Python 2\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/reading.py\", line 553, in __init__\r\n    file_path, **self._options  # NOTE: a comma after **options breaks Python 2\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 434, in __init__\r\n    self._open()\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 440, in _open\r\n    for x in uproot._util.range(self._num_workers)\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 440, in <listcomp>\r\n    for x in uproot._util.range(self._num_workers)\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 88, in __init__\r\n    self._xrd_error(status)\r\n  File \"/home/dc/.local/lib/python3.6/site-packages/uproot/source/xrootd.py\", line 108, in _xrd_error\r\n    status.message, self._file_path\r\nOSError: XRootD error: [ERROR] Operation expired\r\nin file root://eoslhcb.cern.ch//eos/lhcb/grid/prod/lhcb/LHCb/Collision16/PIDCALIB.ROOT/00111823/0000/00111823_00000057_1.pidcalib.root\r\n```\r\n\r\nIt is not a problem with expired credentials, etc. - just rerunning the script will cause it to fail on a different file or not at all.\r\n\r\nThe issue occurs on multiple computers with very different setups.\r\n\r\nuproot 4.0.7\r\nxrootd 5.1.1",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"chrisburr"
     },
     "body":"Interestingly I can't reproduce it on lxplus but I can when reading the data from outside of the CERN. Now to figure out why...",
     "createdAt":"2021-05-06T07:03:15Z",
     "number":709823,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"chrisburr"
     },
     "body":"This isn't really an uproot problem (ROOT also had the same issue) but I'll summarise how I debugged it in case others run in to the same issue.\r\n\r\n1. Repeating the test many times showed that it only ever crashed on a subset of the files mentioned in this issue\r\n2. `eos fileinfo /eos/lhcb/...` showed that they all contained a replica that was on the same server\r\n3. It was reported at https://ggus.eu/?mode=ticket_info&ticket_id=151899 and indeed there was a server with a problem that prevented it from being made visible outside the firewall.",
     "createdAt":"2021-05-07T06:56:46Z",
     "number":709824,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"dcervenkov"
     },
     "body":"That's good news. Thanks for taking care of it @chrisburr!",
     "createdAt":"2021-05-07T07:53:22Z",
     "number":709825,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Thanks, @chrisburr! To make sure your solution remains visible, I'll convert this into a discussion.",
     "createdAt":"2021-05-07T14:59:38Z",
     "number":709826,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"ryuwd"
     },
     "body":"Advance apologies as this is not an 'answer', but adding a similar problem I have encountered in more recent versions of `uproot4` and `xrootd`, to the discussion.\r\n\r\nI am also seeing `Operation expired` errors and `Invalid response` errors when using xrootd with `uproot4` 4.0.11 with `XRootDSource` as the `xrootd_handler`. I tried using `XRootDSource` again in my analysis code after seeing xrootd 5.2.0 was recently released, and some updates had been pushed to uproot4 as well (moving `XRootDSource` back to the default `xrootd_handler`.)\r\n\r\nI'm reading chunks from trees across many files, streaming them from CERN to my institute, then processing using `dask` delayed (see the end of my post for the exact code.) I don't think `uproot4` is the problem, but when using XRootD vector reads (I use `xrootd=5.2.0` from conda-forge in my analysis environment) with `XRootDSource`, reads will occasionally fail with either `Operation expired`, `Invalid response`, and more rarely I am seeing `EPoll: bad file descriptor polling for events`\r\n\r\nWhen I use `MultithreadedXRootDSource` the problems 'go away' and my jobs finish successfully. (Apologies for this vague analysis!)\r\n\r\nLHCb-related: these ntuples come from the analysis production `XicpStStToXicpPiPi`\r\nexample URLs:\r\n```\r\nroot://eoslhcb.cern.ch//eos/lhcb/grid/prod/lhcb/LHCb/Collision16/XICPSTSTTOXICPPIPI_DATA_2016_MAGUP.ROOT/00130129/0000/{id}.root\r\n\r\nroot://eoslhcb.cern.ch//eos/lhcb/grid/prod/lhcb/MC/2016/XICPSTSTTOXICPPIPI_MC.ROOT/00130499/0000/{id}.root\r\n```\r\n\r\nWhen I have some more time to debug this, I'll try to run this through again with `XRootDSource`, collect the tracebacks, and report them properly. Anyway, it seems for lots of XRootD root file streaming that `MultithreadedXRootDSource` is the potentially more stable option (although perhaps not ideal speed-wise?)\r\n\r\n---\r\n\r\n```python\r\n\r\ndef get_file_handler(filename):\r\n    xrootd_src = filename.startswith(\"root://\")\r\n    if not xrootd_src:\r\n        return {\"file_handler\": uproot.MultithreadedFileSource} # otherwise the memory maps overload available Vmem\r\n    elif xrootd_src:\r\n        # uncomment below for MultithreadedXRootDSource\r\n        # return {\"xrootd_handler\": uproot.source.xrootd.MultithreadedXRootDSource}\r\n    return {}\r\n\r\ndef load_root_file(tree, columns, total_file_parts=4):\r\n    def get_tree_part(nentries, part, total_parts):\r\n        part_len = nentries / total_parts\r\n        entry_start = part * part_len\r\n        entry_stop = (\r\n            nentries if (total_parts - 1) == (part) else int(entry_start + part_len)\r\n        )\r\n        return int(entry_start), entry_stop\r\n\r\n    def root_loader(filename, file_part):\r\n        with uproot.open(filename, **get_file_handler(filename)) as rf:\r\n            treeo = rf[tree]\r\n            estart, estop = get_tree_part(\r\n                treeo.num_entries, file_part, total_file_parts\r\n            )\r\n            df = rf[tree].arrays(\r\n                columns, entry_start=estart, entry_stop=estop, library=\"pd\"\r\n            )\r\n        assert (estop - estart) == df.shape[0]\r\n        return df\r\n\r\n    return root_loader\r\n\r\ndef load_root_as_dask_df(filenames, tree, columns, total_file_parts=1):\r\n    meta = get_dtypes_root(filenames[0], tree, columns)\r\n    loader = load_root_file(tree, columns, total_file_parts=total_file_parts)\r\n    dfs = []\r\n\r\n    for fn in filenames:\r\n        for part in range(total_file_parts):\r\n            dfs += [delayed(loader, pure=True, traverse=False)(fn, part)]\r\n\r\n    df = dd.from_delayed(dfs, meta=meta)\r\n\r\n    return df[columns]\r\n\r\n```\r\n",
     "createdAt":"2021-07-07T20:41:01Z",
     "number":976588,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"special-kay-0"
     },
     "body":"I just wanted to report that we are currently having this issue using uproot  4.1.5. \r\nWe are running code on lxplus and trying to access files both on the CERN eos and on a /pnfs file system for another institute. \r\nWe get these OS errors, but I also see jobs getting killed when trying to read files using xrootd, as opposed to directly reading from the local eos mount. \r\n\r\nWe are using uproot.iterate to loop over the files in chunks of defined sizes. I can try to whip together a MWE if needed. \r\nIt seems especially strange in the case of the eos files, since we just try to read the exact same files that we have successfully analysed in the past in uproot but with the root://eosatlas... string prepended. \r\n\r\nI did try setting the file_handler to MultithreadedXRootDSource in iterate, but this didn't improve our situation (jobs were hanging and eventually killed after the first iteration)\r\n\r\n@jpivarski is there a possibility that this needs to be upgraded from discussion to issue again? ",
     "createdAt":"2022-10-20T13:06:18Z",
     "number":3924554,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ryuwd"
        },
        "body":"Hi @special-kay-0,\r\n\r\nIf your files are on eos it might help to take the corresponding file where the `Operation expired` error is happening and try\r\n```\r\neos info <eos path to file>\r\n```\r\nto narrow down the issue.\r\n\r\nI have found that from time to time files become unavailable due to the servers at CERN providing the file going offline.  You can then report that to someone who manages the EOS / grid storage / data management facilities for your collaboration",
        "createdAt":"2022-10-25T03:35:23Z",
        "number":3956697
       },
       {
        "author":{
         "login":"special-kay-0"
        },
        "body":"Hi @ryuwd \r\n\r\nYes, we do deal with intermittent eos access issues. However, there is a clear difference in uproot between accessing them directly via the eos path w.r.t. accessing  with xrootd. \r\nWe have come to expect the occasional problem accessing a given file, but with this xrootd access we are systematically seeing the OSError or hanging file access and excess memory use leading to jobs being killed. ",
        "createdAt":"2022-10-27T15:45:29Z",
        "number":3983473
       },
       {
        "author":{
         "login":"ryuwd"
        },
        "body":"Just learned recently that one can run into XRootD `Operation expired` errors if authentication is not valid. For example,\r\n\r\n - invalid (or no) kerberos ticket\r\n - invalid (or no) eos token provided\r\n - invalid x509 auth\r\n - some other server issue\r\n \r\n The request would time out before the remote has a chance to check authentication and return the more relevant error, hence the cryptic `Operation expired`. You of course wouldn't see such an error with the `/eos` mount, because the authentication is usually transparent to the user in that case.",
        "createdAt":"2023-01-20T05:58:55Z",
        "number":4734316
       },
       {
        "author":{
         "login":"dcervenkov"
        },
        "body":"Interesting - when there's no Kerberos ticket, I get `FileNotFound` with \"Server responded with error: [3010]\" in its arguments.",
        "createdAt":"2023-01-20T08:22:32Z",
        "number":4735106
       }
      ],
      "totalCount":4
     }
    },
    {
     "author":{
      "login":"msnuaa"
     },
     "body":"Hi, all, I am suffering the same issue of `Operation expired` at one of the remote site, and only for some of the files on the same site.\r\nI am using root 6.16 and uproot 4.3.5. Is there a proper way to get rid of this issue?\r\nThanks in advance!",
     "createdAt":"2023-04-17T13:39:39Z",
     "number":5637313,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"gabhijith"
        },
        "body":"I am facing a similar issue on cms-lpc.",
        "createdAt":"2023-05-09T15:35:03Z",
        "number":5850807
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":7
  },
  "createdAt":"2021-05-05T14:08:33Z",
  "number":355,
  "title":"How to handle XRootD error: [ERROR] Operation expired",
  "url":"https://github.com/scikit-hep/uproot5/discussions/355"
 },
 {
  "author":{
   "login":"plasorak"
  },
  "body":"Hi all,\r\n\r\nI'm trying to read an object which has colon in its name. So doing something like:\r\n`data = up.open(\"path/to/file.root\")[\"that:histo:with:colon:in:the:middle\"]`\r\nThat exits with the following error:\r\n```  File \"/usr/local/lib/python3.9/site-packages/uproot/reading.py\", line 1962, in __getitem__\r\n    step = step[item]\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/reading.py\", line 1962, in __getitem__\r\n    step = step[item]\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/reading.py\", line 1962, in __getitem__\r\n    step = step[item]\r\n  [Previous line repeated 992 more times]\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/reading.py\", line 1945, in __getitem__\r\n    if \":\" in item and item not in step:\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/reading.py\", line 1810, in __contains__\r\n    self.key(where)\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/reading.py\", line 1887, in key\r\n    where = uproot._util.ensure_str(where)\r\n  File \"/usr/local/lib/python3.9/site-packages/uproot/_util.py\", line 72, in ensure_str\r\n    if not py2 and isinstance(x, bytes):\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n```\r\nHow would I do that?\r\nThanks.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Short answer: put the file name in a `pathlib.Path`. These are not colon-split. In this mode, you can't put the object path in the same string, it has to be `uproot.open(pathlib.Path(filename))[object_path_here]`.\r\n\r\nFor more details, the thread where we discussed the design of this is #79.",
     "createdAt":"2021-05-18T16:56:55Z",
     "number":754035,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"plasorak"
        },
        "body":"That works for the file name, but not for the _object_ path right?",
        "createdAt":"2021-05-18T17:06:09Z",
        "number":754069
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Oh, I missed that. I hadn't realized that colons would be legal characters in objects\u2014ah, it's TBranches that use colon as part of its constructor syntax.\r\n\r\nFortunately (really lucky, actually, the `TDirectory.key` method ([here](https://github.com/scikit-hep/uproot4/blob/e57e9ac2e06e4dc0d4e777191825fe014549401f/src/uproot/reading.py#L1926-L1988)) doesn't parse colons, so you can do\r\n\r\n```python\r\ndata = uproot.open(\"path/to/file.root\").key(\"that:histo:with:colon:in:the:middle\").get()\r\n```",
        "createdAt":"2021-05-18T17:24:27Z",
        "number":754143
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-05-18T16:24:44Z",
  "number":365,
  "title":"Reading an object with colon in its name",
  "url":"https://github.com/scikit-hep/uproot5/discussions/365"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@nikoladze fixed a leaking-threads issue in XRootD (PR #338).\r\n\r\n@cozzyd added support for HTTP authentication (#339).\r\n\r\n@chrisburr added XRootD version checks to navigate around bugs that have been fixed (PR #358).\r\n\r\n@jpivarski fixed a leaking-threads issue in HTTP (PR #334), division by zero in calculating the number of entries (PR #336), adjusted to the new scikit-hep-testdata (#337), \r\n\r\n@jpivarski is developing the ability for Uproot to write files. At the moment, you can copy any non-TTree objects from another file or create new TObjStrings (PRs #341, #343, #344, #345, #346, #349, #352, #363).\r\n\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.0.8'>4.0.8</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-05-21T22:29:40Z",
  "number":367,
  "title":"4.0.8",
  "url":"https://github.com/scikit-hep/uproot5/discussions/367"
 },
 {
  "author":{
   "login":"GDamen"
  },
  "body":"Hello,\r\nfirst of all, thanks for the amazing work with uproot!\r\n\r\nI'm working on a small portable application to convert .csv to .root files in systems where running the full ROOT framework is not an option (oscilloscopes, etc.). It is my understanding that you are currently implementing in uproot4 the writing to output functionalities. Do you plan to also support writing vectors/arrays, strings, or other complex types to branch? I'm asking because uproot3 does not allow it.\r\n\r\nThanks!\r\nGab",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Vectors/arrays of numbers yes, strings yes, but more complex types than that no. The coverage will be the same as Uproot 3 (you have to dig into the pull requests to find information about saving vectors/arrays of numbers to ROOT files in Uproot 3, but it's there in a somewhat experimental state).\r\n\r\nNote that even though Uproot-writing is my top programming priority, I haven't been able to touch it in a few weeks because of a lot of summer conferences and tutorials. I'll likely get more time by August.",
     "createdAt":"2021-06-08T15:23:53Z",
     "number":840641,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-06-08T12:27:51Z",
  "number":375,
  "title":"Writing std::vector, std::string or complex types",
  "url":"https://github.com/scikit-hep/uproot5/discussions/375"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@jpivarski fixed `how=\"zip\"` for branches with the same name but different numbers of items (PR #369), fixed TH3 with `dd=True` (PR #372), and various tweaks for the 2021 tutorials (PR #376).\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.0.9'>4.0.9</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-06-11T02:49:11Z",
  "number":378,
  "title":"4.0.9",
  "url":"https://github.com/scikit-hep/uproot5/discussions/378"
 },
 {
  "author":{
   "login":"HDembinski"
  },
  "body":"uproot's design does not follow the One Way To Do It principle from the Zen of Python. The interface contains a lot of redundancy, instead of offering a minimal design. The cost of adding redundancy to the interface is longer docs, more required effort to learn the library from the user, and more unit tests to test all that interface. It adds cost both for the developer and the user.\r\n\r\nThe issues permeate the whole library, so I will use only an example for illustration. I pick `uproot.TTree.iterate`.\r\n* The keyword `filter_name` is redundant. The reason that both the keywords `expressions` and `filter_name` exist is probably because the author realized that one could have branch names that look like filter expressions, so that `expressions` cannot always be unambiguously interpreted as filters. The best conclusion from this correct analysis is to not support `filter_name` at all. It is easy in Python to grab all the branch names from the tree and filter them using powerful and concise Python expressions, like list comprehensions. There is no need to introduce a string-based DSL shortcut for this.\r\n* Likewise, `filter_typename`, `filter_branch` are superfluous. `cut` as well, as the docs say, `cut` is not faster than doing the filtering by hand in numpy, therefore is no reason to add this functionality.\r\n\r\nIt is important to resist feature requests for such trivial functionality. Users do not always know what they really need. It is analog to the saying, \"Give a man a fish and he is fed for a day. Show a man how to fish and he is fed for a life-time\". Whenever you build a DSL or a shortcut that solves a problem by saving a few keystrokes, you give the user a fish. It will make that user who requested it happy, but that user would have been served much better by pointing out how the problem he wants to solve can be solved in standard Python without adding extra shortcuts to the library. The shortcut only works for this library. The general Python pattern on how to solve the problem works everywhere. There are certainly exceptions to this rule for code that one has to type interactively a lot, but `uproot.TTree.iterate` is not something you type every cell of a notebook.\r\n\r\nPython's success is to a great deal build on being a well-designed language with a (mostly) well-designed standard library, which in turn is largely based on some core developers caring deeply about minimal sufficient designs that are used consistently everywhere. It requires active effort to fight interface entropy and redundancy and also an understanding why minimal sufficient interfaces are so powerful in the long-run.",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-06-17T12:08:21Z",
  "number":380,
  "title":"uproot design issues",
  "url":"https://github.com/scikit-hep/uproot5/discussions/380"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@beojan added custom behaviors for TGraph, RooCurve, and RooHist (PR #350).\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.0.10'>4.0.10</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-06-25T17:01:01Z",
  "number":382,
  "title":"4.0.10",
  "url":"https://github.com/scikit-hep/uproot5/discussions/382"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"Fixed #383.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.0.11'>4.0.11</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-06-30T18:20:23Z",
  "number":385,
  "title":"4.0.11",
  "url":"https://github.com/scikit-hep/uproot5/discussions/385"
 },
 {
  "author":{
   "login":"peguerosdc"
  },
  "body":"Hi! :wave: My use-case is the following:\r\n\r\nI have lots of files (>100) which add up to ~45GB of data and I want to fit a sklearn classifier with them, which require a `data` (2D) and `target` (1D) arrays.\r\n\r\nNot all of the events in my dataset are useful, so I would like to apply some cuts before fitting my classifier and as it is a lot of data, I can't read it all in one go.\r\n\r\nNot sure if this is possible, but I am trying to do it like this:\r\n\r\n````python\r\ndata = uproot.lazy(\r\n    f\"/data/*.root:my_branch\",\r\n    filter_name=vars_to_use,\r\n    step_size=step_size, num_workers=workers)\r\n# apply some cuts and store the result in \"clean\". For example:\r\nclean = data[ (data.mode == 4) ]\r\n# create the \"target\" array for the classifier\r\ntarget = (clean.charge == +1)\r\n````\r\n\r\nAt this ponit, I have one question: does `clean` store in memory all the events that pass the cuts (sorry, I didn't understand quite well this part in the docs, but I think the answer is **yes**)? I wanted to use `lazy` to avoid doing precisely that. I was expecting that by passing it to the classifier as the `data`, it would read the events only as they are required in the fitting process, but the need of applying cuts is then forcing me to still read all the data and store a large percentage of it.\r\n\r\nI am not sure if the inner workings of uproot and awkward-arrays make this possible, but from the point of view of the user, I think it could be solved by allowing `lazy` to apply cuts directly (the same way as `iterate`).\r\n\r\nAnyway, let's say I figure that out. I am testing with a small subset and when I try to fit my sklearn classifier with `model.fit(clean, target)`, I get the following error:\r\n\r\n````\r\nValueError: Expected 2D array, got 1D array instead:\r\n...\r\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\r\n````\r\n\r\nWhich is because `clean` is not a 2D array but one long \"awkward\" 1D array. Is there a workaround for this or is uproot not meant to work with sklearn and I am doing everything wrong? In `numpy` there's `reshape`, but not in `awkward` and `lazy` forces me to use `ak`.\r\n\r\nIs there another way to achieve what I want? Is it even possible with uproot? Is there an alternative to sklearn that's recommended to use with uproot?\r\n\r\nPlease let me know what you think or if I am misunderstanding something :smiley: ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"uproot.lazy defers reading until a slice of an array is requested, but Scikit-Learn's `fit` function asks for the entire sliced array. For Scikit-Learn to only pull one chunk of the array at a time, it would have to be knowledgeable about chunking\u2014the fitting algorithm would need to be able to deal with batches and the interface would have to recognize that slicing the input and training one slice at a time is a benefit. In other words, Scikit-Learn would have to be \"in on it [the batching of data].\" There might be some interface between Scikit-Learn and Dask, but Scikit-Learn didn't know about Awkward Arrays. (Which is why we want to add interfaces between Awkward and Dask, so that third party libraries would recognize them as lazy.)\r\n\r\nOn the specific point of slicing a lazy array, many kinds of slices are deferred, so I think your cut doesn't cause the array to be read. Passing it to Scikit-Learn's `fit` function definitely does, though.\r\n\r\nOn reshaping, an Awkward Array can't be arbitrarily reshaped because that presupposes that it's rectangular. However, this slice:\r\n\r\n```python\r\narray[:, np.newaxis]\r\n```\r\n\r\nwould do the same job, regardless of whether it's a rectangular NumPy array or an arbitrarily nested Awkward Array. (Scikit-Learn's recommendation could have been more general, but the authors of Scikit-Learn weren't thinking about nested data structures.)\r\n\r\nAs for your main problem, you'll have to manually slice up the array and feed it to `fit` one batch at a time. Some machine learning models can't be trained in batches, or at least, they're not guaranteed to get the same results when they are trained in batches, but often it can be done as an approximation. (I'm thinking of clustering, such needs all points for the exact answer, but would be close enough if iteratively trained in subsamples.) That's a fundamental issue that goes beyond interface. (Even with an interface that accepts a lazy array and extracts batches from it, the algorithm must be capable of batching...)\r\n\r\nA `for` loop over range slices (`array[start:stop]`) would keep the whole array from being read at once. However, using `iterate`, rather than `lazy` is equivalent and more carefully controls memory usage (because `iterate` _knows_ it's being used in a sequential loop and it can release the right memory after each step of the loop; `lazy` has to guess what memory to release based on least recently used).\r\n\r\nFinally, if you go the route of `iterate` and your data are not jagged, nested, or anything like that, you could use `iterate` with `library=\"np\"` and it would be faster. You pay for what you use, and if you're not using laziness or nested data structures, bypassing that infrastructure saves memory and CPU (not necessarily a lot in all cases, but nonzero).",
     "createdAt":"2021-07-07T23:31:11Z",
     "number":976991,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"peguerosdc"
        },
        "body":"Thank you for the very detailed response! It makes a lot of sense that Scikit-Learn needs to be aware of the batching which I hadn't considered. I will mark my question as answered.\r\n\r\nFor the curious, I ended up using `iterate`, but couldn't use `library=\"np\"` because of the structure of my data. I was interested in using `pandas` because I wanted to feed the data into an existing pandas flow, but I found it was faster to read the data using `library=\"ak\"`, applying the cuts and then using `ak.to_pandas()` (per batch) than to directly use `library=\"pd\"`.",
        "createdAt":"2021-07-13T20:02:42Z",
        "number":1000705
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"> I found it was faster to read the data using `library=\"ak\"`, applying the cuts and then using `ak.to_pandas()` (per batch) than to directly use `library=\"pd\"`.\r\n\r\nI'm not really surprised by that. It's hard to make categorical performance statements, but Pandas is generally not very fast. Doing the conversion to Pandas later, when there's less data to convert, could make a noticeable difference.\r\n\r\nTo be fair, part of the cost may be constructing the MultiIndex from the nested data: the \"`offsets`\" top-down description of nesting is almost immediately what you get out of the ROOT file and it's all that Awkward (and Arrow) needs, but a MultiIndex bottom-up description is a different way of representing that nesting (similar to what Parquet uses, by the way). `ak.to_pandas` also has to do that calculation, but later. Oh! And it also gets to use a specialized C function to do it. Uproot doesn't do a for loop over the data or anything, but it has to resort to NumPy tricks to avoid depending on Awkward Array. That could be it.",
        "createdAt":"2021-07-13T20:29:25Z",
        "number":1000816
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-07-07T22:41:07Z",
  "number":390,
  "title":"Can I fit a sklearn classifier using lazy?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/390"
 },
 {
  "author":{
   "login":"FlorianBury"
  },
  "body":"Hi all,\r\n\r\nAs uproot is completely decoupled from ROOT (and therefore PyROOT) I acknowledge I am probably slightly off-topic but I could use some guidance.\r\n\r\nI am working with PyROOT TH1s and TH2s and at some point need to convert back and forth to numpy arrays. Until now I was using a plain python loop with the usual `GetBinContent`, relatively slow but it was fine so far. Unfortunately I have now reached the point where I have to read few hundred TH2s that can have many bins and my computation time skyrocketed. \r\n\r\nI was using in the past `root_numpy.hist2array` and was happy with it I but could not in the current project because I also needed the bin errors which was not supported until [recently](https://github.com/scikit-hep/root_numpy/pull/304). If that is my only option then I would go for it but as `root_numpy` is currently deprecated I am looking for something more stable. \r\n\r\nUproot looks very interesting and I would gladly use it but from I understand there is no way to use a PyROOT object, I tried to dig a bit into uproot-method but I could not find a workaround.\r\n\r\nThe other alternatives that are proposed do not seem implementable either : converting from TTrees does not fit my purpose and the proposition to use RDataFrame as [here](https://root.cern.ch/doc/master/df026__AsNumpyArrays_8py.html) completely puzzles me.\r\n\r\nCould anyone provide some guidance as to what option seems more suitable, or whether I should stick with the deprecated root_numpy ? It would be very much appreciated.\r\n\r\nBest,\r\nFlorian",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"If you have histograms in a ROOT file, load one in Uproot and call `.to_numpy()`. (There are other methods like `values`, `errors`, and `axis(0).edges` for not control\u2014see the documentation.)\r\n\r\nIf your objects are in-memory PyROOT objects, then to use this method, you'd have to save them to a file. Uproot only recognizes serialized ROOT objects. We had talked about making a PyROOT-to-Uproot bridge by temporarily serializing them in in-memory files, but that seems circuitous, and we haven't tried it yet.\r\n\r\nAnother option is to write a C++ function with gInterpreter.Declare that calls GetBinContent in a compiled loop, filling an array. That would be a fast ROOT-only option.\r\n\r\nRDataFrame is only for non-binned data, like TTrees, so it didn't fit your case at all.",
     "createdAt":"2021-07-09T11:25:08Z",
     "number":983753,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"FlorianBury"
        },
        "body":"Forgot to mention that my objects are in-memory, saving them to root files only to reload them with uproot would require too much shuffling around. \r\n\r\nBeing able to go back and forth between PyROOT and Uproot would make my life much easier but I understand it was probably not meant to be that way from the start, and it might not be such a popular feature anyway.\r\n\r\nI could try the gInterpreter way though, thanks for the tip !",
        "createdAt":"2021-07-09T11:38:16Z",
        "number":983789
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I've been [nerd-sniped](https://xkcd.com/356/): I had to find out if the in-memory file method would work. This should probably be turned into an easier-to-use, high-level function in Uproot, but there could be sharp edges we haven't identified yet\u2014it would be easier to find them by doing this manually than if it's wrapped up and hidden in a function.\r\n\r\nSuppose that we have a PyROOT object like this:\r\n\r\n```python\r\nimport ROOT\r\nimport numpy as np\r\nimport uproot\r\n\r\nh = ROOT.TH1F(\"h\", \"\", 10, -5, 5)\r\n```\r\n\r\nWe can serialize the object into an in-memory file (no disk involved), like this:\r\n\r\n```python\r\nROOT.gInterpreter.Declare('''\r\nvoid copy_buffer_for_uproot(char* destination, TMessage& message) {\r\n    memcpy(destination, message.Buffer(), message.Length());\r\n}\r\n''')\r\n\r\nmessage = ROOT.TMessage(ROOT.kMESS_OBJECT)\r\nmessage.WriteObject(h)\r\n\r\nbuffer = np.empty(message.Length(), np.uint8)\r\nROOT.copy_buffer_for_uproot(memoryview(buffer), message)\r\n```\r\n\r\nA ROOT [TMessage](https://root.cern/doc/master/classTMessage.html) is an in-memory buffer, as though it were a file, but it can be for a single object, not on disk, and not compressed, which will make it easier to read back in Uproot. The above implementation lets the TMessage manage its own buffer and copy it into a NumPy array, `buffer` at the end. Alternatively, we could have provided the NumPy array as TMessage's own buffer with\r\n\r\n```python\r\nmessage = ROOT.TMessage(ROOT.kMESS_OBJECT)\r\nbuffer = np.empty(1000, np.uint8)\r\nmessage.SetBuffer(buffer, len(buffer), False)\r\n\r\nmessage.WriteObject(h)\r\n```\r\n\r\nand then look at `buffer[:message.Length()]` immediately after `message.WriteObject(h)`. That method would avoid copying the serialized buffer, but then we'd have to guess an appropriate size for the `buffer`; if `1000` weren't large enough, I don't know what would happen (error message? segfault?), so the technique of copying is certainly safer.\r\n\r\nYou might also be wondering why we had to define a `copy_buffer_for_uproot` C++ function to do that copy. It's because PyROOT interprets functions that return `char*`, such as `TMessage::Buffer`, as Python strings. We really need it to be a pointer (`void*` would have been a better type than `char*`), and the only way I could find to do that is to write it in C++.\r\n\r\nNow that we have the raw bytes of a TH1F object in a NumPy `buffer`, we can read that back in Uproot with\r\n\r\n```python\r\nclass FakeFile(object):\r\n    def class_named(self, classname, version=None):\r\n        return uproot.class_named(classname, version=version)\r\n\r\nfakefile = FakeFile()\r\n\r\nchunk = uproot.source.chunk.Chunk.wrap(None, buffer)\r\ncursor = uproot.source.cursor.Cursor(8)\r\n\r\nh2 = uproot.deserialization.read_object_any(chunk, cursor, {}, fakefile, fakefile, None)\r\n```\r\n\r\nThe `h2` is an Uproot histogram with the same content as `h`, and it's a detached copy\u2014the original `h` can be deleted, changed, etc. without affecting `h2`. The [read_object_any](https://uproot.readthedocs.io/en/latest/uproot.deserialization.read_object_any.html) function reads any type of standalone ROOT object (anything that could be written into a TDirectory), not just histograms, though if you tried this with a TTree, you'd only get the TTree metadata, not any of the event data, so it should be used on self-contained things _like_ histograms. The TMessage itself has an 8-byte header, so the `cursor` steps past that to start on the actual data.\r\n\r\nWhy do we need a FakeFile? The deserialization function has to look up TStreamerInfo to know how to read a serialized class, for a specified version of that class. It first queries the file the object came from because ROOT files should ship with TStreamerInfos for all the classes they contain (that \"should\" is a complicated story). But in this case, the object does not come from a file, and the TMessage doesn't contain any TStreamerInfos, as messages are supposed to be small and lightweight. So we have to defer to any globally defined [class models](https://uproot.readthedocs.io/en/latest/uproot.model.Model.html) in `uproot.classes`, which either came from files that have already been opened in Uproot or they were hard-coded and shipped with Uproot.\r\n\r\nIf the class model isn't there, the above will fail with a DeserializationError\u2014that's another rough edge, and a properly wrapped up function should check to be sure we have a class model for the exact version that this ROOT will write into the TMessage\u2014it matters which ROOT is imported in `import ROOT`. The above example worked because this ROOT (6.24/02) is writing TH1F version 3 into the TMessage:\r\n\r\n```python\r\n>>> ROOT.TClass.GetClass(\"TH1F\").GetClassVersion()\r\n3\r\n```\r\n\r\nand Uproot recognizes version 3:\r\n\r\n```python\r\n>>> uproot.class_named(\"TH1F\").known_versions\r\n{3: <class 'uproot.models.TH.Model_TH1F_v3'>}\r\n```\r\n\r\nThat sort of thing could be automatically checked, though the error message would have a lot of explaining to do if ROOT writes version 2 and Uproot reads version 3!",
     "createdAt":"2021-07-09T14:14:05Z",
     "number":985507,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"FlorianBury"
        },
        "body":"I certainly did not expect you would go down the rabbit hole, thanks !\r\n\r\nIn the meantime I have implemented the C++ equivalent with the `gInterpreter`, do you think I could apply your snippets above or better wait for a more safe implementation within uproot ?\r\nNote that I am currently running ROOT 6.12 (I can upgrade my version but being on a HPC it might not be instantaneous), hence \r\n```python\r\n>>> ROOT.TClass.GetClass(\"TH1F\").GetClassVersion()\r\n2 \r\n``` \r\n\r\nAlso, as I am currently doing PyROOT -> numpy as well as numpy -> PyROOT, but I don't think the latter could use your IO  method. Disclaimer : this is not nerd-sniping ! There must be more funny things to implement in uproot than this, I am just wondering if there is a _relatively_ _easy_ way.\r\n",
        "createdAt":"2021-07-09T14:40:49Z",
        "number":985604
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"We should probably have this PyROOT \u2192 Uproot functionality anyway. I know that we've talked about it somewhere\u2014someone else asked for it\u2014but I can't find that discussion anywhere to link it in here.\r\n\r\nThe method described above is more general than histograms\u2014it should work for any subclass of TObject other than TTree (for the caveat above). However, the class models need to be available because a TMessage doesn't have TStreamerInfos, and your case (ROOT 6.12, whose TH1F version is 2) is an example that wouldn't work. Since it turned out to be pretty easy to find an example of a ROOT distribution with a version that wouldn't work, perhaps this should be addressed in the high-level function. Perhaps the same technique could be used to serialize the ROOT version's TStreamerInfos into TMessages and then deserialize them with Uproot before translating the actual object, but then we'd have to also search for all dependencies of a given class. (For instance, TH1F depends on TH1, TNamed, TObject, TString, TAttLine, TAttFill, TAttMarker, TAxis, TAttAxis, THashList, TList, TSeqCollection, and TCollection; we'd have to keep passing streamers from PyROOT to Uproot until all of these dependencies have been met\u2014not impossible, just more code to write!)\r\n\r\nIf you have a gInterpreter-defined function that gets values out of a TH1/TH2 into a NumPy array using GetBinContent in the compiled loop, that will work for any version of ROOT, though only for histograms, naturally. Since that's your specific problem, then it sounds like you have a solution and you should go for it.",
        "createdAt":"2021-07-09T15:06:31Z",
        "number":985718
       },
       {
        "author":{
         "login":"FlorianBury"
        },
        "body":"Then I am happy to have triggered the thinking on your side !\r\n\r\nThanks again !",
        "createdAt":"2021-07-09T15:12:09Z",
        "number":985736
       }
      ],
      "totalCount":3
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"FYI: this is starting to be implemented in PR #420.",
     "createdAt":"2021-08-24T22:38:33Z",
     "number":1230058,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"https://github.com/scikit-hep/uproot4/blob/ccdec6918620507155cbede024ca92ca251a4f82/tests/test_0420-pyroot-uproot-interoperability.py#L14-L28",
        "createdAt":"2021-08-25T00:51:23Z",
        "number":1230414
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"PR #420 is set for auto-merging, and that will likely happen in a half-hour. Now it's bidirectional: any PyROOT object can be converted into an Uproot Model using\r\n\r\n```python\r\nuproot.from_pyroot(pyroot_object)\r\n```\r\n\r\nand _serializable_ Uproot Models (which consists of only TObjString and histograms, at the moment) can be converted into PyROOT objects using\r\n\r\n```python\r\nuproot_object.to_pyroot()\r\n```\r\n\r\nSerializability is key because both directions go through TMessages. All ROOT objects in PyROOT can be serialized into a TMessage, but only a few Uproot Models have been implemented for writing to files, which is what we reuse to make the TMessage.\r\n\r\nIn principle, all Uproot Models that come from files could be made automatically serializable by keeping a copy of the bytes they were deserialized from, but that would make all objects bigger and I don't know what I think of that.\r\n\r\nThis also ties into writing files because the above makes all PyROOT objects writable by Uproot:\r\n\r\n```python\r\nwith uproot.recreate(\"filename.root\") as file:\r\n    file[\"hist\"] = pyroot_histogram\r\n```",
        "createdAt":"2021-08-26T00:17:01Z",
        "number":1236588
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":3
  },
  "createdAt":"2021-07-09T09:33:38Z",
  "number":392,
  "title":"Convert PyROOT TH1/TH2 to numpy arrays",
  "url":"https://github.com/scikit-hep/uproot5/discussions/392"
 },
 {
  "author":{
   "login":"delaere"
  },
  "body":"Hi,\r\n\r\nI have a pretty standard tree (showing only part of it):\r\n```\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nnMuons               | int32_t                  | AsDtype('>i4')\r\nMuonsPt              | double[]                 | AsJagged(AsDtype('>f8'))\r\ninvMass              | double                   | AsDtype('>f8')\r\n```\r\nnMuons can be 0, in which cas MuonsPt is empty. I want to filter on the MuonsPt, so I naively do:\r\n```\r\ntree.arrays([\"invMass\"],\"(nMuons==2) & (MuonsPt[:,0]>20)\")\r\n```\r\nbut it fails because the cut on MuonsPt is evaluated even for events with nMuons==0, and fails with an index out of range error.\r\n\r\nI managed to do what I want in two steps (first filtering on nMuons, later on MuonsPt) with the array filter synthax, but I would prefer the cut string approach for pedagogical reasons.\r\n\r\nWhat would you suggest in such a case?\r\n\r\nThanks!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"This is not so much about making it a cut string in Uproot as finding a one-liner in Awkward Array. Since operations are columnar, `(nMuons == 2)` is calculated independently of `(MuonsPt[:, 0] > 20)` before computing their intersection (`&`), and the latter is invalid for some array entries. For this application, you could use the `nMuons == 2` as a slice on the first dimension, rather than selecting everything in the first dimension with `:`.\r\n\r\nFor instance, if you start with\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> array = ak.Array([[1, 2, 3], [], [4, 5]])\r\n>>> array\r\n<Array [[1, 2, 3], [], [4, 5]] type='3 * var * int64'>\r\n```\r\n\r\nyou can't do\r\n\r\n```python\r\n>>> (ak.num(array) > 0) & (array[:, 0])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward/highlevel.py\", line 996, in __getitem__\r\n    tmp = ak._util.wrap(self.layout[where], self._behavior)\r\nValueError: in ListOffsetArray64 attempting to get 0, index out of range\r\n```\r\n\r\nbut you can do\r\n\r\n```python\r\n>>> array[ak.num(array) > 0, 0]\r\n<Array [1, 4] type='2 * int64'>\r\n```\r\n\r\nor even\r\n\r\n```python\r\n>>> array[ak.num(array) > 0][:, 0]\r\n<Array [1, 4] type='2 * int64'>\r\n```\r\n\r\nThe thing is that the selection of events (first dimension) that makes the selection of particles (second dimension) valid has to be applied to the same array. When `ak.num(array) > 0` is applied to the first dimension, there aren't any events left that don't have muons.\r\n\r\nFollowing this logic, I was about to say that your cut string could be `\"MuonsPt[nMuons == 2, 0] > 20\"`. But the `> 20` part is a cut on \"events with two muons,\" and you can't apply that cut to \"events,\" which is what the `cut` argument to `tree.arrays` does. In my example, `array[ak.num(array) > 0][:, 0]` returns a boolean array of length 2, but `array` has length 3: it won't be an applicable cut. Here's something you can do instead:\r\n\r\n```python\r\nMuonsPt.mask[nMuons == 2][:, 0] > 20\r\n```\r\n\r\nBy analogy (going back to my example),\r\n\r\n```python\r\n>>> array.mask[ak.num(array) > 0]\r\n<Array [[1, 2, 3], None, [4, 5]] type='3 * option[var * int64]'>\r\n```\r\n\r\nputs a \"None\" in the event that fails the multiplicity cut, and then\r\n\r\n```python\r\n>>> array.mask[ak.num(array) > 0][:, 0]\r\n<Array [1, None, 4] type='3 * ?int64'>\r\n```\r\n\r\npasses that \"None\" through, and\r\n\r\n```python\r\n>>> array.mask[ak.num(array) > 0][:, 0] > 1\r\n<Array [False, None, True] type='3 * ?bool'>\r\n```\r\n\r\ncreates a boolean-or-None array, which at least has the right length; it applies to \"all events,\" not just \"events with two muons\" (going back to your example, by analogy). This would drop values of `\"invMass\"` that are in the same position as a \"False\" and it would return \"None\" for values that are in the same position as a \"None\". Maybe you don't want that; you probably want both of those cases to drop values. You want the equivalent of\r\n\r\n```python\r\n>>> ak.fill_none(array.mask[ak.num(array) > 0][:, 0] > 1, False)\r\n<Array [False, False, True] type='3 * bool'>\r\n```\r\n\r\nwhich, in your case, is\r\n\r\n```\r\nak.fill_none(MuonsPt.mask[nMuons == 2][:, 0] > 20, False)\r\n```\r\n\r\nThis replaces all \"None\" values with \"False\" values, giving you a purely boolean array with \"False\" for both the case of not having two muons and the case of the first muon having a pT less than 20.\r\n\r\nAs usual, turning things into one-liners doesn't always make them more clear. If you're doing this for pedagogy, will your students/audience be able to read that one-liner? They'd need to know about [ak.mask](https://awkward-array.readthedocs.io/en/latest/_auto/ak.mask.html) and [ak.fill_none](https://awkward-array.readthedocs.io/en/latest/_auto/ak.fill_none.html).\r\n\r\n(I'm not sure it was a great idea to introduce the `expressions` and `cut` arguments in [TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) because it puts an artificial boundary between \"calculations that can be done in the `expressions`/`cut` strings\" and \"all possible calculations.\" If someone is following the example of a script that manages to express everything in `expressions`/`cut` but they need to extend it to do something that can't, it won't be clear from the source script how to do that. It's like the ROOT TBrowser: double-clicking is an easier way of getting to the plots, but the fact that double-clicking is possible prevents some new students from finding out that they can also `TTree::Draw`, and those students don't know how to apply a cut (just as `TTree:Draw` prevents students from finding out that they can compute more complex functions by scripting). I think this is why Python has an \"only one way to do something\" ideal, because the existence of a shortcut makes it harder to find out that there's a non-shortcut method, and sometimes the non-shortcut method is needed. So I'm just a little nervous that I made a bad design decision by including the `cut` argument at all...)",
     "createdAt":"2021-08-23T14:08:36Z",
     "number":1222610,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"delaere"
     },
     "body":"Thanks a lot for this detailed and interesting explanation!\r\nI agree with your conclusion that this is probably too advanced for students in my context. I wanted to reduce the exposition to specific array syntax, and the result would be just the opposite. \r\n\r\nThanks again!",
     "createdAt":"2021-08-24T12:56:01Z",
     "number":1227342,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-08-23T09:00:54Z",
  "number":417,
  "title":"Applying cuts on branches that are sometimes empty",
  "url":"https://github.com/scikit-hep/uproot5/discussions/417"
 },
 {
  "author":{
   "login":"unimatrixzero-tertiaryadjunct"
  },
  "body":"Hi, \r\n\r\nI am trying to get data from a root file using uproot4 and I am having trouble with some of the branches. When I use TBranch.branches, I get no subbranches even though there should be quite a few of them (I verified this using TBrowser in ROOT). Does anyone know what could be causing this issue? Also, is there any other way for me to access these subbranches using uproot? \r\n\r\nThanks. ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Well, `TBranch.branches` only goes one level deep, but `TBranch.keys()`, `TBranch.values()`, and `TBranch.items()` are recursive by default. But if you get _zero_ subbranches from `TBranch.branches`, then recursion won't help. Maybe also look in `TTree.show()`?\r\n\r\nAll of these methods go to the same place: the branches that are nested within the TTree/other TBranches (through a TObjArray). There's nowhere else to look.",
     "createdAt":"2021-08-24T22:18:15Z",
     "number":1229974,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"unimatrixzero-tertiaryadjunct"
        },
        "body":"Thanks for your response. I have tried TTree.show(), TBranch.keys(), TBranch.values(), and TBranch.items(). TTree.show() gave me this:\r\n\r\nname                 | typename                                           | interpretation                                                                                      \r\n---------------------+----------------------------------------------------+----------------------------------------------------------\r\nSim                  | VASimulationData                                   | AsStridedObjects(Model_VASimulationData_v8)\r\n\r\nbut no information about subbranches within 'sim'. And as you said, TBranch.keys(), TBranch.values(), and TBranch.items() gave me zero subbranches. I also tried accessing the data from this branch directly using TBranch.array(), but I got a ValueError saying that \"basket 0 has the wrong number of bytes (13860) for interpretation AsStridedObjects(Model_VASimulationData_v8)\". And when I tried Model_TBasket.array() on the other baskets of the branch, I pretty much got the same error. \r\n\r\nI thought about using AsStridedObjects.basket_array(), but I am not sure what to put for the 'cursor_offset' argument (I was able to find information about the other arguments in the documentation on TBranch objects and Model_TBasket objects). Am I on the right track or is there a better way to access the data from the 'sim' branch? ",
        "createdAt":"2021-08-25T01:03:36Z",
        "number":1230438
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"It looks like the TTree has exactly one TBranch, and that TBranch has zero subbranches. So `TTree.keys()` etc. should return a list of that one TBranch (and `TBranch.keys()` on the TBranch should show zero). They should be presenting a consistent picture\u2014a strong \"should\" because they all access the same data.\r\n\r\nROOT's TBrowser may be presenting data members within that one TBranch as things to click on and get a plot, but they're not TBranches. Whether or not they're TBranches depends on whether the file was written with splitting on or off; ROOT considers that an implementation detail, while Uproot considers that user-visible. Since ROOT considers it an implementation detail, TBrowser would show you all the variables you can click on the same way, whether they're implemented in subbranches or a single TBranch. That could be why they look different.\r\n\r\nNot being able to read that TBranch is a bug in Uproot; please report it as an Issue with an example file. Uproot is thinking `VASimulationData` a simple class type that can be read in a \"strided\" way (faster, avoids Python iteration, but only works if the class struct has fixed size, made of types like `int` and `float`, not strings or nested lists). Maybe Uproot is assigning the wrong interpretation and it needs to be a general object, or maybe something else is going wrong. To figure that out, I'll need to see an example of the file, so that I can `show_streamers(\"VASimulationData\")` to verify whether it can be strided and see why the predicted size is not matching the actual TBasket size (which needs to be an integer multiple of a single `VASimulationData` object: that's what the error message is saying).\r\n\r\n`basket_array` would give you the same error for the same reasons. There's also `debug` and `debug_array` methods that I'll probably use, which dump it as raw bytes (and therefore the size doesn't have to be an integer multiple of anything).",
        "createdAt":"2021-08-25T16:26:47Z",
        "number":1234159
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-08-24T20:27:45Z",
  "number":419,
  "title":"No subbranches showing up with TBranch.branches",
  "url":"https://github.com/scikit-hep/uproot5/discussions/419"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"**The main new feature in this release is that Uproot 4 can now write ROOT files.** It is no longer necessary to use Uproot 3 for anything: Uproot 4's capabilities exceed Uproot 3's in all ways that are currently known to me.\r\n\r\nThis was implemented in PRs #405, #406, #408, #409, #412, #414, #415, #416, #420, #421, and was described in Discussion #321. All the new functions are documented, and an overview is presented in the [Getting Started Guide](https://uproot.readthedocs.io/en/latest/basic.html#opening-a-file-for-writing).\r\n\r\nOther updates for this release:\r\n\r\n  * @jpivarski removed warning when XRootD is installed but not used: PR #388.\r\n  * @chrisburr allowed XRootD version strings to be non-numeric (e.g. from a self-built copy of the library): PR #395.\r\n  * @bnavigator marked test 0220 with the network tag, since it uses the network: PR #396.\r\n  * @jpivarski handled dimensions in a leaf-list: #399.\r\n  * @jpivarski removed '@' fields from lazy Forms, fixing lazy TLorentzVector: PR #403.\r\n  * @klieret fixed links to source in Sphinx: PR #410.\r\n  * @jpivarski allowed Float16_t to have up to 32 bits: PR #411.\r\n  * @veprbl added support for reading TDatime: PR #407.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.1.0'>4.1.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-08-27T17:46:12Z",
  "number":424,
  "title":"4.1.0",
  "url":"https://github.com/scikit-hep/uproot5/discussions/424"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@veprbl added support for reading ROOT TTables: PR #418.\r\n\r\n@jpivarski fixed the warning handling for old XRootD clients: PR #425.\r\n\r\n@jpivarski fixed several performance bugs and one bug-bug in writing histograms and TTrees: PRs #426 and #428. See the GitHub conversations for measurements.\r\n\r\nA quantitative rule of thumb derived from that work is that you want to call [TTree.extend](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableTree.html#uproot-writing-writable-writabletree-extend) with no less than ~100 kB per array branch. This is roughly the same scale that would be preferred for reading TTree data (you want TBaskets to be about 100 kB or larger), but in writing, you get to control it.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.1.1'>4.1.1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-08-30T22:10:29Z",
  "number":429,
  "title":"4.1.1",
  "url":"https://github.com/scikit-hep/uproot5/discussions/429"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Assume I have 3 branches in an input file called ```file``` in a tree ```tree```:\r\n```branch1: type = AsJagged(AsDtype('>f4'))```\r\n```branch2: type = AsJagged(AsDtype('>f4'))```\r\n```branch3: type = AsDtype('>f4')```\r\nMy aim is to read in these 3 branches using uproot and save them into a dataframe. If I run \r\n```\r\nfor batch in uproot.iterate(file:tree, filter_name=[branch1, branch2, branch3], library='pd'):\r\n      print(batch)\r\n```\r\n\r\nI run into the following error from the iterate method call:\r\n```\r\n  File \"/afs/cern.ch/user/m/maly/thbb_env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 226, in iterate\r\n    arrays = library.global_index(item, global_offset)\r\n  File \"/afs/cern.ch/user/m/maly/thbb_env/lib/python3.7/site-packages/uproot/interpretation/library.py\", line 1084, in global_index\r\n    index = arrays.index.values  # pandas<0.24.0\r\nAttributeError: 'builtin_function_or_method' object has no attribute 'values'\r\n\r\n```\r\n\r\nThis problem is not present when I read the same 3 branches with \r\n```\r\nuproot.open(file:tree).arrays(filter_name=[branch1, branch2, branch3], library = 'pd' )\r\n```\r\n, which yields a tuple of dataframes with length = number of jagged arrays being read as expected (which is 2 here). \r\nI can't see anything in the ```uproot``` documentation which prohibiits reading multiple jagged arrays into pandas dataframes via the ```iterate``` method, so I'm not sure what is causing the above error. \r\n\r\nI am using version ```4.0.0``` of ```uproot```. ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I moved this Discussion to Issue #433 because it's a bug-report. (I also moved my answer there.)",
     "createdAt":"2021-09-01T14:29:38Z",
     "number":1265940,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-09-01T13:27:56Z",
  "number":430,
  "title":"How do I read multiple branches with jagged structure through ```uproot.iterate()```",
  "url":"https://github.com/scikit-hep/uproot5/discussions/430"
 },
 {
  "author":{
   "login":"maxnoe"
  },
  "body":"Trying to read a ROOT file with custom objects inside, I get this error:\r\n\r\n```\r\nDeserializationError: while reading\r\n\r\n    MHMcCollectionArea version 14 as uproot.dynamic.Model_MHMcCollectionArea_v14 (14798723 bytes)\r\n        MH version 2 as uproot.dynamic.Model_MH_v2 (17 bytes)\r\n            MParContainer version 0 as uproot.dynamic.Model_MParContainer_v0 (6 bytes)\r\n                (base): <TObject None None at 0x7f74b59c1ac0>\r\nBase classes for MParContainer: (TObject)\r\n\r\nexpected 6 bytes but cursor moved by 18 bytes (through MParContainer)\r\nin file ./Output_flute.root\r\nin object /MHMcCollectionAreaEtrue;1\r\n```\r\n\r\nDo I interpret this correctly that *something* is wrong with the streamer info for the `MParContainer` base class?\r\n\r\nAny hints on what I can try to fix would be appreciated.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"The DeserializationError print-out shows what has been deserialized up to the point where it encountered an error, which usually provides some clues. In this case, it thought that the MParContainer's version is 0, which is odd because most version numbers start with 1. (I don't know if that is a necessary rule.) The fact that both of the TObject members, fProcessID and fBits, are both None, is also very unusual. I would guess that the actual error happened before getting to MParContainer.\r\n\r\nIs 14798723 a reasonable number of bytes for MHMcCollectionArea? That would be 14 MiB, something that carries a real dataset, maybe some arrays or lots of small objects.\r\n\r\nThe next steps for digging more deeply is to print out the streamers for these objects. If you've opened a file as `f`, that's the ReadOnlyDirectory; you can get streamers from the ReadOnlyFile within it as `f.file`, using the [ReadOnlyFile.show_streamers](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyFile.html#show-streamers) method:\r\n\r\n```python\r\nf.file.show_streamers()\r\n```\r\n\r\nor to narrow in on MParContainer and its dependencies,\r\n\r\n```python\r\nf.file.show_streamers(\"MParContainer\")\r\n```\r\n\r\nEach of these is a prescription for how-many-bytes-per-field in each object, and base classes (\"BASE\") are serialized as nested objects.\r\n\r\nFor TTree data (which this is not), there's a `debug` method to look at the raw bytes and diagnose what went wrong. (For TTrees, it is more difficult to get the correct interpretation of the data because the type information is spread around among TBranch metadata, TLeaf metadata, and streamers. For non-TTree data, it's always just streamers.) We don't have a method for looking at the raw bytes of an object, but here's a way to do it, if you want to give it a try:\r\n\r\n```python\r\nkey = f.key(\"path/to/object\")\r\n```\r\n\r\nwill give you a [uproot.reading.ReadOnlyKey](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyKey.html), and\r\n\r\n```python\r\nchunk, cursor = key.get_uncompressed_chunk_cursor()\r\n```\r\n\r\ndoes all the reading and decompression but not any of the interpretation, which fails for your object. Then\r\n\r\n```python\r\ncursor.debug(chunk, limit_bytes=80)\r\n```\r\n\r\nprints out raw bytes of the object, up to `limit_bytes=80`, with both decimal and character interpretations. The character interpretation is good for spotting words, like names, titles, and classnames, while the decimal interpretation is good for finding relevant (big-endian) numbers. This [Cursor.debug](https://uproot.readthedocs.io/en/latest/uproot.source.cursor.Cursor.html#debug) method has a lot of options; it's sometimes also useful to set a dtype to interpret the data as floating point numbers. Each object begins with a byte count (e.g. the 14798723 bytes for MHMcCollectionArea), which are easy to spot because they show up as `64` decimals. ROOT's flag to indicate a byte count is a `64` in the high byte:\r\n\r\n```python\r\n>>> np.array([uproot.const.kByteCountMask], \">i4\").view(\"u1\")\r\narray([64,  0,  0,  0], dtype=uint8)\r\n```\r\n\r\n... **Or you can post a small file and I'll look into it when I can find the time.** But the above procedure is what I would do: (1) print out the streamers, (2) print out the raw bytes, and (3) follow the raw bytes through the streamers to see what assumption is wrong.\r\n\r\nThe code generated from the streamers is a class returned by\r\n\r\n```python\r\nf.file.class_of(\"MHMcCollectionArea\")   # also in uproot.dynamic after the first time you attempt to read it\r\n```\r\n\r\nwhich has a `class_code` attribute (string) that you can print out. That auto-generated code is what is failing. The wrong assumption would be in the translation of streamer data into that Python code.",
     "createdAt":"2021-09-08T14:14:33Z",
     "number":1296481,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"maxnoe"
        },
        "body":"Thanks for the quick and detailed response, I have a deeper look later. \r\n\r\nLooking at the code, MParContainer indeed has a `ClassDef(MParContainer, 0)`, and the size of 14MB uncompressed is also not unrealistic, the file is 11MB in total and most of the data should be in that object. \r\n\r\nI'll try if I can figure something out, I can also look into the file using the software used to write it, if that helps narrowing it down (proprietary software build on root)",
        "createdAt":"2021-09-08T15:37:24Z",
        "number":1296895
       },
       {
        "author":{
         "login":"maxnoe"
        },
        "body":"Looking at the `f.file.show_streamers`, I get this for `f.file.show_streamers(\"MH\")`:\r\n\r\n```\r\nIn [15]: f.file.show_streamers('MH')\r\nTObject (v1)\r\n    fUniqueID: unsigned int (TStreamerBasicType)\r\n    fBits: unsigned int (TStreamerBasicType)\r\n\r\nMParContainer (v0): TObject (v1)\r\n\r\nMH (v2): MParContainer (v0)\r\n    fSerialNumber: unsigned char (TStreamerBasicType)\r\n    fNumExecutions: unsigned int (TStreamerBasicType)\r\n```\r\n\r\nBut a `KeyError(1)` for `f.file.show_streamers('MHMcCollectionArea')`: (not the full TB):\r\n```\r\n~/.local/lib/python3.9/site-packages/uproot/streamers.py in _dependencies(self, streamers, out)\r\n    452         out.append((self.name, self.class_version))\r\n    453         for element in self.elements:\r\n--> 454             element._dependencies(streamers, out)\r\n    455 \r\n    456 \r\n\r\n~/.local/lib/python3.9/site-packages/uproot/streamers.py in _dependencies(self, streamers, out)\r\n    770                 streamer = streamer_versions[max(streamer_versions)]\r\n    771             else:\r\n--> 772                 streamer = streamer_versions[base_version]\r\n    773             if (\r\n    774                 streamer is not None\r\n```\r\n\r\nHowever, in the full `f.file.show_streamers`, this appears for `MHMcCollectionArea`:\r\n\r\n```\r\nMHMcCollectionArea (v14): MH (v2)\r\n    fHistAllorig: TH3D* (TStreamerObjectPointer)\r\n    fHistAll: TH3D* (TStreamerObjectPointer)\r\n    fHistWeights: TH3D* (TStreamerObjectPointer)\r\n    fHistSel: TH3D* (TStreamerObjectPointer)\r\n    fHistCol: TH3D* (TStreamerObjectPointer)\r\n    fHistColCoarse: TH2D* (TStreamerObjectPointer)\r\n    fHistEestOverEtrue: TH2D* (TStreamerObjectPointer)\r\n    fHistRadOverEtrue: TH2D* (TStreamerObjectPointer)\r\n    fIsFineBinOK: TH2C* (TStreamerObjectPointer)\r\n    fHistSelInEestBin: TClonesArray (TStreamerObject)\r\n    fHistColInEestBin: TClonesArray (TStreamerObject)\r\n    fHistColInEestBin_orig: TClonesArray (TStreamerObject)\r\n    fAreaMax: double (TStreamerBasicType)\r\n    fMinEvents: int (TStreamerBasicType)\r\n    fMaxRelError: float (TStreamerBasicType)\r\n    fMinFractionOfGoodBins: float (TStreamerBasicType)\r\n    fMaxRelEtrueDepartureFromEest: float (TStreamerBasicType)\r\n    kWobblePatch: bool (TStreamerBasicType)\r\n    fAzBins: int (TStreamerBasicType)\r\n    fUseEnergyEst: bool (TStreamerBasicType)\r\n    fMcEventsNtuple: TNtupleD* (TStreamerObjectPointer)\r\n    fStoreMCEvents: bool (TStreamerBasicType)\r\n    fMcRingAnalysis: bool (TStreamerBasicType)\r\n    fOffsetRingMin: double (TStreamerBasicType)\r\n    fOffsetRingMax: double (TStreamerBasicType)\r\n    fOffsetRingMed: double (TStreamerBasicType)\r\n    fBinsx: MBinning (TStreamerObject)\r\n    fBinsy: MBinning (TStreamerObject)\r\n    fBinsz: MBinning (TStreamerObject)\r\n```",
        "createdAt":"2021-09-08T15:44:18Z",
        "number":1296935
       }
      ],
      "totalCount":2
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"So when it tries to evaluate `streamer_versions[base_version]`, it's passing `base_version = 1` (which does not exist) instead of `2` (which does, and is what's clearly listed in the full print-out)?\r\n\r\nThat's definitely sounding like some Uproot bug. Could you get the 11 MB file to me somehow (here, publicly, or to jpivarski at GMail, privately) and I'll at least figure out why it thinks the dependency has the wrong `base_version`?",
     "createdAt":"2021-09-08T15:54:51Z",
     "number":1297018,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"maxnoe"
        },
        "body":"Here is the file (gzipped for github): \r\n[Output_flute.root.gz](https://github.com/scikit-hep/uproot4/files/7130221/Output_flute.root.gz)\r\n",
        "createdAt":"2021-09-08T15:56:47Z",
        "number":1297031
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I just got a chance to look into it, and it's looking more like a weird file than a bug in Uproot.\r\n\r\nUsually, we're able to show all the streamers with their dependencies\u2014I've been focusing first on this problem of hitting a KeyError when trying to walk up the dependency chain. To print out one streamer without dependencies, you can do:\r\n\r\n```python\r\n>>> f.file.streamers[\"MHMcCollectionArea\"][14].show()\r\nMHMcCollectionArea (v14): MH (v2)\r\n    fHistAllorig: TH3D* (TStreamerObjectPointer)\r\n    fHistAll: TH3D* (TStreamerObjectPointer)\r\n    fHistWeights: TH3D* (TStreamerObjectPointer)\r\n    fHistSel: TH3D* (TStreamerObjectPointer)\r\n    fHistCol: TH3D* (TStreamerObjectPointer)\r\n    fHistColCoarse: TH2D* (TStreamerObjectPointer)\r\n    fHistEestOverEtrue: TH2D* (TStreamerObjectPointer)\r\n    fHistRadOverEtrue: TH2D* (TStreamerObjectPointer)\r\n    fIsFineBinOK: TH2C* (TStreamerObjectPointer)\r\n    fHistSelInEestBin: TClonesArray (TStreamerObject)\r\n    fHistColInEestBin: TClonesArray (TStreamerObject)\r\n    fHistColInEestBin_orig: TClonesArray (TStreamerObject)\r\n    fAreaMax: double (TStreamerBasicType)\r\n    fMinEvents: int (TStreamerBasicType)\r\n    fMaxRelError: float (TStreamerBasicType)\r\n    fMinFractionOfGoodBins: float (TStreamerBasicType)\r\n    fMaxRelEtrueDepartureFromEest: float (TStreamerBasicType)\r\n    kWobblePatch: bool (TStreamerBasicType)\r\n    fAzBins: int (TStreamerBasicType)\r\n    fUseEnergyEst: bool (TStreamerBasicType)\r\n    fMcEventsNtuple: TNtupleD* (TStreamerObjectPointer)\r\n    fStoreMCEvents: bool (TStreamerBasicType)\r\n    fMcRingAnalysis: bool (TStreamerBasicType)\r\n    fOffsetRingMin: double (TStreamerBasicType)\r\n    fOffsetRingMax: double (TStreamerBasicType)\r\n    fOffsetRingMed: double (TStreamerBasicType)\r\n    fBinsx: MBinning (TStreamerObject)\r\n    fBinsy: MBinning (TStreamerObject)\r\n    fBinsz: MBinning (TStreamerObject)\r\n```\r\n\r\nYou had to first look at the `f.file.streamers[\"MHMcCollectionArea\"]` dict to see that the only defined version number was 14, or maybe use [ReadOnlyFile.streamer_named](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyFile.html#streamer-named), which gives you the maximum version in a file if you don't specify.\r\n\r\nThe KeyError happens because TNtupleD (above) depends on TTree version 19, which depends on TAttLine version 1, but your file only has version 2 (brought in by the histograms). Maybe ROOT doesn't write multiple versions of the same class's streamer to a file? At the very least, I should put in some contingency in the dependency-walking code to not crash if the version is not available.\r\n\r\nThat will be a PR soon, but for now it shows you what you're working with:\r\n\r\n```python\r\n>>> f.file.show_streamers(\"MHMcCollectionArea\")\r\nMBinning (v1): MParContainer (v0)\r\n    fEdges: TArrayD (TStreamerObjectAny)\r\n    fType: unsigned char (TStreamerBasicType)\r\n\r\nTRefTable (v3): TObject (v1)\r\n    fSize: int (TStreamerBasicType)\r\n    fParents: TObjArray* (TStreamerObjectPointer)\r\n    fOwner: TObject* (TStreamerObjectPointer)\r\n    fProcessGUIDs: vector<string> (TStreamerSTL)\r\n\r\nTBranch (v12): TNamed (v1), TAttFill (v1)\r\n    fCompress: int (TStreamerBasicType)\r\n    fBasketSize: int (TStreamerBasicType)\r\n    fEntryOffsetLen: int (TStreamerBasicType)\r\n    fWriteBasket: int (TStreamerBasicType)\r\n    fEntryNumber: long long (TStreamerBasicType)\r\n    fOffset: int (TStreamerBasicType)\r\n    fMaxBaskets: int (TStreamerBasicType)\r\n    fSplitLevel: int (TStreamerBasicType)\r\n    fEntries: long long (TStreamerBasicType)\r\n    fFirstEntry: long long (TStreamerBasicType)\r\n    fTotBytes: long long (TStreamerBasicType)\r\n    fZipBytes: long long (TStreamerBasicType)\r\n    fBranches: TObjArray (TStreamerObject)\r\n    fLeaves: TObjArray (TStreamerObject)\r\n    fBaskets: TObjArray (TStreamerObject)\r\n    fBasketBytes: int* (TStreamerBasicPointer)\r\n    fBasketEntry: long long* (TStreamerBasicPointer)\r\n    fBasketSeek: long long* (TStreamerBasicPointer)\r\n    fFileName: TString (TStreamerString)\r\n\r\nTBranchRef (v1): TBranch (v12)\r\n    fRefTable: TRefTable* (TStreamerObjectPointer)\r\n\r\nTObjArray (v3): TSeqCollection (v0)\r\n    fLowerBound: int (TStreamerBasicType)\r\n    fLast: int (TStreamerBasicType)\r\n\r\nTTree (v19): TNamed (v1), TAttLine (v1), TAttFill (v1), TAttMarker (v2)\r\n    fEntries: long long (TStreamerBasicType)\r\n    fTotBytes: long long (TStreamerBasicType)\r\n    fZipBytes: long long (TStreamerBasicType)\r\n    fSavedBytes: long long (TStreamerBasicType)\r\n    fFlushedBytes: long long (TStreamerBasicType)\r\n    fWeight: double (TStreamerBasicType)\r\n    fTimerInterval: int (TStreamerBasicType)\r\n    fScanField: int (TStreamerBasicType)\r\n    fUpdate: int (TStreamerBasicType)\r\n    fDefaultEntryOffsetLen: int (TStreamerBasicType)\r\n    fNClusterRange: int (TStreamerBasicType)\r\n    fMaxEntries: long long (TStreamerBasicType)\r\n    fMaxEntryLoop: long long (TStreamerBasicType)\r\n    fMaxVirtualSize: long long (TStreamerBasicType)\r\n    fAutoSave: long long (TStreamerBasicType)\r\n    fAutoFlush: long long (TStreamerBasicType)\r\n    fEstimate: long long (TStreamerBasicType)\r\n    fClusterRangeEnd: long long* (TStreamerBasicPointer)\r\n    fClusterSize: long long* (TStreamerBasicPointer)\r\n    fBranches: TObjArray (TStreamerObject)\r\n    fLeaves: TObjArray (TStreamerObject)\r\n    fAliases: TList* (TStreamerObjectPointer)\r\n    fIndexValues: TArrayD (TStreamerObjectAny)\r\n    fIndex: TArrayI (TStreamerObjectAny)\r\n    fTreeIndex: TVirtualIndex* (TStreamerObjectPointer)\r\n    fFriends: TList* (TStreamerObjectPointer)\r\n    fUserInfo: TList* (TStreamerObjectPointer)\r\n    fBranchRef: TBranchRef* (TStreamerObjectPointer)\r\n\r\nTNtupleD (v1): TTree (v19)\r\n    fNvar: int (TStreamerBasicType)\r\n\r\nTH2C (v3): TH2 (v4), TArrayC (v1)\r\n\r\nTH2 (v4): TH1 (v7)\r\n    fScalefactor: double (TStreamerBasicType)\r\n    fTsumwy: double (TStreamerBasicType)\r\n    fTsumwy2: double (TStreamerBasicType)\r\n    fTsumwxy: double (TStreamerBasicType)\r\n\r\nTH2D (v3): TH2 (v4), TArrayD (v1)\r\n\r\nTAtt3D (v1)\r\n\r\nTCollection (v3): TObject (v1)\r\n    fName: TString (TStreamerString)\r\n    fSize: int (TStreamerBasicType)\r\n\r\nTSeqCollection (v0): TCollection (v3)\r\n\r\nTList (v5): TSeqCollection (v0)\r\n\r\nTHashList (v0): TList (v5)\r\n\r\nTArray (v1)\r\n    fN: int (TStreamerBasicType)\r\n\r\nTArrayD (v1): TArray (v1)\r\n    fArray: double* (TStreamerBasicPointer)\r\n\r\nTAttAxis (v4)\r\n    fNdivisions: int (TStreamerBasicType)\r\n    fAxisColor: short (TStreamerBasicType)\r\n    fLabelColor: short (TStreamerBasicType)\r\n    fLabelFont: short (TStreamerBasicType)\r\n    fLabelOffset: float (TStreamerBasicType)\r\n    fLabelSize: float (TStreamerBasicType)\r\n    fTickLength: float (TStreamerBasicType)\r\n    fTitleOffset: float (TStreamerBasicType)\r\n    fTitleSize: float (TStreamerBasicType)\r\n    fTitleColor: short (TStreamerBasicType)\r\n    fTitleFont: short (TStreamerBasicType)\r\n\r\nTAxis (v9): TNamed (v1), TAttAxis (v4)\r\n    fNbins: int (TStreamerBasicType)\r\n    fXmin: double (TStreamerBasicType)\r\n    fXmax: double (TStreamerBasicType)\r\n    fXbins: TArrayD (TStreamerObjectAny)\r\n    fFirst: int (TStreamerBasicType)\r\n    fLast: int (TStreamerBasicType)\r\n    fBits2: unsigned short (TStreamerBasicType)\r\n    fTimeDisplay: bool (TStreamerBasicType)\r\n    fTimeFormat: TString (TStreamerString)\r\n    fLabels: THashList* (TStreamerObjectPointer)\r\n\r\nTAttMarker (v2)\r\n    fMarkerColor: short (TStreamerBasicType)\r\n    fMarkerStyle: short (TStreamerBasicType)\r\n    fMarkerSize: float (TStreamerBasicType)\r\n\r\nTAttFill (v2)\r\n    fFillColor: short (TStreamerBasicType)\r\n    fFillStyle: short (TStreamerBasicType)\r\n\r\nTAttLine (v2)\r\n    fLineColor: short (TStreamerBasicType)\r\n    fLineStyle: short (TStreamerBasicType)\r\n    fLineWidth: short (TStreamerBasicType)\r\n\r\nTString (v2)\r\n\r\nTNamed (v1): TObject (v1)\r\n    fName: TString (TStreamerString)\r\n    fTitle: TString (TStreamerString)\r\n\r\nTH1 (v7): TNamed (v1), TAttLine (v2), TAttFill (v2), TAttMarker (v2)\r\n    fNcells: int (TStreamerBasicType)\r\n    fXaxis: TAxis (TStreamerObject)\r\n    fYaxis: TAxis (TStreamerObject)\r\n    fZaxis: TAxis (TStreamerObject)\r\n    fBarOffset: short (TStreamerBasicType)\r\n    fBarWidth: short (TStreamerBasicType)\r\n    fEntries: double (TStreamerBasicType)\r\n    fTsumw: double (TStreamerBasicType)\r\n    fTsumw2: double (TStreamerBasicType)\r\n    fTsumwx: double (TStreamerBasicType)\r\n    fTsumwx2: double (TStreamerBasicType)\r\n    fMaximum: double (TStreamerBasicType)\r\n    fMinimum: double (TStreamerBasicType)\r\n    fNormFactor: double (TStreamerBasicType)\r\n    fContour: TArrayD (TStreamerObjectAny)\r\n    fSumw2: TArrayD (TStreamerObjectAny)\r\n    fOption: TString (TStreamerString)\r\n    fFunctions: TList* (TStreamerObjectPointer)\r\n    fBufferSize: int (TStreamerBasicType)\r\n    fBuffer: double* (TStreamerBasicPointer)\r\n    fBinStatErrOpt: TH1::EBinErrorOpt (TStreamerBasicType)\r\n\r\nTH3 (v5): TH1 (v7), TAtt3D (v1)\r\n    fTsumwy: double (TStreamerBasicType)\r\n    fTsumwy2: double (TStreamerBasicType)\r\n    fTsumwxy: double (TStreamerBasicType)\r\n    fTsumwz: double (TStreamerBasicType)\r\n    fTsumwz2: double (TStreamerBasicType)\r\n    fTsumwxz: double (TStreamerBasicType)\r\n    fTsumwyz: double (TStreamerBasicType)\r\n\r\nTH3D (v3): TH3 (v5), TArrayD (v1)\r\n\r\nTObject (v1)\r\n    fUniqueID: unsigned int (TStreamerBasicType)\r\n    fBits: unsigned int (TStreamerBasicType)\r\n\r\nMParContainer (v0): TObject (v1)\r\n\r\nMH (v2): MParContainer (v0)\r\n    fSerialNumber: unsigned char (TStreamerBasicType)\r\n    fNumExecutions: unsigned int (TStreamerBasicType)\r\n\r\nMHMcCollectionArea (v14): MH (v2)\r\n    fHistAllorig: TH3D* (TStreamerObjectPointer)\r\n    fHistAll: TH3D* (TStreamerObjectPointer)\r\n    fHistWeights: TH3D* (TStreamerObjectPointer)\r\n    fHistSel: TH3D* (TStreamerObjectPointer)\r\n    fHistCol: TH3D* (TStreamerObjectPointer)\r\n    fHistColCoarse: TH2D* (TStreamerObjectPointer)\r\n    fHistEestOverEtrue: TH2D* (TStreamerObjectPointer)\r\n    fHistRadOverEtrue: TH2D* (TStreamerObjectPointer)\r\n    fIsFineBinOK: TH2C* (TStreamerObjectPointer)\r\n    fHistSelInEestBin: TClonesArray (TStreamerObject)\r\n    fHistColInEestBin: TClonesArray (TStreamerObject)\r\n    fHistColInEestBin_orig: TClonesArray (TStreamerObject)\r\n    fAreaMax: double (TStreamerBasicType)\r\n    fMinEvents: int (TStreamerBasicType)\r\n    fMaxRelError: float (TStreamerBasicType)\r\n    fMinFractionOfGoodBins: float (TStreamerBasicType)\r\n    fMaxRelEtrueDepartureFromEest: float (TStreamerBasicType)\r\n    kWobblePatch: bool (TStreamerBasicType)\r\n    fAzBins: int (TStreamerBasicType)\r\n    fUseEnergyEst: bool (TStreamerBasicType)\r\n    fMcEventsNtuple: TNtupleD* (TStreamerObjectPointer)\r\n    fStoreMCEvents: bool (TStreamerBasicType)\r\n    fMcRingAnalysis: bool (TStreamerBasicType)\r\n    fOffsetRingMin: double (TStreamerBasicType)\r\n    fOffsetRingMax: double (TStreamerBasicType)\r\n    fOffsetRingMed: double (TStreamerBasicType)\r\n    fBinsx: MBinning (TStreamerObject)\r\n    fBinsy: MBinning (TStreamerObject)\r\n    fBinsz: MBinning (TStreamerObject)\r\n```",
        "createdAt":"2021-09-09T16:01:31Z",
        "number":1302465
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"PR #437.",
        "createdAt":"2021-09-09T16:03:42Z",
        "number":1302479
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-09-08T11:17:45Z",
  "number":436,
  "title":"Fixing wrong streamer info",
  "url":"https://github.com/scikit-hep/uproot5/discussions/436"
 },
 {
  "author":{
   "login":"mpad"
  },
  "body":"Hello,\r\n\r\nI have a workflow  where I use uproot3 to read in data from root files directly into existing big in-memory arrays.\r\nI was using interpretation like in :\r\n\r\n```\r\n# declaring this for a bunch of variables : \r\nmydic[var_name] = tree[var_name].interpretation.toarray(abigarray)\r\n# later :\r\ntree.arrays(branches=mydic)\r\n```\r\n\r\nand as expected all the `abigarray` are filled in-place.\r\n\r\nIs there a way to do that with uproot4 ? I didn't find it in the doc.\r\nI've seen a `AsDtypeInPlace` class in the interpretation module, but it's currently not implemented so I'm affraid this is not yet possible.\r\nDid I miss something ?\r\n\r\nCheers,\r\n\r\nP-A",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"That's right: I just checked and this is indeed unimplemented.\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/382bb62f47a05f6111d3787e3235f9e56d9e113e/src/uproot/interpretation/numerical.py#L356-L363\r\n\r\nIt probably got pushed to low priority because it seemed like this feature was hardly ever used in Uproot 3. The way I gauge that is by user comments, questions, and requests for bug-fixes, such as this one. In fact, it's telling that this has been the default Uproot version since last December and this is the first time the point has been raised that it's completely missing.\r\n\r\nSo, how important is this in your work? Is it memory or speed prohibitive to let Uproot allocate an array, fill it, copy that into the in-place place where you want it, and then let that temporary array go out of scope?\r\n\r\nIf it's not actually important, then I might just remove the \"reminder\" stub for AsDtypeInPlace (i.e. remove the promise that it would ever be implemented).",
     "createdAt":"2021-09-15T16:14:58Z",
     "number":1334369,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"mpad"
     },
     "body":"Thanks for your fast answer !\r\nI'm doing DNN training with inputs having O(100)features x O(100M) entries . This does not fit in memory, so I split the inputs in multiple files, read data from 1 file into a big array, perform a maximum of in-place operations on this array (essentially pre-processing) and finally feed the array to the NN library (tensorflow in my  case) and then continue on the next file, re-using the array I prepared.\r\nThis is a bit complex but I couldn't find a better compromise in terms of performance (reading from file is fast, calculations on big arrays with numpy/numexpr are efficient) and flexibility (typically, I can easily chose to not load all the features I have in the input).\r\nBy  using as many in-place operations as possible, I can have bigger arrays and thus good perfs.\r\n\r\nSo in this particular case, yes being able to read directly into a given array is important.\r\n\r\nI don't claim I have an optimal solution, but it is based on the idea that when we have to split data into chunks, the bigger the chunks, the better because we can minimize the repetitions of read and preprocessing operations.\r\nTo be honest I'm not 100% sure this idea is correct, but if it is I would expect people dealing with large datasets would try to maximize their array and control their memory usage (thus avoid copies and allocation/deallocation of temporary arrays)...\r\nOn the other hand, I maybe wrong, maybe some solutions based on multithreading are even better. In this case I'd understand this feature I need is not a priority at all.\r\n\r\nCheers,\r\n\r\nP-A",
     "createdAt":"2021-09-15T20:35:25Z",
     "number":1335454,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"There is a limit in flexibility: `AsDtypeInPlace` would only ever work as a replacement for `AsDtype`: it wouldn't be possible to in-place fill a jagged array.\r\n\r\nTaking as a given that you can't do the DNN training in batches\u2014that the DNN training has to be all in memory at once\u2014it's still not strictly necessary for Uproot to write directly into the DNN array. For instance,\r\n\r\n```python\r\n>>> events = uproot.open(\"Zmumu.root:events\")\r\n>>> pretend_this_is_big = np.empty(events.num_entries)\r\n>>> for batch, report in events.iterate([\"px1\"], step_size=10, report=True):\r\n...     pretend_this_is_big[report.tree_entry_start:report.tree_entry_stop] = batch[\"px1\"]\r\n... \r\n>>> pretend_this_is_big\r\narray([-41.19528764,  35.11804977,  35.11804977, ...,  32.37749196,\r\n        32.37749196,  32.48539387])\r\n```\r\n\r\nYou can preallocate your huge array (called `pretend_this_is_big` in the above example) and fill it in arbitrarily small batches (`10` entries in this example, which is ridiculously small). The memory footprint at any given time is the size of the huge array plus the size of one batch (or maybe two batches because it's not explicitly `del`'ed at the end of the for loop). We're taking it as a given that the huge array must exist all at once, but the batches can be arbitrarily small. Granted, if you make the batches too small, the filling process will be slow. This is a technique that trades memory for speed.\r\n\r\nIs there not a reasonable compromise between memory and speed here? Suppose you need to have a TB in RAM for the DNN's array. Uproot is asymptotically fast for batches about 100 kB or so; let's make it comfortable and say you use batches of 10 or 100 MB. Those batches are still negligible compared to your TB training array. (Still true if we're being more realistic and it's actually 10 GB or so.)\r\n\r\n(See [TTree.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#iterate) for `step_size`, which can be set to a size in memory, rather than a number of entries, and `report` to get the current index of iteration.)\r\n\r\nI'm mentioning this because if this works, you can be up and running today. If you have to wait for `AsDtypeInPlace` to be implemented, you'd have to wait longer. (Unless you want to implement it; I can help if you're so inclined.)",
        "createdAt":"2021-09-15T20:58:57Z",
        "number":1335529
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"mpad"
     },
     "body":"Hello,\r\n\r\nThanks for the detailed suggestion ! I'm just a bit worried that repeating the instruction `pretend_this_is_big[atart:stop] = batch[varname]` for many variables and steps might kill the performances... but possibly this won't be a big impact and I guess I'll try anyway.\r\n\r\nI'm not sure how difficult for me it would be to implement `AsDtypeInPlace` : if you can give me a few pointers, where to look in the code, I'll get an idea if I can help or not.",
     "createdAt":"2021-09-16T10:51:40Z",
     "number":1341182,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"AsDtypeInPlace inherits from two superclasses, AsDtype and through that, Numerical. There are three steps in interpreting a rectilinear array:\r\n\r\n   1. Taking the decompressed data from a TBasket and interpreting it as the right dtype. This is implemented in AsDtype and not Numerical, and the implementation is [AsDtype.basket_array](https://github.com/scikit-hep/uproot4/blob/382bb62f47a05f6111d3787e3235f9e56d9e113e/src/uproot/interpretation/numerical.py#L313-L353). This method is already zero-copy, so there would be no reason to override it in AsDtypeInPlace. All it does (apart from calling any user-defined hooks) is `data.view(dtype).reshape((-1,) + shape)`. So you can leave this as an inherited function.\r\n   2. Copying each interpreted TBasket into a single output array. For in-place operations, this would be the given array, supplied by a user. The default implementation would be inherited from Numerical (AsDtype does not override it), in [Numerical.final_array](https://github.com/scikit-hep/uproot4/blob/382bb62f47a05f6111d3787e3235f9e56d9e113e/src/uproot/interpretation/numerical.py#L313-L353). This implementation would either need to be overridden or modified to work differently in AsDtypeInPlace because this implementation creates an `output = library.empty((length,), self.to_dtype)`. You would want to derive the `output` from a user-supplied `_to_fill`, like this: `output = self._to_fill[:length * self.to_dtype.itemsize].view(self.to_dtype)`. (Ensure that the `_to_fill` is large enough!) There's a lot of slicing logic here that would be best _not_ to copy-paste, so perhaps the Numerical implementation could check to see if it has `hasattr(self, \"_to_fill\")` and if it does, use that, if it doesn't, create a new array, and only the AsDtypeInPlace would have a `_tofill`?\r\n   3. The `final_array` method calls `output = library.finalize(output, branch, self, entry_start, entry_stop)`, which lets NumPy, Awkward, or Pandas have the last word on what the output looks like. AsDtypeInPlace can _only_ work with `library=\"np\"` (an exception should be raised early if any other library is used), and [NumPy.finalize](https://github.com/scikit-hep/uproot4/blob/382bb62f47a05f6111d3787e3235f9e56d9e113e/src/uproot/interpretation/library.py#L201-L228) seems to let non-jagged arrays through without modification, so this should be good as long as you enforce that AsDtypeInPlace can't be used with anything but `library=\"np\"`.\r\n\r\nWith these modifications, the following data transformations would happen, from start to finish:\r\n\r\n   * Data read from disk (if you're using the default `file_handler=uproot.MemmapSource`) is actually a page of virtual memory in the operating system (the memory-map). That's a minimal-copy technique. The alternative (`uproot.MultithreadedFileSource`, a plain file handle) is to copy from virtual memory into an array in the Python process.\r\n   * If your ROOT file has any compression, it _must_ be copied into a new buffer to decompress it, one TBasket at a time. There's no in-place alternative: decompressed data can have a different size (hopefully bigger!) than the original, so it must be a new array. If your ROOT file has no compression, the \"uncompressed TBaskets\" might still be the memory-map.\r\n   * By inheriting `AsDtype.basket_array`, you just view those bytes in the correct interpretation, which for ROOT is always big-endian.\r\n   * By overriding or otherwise modifying the behavior of `Numerical.final_array`, you copy the interpreted TBasket arrays into the output array, and the bytes are flipped to native-endian during the copy. There is no way around doing this copy\u2014even if the ROOT file were uncompressed\u2014since the TBaskets are not concatenated, but you need the output to be concatenated. The fact that we get to do the endianness in this copy is a nice bonus. (The endianness conversion is in the distinction between `from_dtype` and `to_dtype`: the first is big-endian, the second is native-endian.)\r\n\r\nSo there _is_ copying happening\u2014from the uncompressed TBasket arrays (which might or might not be OS pages) to the desired output array. But the TBasket arrays are only as large as the baskets (rarely as large as a MB each), and there is only one big array. It's like the \"many small temporary arrays filling parts of the big array and then being deleted\" approach I proposed above, except with one fewer (small) copy, from TBaskets to batches. With AsDtypeInPlace, data are copied directly from the TBaskets into your given array.\r\n\r\nThe AsDtypeInPlace class will need to take an array in its `__init__` instead of a `to_dtype`, and it might infer the `from_dtype` as the big-endian version of the given array's dtype. Instead of having `_to_type` as an instance attribute, it would have `_to_fill` as the instance attribute and would derive `to_dtype` as a property.\r\n\r\nThat's all I can think of!",
        "createdAt":"2021-09-16T17:01:06Z",
        "number":1343402
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"mpad"
     },
     "body":"Hi,\r\n\r\nWith a bit of delay, I could give a try and had something working.\r\nI've opened the MR  https://github.com/scikit-hep/uproot4/pull/487 to discuss if what I did can be a solution",
     "createdAt":"2021-10-27T12:19:18Z",
     "number":1544986,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"mpad"
     },
     "body":"Thanks for helping with this PR and accepting it !\r\nAny idea when a next release including it will be available ?\r\n",
     "createdAt":"2021-10-28T21:40:49Z",
     "number":1555637,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I started the process toward 4.1.7, since I wanted to get a restriction on Awkward version out, too.",
        "createdAt":"2021-10-28T22:08:54Z",
        "number":1555754
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":5
  },
  "createdAt":"2021-09-15T15:44:13Z",
  "number":440,
  "title":"Read into an existing numpy array with uproot4",
  "url":"https://github.com/scikit-hep/uproot5/discussions/440"
 },
 {
  "author":{
   "login":"bhbam"
  },
  "body":"Hi, I am trying to using uproot to read root file into pandas data frame. The root file have some structured data branches some not.\r\n\r\n`tree.show() ` \r\nname                      | typename                     | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nEvent                      | struct {int64_t run; ...  | AsDtype(\"[('run', '>i8'), (...\r\nGenPhoton1           | struct {double pt; do... | AsDtype(\"[('pt', '>f8'), ('...\r\nGenPhoton2          | struct {double pt; do... | AsDtype(\"[('pt', '>f8'), ('...\r\nPhoton1                  | struct {double pt; do... | AsDtype(\"[('pt', '>f8'), ('...\r\nPhoton2                 | struct {double pt; do... | AsDtype(\"[('pt', '>f8'), ('...\r\nVertex0                  | struct {double vx; do... | AsDtype(\"[('vx', '>f8'), ('...\r\nPrimaryVertex       | struct {double vx; do... | AsDtype(\"[('vx', '>f8'), ('...\r\nnPV                        | int32_t                            | AsDtype('>i4')\r\nBeamSpot             | struct {double x0; do...  | AsDtype(\"[('x0', '>f8'), ('...\r\n\r\nI can see branch \"nPV\" is different from other branch that why I think I'm getting error when I'm trying to read this to pandas data frame.\r\nKeyError: ('nPV', nan) -> This is the error showing.\r\nIf I remove \"nPV\" branch than it working fine.\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Could you include the whole stack trace and maybe a copy of the file?\r\n\r\nIs it able to read all of the branches with `library=\"np\"` or `library=\"ak\"`, but not `library=\"pd\"`?\r\n\r\nThe \"KeyError: ('nPV', nan)\" is a bug; some step in the operation is trying to find a key in a dict that is a string and a number: `('nPV', nan)`, but NaN is a \"[not a number](https://en.wikipedia.org/wiki/NaN)\" value that is never equal to itself:\r\n\r\n```python\r\n>>> float(\"nan\") == float(\"nan\")\r\nFalse\r\n```\r\n\r\nSo if a key in that dict included NaN, it shouldn't (in principle*) ever be accessible:\r\n\r\n```python\r\n>>> internal_dict = {(\"nPV\", float(\"nan\")): 3.14, (\"nPV\", 123): 2.71}\r\n>>> internal_dict[(\"nPV\", float(\"nan\"))]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nKeyError: ('nPV', nan)\r\n>>> internal_dict[(\"nPV\", 123)]\r\n2.71\r\n```\r\n\r\nThe Uproot code should not be allowing the second item of this tuple to be NaN. It's probably a computed value, and your data includes a NaN in it somewhere that is making the computed value also NaN. But I can't debug it if I don't see the details.\r\n\r\n<br><br><br>\r\n\r\n------------------\r\n\r\n*As a caveat, to say \"in principle,\" NumPy's `np.nan` sometimes matches itself in dicts and sometimes doesn't. I haven't figured out why, but that might even be a bug in NumPy or Python. (An extreme corner case; NaN is not a good value to use in dicts!)\r\n\r\n```python\r\n>>> internal_dict = {(\"nPV\", np.nan): 3.14, (\"nPV\", 123): 2.71}\r\n>>> internal_dict[(\"nPV\", np.nan)]\r\n3.14\r\n>>> np.nan + 1\r\nnan\r\n>>> internal_dict[(\"nPV\", np.nan + 1)]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nKeyError: ('nPV', nan)\r\n```",
     "createdAt":"2021-09-22T22:26:36Z",
     "number":1372059,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"bhbam"
     },
     "body":"Iibrary = 'np' is working but I am using library='pd'\r\n\r\nHere is the error\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-51-911d1f020c2a> in <module>\r\n----> 1 df = tree.arrays(library ='pd')\r\n\r\n/Applications/anaconda3/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in arrays(self, expressions, cut, filter_name, filter_typename, filter_branch, aliases, language, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library, how)\r\n   1173         ]\r\n   1174 \r\n-> 1175         return library.group(output, expression_context, how)\r\n   1176 \r\n   1177     def iterate(\r\n\r\n/Applications/anaconda3/lib/python3.8/site-packages/uproot/interpretation/library.py in group(self, arrays, expression_context, how)\r\n    968 \r\n    969             if all(isinstance(x.index, _pandas_rangeindex()) for x in arrays.values()):\r\n--> 970                 return _pandas_memory_efficient(pandas, arrays, names)\r\n    971 \r\n    972             indexes = []\r\n\r\n/Applications/anaconda3/lib/python3.8/site-packages/uproot/interpretation/library.py in _pandas_memory_efficient(pandas, series, names)\r\n    792             out = series[name].to_frame(name=name)\r\n    793         else:\r\n--> 794             out[name] = series[name]\r\n    795         del series[name]\r\n    796     if out is None:\r\n\r\nKeyError: ('nPV', nan)\r\n",
     "createdAt":"2021-09-22T22:46:29Z",
     "number":1372104,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"It's being fixed in https://github.com/scikit-hep/uproot4/pull/449.\r\n\r\nThe columns of your DataFrame must have multiple levels of structure (e.g. \"Event\" as an outer name and \"run\" as an inner name; \"GenPhoton1\" as an outer name and \"pt\" as an inner name) and also a single level of structure (just \"nPV\"). To make them the same length, I had been using `None` as a filler, so that `[(\"Event\", \"run\"), ..., (\"GenPhoton1\", \"pt\"), ..., (nPV, None), ...]` are passed to Pandas to build the MultiIndex of column names. Pandas turned the `None` into NaN, and then it couldn't look up the key because that key wasn't equal to itself. So I changed the filler to `\"\"` (the empty string).\r\n\r\nHowever, it means that you'll have to access the \"nPV\" column as `(\"nPV\", \"\")`.\r\n\r\nThe error itself had nothing to do with reading, which was indicated by the fact that `library=\"np\"` worked.",
     "createdAt":"2021-09-22T23:36:21Z",
     "number":1372203,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"bhbam"
     },
     "body":"Thanks for the explanation. Now it makes lots of sense for me. \r\nSorry I am again throwing some questions, I am totally new to pandas and uproot\r\n   `df1 = tree.arrays(filter_name='nPV', library ='pd')`\r\nThis command separately worked fine. Read nPV branch of root file to pandas data frame.\r\n\r\n`df2= tree.arrays(filter_name=['Event','GenPhoton1','GenPhoton2','Photon1','Photon2','Vertex0','PrimartVertex','BeamSpot'],library ='pd')`\r\nThis command reads all remaining branches branches.\r\n\r\n`df=pd.concat([df2, df1], axis=1)`\r\nFinally with this command I am able to join to data set to get final data frame.\r\n\r\n\r\nI can tell from the data frame,  all other quantities have multi-indexing but nPV  have single. Can I change  double index quantities into single for example ('Event', 'run')->'Event_run'. I saw this type of indexing structure in root_pandas but it is now outdated. Is there any way we can apply same analogy here and read all root  branches in pandas data frame? OR is there any better way to do this?\r\n",
     "createdAt":"2021-09-23T02:33:17Z",
     "number":1372539,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"You can do \"nPV\" and \"everything but nPV\" because the bug (before PR #499) is in merging two-level column names with one-level.\r\n\r\nUproot creates DataFrames in one way with the expectation that if you need a different DataFrame format, you can use Pandas to manipulate it. Providing more flexibility on Uproot's side would complicate the parameterization: `library=\"pd\"` is one function argument, if we wanted to parameterize it, we'd have to add more function arguments that only apply to the Pandas case. That could get confusing.\r\n\r\nI don't know if there's a fancy way to convert `(\"Event\", \"run\")` column names into `\"Event_run\"`, but it could be done with a for loop/list comprehension. (The number of column names is not large; speed is not a concern.) The `\"Event_run\"` form contains less information than the `(\"Event\", \"run\")` form, since names or subnames that originally had underscores in them can no longer be distinguished. That's why we went for the general form.",
        "createdAt":"2021-09-23T12:53:00Z",
        "number":1374565
       },
       {
        "author":{
         "login":"bhbam"
        },
        "body":"Thank you for the clarifications this helps lots.",
        "createdAt":"2021-09-23T15:08:03Z",
        "number":1375236
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":4
  },
  "createdAt":"2021-09-22T22:05:24Z",
  "number":448,
  "title":"Leaf-lists (structs) and non-leaf-lists in the same Pandas DataFrame",
  "url":"https://github.com/scikit-hep/uproot5/discussions/448"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@nsmith- fixed Awkward Forms for objects: PR #441, and implemented regular array support for TClonesArray: PR #442.\r\n\r\n@jpivarski fixed Pandas representations of leaf-lists (structs) and non-leaf-lists in the same DataFrame: PR #449. Fixed `uproot.lazy` for empty TTrees and boost-histogram with no title: #451. Fixed issue #439 by checking to see if an array is an Awkward Array before checking to see if it could be NumPy: #453. Fixed both Delphes bugs in issue #438: PR #454.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.1.3'>4.1.3</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-09-29T19:43:00Z",
  "number":455,
  "title":"4.1.3",
  "url":"https://github.com/scikit-hep/uproot5/discussions/455"
 },
 {
  "author":{
   "login":"denehoffman"
  },
  "body":"I've refrained from asking a question about this since my understanding of the ROOT data structure is fairly limited, but I work with TTrees which contain branches that have TClonesArrays of TLorentzVectors, and they don't seem to read properly when I use the usual Uproot methods. I noticed in one of the recent pull requests ([442](https://github.com/scikit-hep/uproot4/pull/442)) that something related to TClonesArrays was implemented, but I'm still having the same problem, so I was wondering if it is an issue with my usage of Uproot or if it's an issue with how my collaboration generates the TTrees.\r\n\r\nFor some context, we have a single TTree with multiple branches. However, for each event, in our branches of 4-vectors, we have a TClonesArray object that doesn't necessarily contain the same number of vectors for each event (we say there are multiple \"combos\" or combinations of particles which can constitute each event, and this is the way they're stored). Unfortunately, I believe this is the cause of a problem when I try to load one of these branches into an array, since I get the error:\r\n\r\n```\r\n>>> tree['Beam__X4_Measured'].array()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 2105, in array\r\n    False,\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3510, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3461, in basket_to_array\r\n    library,\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/interpretation/objects.py\", line 141, in basket_array\r\n    form = self.awkward_form(branch.file, index_format=\"i64\")\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/interpretation/objects.py\", line 122, in awkward_form\r\n    self._branch.file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/containers.py\", line 312, in awkward_form\r\n    self._model, file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/_util.py\", line 553, in awkward_form\r\n    file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/model.py\", line 1153, in awkward_form\r\n    file, index_format, header, tobject_header, breadcrumbs\r\n  File \"<dynamic>\", line 35, in awkward_form\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/model.py\", line 677, in awkward_form\r\n    classname_decode(cls.__name__)[0]\r\nuproot.interpretation.objects.CannotBeAwkward: TObjArray\r\n```\r\n\r\nIs there some simple thing I can do about this? I can see the data in a single event easily with\r\n\r\n```\r\n>>> tree['Beam__X4_Measured'].debug(0)\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 12  84  67 108 111 110 101 115  65 114 114  97 121   0  64   0   1  57   0   4\r\n---   T   C   l   o   n   e   s   A   r   r   a   y ---   @ --- ---   9 --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   3   0   0   0  15  84  76 111 114 101 110 116 122  86\r\n--- --- --- --- --- --- --- --- --- --- ---   T   L   o   r   e   n   t   z   V\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n101  99 116 111 114 115  16  84  76 111 114 101 110 116 122  86 101  99 116 111\r\n  e   c   t   o   r   s ---   T   L   o   r   e   n   t   z   V   e   c   t   o\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n114  59  52   0   0   0   4   0   0   0   0   1  64   0   0  60   0   4   0   1\r\n  r   ;   4 --- --- --- --- --- --- --- --- ---   @ --- ---   < --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   3   0   0   0  64   0   0  36   0   3   0   1   0   0   0   0\r\n--- --- --- --- --- --- --- ---   @ --- ---   $ --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  3   0   0   0  63 139 127  19 184  78 153 110 191 177  75  31 139  25  47 220\r\n--- --- --- ---   ? --- --- --- ---   N ---   n --- ---   K --- --- ---   / ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64  77 248  15 190 160 129 246 192  38  67  41 156 187  69 128   1  64   0   0\r\n  @   M --- --- --- --- --- --- ---   &   C   ) --- ---   E --- ---   @ --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 60   0   4   0   1   0   0   0   0   3   0   0   0  64   0   0  36   0   3   0\r\n  < --- --- --- --- --- --- --- --- --- --- --- ---   @ --- ---   $ --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  1   0   0   0   0   3   0   0   0  63 139 127  19 184  78 153 110 191 177  75\r\n--- --- --- --- --- --- --- --- ---   ? --- --- --- ---   N ---   n --- ---   K\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 31 139  25  47 220  64  77 248  15 190 160 129 246 192   8 228 140 178 237  22\r\n--- --- ---   / ---   @   M --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1  64   0   0  60   0   4   0   1   0   0   0   0   3   0   0   0  64   0\r\n--- ---   @ --- ---   < --- --- --- --- --- --- --- --- --- --- --- ---   @ ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0  36   0   3   0   1   0   0   0   0   3   0   0   0  63 139 127  19 184  78\r\n---   $ --- --- --- --- --- --- --- --- --- --- --- ---   ? --- --- --- ---   N\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n153 110 191 177  75  31 139  25  47 220  64  77 248  15 190 160 129 246 192   9\r\n---   n --- ---   K --- --- ---   / ---   @   M --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 88  79  18 237  22   0   1  64   0   0  60   0   4   0   1   0   0   0   0   3\r\n  X   O --- --- --- --- ---   @ --- ---   < --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0  64   0   0  36   0   3   0   1   0   0   0   0   3   0   0   0  63\r\n--- --- ---   @ --- ---   $ --- --- --- --- --- --- --- --- --- --- --- ---   ?\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n139 127  19 184  78 153 110 191 177  75  31 139  25  47 220  64  77 248  15 190\r\n--- --- --- ---   N ---   n --- ---   K --- --- ---   / ---   @   M --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n160 129 246  64  20  26 208  70 137 117   0\r\n--- --- ---   @ --- --- ---   F ---   u ---\r\n```\r\nSo this event appears to have a TClonesArray with two entries, or two combos, but other events might have one, or three, or more. Originally I thought my problem was just something about Uproot not playing well with TClonesArrays, but now I'm fairly certain it has to do with the arrays having different sizes. I'm admittedly out of my depth here, if there's no solution, or no easy solution to this, please let me know.\r\n\r\nEdit: For those curious, I there is a simple method (in ROOT or PyROOT) for flattening these structures such that each branch contains one event per combo, which is simply looping over all events, and sublooping over each combo and writing a new event to a new branch for each combo. If there's a way to do this using uproot, that would also be of interest to me.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Sorry that I didn't get back to this earlier, but maybe try `library=\"np\"`? Since this is not strictly numerical data, that NumPy array would have `dtype=\"O\"` and it would effectively be a Python list. Iteration over it would run at Python speeds, but slow access is better than no access.\r\n\r\nThe error message is saying that it can't convert the data into an Awkward Array. Your array contains a TObjArray, which is a dynamically typed list: it can contain any TObject; you don't know what types are in it until you read it. Awkward Arrays have to be statically typed so that we can split up the data into columnar buffers, which requires knowing which fields all nested records contain. Maybe I need to expand upon that error message.\r\n\r\nI'm not guaranteeing that it will work with `library=\"np\"`, since TClonesArray encodings are full of exceptions. (TClonesArray was early ROOT; the patterns settled down and got more regular later.) But it's worth a try. (Digging any deeper into it would require a copy of the file.)",
     "createdAt":"2021-10-08T15:48:53Z",
     "number":1449076,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"denehoffman"
        },
        "body":"Hello, thanks for the suggestion!` tree['Beam__X4_Measured'].array(library=\"np\")` gives me a different exception:\r\n\r\n```\r\nwhile reading\r\n\r\n    TClonesArray version 4 as uproot.dynamic.Model_TClonesArray_v4 (317 bytes)\r\n        TObjArray version None as uproot.models.TObjArray.Model_TObjArray (? bytes)\r\n            (base): <TObject None None at 0x7ff2384c9d30>\r\n            fName: 'TLorentzVectors'\r\n            fSize: 273960047\r\n            fLowerBound: 1919250036\r\n            TClonesArray version 4 as uproot.dynamic.Model_TClonesArray_v4 (317 bytes)\r\n                TObjArray version None as uproot.models.TObjArray.Model_TObjArray (? bytes)\r\n                    (base): <TObject None None at 0x7ff2384d7048>\r\n                    fName: 'TLorentzVectors'\r\n                    fSize: 273960047\r\n                    fLowerBound: 1919250036\r\n                    TClonesArray version 4 as uproot.dynamic.Model_TClonesArray_v4 (382 bytes)\r\n                        TObjArray version None as uproot.models.TObjArray.Model_TObjArray (? bytes)\r\n                            (base): <TObject None None at 0x7ff2384d72e8>\r\n                            fName: 'TLorentzVectors'\r\n                            fSize: 273960047\r\n                            fLowerBound: 1919250036\r\n\r\nmany, many lines of this...\r\n\r\nfLowerBound: 1919250036\r\nBase classes for TObjArray: TObject?\r\nMembers for TObjArray: fName?, fSize?, fLowerBound?\r\n\r\nattempting to get bytes 978740641:978740645\r\noutside expected range 0:331 for this Chunk\r\nin file tree_sum_AMO_40856_42559.root\r\nin object /ksks__B4_Tree;1\r\n```\r\n\r\nAny suggestions here? It seems like it's capable of recognizing that the TClonesArrays have TLorentzVectors in them, but then something goes wrong at the end of the file. Also thank you for the quick response, I'm a big fan of your work with uproot! I've also attached a copy of the ROOT file I'm working with: [ROOT File](https://drive.google.com/file/d/1RJqBsXGORxtM9CIVxfw1acT3U8ctQzzj/view?usp=sharing)",
        "createdAt":"2021-10-08T16:34:27Z",
        "number":1449350
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"The `fSize` is the number of elements in the TObjArray, and these numbers look too large. (Even if it's a big file, I doubt you have that many list items in the same entry.) I might have thought that the reading process is off by a byte or something, because that can lead to very wrong integer interpretations, but then, the strings (`\"TLorentzVectors\"`) would be off, too, and they're not.\r\n\r\nAlso, is it really nested that deeply? What this print-out is showing you is how much it managed (or thinks it managed) to read before it got stuck, and we can usually tell where it went wrong by the first appearance of a wrong value. If it's not supposed to be so deeply nested, then that's another indicator. (Maybe it's not moving forward in the byte stream and therefore thinking that a single TObjArray is nested within itself? Was the eventual exception StackOverflow? Oh it might not tell you because it captured the exception to show you that error message...)",
        "createdAt":"2021-10-08T16:49:15Z",
        "number":1449443
       },
       {
        "author":{
         "login":"denehoffman"
        },
        "body":"It's a tiny file relatively, only ~20Mb I think. I don't think it's nested at all like it's showing here, but I don't have a firm understanding of how this data file was created in the first place, since I work with events post-construction. I was able to get (most of?) the stack trace using the `execption.__traceback__` object:\r\n\r\n```\r\nFile \"demo.py\", line 10, in <module>\r\n    a = branch.array(library='np')\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 2105, in array\r\n    False,\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3510, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/behaviors/TBranch.py\", line 3461, in basket_to_array\r\n    library,\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/interpretation/objects.py\", line 159, in basket_array\r\n    self._model, branch, context, byte_offsets, data, cursor_offset\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/interpretation/objects.py\", line 595, in to_numpy\r\n    output[i] = self[i]\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/interpretation/objects.py\", line 616, in __getitem__\r\n    self._branch,\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/containers.py\", line 321, in read\r\n    return cls.read(chunk, cursor, context, file, selffile, parent)\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/model.py\", line 1310, in read\r\n    chunk, cursor, context, file, selffile, parent, concrete=concrete\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/model.py\", line 821, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"<dynamic>\", line 7, in read_members\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/model.py\", line 821, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/models/TObjArray.py\", line 56, in read_members\r\n    chunk, cursor, context, file, self._file, self._parent\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/deserialization.py\", line 220, in read_object_any\r\n    bcnt = numpy.int64(cursor.field(chunk, _read_object_any_format1, context))\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/source/cursor.py\", line 215, in field\r\n    return format.unpack(chunk.get(start, stop, self, context))[0]\r\n  File \"/home/nhoffman/.local/lib/python3.6/site-packages/uproot/source/chunk.py\", line 402, in get\r\n    self._source.file_path,\r\n```\r\n\r\nThe last line isn't a typo, and looking at that line in chunk.py, it looks like the error is a `DeserializationError`.",
        "createdAt":"2021-10-08T17:39:25Z",
        "number":1449718
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"The proximate cause of the error is that while Uproot was interpreting bytes of data in the chunk that it had extracted from the file, it ran to the end of the chunk of data. If it hadn't been limited to this pre-assigned chunk, it probably would have kept going and run out of data at the end of the file. That's what the last two lines are: a Cursor trying to get a field of data (an integer `bcnt`, \"byte count\") from one step beyond the end of the Chunk. Since this is coming from a TTree, the Chunk is the entire TBasket.\r\n\r\nI was thinking this might be a runaway process, resulting in a StackOverflow that got caught and reissued as a DeserializationError. You've shown that it's an explicit DeserializationError. It's probably still a runaway process, but it ran out of data before it ran out of stack.\r\n\r\nI've downloaded the file and will take a look at it.",
        "createdAt":"2021-10-08T19:06:51Z",
        "number":1450093
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"It's resolved in PR #467.\r\n\r\nIt _was_ the case that the apparent deep nesting was because it was walking over the same bytes repeatedly, thinking it was finding objects within objects. The real problem (ultimate cause!) was that we just didn't have any implementation of non-split TClonesArrays. All the examples of TClonesArray I've seen have been split, with each field of the nested objects in a different branch. I think your file could have been written that way, but wasn't.\r\n\r\nUsually, we get the data layout of a class from its TStreamerInfo\u2014that's how we can deal with the thousands of classes in the ROOT codebase and user-defined classes outside of C++, without a library of code with pre-written deserializers. Occasionally, the TStreamerInfo's prescription does not match how the class was actually serialized, sometimes because somebody overrode the standard streamer with a customized one for performance, and sometimes because the class in question predated the adoption of TStreamerInfo (around ROOT 3, year 2000). TClonesArray is probably one of those old ones.\r\n\r\n[TClonesArray's custom reading code](https://github.com/root-project/root/blob/e74738be8d342295457bf5e17979951415127059/core/cont/src/TClonesArray.cxx#L743-L852) does a lot of strange things. Its TStreamerInfo says that it's laid out like a subclass of TObjArray with no other data, but it's nothing like that at all. The TObjArray code was able to read the name `\"TLorentzVectors\"` (plural) because they both start with TObject and an `fName` string, but after that, they differ wildly. TClonesArray follows the `fName` with a string containing the class name and version of the contained objects, and _it parses that string to get the version number_ ([here in C++](https://github.com/root-project/root/blob/e74738be8d342295457bf5e17979951415127059/core/cont/src/TClonesArray.cxx#L754-L761) and [here in Python](https://github.com/scikit-hep/uproot4/blob/c8b996fae1498a53013f2dfd784f777a44105441/src/uproot/models/TClonesArray.py#L46-L54))! Then TClonesArray also has a \"do you exist?\" byte before each object, while TObjArray goes through a more generic routine that could allow for missing data (and arbitrarily typed data).\r\n\r\nSo the unsplit TClonesArray is indeed weird; I'm surprised this has never come up before.",
        "createdAt":"2021-10-08T21:23:45Z",
        "number":1450556
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"The tests finished, so I merged the PR into Uproot's main branch (on GitHub; no release to PyPI yet).",
        "createdAt":"2021-10-08T21:25:03Z",
        "number":1450559
       },
       {
        "author":{
         "login":"denehoffman"
        },
        "body":"Thank you so much, I really appreciate it, this is going to save me a ton of time (and sweat and tears)!",
        "createdAt":"2021-10-09T03:11:27Z",
        "number":1451195
       }
      ],
      "totalCount":7
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-10-06T22:02:06Z",
  "number":462,
  "title":"Problem with reading (awkward) TClonesArray of TLorentzVectors",
  "url":"https://github.com/scikit-hep/uproot5/discussions/462"
 },
 {
  "author":{
   "login":"deependra170598"
  },
  "body":"After reading a root file, can I get mean, variance etc parameters of histogram? ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Say `.to_hist()` and use the [hist library](https://github.com/scikit-hep/hist)'s routines.",
     "createdAt":"2021-10-08T03:12:57Z",
     "number":1445760,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-10-07T23:13:25Z",
  "number":464,
  "title":"How to get histogram's parameters (mean,sigma)?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/464"
 },
 {
  "author":{
   "login":"archiron"
  },
  "body":"Hi, I started to work with uproot and I have a weird effect;\r\nif after opening a root file and reading an histo [1], I try to read the _values/_errors arrays [2], I only empty arrays returned.\r\n\r\nIf before reading I use a statement such as : \r\n        values_1 = hist_1_4.values()\r\n        errors_1 = hist_1_4.errors()\r\nthen, I can have access to the _values/_errors arrays as in point [2].\r\nIs this way correct and known ?\r\n\r\nArnaud\r\n\r\n[1] : fileName1 = rootFilesPath + '/' + \"DQM_V0001_R000000001__RelValZEE_14__CMSSW_12_1_0_pre2-121X_mcRun3_2021_realistic_v1-v1__DQMIO.root\"\r\n    histoPath = \"DQMData/Run 1/EgammaV/Run summary/ElectronMcSignalValidator\"\r\n    datasets = ['h_ele_PhiMnPhiTrueVsEta_pfx']\r\n    rootFile = up4.open(fileName1)\r\n    root_tree_1_4 = rootFile[histoPath]\r\n    hist_1_4 = root_tree_1_4[dataset]\r\n\r\n[2] : for n1, v1 in hist_1_4.__dict__.items():\r\n            if n1.startswith(\"_values\"):\r\n                allValues_1 = v1\r\n            elif n1.startswith(\"_errors\"):\r\n                allErrors_1 = v1\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I don't remember what's in `_values` and `_errors`, but they're probably caches.\r\n\r\nA general rule in Python is that anything that starts with an underscore is an internal detail, equivalent to `private` in C++, not part of the library's interface. Private functions/classes/attributes/variables won't, in general, behave in predictable ways, and they can be removed or changed from one version of a library to the next.\r\n\r\nSo the important question is whether `values()` and `errors()` behave as you expect. These methods don't start with underscores and they're documented (here: [values](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TH1.TH1.html#values) and [errors](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TH1.TH1.html#errors), and also as satisfying a [general protocol (UHI)](https://uhi.readthedocs.io/en/latest/plotting.html#using-the-protocol), which gives the same meaning to these methods across multiple libraries). Thus, they're part of the public interface.\r\n\r\nAlso, it would help if code snippets in GitHub are [surrounded by triple-backticks](https://docs.github.com/en/github/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks), since this formats them as code. Particularly in Python, the meaning of the code depends on indentation, and without the formatting, the indentation is removed!",
     "createdAt":"2021-10-08T15:11:51Z",
     "number":1448882,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"archiron"
        },
        "body":"Hi, thanks for your reply.\r\nI actually wanted to use `_values `/ `_errors` because It does not contains under/overflow values. But in fact, this is easier to use _official_ form such as `values()` or `errors()`.\r\n\r\nArnaud\r\n\r\nP.S.\r\nsorry for the code format I forgot to use.",
        "createdAt":"2021-10-11T10:08:57Z",
        "number":1458268
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-10-08T14:24:41Z",
  "number":466,
  "title":"values/errors read",
  "url":"https://github.com/scikit-hep/uproot5/discussions/466"
 },
 {
  "author":{
   "login":"denehoffman"
  },
  "body":"I'm trying to read in data from an existing TTree, reformat/skim it down, and write it to a new TTree. The only problem is that my entire collaboration uses an outdated version of ROOT which apparently can't read version-20 TTrees:\r\n```\r\nError in <TBufferFile::ReadClassBuffer>: Could not find the StreamerInfo for version 20 of the class TTree, object skipped at offset 51\r\nError in <TBufferFile::CheckByteCount>: object of class TTree read too few bytes: 2 instead of 5528\r\n```\r\nThe tree can be opened (in ROOT) just fine on my local computer running the latest ROOT. When I look at the input tree versions, I see that\r\n```\r\n>>> uproot.open(\"tree_sum_AMO_30274_31057.root\")['ksks__B4_Tree'].__class__\r\n<class 'uproot.models.TTree.Model_TTree_v19'>\r\n```\r\nso version-19 trees are fine, but the output trees have\r\n```\r\n>>> uproot.open(\"flattree_AMO.root\")['kin'].__class__\r\n<class 'uproot.models.TTree.Model_TTree_v20'>\r\n```\r\nand these ones can't be read by the collaboration's version of ROOT. Is there some way to specify using the previous version when writing?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Version 20 of TTree is hard-coded in Uproot's writer (had to pick something), and while it would be possible to add code to skip some fields and make it version 19, that feels complicate the code, not to mention the interface (you'd have to specify the version somehow).\r\n\r\nBut the error message is suggesting something worse: old versions of ROOT are supposed to be able to read new data because every file with a TTree in it should have a TStreamerInfo for that TTree version (same for all other classes). In the file that you've written, does\r\n\r\n```python\r\nf.file.show_streamers()\r\n```\r\n\r\nnot include TTree version 20? If it doesn't, that's bad news.",
     "createdAt":"2021-10-13T11:10:35Z",
     "number":1470108,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"denehoffman"
        },
        "body":"It actually doesn't include anything, in fact, it didn't return any text at all. I looked back at my code and realized I had never closed the file, so I changed a `output_file = uproot.recreate(str(output_file_path))` to `with uproot.recreate(str(output_file_path)) as output_file:` and indented all the code below it properly, but I still get trees that can only be read on ROOT version 6.25/01 and not my cluster's ROOT version 6.08/06.\r\n\r\nHere's the minimal code I've been able to replicate this with:\r\n```\r\nimport numpy as np\r\nimport uproot\r\n\r\nwith uproot.recreate(\"demo.root\") as output_file:\r\n    output_file.mktree(\"kin\", {\"Weight\": \"f4\", \"Energy\": (\"f4\", (3,))})\r\n    for i in range(10):\r\n        weights = np.random.rand(10000)\r\n        energies = np.random.rand(10000, 3)\r\n        output_file[\"kin\"].extend({\"Weight\": weights, \"Energy\": energies})\r\n\r\nwith uproot.open(\"demo.root\") as input_file:\r\n    input_file.file.show_streamers()\r\n```\r\n[demo.root](https://drive.google.com/file/d/1U-4oaN5DpbKFJZjdZ92tMxmqYR3jwzVM/view?usp=sharing)\r\n\r\nLet me know what I'm doing wrong here, I'm sure I made some silly mistake!",
        "createdAt":"2021-10-13T16:50:45Z",
        "number":1472178
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Thanks; it looks like it's not writing any streamers at all, which is not intended. I'll convert this Discussion into an Issue.\r\n\r\nUsing context managers (`with` blocks) is the right way to do it and it ensures that the files get properly closed, but Uproot doesn't wait for file-closure to invoke the streamer-writing process. In fact (whether this is a good idea or not), after every assignment, it calls `flush` to ensure that the file-updates are on their way. (`flush` doesn't guarantee that it writes to disk, but I think it gets all of those file-changes to the operating system, so that they're sure to be written even if the Python process is closed.) Nevertheless, using context managers is the safest you can be.",
        "createdAt":"2021-10-13T16:59:14Z",
        "number":1472233
       },
       {
        "author":{
         "login":"denehoffman"
        },
        "body":"Sounds good. Out of curiosity, do you know why I'm able to read these files fine on a newer version of ROOT but not the older version? Does the newer version just have some expectation of the streamer info which happens to match what was written?",
        "createdAt":"2021-10-13T17:08:49Z",
        "number":1472279
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Yeah, the streamer is built into ROOT as reflection info in `TClass` (I assume). Starting in Uproot 4, Uproot has a few class + version combinations built in, so that it can avoid reading streamers in typical cases, but not nearly as many as ROOT. This sort of thing could be put into a giant database, rather than encoding it as code, but I haven't undertaken that project.\r\n\r\nThe fact that ROOT (and now Uproot) can read some class + version combinations without having to resort to streamers unfortunately masks situations like this, in which the streamers are actually missing.",
        "createdAt":"2021-10-13T17:15:55Z",
        "number":1472320
       }
      ],
      "totalCount":4
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-10-13T00:26:45Z",
  "number":470,
  "title":"Is it possible to specify which TTree version to use when writing a new tree?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/470"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@jpivarski added TStreamerInfo for TTrees in files written by Uproot. **If you're using an older version, you're producing files without TStreamerInfo!** (Which means they won't be readable in some older versions of ROOT, and possibly not new ones, either.) PR #472.\r\n\r\nAlso corrected the TLeaf fTitle for jagged arrays, which is needed to read jagged arrays back in ROOT's `TTree::Draw` and `TTree::Scan`, but not for iteration in PyROOT (which is what we had been using for testing). PR #458.\r\n\r\nAdded an implementation of non-split TClonesArray, which makes more files readable: PR #467, and added RNTuple to the `must_be_attached` list, which enables RNTuple objects to read more data (like TTrees): PR #463. That's just a step in developing RNTuple-reading capabilities.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.1.4'>4.1.4</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-10-13T21:01:37Z",
  "number":473,
  "title":"4.1.4",
  "url":"https://github.com/scikit-hep/uproot5/discussions/473"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@jpivarski fixed another Uproot-writing bug: writing TTrees didn't update the FreeSegments record, which means that writing another TTree (with another file handle, with `uproot.update` or ROOT) could overwrite the first TTree.\r\n\r\n**Like 4.1.4, this is an important bug-fix for anyone creating a robust dataset of ROOT files.** (It doesn't affect you if you consider all files to be write-once only, but if you're ever thinking of updating files, this will matter. And besides, correctness is important in principle!)\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.1.5'>4.1.5</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-10-14T15:01:50Z",
  "number":476,
  "title":"4.1.5",
  "url":"https://github.com/scikit-hep/uproot5/discussions/476"
 },
 {
  "author":{
   "login":"niamh-h"
  },
  "body":"Hi! I'm trying to use a SciKit classifier on some data by reading it in from multiple ROOT TTrees to Pandas DataFrames (so I use uproot.open()). I then want to update the existing files with the classification data (e.g. confidence scores/probabilities etc) using uproot.update(). I'm having trouble with updating the files as they don't seem to be closing correctly despite being opened in a with statement. I've tried different ways of updating the files (below) but they all lead to the following error when I open the file in ROOT to check: \r\n\r\n```\r\nError in <TBufferFile::CheckByteCount>: object of class TList read too few bytes: 17391 instead of 17773\r\nError in <TBufferFile::CheckByteCount>: Byte count probably corrupted around buffer position 64:\r\n\t17773 for a possible maximum of -9\r\n```\r\n\r\nI think this means it's not closing correctly? If I then retry the python code with the same ROOT file I get an error when trying to open it in a writable form using uproot.update() (but no error when using uproot.open()), I'm assuming because it's been corrupted: (FYI I have updated to uproot 4.1.5)\r\n\r\n`ValueError: Uproot can't read TKey version -1 for writing, only version 4`\r\n\r\nThis is the code for two ways I've tried updating the file: \r\n```\r\nwith uproot.update('Datafile.root') as file:\r\n  file.mktree('t', {'branch1': np.int64, 'branch2':np.int64}, 'title')\r\n  file['title'].extend({'branch1:df.loc[:,'branch1'], 'branch2':df.loc[:,'branch2']})\r\n```\r\n```\r\nfile = uproot.update('Datafile.root)\r\nfile['df'] = df\r\nfile.close()\r\n```\r\n(I've also tried this way inside a with statement). \r\nHope I've explained this ok, any help would be appreciated! ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"It's not a matter of not closing correctly, since you're using a `with` statement and, anyway, each modification to a file in Uproot is intended to take it from a valid state to a valid state, ending with a `flush` so `close()` doesn't actually invoke any clean-up\u2014it's already cleaned up. (All that `with` is doing here is making us _doubly_ confident that it has finished flushing.)\r\n\r\nWhat seems to be happening here is a more general kind of bug on the Uproot side. You'll have to show me (1) a copy of a pre-updated file, (2) what steps you take to write to it with a dummy DataFrame, not your whole analysis, just one that can be created on the command line, and (3) how it fails to be re-openable with `uproot.update`. Then I'll figure out what Uproot is doing wrong in step (2).\r\n\r\nThat should be opened as an issue, rather than a discussion.",
     "createdAt":"2021-10-20T11:43:13Z",
     "number":1507859,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-10-20T09:39:08Z",
  "number":479,
  "title":"Updating ROOT file with Pandas DataFrame error",
  "url":"https://github.com/scikit-hep/uproot5/discussions/479"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@duncanmmacleod included the test suite in the source distribution: PR #477.\r\n\r\n@btovar fixed exception handling in XRootD callback threads by passing them on to the main thread: PR #480.\r\n\r\n@jpivarski restricted Uproot 4.x to Awkward 1.x: PR #478. Also fixed some bugs when writing TTrees to preexisting files (\"update\" mode): PR #488.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.1.6'>4.1.6</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-10-28T00:04:52Z",
  "number":490,
  "title":"4.1.6",
  "url":"https://github.com/scikit-hep/uproot5/discussions/490"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@mpad added an `AsDtypeInPlace` interpretation to read data into a preallocated array (in `library=\"np\"` mode): PR #487.\r\n\r\n@jpivarski restricted Uproot 4.x to Awkward 1.x in `pip install uproot[dev]` as well as at runtime.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.1.7'>4.1.7</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-10-28T22:46:31Z",
  "number":492,
  "title":"4.1.7",
  "url":"https://github.com/scikit-hep/uproot5/discussions/492"
 },
 {
  "author":{
   "login":"XDongiang"
  },
  "body":"I meet same problem fixed in uproot3, but i meet it again in uproot4!\r\n```\r\n[array([[ 0.7882904 ,  0.61012289,  0.04211533,  0.06050724],\r\n        [ 0.55939287,  0.13029095,  0.16960895,  0.15316812],\r\n        [ 1.37675754, -0.7024612 , -1.02975954, -0.31287042],\r\n        [ 0.96134215, -0.03795264,  0.81803526,  0.09919506]])\r\n array([[ 0.60124435, -0.30947612, -0.14544959, -0.02909869],\r\n        [ 1.1608169 , -0.63750589,  0.75457903, -0.35773679],\r\n        [ 0.83054014, -0.02685332, -0.56037676,  0.3624041 ],\r\n        [ 1.09318157,  0.97383533, -0.04875268,  0.02443138]])\r\n array([[ 0.5964254 ,  0.29379539, -0.04998751,  0.15228852],\r\n        [ 1.24460004,  0.74962303,  0.45007822,  0.73539593],\r\n        [ 0.90096949, -0.48115726, -0.48470123, -0.31871845],\r\n        [ 0.94378803, -0.56226116,  0.08461052, -0.568966  ]]) ...\r\n array([[ 0.65629214,  0.10312235, -0.39342784,  0.14691054],\r\n        [ 1.18158272, -0.29126532, -1.0321247 ,  0.04799862],\r\n        [ 1.23043986, -0.01224176,  1.11501659, -0.16387006],\r\n        [ 0.61746824,  0.20038474,  0.31053595, -0.0310391 ]])\r\n array([[ 0.55299992,  0.16850911,  0.10276983, -0.15210185],\r\n        [ 0.68238664,  0.39755071, -0.03958946, -0.24964106],\r\n        [ 1.24395833, -0.67337457,  0.91814977, -0.08534078],\r\n        [ 1.20643806,  0.10731475, -0.98133014,  0.48708369]])\r\n array([[ 0.79923378,  0.01862661, -0.45713698, -0.43097156],\r\n        [ 1.02001038,  0.25929245, -0.58030893, -0.6266683 ],\r\n        [ 0.65326153, -0.07903001,  0.37375   ,  0.19261061],\r\n        [ 1.21327726, -0.19888905,  0.66369591,  0.86502926]])]\r\n```\r\nhttps://github.com/scikit-hep/uproot3#multiple-values-per-event-fixed-size-arrays\r\n```\r\ntree = uproot3.open(\"http://scikit-hep.org/uproot3/examples/nesteddirs.root\")[\"one/two/tree\"]\r\narray = tree.array(\"ArrayInt64\", entrystop=20)\r\narray\r\n# array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n#        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\r\n#        [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\r\n#        [ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\r\n#        [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\r\n#        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\r\n#        [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\r\n#        [ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7],\r\n#        [ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8],\r\n#        [ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9],\r\n#        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\r\n#        [11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\r\n#        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\r\n#        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13],\r\n#        [14, 14, 14, 14, 14, 14, 14, 14, 14, 14],\r\n#        [15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\r\n#        [16, 16, 16, 16, 16, 16, 16, 16, 16, 16],\r\n#        [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\r\n#        [18, 18, 18, 18, 18, 18, 18, 18, 18, 18],\r\n#        [19, 19, 19, 19, 19, 19, 19, 19, 19, 19]])\r\n```",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"XDongiang"
     },
     "body":"I did not get the correct shape using uproot3, so I very much doubt that your program is correct.Doing rigorous scientific research requires stable tools, not error-prone toys. ",
     "createdAt":"2021-11-05T12:28:54Z",
     "number":1594690,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Both Uproot 3 and Uproot 4 return an array of the correct shape for this file/tree/branch. In Uproot 4,\r\n\r\n```python\r\n>>> import uproot\r\n>>> tree = uproot.open(\"http://scikit-hep.org/uproot3/examples/nesteddirs.root\")[\"one/two/tree\"]\r\n>>> tree[\"ArrayInt64\"].array()\r\n<Array [[0, 0, 0, 0, 0, ... 99, 99, 99, 99]] type='100 * 10 * int64'>\r\n```\r\n\r\nor as a NumPy array,\r\n\r\n```python\r\n>>> tree[\"ArrayInt64\"].array(library=\"np\")\r\narray([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\r\n       [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\r\n       [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\r\n       ...\r\n       [97, 97, 97, 97, 97, 97, 97, 97, 97, 97],\r\n       [98, 98, 98, 98, 98, 98, 98, 98, 98, 98],\r\n       [99, 99, 99, 99, 99, 99, 99, 99, 99, 99]])\r\n```\r\n\r\nwhich has shape `(100, 10)`.\r\n\r\nDoing the same thing in ROOT,\r\n\r\n```python\r\n>>> import ROOT\r\n>>> file = ROOT.TFile(\"nesteddirs.root\")\r\nTClass::Init:0: RuntimeWarning: no dictionary for class Event is available\r\nTClass::Init:0: RuntimeWarning: no dictionary for class P3 is available\r\n>>> tree = file.Get(\"one/two/tree\")\r\n>>> for x in tree:\r\n...     print(list(x.ArrayInt64))\r\n... \r\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\r\n[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\r\n...\r\n[97, 97, 97, 97, 97, 97, 97, 97, 97, 97]\r\n[98, 98, 98, 98, 98, 98, 98, 98, 98, 98]\r\n[99, 99, 99, 99, 99, 99, 99, 99, 99, 99]\r\n```\r\n\r\n> Doing rigorous scientific research requires stable tools, not error-prone toys.\r\n\r\nReally? If this had been a bug, I would have fixed it right away, added a test, and released a new version. The response is usually same-day. That way, the distribution of testing and maintenance conforms to the use-cases: the bugs people report get fixed and the tests ensure that they never regress\u2014progress is always forward, though it is iterative. If we tried to guarantee perfection before each release, we would first have to define a domain of use-cases to test, but the users have a much better idea about what their own problems are than we do. This is, loosely speaking, the \"agile\" method, to be contrasted with \"big design up-front,\" which has been disfavored [for the past two decades](https://en.wikipedia.org/wiki/Agile_software_development#History) because it never was very effective. (It's a guiding principle of the ROOT project as well: see the [intro to the ROOT User's Guide](https://root.cern.ch/root/htmldoc/guides/users-guide/ROOTUsersGuide.html#introduction). That intro was added [in August 2000](https://web.archive.org/web/20030210203640/ftp://root.cern.ch/root/Users_Guide_3_2c_TwoInOne.pdf).)\r\n\r\n_But,_ there was no bug. The above is the correct behavior.",
     "createdAt":"2021-11-05T13:53:54Z",
     "number":1595155,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"XDongiang"
        },
        "body":"Have you really tested your code strictly? LOL",
        "createdAt":"2021-11-06T00:38:38Z",
        "number":1597894
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"XDongiang"
     },
     "body":"![image](https://user-images.githubusercontent.com/43576361/140592199-d7c3cf79-34d5-4daf-9d94-9f9cc632796a.png)\r\n![image](https://user-images.githubusercontent.com/43576361/140592247-4ff506f4-297c-48b5-8b7e-5aff261fbf22.png)\r\nuproot3 also has wrong shape",
     "createdAt":"2021-11-06T00:51:53Z",
     "number":1597912,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Without access to PhiKK_DATA.root, I have no context to know that what you're showing me is wrong. In the case of nesteddirs.root, the shape `(100, 10)` is correct.\r\n\r\nAlthough Uproot 3 and 4 are rather different in their presentation of the data, they do look consistent with each other here. This seems to be some TBranch with a variable-length type (in principle, the array in an event can have any length) with constant length for all the entries shown (in practice, the first and last arrays happen to have the same lengths). That kind of data can be most efficiently turned into a 2-dimensional NumPy array by reading it into an Awkward Array (`library=\"ak\"`, the default) and then passing that through [ak.to_numpy](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_numpy.html) (faster than NumPy dtype=\"O\", because that dtype is just a collection of Python references; Awkward preserves the contiguousness of the numerical data).\r\n\r\nOn the other hand, I don't even know what aspect of what you're showing me is not what you want. I'm just guessing.",
        "createdAt":"2021-11-06T02:18:12Z",
        "number":1598028
       },
       {
        "author":{
         "login":"XDongiang"
        },
        "body":"Thank you for your answer, What you answered is exactly what i need.\r\nBecause of the previous question, I don\u2019t have much confidence in this program",
        "createdAt":"2021-11-06T03:18:46Z",
        "number":1598099
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":3
  },
  "createdAt":"2021-11-05T12:05:39Z",
  "number":497,
  "title":"Multiple values per event in uproot4",
  "url":"https://github.com/scikit-hep/uproot5/discussions/497"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@masonproffitt fixed warning message for old versions of XRootD/pyxrootd (before 4.11.1): PR #501.\r\n\r\n@jpivarski fixed conflicts between explicitly specified TBranches and automatically generated counter-TBranches when they have the same name: PR #499. Also, `uproot.lazy` identifies the only TTree in a file even if there are multiple with the same name, different cycle numbers: PR #500.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.1.8'>4.1.8</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-11-08T18:54:34Z",
  "number":502,
  "title":"4.1.8",
  "url":"https://github.com/scikit-hep/uproot5/discussions/502"
 },
 {
  "author":{
   "login":"ZhenxuanZhang-Jensen"
  },
  "body":"Hello everyone:\r\nI am trying to do some cuts in the .arrays, but no any description can do that with more than two cut apply.\r\nfor example:\r\nevents_250 = uproot.open(\"M250.root:Events\")\r\nand then apply two cuts on it:\r\nevents_250.arrays(cut = (\"GEN_deltaR_MAX4Q>1.2\" and \"GEN_deltaR_MIN4Q<0.8\"),library =\"np\")[\"GEN_deltaR_MIN4Q\"]",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"You can write `cut = \"(GEN_deltaR_MAX4Q>1.2) & (GEN_deltaR_MIN4Q<0.8)\"` (with parentheses!), but also, there's no strong reason to put the cut in this string. You can cut the arrays after the `.arrays` call just as easily:\r\n\r\n```python\r\nttree = uproot.open(\"M250.root:Events\")\r\none_branch = ttree[\"GEN_deltaR_MIN4Q\"]\r\nGEN_deltaR_MAX4Q = one_branch.array(library =\"np\")\r\nGEN_deltaR_MAX4Q_cut = GEN_deltaR_MAX4Q[(GEN_deltaR_MAX4Q>1.2) & (GEN_deltaR_MIN4Q<0.8)]\r\n```\r\n\r\nThe `cut` argument does nothing more than put the string you give it into square brackets, which is how you cut in NumPy.\r\n\r\nAlso, calling `.arrays` without specifying any `expressions` or `filter_branches` reads all branches from disk. Putting `[\"GEN_deltaR_MIN4Q\"]` after it picks out one of the already-read branches and throws away the rest. `.array` (above) is the easiest way to read only one branch, but you can also use `.arrays` with `expressions` or `filter_branches` to read a selection of branches.\r\n\r\nIt might not be relevant on small files, but on big files, reading unnecessary branches can be slow or impossible of you don't have enough memory.",
     "createdAt":"2021-11-18T12:50:12Z",
     "number":1664279,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"Thank you for your answer. I still have confusion about how you apply the cut to all_events. it seems all_events is not even a variable or anything. The other thing is about the `.array` , you use one_branch.array only read one branch without any expressions?",
        "createdAt":"2021-11-18T12:57:29Z",
        "number":1664319
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Sorry, I fixed it above. I was trying to write it on my phone this morning, and I changed the code a few times for clarity.\r\n\r\nIn the above version, `one_branch` is a branch object and `GEN_deltaR_MAX4Q` is a NumPy array. If you pull a TBranch out of a TTree (`one_branch`), you can call [TBranch.array](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#array) on it to get one array. If you don't, you can call [TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) on the TTree to get multiple arrays. With `library=\"np\"`, it's a Python dict of NumPy arrays; with `library=\"ak\"` (default), it's a single Awkward Array (with record structure instead of a dict, which is a bit more flexible); with `library=\"pd\"`, it's a single Pandas DataFrame. See the links above for details.\r\n\r\nBut as for your original question, to get the \"logical and\" of multiple cuts, use the `&` operator with parentheses. (For the importance of parentheses, see #508.) It does not matter whether you do that inside the `cut` string or in Python after the array has been loaded. (A lot of people have been thinking that there's something special about putting the cut in the `cut` string and have been going to great lengths to express difficult things in it, when they could more easily be done in multiple lines of Python code. I want everyone to know that the only advantage `cut` provides is the convenience of putting the code inside square brackets for you!)",
        "createdAt":"2021-11-18T13:39:12Z",
        "number":1664578
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2021-11-18T03:11:01Z",
  "number":510,
  "title":"How to apply two cut with in .arrays?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/510"
 },
 {
  "author":{
   "login":"CaetanoE"
  },
  "body":"I just started playing with uproot because it seems so useful, however, I wanted to know if there is a way to apply multithreading functionalities to my script because it is much slower than using RDataframe, or if its better to just read the data a generate the histograms using RDataframe and then plot with uproot.\r\n\r\nThanks all for your time :) ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"A detailed description is here: https://uproot.readthedocs.io/en/latest/basic.html#parallel-processing\r\n\r\nUnless your workflow is dominated by decompression, parallelization is not likely to accelerate it much because of Python's Global Interpreter Lock (GIL). That's why [Coffea analyses](https://coffeateam.github.io/coffea/) usually parallelize by distributing among Python processes, rather than threads.\r\n\r\nUproot also doesn't do the plotting itself\u2014the idea is that it's _exclusively_ an I/O library\u2014that's passed to [hist](https://hist.readthedocs.io/en/latest/), [mplhep](https://mplhep.readthedocs.io/en/latest/), Matplotlib, etc. If you're using RDataFrame for the bulk processing, you might find it easier to go directly to one of those libraries, rather than involving Uproot.",
     "createdAt":"2021-11-30T09:52:47Z",
     "number":1721115,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"CaetanoE"
     },
     "body":"Got it, thanks for the fast response @jpivarski ",
     "createdAt":"2021-11-30T10:35:58Z",
     "number":1721430,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2021-11-30T08:57:42Z",
  "number":515,
  "title":"Multitrheading in uproot \ud83e\uddf6",
  "url":"https://github.com/scikit-hep/uproot5/discussions/515"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"**The 4.1.x series is the last to support Python 2.7 and Python 3.5. From 4.2.0 onward, only Python 3.6 and above will be supported.**\r\n\r\n(Any bug-fixes that absolutely must be used in an old Python environment will need to be versions 4.1.10, 4.1.11, etc., and these won't include features and regular maintenance that go into the main branch, 4.2.x and beyond.)\r\n\r\nIn this version:\r\n\r\n@pfackeldey removed an unnecessary copy of memory-mapped data.\r\n\r\n@veprbl exposed the `data` property of TTable: PR #506.\r\n\r\n@jpivarski added an example of RNTuple header parsing: accidentally as 4bb0f6aedbe7e9a8ba8feb166a16e71274259d26, rather than a formal PR. Added more careful DataFrame-checking code in TTree-writing, to avoid a spurious error message: PR #517. Fixed a picking issue with histograms (just the TAxis) by ensuring that all Behaviors are not Python Abstract Base Classes: PR #521. \n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.1.9'>4.1.9</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2021-12-08T16:36:42Z",
  "number":524,
  "title":"4.1.9",
  "url":"https://github.com/scikit-hep/uproot5/discussions/524"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"I am trying to develop code which will parse a series of branches and use uproot to read those branches. The branches could be passed as a regex pattern or as an exact name or as an expression. \r\n\r\nI am looking to combine all the requested branches into an array and passing them in one go to uproot.  The TBranch argument  ```filter_name``` fails to match the expressions and the expressions list fails to process regex patterns. Even if I group expressions together and filters together,  ```exoressions``` always overrides the ```filter_name``` argument. Is it possible to allow the ```filter_name``` argument to process an expression or for the ```expression```  argument to process regex patterns?\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I think that combining the functionality of `filter_name` and `expressions` would severely complicate them. They're kept separate because `*` has a very different meaning in regular expression search than it does in mathematical formulas. Even if there's a way to make this two-pass thing work, isn't that going to lead to more confusion?\r\n\r\nI have to ask, why is it important for so much of the analysis to be done in a one-line function call? I don't know of a real reason to use `expressions` as mathematical formulas at all, except for the fact that the functionality had to be implemented somewhere to support TTree's built-in aliases, so it might as well be exposed more generally. Why not use `filter_name` to get a set of uncomputed arrays and do array manipulation in Python statements, rather than in a string? It _is_ the same evaluation; there is no performance advantage or anything.",
     "createdAt":"2021-08-27T17:52:53Z",
     "number":8308455,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"MoAly98"
     },
     "body":"Hi @jpivarski, \r\n\r\nThanks a lot for the quick reply.\r\n\r\nYou make a fair point about the confusion. I\u2019m not entirely sure how regex processing is taking place in Uproot backend, but I would have thought the use of the syntax `/pattern/i` would distinguish between an expression and a regex pattern? \r\n\r\nThe reason I was looking into this is because we are writing a framework which will parse from a configuration file, so the exact expressions are unknown until runtime. I can\u2019t think of a straight-forward way (that doesn\u2019t involve writing up an interpreter) to breakdown the expression from the config, call the branches individually then perform the calculation on them. There is also the\r\nproblem of interpreting slicing expressions (e.g. Branch[:,0])",
     "createdAt":"2021-08-27T20:02:09Z",
     "number":8308456,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Yes, the slashes at the beginning and end of the string (`/pattern/` with possible flags like `i`) is the signal that this is a regex filter:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/a17aca8cc07ba203e48fe63dc0d5f0b6749de4a1/src/uproot/_util.py#L156-L161\r\n\r\nwith\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/a17aca8cc07ba203e48fe63dc0d5f0b6749de4a1/src/uproot/_util.py#L119\r\n\r\nbut if you need the calculation to go in stages, first a regex, then a computation, that would get messy. Should there be two strings? Should characters like `*` be quoted as `\\*` in the regex phase, then interpreted as operators in the computation phase? I'm just having trouble seeing that as making people's lives easier, rather than harder.\r\n\r\nAnd even then, the `expressions` are not very constrained: they can be any Python _expression_, which is about half of the Python language (the other half being statements). If I'm not misunderstanding which direction you're going in, you could impose constraints in the configuration language and use that to generate Python. If you're trying to go the other way, taking a Python script and turning it into something declarative, that direction is not possible.\r\n\r\nSo back to the direction I think you're going in: if you're defining a configuration file language, you can add knobs and dials that generate Python code as easily as it could generate Uproot `expression` strings. After all, they're both the same language, Python. You mention \"writing up an interpreter\"\u2014it wouldn't need to be all that, since generating the text of Python code from templates is not as much work as building an interpreter, it's more of a source code to source code translation. Also, by \"configuration file language,\" I don't mean writing a parser, I mean interpreting nested elements in YAML: that's pretty common for configuration files now. The main thing is that I don't see how what you need to generate is worse or harder if you're generating Python source code that gets run through `exec` than it would be if you were generating one line of Python source code that _I_ run through `exec`!\r\n\r\nIt happens here:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/a17aca8cc07ba203e48fe63dc0d5f0b6749de4a1/src/uproot/language/python.py#L159-L184\r\n\r\nThere's some manipulation of the Python AST to recognize `\"nested.branch.name\"` as a single variable, rather than unpacking a class instance, but other than that, it all goes through Python's `eval` in the end.\r\n\r\nI should mention that there's an [ongoing project](https://github.com/AdvaitDhingra/formulate1) to add TTree::Draw as a language for `expressions`, possibly as a future default. Interpreting the strings as Python happened to be the easiest to implement, not our highest preference.",
     "createdAt":"2021-08-27T21:25:42Z",
     "number":8308457,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"MoAly98"
     },
     "body":"Hi @jpivarski \r\n\r\nThanks a lot for your reply. I will like into your suggestions. I wasn't looking for anything particularly complicated -- the goal was simply allow a user to specify something like \r\nnew_branch = Branch(expression = (branch1*branch2)/2)\r\nregex_banch = Branch(filter=*pattern*)\r\n and in the back-end I would change ```pattern -> /pattern/i ``` to ensure it is explicitly defiend as a regex pattern and I had hoped i would be able to pass this expression for the new branch at the same time to ```uproot.iterate()``` for example so that i can read both branches requested by user. Anyways, thanks a lot for the very detailed and helpful answers :) ",
     "createdAt":"2021-09-01T10:30:27Z",
     "number":8308458,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I think I kept this open to be informative, but we can do that with Discussions now, so I'll convert this Issue into a Discussion.",
     "createdAt":"2024-01-30T16:17:08Z",
     "number":8308459,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":5
  },
  "createdAt":"2021-08-27T17:19:56Z",
  "number":1111,
  "title":"Allowing ```TBranch:: filter_name``` to process expressions or  ```TBranch:: expressions``` to process regex?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/1111"
 }
]
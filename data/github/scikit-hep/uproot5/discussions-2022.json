[
 {
  "author":{
   "login":"EnginEren"
  },
  "body":"Dear all,\r\n\r\nThanks a lot for proving us uproot4. I really enjoy using it. \r\n\r\nI was wondering if it's possible to read events with GPU. Currently, I'm reading 0.5 Million events like \r\n\r\n```python\r\nup4_events = uproot.open(\"./isoMuon/P4f_zzorww_isoMu.root:zzorww;1\", num_workers=8)\r\n```\r\nit takes like 220 ms. I also have Nvidia P100 in my system and I'm really curious how much I can reduce this 220ms reading time.\r\n\r\nCheers,\r\nEngin\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"There's a number of things to think about; the first is that the data have to get from disk to the GPU somehow. It will usually get there via RAM, though supercomputers can go directly from network to GPUs (and they have internal networks that are faster than disk access; it's usually the other way around if the data have to go through the internet).\r\n\r\nHere's a good summary of the storage latency hierarchy: https://blog.codinghorror.com/the-infinite-space-between-words/\r\n\r\nWhether you go from disk to RAM to GPU or directly from disk to GPU (such a thing would require special hardware), the bottleneck is going to be the disk (unless your operating system has already cached those parts of the disk in RAM for you; be aware of that in tests\u2014I use [vmtouch](https://hoytech.com/vmtouch/)). A first suggestion if you want to do some computations on the GPU would be to read it using Uproot as usual and then copy the data from NumPy arrays (RAM) to CuPy arrays (GPU global memory).\r\n\r\nThe data for a TBranch of a ROOT file is split into chunks called TBaskets. Combining these chunks into a single, contiguous array requires an extra data-copy, and you _could_ save some time by making that data-copy also a copy from RAM to GPU. If your TBranch has an [uproot.AsDtype](https://uproot.readthedocs.io/en/latest/uproot.interpretation.numerical.AsDtype.html) interpretation, you can provide a pre-allocated CuPy array with [uproot.AsDtype.inplace](https://uproot.readthedocs.io/en/latest/uproot.interpretation.numerical.AsDtype.html#inplace) and then Uproot will fill that array instead of creating a new one if you pass the [AsDtypeInPlace](https://uproot.readthedocs.io/en/latest/uproot.interpretation.numerical.AsDtypeInPlace.html) as a new interpretation to [uproot.TBranch.array](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#array). If the total time is dominated by the disk read, this won't be a noticeable improvement, but it's a possible help if all other stars align.\r\n\r\nOn top of that, your data are probably compressed, and the decompression stage has to happen in RAM. There aren't many decompression algorithms written for GPUs, and I suspect it's because compression/decompression requires sequential access to deal with variable-length structures, which doesn't fit the GPU's model of parallelism very well. If your data are compressed with the slowest algorithm, LZMA, then that can even dominate over disk-reading. If somebody someday manages to write LZMA decompressors on GPUs, that would be a breakthrough. It's hard, though. I just did another search for such things, and the closest I found were \"LZMA-like compression ratios,\" meaning not the specific LZMA format, but another one. That doesn't help if your data are in a format whose only decompressors are CPU-bound. It still looks like decompression is CPU-bound for the foreseeable future.\r\n\r\nOn top of that, if your data are not simple types, but have to be interpreted [uproot.AsObjects](https://uproot.readthedocs.io/en/latest/uproot.interpretation.objects.AsObjects.html), then it's even worse: they're being interpreted with Python code. There's a project in development to replace that Python code with considerably faster, though still CPU-bound, Forth code (https://arxiv.org/abs/2102.13516). We should see factors of 100\u00d7 speedup for those cases, though the algorithms are still (necessarily!) very GPU-unfriendly. To be clear, this does not apply to AsDtype, AsJagged, AsStridedObjects, etc.\r\n\r\nOnce the data are on GPUs, you'll need tools to perform calculations on them there. [CuPy](https://cupy.dev/) has a good set of tools (which can be used with [Numba](http://numba.pydata.org/numba-doc/latest/cuda/)), but only if your data are non-jagged [uproot.AsDtype](https://uproot.readthedocs.io/en/latest/uproot.interpretation.numerical.AsDtype.html). Awkward Array backends for GPUs are in development, but not ready yet. If your data are strictly tabular, then that's not an issue for you.\r\n\r\nSo, to summarize, [uproot.AsDtype.inplace](https://uproot.readthedocs.io/en/latest/uproot.interpretation.numerical.AsDtype.html#inplace) with a CuPy array is one thing you can do, and it can make a difference if your data are not compressed and they are in your operating system's cache or some very fast disk (e.g. NVMe). But if you have a more typical situation, the disk and/or decompression are bottlenecks that the data have to pass through before they can even get to the GPU.",
     "createdAt":"2022-01-05T16:49:51Z",
     "number":1913660,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"EnginEren"
     },
     "body":"Hi @jpivarski ,\r\n\r\nthank you very much for your detailed and comprehensive explanations!\r\n\r\nIndeed, as a HEP fellow, I have `AsJagged(AsDtype('>i4'))`. (i.e the number of isolated-muons varies from event to event). That means, I'm looking forward to Awkward Array backends for GPUs.\r\n\r\nCheers,\r\nEngin\r\n\r\n\r\n",
     "createdAt":"2022-01-06T09:22:19Z",
     "number":1917379,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Good to know! For the foreseeable future, Uproot will be reading AsJagged using the CPU only, and some of these cases (`std::vector`) will get a mild improvement from the AwkwardForth project (summer 2022), but others (variable length array, not STL) are already as optimized as they can be. (`AsJagged(AsDtype('>i4'))` is _not_ currently implemented with a Python loop, in either case, so there isn't a ~100\u00d7 potential speedup for us to take advantage of.) I don't foresee adding an \"AsJaggedInPlace\" like \"AsDtypeInPlace\" because the interface for such a thing would be pretty complicated. If it existed, it would be hard to use.\r\n\r\nOnce read on the CPU, you'd be able to copy it to a GPU and perform calculations on it on the GPU in a future version of Awkward Array. Last summer, we did some GPU development, to the extent that you could perform NumPy ufuncs and a few `ak.*` operations on GPU-bound Awkward Arrays, but that and a few other projects pointed to the need to refactor Awkward Array to support these things better. We're in the middle of that refactoring, and Awkward 2.0 will be released this spring which would be _ready_ for developing the GPU backend in an easier way. (In the process of preparing Awkward 2.0 for more complete GPU support, we broke the Awkward 1.x experimental support that was there. We should view last summer's work as a learning experience for the implementation: don't try to use it!)\r\n\r\nWe have a deadline of _full_ GPU support by 2024 ([this project](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2103945)), but most parts should be available before the end date because that project includes use-cases, not just implementation. The reason I'm telling you this is to let you know that the timescale for GPU support is on the scale of a year or two, and that might or might not be relevant for your project.",
        "createdAt":"2022-01-06T15:53:13Z",
        "number":1919460
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-01-05T16:04:53Z",
  "number":535,
  "title":"Uproot4 with GPU option",
  "url":"https://github.com/scikit-hep/uproot5/discussions/535"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi, \r\n\r\nI'm writing a framework which will process root files using uproot 4. In the framework, the user can request that TBranches with names `branches = [br1, br2]` are read from a TTree `tree` N root files. Before I throw these branches and tree to uproot to do the reading, I would like to check that the branches and the tree are available in each of the files and skip the files where the branches/tree is missing. What would be the fastest way of doing this with uproot? At the moment I'm doing this check in a `for` loop with `uproot.open(file)` then examining the available keys, but this is rather slow.  I thought about using `uproot.lazy` and examine the keys, but somehow this is slower than using `uproot.open`. \r\n\r\nI also wonder if you have suggestions on handling this at file reading time without breaking the code, in a way that would avoid the `for` loop all together so that the project scales. \r\n\r\nThank you very much. ",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Avoiding `for` loops only improves performance of numerical data processing when the data are in an array (flat memory buffer, not Python objects). It also can only apply if deriving the data doesn't do a lot of Pythonic `for` loops itself.\r\n\r\nFinding out what TObjects and TBranches exist in a ROOT file is non-numeric, operating on data that are not array-like at all. In Uproot, this is implemented the only way it can be in Python: with loops, classes, dict-lookups, etc. After all of that work has been done, avoiding a `for` loop when inspecting it (which isn't possible, the way it's set up) wouldn't provide any performance advantage.\r\n\r\nBut anyway, this fits the _intended_ performance model of Uproot anyway: the idea is that there's a small number of TBranches (only thousands or tens of thousands) but a large number of numerical entries in the TTree (millions or billions). Pythonic code is used for the small stuff and NumPy casting is used for the big stuff, wherever possible (i.e. as long as the data types are numeric).\r\n\r\n`uproot.lazy` is the same thing plus more work, setting up data-reading on demand. It's well-suited to exploration, in which it would be annoying to have to specify up front which TBranches you need. If you know which TBranches you need\u2014for instance, because you're writing a framework\u2014then `uproot.lazy` is only counter-productive. (This is especially true for remote files, since lazy patterns generate many more individual requests, as it discovers what you're interested in, and latency-limited remote files are optimized by specifying what you want to read in as few requests as possible.)\r\n\r\nIf you know the name of the TTree you want to read, ask for it in square brackets rather than looping over the names of TObjects in the file:\r\n\r\n```python\r\nwith uproot.open(filename) as file:\r\n    tree = file[\"name_of_ttree\"]\r\n```\r\n\r\nThis will cause Uproot to read the file header, use that to find the root directory, read the root directory (but none of the TObjects or subdirectories), put the \"key names \u2192 TKey objects\" into a dict for fast lookup, use the dict once to get the TKey corresponding to your TTree, and then read the TTree header, which includes the names and types of all the TBranches. At this point, none of the numerical data has been read.\r\n\r\nIf you instead loop over the TObject names in the file:\r\n\r\n```python\r\nwith uproot.open(filename) as file:\r\n    for keyname in file:\r\n        if keyname = \"name_of_ttree\":\r\n            tree = file[keyname]\r\n            break\r\n```\r\n\r\nthen (I think) it does a recursive walk, which unnecessarily reads all the subdirectories. If your file is on a latency-limited remote network and you have a lot of subdirectories, this would save you a round-trip request-response for each subdirectory. If you want to ensure that the TTree exists, there's the `in` keyword (which I think reuses the \"key names \u2192 TKey objects\" dict). Catching KeyErrors can be slow because Uproot KeyErrors get a full list of key names and sorts them by similarity in spelling to what you asked for to try to improve the user experience.\r\n\r\nIf your ROOT files contain only one object and it's the TTree of interest, then the above isn't going to help at all.\r\n\r\nAlso, be sure that you're not checking for the existence of TBranches by reading the TBranches. You can do\r\n\r\n```python\r\n\"branch_name\" in tree\r\n```\r\n\r\nto see if the TBranch exists by querying the already-read TTree metadata, rather than\r\n\r\n```python\r\narray = tree[\"branch_name\"].array()\r\n```\r\n\r\nto actually read the numerical data from the file.\r\n\r\nAnyway, that's the bottom of the barrel; there isn't much faster it could go, given what you're trying to do. ROOT (C++) and UnROOT (Julia) avoid the cost of interpreting the ROOT file headers in Python, which could be an issue if and only if you're not limited by disk access or remote network latency, which are independent of programming language.\r\n\r\nRereading your original question, it sounds like you're doing a first pass to see which files exist and have the TTrees and TBranches you need, then a second pass over the good ones. Since there is an unavoidable amount of hardware (disk and/or network) and software (Python interpretation) work involved in opening a file and reading TTree metadata, why do it twice? Why not skip missing files/missing TTrees/missing TBranches in the same loop that does the final processing of the data, so that you only read it once? Is it because a user might decide not to do the processing based on whether all the expected data exist or not?\r\n\r\nAlternatively, if you have some control over your set of files, maybe you'd want to inspect them all once and put the relevant metadata into a database? Databases are a much faster format for querying than the headers of ROOT files. There would even be a potential to read the TTree metadata _zero_ times in the user processing loop by also storing the TBranch interpretations and seek locations of the TBaskets, but that would be an advanced project (see Coffea).",
     "createdAt":"2022-01-17T17:28:46Z",
     "number":1985110,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-17T16:40:38Z",
  "number":539,
  "title":"Fastest way to check content of many files with uproot?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/539"
 },
 {
  "author":{
   "login":"ryuwd"
  },
  "body":"This is of course something of an edge-case with [EOS tokens](https://eos-docs.web.cern.ch/using/tokens.html), which contain colons `:`. This means xrootd paths can have a format which may confuse `uproot4`, which looks for an object-in-file path by default:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/3fe33aff6d6ed88203eed90f4edb5f2bc720f3e6/src/uproot/_util.py#L241\r\n\r\nThis means when using `uproot.open('root://some_xrootd_path//eos/mount?authx=token:with_colon')`, the token (needed for the authentication) will be cut short.\r\n\r\nCan we switch off the object-in-file path parsing, or workaround it somehow?\r\n\r\nVersion 4.1.9",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"You can use `pathlib.Path` objects, which asserts that this _must_ be a file path, no colon funny business, or you can use this syntax:\r\n\r\n```python\r\nuproot.open({\"root://some_xrootd_path//eos/mount?authx=token:with_colon\": \"\"})  # left and right of colon as a dict\r\n```\r\n\r\nThe last time this was asked was #365, and the first time, which generated a long discussion about what we should do, was #79. As soon as I saw the corner cases, I thought it would be good to remove the feature entirely (convenience for most people shouldn't make things aggravating for some), but the ensuing discussion convinced me that there are cases in which the colon is necessary, such as selecting a lot of files with a wildcard. (The dict syntax also allows for that, too, but it is non-obvious. The file-not-found error message suggests it, and it's in the [uproot.open](https://uproot.readthedocs.io/en/latest/uproot.reading.open.html) documentation, but that still doesn't qualify as \"easy to find.\")",
     "createdAt":"2022-01-18T17:38:29Z",
     "number":1992152,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ryuwd"
        },
        "body":"FYI: the `{xrootd_url: \"\"}` workaround no longer works - uproot will insist on a non-empty tree name\r\n\r\nusing `Path(xrootd_url)` unfortunately results in `FileNotFoundError: file not found ([FATAL] Invalid address)` as `pathlib.Path` mangles it\r\n\r\nuproot 4.3.7",
        "createdAt":"2022-12-07T10:47:15Z",
        "number":4331714
       },
       {
        "author":{
         "login":"ryuwd"
        },
        "body":"Apologies for digging up this old discussion!",
        "createdAt":"2022-12-07T10:48:05Z",
        "number":4331717
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"What about `{xrootd_url: None}`? I was reminded in a recent email that `{xrootd_url: \"\"}` looks for an object in the file named `\"\"`.\r\n\r\n(For history, this colon-parsing was a community decision, but a contentious one (#79).)",
        "createdAt":"2022-12-07T15:13:24Z",
        "number":4334035
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-18T16:22:45Z",
  "number":541,
  "title":"Not possible to escape colon in filenames / avoid object-in-file path syntax (?)",
  "url":"https://github.com/scikit-hep/uproot5/discussions/541"
 },
 {
  "author":{
   "login":"CaetanoE"
  },
  "body":"Hi all, I was reading carefully the uproot documentation but I can't find exactly what I want to do. I need to read a root file, define some new branches using the data in existing branches and apply some cuts to this tree.\r\n\r\nUsing RDataframe for example is very easy because I can read the tree, .Define(---) variables and .Filter(---) them but I cannot do the same in uproot.\r\n\r\n I have tried to read the files using `uproot.concatenate` and then apply the cuts and aliases similar to how it is shown in the tutorial ` events.arrays(\"(pt1 > 50) & ((E1>100) | (E1<90))\", aliases={\"pt1\": \"sqrt(px1**2 + py1**2)\"})` but I can't make it work.\r\n\r\nThanks in advance for your time, this might be a stupid question.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Not a stupid question, of course! I was going to say that a similar thing was asked on Gitter, but I see that was you. Since you've been working on this for a few days, I'll write out a detailed explanation.\r\n\r\nThe only hard/not currently possible part of what you want to do is adding branches to an existing TTree. The problem there is that Uproot would have to overwrite the TTree metadata (the part of the file that specifies what branches exist and what their types are) but not the preexisting data. That function just hasn't been written, and I can tell you that such a function wouldn't be a small extension of what already exists. It would be a large project.\r\n\r\nIf you want to write the results to a different file (or not write the results at all, but histogram them or something), then that's a completely different thing. I'll address that.\r\n\r\nInstead of jumping to the answer, I'm going to walk through the thought process that I'd use to approach a problem like this. Suppose that we have a bunch of ROOT files. I'd start by picking one of them and opening that in an interactive prompt. I use the basic Python prompt directly, but IPython and Jupyter are also good. The main thing is to avoid putting all of your code in a script and then puzzling over why it doesn't run or produce any output.\r\n\r\nI don't have your sample, so let's say that the file is uproot-Zmumu.root:\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> skhep_testdata.data_path(\"uproot-Zmumu.root\")\r\n'/home/jpivarski/.local/skhepdata/uproot-Zmumu.root'\r\n```\r\n\r\nFirst, I'll open it without even using a `with` statement (something you should always do in scripts). The reason is that I don't want it to close right away.\r\n\r\n```python\r\n>>> infile = uproot.open(skhep_testdata.data_path(\"uproot-Zmumu.root\"))\r\n>>> events = infile[\"events\"]\r\n>>> events.show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nType                 | char*                    | AsStrings()\r\nRun                  | int32_t                  | AsDtype('>i4')\r\nEvent                | int32_t                  | AsDtype('>i4')\r\nE1                   | double                   | AsDtype('>f8')\r\npx1                  | double                   | AsDtype('>f8')\r\npy1                  | double                   | AsDtype('>f8')\r\npz1                  | double                   | AsDtype('>f8')\r\npt1                  | double                   | AsDtype('>f8')\r\neta1                 | double                   | AsDtype('>f8')\r\nphi1                 | double                   | AsDtype('>f8')\r\nQ1                   | int32_t                  | AsDtype('>i4')\r\nE2                   | double                   | AsDtype('>f8')\r\npx2                  | double                   | AsDtype('>f8')\r\npy2                  | double                   | AsDtype('>f8')\r\npz2                  | double                   | AsDtype('>f8')\r\npt2                  | double                   | AsDtype('>f8')\r\neta2                 | double                   | AsDtype('>f8')\r\nphi2                 | double                   | AsDtype('>f8')\r\nQ2                   | int32_t                  | AsDtype('>i4')\r\nM                    | double                   | AsDtype('>f8')\r\n```\r\n\r\nWhat I want to do next is going to come in two stages: I want to read data and then perform calculations. RDataFrame \"lazily\" gets data when you've defined some actions for it, but Uproot is \"eager\"\u2014it does what you say when you tell it to. (There's an [uproot.lazy](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.lazy.html), but we're rethinking how it ought to work because we'd like to incorporate [Dask](https://dask.org/), so I won't talk about that.)\r\n\r\nOne way to do this is to read all the data from the file into memory and then use what you want for calculations, but this is wasteful. Another is to read each array as you need it (what [uproot.lazy](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.lazy.html) currently does), but that's not a good pattern, either. (RDataFrame's pattern of reading all of the branches you need when you say \"go\" is what Dask does, which is why we're thinking about that.) Since you know that you want to calculate things with `px1`, `py1`, and `E1`, we can read just those with [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays).\r\n\r\n```python\r\n>>> events.arrays([\"px1\", \"py1\", \"E1\"])\r\n<Array [{px1: -41.2, py1: 17.4, ... E1: 81.6}] type='2304 * {\"px1\": float64, \"py...'>\r\n```\r\n\r\nIn a realistic situation, you'd probably want to read \"everything that starts with 'Muon_',\" which would be laborious to type this way. Instead of a list `[\"px1\", \"py1\", \"E1\"]`, you could use wildcards in `filter_name`:\r\n\r\n```python\r\n>>> events.arrays(filter_name=[\"p*1\", \"E1\"])\r\n<Array [{E1: 82.2, ... phi1: 0.037}] type='2304 * {\"E1\": float64, \"px1\": float64...'>\r\n```\r\n\r\nHowever, you might have just attempted to read too much from disk. What if there are a lot of TBranches starting with \"p\" and ending with \"1\"? You can check this more carefully by passing exactly the same `filter_name` to [uproot.TTree.keys](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#keys), which will list the names of the TBranches without trying to read them.\r\n\r\n```python\r\n>>> events.keys(filter_name=[\"p*1\", \"E1\"])\r\n['E1', 'px1', 'py1', 'pz1', 'pt1', 'phi1']\r\n```\r\n\r\nThat filter _is_ too broad. Let's narrow it to `\"p[xy]1\"`:\r\n\r\n```python\r\n>>> events.keys(filter_name=[\"p[xy]1\", \"E1\"])\r\n['E1', 'px1', 'py1']\r\n```\r\n\r\nBetter.\r\n\r\nIn the two-step procedure, the first step is to read the data. Let's put it in a variable (suggestively) named `batch`.\r\n\r\n```python\r\n>>> batch = events.arrays(filter_name=[\"p[xy]1\", \"E1\"])\r\n>>> batch\r\n<Array [{E1: 82.2, px1: -41.2, ... py1: 1.2}] type='2304 * {\"E1\": float64, \"px1\"...'>\r\n```\r\n\r\nThis is an [Awkward Array](https://awkward-array.org/quickstart.html) (because we didn't ask for NumPy with `library=\"np\"`). Formally, it's an array of records with fields E1, px1, and py1:\r\n\r\n```python\r\n>>> batch.type\r\n2304 * {\"E1\": float64, \"px1\": float64, \"py1\": float64}\r\n```\r\n\r\nbut it's very easy (computationally inexpensive) to pull out a purely numerical array for each field individually:\r\n\r\n```python\r\n>>> batch.E1\r\n<Array [82.2, 62.3, 62.3, ... 81.3, 81.3, 81.6] type='2304 * float64'>\r\n>>> batch.px1\r\n<Array [-41.2, 35.1, 35.1, ... 32.4, 32.5] type='2304 * float64'>\r\n>>> batch.py1\r\n<Array [17.4, -16.6, -16.6, ... 1.2, 1.2, 1.2] type='2304 * float64'>\r\n```\r\n\r\nFurthermore, these can be used in calculations to make new arrays:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> pt1 = np.sqrt(batch.px1**2 + batch.py1**2)\r\n>>> pt1\r\n<Array [44.7, 38.8, 38.8, ... 32.4, 32.4, 32.5] type='2304 * float64'>\r\n```\r\n\r\nAs in NumPy, expressions involving full arrays apply to each element of the arrays. It has the same for as if we had applied it to just the first element:\r\n\r\n```python\r\n>>> np.sqrt(batch.px1[0]**2 + batch.py1[0]**2)\r\n44.73220000003612\r\n```\r\n\r\nbut doing it one array (\"column\") at a time is much faster in Python.\r\n\r\nThe cut is just another expression, though its result is a boolean.\r\n\r\n```python\r\n>>> cut = (pt1 > 50) & ((batch.E1 > 100) | (batch.E1 < 90))\r\n>>> cut\r\n<Array [False, False, False, ... False, False] type='2304 * bool'>\r\n```\r\n\r\nAn array of booleans applied to an Awkward Array performs the cut.\r\n\r\n```python\r\n>>> batch[cut]\r\n<Array [{E1: 133, px1: 71.1, ... py1: -26.1}] type='268 * {\"E1\": float64, \"px1\":...'>\r\n```\r\n\r\n(note that the length of this array is about 10% of the original).\r\n\r\nNow I should talk about the \"aliases\" keyword argument. The [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) function can compute quantities, and the \"aliases\" argument defines subexpressions that can be used in other expressions. The first argument of [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) is interpreted as a list of expressions to compute:\r\n\r\n```python\r\n>>> events.arrays([\"sqrt(px1**2 + py1**2)\"])\r\n<Array [{'sqrt(px1**2 + py1**2)': 44.7, ... ] type='2304 * {\"sqrt(px1**2 + py1**...'>\r\n```\r\n\r\nYou probably don't want the field name of that Awkward Array to be \"`sqrt(px1**2 + py1**2)`\", so defining an alias can let you shorten it:\r\n\r\n```python\r\n>>> events.arrays([\"pt1\"], aliases={\"pt1\": \"sqrt(px1**2 + py1**2)\"})\r\n<Array [{pt1: 44.7}, ... {pt1: 32.4}] type='2304 * {\"pt1\": float64}'>\r\n```\r\n\r\nSo far, so good. You also wanted to apply \"`(pt1 > 50) & ((E1>100) | (E1<90))`\" as a cut. We can do that by passing it to the \"cut\" keyword argument:\r\n\r\n```python\r\n>>> events.arrays([\"pt1\"], cut=\"(pt1 > 50) & ((E1>100) | (E1<90))\", aliases={\"pt1\": \"sqrt(px1**2 + py1**2)\"})\r\n<Array [{pt1: 77}, ... {pt1: 72.9}] type='269 * {\"pt1\": float64}'>\r\n```\r\n\r\nAnd if you really did want to read every single TBranch in the TTree, you could just leave off the first argument.\r\n\r\n```python\r\n>>> events.arrays(cut=\"(pt1 > 50) & ((E1>100) | (E1<90))\", aliases={\"pt1\": \"sqrt(px1**2 + py1**2)\"})\r\n<Array [{Type: 'GT', Run: 148031, ... M: 96.1}] type='269 * {\"Type\": string, \"Ru...'>\r\n```\r\n\r\nSo I _could have_ answered this question by just saying, \"Put the letters '`cut=`\" after your first parenthesis.\" That would have solved the immediate problem. However, you were thinking of the \"aliases\" argument as something like RDataFrame's Define and the \"cut\" argument as something like RDataFrame's Filter, which they are not. RDataFrame sets up a workflow to lazily draw data from your ROOT files; Define and Filter construct steps in that workflow. [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) eagerly reads data from your ROOT files; \"aliases\" and \"cut\" are _shortcuts_ (as in \"syntactic sugar\") for computing expressions on Awkward Arrays.\r\n\r\nThe reason that matters are:\r\n\r\n   1. Laziness vs eagerness: you might not care until you run out of memory, but then you do!\r\n   2. The code in an RDataFrame string is C++ (because it sets up a C++ workflow). The code in an [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) string is Python, acting on arrays. (The \"`E1`\" and the \"`px1`\" in the expressions are whole arrays.)\r\n   3. If your analysis gets any more complicated than defining and filtering, you'll find the [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) strings to be confining. A lot of people have been asking me how to shoehorn their problems into these strings, but they're just convenient shorthands. Pulling the data out of the strings into Python allows you to write more than one line of code. (This is not to say RDataFrame strings are confining: you can write multi-line C++ in those strings.)\r\n\r\nContinuing from there, the next thing to consider is scaling this up to multiple files. The [uproot.TTree.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#iterate) function supports the same arguments as [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays), but it iterates over batches. Thus, when we get a block of code working for one batch, we can replace \"arrays\" with \"iterate\" and put it in a loop:\r\n\r\n```python\r\n>>> for batch in events.iterate(filter_name=[\"p[xy]1\", \"E1\"]):\r\n...     pt1 = np.sqrt(batch.px1**2 + batch.py1**2)\r\n...     cut = (pt1 > 50) & ((batch.E1 > 100) | (batch.E1 < 90))\r\n...     print(batch[cut])\r\n... \r\n[{E1: 133, px1: 71.1, py1: 29.5}, {E1: 88.1, ... {E1: 169, px1: -68, py1: -26.1}]\r\n```\r\n\r\nThis finished in one iteration because the file is so small. Note that you can tune \"step_size\" as needed.\r\n\r\nNow for multiple files: [uproot.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html) supports the same arguments as [uproot.TTree.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#iterate), but it takes wildcarded file names (and a TTree name after a colon).\r\n\r\n```python\r\n>>> filenames = \"~/.local/skhepdata/uproot-Zmumu*.root:events\"\r\n>>> for batch in uproot.iterate(filenames, filter_name=[\"p[xy]1\", \"E1\"]):\r\n...     pt1 = np.sqrt(batch.px1**2 + batch.py1**2)\r\n...     cut = (pt1 > 50) & ((batch.E1 > 100) | (batch.E1 < 90))\r\n...     print(batch[cut])\r\n... \r\n[{E1: 133, px1: 71.1, py1: 29.5}, {E1: 88.1, ... {E1: 169, px1: -68, py1: -26.1}]\r\n[{E1: 133, px1: 71.1, py1: 29.5}, {E1: 88.1, ... {E1: 169, px1: -68, py1: -26.1}]\r\n[{E1: 133, px1: 71.1, py1: 29.5}, {E1: 88.1, ... {E1: 169, px1: -68, py1: -26.1}]\r\n[{E1: 133, px1: 71.1, py1: 29.5}, {E1: 88.1, ... {E1: 169, px1: -68, py1: -26.1}]\r\n[{E1: 133, px1: 71.1, py1: 29.5}, {E1: 88.1, ... {E1: 169, px1: -68, py1: -26.1}]\r\n[{E1: 133, px1: 71.1, py1: 29.5}, {E1: 88.1, ... {E1: 169, px1: -68, py1: -26.1}]\r\n```\r\n\r\n(I happen to have a lot of files in that directory matching the `uproot-Zmumu*.root` pattern, all with different compression settings.)\r\n\r\nNow if you want these results to go into a new file, you can open a file for writing:\r\n\r\n```python\r\n>>> outfile = uproot.recreate(\"outfile.root\")\r\n>>> outfile.mktree(\"events\", {\"E1\": np.float64, \"px1\": np.float64, \"py1\": np.float64})\r\n<WritableTree '/events' at 0x7fb540f91fd0>\r\n>>> for batch in uproot.iterate(filenames, filter_name=[\"p[xy]1\", \"E1\"]):\r\n...     pt1 = np.sqrt(batch.px1**2 + batch.py1**2)\r\n...     cut = (pt1 > 50) & ((batch.E1 > 100) | (batch.E1 < 90))\r\n...     outfile[\"events\"].extend(batch[cut])\r\n... \r\n```\r\n\r\nIn a script, you would definitely want to use a `with` statement ([because of these reasons](https://realpython.com/python-with-statement/)). Same for any input files.\r\n\r\n```python\r\nwith uproot.recreate(\"outfile.root\") as outfile:\r\n    outfile.mktree(\"events\", {\"E1\": np.float64, \"px1\": np.float64, \"py1\": np.float64})\r\n    for batch in uproot.iterate(filenames, filter_name=[\"p[xy]1\", \"E1\"]):\r\n        pt1 = np.sqrt(batch.px1**2 + batch.py1**2)\r\n        cut = (pt1 > 50) & ((batch.E1 > 100) | (batch.E1 < 90))\r\n        outfile[\"events\"].extend(batch[cut])\r\n```\r\n\r\nNow outfile.root contains a TTree named \"events\" with filtered TBranches E1, px1, and py1 in it. If that's not what you wanted, I'm sure you can refine the workflow.\r\n\r\nThe main, overarching point that I wanted to make, though, is the process. You don't start with a code block like the one I've written above. You open an interactive prompt and try out small things, then scale them up. Uproot's interface was designed around that kind of process: [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays), [uproot.TTree.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#iterate), and [uproot.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html) take most of the same arguments so that you can try small tests with one and then swap it for the other. Same thing for [uproot.TTree.keys](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#keys) and [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays): they take the same \"filter_name\" argument so that you can test it out without reading data (which might be slow).\r\n\r\nThe expression that you wrote, `events.arrays(\"(pt1 > 50) & ((E1>100) | (E1<90))\", aliases={\"pt1\": \"sqrt(px1**2 + py1**2)\"})` does compute something: it makes an array of booleans that are true for each event that passes the cut and false for the ones that don't. It has a crazy-long field name, but if you print out a few values, you'd see that it's almost there. I don't know this for sure, but I'm guessing this was in a script and therefore hard to inspect? It really was very close.",
     "createdAt":"2022-01-19T22:18:37Z",
     "number":2001281,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"CaetanoE"
        },
        "body":"Thank you very much @jpivarski for taking the time to write this very nice and useful tutorial on uproot. I've been trying some things the last few days (thats why the question on glitter), but at the end I came to similar problems. It is true that I might be approaching the problem thinking about how to do it with Rdataframe and that why it ended up being quite a mess.\r\n\r\nNow thanks to you I think everything is crystal clear for me. Thank you again for your time, i hope a lot of people encounter this post when starting using uproot because it is really useful.\r\n\r\nCheers, Caetano.\r\n\r\nPS: I feel like I want to send you a box of chocolates in gratitude or something.  \ud83d\ude0a",
        "createdAt":"2022-01-20T09:10:50Z",
        "number":2004505
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-19T12:15:44Z",
  "number":543,
  "title":"Aliases and cuts when reading ROOT file",
  "url":"https://github.com/scikit-hep/uproot5/discussions/543"
 },
 {
  "author":{
   "login":"aphecetche"
  },
  "body":"Hi,\r\n\r\nI have a bunch of files, where each file contains two trees : a `genTree` with input kinematics etc... and an `eventTree` that is the result of the passage through (geant) simulation of the input particles (and has the very same structure as some real data). Nothing fancy here I would say. Both trees are thus \"aligned\" by construction, i.e. entry n of `genTree` relates to entry n of `eventsTree`. \r\n\r\nHow do I iterate over those two trees over many files ? Should I iterate over one tree and get the entry limits of each batch to read the second file (assuming there's some naming convention to infer the name of the second file from the first one) ? or is there a magic `uproot.iterate` syntax to do it for me ? :wink: \r\n\r\nThanks,",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Since the two trees are aligned by construction and you don't have to match values of one of TTree A's branches with one of TTree B's branches (as a `JOIN` key, as in SQL), then you can just use Python's `zip`. I was about to say \"`itertools.izip`\" because you absolutely want the two `uproot.iterate` iterators to be evaluated lazily through the iteration (so the memory use doesn't explode), but I'm behind the times: Python 3's built-in `zip` does that.\r\n\r\nIteration goes over batches of data, not individual entries, so the main thing is that you'll want both `uproot.iterate` iterators to give you the same number of entries in every batch. You can do that by passing an integer to `step_size` ([see documentation](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html)). The default is `step_size=\"100 MB\"`, which might be different numbers of entries in different files, but `step_size=10000` is either 10 thousand entries or all the remaining entries in a file (if fewer).\r\n\r\nHere's an example that uses very small `step_size` for illustration. uproot-Zmumu-zlib.root and uproot-Zmumu-lzma.root contain the same data compressed differently, so we should see the same numbers in each pair.\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> left_iterator = uproot.iterate(\r\n...     skhep_testdata.data_path(\"uproot-Zmumu-zlib.root\") + \":events\",\r\n...     filter_names=[\"p[xyz][12]\", \"E[12]\"],\r\n...     step_size=1000,\r\n... )\r\n>>> right_iterator = uproot.iterate(\r\n...     skhep_testdata.data_path(\"uproot-Zmumu-lzma.root\") + \":events\",\r\n...     filter_names=[\"p[xyz][12]\", \"E[12]\"],\r\n...     step_size=1000,\r\n... )\r\n>>> for left, right in zip(left_iterator, right_iterator):\r\n...     print(repr(left.px1))\r\n...     print(repr(right.px1))\r\n...     print()\r\n... \r\n<Array [-41.2, 35.1, 35.1, ... -5.52, -26.7] type='1000 * float64'>\r\n<Array [-41.2, 35.1, 35.1, ... -5.52, -26.7] type='1000 * float64'>\r\n\r\n<Array [26, 26, 26, ... -39.2, -39.2, 32.3] type='1000 * float64'>\r\n<Array [26, 26, 26, ... -39.2, -39.2, 32.3] type='1000 * float64'>\r\n\r\n<Array [-43.4, -43.4, -43.2, ... 32.4, 32.5] type='304 * float64'>\r\n<Array [-43.4, -43.4, -43.2, ... 32.4, 32.5] type='304 * float64'>\r\n```\r\n\r\nIf you use [uproot.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html) to iterate over many files, rather than [uproot.TTree.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#iterate) to iterate over one TTree, you'll need to be careful that the files themselves align. If you use a wildcard (which goes to Python's [glob.glob](https://docs.python.org/3/library/glob.html)), you're at the mercy of whatever order the filesystem decides to give you the files. I personally wouldn't trust that; I'd either generate two explicit lists (evaluate [glob.glob](https://docs.python.org/3/library/glob.html) explicitly) and sort them, or generate one set of filenames from the other. If you pass explicit lists of strings (no wildcards) to [uproot.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html), it will use them in the order you request. However, you can also just do your own loop over files and call [uproot.TTree.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#iterate) on each one. Be sure to put the [uproot.open](https://uproot.readthedocs.io/en/latest/uproot.reading.open.html) calls in a `with` statement so that you don't end up with a bunch of open file handles.",
     "createdAt":"2022-01-20T18:13:01Z",
     "number":2008799,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"aphecetche"
        },
        "body":"Thanks a lot for the detailed answer !",
        "createdAt":"2022-01-21T11:21:35Z",
        "number":2014351
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-20T17:30:36Z",
  "number":545,
  "title":"Iterate over two trees (with same entries numbering) over many files ?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/545"
 },
 {
  "author":{
   "login":"asnaylor"
  },
  "body":"Hi,\r\nI'm trying to read variables inside a vector of structs which is inside a vector of structs, here's the simplified c++ header of the ROOT file:\r\n```c++\r\nstruct stepInfo\r\n{\r\n    Int_t StepNumber;\r\n    std::string VolumeName;\r\n    Double_t Position_mm[3];\r\n    Double_t ParticleEnergy_keV;\r\n};\r\nstruct trackInfo\r\n{\r\n    std::string ParticleName;\r\n    Int_t TrackID;\r\n    std::vector<stepInfo> steps;\r\n};\r\n\r\nstd::vector<trackInfo> tracks; // TBranch\r\n```\r\nUproot can load `tracks.ParticleName` into an Awkward Array fine but when you try to load `tracks.steps` you get:\r\n```python\r\nNotImplementedError: memberwise serialization of AsArray\r\n```\r\nI have seen there is an open issue https://github.com/scikit-hep/uproot4/issues/38 about ROOT memberwise splitting but i wondered if i can force the interpretation, tell uproot what the structure was e.g.:\r\n```python\r\ninterpretation = uproot.AsDtype(....)\r\narray = uproot_tree['tracks.steps'].array(interpretation)\r\n```\r\n\r\nIs this possible? If so how do you actually go about doing this?\r\n\r\nThanks",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"As is intended by the NotImplementedError message, this is an unsolved problem. It's not just a matter of Uproot failing to recognize the interpretation, in which case I could give you a hand-constructed `Interpretation` that you could apply. It's that we don't have any algorithms written in Python that will deserialize data in that form. (Also, it's not just the fact that two structs and a `std::vector` are nested: ROOT can serialize such objects without the \"memberwise\" option being turned on, and Uproot can read that data. It's this \"memberwise\" switch that we haven't figured out.)\r\n\r\nHere's the central issue on the problem: https://github.com/scikit-hep/uproot4/issues/38\r\n\r\nHere is a first step, in which @kratsg has implemented part of it: https://github.com/scikit-hep/uproot4/pull/298\r\n\r\nAnd here are two unfinished drafts to go further with it: https://github.com/scikit-hep/uproot4/pull/314 and https://github.com/scikit-hep/uproot4/pull/318.\r\n\r\nIf you want to look into this, you can see the raw bytes in one entry (entry `i`) by:\r\n\r\n```python\r\nuproot_tree['tracks.steps'].debug(i)\r\nuproot_tree['tracks.steps'].debug_array(i)\r\n```\r\n\r\nThey have docstrings that are also available online [here](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#debug) and [here](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#debug-array). The first is for printing out the bytes in a way that may be helpful for debugging, and the second returns the data as an uninterpreted NumPy array for programmatic access.\r\n\r\nFrom what you say about being able to read `particleName`, it looks like the TBranches are split, so the TBranch `tracks.steps` would _only_ be the step values. The general pattern with memberwise splitting is that it would put all the \"StepNumber\" values first, then all the \"VolumeName\", then all the \"Position_mm\", then all the \"ParticleEnergy_keV\" (whereas non-memberwise would have one \"StepNumber\", followed by one \"VolumeName\", one \"Position_mm\", and one \"ParticleEnergy_keV\" before moving on to the next \"StepNumber\", etc.). The `std::vector` would also have some header to indicate how many stepInfos there are. Everything is big-endian, so if you're interpreting them with Python's [struct.unpack](https://docs.python.org/3/library/struct.html), a `>` would be needed in the format string.\r\n\r\nThat's all the information that I can give you right now. Also, if you post your ROOT file here, someone might be able to help you.",
     "createdAt":"2022-01-22T21:24:31Z",
     "number":2024934,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"asnaylor"
        },
        "body":"Thanks for the quick response. Ah right, so the way/order the variables are stored is different. My experiment has moved onto a more uproot friendly ROOT format, it's just the old files that are like this so I'll just use pyROOT to pull out the data of those files. ",
        "createdAt":"2022-01-22T23:25:00Z",
        "number":2025379
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"That's a great idea, switching to PyROOT for the older files, as that certainly works. The development effort needed for this would make a lot more sense if it were addressing a new or upcoming data format.\r\n\r\nThere seems to be a trend now toward simpler data. CMS's NanoAOD is getting a lot of attention, and ATLAS has had a few similar projects going for a while now. It's interesting to see particles measured in keV getting the same treatment!",
        "createdAt":"2022-01-24T22:05:00Z",
        "number":2038270
       },
       {
        "author":{
         "login":"kratsg"
        },
        "body":"Yes, like @jpivarski said, this is how it works. I think we learned this clearest from the `std::pair` example, because we learned that the first thing you need to do is make a \"shell\" of all the various objects in the event, and then read each member one by one. So the two different ways are:\r\n\r\nnon-memberwise\r\n\r\n```\r\n[[Jet0.X, Jet0.Y, Jet0.Z, Jet0.M, ...], [Jet1], ... [JetN]]\r\n```\r\n\r\nversus memberwise\r\n\r\n```\r\n[[Jet0.X, Jet1.X, Jet2.X, ..., JetN.X], [Jet0.Y, ..., JetN,Y], [Jet.Z], ....]\r\n```\r\n\r\nYou can almost think of it as the transpose in a way.",
        "createdAt":"2022-01-24T23:40:25Z",
        "number":2038759
       }
      ],
      "totalCount":3
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-22T14:12:41Z",
  "number":549,
  "title":"Reading a std::vector<struct> inside another std::vector<struct>",
  "url":"https://github.com/scikit-hep/uproot5/discussions/549"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi, \r\n\r\nI am running into problems trying to process data from a root file with `uproot` where the branches read are differently jagged. For example, i want to read `leptons_pt` branch and `jets_pt` branch. I need to extract these branches from many files, then join them together so that i have one data structure with the kinematics for all events. I can read the data through awkward, but then I am unable to join awkward arrays from different files on event index (I raised a discussion on the `awkward` github page). To be clearer, if I have an awkard array from one file \r\n```\r\narray1 = {'leptons_pt'=[[1,2,3], [4,5]], 'jets_pt', [[7,8,9,10],[11]] } # from file1\r\narray2 = {'leptons_pt'=[[12,13], [14]], 'jets_pt', [[15,16],[17,18]] } # from file2\r\n```\r\nI am unable to merge them such that I get \r\n```\r\narray 3 = {'leptons_pt'=[[1,2,3], [4,5], [12,13],[14]], 'jets_pt', [[7,8,9,10],[11],[15,16],[17,18]] }\r\n```\r\n\r\nI have tried to get around this problem by reading data with `pandas` then using `pd.concat` or similar functions, but I wasn't able to find a way to get a single `pandas` dataframe to encapsulate this hierarchial structure with varied indexes. Even if there was a way, I would think that it would involve generating a new index in a multi-index dataframe for each branch -- this would be both redundant and difficult to use since it is hard to keep track of which column follows which index. I would love to be corrected on that If I have a misconception. \r\n\r\nOne solution is to build a `pandas` dataframe with column elements being arrays -- this way the dataframe will only worry about the event index. To achieve this I tried the following:\r\n```\r\ndata = uproot.concatenate(file, branches, library='ak')\r\ndf = pd.DataFrame(columns =list(data.fields) )\r\nfor i, field in enumerate(data.fields):\r\n   df[field] = data[field].tolist()\r\n```\r\nbut the conversion of an awkward array to list seems to be incredibly slow -- O(1min) per branch for 0.5million events for a simple branch with shape `(nevents,2)`. Is it normal for the conversion to take this long? This also makes me wonder if `uproot` can output the data as a list of lists directly as one of the options? \r\n\r\nIt's worth noting that when this code is used, which branches will be extracted from the root file is something that varies.\r\n\r\nDo you have any suggestions on how to better handle a situation like this without sacrificing performance? \r\n\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"What you would get out of files 1 and 2, if you are only using [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) like\r\n\r\n```python\r\nwith uproot.open(\"file1.root:tree_name\") as tree:\r\n    array1 = tree.arrays(filter_name=[\"leptons_pt\", \"jets_pt\"])\r\nwith uproot.open(\"file2.root:tree_name\") as tree:\r\n    array2 = tree.arrays(filter_name=[\"leptons_pt\", \"jets_pt\"])\r\n```\r\n\r\nis\r\n\r\n```python\r\n>>> array1 = ak.Array([{\"leptons_pt\": [1, 2, 3], \"jets_pt\": [7, 8, 9, 10]}, {\"leptons_pt\": [4, 5], \"jets_pt\": [11]}])\r\n>>> array2 = ak.Array([{\"leptons_pt\": [12, 13], \"jets_pt\": [15, 16]}, {\"leptons_pt\": [14], \"jets_pt\": [17, 18]}])\r\n```\r\n\r\nThat is, [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) makes an Awkward Array with a field for each requested TBranch name. You can project-out fields from an array to look at them individually:\r\n\r\n```python\r\n>>> array1.leptons_pt\r\n<Array [[1, 2, 3], [4, 5]] type='2 * var * int64'>\r\n>>> array1.jets_pt\r\n<Array [[7, 8, 9, 10], [11]] type='2 * var * int64'>\r\n>>> array2.leptons_pt\r\n<Array [[12, 13], [14]] type='2 * var * int64'>\r\n>>> array2.jets_pt\r\n<Array [[15, 16], [17, 18]] type='2 * var * int64'>\r\n```\r\n\r\nThen, to get `array3`, you [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) them:\r\n\r\n```python\r\n>>> array3 = ak.concatenate([array1, array2])\r\n```\r\n\r\nwhich looks like\r\n\r\n```python\r\n>>> array3.tolist()\r\n[\r\n    {'leptons_pt': [1, 2, 3], 'jets_pt': [7, 8, 9, 10]},\r\n    {'leptons_pt': [4, 5], 'jets_pt': [11]},\r\n    {'leptons_pt': [12, 13], 'jets_pt': [15, 16]},\r\n    {'leptons_pt': [14], 'jets_pt': [17, 18]},\r\n]\r\n```\r\n\r\nor\r\n\r\n```python\r\n>>> array3.leptons_pt\r\n<Array [[1, 2, 3], [4, 5], [12, 13], [14]] type='4 * var * int64'>\r\n>>> array3.jets_pt\r\n<Array [[7, 8, 9, 10], ... 16], [17, 18]] type='4 * var * int64'>\r\n```\r\n\r\nBut that's also what [uproot.concatenate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.concatenate.html) does. One call to\r\n\r\n```python\r\narray3 = uproot.concatenate([\"file1.root:tree_name\", \"file2.root:tree_name\"])\r\n```\r\n\r\nand you should be done. I suspect that your troubles with concatenation are solvable: see my answer to your question on scikit-hep/awkward-1.0/discussions/1251.\r\n\r\n----------------------\r\n\r\nConversion to Pandas is often the slowest link, though I just re-read your question and you've actually hit a bottleneck in `tolist`. I'll get to that later. I wouldn't use Pandas just to concatenate here\u2014it's a lot of overhead to convert into and out of it for functionality that is already available in the array form. Worse still, Pandas's data model doesn't allow for columns in the same DataFrame to have different MultiIndexes, so everything lepton-related would have to be in different DataFrames from everything jet-related, which further complicates this potential solution.\r\n\r\n-------------------------\r\n\r\nAbout getting data directly from Uproot that is already lists, this is approximately what `library=\"np\"` does. The NumPy backend doesn't touch anything from Awkward Array or Pandas (which is why Uproot has NumPy as its own _strict_ dependency) and if you have jagged arrays, it returns a NumPy `dtype=\"O\"` array of NumPy arrays.\r\n\r\nThat is, instead of this:\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> tree = uproot.open(skhep_testdata.data_path(\"uproot-HZZ.root\"))[\"events\"]\r\n>>> tree[\"Muon_Px\"].array()\r\n<Array [[-52.9, 37.7], ... 1.14], [23.9]] type='2421 * var * float32'>\r\n```\r\n\r\nyou get this:\r\n\r\n```python\r\n>>> tree[\"Muon_Px\"].array(library=\"np\")\r\narray([array([-52.899456,  37.73778 ], dtype=float32),\r\n       array([-0.81645936], dtype=float32),\r\n       array([48.98783  ,  0.8275667], dtype=float32), ...,\r\n       array([-29.756786], dtype=float32),\r\n       array([1.1418698], dtype=float32),\r\n       array([23.913206], dtype=float32)], dtype=object)\r\n```\r\n\r\nThis NumPy `dtype` actually just stores pointers to Python objects, so any Python data type can be stored in it.\r\n\r\n```python\r\n>>> np.array([1, [1, 2, 3], \"hello\", tree], dtype=\"O\")\r\narray([1, list([1, 2, 3]), 'hello',\r\n       <TTree 'events' (51 branches) at 0x7f5a6f6500d0>], dtype=object)\r\n```\r\n\r\nIn other words, this container type is essentially a [generic Sequence](https://docs.python.org/3/library/stdtypes.html#sequence-types-list-tuple-range): it can be iterated over, accessed by index, changed in place, etc. The only thing that Python lists have and NumPy arrays with `dtype=\"O\"` lack is the ability to append/extend. But, being a _generic_ sequence and not an array-like block of memory, arrays with `dtype=\"O\"` also lack NumPy's performance and advanced slicing features\u2014NumPy doesn't know what types of objects such an array contains, so it can't do any deep manipulations.\r\n\r\n```python\r\n>>> as_awkward = tree[\"Muon_Px\"].array(library=\"ak\")\r\n>>> as_awkward[as_awkward > 10]\r\n<Array [[37.7], [], [49], ... [], [], [23.9]] type='2421 * var * float32'>\r\n```\r\n\r\n```python\r\n>>> as_numpy = tree[\"Muon_Px\"].array(library=\"np\")\r\n>>> as_numpy[as_numpy > 10]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n```\r\n\r\n----------------\r\n\r\n\r\nOkay, about `tolist` being slow. The general design philosophy of Awkward Array is that arrays are large, precompiled operations on large memory blocks are fast, and Python is slow. When you ask `ak.this` or `ak.that` to do something, it can do a considerable amount of work setting up to call one precompiled function because the setup time is independent of the size of the array (i.e. time complexity _O(1)_) and the precompiled function depends on the size of the array (time complexity _O(n)_, where _n_ is the length of the array). That's not the case for an operation that extracts one element, such as `my_big_array[42]`. Here, the fact that we have a large set-up/tear-down time (microseconds or milliseconds) is a killer because this operation doesn't do anything _O(n)_; it's _all_ set-up/tear-down. Iteration is even worse because it's calling `my_big_array[0]`, `my_big_array[1]`, ...\r\n\r\nIn the current version of Awkward Array, `tolist` is implemented through iteration. The idea was that if you're asking for Python objects, then you don't care about performance. However, that's a little too harsh. Although we can't make `my_big_array[42]` much faster (overriding `__getitem__` has a lot of overhead relative to a Python builtin list get-item), `tolist` does not need to be implemented through iteration. In [Awkward version 2.x](https://github.com/scikit-hep/awkward-1.0/discussions/1151), it was reimplemented using a different algorithm (inside-out, rather than outside-in), with a substantial speedup:\r\n\r\n```python\r\nIn [1]: import awkward as ak, numpy as np\r\n\r\nIn [2]: array_v1 = ak.Array(np.random.normal(0, 1, (500000, 2)))\r\n\r\nIn [3]: %timeit as_list = array_v1.tolist()\r\n15 s \u00b1 137 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [4]: array_v2 = ak._v2.Array(np.random.normal(0, 1, (500000, 2)))\r\n\r\nIn [5]: %timeit as_list = array_v2.tolist()\r\n113 ms \u00b1 1.46 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\nMy computer is apparently faster than yours (15 seconds instead of 1 minute for half a million pairs), but the next major version will make this `tolist` case 130\u00d7 faster.",
     "createdAt":"2022-01-25T22:25:08Z",
     "number":2046764,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-01-25T17:27:03Z",
  "number":550,
  "title":"Reading branches with varying jaggedness",
  "url":"https://github.com/scikit-hep/uproot5/discussions/550"
 },
 {
  "author":{
   "login":"tamasgal"
  },
  "body":"This is related to an older discussion [1] funnily from one of my colleagues `;)`\r\n\r\nI ran into the same issue while porting code from `uproot3` to `uproot4`. In `uproot3` we used the `.lazyarray()` implementation to iterate through very large branches and were able to pass our custom interpretations where `uproot3` could not figure those out. `uproot4` is also failing for a couple of branches but does a much better job in general. Unfortunately for one of our largest branches it still fails, so we need to help out with a custom interpretation.\r\n\r\nAs you mentioned in the discussion[2], the `.iterate()` implementation in `uproot4` is fairly complicated and by that time it did not support custom interpretations. I found in the current docs[2] that there is an `interpretation_executor` parameter:\r\n\r\n> [interpretation_executor](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyFile.html#uproot-reading-readonlyfile-interpretation-executor) (None or Executor with a submit method) \u2013 The executor that is used to interpret uncompressed TBasket data as arrays; if None, the file\u2019s interpretation_executor is used.\r\n\r\nI looked at the code but could not figure out how to create such an executor. I found `TrivialExecutor` but not much docs, so I thought I'll ask first before I spend hours of hacking or until I figure out that it's a dead end `;)`\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/53c5a99e3f5e428769542e140b1d0c76a4997fa9/src/uproot/source/futures.py#L58\r\n\r\nAnyways, I thought that this is something new and there is a mechanism now to hook into the interpretation now but then I saw via `git blame` that `interpretation_executor` was already present when the discussion[2] was ongoing `;)` which might be this aforementioned \"dead-end\".\r\n\r\nSo my question is: is there a preferred way to teach `uproot` upstreams how to interpret something (via the class name) so that it automagically knows which interpretation to pick when access data? I was going to dive into the sources but the mechanics are different to `uproot3` and I am sure it's much more effective if you toss me in the right direction.\r\n\r\nBelow is the full session with one of our publicly available test files, which shows all the streamers and also demonstrates that the custom interpretation passed to `.array()` is working nicely.\r\n\r\nIn this particular case, the streamer interpretation of `KM3NETDAQ::JDAQSummaryFrame` is missing but `uproot4` parsed correctly that the corresponding branch is a `vector<KM3NETDAQ::JDAQSummaryFrame>`.\r\n\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'4.1.9'\r\n>>> from km3net_testdata import data_path\r\n>>> f = uproot.open(data_path(\"online/km3net_online.root\"))\r\n>>> f.file.show_streamers()\r\nEvt (v9): AAObject (v1)\r\n    id: int (TStreamerBasicType)\r\n    det_id: int (TStreamerBasicType)\r\n    mc_id: int (TStreamerBasicType)\r\n    run_id: int (TStreamerBasicType)\r\n    mc_run_id: int (TStreamerBasicType)\r\n    frame_index: int (TStreamerBasicType)\r\n    trigger_mask: unsigned long long (TStreamerBasicType)\r\n    trigger_counter: unsigned long long (TStreamerBasicType)\r\n    overlays: unsigned int (TStreamerBasicType)\r\n    t: TTimeStamp (TStreamerObjectAny)\r\n    hits: vector<Hit> (TStreamerSTL)\r\n    trks: vector<Trk> (TStreamerSTL)\r\n    w: vector<double> (TStreamerSTL)\r\n    w2list: vector<double> (TStreamerSTL)\r\n    w3list: vector<double> (TStreamerSTL)\r\n    mc_t: double (TStreamerBasicType)\r\n    mc_hits: vector<Hit> (TStreamerSTL)\r\n    mc_trks: vector<Trk> (TStreamerSTL)\r\n    comment: TString (TStreamerString)\r\n    index: int (TStreamerBasicType)\r\n    flags: int (TStreamerBasicType)\r\n\r\nAAObject (v1): TObject (v1)\r\n    usr: vector<double> (TStreamerSTL)\r\n    usr_names: vector<string> (TStreamerSTL)\r\n\r\nTObject (v1)\r\n    fUniqueID: unsigned int (TStreamerBasicType)\r\n    fBits: unsigned int (TStreamerBasicType)\r\n\r\nTTimeStamp (v1)\r\n    fSec: int (TStreamerBasicType)\r\n    fNanoSec: int (TStreamerBasicType)\r\n\r\nHit (v102): AAObject (v1)\r\n    id: int (TStreamerBasicType)\r\n    dom_id: int (TStreamerBasicType)\r\n    channel_id: unsigned int (TStreamerBasicType)\r\n    tdc: unsigned int (TStreamerBasicType)\r\n    tot: unsigned int (TStreamerBasicType)\r\n    trig: int (TStreamerBasicType)\r\n    pmt_id: int (TStreamerBasicType)\r\n    t: double (TStreamerBasicType)\r\n    a: double (TStreamerBasicType)\r\n    pos: Vec (TStreamerObjectAny)\r\n    dir: Vec (TStreamerObjectAny)\r\n    pure_t: double (TStreamerBasicType)\r\n    pure_a: double (TStreamerBasicType)\r\n    type: int (TStreamerBasicType)\r\n    origin: int (TStreamerBasicType)\r\n    pattern_flags: unsigned int (TStreamerBasicType)\r\n\r\nVec (v3)\r\n    x: double (TStreamerBasicType)\r\n    y: double (TStreamerBasicType)\r\n    z: double (TStreamerBasicType)\r\n\r\nTrk (v6): AAObject (v1)\r\n    id: int (TStreamerBasicType)\r\n    pos: Vec (TStreamerObjectAny)\r\n    dir: Vec (TStreamerObjectAny)\r\n    t: double (TStreamerBasicType)\r\n    E: double (TStreamerBasicType)\r\n    len: double (TStreamerBasicType)\r\n    lik: double (TStreamerBasicType)\r\n    type: int (TStreamerBasicType)\r\n    rec_type: int (TStreamerBasicType)\r\n    rec_stages: vector<int> (TStreamerSTL)\r\n    fitinf: vector<double> (TStreamerSTL)\r\n    hit_ids: vector<int> (TStreamerSTL)\r\n    error_matrix: vector<double> (TStreamerSTL)\r\n    comment: string (TStreamerSTLstring)\r\n\r\nKM3NETDAQ::JDAQTimeslice (v4): KM3NETDAQ::JDAQPreamble (v1), KM3NETDAQ::JDAQTimesliceHeader (v2)\r\n    vector<KM3NETDAQ::JDAQSuperFrame>: vector<KM3NETDAQ::JDAQSuperFrame> (TStreamerSTL)\r\n\r\nKM3NETDAQ::JDAQPreamble (v1): KM3NETDAQ::JDAQAbstractPreamble (v1), TObject (v1)\r\n\r\nKM3NETDAQ::JDAQAbstractPreamble (v1)\r\n    length: int (TStreamerBasicType)\r\n    type: int (TStreamerBasicType)\r\n\r\nKM3NETDAQ::JDAQTimesliceHeader (v2): KM3NETDAQ::JDAQHeader (v2)\r\n\r\nKM3NETDAQ::JDAQHeader (v2): KM3NETDAQ::JDAQChronometer (v3)\r\n\r\nKM3NETDAQ::JDAQChronometer (v3)\r\n    detector_id: int (TStreamerBasicType)\r\n    run: int (TStreamerBasicType)\r\n    frame_index: int (TStreamerBasicType)\r\n    timeslice_start: KM3NETDAQ::JDAQUTCExtended (TStreamerObjectAny)\r\n\r\nKM3NETDAQ::JDAQTimesliceL0 (v1): KM3NETDAQ::JDAQTimeslice (v4)\r\n\r\nKM3NETDAQ::JDAQTimesliceL1 (v1): KM3NETDAQ::JDAQTimeslice (v4)\r\n\r\nKM3NETDAQ::JDAQUTCExtended (v1)\r\n    UTC_seconds: KM3NETDAQ::JDAQUTCExtended::JUINT32_t (TStreamerBasicType)\r\n    UTC_16nanosecondcycles: KM3NETDAQ::JDAQUTCExtended::JUINT32_t (TStreamerBasicType)\r\n\r\nKM3NETDAQ::JDAQSuperFrame (v3): KM3NETDAQ::JDAQPreamble (v1), KM3NETDAQ::JDAQSuperFrameHeader (v2), KM3NETDAQ::JDAQFrame (v1)\r\n\r\nKM3NETDAQ::JDAQSuperFrameHeader (v2): KM3NETDAQ::JDAQHeader (v2), KM3NETDAQ::JDAQModuleIdentifier (v1), KM3NETDAQ::JDAQFrameStatus (v1)\r\n\r\nKM3NETDAQ::JDAQModuleIdentifier (v1)\r\n    id: int (TStreamerBasicType)\r\n\r\nKM3NETDAQ::JDAQFrameStatus (v1)\r\n    daq: int (TStreamerBasicType)\r\n    status: int (TStreamerBasicType)\r\n    fifo: int (TStreamerBasicType)\r\n    status_3: int (TStreamerBasicType)\r\n    status_4: int (TStreamerBasicType)\r\n\r\nKM3NETDAQ::JDAQFrame (v1)\r\n    numberOfHits: int (TStreamerBasicType)\r\n    buffer: KM3NETDAQ::JDAQHit* (TStreamerLoop)\r\n\r\nKM3NETDAQ::JDAQTimesliceL2 (v1): KM3NETDAQ::JDAQTimeslice (v4)\r\n\r\nKM3NETDAQ::JDAQTimesliceSN (v1): KM3NETDAQ::JDAQTimeslice (v4)\r\n\r\nKM3NETDAQ::JDAQEvent (v4): KM3NETDAQ::JDAQPreamble (v1), KM3NETDAQ::JDAQEventHeader (v3)\r\n    triggeredHits: vector<JDAQTriggeredHit> (TStreamerSTL)\r\n    snapshotHits: vector<JDAQSnapshotHit> (TStreamerSTL)\r\n\r\nKM3NETDAQ::JDAQEventHeader (v3): KM3NETDAQ::JDAQHeader (v2), KM3NETDAQ::JDAQTriggerCounter (v1), KM3NETDAQ::JDAQTriggerMask (v1)\r\n    overlays: unsigned int (TStreamerBasicType)\r\n\r\nKM3NETDAQ::JDAQTriggerCounter (v1)\r\n    trigger_counter: KM3NETDAQ::JTriggerCounter_t (TStreamerBasicType)\r\n\r\nKM3NETDAQ::JDAQTriggerMask (v1)\r\n    trigger_mask: KM3NETDAQ::JTriggerMask_t (TStreamerBasicType)\r\n\r\nKM3NETDAQ::JDAQSummaryslice (v6): KM3NETDAQ::JDAQPreamble (v1), KM3NETDAQ::JDAQSummarysliceHeader (v2)\r\n    vector<KM3NETDAQ::JDAQSummaryFrame>: vector<KM3NETDAQ::JDAQSummaryFrame> (TStreamerSTL)\r\n\r\nKM3NETDAQ::JDAQSummarysliceHeader (v2): KM3NETDAQ::JDAQHeader (v2)\r\n\r\nTNamed (v1): TObject (v1)\r\n    fName: TString (TStreamerString)\r\n    fTitle: TString (TStreamerString)\r\n\r\nJTRIGGER::JTriggerParameters (v11): TObject (v1)\r\n    trigger3DShower: JTRIGGER::JTrigger3DShower_t::JParameters (TStreamerObjectAny)\r\n    triggerMXShower: JTRIGGER::JTriggerMXShower_t::JParameters (TStreamerObjectAny)\r\n    trigger3DMuon: JTRIGGER::JTrigger3DMuon_t::JParameters (TStreamerObjectAny)\r\n    triggerNB: JTRIGGER::JTriggerNB_t::JParameters (TStreamerObjectAny)\r\n    highRateVeto_Hz: double (TStreamerBasicType)\r\n    L2Min: int (TStreamerBasicType)\r\n    ctMin: double (TStreamerBasicType)\r\n    TMaxLocal_ns: double (TStreamerBasicType)\r\n    TMaxEvent_ns: double (TStreamerBasicType)\r\n    numberOfBins: int (TStreamerBasicType)\r\n    combineL1: bool (TStreamerBasicType)\r\n    L2: JTRIGGER::JL2Parameters (TStreamerObject)\r\n    SN: JTRIGGER::JL2Parameters (TStreamerObject)\r\n    writeTimeslices: JTRIGGER::JPrescaler (TStreamerObject)\r\n    writeSummary: JTRIGGER::JPrescaler (TStreamerObject)\r\n    writeL0: JTRIGGER::JPrescaler (TStreamerObject)\r\n    writeL1: JTRIGGER::JPrescaler (TStreamerObject)\r\n    writeL2: JTRIGGER::JPrescaler (TStreamerObject)\r\n    writeSN: JTRIGGER::JPrescaler (TStreamerObject)\r\n\r\nJTRIGGER::JTrigger3DShower_t::JParameters (v3)\r\n    enabled: bool (TStreamerBasicType)\r\n    numberOfHits: int (TStreamerBasicType)\r\n    numberOfModules: int (TStreamerBasicType)\r\n    DMax_m: double (TStreamerBasicType)\r\n    TMaxExtra_ns: double (TStreamerBasicType)\r\n    factoryLimit: int (TStreamerBasicType)\r\n\r\nJTRIGGER::JTriggerMXShower_t::JParameters (v3)\r\n    enabled: bool (TStreamerBasicType)\r\n    numberOfHits: int (TStreamerBasicType)\r\n    numberOfModules: int (TStreamerBasicType)\r\n    DMax_m: double (TStreamerBasicType)\r\n    TMaxExtra_ns: double (TStreamerBasicType)\r\n    factoryLimit: int (TStreamerBasicType)\r\n\r\nJTRIGGER::JTrigger3DMuon_t::JParameters (v3)\r\n    enabled: bool (TStreamerBasicType)\r\n    numberOfHits: int (TStreamerBasicType)\r\n    numberOfModules: int (TStreamerBasicType)\r\n    DMax_m: double (TStreamerBasicType)\r\n    roadWidth_m: double (TStreamerBasicType)\r\n    gridAngle_deg: double (TStreamerBasicType)\r\n    TMaxExtra_ns: double (TStreamerBasicType)\r\n    factoryLimit: int (TStreamerBasicType)\r\n\r\nJTRIGGER::JTriggerNB_t::JParameters (v10)\r\n    enabled: bool (TStreamerBasicType)\r\n    pmts: JTRIGGER::JPMTSelector (TStreamerObject)\r\n    DMax_m: double (TStreamerBasicType)\r\n\r\nJTRIGGER::JPMTSelector (v1): TObject (v1)\r\n    vector<JTRIGGER::JPMTIdentifier_t>: vector<JTRIGGER::JPMTIdentifier_t> (TStreamerSTL)\r\n\r\nJTRIGGER::JPMTIdentifier_t (v8): TObject (v1)\r\n    moduleID: int (TStreamerBasicType)\r\n    pmtAddress: int (TStreamerBasicType)\r\n\r\nJTRIGGER::JL2Parameters (v1): TObject (v1)\r\n    numberOfHits: int (TStreamerBasicType)\r\n    TMaxLocal_ns: double (TStreamerBasicType)\r\n    ctMin: double (TStreamerBasicType)\r\n\r\nJTRIGGER::JPrescaler (v2): TObject (v1)\r\n    prescale: long long (TStreamerBasicType)\r\n    counter: long long (TStreamerBasicType)\r\n\r\nTTree (v19): TNamed (v1), TAttLine (v2), TAttFill (v2), TAttMarker (v2)\r\n    fEntries: long long (TStreamerBasicType)\r\n    fTotBytes: long long (TStreamerBasicType)\r\n    fZipBytes: long long (TStreamerBasicType)\r\n    fSavedBytes: long long (TStreamerBasicType)\r\n    fFlushedBytes: long long (TStreamerBasicType)\r\n    fWeight: double (TStreamerBasicType)\r\n    fTimerInterval: int (TStreamerBasicType)\r\n    fScanField: int (TStreamerBasicType)\r\n    fUpdate: int (TStreamerBasicType)\r\n    fDefaultEntryOffsetLen: int (TStreamerBasicType)\r\n    fNClusterRange: int (TStreamerBasicType)\r\n    fMaxEntries: long long (TStreamerBasicType)\r\n    fMaxEntryLoop: long long (TStreamerBasicType)\r\n    fMaxVirtualSize: long long (TStreamerBasicType)\r\n    fAutoSave: long long (TStreamerBasicType)\r\n    fAutoFlush: long long (TStreamerBasicType)\r\n    fEstimate: long long (TStreamerBasicType)\r\n    fClusterRangeEnd: long long* (TStreamerBasicPointer)\r\n    fClusterSize: long long* (TStreamerBasicPointer)\r\n    fBranches: TObjArray (TStreamerObject)\r\n    fLeaves: TObjArray (TStreamerObject)\r\n    fAliases: TList* (TStreamerObjectPointer)\r\n    fIndexValues: TArrayD (TStreamerObjectAny)\r\n    fIndex: TArrayI (TStreamerObjectAny)\r\n    fTreeIndex: TVirtualIndex* (TStreamerObjectPointer)\r\n    fFriends: TList* (TStreamerObjectPointer)\r\n    fUserInfo: TList* (TStreamerObjectPointer)\r\n    fBranchRef: TBranchRef* (TStreamerObjectPointer)\r\n\r\nTAttLine (v2)\r\n    fLineColor: short (TStreamerBasicType)\r\n    fLineStyle: short (TStreamerBasicType)\r\n    fLineWidth: short (TStreamerBasicType)\r\n\r\nTAttFill (v2)\r\n    fFillColor: short (TStreamerBasicType)\r\n    fFillStyle: short (TStreamerBasicType)\r\n\r\nTAttMarker (v2)\r\n    fMarkerColor: short (TStreamerBasicType)\r\n    fMarkerStyle: short (TStreamerBasicType)\r\n    fMarkerSize: float (TStreamerBasicType)\r\n\r\nTBranchElement (v9): TBranch (v12)\r\n    fClassName: TString (TStreamerString)\r\n    fParentName: TString (TStreamerString)\r\n    fClonesName: TString (TStreamerString)\r\n    fCheckSum: unsigned int (TStreamerBasicType)\r\n    fClassVersion: int (TStreamerBasicType)\r\n    fID: int (TStreamerBasicType)\r\n    fType: int (TStreamerBasicType)\r\n    fStreamerType: int (TStreamerBasicType)\r\n    fMaximum: int (TStreamerBasicType)\r\n    fBranchCount: TBranchElement* (TStreamerObjectPointer)\r\n    fBranchCount2: TBranchElement* (TStreamerObjectPointer)\r\n\r\nTBranch (v12): TNamed (v1), TAttFill (v2)\r\n    fCompress: int (TStreamerBasicType)\r\n    fBasketSize: int (TStreamerBasicType)\r\n    fEntryOffsetLen: int (TStreamerBasicType)\r\n    fWriteBasket: int (TStreamerBasicType)\r\n    fEntryNumber: long long (TStreamerBasicType)\r\n    fOffset: int (TStreamerBasicType)\r\n    fMaxBaskets: int (TStreamerBasicType)\r\n    fSplitLevel: int (TStreamerBasicType)\r\n    fEntries: long long (TStreamerBasicType)\r\n    fFirstEntry: long long (TStreamerBasicType)\r\n    fTotBytes: long long (TStreamerBasicType)\r\n    fZipBytes: long long (TStreamerBasicType)\r\n    fBranches: TObjArray (TStreamerObject)\r\n    fLeaves: TObjArray (TStreamerObject)\r\n    fBaskets: TObjArray (TStreamerObject)\r\n    fBasketBytes: int* (TStreamerBasicPointer)\r\n    fBasketEntry: long long* (TStreamerBasicPointer)\r\n    fBasketSeek: long long* (TStreamerBasicPointer)\r\n    fFileName: TString (TStreamerString)\r\n\r\nTLeafElement (v1): TLeaf (v2)\r\n    fID: int (TStreamerBasicType)\r\n    fType: int (TStreamerBasicType)\r\n\r\nTLeaf (v2): TNamed (v1)\r\n    fLen: int (TStreamerBasicType)\r\n    fLenType: int (TStreamerBasicType)\r\n    fOffset: int (TStreamerBasicType)\r\n    fIsRange: bool (TStreamerBasicType)\r\n    fIsUnsigned: bool (TStreamerBasicType)\r\n    fLeafCount: TLeaf* (TStreamerObjectPointer)\r\n\r\nKM3NETDAQ::JDAQHit (v1)\r\n\r\nTList (v5): TSeqCollection (v0)\r\n\r\nTSeqCollection (v0): TCollection (v3)\r\n\r\nTCollection (v3): TObject (v1)\r\n    fName: TString (TStreamerString)\r\n    fSize: int (TStreamerBasicType)\r\n\r\nTString (v2)\r\n\r\nTBranchRef (v1): TBranch (v12)\r\n    fRefTable: TRefTable* (TStreamerObjectPointer)\r\n\r\nTRefTable (v3): TObject (v1)\r\n    fSize: int (TStreamerBasicType)\r\n    fParents: TObjArray* (TStreamerObjectPointer)\r\n    fOwner: TObject* (TStreamerObjectPointer)\r\n    fProcessGUIDs: vector<string> (TStreamerSTL)\r\n\r\nTObjArray (v3): TSeqCollection (v0)\r\n    fLowerBound: int (TStreamerBasicType)\r\n    fLast: int (TStreamerBasicType)\r\n>>> f[\"KM3NET_SUMMARYSLICE/KM3NET_SUMMARYSLICE/vector<KM3NETDAQ::JDAQSummaryFrame>\"].array()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/tamasgal/Dev/km3io/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 2095, in array\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/Users/tamasgal/Dev/km3io/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 3521, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/Users/tamasgal/Dev/km3io/venv/lib/python3.9/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/Users/tamasgal/Dev/km3io/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 3465, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/Users/tamasgal/Dev/km3io/venv/lib/python3.9/site-packages/uproot/interpretation/objects.py\", line 141, in basket_array\r\n    form = self.awkward_form(branch.file, index_format=\"i64\")\r\n  File \"/Users/tamasgal/Dev/km3io/venv/lib/python3.9/site-packages/uproot/interpretation/objects.py\", line 121, in awkward_form\r\n    return self._model.awkward_form(\r\n  File \"/Users/tamasgal/Dev/km3io/venv/lib/python3.9/site-packages/uproot/containers.py\", line 792, in awkward_form\r\n    uproot._util.awkward_form(\r\n  File \"/Users/tamasgal/Dev/km3io/venv/lib/python3.9/site-packages/uproot/_util.py\", line 553, in awkward_form\r\n    return model.awkward_form(\r\n  File \"/Users/tamasgal/Dev/km3io/venv/lib/python3.9/site-packages/uproot/model.py\", line 676, in awkward_form\r\n    raise uproot.interpretation.objects.CannotBeAwkward(\r\nuproot.interpretation.objects.CannotBeAwkward: KM3NETDAQ::JDAQSummaryFrame\r\n>>> interpretation = uproot.interpretation.jagged.AsJagged(\r\n...         uproot.interpretation.numerical.AsDtype(\r\n...             [\r\n...                 (\"dom_id\", \">i4\"),\r\n...                 (\"dq_status\", \"u4\"),\r\n...                 (\"hrv\", \"<u4\"),\r\n...                 (\"fifo\", \"<u4\"),\r\n...                 (\"status3\", \"u4\"),\r\n...                 (\"status4\", \"u4\"),\r\n...             ] + [(f\"ch{c}\", \"u1\") for c in range(31)]\r\n...         ), header_bytes=10,\r\n... )\r\n>>> f[\"KM3NET_SUMMARYSLICE/KM3NET_SUMMARYSLICE/vector<KM3NETDAQ::JDAQSummaryFrame>\"].array(interpretation)\r\n<Array [[{dom_id: 806451572, ... ch30: 48}]] type='3 * var * {\"dom_id\": int32, \"...'>\r\n```\r\n\r\n[1] https://github.com/scikit-hep/uproot4/discussions/354\r\n[2] https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html?highlight=iterate#iterate",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I'll go for the easy one first: \"Executor\" is capitalized in the docs because it's this: https://docs.python.org/3/library/concurrent.futures.html#executor-objects\r\n\r\nThere's a way to make Sphinx docs cross-reference, but I've never set it up.\r\n\r\nActually, anything that duck-types with this kind of Executor will work: it has to have a `submit`, `map`, and `shutdown` method, though the last of these can be trivial. In the Uproot codebase, a few executors have been implemented for various reasons, mostly to tie threads and file/remote file handles to one another so they'll be resource-collected at the end of `with uproot.open(...)` blocks. The TrivialExecutor is a dummy for the case of local file reading, which doesn't pay to do asynchronously because the operating system is already tracking your usage and optimistically reading ahead.\r\n\r\nProbably the most common Executor one might use for I/O is the [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor), since the [ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor) would have to shuttle data through a socket after it reads it, which effectively means twice the I/O. I did some early experiments with [ThreadPoolExecutors](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) (see the plot under the [Uproot 3 documentation for this feature](https://github.com/scikit-hep/uproot3#parallel-array-reading)). The only cases where you get much benefit from it are those in which decompression is a computationally intensive task, such as the LZMA in that example. Decompression routines take large blocks of data by pointer into compiled code that releases the GIL. If, instead of decompression, it's `AsObjects` interpretation that's the bottleneck, multithreading doesn't help much (or can hurt). The `AsObjects` interpretation is performed by Python code, so the GIL is a blocker.",
     "createdAt":"2022-02-08T21:12:13Z",
     "number":2136520,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"As for passing down an interpretation, yes it is possible. I wasn't sure that I retained this feature from Uproot 3 because it's so hard to describe, and I guess I didn't document it for that reason, but [if you pass a dict as `expressions`, the keys are branch names to read and the values are interpretations](https://github.com/scikit-hep/uproot4/blob/53c5a99e3f5e428769542e140b1d0c76a4997fa9/src/uproot/behaviors/TBranch.py#L3289-L3336).\r\n\r\nHere's an example that reads `px1` as the big-endian, 64-bit floats that they are, `py1` as integers, and `pz1` as little-endian floats. (Oh, and whatever works for `arrays` works for `iterate`, as they go through the same `_regularize_expressions`.)\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> tree = uproot.open(skhep_testdata.data_path(\"uproot-Zmumu.root\"))[\"events\"]>>> tree.arrays([\"Px\", \"Py\", \"Pz\"])\r\n>>> tree.arrays(\r\n...     {\"px1\": uproot.AsDtype(\">f8\"), \"py1\": uproot.AsDtype(\">i8\"), \"pz1\": uproot.AsDtype(\"<f8\")},\r\n...     entry_stop=1,\r\n... ).tolist()\r\n[{'px1': -41.1952876442, 'py1': 4625600239601887419, 'pz1': -8.641303535866137e-247}]\r\n```\r\n\r\nIf some branches should use default interpretations, passing `None` as the associated value would do that. This feature must wreak havoc on `expressions` that are computed from multiple branches...\r\n\r\n```python\r\n>>> tree.arrays({\"px1 + py1\": uproot.AsDtype(\">f8\")})\r\n...\r\nuproot.exceptions.KeyInFileError: not found: 'px1 + py1'\r\n\r\n    Available keys: 'px1', 'py1', 'pz1', 'pt1', 'px2', 'py2', 'E1', 'phi1', 'Q1', 'pz2', 'pt2', 'M', 'Type',\r\n    'eta1', 'E2', 'phi2', 'Q2', 'Run', 'eta2', 'Event'\r\n\r\nin file /home/jpivarski/.local/skhepdata/uproot-Zmumu.root\r\nin object /events;1\r\n```\r\n\r\nOh, that's nice. If using a dict, the keys _must_ be branch names; computed expressions are not allowed. That's good.\r\n\r\nThis is a difficult feature to explain, including why you'd ever even want it; it would make the documentation even denser than it is. But when you need it, you need it, so I'm glad you asked in a Discussion.",
     "createdAt":"2022-02-08T21:30:23Z",
     "number":2136626,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"tamasgal"
        },
        "body":"Oh wow, that's neat, many thanks Jim! I was almost on the right path at some point but did not figure that out `;)` Indeed this is exactly what I need and it works like a charm.\r\n\r\nJust for the record:\r\n\r\n```python\r\n>>> data = f[\"KM3NET_SUMMARYSLICE/KM3NET_SUMMARYSLICE\"].arrays(\r\n    {\"vector<KM3NETDAQ::JDAQSummaryFrame>\": interpretation}\r\n)\r\n>>> data['vector<KM3NETDAQ::JDAQSummaryFrame>'].dom_id\r\n<Array [[806451572, 806455814, ... 809544061]] type='3 * var * int32'>\r\n```\r\n\r\n> But when you need it, you need it, so I'm glad you asked in a Discussion.\r\n\r\nYes, this is unfortunately needed for most of our Tier-0 DAQ files and I have to port a couple of interpretations, but I am glad that such an easy solution exists. I already have most of the interpretations for the `uproot3` based library, so it will be an easy port. We have not really used this part of the Python I/O library due to the performance drawbacks of the old `uproot3/awkward0` lazy-array thing, which gave us `dtype('O')` arrays, so I am really looking forward to the shiny new `awkward` arrays which we already use successfully in combination with Numba in the higher tier data processing.",
        "createdAt":"2022-02-09T00:02:09Z",
        "number":2137285
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-02-08T11:17:53Z",
  "number":555,
  "title":"Inject custom interpretation upstreams, e.g. for *.iterate()",
  "url":"https://github.com/scikit-hep/uproot5/discussions/555"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"**Dropped support for Pythons 2.7 and 3.5:** PR #525. From now on, Uproot only supports Python 3.6 and up.\r\n\r\n@henryiii dropped Python2isms from the codebase: PR #526.\r\n\r\n@jrueb made writing to Python file handles work (fixed a half-finished, forgotten implementation): PR #538.\r\n\r\n@jpivarski fixed `cut` jagged-array corner-case in `library=\"pd\"`. Fixed a case in which the instance version is 0, but the streamer version is not: PR #537. Fixed `uproot.WritableTTree.extend` when the metadata needs to be rewritten: PR #547. When checking to see if `something` in `file[\"name\"] = something` is an Awkward Array or Pandas DataFrame for creating a TTree, also check for superclasses (the whole mro): PR #557.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.2.0'>4.2.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-02-14T20:50:34Z",
  "number":560,
  "title":"4.2.0",
  "url":"https://github.com/scikit-hep/uproot5/discussions/560"
 },
 {
  "author":{
   "login":"jkwinter"
  },
  "body":"Hi, I am trying to convert data from an .lhe format into a ROOT file and I would like to store a numpy array of the four-momentum of each particle as a branch in a tree, for which I am using the scikit-hep LorentzVector class. What type should I specify for the branch when I do mktree(treename,{'branch1' : <type>} title = \"\")?\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Uproot doesn't (currently) write the TLorentzVector or `ROOT::Math::LorentzVector<ROOT::Math::PxPyPzM4D<double>>` types, so you'll have to write the four components as separate TBranches:\r\n\r\n```python\r\nwritable_directory.mktree(\"treename\", {\"px\": np.float64, \"py\": np.float64, \"pz\": np.float64, \"E\": np.float64})\r\nwritable_directory[\"treename\"].extend({\r\n    \"px\": [p4.x for p4 in many_lorentz_vectors],\r\n    \"py\": [p4.y for p4 in many_lorentz_vectors],\r\n    \"pz\": [p4.z for p4 in many_lorentz_vectors],\r\n    \"E\": [p4.t for p4 in many_lorentz_vectors],\r\n})\r\n```\r\n\r\n\r\n",
     "createdAt":"2022-03-01T20:15:37Z",
     "number":2276103,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jkwinter"
        },
        "body":"I see, thank you. Are there plans to allow for these types?",
        "createdAt":"2022-03-04T10:24:20Z",
        "number":2294305
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"It would be a reasonable thing to do. If you make it a feature request, it won't be forgotten, though I don't think it will be implemented soon (this month).\r\n\r\n\u2192 Unless anybody out there wants to try it! I'll help you through the process, if you do! \u2190",
        "createdAt":"2022-03-04T20:05:40Z",
        "number":2297480
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-03-01T17:59:39Z",
  "number":567,
  "title":"Writing Branch of LorentzVector objects to a tree",
  "url":"https://github.com/scikit-hep/uproot5/discussions/567"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@henryiii removed `wheel` from pyproject.toml: PR #565.\r\n\r\n@jpivarski added a rule to skip parsing Float16/Double32 TBranch titles if the title is not parsable (and just assume default number of bits): PR #561. Removed references to deprecated `distutils` and Pandas `Int64Index`: PR #564. Removed the rule that interpreted `fBits` as 1 byte (it's 4 bytes everywhere except in some branches of some Delphes files): PR #570.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.2.1'>4.2.1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-03-08T15:29:34Z",
  "number":571,
  "title":"4.2.1",
  "url":"https://github.com/scikit-hep/uproot5/discussions/571"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@jpivarski fixed #572: PR #573.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.2.2'>4.2.2</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-03-14T21:00:55Z",
  "number":574,
  "title":"4.2.2",
  "url":"https://github.com/scikit-hep/uproot5/discussions/574"
 },
 {
  "author":{
   "login":"prakhub"
  },
  "body":"Hi\r\n\r\nI am currently trying to read some ROOT files generated in [Allpix\u00b2](https://gitlab.cern.ch/allpix-squared/allpix-squared) and am experimenting with `uproot` in order to keep the analysis code more concise and \"pythonic\". The file I am trying to read ([link](https://github.com/scikit-hep/uproot4/files/8632323/data.txt), rename `data.txt` to `data.root`) consist of a TTree with a TBranch (\"DepositedCharge\") storing `std::vector<allpix::DepositedCharge*>` objects (see below)\r\n```\r\n>>> import uproot\r\n>>> branch = uproot.open('data.root:DepositedCharge/mydetector')\r\n>>> branch.show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nmydetector           | std::vector<allpix::Depo | AsObjects(AsVector(True, AsPoi\r\n>>> branch.typename\r\n'std::vector<allpix::DepositedCharge*>'\r\n>>> branch.interpretation\r\nAsObjects(AsVector(True, AsPointer(Model_allpix_3a3a_DepositedCharge)))\r\n```\r\n\r\nWhen trying to read the branch into an array I get the following error:\r\n```\r\n>>> branch = uproot.open('data.root:DepositedCharge/mydetector')\r\n>>> branch_arrays = branch.arrays(library=\"ak\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 1059, in arrays\r\n    return self.parent.arrays(\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 1125, in arrays\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3502, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3446, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/interpretation/objects.py\", line 140, in basket_array\r\n    form = self.awkward_form(branch.file, index_format=\"i64\")\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/interpretation/objects.py\", line 120, in awkward_form\r\n    return self._model.awkward_form(\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/containers.py\", line 779, in awkward_form\r\n    uproot._util.awkward_form(\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/_util.py\", line 555, in awkward_form\r\n    return model.awkward_form(\r\n  File \"/home/hop/anaconda3/envs/hephy/lib/python3.10/site-packages/uproot/containers.py\", line 551, in awkward_form\r\n    raise uproot.interpretation.objects.CannotBeAwkward(\"arbitrary pointer\")\r\nuproot.interpretation.objects.CannotBeAwkward: arbitrary pointer\r\n```\r\nIf I use `numpy` instead of `awkward` (`branch.arrays(library='np')`) then the file can be read without error:\r\n```\r\n>>> branch = uproot.open('data.root:DepositedCharge/mydetector')\r\n>>> branch_arrays = branch.arrays(library=\"np\")\r\n>>> branch_arrays\r\narray([<STLVector [<allpix::DepositedCharge (version 4) at 0x7f7ac0ff0d90>, ...] at 0x7f7ac0ff0e50>,\r\n       <STLVector [<allpix::DepositedCharge (version 4) at 0x7f7ab9bae620>, ...] at 0x7f7ab9bdaf20>,\r\n        ...\r\n       <STLVector [<allpix::DepositedCharge (version 4) at 0x7f7ab9bea0e0>, ...] at 0x7f7ab9be9ff0>,\r\n       <STLVector [<allpix::DepositedCharge (version 4) at 0x7f7ab9bfebc0>, ...] at 0x7f7ab9bfead0>],\r\n       dtype=object)\r\n>>>\r\n```\r\nHowever, reading the array is extremly slow (~5s for ~0.5MB data).\r\n\r\nWhat do I need to fix in order to use `awkward` (which hopefully speeds up the execution time)?\r\n\r\nFor the `<allpix::DepositedCharge>` objects, header files are available at [DepositedCharge.hpp](https://gitlab.cern.ch/allpix-squared/allpix-squared/-/blob/master/src/objects/DepositedCharge.hpp).\r\nI wrote code aimed at the same goal using `PyROOT` where I load the object definitions from a shared library (using `gSystem.Load(\"/path/to/allpix-squared/lib/libAllpixObjects.so\"`).\r\nIs there some way to pass the same information to `uproot`?\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"If the object you want to read has pointers within it, then it does not fit in Awkward Array's type system. C++ objects are arbitrary graphs, but objects in Awkward Arrays must be trees.\r\n\r\nYou may be able to read this with `library=\"np\"`, since that turns the data into Python objects (slowly). Python objects are also arbitrary graphs.\r\n\r\nYou can do it with PyROOT as well by giving ROOT the shared library describing the C++ classes that it should fill (libAllpixObjects.so). This is because PyROOT objects are just proxies pointing to C++ data. That makes the PyROOT solution a little different from Uproot's `library=\"np\"`: since Uproot's objects are pure Python, they should be pickleable and I expect that the PyROOT objects are not, but this difference might not be relevant to you. Neither Uproot's `library=\"np\"` nor PyROOT are very fast because Python is in the loop in any iteration over these objects.\r\n\r\nUproot can't make use of any C++ libraries (like libAllpixObjects.so) because it doesn't run any C++ code. Awkward Array wouldn't be able to use it, either, because its data model is completely different from C's notion of a `struct`: Awkward Arrays are columnar (a different memory layout), and compiled C++ code specifies a particular memory layout.\r\n\r\nIn the end, there isn't much that can be done with this data type because it has cross-linking pointers. Awkward Array generalized arrays beyond just numbers, but it hasn't generalized it to the extent of supporting cross-links. (There were some attempts to do that in Awkward 0.x, but they were fragile\u2014didn't work very well\u2014and not in high demand.) This sounds like a perfect job for Julia and [UnROOT](https://github.com/JuliaHEP/UnROOT.jl), but then your project wouldn't be \"Pythonic\" so much as \"Julionic.\" (What's the right word?)\r\n\r\nNote: I don't know if UnROOT can read general data types like this one; I'm just saying that it fits the Julia type system, as Julia has an ordinary language's type system and not a specialized DSL for array programming in an interpreted language.",
     "createdAt":"2022-05-05T18:06:18Z",
     "number":2695888,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"prakhub"
        },
        "body":"Hi @jpivarski\r\nThank you very much for this in-depth Answer!\r\n\r\nI will have a look at UnROOT, it seems that there is already some (experimental) [support](https://juliahep.github.io/UnROOT.jl/dev/advanced/custom_branch/) for using custom structs.\r\nBut I guess at the end of the day, it is probably easiest to just iterate over the file using ROOT itself and output the data in a new Tree or some other data format entirely.",
        "createdAt":"2022-05-06T09:09:52Z",
        "number":2699305
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-05-05T15:30:21Z",
  "number":590,
  "title":"Reading Allpix\u00b2 files - uproot.interpretation.objects.CannotBeAwkward: arbitrary pointer",
  "url":"https://github.com/scikit-hep/uproot5/discussions/590"
 },
 {
  "author":{
   "login":"kjmeagher"
  },
  "body":"I am trying to use uproot to replicate the structure of data that is currently being written in root. Each leaf has 3 integers. \r\nThe when opened in uproot the branch has a typename `uint16_t[][3] ` and interpretation `AsJagged(AsDtype(\"('>u2', (3,))\"))` and I can read the data as expected. However, I cannot figure how to write such a tree with uproot. I couldn't find anything in the uproot documentation that addresses this. This is what I tried so far.\r\n\r\n```\r\nimport uproot\r\nimport awkward as ak\r\noutfile = uproot.recreate('outfile.root')\r\n\r\ndesc = {'vector' : 'var * 3 * uint16'}\r\ntree = outfile.mktree(\"Tree\", desc, title=\"Title\")\r\n\r\nd = {\r\n     'vector': ak.Array([ \r\n                    [ [1,1,1], [2,2,2] ,[3,3,3] ],\r\n                    [ [4,4,4], [5,5,5] ] \r\n                    ])\r\n     }\r\nprint(d)\r\ntree.extend(d)\r\n```",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"You need to make the ak.Array regular (or just use a NumPy array, which are all regular). The `mktree` descriptor is good, but data passed to `extend` is not cast as the descriptor (unless it's pure Python lists of `int` and `float`); it's checked against the descriptor and will complain if there's a mismatch.\r\n\r\nFixed-length lists are a different type from variable-length lists. In your descriptor, the \"`var`\" refers to a variable-length dimension and the \"`3`\" refers to a fixed-length dimension.\r\n\r\nWhen you construct an ak.Array from Python data, it is all variable-length because Python lists don't have fixed lengths.\r\n\r\n```python\r\n>>> varlen = ak.Array([[[1, 1, 1], [2, 2, 2]], [[4, 4, 4], [5, 5, 5]]])\r\n>>> varlen\r\n<Array [[[1, 1, 1], [2, 2, ... 4], [5, 5, 5]]] type='2 * var * var * int64'>\r\n```\r\n\r\nNote the \"`var * var`\" in the type.\r\n\r\nWhen you construct an ak.Array from a NumPy array, it's all regular-length because NumPy dimensions have fixed lengths.\r\n\r\n```python\r\n>>> reglen = ak.Array(np.array([[[1, 1, 1], [2, 2, 2]], [[4, 4, 4], [5, 5, 5]]]))\r\n>>> reglen\r\n<Array [[[1, 1, 1], [2, 2, ... 4], [5, 5, 5]]] type='2 * 2 * 3 * int64'>\r\n```\r\n\r\nNote the \"`2 * 3`\" in the type.\r\n\r\nYou can also turn variable-length dimensions into fixed-length dimensions with [ak.to_regular](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_regular.html) (there's also an [ak.from_regular](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_regular.html)).\r\n\r\n```python\r\n>>> ak.to_regular(varlen, axis=1)\r\n<Array [[[1, 1, 1], [2, 2, ... 4], [5, 5, 5]]] type='2 * 2 * var * int64'>\r\n>>> ak.to_regular(ak.to_regular(varlen, axis=1), axis=2)\r\n<Array [[[1, 1, 1], [2, 2, ... 4], [5, 5, 5]]] type='2 * 2 * 3 * int64'>\r\n```\r\n\r\nAnother mismatch is the numeric type: \"`int64`\" versus \"`uint16`\". If you construct the ak.Array from a NumPy array with the desired dtype, the ak.Array will have that dtype, too. If you have an ak.Array made from Python data, it will default to 64-bit numbers. You can change that with [ak.values_astype](https://awkward-array.readthedocs.io/en/latest/_auto/ak.values_astype.html).\r\n\r\nIt looks like you want mixed variable-length and regular-length: the only way to do that is with an Awkward Array. (Python lists will be interpreted as all-variable and NumPy arrays are all-regular.)",
     "createdAt":"2022-05-05T17:32:09Z",
     "number":2695697,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"kjmeagher"
        },
        "body":"Thanks for the quick reply. This makes more sense now, but I still seem to have problems, I am not sure if I am still doing anything wrong or if I encountered a bug. \r\n\r\nIf I run this:\r\n```\r\nimport uproot\r\nimport awkward as ak\r\noutfile = uproot.recreate('outfile.root')\r\n\r\ndesc = {'vector' : 'var * 3 * uint16'}\r\ntree = outfile.mktree(\"Tree\", desc, title=\"Title\")\r\n\r\nx = ak.Array([ [ [1,1,1], [2,2,2] ,[3,3,3] ],\r\n               [ [4,4,4], [5,5,5]],\r\n               [ [6,6,6]],\r\n               [ [7,7,7], [8,8,8]]\r\n               ])\r\n\r\nx = ak.to_regular(x, axis=2)\r\nx = ak.values_astype(x,'uint16')\r\ntree.extend({'vector':x})\r\noutfile.close()\r\n```\r\n\r\nIt runs without error but I can't read the output. \r\n\r\n```import uproot\r\nwith uproot.open('outfile.root') as f:\r\n    f[\"Tree\"].show()\r\n    f[\"Tree\"]['vector'].array()\r\n```\r\ngives this output\r\n```\r\name                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nnvector              | int32_t                  | AsDtype('>i4')\r\nvector               | uint16_t[][3]            | AsJagged(AsDtype(\"('>u2', (...\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/site-packages/uproot/interpretation/numerical.py\", line 340, in basket_array\r\n    output = data.view(dtype).reshape((-1,) + shape)\r\nValueError: cannot reshape array of size 8 into shape (3)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/kjm/icecube/icetray/uprootwriter/printfile.py\", line 5, in <module>\r\n    f[\"Tree\"]['vector'].array()\r\n  File \"/usr/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 2078, in array\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/usr/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3502, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/usr/lib/python3.10/site-packages/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/usr/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3446, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/usr/lib/python3.10/site-packages/uproot/interpretation/jagged.py\", line 174, in basket_array\r\n    content = self._content.basket_array(\r\n  File \"/usr/lib/python3.10/site-packages/uproot/interpretation/numerical.py\", line 342, in basket_array\r\n    raise ValueError(\r\nValueError: basket 0 in tree/branch /Tree;1:vector has the wrong number of bytes (16) for interpretation AsDtype(\"('>u2', (3,))\")\r\nin file outfile.root\r\n```\r\n\r\nIf i try to read this file in ROOT i get the following output. \r\n```\r\nroot [1] Tree->Scan()\r\n***********************************************\r\n*    Row   * Instance * nvector.n * vector.ve *\r\n***********************************************\r\nWarning in <TBasket::ReadBasketBuffers>: basket:vector has fNevBuf=4 but fEntryOffset=0, pos=310, len=145, fNbytes=130, fObjlen=72, trying to repair\r\n*        0 *        0 *         3 *         1 *\r\n*        0 *        1 *         3 *         1 *\r\n*        0 *        2 *         3 *         1 *\r\n*        0 *        3 *         3 *         2 *\r\n*        0 *        4 *         3 *         2 *\r\n*        0 *        5 *         3 *         2 *\r\n*        0 *        6 *         3 *         3 *\r\n*        0 *        7 *         3 *         3 *\r\n*        0 *        8 *         3 *         3 *\r\n*        1 *        0 *         2 *     60416 *\r\n*        1 *        1 *         2 *         0 *\r\n*        1 *        2 *         2 *     18541 *\r\n*        1 *        3 *         2 *     19645 *\r\n*        1 *        4 *         2 *      9472 *\r\n*        1 *        5 *         2 *     18688 *\r\n*        2 *        0 *         1 *         0 *\r\n*        2 *        1 *         1 *       130 *\r\n*        2 *        2 *         1 *      1004 *\r\n*        3 *        0 *         2 *         0 *\r\n*        3 *        1 *         2 *       130 *\r\n*        3 *        2 *         2 *      1004 *\r\n*        3 *        3 *         2 *         0 *\r\n*        3 *        4 *         2 *        72 *\r\n*        3 *        5 *         2 *     27980 *\r\n***********************************************\r\n```\r\nIt looks like after the first row the contents is random memory.",
        "createdAt":"2022-05-06T17:02:08Z",
        "number":2702151
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-05-05T16:57:24Z",
  "number":591,
  "title":"Writing leafs with fixed number of objects",
  "url":"https://github.com/scikit-hep/uproot5/discussions/591"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@jpivarski fixed the use of Unicode names for objects in ROOT files: PR #577. Made `flow=True` mean the same thing for Uproot histograms as hist: PR #582. Added an interpretation for `ROOT::VecOps::RVec`: PR #593. Replaced an error message in HTTPSource with a fallback to the non-multipart-GET case: PR #594. Put an upper limit on the time KeyInFileError takes to print itself: PR #595.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.2.3'>4.2.3</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-05-11T21:49:10Z",
  "number":596,
  "title":"4.2.3",
  "url":"https://github.com/scikit-hep/uproot5/discussions/596"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi \r\n\r\nI was wondering what uproot experts/users recommend as the best way (for performance) to process large data when reading from uproot? I am trying to process data on lxplus from EOS, where i have 65 GBs of data split over many ~1GB ROOT files. From each of these ROOT files I am reading quite a lot of the jagged branches and some of them are quite large (e.g. jets). At this stage I don't want to limit how much I read from each branch so I would like to keep all these objects. The way I process the files at the moment is by grouping them into chunks of 2 (or 4, not much difference) GBs and then run uproot.concatenate on each chunk, and then dump the content out before reading the next chunk. This seems to take O(300-400s) per chunk on lxplus which means I have to wait O(hours) to process these datasets. I tried moving the files onto AFS, where the code runs, but it is still slow. I also tested making a clean virtual environment to run the code in, but also still slow. Running on my local computer with 10 cores and 32GBs of RAM each chunk can take ~30s. Am I just limited by lxplus resources or is there a way I can make the I/O faster? \r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"What you're doing already sound like the best practices. The possible bottlenecks are (1) disk access rate, (2) decompression rate, (3) data type interpretation rate, and (4) header-reading overhead. The files are large enough to address (4), as long as the TBaskets within the file are also large enough. If these are single-jagged (i.e. `double[]` or `std::vector<double>`), not any more deeply nested types, then we can rule out (3). If it's not LZMA, then we can rule out (2).\r\n\r\nSome gotchas to look out for (a checklist):\r\n\r\n   * Are you sure you're not reading more branches than you need? If you don't specify an `expressions` or `filter_*` to [uproot.concatenate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.concatenate.html), then you'll be reading all branches, regardless of whether you need them.\r\n   * It's pretty easy to tell if you're reading more entries than you need when you use [uproot.concatenate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.concatenate.html) because you're reading whole files. If you slice off entries in one task because they belong to another task, that would be lost efficiency, but I think you'd be aware of that.\r\n   * Are you sure you're not doing any Python for loops over the data? If the data are deeper than single-jagged, then Uproot is doing a Python for loop internally, but it sounds like that's not your case. In your code, you should be able to see explicit Python for loops and list comprehensions by the word `for`, but what about internal loops in functions like `sum`, `max`, or `map`? These can all be replaced by NumPy or Awkward equivalents.\r\n\r\nThere are also some things you can check that you can't do anything to improve, but at least know that what you have is top speed, so that you don't needlessly spend time on optimization.\r\n\r\n  * Check the typical TBasket size within the ROOT files. Each TBranch has _n_ TBaskets, which you can find with [uproot.TBranch.num_baskets](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#num-baskets). If you calculate individual TBaskets to be at the kB scale or smaller, then you'll suffer from (4) header-reading overhead. Unfortunately, that's baked into the file. If you're going to be running over the data more than once, it would pay to make the TBaskets reasonably large, like MB scale.\r\n  * What's the raw disk access rate? When everything goes well\u2014minimal header-reading overhead, minimal interpretation rate, minimal decompression time\u2014then Uproot is limited by the time it takes to get bytes off disk and into RAM. Try making a big file of junk and loading it into memory with `tmp = open(\"filename.junk\", \"rb\").read()`. Be careful, though, that it's not already in RAM: modern operating systems cache segments of disk that have been recently accessed. You can control this with [vmtouch](https://hoytech.com/vmtouch/), but if it's lxplus (same disks accessed by many computers), it may be easiest to make the junk file with one computer and try to access it with another, since the latter has never heard of this file and wouldn't have it in RAM. That's a \"cold\" read.\r\n  * If the data are LZMA compressed, decompressing them will be slow, but you can test this, too. The [lzma command](https://linux.die.net/man/1/lzma) can be used to compress and uncompress a large junk file to find out how long that can be expected to take. (This is irrelevant if your file is not LZMA compressed, and if it's the issue, there's no way to solve it except by recreating the files with a different compression algorithm.)\r\n\r\nI wouldn't expect AFS to be a fast disk, since it's known for being one of the slower (and older) ways of maintaining filesystem consistency on a distributed system. On lxplus, AFS is (was?) used for home directories, which are \"small, but important\" data. You want this on some disk intended for big datasets. I don't know what lxplus options exist. If you find that your limiting factor is raw disk access (just loading a junk file into RAM accounts for most of the time), then you'll want to ask lxplus experts what your options are.",
     "createdAt":"2022-05-13T18:16:33Z",
     "number":2747683,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"Hi Jim, Thanks a lot for the detailed answer. Onto your checklist:\r\n- I call uproot with the call `uproot.concatenate(files, branches)`, so I think we are good on this side\r\n- You're right, this wouldn't happen in my case since the way I chunk is to simply go through the list of files and append them to a list until the total size of the files in that list reaches 3GBs. If I file takes the set beyond the 3GB boundry, it goes to the next list. This way there is no single file present in 2 chunks.\r\n- I explicitly check the time taken by `uproot.concatenate` so even if I had loops my time estimate wouldn't know about it. On that note im pretty sure I don't have any event loops -- i do have loops over the the field names but this is a fairly small list. \r\n\r\nFor your bottlnecks list:\r\n\r\n- I checked inside the ROOT files and it seems like for the jagged branches, the basket sizes are ~20-50kbs and for flat branches ~1kb. Is changing the TBasket size something that can be done from within ROOT? On that note, I tried reading a reduced set of  branches from a set of files that total to ~3GBs with `RDataframes` and with `uproot.concatenate` and both use up the same amount of time with uproot being a bit faster.\r\n- I checked that by creating a file using ```truncate -s 4G foo``` then opening like you said ... for a 4GB file the `timeit` magic suggests the longest run took 200s (which is the bulk of the time spent by uproot reading same size files). Maybe the access rate  combined with the Basket size are the bottlenecks?\r\n- I am not sure how to check the compression of a ROOT file, so couldn't test that. \r\n",
        "createdAt":"2022-05-19T14:00:54Z",
        "number":2783903
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Checking compression is tricky. There are [uproot.ReadOnlyFile.compression](https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyFile.html#compression) and [uproot.TBranch.compression](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#compression) flags that report what the ROOT files _says_ the compression algorithm and level are\u2014so start there\u2014but the actual compression of a TBasket can be anything, and in principle it can vary from TBasket to TBasket (though I don't think it would for most methods of creating ROOT files).\r\n\r\nIf the average basket size is one to tens of kB, that's borderline good. Their headers are about 80 bytes each, and each requires [about 100 lines of Python code](https://github.com/scikit-hep/uproot4/blob/1f2e5d2a9fc22a6eac83d38d1d6cbe3c46c5396f/src/uproot/models/TBasket.py#L212-L317) to process before ingesting thousands (one to tens of kB) of numerical values in a vectorized read. In [very old performance plots](https://github.com/scikit-hep/uproot3#uproot), tens of kB TBasket size was the turn-over point where Uproot's vectorized read starts providing some improvement over C++ event loops. The value of recreating all of your input data with larger TBaskets might be noticeable, but not dramatic.\r\n\r\nDid you say that reading 4 GB of random data takes 200 seconds and reading 4 GB of data from ROOT takes 300\u2012400 seconds? That's not bad: all of the extra work of reading and navigating TFile, TDirectory, TTree, and TBasket headers, decompressing, possibly removing `std::vector` headers if it's `std::vector` instead of variable-length arrays, and concatenating all of the little TBasket arrays into big arrays is on the same scale as raw data reading. If the overhead were much less, you would still be waiting for the disk.\r\n\r\nTaking the worst case, that the full workflow is 2\u00d7 slower than it possibly could be, does that make a big difference in some metric? That is, if you're informally launching jobs and dealing with them when they're done, 5 hours vs 10 hours may be equivalent. (E.g. leave it, do something else, deal with the results the following day.) If you're on a cloud infrastructure and paying for every CPU-second, then yeah: paying twice as much is an important metric! Then you'd want to think about increasing TBasket size (in ROOT: [TTree::SetAutoFlush](https://root.cern.ch/doc/master/classTTree.html#ad4c7c7d70caf5657104832bcfbd83a9f) or [RSnapshotOptions::fAutoFlush](https://root.cern/doc/master/structROOT_1_1RDF_1_1RSnapshotOptions.html); in Uproot: size of the array sent to [uproot.WritableTree.extend](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableTree.html#extend)), LZ4 compression (in ROOT: [TFile::SetCompressionAlgorithm](https://root.cern.ch/doc/master/classTFile.html#a8ed2b3d3f644d739766e16cb70a49393) and following; in Uproot: [uproot.recreate](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.recreate.html) with explicit `compression`).",
        "createdAt":"2022-05-19T14:39:51Z",
        "number":2784196
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-05-13T17:09:06Z",
  "number":597,
  "title":"Best way to process many, long jagged arrays with uproot?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/597"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi experts, \r\n\r\nLet's say I am trying to access multiple trees from the same _set of files_ which can fit into memory, call it `files`. The documentation suggests that for reading sets of files, `uproot.concatenate` is the way to go. There are multiple ways I can think to do this:\r\n\r\n1. Simply make two sets of file lists `files_tree1` and `files_tree2`  where `files_treeX = [f+\":treeX\" for f in files]`, pass each list to` uproot.concatenate` in a separate call:\r\n\r\n```\r\nuproot.concatenate(files_tree1, filter_name = tree1_branches)\r\nuproot.concatenate(files_tree2, filter_name = tree2_branches)\r\n```\r\nThis I think is inefficient, since the same files wil be opened twice. Maybe caching would help, but feels like it can be done better. \r\n\r\n2. [Naively] make a combined  files list: `files_all = files_tree1 + files_tree2` and combined branches list: `all_branches = tree1_branches + tree2_branches]` and pass these to one uproot call, hoping uproot magic will know what to do. This surprisngly (at least to me) did not crash. Uproot just produced an `awkward` array which is a `union` of 2 awkward arrays `tree1_branches` (N entries) and `tree2_branches` (M entries), with `ak.type(data)` giving :\r\n```\r\nN+M * union[{\"tree1_branch1\": var * float32, \"tree1_branch2\": var * float32}, {\"tree2_branch1\": int32}]\r\n```\r\nso calling `data[N+M-1]` gives the last entry from `tree2` and `data[0]` gives the first entry of `tree1`. I guess it can be expected behaviour from the `global_index` that `uproot.concatenate()` seems to keep track of (or maybe I'm completely off)? \r\n\r\nAnyway, I think we still open each file twice, which is non-ideal. \r\n\r\n3. I can do a manual loop over the files and call `uproot.open` on each of them, then access the keys from the structure we get back. This way each file is opened once. \r\n\r\nMy questions are: \r\n\r\n- What does `uproot.concatenate` do in the background that makes it more performant (if that's even true) than `uproot.open` inside a loop over files?  What I can see quickly from a skim over the source code is that `concatenate` loops over the files one by one, opening them as `ReadOnlyFile` then grabbing the data, but I am probably missing something subtle in the steps. \r\n- What do you recommend as best practicei n reading multiple trees from many files? ",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-05-26T17:32:17Z",
  "number":599,
  "title":"What is `uproot.concatenate` really doing?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/599"
 },
 {
  "author":{
   "login":"MoAly98"
  },
  "body":"Hi experts, \r\n\r\nLet's say I am trying to access multiple trees from the same _set of files_ which can fit into memory, call it `files`. The documentation suggests that for reading sets of files, `uproot.concatenate` is the way to go. There are multiple ways I can think to do this:\r\n\r\n1. Simply make two sets of file lists `files_tree1` and `files_tree2`  where `files_treeX = [f+\":treeX\" for f in files]`, pass each list to` uproot.concatenate` in a separate call:\r\n\r\n```\r\nuproot.concatenate(files_tree1, filter_name = tree1_branches)\r\nuproot.concatenate(files_tree2, filter_name = tree2_branches)\r\n```\r\nThis I think is inefficient, since the same files wil be opened twice. Maybe caching would help, but feels like it can be done better. \r\n\r\n2. [Naively] make a combined  files list: `files_all = files_tree1 + files_tree2` and combined branches list: `all_branches = tree1_branches + tree2_branches]` and pass these to one uproot call, hoping uproot magic will know what to do. This surprisngly (at least to me) did not crash. Uproot just produced an `awkward` array which is a `union` of 2 awkward arrays `tree1_branches` (N entries) and `tree2_branches` (M entries), with `ak.type(data)` giving :\r\n```\r\nN+M * union[{\"tree1_branch1\": var * float32, \"tree1_branch2\": var * float32}, {\"tree2_branch1\": int32}]\r\n```\r\nso calling `data[-1]` gives the last entry from `tree2` and `data[0]` gives the first entry of `tree1`. I guess it can be expected behaviour from the `global_index` that `uproot.concatenate()` seems to keep track of (or maybe I'm completely off)? \r\n\r\nAnyway, I think we still open each file twice, which is non-ideal. \r\n\r\n3. I can do a manual loop over the files and call `uproot.open` on each of them, then access the keys from the structure we get back. This way each file is opened once. \r\n\r\nMy questions are:\r\n-  What does `uproot.concatenate` do in the background that makes it more performant (if that's even true) than `uproot.open` inside a loop over files? What I can see quickly from a skim over the source code is that `concatenate` loops over the files one by one, opening them as `ReadOnlyFile` then grabbing the data, but I am probably missing something subtle in the steps. \r\n- What do you recommend as best practice to ready multiple trees from many files?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Here's the implementation of `uproot.concatenate`: https://github.com/scikit-hep/uproot4/blob/6fea96e3e61814cdeb21a3527c215f1911759d9a/src/uproot/behaviors/TBranch.py#L343-L382\r\n\r\nIt doesn't do much, just iterates over the files, reads the requested data, and calls `ak.concatenate` to concatenate them. I can see how what you tried ends up producing a union array (concatenate two arrays with different types and you'll get a union array of data that is first all the first type, then all the second type). You very likely don't want the union array because it can make downstream processing harder: even methods that can deal with union arrays at all have to take a conservative approach about what's in them.\r\n\r\nIf you want to avoid opening the same files multiple times, you'd have to forsake the convenience of `uproot.concatenate`, loop over the set of files yourself, reading the two TTrees from each and building up a list to concatenate. Or maybe this is a blessing: if your analysis can work on smaller chunks, you can do it with smaller overall memory use.\r\n\r\nIt doesn't make sense to try to cover a lot of use-cases in `uproot.concatenate` because that would complicate its interface, and then it would not be a very good convenience function anymore. The reason it exists in the first place is because some people were using `uproot.lazy` just to get a multi-file interface, but they were suffering from the performance problems of lazy arrays. `uproot.concatenate` has the interface of `uproot.lazy`, but reads eagerly (good when you have enough memory!).",
     "createdAt":"2022-06-03T16:46:40Z",
     "number":2879685,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-05-26T17:33:09Z",
  "number":601,
  "title":"What does `uproot.concatenate` really do?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/601"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@kkothari2001 implemented `uproot.dask` (for `library=\"np\"` only), a new function to provide Uproot data as Dask arrays: PRs #578, #603. Added another new function, `uproot.num_entries`, for getting the number of TTree entries from a set of files with minimal reading: PR #609.\r\n\r\n@jpivarski removed tests that rely on a ROOT TStreamerInfo behavior: PR #612. Changed `awkward_form` arguments to `(self, file, context)` so that new pseudo-arguments can be passed through the `context` without another major code change: PR #611. This is to prepare for @aryan26roy's AwkwardForth implementation of `AsObjects` interpretations (see #610, WIP targeting Uproot 5).\r\n\r\n**Note:** this is the last Uproot 4 release before the repo branches into `main` (for version 5) and `main-v4` (for bug-fixes in Uproot 4, starting with 4.3.0). Uproot 5 will incorporate Awkward version 2, as described in the [Awkward road map](https://github.com/scikit-hep/awkward/wiki#grand-view-and-history), but it will have minimal interface changes relative to Uproot 4 (mostly just `uproot.lazy` \u2192 `uproot.dask`).\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot4/releases/tag/4.2.4'>4.2.4</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-06-21T12:20:01Z",
  "number":613,
  "title":"4.2.4",
  "url":"https://github.com/scikit-hep/uproot5/discussions/613"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"First Uproot 4 release from the bug-fix branch.\r\n\r\n@jpivarski added a TMatrixTSym model (long stalled PR): #484.\r\n\r\n@aryan26roy restored the argument list of `Interpretation.awkward_form`, a public function: PR #618.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/4.3.0'>4.3.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-06-21T22:05:19Z",
  "number":619,
  "title":"4.3.0",
  "url":"https://github.com/scikit-hep/uproot5/discussions/619"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@jpivarski applied this fix: https://github.com/scikit-hep/uproot5/pull/618#discussion_r903623799.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/4.3.1'>4.3.1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-06-22T15:34:59Z",
  "number":626,
  "title":"4.3.1",
  "url":"https://github.com/scikit-hep/uproot5/discussions/626"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@jpivarski cleanly ensured all `Interpretation.awkward_form` signatures have been reverted (even `AsJagged`): PR #627.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/4.3.2'>4.3.2</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-06-22T17:30:13Z",
  "number":628,
  "title":"4.3.2",
  "url":"https://github.com/scikit-hep/uproot5/discussions/628"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@jpivarski fixed an _O(n\u00b2)_ scaling bug in getting data from TDirectories. Now it's _O(n)_. (Matters for large TDirectories.)\r\n\r\nPR #638 in `main` and #639 in `main-v4`. This release is from `main-v4`.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/4.3.3'>4.3.3</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-07-02T19:51:49Z",
  "number":640,
  "title":"4.3.3",
  "url":"https://github.com/scikit-hep/uproot5/discussions/640"
 },
 {
  "author":{
   "login":"qibin2020"
  },
  "body":"  Hi I am very new to uproot from ROOT. I was wondering is there some recommended way to do histogram division in uproot?\r\n\r\n  As in my previous ROOT workflow we frequently calculate ratio histograms by TH1.Divide(TH1), and store them for later usage(e.g. crazy ratio of ratio) or plotting. I am not sure how to adapt this workflow in uproot since I haven't find equivalent method in any hist or boost-histogram Hist object. I found the hist.plot_ratio is useful but not easy to generalized to, e.g. plot multiple ratios. \r\n\r\n  So does anyone have some ideas on this or we may still need to import ROOT to handle the histogram division operation and finally convert to uproot-hist?\r\n\r\n  I am using uproot4.2.0 shipped with scikit-hep  [v3.6.0](https://github.com/scikit-hep/scikit-hep/releases/tag/v3.6.0).\r\n\r\n  (Pls correct me if asked at a wrong place and many thanks)",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"MoAly98"
     },
     "body":"Hi Qibin! I am not an expert, but as far as I understand the division of histograms in `boost_histogram` is not fully complete yet. AFAIK the division of weighted histograms is not done yet (see [boost-histogrma issue](https://github.com/boostorg/histogram/issues/352)).  Possibly one way around this is to define your own `divide` function to divide the histogram values as `numpy` arrays then compute the error using the [ratio_uncertainty method from scikit-hep/Hist ](https://hist.readthedocs.io/en/latest/reference/hist.intervals.html?highlight=ratio_uncertainty#hist.intervals.ratio_uncertainty) ?  You can then assign to the results to a boost-histogram/ Hist object. \r\n\r\nI am sure there are better ideas from the experts here, but maybe that's one way :)",
     "createdAt":"2022-07-05T11:17:54Z",
     "number":3084181,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"qibin2020"
        },
        "body":"Thanks a lot! I think this is a good workaround and I will have a try. (Also thanks for the suggestion of `ratio_uncertainty`, I realize the uncertainty of ratio is not trivial so this might be a challenge for the generalization)",
        "createdAt":"2022-07-05T12:23:22Z",
        "number":3084557
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I think @MoAly98's suggestion is the best one: get the NumPy arrays and do the division manually. Histogram division is tricky because it will assume some error propagation, and usually the wrong one. Are the bins of the numerator statistically independent of the bins of the denominator (as is often the case with histogram addition)? Or is the numerator a _subset_ of the denominator (i.e. highly dependent, but a common use-case: efficiency plots)?\r\n\r\nAlthough histogram packages can implement these, it can be hard to be sure that you're communicating with the histogram package on what the error bars for the histogram mean, and since this is vital to the final result, doing it explicitly (with NumPy arrays) is prudent for self-documentation, not just a work-around. Maybe if the histogram package has very well-named methods, like `divide_as_independent` or `divide_as_subset`, the intention would be clear, but if it's just an overloaded `/` operator, that's dangerous. Even if it works, a reader of the code might not realize your intent.",
        "createdAt":"2022-07-05T13:26:02Z",
        "number":3085069
       },
       {
        "author":{
         "login":"qibin2020"
        },
        "body":"Yes I agree. The `/` operation is not well-defined for the histogram itself without the knowledge of the statistical meaning of the deno/nom.\r\n\r\nBesides I just found the codes of hist.plot_hist() have `ratio_uncertainty` and options for multiple uncertainty cases even the `pull`. So maybe inheriting this func and extending the functionality (like multiple ratio plotting) is also a convenient workflow for analysis.\r\n\r\nMarked @MoAly98 's as answer and thanks for the helpful discussion!",
        "createdAt":"2022-07-05T13:45:09Z",
        "number":3085237
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"(Thanks for marking answers!)",
        "createdAt":"2022-07-05T14:01:44Z",
        "number":3085364
       }
      ],
      "totalCount":4
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2022-07-05T10:49:19Z",
  "number":642,
  "title":"Simple way to evaluate ratio plot in uproot?",
  "url":"https://github.com/scikit-hep/uproot5/discussions/642"
 },
 {
  "author":{
   "login":"Elliot-Wadge"
  },
  "body":"Hey, sorry I'm new to uproot and I'm struggling with how to properly extract what I need from root files and also manage the memory being allocated. Here's what I've been trying to make work, however the ram usage will just increase with every iteration until my computer crashes. \r\n\r\n\r\n```\r\nimport uproot\r\nimport gc\r\nimport psutil\r\n\r\nwith uproot.open(\"example.root\") as root_file:\r\n    for key in root_file.keys():\r\n        gg_counts = root_file[key].values()\r\n        x = root_file[key].axis('x').centers()\r\n        # do something useful\r\n        del x\r\n        del gg_counts\r\n        collected = gc.collect()\r\n        print('RAM memory % used:', psutil.virtual_memory()[2])\r\n```\r\n\r\nHere I've tried deleting and garbage collecting but I'm guessing that the root_file is keeping a reference to the loaded arrays so this doesn't work. I was hoping someone could tell me the proper way to do this. \r\n\r\nThanks!",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"Elliot-Wadge"
     },
     "body":"I did find one work around which is just reversing the order of the for loop and the with statement but is there a more proper way of doing this?",
     "createdAt":"2022-07-09T06:22:10Z",
     "number":3111472,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"  I have recently been working a lot on using `uproot` to handle large inputs that don't fit into memory, so I may be able to share my insights from many discussions with the experts, but I would need a bit more information :)\r\n  \r\n  I may be missing something really straight-forward, but can you clarify a abit more what you are aiming to do? Are you expecting to extract all the data from the ROOT files? how large are ROOT files and what's the content (are they histograms or events organised in trees and branches?). I am also unsure how flipping the order of the `for` loop and `with` statement helps? Do you know the `keys` you want in advance? because if not then you will have to open the file anway to get them, right? \r\n\r\nSorry if I'm missing something trivial here :)  ",
        "createdAt":"2022-07-09T10:46:56Z",
        "number":3112157
       },
       {
        "author":{
         "login":"Elliot-Wadge"
        },
        "body":"great! Okay so\r\n\r\n1. No I don't need to extract all the data from the ROOT file at one time, but I do need to access most of the data at some point, but it doesn't have to all be in memory at the same time\r\n2. the ROOT file is about 1Gb and I'm accessing 2D histograms from it which are about 30-50mb (in python), I make a few projections, fit a few peaks and then I'm done with the histogram and it should be removed from memory\r\n3. flipping the for and the with statement helps because when you close the root file at the end of the with statement it is no longer referencing  the histogram so python's built in garbage collector will now delete the histogram from memory freeing it. However if the with statement is outside the for loop the file is never closed through the 160 histograms opening and so the loaded histograms are never dereferenced and so the garbage collector doesn't collect it (I think) they just build up until their all in memory and it crashes.\r\n4. Yes I do know what keys I want before hand, the code snippet there was just an example of what would be a worst case I guess. There's about 160 TH2D histograms that I need to access and perform some fits on.\r\n\r\nHope that makes it clearer, let me know if doesn't, thanks",
        "createdAt":"2022-07-09T17:17:08Z",
        "number":3113183
       },
       {
        "author":{
         "login":"MoAly98"
        },
        "body":"Yes that's clear :)\r\n\r\nI am not a fan of swapping the loops -- opening and closing the file ~200 times does not sound like the most efficient way to do this and it will not scale well (but I may be wrong). \r\n\r\nFrom my understanding, this looks like a possible use-case for `uproot.lazy` -- you can just open the file lazily and then through a loop over the keys you can apply your functions to perform the fits and all. This would mean that only one histogram is loaded at a time. You can probably do this more efficiently by processing chunks of histograms in one go rather than 1-by-1. I don't think you would need to use a garabage collector at all when doing that -- python memory handling is often good enough (at least I rarely ever need to `gc` myself). Can you see any limitation from using lazy loading?\r\n\r\nYou might also benefit from using [coffea processors](https://coffeateam.github.io/coffea/notebooks/processor.html) -- I am not a power user of `coffea` **yet**, but I believe you will not need to worry about the memory management when running your code inside the processor. You can also benefit from the `coffea` executors to parallelize your workflow and operate on multiple histograms at the same time. \r\n\r\nThat's just my 2 cents, and I am sure a more elaborate answer or suggestion can come from the experts and we can both learn something new :)\r\n",
        "createdAt":"2022-07-09T20:07:31Z",
        "number":3113589
       },
       {
        "author":{
         "login":"Elliot-Wadge"
        },
        "body":"Yes, that's exactly the reason I didn't want to swap the loops as well haha. Okay great I'll have a look at the documentation for that, that sounds perfect. Yes the garbage collector was more of a last resort attempt to force the memory to be free but it didn't work anyway, the python garbage collecting should be fine.\r\n\r\nThanks!",
        "createdAt":"2022-07-09T20:17:33Z",
        "number":3113616
       }
      ],
      "totalCount":4
     }
    },
    {
     "author":{
      "login":"Elliot-Wadge"
     },
     "body":"Since uproot.lazy is deprecated I found that setting object_cache=None in uproot.open() solved my problems and was the simplest solution\r\n",
     "createdAt":"2022-07-14T22:34:54Z",
     "number":3150692,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-07-09T06:15:14Z",
  "number":648,
  "title":"Memory management issues",
  "url":"https://github.com/scikit-hep/uproot5/discussions/648"
 },
 {
  "author":{
   "login":"scarsi99"
  },
  "body":"Hi everyone\r\n\r\nI would like to ask you how can I append data in a root file\r\nI noticed that there exist the extend method but it can only be used on a writing object (created with create/recreate). What if i open a tree with open/update and i would like to append some data to that file?\r\n\r\nDoes anyone has s solution for me?\r\n\r\nThanks in advance",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"It might be a disappointing answer, but at least I can save you some time by saying it: no, you can't extend an existing TTree. It's not a feature of the codebase (right now, and probably forever).\r\n\r\nWhen we create the TTree object ourselves, we can ensure that it's in a particular state, and therefore confidently extend it. But if we adopt TTrees made by ROOT, we have to fully understand all of its possible states, so that we don't take something from a state we don't understand into an erroneous state. Since a changing TTree is a moving target, it's not even easy to produce a repeatable error report.\r\n\r\n(That was the situation with objects in TDirectories\u2014in order to `uproot.update`, which Uproot 3 couldn't do, we had to fully understand ROOT's disk allocation algorithm and all the states it could get itself into. It's difficult because the testing examples we make ourselves likely don't represent all the possible ways files can be made.)",
     "createdAt":"2022-07-10T20:12:10Z",
     "number":3117067,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"scarsi99"
     },
     "body":"Hi\n\nThanks for your feedback. I can try to better explain my problem\nI am referring to data coming from High Energy Physics at CERN. I have a different file for each spill of particle, but I want to produce just one file for each data taking (which is composed of lot of spills)\nAt the moment I am able to produce a tree at the end of the run by first joining all the ascii files and then storing the data into the root file. But if I want to dynamically append data as they're produced, I real time, do you have any suggestion?\n\nI also took a look at uproot.concatenate, which can join different TTree object, even if I cannot completely understand how to exploit the returned object\n\nThanks in advance\n\nStefano Carsi\n\nDa: Jim Pivarski ***@***.***>\nInviato: domenica 10 luglio 2022 22:12\nA: scikit-hep/uproot5 ***@***.***>\nCc: CARSI STEFANO ***@***.***>; Author ***@***.***>\nOggetto: Re: [scikit-hep/uproot5] Appending data in a TTree object (Discussion #649)\n\n\nIt might be a disappointing answer, but at least I can save you some time by saying it: no, you can't extend an existing TTree. It's not a feature of the codebase (right now, and probably forever).\n\nWhen we create the TTree object ourselves, we can ensure that it's in a particular state, and therefore confidently extend it. But if we adopt TTrees made by ROOT, we have to fully understand all of its possible states, so that we don't take something from a state we don't understand into an erroneous state. Since a changing TTree is a moving target, it's not even easy to produce a repeatable error report.\n\n(That was the situation with objects in TDirectories-in order to uproot.update, which Uproot 3 couldn't do, we had to fully understand ROOT's disk allocation algorithm and all the states it could get itself into. It's difficult because the testing examples we make ourselves likely don't represent all the possible ways files can be made.)\n\n-\nReply to this email directly, view it on GitHub<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fscikit-hep%2Fuproot5%2Fdiscussions%2F649%23discussioncomment-3117067&data=05%7C01%7Cscarsi%40studenti.uninsubria.it%7Cf1a80c692b2a4abf028e08da62b07faa%7C9252ed8bdffc401c86ca6237da9991fa%7C0%7C0%7C637930807442145233%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=ii9eyl0I8Qgg2mxi%2F5AZp35nUnV4aU5XGWQN59hVOc0%3D&reserved=0>, or unsubscribe<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAYGAYATN2I63POJMVVW7PLDVTMVCLANCNFSM53E4LB7A&data=05%7C01%7Cscarsi%40studenti.uninsubria.it%7Cf1a80c692b2a4abf028e08da62b07faa%7C9252ed8bdffc401c86ca6237da9991fa%7C0%7C0%7C637930807442145233%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=6FKBeX7nDeDnywLqmLymFa5OYaYpxbu%2FHojanuZW4Hs%3D&reserved=0>.\nYou are receiving this because you authored the thread.Message ID: ***@***.******@***.***>>\n",
     "createdAt":"2022-07-11T07:26:03Z",
     "number":3119074,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"If the intermediate files are small, ASCII might not be a bad choice. Formats intended for large datasets are more efficient in the long run, but if you use them to make small datasets, they can be worse. ROOT files have a lot of sources of overhead, from the file header to TDirectories, to the TTree header.\r\n\r\nIf you want to go directly to a ROOT file, you can keep the process open. A single process can continue to accumulate data to an output file. But even then, it's possible to make one file inefficient by calling [uproot.TTree.extend](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableTree.html#extend) many times: each call to `extend` makes a new TBasket (an object within the file, with its own overhead)\u2014it's only efficient if TBaskets are large. So you'd have to fill arrays in memory, which might extend over multiple spills, before writing a large batch as a TBasket with `extend`. Compared to that, writing small ASCII files that you later collect may be simpler and more robust.\r\n\r\nWhat [uproot.concatenate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.concatenate.html) does is it reads multiple files and gives you a concatenated array of all their data. It does not write concatenated files. Of course, you could use it as a way to accumulate: if you've written many small ROOT files, `concatenate` would show them all to you as a single array and you could write that to an output file as a single TBasket with `extend`...",
        "createdAt":"2022-07-11T11:46:26Z",
        "number":3120685
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"scarsi99"
     },
     "body":"Dear Jim,\n\nthanks again for your response.\n\nActually ASCII files are not a good idea: we have something like 1000 good events per spill and for each events we acquire (at least) one 32 ch digitizer and for each channel we export 1024 pts for waveforms, so... lots of data\nThe idea of using a TTree object come for two reasons:\n\n  *   The need of compressing data\n  *   The TTree can be used both on python and ROOT\n\nIf I understand right, what you suggested me is to open with uproot.recreate at the beginning of the run and for each spill to call uproot.TTree.extend. I have a couple of questions\n\n  *   First of all, when data are actually stored? Each time I call the extend method, data are pushed into the file?\n  *   Since I plan to save data on a shared folder where all my colleagues can access, is it a problem if someone try to load a TTree file which is still open for writing in my script? If it's not a problem, will he be able to see data until the last call of extend? Actually, it would be nice if my colleagues could load from their script the TTree and as a new spill arrives, by recalling the same script they can access to the newer data. As I explain later, if they would have to load all the ascii, they cannot \"simply\" get rid of the waveform, instead if they can access a TTree, they can select which fields (I don't know the exact word, sorry) to access\n\nOtherwise, could you provide me a piece of code to write in a single TTree object what it is returned by uproot.concatenate? Is it so different from iterating over each TTree file and then use the extend method? Finally, someone reported me that ROOT has a special method \"hadd\" for \"appending data\" in a TTree object as I was wondering... do you have any further suggestions?\n\nIn conclusion, after my explanations, can I ask you what is your final suggestion?\nTake into account that waveforms data are most of the weights but usually no one want to take care of them (in the ASCII files are stored some other data, i.e. pulse height and timing information of the waveform, which are enough for most of the analysis).\n\nI also will take some performance tests in order to decide what is the best idea\n\nThanks again in advance\nKind regards\n\nStefano Carsi\n\nDa: Jim Pivarski ***@***.***>\nInviato: luned\u00ec 11 luglio 2022 13:47\nA: scikit-hep/uproot5 ***@***.***>\nCc: CARSI STEFANO ***@***.***>; Author ***@***.***>\nOggetto: Re: [scikit-hep/uproot5] Appending data in a TTree object (Discussion #649)\n\n\nIf the intermediate files are small, ASCII might not be a bad choice. Formats intended for large datasets are more efficient in the long run, but if you use them to make small datasets, they can be worse. ROOT files have a lot of sources of overhead, from the file header to TDirectories, to the TTree header.\n\nIf you want to go directly to a ROOT file, you can keep the process open. A single process can continue to accumulate data to an output file. But even then, it's possible to make one file inefficient by calling uproot.TTree.extend<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fuproot.readthedocs.io%2Fen%2Flatest%2Fuproot.writing.writable.WritableTree.html%23extend&data=05%7C01%7Cscarsi%40studenti.uninsubria.it%7C8f948dc89b7249549f6508da633306dd%7C9252ed8bdffc401c86ca6237da9991fa%7C0%7C0%7C637931368120743954%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=N%2BeBlK20ITy68BA5RRaxQ46BL8Xs%2FYdjSz4ApbSEPY4%3D&reserved=0> many times: each call to extend makes a new TBasket (an object within the file, with its own overhead)-it's only efficient if TBaskets are large. So you'd have to fill arrays in memory, which might extend over multiple spills, before writing a large batch as a TBasket with extend. Compared to that, writing small ASCII files that you later collect may be simpler and more robust.\n\nWhat uproot.concatenate<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fuproot.readthedocs.io%2Fen%2Flatest%2Fuproot.behaviors.TBranch.concatenate.html&data=05%7C01%7Cscarsi%40studenti.uninsubria.it%7C8f948dc89b7249549f6508da633306dd%7C9252ed8bdffc401c86ca6237da9991fa%7C0%7C0%7C637931368120743954%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=4qTCUAyGmr9MhrWWQFswif28zaFlQ9sDPiXCub6RItA%3D&reserved=0> does is it reads multiple files and gives you a concatenated array of all their data. It does not write concatenated files. Of course, you could use it as a way to accumulate: if you've written many small ROOT files, concatenate would show them all to you as a single array and you could write that to an output file as a single TBasket with extend...\n\n-\nReply to this email directly, view it on GitHub<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fscikit-hep%2Fuproot5%2Fdiscussions%2F649%23discussioncomment-3120685&data=05%7C01%7Cscarsi%40studenti.uninsubria.it%7C8f948dc89b7249549f6508da633306dd%7C9252ed8bdffc401c86ca6237da9991fa%7C0%7C0%7C637931368120743954%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=TMsRpbryqAPFXoCK5Mir8UASZP6boc1E3iYD8TPPh6Q%3D&reserved=0>, or unsubscribe<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAYGAYARH6S6UKHHNXHQTX53VTQCR5ANCNFSM53E4LB7A&data=05%7C01%7Cscarsi%40studenti.uninsubria.it%7C8f948dc89b7249549f6508da633306dd%7C9252ed8bdffc401c86ca6237da9991fa%7C0%7C0%7C637931368120743954%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=5pI9098gTYFr%2B%2BDMB4ZCwG14CGINoGYmaZb9nEGbmME%3D&reserved=0>.\nYou are receiving this because you authored the thread.Message ID: ***@***.******@***.***>>\n",
     "createdAt":"2022-07-12T08:04:01Z",
     "number":3127206,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Data are actually stored when the `extend` method is called. The ROOT file is in an invalid state for the shortest possible time during that method call, and we insert data before inserting pointers to it, so that the TTree object is valid but not up-to-date during most of the writing. If another reader reads old data, it should be okay.\r\n\r\nSince the data associated with each spill is big, what about putting each spill in a separate TTree?\r\n\r\nI was going to suggest HDF5 (designed for sharing big arrays, can be used in C++ and Python) until I noticed you said \"at least\" one channel, which sounds like you have variable-length data.\r\n\r\nSince the data associated with each spill is big, what about one TTree per spill, which is readable while collecting spills and can be concatenated later. A TTree has a lot of header metadata associated with it, so it would be a bad idea to fill a new TTree for a small amount of data, but you spill datasets are big.\r\n\r\nTake these as suggestions and try it out. If it were my own problem, I wouldn't settle on a method until I tried some things and found out just how big the data are compared with the for headers.",
     "createdAt":"2022-07-12T12:19:28Z",
     "number":3128836,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"scarsi99"
     },
     "body":"Dear Jim,\n\nthanks again for your help\n\nOf course I will do some tries to decide what is the best solution for me, based on your suggestions\n\nFor your information, I do not have variable-length data, my ascii are always a \"matrix\": unknown amount of lines, can vary spill per spill, but always the same amount of columns. I was referring to the fact that for different setup we can have one or two digitizer (32 or 64 channel) but does not vary during a data taking\n\nJust to conclude, I promise I won't bother you again, when you suggest me to put in different TTree, do you mean to put in different directoryes of the same object (which I can do with the update method) or to create different files, one per each spill?\nMy concerns are that if I create a different file for each spill and then merging in a separate moment, my colleagues have to use different files for the same data\n\nIt is a real pity I cannot open an existing file and simply calling the extend method, but I understand the technical complications\n\nBest regards\n\nStefano Carsi\n\nDa: Jim Pivarski ***@***.***>\nInviato: marted\u00ec 12 luglio 2022 14:20\nA: scikit-hep/uproot5 ***@***.***>\nCc: CARSI STEFANO ***@***.***>; Author ***@***.***>\nOggetto: Re: [scikit-hep/uproot5] Appending data in a TTree object (Discussion #649)\n\n\nData are actually stored when the extend method is called. The ROOT file is in an invalid state for the shortest possible time during that method call, and we insert data before inserting pointers to it, so that the TTree object is valid but not up-to-date during most of the writing. If another reader reads old data, it should be okay.\n\nSince the data associated with each spill is big, what about putting each spill in a separate TTree?\n\nI was going to suggest HDF5 (designed for sharing big arrays, can be used in C++ and Python) until I noticed you said \"at least\" one channel, which sounds like you have variable-length data.\n\nSince the data associated with each spill is big, what about one TTree per spill, which is readable while collecting spills and can be concatenated later. A TTree has a lot of header metadata associated with it, so it would be a bad idea to fill a new TTree for a small amount of data, but you spill datasets are big.\n\nTake these as suggestions and try it out. If it were my own problem, I wouldn't settle on a method until I tried some things and found out just how big the data are compared with the for headers.\n\n-\nReply to this email directly, view it on GitHub<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fscikit-hep%2Fuproot5%2Fdiscussions%2F649%23discussioncomment-3128836&data=05%7C01%7Cscarsi%40studenti.uninsubria.it%7Cadc3210787b64a905e6008da6400cbdf%7C9252ed8bdffc401c86ca6237da9991fa%7C0%7C0%7C637932251833203877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=4c7Xv1ix1O1E5LNO16lceHsheeQy4Q9QgbUlTbW5kTk%3D&reserved=0>, or unsubscribe<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAYGAYAQILBKQJP3FJTGMDSLVTVPFXANCNFSM53E4LB7A&data=05%7C01%7Cscarsi%40studenti.uninsubria.it%7Cadc3210787b64a905e6008da6400cbdf%7C9252ed8bdffc401c86ca6237da9991fa%7C0%7C0%7C637932251833203877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=%2FGcnMugQPuelGOD2f5ojMnzXOAsFyjOg5fKyB1dkCa0%3D&reserved=0>.\nYou are receiving this because you authored the thread.Message ID: ***@***.******@***.***>>\n",
     "createdAt":"2022-07-12T12:35:11Z",
     "number":3128923,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"I meant that different TTrees in the same file (different names in the TDirectory) or different TTrees in different files are both options. Considering that you want people to be able to read this while it's being written, it will be more robust if it is more granular, anyway. Reading an object that is in the process of being updated is inherently unstable\u2014it can be solved by \"locking\" (preventing readers from reading while the data are being written) and also by distinguishing between old objects, which can be freely read, and new objects, which are in the process of being read and are not ready yet.\r\n\r\nSince you say that the data have fixed size, HDF5 becomes a more much attractive option. The format and ecosystem of tools around it are very mature, and it addresses your exact problems. See, for instance, [h5cpp](http://h5cpp.org/) for C++ and [h5py](https://www.h5py.org/) for Python. The compression options are generalized into a \"filter pipeline\" of algorithms to apply to the data between when it's in memory and when it's on disk, there are workflows for shared reading while writing in chunks (the \"old data/new data\" split I described above, but presented as a view of a single large array), and it's widely used in supercomputing applications. (All the references to \"MPI\" are for massive scale-out of non-embarrassingly parallel tasks\u2014overkill for most HEP problems.) The one major drawback of HDF5 is that they only consider rectangular arrays (even sparse and chunked arrays, which are variable-length on disk, are at least logically rectangular). But that happens to fit your use-case, so when you're exploring options, it would be a good idea to keep HDF5 in mind.",
     "createdAt":"2022-07-12T13:34:40Z",
     "number":3129412,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":6
  },
  "createdAt":"2022-07-10T11:39:25Z",
  "number":649,
  "title":"Appending data in a TTree object",
  "url":"https://github.com/scikit-hep/uproot5/discussions/649"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"This is the first 5.0 (pre) release!\r\n\r\nIt should _not_ be considered stable, and it strictly _requires_ Awkward 2.0, so you need to have the latest pre-release of that. This may be reflected in the lazy import of `awkward`; if not, a subsequent 5.0 pre-release will get that right.\r\n\r\nChanges relative to any 4.x release:\r\n\r\n@kkothari2001 removed the `uproot.lazy` function (though a stub remains for the sake of keeping the documentation until 5.0 is actually released): PR #615. Updated the entire codebase from Awkward 1.x to Awkward 2.x: PR #620.\r\n\r\n@aryan26roy changed arguments in the `awkward_form` method: PR #617. (@jpivarski fixed a bug in it: PR #622.) @aryan26roy completed an AwkwardForth-based reader for `AsStrings`: PR #616. Made the Python code generation in streamers.py more readable: #629. Revised the way that the AwkwardForth code generator would pass through the control flow: PR #636.\r\n\r\n@henryiii added dependabot for GitHub Actions: PR #631. Cleaned up flake8: PR #527.\r\n\r\n@jpivarski fixed a performance bug, to allow linear-time iteration over TDirectory fields: PR #638 (backported to v4 as well). Set up as close as possible to full-coverage tests for AwkwardForth: #637.\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/5.0.0rc1'>5.0.0rc1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-07-22T17:04:24Z",
  "number":653,
  "title":"5.0.0rc1",
  "url":"https://github.com/scikit-hep/uproot5/discussions/653"
 },
 {
  "author":{
   "login":"artlbv"
  },
  "body":"I'm looking at CMS MiniAOD files using uproot and in principle it works fine for many cases (non-CMS branches of course), but there is one particular case where the uproot flattening loses some information:\r\nwhen there are 2D arrays (i.e. array in array) both dimensions are flattened together and the 2D index becomes 1D. \r\nThe problem is that the 1st layer index actually corresponds to an important feature (the bunch-crossing [BX] number), whereas the second is just the index in the array. Given that not all events have an equal number of BX filled (and the # of objects differs) one cannot restore the BX index after flattening. \r\n\r\nExample of C++ analysis code: https://github.com/lathomas/JetMETStudies/blob/master/JMEAnalyzer/plugins/JMEAnalyzer.cc#L859-L872\r\n\r\nPrintout of uproot's `show`:\r\n```\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nl1tMuonBXVector_g... | edm::Wrapper<BXVector... | AsGroup(<TBranchElement 'l1...\r\nl1tMuonBXVector_g... | bool                     | AsDtype('bool')\r\nl1tMuonBXVector_g... | (group of l1tMuonBXVe... | AsGroup(<TBranchElement 'l1...\r\nl1tMuonBXVector_g... | int32_t                  | AsDtype('>i4')\r\nl1tMuonBXVector_g... | int32_t                  | AsDtype('>i4')\r\nl1tMuonBXVector_g... | vector<l1t::Muon>        | AsGroup(<TBranchElement 'l1...\r\nl1tMuonBXVector_g... | float[]                  | AsJagged(AsDtype('>f4', 'fl...\r\nl1tMuonBXVector_g... | float[]                  | AsJagged(AsDtype('>f4', 'fl...\r\n...\r\nl1tMuonBXVector_g... | double[]                 | AsJagged(AsDtype('>f8'))\r\nl1tMuonBXVector_g... | int32_t[]                | AsJagged(AsDtype('>i4'))\r\nl1tMuonBXVector_g... | std::vector<uint32_t>    | AsJagged(AsDtype('>u4'), he...\r\n```\r\n\r\n(By the way, is there a way to display the full branch name for show? this is quite inconvenient for long branch names (as they typically are in MiniAOD..)\r\n\r\n## Example processing in uproot:\r\n\r\nOpen arrays\r\n```python\r\npattern = \"l1tMuonBXVector_gmtStage2Digis_Muon_RECO.\"\r\npattern += \"/l1tMuonBXVector_gmtStage2Digis_Muon_RECO.obj\"\r\npattern += \"/l1tMuonBXVector_gmtStage2Digis_Muon_RECO.obj\"\r\n\r\ngmt_muons = f[\"Events].arrays(\r\n    filter_name = pattern+\"*\", \r\n    entry_stop = 10, \r\n)\r\n```\r\n\r\nPrint out `pt`: \r\n```\r\n$: ak.to_list(gmt_muons[\"m_state.p4polar_.fcoordinates.fpt\"][:10])\r\n\r\n[[88.5],\r\n [36.0],\r\n [44.5, 12.0, 2.5],\r\n [54.0],\r\n...\r\n```\r\n\r\nI actually found a potentially useful branch (the last in the `show` output which is the vector of ints):\r\n`'l1tMuonBXVector_gmtStage2Digis_Muon_RECO.obj.itrs_'`\r\n\r\nIt contains:\r\n```\r\n[[0, 0, 0, 1, 1],\r\n [0, 0, 0, 1, 1],\r\n [0, 0, 0, 3, 3],\r\n...\r\n```\r\n\r\nWhen I print out both `pt` and this `iters`:\r\n```\r\n[{'fPt': [88.5],                                                                                                                                                                                            \r\n  '.itrs_': [0, 0, 0, 1, 1]},                                                                                                                                                                               \r\n {'fPt': [36.0],                                                                                                                                                                                            \r\n  '.itrs_': [0, 0, 0, 1, 1]},                                                                                                                                                                               \r\n {'fPt': [44.5,  12.0,  2.5],                                                                                                                                                                                                    \r\n  '.itrs_': [0, 0, 0, 3, 3]},                                                                                                                                                                               \r\n {'fPt': [54.0],                                                                                                                                                                                            \r\n  '.itrs_': [0, 0, 0, 1, 1]},                                                                                                                                                                               \r\n {'fPt': [49.5],                                                                                                                                                                                            \r\n  'itrs_': [0, 0, 0, 1, 1]},                                                                                                                                                                                \r\n {'fPt': [77.0],                                                                                                                                                                                            \r\n  'itrs_': [0, 0, 0, 1, 1]},                                                                                                                                                                                \r\n {'fPt': [43.0, 13.0,   7.0],                                                                                                                                                                                                    \r\n  'itrs_': [0, 0, 0, 3, 3]},                                                                                                                                                                                \r\n {'fPt': [36.0,    2.0,  1.0],                                                                                                                                                                                                    \r\n  'itrs_': [0, 0, 0, 2, 3]},                                                                                                                                                                                \r\n {'fPt': [101.5],                                                                                                                                                                                           \r\n  'itrs_': [0, 0, 0, 1, 1]},                                                                                                                                                                                \r\n {'fPt': [108.5,  10.5],                                                                                                                                                                                                   \r\n  'itrs_': [0, 0, 0, 1, 2]}]  \r\n```\r\n\r\nI get the impression that this `iters` gets incremented in the element after the element that had content.\r\nIf that is true (more a CMS-specific question) then I can probably decode the first-level index from `iters` (need to think about an elegant way to do that with `ak`).\r\n\r\nBut it would be nice from `uproot` to somehow also give a handle on the index it decoded.\r\n\r\nP.s. here is the CMS internal file `/eos/cms/tier0/store/data/Run2022C/SingleMuon/MINIAOD/PromptReco-v1/000/355/872/00000/0fa8e1c9-699c-48b8-8622-1c42f7c0dfaa.root`\r\nSomehow these branches are not in the OpenData release unfortunately.",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-07-28T07:52:17Z",
  "number":655,
  "title":"Retrieve index from nested arrays (2D) in ROOT files",
  "url":"https://github.com/scikit-hep/uproot5/discussions/655"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"@kakwok fixed uninitialized attributes of ReadOnlyDirectory: PR #660 (v5) and #661 (v4).\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/4.3.4'>4.3.4</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-08-04T15:26:46Z",
  "number":664,
  "title":"4.3.4",
  "url":"https://github.com/scikit-hep/uproot5/discussions/664"
 },
 {
  "author":{
   "login":"kgizdov"
  },
  "body":"I've noticed a trend that changing the major version of the `uproot` library results in repo name changes, source tree name changes, packaging changes, hardcoded test string changes and others, which is unnecessary and makes it very difficult to work with the package.\r\n\r\nI don't see why `uproot` doesn't just do what every other package, whether Python or not, does - major versions with breaking changes are listed, and people who rely on it limit their requirements.\r\n\r\nThis is also extensively covered in PEP rules.\r\n\r\nOn top of all that, it goes against the principles behind having a centralised version control system. Major version histories get disconnected, it's impossible to follow the changes, etc. But I think the biggest, in my opinion, is it discourages people from contributing because it's cumbersome.\r\n\r\nEven complete rewrites across major versions do not necessitate new repositories if it's the same project. It would make life much easier for users and contributors if `uproot` remained `uproot` everywhere and only its version was changed. :)",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"Moelf"
     },
     "body":"I second this just because I come from an ecosystem that uses semVer so major release is \"just another release\" on GitHub.\n\nBut otherwise I don't see it as a big deal ",
     "createdAt":"2022-08-06T16:38:30Z",
     "number":3340154,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"After the mistake with Uproot 3 \u2192 4, the PyPI package and GitHub repo have not changed. Uproot 4 and Uproot 5 will use the same PyPI package and do use the same GitHub repo as each other; only the major version number changes. The GitHub repo _name_ was changed from \"`uproot4`\" to \"`uproot5`\" with redirects (URLs don't have to change) because the name \"`uproot4`\" is misleading and \"`uproot`\" is unavailable, due to it being a redirect to \"`uproot3`\".\r\n\r\n(Incidentally, there was _reasoning_ behind the Uproot 3 \u2192 4 mistake, though on the whole it was a bad thing to do. Having two PyPI names made it possible to load both major versions in the same Python session, a feature that I already needed and could not get with Uproot 4 \u2192 5 because I'm following this best practice. Of course, giving up on that is the lesser of two evils. The git repo was changed in Uproot 3 \u2192 4 because Uproots 1 \u2192 2 \u2192 3 had too many commits, some of them involving large files, and that made cloning the repo slow. That problem could have been better fixed by manipulating git history, which I generally try to avoid.)\r\n\r\nBut anyway, this is not a trend because Uproot 4 \u2192 5 does not change PyPI names or git repos.",
     "createdAt":"2022-08-06T18:04:40Z",
     "number":3340505,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"kgizdov"
     },
     "body":"It does change the git repos:\r\n```\r\n==> Retrieving sources...\r\n  -> Downloading uproot-4.3.5...\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r\n  0    14    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r\ncurl: (22) The requested URL returned error: 404\r\n==> ERROR: Failure while downloading https://github.com/scikit-hep/uproot4/archive/v4.3.5.tar.gz\r\n    Aborting...\r\n==> ERROR: Failed to generate new checksums\r\n```\r\nMaybe people are unaware, but GitHub now has an entirely new repository and only an HTML 301 web redirect, nothing else. No repo mirroring is happening.\r\n\r\nAnd I'm afraid this will have more yet unforeseen side effects. This v5 should have been a branch, not a new repo, a rename, or something else of this sort.\r\n\r\nI raised the same issue when this project was in its infancy and again when the messy 2->3 and 3-4 versions happened, and at no point, the reasons or even the final goal justified this sort of thing. Git branches and package versioning exist for a reason.",
     "createdAt":"2022-09-23T14:00:57Z",
     "number":3717589,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Not just for Awkward/Uproot, but for other Scikit-HEP projects, it's common to move a git repo to a new URL. The primary use-case for that is when someone develops a project under their own account, `github-user/awesome-project`, and then wants to transfer it to Scikit-HEP: `scikit-hep/awesome-project`. This may be a new git repo (not two mirrored git repos), but it preserves the git history, all the GitHub metadata, and sets up redirect links from old to new.\r\n\r\nI just tried downloading v4.3.5.tar.gz from uproot4 and uproot5, and indeed there's a redirect there:\r\n\r\n```\r\n% wget https://github.com/scikit-hep/uproot5/archive/refs/tags/v4.3.5.tar.gz              \r\n--2022-09-23 09:38:35--  https://github.com/scikit-hep/uproot5/archive/refs/tags/v4.3.5.tar.gz\r\nResolving github.com (github.com)... 140.82.114.3\r\nConnecting to github.com (github.com)|140.82.114.3|:443... connected.\r\nHTTP request sent, awaiting response... 302 Found\r\nLocation: https://codeload.github.com/scikit-hep/uproot5/tar.gz/refs/tags/v4.3.5 [following]\r\n--2022-09-23 09:38:36--  https://codeload.github.com/scikit-hep/uproot5/tar.gz/refs/tags/v4.3.5\r\nResolving codeload.github.com (codeload.github.com)... 140.82.113.10\r\nConnecting to codeload.github.com (codeload.github.com)|140.82.113.10|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: unspecified [application/x-gzip]\r\nSaving to: \u2018v4.3.5.tar.gz\u2019\r\n\r\nv4.3.5.tar.gz                   [ <=>                                        ] 788.19K  4.48MB/s    in 0.2s    \r\n\r\n2022-09-23 09:38:36 (4.48 MB/s) - \u2018v4.3.5.tar.gz\u2019 saved [807110]\r\n```\r\n\r\nand\r\n\r\n```\r\n% wget https://github.com/scikit-hep/uproot4/archive/refs/tags/v4.3.5.tar.gz \r\n--2022-09-23 09:38:54--  https://github.com/scikit-hep/uproot4/archive/refs/tags/v4.3.5.tar.gz\r\nResolving github.com (github.com)... 140.82.114.3\r\nConnecting to github.com (github.com)|140.82.114.3|:443... connected.\r\nHTTP request sent, awaiting response... 301 Moved Permanently\r\nLocation: https://github.com/scikit-hep/uproot5/archive/refs/tags/v4.3.5.tar.gz [following]\r\n--2022-09-23 09:38:54--  https://github.com/scikit-hep/uproot5/archive/refs/tags/v4.3.5.tar.gz\r\nReusing existing connection to github.com:443.\r\nHTTP request sent, awaiting response... 302 Found\r\nLocation: https://codeload.github.com/scikit-hep/uproot5/tar.gz/refs/tags/v4.3.5 [following]\r\n--2022-09-23 09:38:54--  https://codeload.github.com/scikit-hep/uproot5/tar.gz/refs/tags/v4.3.5\r\nResolving codeload.github.com (codeload.github.com)... 140.82.113.10\r\nConnecting to codeload.github.com (codeload.github.com)|140.82.113.10|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: unspecified [application/x-gzip]\r\nSaving to: \u2018v4.3.5.tar.gz\u2019\r\n\r\nv4.3.5.tar.gz                   [ <=>                                        ] 788.19K  4.38MB/s    in 0.2s    \r\n\r\n2022-09-23 09:38:54 (4.38 MB/s) - \u2018v4.3.5.tar.gz\u2019 saved [807110]\r\n```\r\n\r\nAs you point out, there's a [HTTP 301: moved permanently](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/301) in the logs. But that's what we want: HTTP clients know to follow redirects to a new URL, just like they have to follow the [HTTP 302: found](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/302), which happens for both URLs.\r\n\r\nOne rule that we adhere to when we use GitHub's \"[rename repo](https://docs.github.com/en/repositories/creating-and-managing-repositories/renaming-a-repository)\" feature is that we don't move a repo into a URL that has ever been occupied by another repo, because then URLs to issue and PR numbers would change their meaning. Renames need to go to a URL that has never been used before. This is why \"scikit-hep/uproot4\" couldn't become just \"scikit-hep/uproot\", as I would have really, really preferred, and why \"scikit-hep/awkward-1.0\" couldn't become \"scikit-hep/awkward-array\", though the solution of using \"scikit-hep/awkward\" is a satisfying one (repo name is the same as the PyPI and conda-forge package name).\r\n\r\nVersions and git-branches are best practices, and we use those. I had no reason to suspect (still don't) that GitHub's \"rename repo\" feature is something to be avoided, provided that we rename to a new URL that has never been used. I wouldn't expect it to make a mirror repo (which would probably be worse than a redirect, in terms of keeping distributed data consistent), I expect it to make HTTP redirects, which it does, and I don't see what's wrong with HTTP redirects.\r\n\r\nIf there is something wrong with HTTP redirects, then we would also need to rethink how we adopt `github-user/awesome-project` into `scikit-hep/awesome-project` when new projects join Scikit-HEP. Keeping them always under personal `github-user` accounts is not a good way for projects to evolve from single-author ideas into community-maintained resources.\r\n\r\n> I raised the same issue when this project was in its infancy and again when the messy 2->3 and 3-4 versions happened, and at no point, the reasons or even the final goal justified this sort of thing. Git branches and package versioning exist for a reason.\r\n\r\n(Not 2 \u2192 3; that was an ordinary version bump from [2.9.11](https://github.com/scikit-hep/uproot3/releases/tag/2.9.11) to [3.0.0b1](https://github.com/scikit-hep/uproot3/releases/tag/3.0.0b1), and it happened in August 2018.)\r\n\r\nFor reference, [here is your message](https://groups.google.com/g/scikit-hep-admins/c/RRQXCUKn3sQ/m/b9kcBQmXAQAJ) from February 2021, when you raised this issue before. I gave a long explanation about why we did that, which ended with\r\n\r\n> I think I wouldn't do it that way again.\r\n\r\nAnd so we didn't. Uproot version 5 is right now a git branch (`main` versus the legacy `main-v4`) and it is being released as an rc series, starting with [5.0.0rc1](https://github.com/scikit-hep/uproot5/releases/tag/5.0.0rc1) (I just released [5.0.0rc3](https://github.com/scikit-hep/uproot5/releases/tag/v5.0.0rc3) this morning). In early December, it will be released as 5.0.0. Independent of that, the git repo's URL was changed to not be misleading, and there is an HTTP redirect so that all old URLs continue to work as before.\r\n\r\nWe did not make a version 5 git repo without version 4 history (as we did with 4 and 3, for the reasons I described in that email thread), and we did not make new PyPI package names. I'm still glad we've buried the Uproot 1\u20123 history in an archived repo, but the PyPI package names was definitely a problem that was not justified by the reasons we had for it. You could call it a failed experiment. If anybody's thinking of doing something similar, I can show them concrete reasons why they shouldn't. But in particular, _we didn't_ do it again.",
        "createdAt":"2022-09-23T15:35:00Z",
        "number":3718309
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"kgizdov"
     },
     "body":"Firstly, I understand all that and am glad about the bits that didn't happen this time.\r\n\r\nHowever, I feel like I am misrepresenting what I mean. I am not looking for an explanation. Forgive me for being blunt. I guess I must be in order to get my point across.\r\n\r\nI am a member of Scikit-HEP with a package of my own, so I know how organisation adoption works. That's not the issue at the moment.\r\n\r\nSecondly, I am not disagreeing with the methods employed to rename the repo.\r\n\r\nMy point is that this should not be done at all, and I can see no justification for why it is being done.\r\n\r\nFor example, when Scikit-HEP adds a repo, it changes the repo owner; it does not change the name from `mypackage` to `mypackage2.0-best-version-ever_final_v2`. Specifically, repo owner change is a proper justification for why the URL is being changed.\r\n\r\nThirdly, I do not particularly care about the URL itself. The issue is deeper and more conceptual.\r\n\r\nFinally, the idea that `uproot4` moving to `uproot5` is somehow futureproofing `uproot6` and so on is ridiculous. There should be a repo with a name that does not contain the major version number and represents all past, current and future work done on `uproot`. That is the proper way of doing things. And by the proper way, I mean literally, everyone else does this for numerous reasons, the most important of which is - they have tried and eventually failed to do the opposite. That is why we have now arrived with versioning policies spanning every language, packaging tool, software ecosystem, etc.",
     "createdAt":"2022-09-23T16:25:44Z",
     "number":3718702,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"Okay, so it's just the choice of name, `scikit-hep/uproot5`, then. I get that: it would be best if the repo did not have a version number in it at all.\r\n\r\nWhat had to happen was that it could not be named `scikit-hep/uproot4` anymore. Then it would have a wrong version number in the name, and as bad as it is to have a version number in the name, it's worse to have a wrong version number in the name. Someone looking at that might think, \"Surely I've come to the wrong site...\"\r\n\r\nIt also could not be named `scikit-hep/uproot` because the Uproot 1\u20123 repo once had that URL. I strongly considered doing that, on the grounds that \"uproot\" is what it should be called and the original URL had been changed for more than a year at that point; Google seemed to have reindexed. But in the end, clobbering a global namespace of URLs is the same type of error as clobbering a global namespace of PyPI names, and we know that's bad.\r\n\r\nI asked about possible names on Gitter and Slack many months in advance of doing it (no response). I don't want to keep moving from `uproot5` to `uproot6`[^1]: it's unaesthetic, though it doesn't do damage because HTTP redirects work and none of these URLs have been taken. If you can think of an alternative, satisfying the constraints:\r\n\r\n   * not a URL we've used before\r\n   * not including a version number in the URL, especially not a wrong one\r\n   * something easily guessed or not too weird for human eyes when they see it\r\n\r\nthen we should consider it.\r\n\r\n[^1]: I don't see any reason why there would be an Uproot 6 in the future: the codebase seems to be converging. The 4 \u2192 5 changes wouldn't be significant enough to bump the major version number if it weren't for the scale of changes in the Awkward codebase, which is also converging, albeit later than Uproot did.",
        "createdAt":"2022-09-23T17:17:13Z",
        "number":3719040
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":4
  },
  "createdAt":"2022-08-06T15:47:23Z",
  "number":666,
  "title":"Adhere to Python Naming Conventions",
  "url":"https://github.com/scikit-hep/uproot5/discussions/666"
 },
 {
  "author":{
   "login":"roy-brener-cern"
  },
  "body":"Dear `uproot` community,\r\n\r\nIs there a way to open a `.root` file with `uproot` in cases where the file is symbolically-linked to its actual location (\"target\") on another sever/location?\r\nIn standard `ROOT` for example, there is an option of using `TDavixFile`, _e.g._\r\n(in shell)\r\n`ls -l root_file.root`\r\nto obtain the symbolic link target, and then in (py)ROOT\r\n```\r\nimport ROOT\r\nfile = ROOT.TDavixFile(symbolic_link_target)\r\n```\r\n\r\n(`TDavixFile` is used in such cases instead of `TFile`)\r\n\r\nThis works fine with `ROOT`. I've not found a parallel option in `uproot`, but perhaps the option exists somewhere/how...?\r\nAny help or suggestion would be warmly welcomed.\r\n\r\nMany thanks in advance,\r\n\r\nRoy",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"If it's a filesystem symbolic link, then you can just use the link file itself:\r\n\r\n```\r\n$ ls -l root_file.root\r\nlrwxrwxrwx  1 i_made this_up   1066 Oct 14 21:46 root_file.root -> /elsewhere/in/the/filesystem/here.root\r\n```\r\n\r\n```python\r\n>>> import uproot\r\n>>> with uproot.open(\"root_file.root\") as file:\r\n...\r\n```\r\n\r\nThat is to say, let the filesystem do its thing with symbolic links. (Uproot isn't handling that; when Python asks for a file at `root_file.root`, it gets data from the linked file. ROOT would do that, too, with its ordinary `TFile`.)\r\n\r\nBut if you need to bring `TDavixFile` into it, then you're not talking about ordinary symbolic links, the kind that can be identified with `ls -l`. Uproot doesn't have a Davix handler. If you're just using Davix to follow a `root://` or `https://` URL, then use the URL because Uproot _does_ have XRootD and HTTP handlers, but not Davix, specifically.\r\n",
     "createdAt":"2022-08-08T18:26:31Z",
     "number":3351840,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"roy-brener-cern"
     },
     "body":"Hi Jim,\r\n\r\nMany thanks for your quick reply.\r\n\r\n`uproot.open(\"root_file.root\")` doesn't work for me as it's indeed the other kind of symbolic link _i.e._ to a different file system.\r\n\r\nActually, I **can** identify the target of it using `ls -l root_file.root` but to be very specific my full command is\r\n`ls -l root_file.root | awk '{print $NF}' | xargs -i echo \"davs://lcg-se01.icepp.jp:18443{}\"`\r\nwhere `davs://lcg-se01.icepp.jp:18443` is the path to the target file system in which `root_file.root` is actually stored which I need to add as to point to the file location.\r\nThis works with `TDavixFile` (I don't know exactly what that protocol is except that it works --- its documentation seems a bit lacking https://root.cern/doc/master/namespaceDavix.html). If you have any suggestion how to make this work with uproot I would be very grateful.\r\n\r\nKind regards,\r\n\r\nRoy",
     "createdAt":"2022-08-08T19:12:55Z",
     "number":3352187,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I don't know much about the DaviX protocol ([docs](https://davix.web.cern.ch/davix/docs/devel/), [GitHub](https://github.com/cern-fts/davix), [Wikipedia](https://en.wikipedia.org/wiki/DaviX)). I just learned that you're supposed to capitalize the \"X\"!\r\n\r\nIf the actual file-reading resolves to an HTTP or XRootD URL, then you can use that directly in the [uproot.open](https://uproot.readthedocs.io/en/latest/uproot.reading.open.html) function. I'm going to leave this open for anybody who has more expertise and might be able to say how DaviX relates to HTTP or XRootD.\r\n\r\nIf we really need a new \"DavixSource\" in Uproot, that's a medium-sized undertaking. Uproot currently has two remote protocols:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/02c4cd8ab61460cebf4bf67da4ec2fcf7d175a3f/src/uproot/source/http.py#L532\r\n\r\nand\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/02c4cd8ab61460cebf4bf67da4ec2fcf7d175a3f/src/uproot/source/xrootd.py#L250\r\n\r\nA DavixSource would have to link Uproot's `Source` protocol to DaviX requests. Something like that would take at least a week and at most a month for someone unfamiliar with the code.\r\n\r\nLooking at the DaviX documentation (and [PyPI](https://pypi.org/search/?q=davix)), I'm not finding any Python bindings. Having to also create Python bindings for DaviX would make this a somewhat-larger-than-medium-sized project. It could potentially be done through the [command-line tools](https://davix.web.cern.ch/davix/docs/devel/cli-examples.html), but that would be glitchy and probably doesn't support [multi-range queries](https://davix.web.cern.ch/davix/docs/devel/advanced.html#multi-range-queries), which is an important feature.\r\n\r\nYou _might_ need to wait until someone writes a quick pybind11 binding for it (or convince someone to do that) if they really don't have a Python interface. It would be a well-scoped student project.",
        "createdAt":"2022-08-08T19:42:54Z",
        "number":3352345
       },
       {
        "author":{
         "login":"roy-brener-cern"
        },
        "body":"Hi Jim,\r\n\r\nI understand.\r\n\r\nAlas, `uproot.open` doesn't seem to work with this `davs://lcg-se01.icepp.jp:18443` path/protocol. If you're on CERN `lxplus` I would be happy to provide some example files for you to try, but given your comments and my trials I'm doubtful anything would change without updates to `uproot` like the ones you've suggested be applied.\r\n\r\nIn the meantime we'll try to move these files to somewhere accessible with `uproot.open`.\r\n\r\nThanks,\r\n\r\nRoy",
        "createdAt":"2022-08-09T07:34:17Z",
        "number":3355518
       }
      ],
      "totalCount":2
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-08-08T18:12:34Z",
  "number":667,
  "title":"Access symbolic linked .root files with uproot",
  "url":"https://github.com/scikit-hep/uproot5/discussions/667"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n* feat: `from_map` like optimization for dask arrays by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/679\r\n* feat: Finalizing AwkwardForth reader for Uproot by @aryan26roy in https://github.com/scikit-hep/uproot5/pull/644\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: Avoid triggering temporary dask-awkward/awkward incompatibility. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/694\r\n* fix: Do not write incorrect fSumw2 in histograms (v5). by @jpivarski in https://github.com/scikit-hep/uproot5/pull/698\r\n* fix: Fixes uproot.dask bug with empty branches by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/700\r\n* fix: Use `from_map` optimization for delayed numpy arrays and add tests with empty branches for the same by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/703\r\n* fix: use ctx manager to ensure resources are freed by @agoose77 in https://github.com/scikit-hep/uproot5/pull/713\r\n* fix: ReadOnlyDirectory should provide the largest abs(cycle) when cycle is unspecified, not the largest cycle. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/715\r\n* fix: regularize ROOT type aliases to C fundamental type names. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/717\r\n\r\n## Other\r\n\r\n* test: adjust for boost-histogram 1.3.2's _storage_type deprecation. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/719\r\n* ci: Lint PR titles according to conventional commits by @jpivarski in https://github.com/scikit-hep/uproot5/pull/689\r\n* ci: use concurrency group for `semantic-pr-title` by @agoose77 in https://github.com/scikit-hep/uproot5/pull/691\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/706\r\n* docs: Installation requirements and error text for dask/dask-awkward in extras.py by @jpivarski in https://github.com/scikit-hep/uproot5/pull/690\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/716\r\n* docs: Add image for dask docs by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/708\r\n* docs: add veprbl as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/728\r\n* docs: add nikoladze as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/729\r\n* docs: add klieret as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/730\r\n* docs: add dcervenkov as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/731\r\n* docs: add beojan as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/732\r\n* docs: add agoose77 as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/733\r\n* docs: add a CITATION.cff for Uproot. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/726\r\n* chore: dask_awkward.test_utils moved in ContinuumIO/dask-awkward#76 by @jpivarski in https://github.com/scikit-hep/uproot5/pull/714\r\n\r\n## New Contributors\r\n* @agoose77 made their first contribution in https://github.com/scikit-hep/uproot5/pull/691\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/uproot5/compare/5.0.0rc2...v5.0.0rc3\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/v5.0.0rc3'>Version 5.0.0rc3</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-09-23T13:52:11Z",
  "number":735,
  "title":"Version 5.0.0rc3",
  "url":"https://github.com/scikit-hep/uproot5/discussions/735"
 },
 {
  "author":{
   "login":"dcervenkov"
  },
  "body":"Uproot can open and read remote files, but AFAIK, it cannot write them - `recreate()` needs a local path only.\r\n\r\nDo you plan to support this? Or is there a reason why this can't work/is problematic?\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"Moelf"
     },
     "body":"just write to a local file and `xrdcp`",
     "createdAt":"2022-09-28T15:20:07Z",
     "number":3756236,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"dcervenkov"
     },
     "body":"Yes, that's the workaround I'm currently using, but I'm wondering if one could use `iterate` and `recreate` to process large file(s) that cannot fit into local storage by chunks.",
     "createdAt":"2022-09-28T15:40:51Z",
     "number":3756237,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Yeah, we're not _planning_ on supporting it, though there's a placeholder for it: the local file-based Sink is implemented as an abstraction that could (in principle) be replaced by a remote file Sink.\r\n\r\nIf it is implemented, I don't see a way to get the performance reasonable for any format except RNTuple. TTrees, for instance, involve a lot of seeking forward and back to keep the file state valid. For a remote file, that means lots of round-trip communications, which is bad for low-latency environments. Maybe we could let the file state get invalid, but we do have to physically write some things because we can't keep it all in memory and dump at the end. (Or if you can, make a memory file and send it all at once!)\r\n\r\nFor RNTuple, it's plausible, since all information that isn't known until a time $T$ in writing is written after data written at time $t < T$. That is, all of the \"number of entries,\" \"where to find chunks,\" etc. are in a footer that gets written last or repeatedly re-written. So the RNTuple part could plausibly be written over a low-latency network well, but it's embedded within traditional ROOT I/O that will still require some seeking back and forth.\r\n\r\nIf we do implement remote Sinks, so that RNTuple can take advantage of them (and we let TTree be inefficient), we'd probably want to do it through https://github.com/CoffeaTeam/fsspec-xrootd to simplify the interface.\r\n\r\nBottom line: not planning on it, but we _could_ change our plans, depending on how much RNTuple improves the situation.\r\n\r\n(I'm going to make this a Discussion, rather than an Issue.)",
     "createdAt":"2022-09-28T19:23:14Z",
     "number":3756238,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"dcervenkov"
        },
        "body":"Thanks, @jpivarski, that was educative. I checked out RNTuple, and it does look interesting; hope it makes it out of experimental and perhaps into Uproot one day.",
        "createdAt":"2022-09-28T20:58:19Z",
        "number":3756898
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":3
  },
  "createdAt":"2022-09-28T15:17:04Z",
  "number":739,
  "title":"Writing remote files via XRootD",
  "url":"https://github.com/scikit-hep/uproot5/discussions/739"
 },
 {
  "author":{
   "login":"ZhenxuanZhang-Jensen"
  },
  "body":"Hi experts,\r\n   Recently I am trying to produce the cut-flow table for my analysis. And it was very easy if I did this in C++ \"for loop\". I just need to for over all the events and do all the cuts step by step in the events and count the events numbers after each cut like this:\r\n```\r\nfor events:\r\n   pt>20\r\n   pt_counts += 1\r\n   eta<2.4\r\n   pt_eta_counts += 1\r\neff_pt = pt_count / n_events\r\neff_pt_eta = pt_eta_count / n_events\r\n```\r\n   And for now, if we use python with uproot and do all the parallel operation, I only can use mask method to cut each array elements, but I'm not sure how to do this step by step and have the cut-flow results.\r\n   Any ideas are highly appreciated! Thanks in advanced!\r\nBest regards,\r\nZhenxuan\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"agoose77"
     },
     "body":"Hi @ZhenxuanZhang-Jensen, I'm not familiar with what \"cut-flow\" means (though I'm sure it's probably standard HEP language), but I think I can understand what you're trying do.\r\n\r\nTo compute the number o\r\n```python\r\nis_large_pt = events.pt > 20\r\nis_small_eta = events.eta < 2.4\r\n\r\neff_pt = ak.count_nonzero(is_large_pt) / len(events)\r\n\r\nis_large_pt_and_small_eta = np.logical_and(\r\n    is_large_pt, is_small_eta\r\n)\r\neff_pt_eta = ak.count_nonzero(is_large_pt_and_small_eta) / len(events)\r\n```\r\n\r\nThis corresponds to :\r\n```python\r\nn_large_pt = 0\r\nn_large_pt_small_eta = 0\r\n\r\nfor event in events:\r\n    if event.pt > 20:\r\n        n_large_pt += 1\r\n        if event.eta < 2.4:\r\n            n_large_pt_small_eta += 1\r\n\r\neff_pt = n_large_pt / len(events)\r\neff_pt_eta = n_large_pt_small_eta / len(events)\r\n```",
     "createdAt":"2022-09-29T14:19:44Z",
     "number":3763310,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"yes, thanks a lot, this looks feasible for me. thank you very much for the quick reply. Then I guess I need to find a way to implement the code in my framework, which uses uproot to open a root file. Anyway, this seems like a great solution for me, let me check. thanks",
        "createdAt":"2022-09-29T15:13:36Z",
        "number":3763760
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"MoAly98"
     },
     "body":"Hi, \r\nif you want something quick and dirty you can do possibly:\r\n\r\n```\r\ncounts = {}\r\nwith uproot.open(\"myfile.root\") as f:\r\n    events = f[\"tree\"]\r\n    counts['all'] = ak.num(events.pt, axis = 0)\r\n    events = events[events.pt>20]\r\n    counts[\"pt>20\"] = ak.num(events.pt, axis = 0) \r\n    events = events[events.eta< 2.4] \r\n    counts[\"eta<2.4\"] = ak.num(events.pt, axis = 0) \r\n\r\n```\r\n\r\nbut then the way you open the file can be optimized based on the size of your input, and the way you store the counts can also be done a bit more neatly. ",
     "createdAt":"2022-09-29T14:20:52Z",
     "number":3763319,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ZhenxuanZhang-Jensen"
        },
        "body":"yes, I think to do this too. I mean if I can't find a way to add a function to my framework to cut safely, then I probably will use your suggestion code to simply open a root file and do the cut-flow check. Thanks.",
        "createdAt":"2022-09-29T15:16:53Z",
        "number":3763792
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2022-09-29T14:13:40Z",
  "number":740,
  "title":"Cut-flow table with python",
  "url":"https://github.com/scikit-hep/uproot5/discussions/740"
 },
 {
  "author":{
   "login":"FlorianBen"
  },
  "body":"Dear experts,\r\n\r\nNewcomer\u2019s question. I\u2019m using more and more uproot for histogram post-processing but, sometime, I have some issue to get histograms from root files. I put a MWE here and a root file as [sample](https://github.com/FlorianBen/uproot_mwe/blob/main/output.root?raw=true):\r\n\r\n```python\r\n#!/usr/bin/env python\r\nimport uproot\r\n\r\ndef main():\r\n    file = uproot.open('scripts/output.root')\r\n    print(file.classnames()) # Show 2 TH1D in file\r\n    h1neutron_amp = file['h1neutron_amp;1'] # Get first hist\r\n    print(h1neutron_amp.values()) # Work fine\r\n    h1neutron_rate = file['h1neutron_rate;1'] # Traceback here\r\n    print(h1neutron_rate.values())\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nThe file contains two TH1D, I can get the \u2018h1neutron_amp\u2019 without problem. But when I try to get the other one, I get this traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/local/home/fb250757/Documents/scripts/uproot_hist.py\", line 15, in <module>\r\n    main()\r\n  File \"/local/home/fb250757/Documents/scripts/uproot_hist.py\", line 10, in main\r\n    h1neutron_rate = file['h1neutron_rate;1']\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/reading.py\", line 2106, in __getitem__\r\n    return self.key(where).get()\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/reading.py\", line 2509, in get\r\n    out = cls.read(chunk, cursor, context, self._file, selffile, parent)\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/model.py\", line 1296, in read\r\n    versioned_cls.read(\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/model.py\", line 806, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/models/TH.py\", line 1586, in read_members\r\n    file.class_named(\"TH1\", 8).read(\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/model.py\", line 806, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/models/TH.py\", line 538, in read_members\r\n    self._members[\"fFunctions\"] = file.class_named(\"TList\").read(\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/model.py\", line 806, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/models/TList.py\", line 50, in read_members\r\n    item = uproot.deserialization.read_object_any(\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/deserialization.py\", line 271, in read_object_any\r\n    obj = cls.read(chunk, cursor, context, file, selffile, parent)\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/model.py\", line 1296, in read\r\n    versioned_cls.read(\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/model.py\", line 806, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"<dynamic>\", line 17, in read_members\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/deserialization.py\", line 271, in read_object_any\r\n    obj = cls.read(chunk, cursor, context, file, selffile, parent)\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/model.py\", line 1296, in read\r\n    versioned_cls.read(\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/model.py\", line 806, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"<dynamic>\", line 10, in read_members\r\n  File \"/local/home/fb250757/Documents/Prog/Python/Env/Science/lib/python3.8/site-packages/uproot/containers.py\", line 1249, in read\r\n    raise NotImplementedError(\r\nNotImplementedError: non-memberwise serialization of AsMap\r\nin file scripts/output.root\r\n```\r\n\r\nUnfortunately, the traceback is a too cryptic for my understanding. Do you have an idea on where this error may come from, please?\r\n\r\nThanks for your help.\r\n\r\nOS: Ubuntu 20.04\r\npython: 3.8.10\r\nuproot: 4.3.5",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"This is an unimplemented part of ROOT serialization. (That's why the error type is `NotImplementedError`.) There's an open issue in Uproot to deal with it, and an example file containing this type has been identified: https://github.com/scikit-hep/uproot5/issues/556. So it's just a matter of somebody getting to it and implementing it. Since you ran into it, I'll bump it up in my mental priority queue.",
     "createdAt":"2022-10-05T13:08:56Z",
     "number":3805065,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"Moelf"
     },
     "body":"btw how old is this file, it seems to be made with a relatively old ROOT version",
     "createdAt":"2022-10-05T13:17:44Z",
     "number":3805151,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"FlorianBen"
        },
        "body":"Hello, the file in the attachment is just a small part of a bigger ROOT file generated by compiled routines. I recompiled the routines recently with a modern setup (ROOT 6.26/02).\r\n\r\nHowever, I didn't dive too much in the code base of the routines, so maybe they rely on deprecated functions or other old ROOT magic.",
        "createdAt":"2022-10-06T02:01:55Z",
        "number":3810237
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"My other example of a file with non-memberwise `std::map` serialization (#556) is also a histogram, and the `std::map` is the parameterization of a TFormula, which is a fit function (TF1) attached to the histogram. Your stack trace also shows that it failed while deserializing `fFunctions`, so I'm going to guess that it's the same thing. I've released a fix in [Uproot 4.3.6](https://pypi.org/project/uproot/4.3.6/). Try upgrading and seeing if it fixes the problem.\r\n\r\n(In retrospect, I should have asked you to test on the git branch before merging it, because we could have adapted to any surprises that you encounter. But, oh well, at least it will be easier for you to test. Just `pip install -U uproot` and try again. If it doesn't work, then I'll have to do another patch-release.)",
     "createdAt":"2022-10-05T19:35:40Z",
     "number":3808474,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"FlorianBen"
        },
        "body":"Hello,\r\nIndeed, the issue was only for TH1s with fitting functions attached. I upgraded uproot to 4.3.6 with pip, and uproot opens correctly the histograms now (I tested several, not all of them). \r\nThanks for your swift actions.",
        "createdAt":"2022-10-06T01:45:05Z",
        "number":3810183
       }
      ],
      "totalCount":1
     }
    }
   ],
   "totalCount":3
  },
  "createdAt":"2022-10-05T01:42:37Z",
  "number":744,
  "title":"Reading histograms with attached fit functions",
  "url":"https://github.com/scikit-hep/uproot5/discussions/744"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n* feat: implemented NON-memberwise deserialization for AsMap (v4). by @jpivarski in https://github.com/scikit-hep/uproot5/pull/747\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: ReadOnlyDirectory should provide the largest abs(cycle) when cycle is unspecified, not the largest cycle (v4). by @jpivarski in https://github.com/scikit-hep/uproot5/pull/721\r\n* fix: regularize ROOT type aliases to C fundamental type names (v4). by @jpivarski in https://github.com/scikit-hep/uproot5/pull/718\r\n\r\n## Other\r\n\r\n* test: adjust for boost-histogram 1.3.2's _storage_type deprecation (v4). by @jpivarski in https://github.com/scikit-hep/uproot5/pull/720\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/uproot5/compare/v4.3.5...v4.3.6\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/v4.3.6'>Version 4.3.6</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-10-05T19:29:07Z",
  "number":748,
  "title":"Version 4.3.6",
  "url":"https://github.com/scikit-hep/uproot5/discussions/748"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"This is the first Uproot release that requires `awkward>=2.0.0rc1`. See [Awkward Array v2.0.0rc1](https://github.com/scikit-hep/awkward/releases/tag/v2.0.0rc1).\r\n\r\n## New features\r\n\r\n* feat: implemented NON-memberwise deserialization for AsMap. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/746\r\n* feat: Added column_projection optimization by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/755\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: avoid empty TBasket issue in embedded TBasket by @jpivarski in https://github.com/scikit-hep/uproot5/pull/751\r\n* fix: don't use Awkward in test_0751 that doesn't need it by @jpivarski in https://github.com/scikit-hep/uproot5/pull/753\r\n\r\n## Other\r\n\r\n* ci: remove GHA workaround for macOS Python 3.11 by @henryiii in https://github.com/scikit-hep/uproot5/pull/743\r\n* chore: some cleanup inpsired by refurb by @henryiii in https://github.com/scikit-hep/uproot5/pull/745\r\n* chore(deps): update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/737\r\n* chore(deps): bump amannn/action-semantic-pull-request from 4 to 5 by @dependabot in https://github.com/scikit-hep/uproot5/pull/757\r\n* chore: update Uproot to require Awkward 2.0.0rc1. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/765\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/uproot5/compare/v5.0.0rc4...v5.0.0rc5\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/v5.0.0rc5'>Version 5.0.0rc5</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-10-26T19:09:33Z",
  "number":767,
  "title":"Version 5.0.0rc5",
  "url":"https://github.com/scikit-hep/uproot5/discussions/767"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n* feat: support categorical axes on boost histograms by @lobis in https://github.com/scikit-hep/uproot5/pull/764\r\n* feat: warn about TBranch name, alias name conflict. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/776\r\n* feat: any Mapping assigned to a WritableDirectory is interpreted as a TTree or failure, no fall-through. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/779\r\n* feat: add 'interp_options' mechanism and ak_add_doc. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/784\r\n* feat: Use awkward pandas, instead of the existing code that explodes Pandas Dataframes by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/734\r\n* feat: made 'very optional' arguments keyword-only by @jpivarski in https://github.com/scikit-hep/uproot5/pull/787\r\n* feat: adjust for name change in scikit-hep/awkward#1919. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/788\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: histogram weights not handled correctly in hist / boost conversion by @lobis in https://github.com/scikit-hep/uproot5/pull/774\r\n* perf: streamline metadata handling for TBranch name lookup and uproot.dask by @jpivarski in https://github.com/scikit-hep/uproot5/pull/772\r\n* fix: ensure AwkwardForth fallback path is tested without history. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/780\r\n* fix: all AwkwardForth Forms now agree with awkward_form method output. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/790\r\n\r\n## Other\r\n\r\n* refactor: final refactoring for Forth generation by @aryan26roy in https://github.com/scikit-hep/uproot5/pull/749\r\n* test: make tests parallelizable (custom_classes in uproot.open). by @jpivarski in https://github.com/scikit-hep/uproot5/pull/786\r\n* docs: fix TRef.py doc urls by @veprbl in https://github.com/scikit-hep/uproot5/pull/782\r\n* docs: `uproot.dask` docs by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/702\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/773\r\n* ci: pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/783\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/789\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/793\r\n* chore(deps): bump pypa/gh-action-pypi-publish from 1.5.1 to 1.6.1 by @dependabot in https://github.com/scikit-hep/uproot5/pull/792\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/uproot5/compare/v5.0.0rc6...v5.0.0rc7\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/v5.0.0rc7'>Version 5.0.0rc7</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-06T23:01:04Z",
  "number":794,
  "title":"Version 5.0.0rc7",
  "url":"https://github.com/scikit-hep/uproot5/discussions/794"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## Uproot version 5.0.0\r\n\r\nUproot version 5 has a few major new features, one removal (`uproot.lazy`), and is based on [Awkward Array version 2](https://github.com/scikit-hep/awkward/releases/tag/v2.0.0) instead of version 1.\r\n\r\n### uproot.lazy \u2192 uproot.dask\r\n\r\n@kkothari2001 upgraded Uproot from Awkward version 1 to version 2, the major part of which was replacing `uproot.lazy`, which is based on Awkward 1's virtual and partitioned lazy arrays, with the new Dask collection, [dask-awkward](https://github.com/dask-contrib/dask-awkward). The entry point for this function is `uproot.dask`.\r\n\r\n@kkothari2001 also simplified Uproot's Pandas backend, which used to \"explode\" ragged arrays from ROOT into Pandas DataFrames with a non-trivial MultiIndex. Now, it takes advantage of [awkward-pandas](https://github.com/intake/awkward-pandas) to put ragged (and more complex) Awkward Arrays directly into Pandas columns.\r\n\r\nIf you want the old behavior, you can read data using `library=\"ak\"` to get an Awkward Array, and use [ak.to_dataframe](https://awkward-array.org/doc/main/reference/generated/ak.to_dataframe.html) to \"explode\" the data into a MultiIndex.\r\n\r\n### TTree-reading with AwkwardForth\r\n\r\n@aryan26roy added a new code path to the TTree-reading routines to read them with AwkwardForth instead of pure Python. Users won't see any _interface_ changes due to this code, but the performance of reading TBranches with `AsObject` or `AsStrings` Interpretations should be orders of magnitude faster. For example, `std::vector<std::vector<float>>` reading is now 400\u00d7 faster.\r\n\r\n### Reading RNTuples\r\n\r\n@Moelf added a complete reader of RNTuple data with most of an RNTuple-writer in an unmerged pull request (#705). Although the RNTuple format is still in development, this is a very good start at reading RNTuple data, whose structure is a close match to Awkward Arrays (so the translation is more one-to-one than it is for TTrees, for instance).\r\n\r\n## New features\r\n\r\n* feat: move to hatchling by @henryiii in https://github.com/scikit-hep/uproot5/pull/688\r\n* feat: `from_map` like optimization for dask arrays by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/679\r\n* feat: Finalizing AwkwardForth reader for Uproot by @aryan26roy in https://github.com/scikit-hep/uproot5/pull/644\r\n* feat: implemented NON-memberwise deserialization for AsMap. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/746\r\n* feat: Added column_projection optimization by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/755\r\n* feat: support categorical axes on boost histograms by @lobis in https://github.com/scikit-hep/uproot5/pull/764\r\n* feat: warn about TBranch name, alias name conflict. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/776\r\n* feat: any Mapping assigned to a WritableDirectory is interpreted as a TTree or failure, no fall-through. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/779\r\n* feat: add 'interp_options' mechanism and ak_add_doc. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/784\r\n* feat: Use awkward pandas, instead of the existing code that explodes Pandas Dataframes by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/734\r\n* feat: made 'very optional' arguments keyword-only by @jpivarski in https://github.com/scikit-hep/uproot5/pull/787\r\n* feat: adjust for name change in scikit-hep/awkward#1919. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/788\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: depend on packaging, not setuptools vendored packaging by @henryiii in https://github.com/scikit-hep/uproot5/pull/684\r\n* fix: Avoid triggering temporary dask-awkward/awkward incompatibility. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/694\r\n* fix: Do not write incorrect fSumw2 in histograms (v5). by @jpivarski in https://github.com/scikit-hep/uproot5/pull/698\r\n* fix: Fixes uproot.dask bug with empty branches by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/700\r\n* fix: Use `from_map` optimization for delayed numpy arrays and add tests with empty branches for the same by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/703\r\n* fix: use ctx manager to ensure resources are freed by @agoose77 in https://github.com/scikit-hep/uproot5/pull/713\r\n* fix: ReadOnlyDirectory should provide the largest abs(cycle) when cycle is unspecified, not the largest cycle. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/715\r\n* fix: regularize ROOT type aliases to C fundamental type names. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/717\r\n* fix: avoid empty TBasket issue in embedded TBasket by @jpivarski in https://github.com/scikit-hep/uproot5/pull/751\r\n* fix: don't use Awkward in test_0751 that doesn't need it by @jpivarski in https://github.com/scikit-hep/uproot5/pull/753\r\n* fix: working TList serialization by @lobis in https://github.com/scikit-hep/uproot5/pull/763\r\n* fix: histogram weights not handled correctly in hist / boost conversion by @lobis in https://github.com/scikit-hep/uproot5/pull/774\r\n* perf: streamline metadata handling for TBranch name lookup and uproot.dask by @jpivarski in https://github.com/scikit-hep/uproot5/pull/772\r\n* fix: ensure AwkwardForth fallback path is tested without history. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/780\r\n* fix: all AwkwardForth Forms now agree with awkward_form method output. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/790\r\n* fix: Uproot tests now work with Awkward 2.0.0. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/795\r\n\r\n## Other\r\n\r\n* Manually add a Model for TMatrixTSym<double>. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/484\r\n* Updating docs and test in response to the removal of uproot.lazy in Uproot5 by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/615\r\n* changed arguments for awkward_form by @aryan26roy in https://github.com/scikit-hep/uproot5/pull/617\r\n* Completed the Forth based AsStrings reader. by @aryan26roy in https://github.com/scikit-hep/uproot5/pull/616\r\n* Actually pass user-specified 'awkward_form' arguments into context. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/622\r\n* Fix annoying gaps in test files. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/625\r\n* Cleaning up string generation in streamers.py by @aryan26roy in https://github.com/scikit-hep/uproot5/pull/629\r\n* chore: add dependabot for actions by @henryiii in https://github.com/scikit-hep/uproot5/pull/631\r\n* Bump actions/checkout from 2 to 3 by @dependabot in https://github.com/scikit-hep/uproot5/pull/632\r\n* Bump actions/upload-artifact from 2 to 3 by @dependabot in https://github.com/scikit-hep/uproot5/pull/633\r\n* Bump pypa/gh-action-pypi-publish from 1.4.2 to 1.5.0 by @dependabot in https://github.com/scikit-hep/uproot5/pull/634\r\n* Bump actions/download-artifact from 2 to 3 by @dependabot in https://github.com/scikit-hep/uproot5/pull/635\r\n* Forth based ROOT reader (revised) by @aryan26roy in https://github.com/scikit-hep/uproot5/pull/636\r\n* Set up tests for AsObjects, for the AwkwardForth reader by @jpivarski in https://github.com/scikit-hep/uproot5/pull/637\r\n* Iterate over objects in TDirectory in linear time. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/638\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/641\r\n* Awkward v2 update by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/620\r\n* docs: add aryan26roy as a contributor for code by @allcontributors in https://github.com/scikit-hep/uproot5/pull/645\r\n* docs: add kkothari2001 as a contributor for code by @allcontributors in https://github.com/scikit-hep/uproot5/pull/646\r\n* docs: add Moelf as a contributor for code by @allcontributors in https://github.com/scikit-hep/uproot5/pull/647\r\n* chore: cleanup flake8 by @henryiii in https://github.com/scikit-hep/uproot5/pull/527\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/650\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/654\r\n* Bump pypa/gh-action-pypi-publish from 1.5.0 to 1.5.1 by @dependabot in https://github.com/scikit-hep/uproot5/pull/656\r\n* Primitive Support for RNTuple by @Moelf in https://github.com/scikit-hep/uproot5/pull/630\r\n* Set ReadOnlyDirectory attributes when fSeekKeys == 0 by @kakwok in https://github.com/scikit-hep/uproot5/pull/660\r\n* docs: add kakwok as a contributor for code by @allcontributors in https://github.com/scikit-hep/uproot5/pull/663\r\n* Dask awkward support for uproot.dask by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/652\r\n* pathlib.Path drops '//' (naturally), but it's sometimes used for URLs by @jpivarski in https://github.com/scikit-hep/uproot5/pull/670\r\n* Gets the number of overflow bins for hist.axis.IntCategory, at least. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/671\r\n* Prevent std::pair from being AsStridedObjects. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/673\r\n* Implement transformed axis from boost-histogram/hist. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/675\r\n* AsDynamic has no self._header. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/674\r\n* Fixed TTree write_anew in a subdirectory (consistent caches). by @jpivarski in https://github.com/scikit-hep/uproot5/pull/677\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/657\r\n* Implement stl containers for RNTuple by @Moelf in https://github.com/scikit-hep/uproot5/pull/662\r\n* Multiple clusters support for RNTuple by @Moelf in https://github.com/scikit-hep/uproot5/pull/682\r\n* ci: Get test dependencies from one source by @jpivarski in https://github.com/scikit-hep/uproot5/pull/686\r\n* ci: autocancel repeated runs by @henryiii in https://github.com/scikit-hep/uproot5/pull/685\r\n* ci: use mamba by @henryiii in https://github.com/scikit-hep/uproot5/pull/683\r\n* ci: Lint PR titles according to conventional commits by @jpivarski in https://github.com/scikit-hep/uproot5/pull/689\r\n* docs: Installation requirements and error text for dask/dask-awkward in extras.py by @jpivarski in https://github.com/scikit-hep/uproot5/pull/690\r\n* ci: use concurrency group for `semantic-pr-title` by @agoose77 in https://github.com/scikit-hep/uproot5/pull/691\r\n* docs: Add image for dask docs by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/708\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/706\r\n* chore: dask_awkward.test_utils moved in ContinuumIO/dask-awkward#76 by @jpivarski in https://github.com/scikit-hep/uproot5/pull/714\r\n* test: adjust for boost-histogram 1.3.2's _storage_type deprecation. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/719\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/716\r\n* add veprbl as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/728\r\n* add nikoladze as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/729\r\n* add klieret as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/730\r\n* add dcervenkov as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/731\r\n* add beojan as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/732\r\n* add agoose77 as a contributor by @allcontributors in https://github.com/scikit-hep/uproot5/pull/733\r\n* docs: add a CITATION.cff for Uproot. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/726\r\n* chore: drop Python 3.6. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/742\r\n* ci: remove GHA workaround for macOS Python 3.11 by @henryiii in https://github.com/scikit-hep/uproot5/pull/743\r\n* chore: some cleanup inpsired by refurb by @henryiii in https://github.com/scikit-hep/uproot5/pull/745\r\n* chore(deps): update pre-commit hooks by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/737\r\n* chore(deps): bump amannn/action-semantic-pull-request from 4 to 5 by @dependabot in https://github.com/scikit-hep/uproot5/pull/757\r\n* chore: update Uproot to require Awkward 2.0.0rc1. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/765\r\n* ci: update to Python 3.11 final by @henryiii in https://github.com/scikit-hep/uproot5/pull/766\r\n* docs: add lobis as a contributor for code by @allcontributors in https://github.com/scikit-hep/uproot5/pull/771\r\n* refactor: refactor Forth generation by @aryan26roy in https://github.com/scikit-hep/uproot5/pull/710\r\n* chore: remove Identifier and `\"uproot\"` parameter. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/770\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/773\r\n* refactor: final refactoring for Forth generation by @aryan26roy in https://github.com/scikit-hep/uproot5/pull/749\r\n* docs: fix TRef.py doc urls by @veprbl in https://github.com/scikit-hep/uproot5/pull/782\r\n* ci: pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/783\r\n* docs: `uproot.dask` docs by @kkothari2001 in https://github.com/scikit-hep/uproot5/pull/702\r\n* test: make tests parallelizable (custom_classes in uproot.open). by @jpivarski in https://github.com/scikit-hep/uproot5/pull/786\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/789\r\n* chore(deps): bump pypa/gh-action-pypi-publish from 1.5.1 to 1.6.1 by @dependabot in https://github.com/scikit-hep/uproot5/pull/792\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/793\r\n\r\n## New Contributors\r\n* @aryan26roy made their first contribution in https://github.com/scikit-hep/uproot5/pull/617\r\n* @dependabot made their first contribution in https://github.com/scikit-hep/uproot5/pull/632\r\n* @Moelf made their first contribution in https://github.com/scikit-hep/uproot5/pull/630\r\n* @agoose77 made their first contribution in https://github.com/scikit-hep/uproot5/pull/691\r\n* @lobis made their first contribution in https://github.com/scikit-hep/uproot5/pull/763\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/uproot5/compare/4.2.4...v5.0.0\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/v5.0.0'>Version 5.0.0</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-10T03:59:07Z",
  "number":796,
  "title":"Version 5.0.0",
  "url":"https://github.com/scikit-hep/uproot5/discussions/796"
 },
 {
  "author":{
   "login":"ast0815"
  },
  "body":"In the [getting started guide](https://uproot.readthedocs.io/en/latest/basic.html#nested-data-structures) it says that nested structures will automatically be flattened into a DataFrame with MultiIndex when importing using Pandas. This does not seem to be the case any more:\r\n\r\n```python\r\ndf = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library=\"pd\")\r\nprint(df)\r\n```\r\n\r\n```\r\n      NMuon  ...                                   Muon_Pz\r\n0         2  ...  [-8.16079330444336, -11.307581901550293]\r\n1         1  ...                      [20.199968338012695]\r\n2         2  ...   [11.168285369873047, 36.96519088745117]\r\n3         2  ...   [403.84844970703125, 335.0942077636719]\r\n4         2  ...  [-89.69573211669922, 20.115053176879883]\r\n...     ...  ...                                       ...\r\n2416      1  ...                      [61.715789794921875]\r\n2417      1  ...                       [160.8179168701172]\r\n2418      1  ...                      [-52.66374969482422]\r\n2419      1  ...                       [162.1763153076172]\r\n2420      1  ...                       [54.71943664550781]\r\n```\r\n\r\nSo I guess that should be updated and maybe an example given how to use the awkward accessor on the structured columns. Assuming that that is the intended way of going about things.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"You're right: this is a new feature and the docs are out of date. Thanks for the heads-up!\r\n\r\nWhat's happening now is that any non-flat data uses the Awkward dtype provided by [awkward-pandas](https://github.com/intake/awkward-pandas).\r\n\r\nIf you _want_ to explode the data as before, you can get it as an Awkward Array and use [ak.to_dataframe](https://awkward-array.org/doc/main/reference/generated/ak.to_dataframe.html):\r\n\r\n```python\r\n>>> import uproot\r\n>>> import awkward as ak\r\n>>> ak.to_dataframe(events.arrays(filter_name=\"/(Jet|Muon)_P[xyz]/\", library=\"ak\"))\r\n                   Jet_Px     Jet_Py      Jet_Pz    Muon_Px    Muon_Py     Muon_Pz\r\nentry subentry                                                                    \r\n1     0        -38.874714  19.863453   -0.894942  -0.816459 -24.404259   20.199968\r\n3     0        -71.695213  93.571579  196.296432  22.088331 -85.835464  403.848450\r\n      1         36.606369  21.838793   91.666283  76.691917 -13.956494  335.094208\r\n4     0          3.880162 -75.234055 -359.601624  45.171322  67.248787  -89.695732\r\n      1          4.979580 -39.231731   68.456718  39.750957  25.403667   20.115053\r\n...                   ...        ...         ...        ...        ...         ...\r\n2414  0         33.961163  58.900467  -17.006561  -9.204197 -42.204014  -64.264900\r\n2416  0         37.071465  20.131996  225.669037 -39.285824 -14.607491   61.715790\r\n2417  0        -33.196457 -59.664749  -29.040150  35.067146 -14.150043  160.817917\r\n2418  0         -3.714818 -37.202377   41.012222 -29.756786 -15.303859  -52.663750\r\n2419  0        -36.361286  10.173571  226.429214   1.141870  63.609570  162.176315\r\n\r\n[2038 rows x 6 columns]\r\n```",
     "createdAt":"2022-12-14T19:37:07Z",
     "number":4403483,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"ast0815"
        },
        "body":"~~Thanks! This works like a charm.~~ ~~Spoke too soon. :(~~ Your method works after all.",
        "createdAt":"2022-12-15T11:28:32Z",
        "number":4409240
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"ast0815"
     },
     "body":"Oops, I take it back. Something weird is going on:\r\n\r\n```python\r\nstructured_tree = uproot.open(\"HZZ.root\")[\"events\"]\r\n\r\ndf = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library='pd')\r\nwith open(\"structured_df.txt\", \"w\") as f:\r\n    print(df, file=f)\r\n\r\nimport awkward as ak\r\n\r\narr = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"])\r\ndf = ak.to_dataframe(arr)\r\nwith open(\"flattened_df.txt\", \"w\") as f:\r\n    print(df, file=f)\r\n\r\nidx = pd.IndexSlice\r\ndf = df.loc[idx[:,0], :]\r\nwith open(\"sliced_df.txt\", \"w\") as f:\r\n    print(df, file=f)\r\n\r\ndf = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library=\"pd\")\r\ndf = df[df.NMuon > 0]\r\ndf[\"Muon_Px\"] = df.Muon_Px.ak[:,0]\r\ndf[\"Muon_Py\"] = df.Muon_Py.ak[:,0]\r\ndf[\"Muon_Pz\"] = df.Muon_Pz.ak[:,0]\r\nwith open(\"sliced_alt_df.txt\", \"w\") as f:\r\n    print(df, file=f)\r\n```\r\n\r\n```\r\n$ head *df.txt \r\n\r\n==> flattened_df.txt <==\r\n                NMuon    Muon_Px    Muon_Py     Muon_Pz\r\nentry subentry                                         \r\n0     0             2 -52.899456 -11.654672   -8.160793\r\n      1             2  37.737782   0.693474  -11.307582\r\n1     0             1  -0.816459 -24.404259   20.199968\r\n2     0             2  48.987831 -21.723139   11.168285\r\n      1             2   0.827567  29.800508   36.965191\r\n...               ...        ...        ...         ...\r\n2416  0             1 -39.285824 -14.607491   61.715790\r\n2417  0             1  35.067146 -14.150043  160.817917\r\n\r\n==> sliced_alt_df.txt <==\r\n      NMuon    Muon_Px    Muon_Py    Muon_Pz\r\n0         2 -52.899456 -11.654672  -8.160793\r\n1         1  -0.816459 -24.404259  20.199968\r\n2         2  48.987831 -21.723139  11.168285\r\n3         2  22.088331 -85.835464  403.84845\r\n4         2  45.171322  67.248787 -89.695732\r\n...     ...        ...        ...        ...\r\n2416      1  23.913206 -35.665077  54.719437\r\n2417      1  23.913206 -35.665077  54.719437\r\n2418      1  23.913206 -35.665077  54.719437\r\n\r\n==> sliced_df.txt <==\r\n                NMuon    Muon_Px    Muon_Py     Muon_Pz\r\nentry subentry                                         \r\n0     0             2 -52.899456 -11.654672   -8.160793\r\n1     0             1  -0.816459 -24.404259   20.199968\r\n2     0             2  48.987831 -21.723139   11.168285\r\n3     0             2  22.088331 -85.835464  403.848450\r\n4     0             2  45.171322  67.248787  -89.695732\r\n...               ...        ...        ...         ...\r\n2416  0             1 -39.285824 -14.607491   61.715790\r\n2417  0             1  35.067146 -14.150043  160.817917\r\n\r\n==> structured_df.txt <==\r\n      NMuon  ...                                   Muon_Pz\r\n0         2  ...  [-8.16079330444336, -11.307581901550293]\r\n1         1  ...                      [20.199968338012695]\r\n2         2  ...   [11.168285369873047, 36.96519088745117]\r\n3         2  ...   [403.84844970703125, 335.0942077636719]\r\n4         2  ...  [-89.69573211669922, 20.115053176879883]\r\n...     ...  ...                                       ...\r\n2416      1  ...                      [61.715789794921875]\r\n2417      1  ...                       [160.8179168701172]\r\n2418      1  ...                      [-52.66374969482422]\r\n```\r\n\r\nIt looks like things work fine at the beginning, but then things go awkward (pun intended). The last `Muon_Pz` in the flattened df, has the value of the second-to-last in the original.\r\n\r\nAnd it gets even weirder when I use the `awkward-pandas` accessor to slice things myself directly. The last couple of entries just have identical entries.\r\n\r\nI think I have the newest versions of awkward installed too:\r\n\r\n```\r\n$ pip list | grep awkward\r\nawkward                        2.0.0\r\nawkward-cpp                    2\r\nawkward-pandas                 2022.12a1\r\n```",
     "createdAt":"2022-12-15T11:48:16Z",
     "number":4409388,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"You lost me at this line:\r\n\r\n> ```python\r\n> idx = pd.IndexSlice\r\n> df = df.loc[idx[:,0], :]\r\n> ```\r\n\r\n`pd.IndexSlice` makes a 2-tuple of `(slice(None), 0)`, so this `loc` is getting `((slice(None), 0), slice(None))`. I guess I don't understand Pandas `loc` semantics well enough to know what's _supposed_ to happen there.\r\n\r\nBut it seems like the bottom line of what you're trying to do works:\r\n\r\n```python\r\nimport uproot, skhep_testdata\r\nstructured_tree = uproot.open(skhep_testdata.data_path(\"uproot-HZZ.root\"))[\"events\"]\r\n\r\ndf = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library='pd')\r\narr = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library='ak')\r\n\r\nfrom_df = list(df[df.NMuon > 0].Muon_Px.ak[:, 0])\r\nfrom_arr = list(arr[arr.NMuon > 0].Muon_Px[:, 0])\r\n\r\nassert from_df == from_arr\r\n```\r\n\r\nWhat the above doesn't test is the Awkward \u2192 Pandas transformation through `ak.to_dataframe`, but that's an old function and my Bayesian prior is to trust it because I think if there was a glaring bug, it would have been found already.\r\n\r\nAh heck, I'll try it. Starting from the above,\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> df2 = ak.to_dataframe(arr)\r\n>>> (df2.NMuon == 0).any()\r\nFalse\r\n```\r\n\r\nThere cannot be any rows with `NMuon == 0` because then there would be no `Muon_Px`, `Muon_Py`, `Muon_Pz` data to show. This is a consequence of the exploding: you can't represent empty lists. (That's part of why I didn't like this representation.)\r\n\r\nTo do the next step of selecting just the rows with `subentry == 0`, I learned what the nested slice syntax you used means (from [this documentation](https://pandas.pydata.org/docs/user_guide/advanced.html#using-slicers)). Now I understand why you were doing it.\r\n\r\n```python\r\n>>> from_df2 = list(df2.loc[(slice(None), 0), :].Muon_Px)\r\n>>> assert from_df2 == from_df\r\n```\r\n\r\nBut the result is the same. You said this was an issue for pz, but I tried all three components and the lists are all the same.\r\n\r\nI don't see a bug here. My `awkward-pandas` is the latest version, checked out of GitHub. The version you're using is from last week, not too long ago, and I don't think @douglasdavis was working on awkward-pandas in the past week. (No, [no changes other than documentation](https://github.com/intake/awkward-pandas/compare/2022.12a1...main).)\r\n\r\n~~Could you make a reproducer that shows exactly~~\r\n\r\nWoah, I see actually different numbers than you do, when I run the same command:\r\n\r\n```python\r\n>>> ak.to_dataframe(structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"]))\r\n                NMuon    Muon_Px    Muon_Py     Muon_Pz\r\nentry subentry                                         \r\n0     0             2 -52.899456 -11.654672   -8.160793\r\n      1             2  37.737782   0.693474  -11.307582\r\n1     0             1  -0.816459 -24.404259   20.199968\r\n2     0             2  48.987831 -21.723139   11.168285\r\n      1             2   0.827567  29.800508   36.965191\r\n...               ...        ...        ...         ...\r\n2416  0             1 -39.285824 -14.607491   61.715790\r\n2417  0             1  35.067146 -14.150043  160.817917\r\n2418  0             1 -29.756786 -15.303859  -52.663750\r\n2419  0             1   1.141870  63.609570  162.176315\r\n2420  0             1  23.913206 -35.665077   54.719437\r\n\r\n[3825 rows x 4 columns]\r\n```\r\n\r\nYour `sliced_alt_df.txt` only goes up to entry 2418 and it has my entry 2420 appearing 3 times. It's also odd that your Pandas print-outs are not symmetric around the \"`...`\". They usually show as many entries at the end as they do at the beginning, but yours show more entries at the beginning. Could it be that you've found a Pandas bug? That's implausible but\u2014oh, you're using `head` to print them out, and that's why the ends are being cut off. It doesn't explain why you see the last entry repeated, though.\r\n\r\nI'm going to leave it at that. There are other mysteries going on here. Could you try them in an interactive prompt or in Jupyter, or as many ways as possible because maybe the display-method that you've chosen is garbling things?",
        "createdAt":"2022-12-15T14:48:19Z",
        "number":4410819
       }
      ],
      "totalCount":1
     }
    },
    {
     "author":{
      "login":"ast0815"
     },
     "body":"Right, I forgot about the `head`. Sorry about that. So the output of the flattening method that you suggested actually works as it should.\r\n\r\nJust my alternative way of using the `awkward-pandas` accessor produces garbled data.\r\n\r\n```Python\r\ndf = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library=\"pd\")\r\ndf = df[df.NMuon > 0]\r\ndf[\"Muon_Px\"] = df.Muon_Px.ak[:,0]\r\ndf[\"Muon_Py\"] = df.Muon_Py.ak[:,0]\r\ndf[\"Muon_Pz\"] = df.Muon_Pz.ak[:,0]\r\nprint(df)\r\n```\r\n\r\n```\r\n      NMuon    Muon_Px    Muon_Py    Muon_Pz\r\n0         2 -52.899456 -11.654672  -8.160793\r\n1         1  -0.816459 -24.404259  20.199968\r\n2         2  48.987831 -21.723139  11.168285\r\n3         2  22.088331 -85.835464  403.84845\r\n4         2  45.171322  67.248787 -89.695732\r\n...     ...        ...        ...        ...\r\n2416      1  23.913206 -35.665077  54.719437\r\n2417      1  23.913206 -35.665077  54.719437\r\n2418      1  23.913206 -35.665077  54.719437\r\n2419      1  23.913206 -35.665077  54.719437\r\n2420      1  23.913206 -35.665077  54.719437\r\n\r\n[2362 rows x 4 columns]\r\n```\r\n\r\nAnd if I do not filter out the muonless events I would get an indexing error. There can be rows without muons in the structured `DataFrame`. Those will just have empty \"lists\" in the momentum column:\r\n\r\n```python\r\ndf = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library=\"pd\")\r\ndf = df[df.NMuon == 0]\r\nprint(df)\r\n```\r\n```\r\n      NMuon Muon_Px Muon_Py Muon_Pz\r\n43        0      []      []      []\r\n44        0      []      []      []\r\n71        0      []      []      []\r\n85        0      []      []      []\r\n186       0      []      []      []\r\n...     ...     ...     ...     ...\r\n2194      0      []      []      []\r\n2207      0      []      []      []\r\n2231      0      []      []      []\r\n2243      0      []      []      []\r\n2408      0      []      []      []\r\n\r\n[59 rows x 4 columns]\r\n```\r\n\r\nSo I guess this is a bug in `awkward-pandas`?",
     "createdAt":"2022-12-15T15:12:12Z",
     "number":4411038,
     "replies":{
      "nodes":[
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"> So I guess this is a bug in `awkward-pandas`?\r\n\r\nNot a bug but a feature: the file really does have events with zero muons (and other events with zero electrons, etc.) but the exploded view is incapable of representing that. The structured view that awkward-pandas provides is capable of describing empty lists.\r\n\r\nThere's another thing that the exploded view can't do: put muons and electrons in the same DataFrame, because there are different numbers of each. With awkward-pandas, there should be no trouble doing that.\r\n\r\nSo your workflow will change if you take advantage of this, or you can go through `ak.to_dataframe` to do it the old way.",
        "createdAt":"2022-12-15T16:41:27Z",
        "number":4411765
       },
       {
        "author":{
         "login":"ast0815"
        },
        "body":"I did not mean the empty muon lists. That absolutely makes sense. But how my accessor workflow produces a clearly wrong data output.\r\n\r\nMaybe I am using it wrong, but I would expect that the accessor way of doing this should produce the same numbers as the flattening workflow. It definitely seems wrong that it just repeats the same numbers over and over at the end.",
        "createdAt":"2022-12-16T11:50:54Z",
        "number":4420842
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"I'm not a Pandas expert, but here's my understanding of what's happening. \r\n\r\nThe `__getitem__` method on `awkward-pandas` returns a `pd.Series`. This returned series does not refer to the original series index. \r\n\r\nYour code, rewritten into separate phases, is\r\n```python\r\ndf = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library=\"pd\")\r\n\r\ndf2 = df[df.NMuon > 0]\r\n\r\ndf3 = df2.copy()\r\ndf3[\"Muon_Px\"] = df2.Muon_Px.ak[:, 0]\r\ndf3[\"Muon_Py\"] = df2.Muon_Py.ak[:, 0]\r\ndf3[\"Muon_Pz\"] = df2.Muon_Pz.ak[:, 0]\r\n\r\nprint(df3)\r\n```\r\n\r\nBy slicing `df`, `df2` has a non-consecutive index; there are gaps that follow from the `False` elements of the boolean slice. Therefore, when you slice the `.ak` accessor, the returned series will have a different index \u2014 the default Pandas index is consecutive integers. Pandas then, I presume, aligns the elements at the indices, and fills the remainder.\r\n\r\nIf you don't need the `df3` index to correspond to the `df` index, you can reset it:\r\n```python\r\ndf = structured_tree.arrays([\"NMuon\", \"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library=\"pd\")\r\n\r\ndf2 = df[df.NMuon > 0]\r\n\r\ndf3 = df2.reset_index()\r\ndf3[\"Muon_Px\"] = df2.Muon_Px.ak[:, 0]\r\ndf3[\"Muon_Py\"] = df2.Muon_Py.ak[:, 0]\r\ndf3[\"Muon_Pz\"] = df2.Muon_Pz.ak[:, 0]\r\n\r\nprint(df3)\r\n```",
        "createdAt":"2022-12-16T12:29:24Z",
        "number":4421112
       },
       {
        "author":{
         "login":"ast0815"
        },
        "body":"Ok, with the `reset_index` in there it seems to work as intended:\r\n\r\n```\r\n      index  NMuon    Muon_Px    Muon_Py     Muon_Pz\r\n0         0      2 -52.899456 -11.654672   -8.160793\r\n1         1      1  -0.816459 -24.404259   20.199968\r\n2         2      2  48.987831 -21.723139   11.168285\r\n3         3      2  22.088331 -85.835464   403.84845\r\n4         4      2  45.171322  67.248787  -89.695732\r\n...     ...    ...        ...        ...         ...\r\n2357   2416      1 -39.285824 -14.607491    61.71579\r\n2358   2417      1  35.067146 -14.150043  160.817917\r\n2359   2418      1 -29.756786 -15.303859   -52.66375\r\n2360   2419      1    1.14187   63.60957  162.176315\r\n2361   2420      1  23.913206 -35.665077   54.719437\r\n\r\n[2362 rows x 5 columns]\r\n```\r\n\r\nI would still classify this as a bug though. It certainly is unexpected behaviour. Whether it is in `pandas` or in `awkward-pandas` I cannot say. I guess I should just open an issue on the latter and see what happens if someone with expert knowledge takes a look.\r\n\r\nIn any case, thanks all!",
        "createdAt":"2022-12-16T12:52:39Z",
        "number":4421253
       },
       {
        "author":{
         "login":"agoose77"
        },
        "body":"If this were a bug, it's firmly in `awkward-pandas`; we return a `pd.Series` without an index.\r\n\r\nI'm mulling over whether this is a bug. At the user-level, I might expect slices of the `ak` accessor to return a series with the appropriate index. I suspect it should be feasible; if we determine the kind of indexing being performed, then we could also apply that to the series index. It would only matter for slices that touch the first dimension.\r\n\r\n@douglasdavis pinging you, as you probably want to be involved in this conversation! :) Does awkward-pandas support multi-index too, or are they mutually exclusive?",
        "createdAt":"2022-12-16T13:02:49Z",
        "number":4421318
       },
       {
        "author":{
         "login":"douglasdavis"
        },
        "body":"awkward-pandas is young enough to say we haven't thought deeply about indexing yet! It looks like an issue has been opened in the awkward-pandas repository, and that is probably the best place to discuss this!",
        "createdAt":"2022-12-16T17:20:43Z",
        "number":4425527
       },
       {
        "author":{
         "login":"jpivarski"
        },
        "body":"I'm surprised that it didn't automatically cross-link: https://github.com/intake/awkward-pandas/issues/27\r\n\r\nSeeing this in GitHub now, rather than email, I realize that @ast0815 opened that issue: it wasn't coincidentally discovered at the same time!",
        "createdAt":"2022-12-16T17:30:39Z",
        "number":4425833
       }
      ],
      "totalCount":7
     }
    }
   ],
   "totalCount":3
  },
  "createdAt":"2022-12-14T17:18:22Z",
  "number":803,
  "title":"Reading data into Pandas in Uproot 5; awkward-pandas versus MultiIndex",
  "url":"https://github.com/scikit-hep/uproot5/discussions/803"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n_(none!)_\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: protect Uproot's 'project_columns' from Dask node names. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/801\r\n\r\n## Other\r\n\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/799\r\n* docs: update documentation about Pandas; we don't do MultiIndex anymore. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/802\r\n* chore(deps): bump pypa/gh-action-pypi-publish from 1.6.1 to 1.6.4 by @dependabot in https://github.com/scikit-hep/uproot5/pull/797\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/uproot5/compare/v5.0.0...v5.0.1\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/v5.0.1'>Version 5.0.1</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-15T18:39:12Z",
  "number":805,
  "title":"Version 5.0.1",
  "url":"https://github.com/scikit-hep/uproot5/discussions/805"
 },
 {
  "author":{
   "login":"jpivarski"
  },
  "body":"## New features\r\n\r\n_(none!)_\r\n\r\n## Bug-fixes and performance\r\n\r\n* fix: uproot.dask: Protect against `branches=None` in `project_columns` by @douglasdavis in https://github.com/scikit-hep/uproot5/pull/806\r\n* fix: AsStridedObjects.awkward_form was still including the '@' members. by @jpivarski in https://github.com/scikit-hep/uproot5/pull/808\r\n\r\n## Other\r\n\r\n* ci: [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/scikit-hep/uproot5/pull/807\r\n\r\n\r\n**Full Changelog**: https://github.com/scikit-hep/uproot5/compare/v5.0.1...v5.0.2\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/scikit-hep/uproot5/releases/tag/v5.0.2'>Version 5.0.2</a>.</em>",
  "comments":{
   "nodes":[],
   "totalCount":0
  },
  "createdAt":"2022-12-22T21:07:11Z",
  "number":809,
  "title":"Version 5.0.2",
  "url":"https://github.com/scikit-hep/uproot5/discussions/809"
 }
]
[
 {
  "author_association":"MEMBER",
  "body":"@chrisburr This is the modification to Uproot `Sources` that will make it possible to fetch all the desired baskets in a multipart request. The cost is that it creates a new distinction between `Source` (from which data can be drawn) and `Chunk` (data ready to be read by a `Cursor`). I'll have to work that into every part of Uproot\u2014to start each object by asking for a set of `Chunks` which fill in the background while I interpret them. (The whole library needs to be refactored, but I had to do that for Awkward 1.0 anyway.)",
  "created_at":"2020-05-13T02:42:24Z",
  "id":627710577,
  "issue":1,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNzcxMDU3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T02:42:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is wonderful, thank you very much.\r\n\r\nIt looks like `uproot4` isn't in a usable enough state yet that I can try benchmarking. Can you ping me once it's possible to read some kind of large ROOT file? Flat TTrees are my default, but I can try something else if that better aligns with your roadmap for uproot4.\r\n\r\nFor the sake of tying issues together: https://github.com/scikit-hep/uproot/issues/393",
  "created_at":"2020-05-13T04:55:59Z",
  "id":627745498,
  "issue":1,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNzc0NTQ5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T04:55:59Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @jpivarski, I noticed while looking at the development of uproot4 that you have a copy-and-paste typo \"BSD 3-Clause License; see https://github.com/jpivarski/awkward-1.0/blob/master/LICENSE\" at the beginning of various files ...",
  "created_at":"2020-05-13T07:14:26Z",
  "id":627795354,
  "issue":1,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNzc5NTM1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T07:14:26Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"> It looks like `uproot4` isn't in a usable enough state yet that I can try benchmarking. Can you ping me once it's possible to read some kind of large ROOT file? Flat TTrees are my default, but I can try something else if that better aligns with your roadmap for uproot4.\r\n\r\nThere's not even anything ROOT-related in here yet. I just wanted to get the physical layer figured out before building on top of it because it implies a difference in how everything else is written. Uproots 1\u20123 were written with the idea in mind that I have a memory-mapped file, which means any byte position can be accessed at any time, and trivially in parallel (no state). Then the other interfaces were made to look like memory-mapped files, with bad consequences for performance. One of the things that I didn't fully recognize when I started Uproot was that most users would be accessing remote files most of the time.\r\n\r\nIt would be possible to do performance tests on randomly chosen ranges of a file. The test would consist of creating a `Source` with some `num_workers`, asking for `chunks` and then iterating through those chunks, asking for `raw_data` from each one (because they're not fully downloaded until you can access the `raw_data`). For realistic byte ranges, you could use Uproot 3 to look at a branch's `_fBasketSeek` (byte positions where its baskets start; only up to `numbaskets` are meaningful) and its `_fBasketBytes` (number of bytes in each basket). This would allow you to create realistic `ranges` (list of `(start, stop)` tuples, where `stop` is `start + num_bytes`).\r\n\r\nThat would test everything involved in fetching data in a hopefully efficient way; the part it leaves out is interpreting the data, which is subdominant in remote file access over regular networks.\r\n\r\nIf I'm making a fundamental mistake in these file access methods, then I don't want to design around it. I've been thinking about how I can adequately test that. Do you have a suggestion for a publicly available test file? Would a test through a household ISP mean anything, given that most of these files would be used on university or lab computers or on the GRID?",
  "created_at":"2020-05-13T11:03:41Z",
  "id":627910711,
  "issue":1,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNzkxMDcxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T11:03:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Hi @jpivarski, I noticed while looking at the development of uproot4 that you have a copy-and-paste typo \"BSD 3-Clause License; see https://github.com/jpivarski/awkward-1.0/blob/master/LICENSE\" at the beginning of various files ...\r\n\r\nGood catch! Thanks\u2014I'll take care of that as soon as I can.\r\n\r\nIn fact, the Awkward ones shouldn't point to `jpivarski` as the owner anymore. It's been a long time since I moved that to `scikit-hep`.",
  "created_at":"2020-05-13T11:04:42Z",
  "id":627911063,
  "issue":1,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNzkxMTA2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T11:04:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> If I'm making a fundamental mistake in these file access methods, then I don't want to design around it. I've been thinking about how I can adequately test that. Do you have a suggestion for a publicly available test file?\r\n\r\nI default to using these three open data files for XRootD:\r\n```bash\r\n$ xrdfs root://eospublic.cern.ch/ ls -l /eos/opendata/lhcb/AntimatterMatters2017/data\r\n-r-- 2017-03-07 13:53:05   666484974 /eos/opendata/lhcb/AntimatterMatters2017/data/B2HHH_MagnetDown.root\r\n-r-- 2017-03-07 13:53:08   444723234 /eos/opendata/lhcb/AntimatterMatters2017/data/B2HHH_MagnetUp.root\r\n-r-- 2017-03-07 13:53:08     2272072 /eos/opendata/lhcb/AntimatterMatters2017/data/PhaseSpaceSimulation.root\r\n```\r\n\r\nI've just copied them to CERN S3 so they can be accessed using standard HTTP(S) as well:\r\n* https://scikit-hep-test-data.s3.cern.ch/B2HHH_MagnetDown.root\r\n* https://scikit-hep-test-data.s3.cern.ch/B2HHH_MagnetUp.root\r\n* https://scikit-hep-test-data.s3.cern.ch/PhaseSpaceSimulation.root\r\n\r\n> Would a test through a household ISP mean anything, given that most of these files would be used on university or lab computers or on the GRID?\r\n\r\nIt's definitely useful as the quality of institute connections varies a lot in my experience and it's more likely to expose issues with latency, especially given you'll be accessing them from the US. I think a good implementation should only be limited by bandwidth when reading large enough files.\r\n\r\nIf you put together a test I can easily try it at a few different institutes as well as pushing a test job to every LHCb grid site.",
  "created_at":"2020-05-13T11:45:51Z",
  "id":627928856,
  "issue":1,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNzkyODg1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T11:45:51Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"Great, I'll do that! I'll make a script that uses three Uproot 4 accessors (HTTPSource, HTTPMultipartSource, and XRootDSource), synchronous and with a variable number of asynchronous workers, to request all of the byte ranges that are associated with TBaskets in these files, though they won't interpret any of them. Knowing how this fares on a home ISP in the U.S. and on LHCb grid sites would make a good test.\r\n\r\nThanks!",
  "created_at":"2020-05-13T12:58:33Z",
  "id":627966074,
  "issue":1,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyNzk2NjA3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-13T12:58:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Using original Uproot, I could find basket byte ranges for some interesting branches in B2HHH_MagnetDown.root (the biggest file).\r\n\r\n```python\r\nimport uproot\r\n\r\nfile = uproot.open(\"root://eospublic.cern.ch//eos/opendata/lhcb/AntimatterMatters2017/data/B2HHH_MagnetDown.root\")\r\ntree = file[\"DecayTree\"]\r\n\r\nbasic = tree[\"H1_PX\"]\r\nuneven = tree[\"B_FlightDistance\"]\r\nmanysmall = tree[\"H1_ProbK\"]\r\nsmall = tree[\"H1_Charge\"]\r\nsmaller = tree[\"H2_isMuon\"]\r\n\r\nbasic_ranges = [(basic._fBasketSeek[i], basic._fBasketSeek[i] + basic._fBasketBytes[i]) for i in range(basic.numbaskets - 1)]\r\nuneven_ranges = [(uneven._fBasketSeek[i], uneven._fBasketSeek[i] + uneven._fBasketBytes[i]) for i in range(uneven.numbaskets - 1)]\r\nmanysmall_ranges = [(manysmall._fBasketSeek[i], manysmall._fBasketSeek[i] + manysmall._fBasketBytes[i]) for i in range(manysmall.numbaskets - 1)]\r\nsmall_ranges = [(small._fBasketSeek[i], small._fBasketSeek[i] + small._fBasketBytes[i]) for i in range(small.numbaskets - 1)]\r\nsmaller_ranges = [(smaller._fBasketSeek[i], smaller._fBasketSeek[i] + smaller._fBasketBytes[i]) for i in range(smaller.numbaskets - 1)]\r\n```\r\n\r\nThey are:\r\n\r\n```python\r\nbasic_ranges = [(251420, 280469), (780279, 809438), (1292884, 1321939), (1821214, 1850352), (2332472, 2361318), (2857889, 2886861), (3367462, 3396250), (3892727, 3921697), (4401016, 4429898), (4926581, 4955399), (5437618, 5466655), (5964075, 5993014), (6473347, 6502388), (6998757, 7027470), (7507959, 7536831), (8034231, 8063034), (8541718, 8570466), (9067833, 9096666), (9579832, 9609045), (10110520, 10139632), (10622426, 10651298), (11150013, 11178954), (11661628, 11690624), (12190895, 12219982), (12703340, 12732453), (13231007, 13259869), (13739935, 13768723), (14265399, 14294138), (14773868, 14802549), (15299804, 15328790), (15810002, 15838815), (16336102, 16365055), (16847128, 16876170), (17376370, 17405411), (17888280, 17917314), (18416175, 18445180), (18926411, 18955271), (19452399, 19481414), (19962846, 19991849), (20489617, 20518479), (20997380, 21025969), (21521608, 21550462), (22031445, 22060380), (22558370, 22587191), (23068841, 23098022), (23596471, 23625270), (24105501, 24134276), (24631436, 24660325), (25140195, 25168997), (25666725, 25695856), (26178145, 26206933), (26703782, 26732728), (27214337, 27243259), (27741551, 27770434), (28251981, 28280997), (28780844, 28809910), (29291805, 29320718), (29817896, 29846521), (33613618, 35293020), (63654038, 65332970), (93692084, 95369623), (123704028, 125380872), (153715738, 155391530), (183705174, 185383765), (213736113, 215415992), (243778787, 245457590), (273817842, 275497423), (303871772, 305549510), (333888042, 335567197), (363927863, 365606706), (393961504, 395639408), (423978870, 425656912), (454003981, 455680379), (484012255, 485690211), (514040039, 515718739), (544066058, 545743627), (574080763, 575758567), (604106227, 605785458), (634148292, 635826179)]\r\n\r\nuneven_ranges = [(238, 30602), (511346, 541717), (1042200, 1072598), (1552281, 1582633), (2082823, 2113178), (2590514, 2620878), (3117502, 3147866), (3626040, 3656409), (4151403, 4181753), (4658857, 4689210), (5186624, 5216984), (5697309, 5727667), (6222637, 6252986), (6732777, 6763116), (7256552, 7286909), (7768511, 7798861), (8291503, 8321866), (8800418, 8830781), (9327326, 9357678), (9841931, 9872296), (10371537, 10401889), (10882131, 10912482), (11409998, 11440365), (11922486, 11952848), (12451733, 12482083), (12964175, 12994537), (13489542, 13519915), (13998868, 14029240), (14523806, 14554182), (15032213, 15062584), (15559594, 15589949), (16068926, 16099281), (16595413, 16625749), (17108114, 17138456), (17637099, 17667466), (18148291, 18178658), (18676089, 18706452), (19185270, 19215623), (19711273, 19741655), (20223209, 20253558), (20747832, 20778207), (21254785, 21285147), (21780501, 21810865), (22291455, 22321812), (22817015, 22847374), (23329758, 23360119), (23855257, 23885627), (24364450, 24394814), (24890372, 24920748), (25398220, 25428602), (25927845, 25958194), (26436644, 26467018), (26963321, 26993698), (27474229, 27504582), (28000727, 28031086), (28512538, 28542907), (29041155, 29071512), (29551278, 29581634), (30076327, 30077343), (30082674, 31841731), (60123086, 61882198), (90161266, 91920347), (120173262, 121932215), (150184830, 151943811), (180174410, 181933416), (210205164, 211964220), (240247845, 242006810), (270286755, 272045891), (300341112, 302099992), (330357158, 332116164), (360397177, 362156167), (390430747, 392189705), (420448018, 422207094), (450473187, 452232152), (480481471, 482240373), (510509265, 512268368), (540535389, 542294436), (570549677, 572308845), (600575431, 602334473), (630617551, 632376584)]\r\n\r\nmanysmall_ranges = [(61217, 78285), (572360, 589389), (1103220, 1120226), (1613265, 1630346), (2143804, 2160726), (2651522, 2668506), (3178504, 3195370), (3687052, 3703999), (4212386, 4229319), (4719835, 4736782), (5247626, 5264696), (5758287, 5775336), (6283620, 6300654), (6793745, 6810559), (7317519, 7334487), (7829476, 7846405), (8352494, 8369433), (8861429, 8878434), (9388294, 9405420), (9902929, 9920041), (10432526, 10449540), (10943123, 10960164), (11470998, 11488019), (11983481, 12000506), (12512701, 12529767), (13025180, 13042215), (13550544, 13567417), (14059853, 14076760), (14584826, 14601746), (15093216, 15110223), (15620565, 15637452), (16129914, 16146942), (16656366, 16673378), (17169080, 17186152), (17698106, 17715133), (18209312, 18226386), (18737081, 18754107), (19246231, 19263293), (19772286, 19789351), (20284210, 20301152), (20808818, 20825690), (21315777, 21332782), (21841506, 21858498), (22352426, 22369352), (22878010, 22895127), (23390757, 23407721), (23916238, 23933214), (24425450, 24442433), (24951385, 24968245), (25459227, 25476299), (25988842, 26005820), (26497670, 26514599), (27024322, 27041267), (27535224, 27552226), (28061735, 28078698), (28573536, 28590605), (29102128, 29119134), (29612250, 29629149), (30077891, 30078272), (38642917, 39625943), (68681721, 69664876), (98715547, 99697385), (128725613, 129707189), (158733908, 159714686), (188732103, 189714167), (218766765, 219750580), (248806468, 249788960), (278848156, 279831120), (308895796, 309877753), (338916653, 339899114), (368955427, 369937648), (398986418, 399968443), (429004087, 429986758), (459024130, 460005017), (489037414, 490020103), (519067153, 520049974), (549089561, 550071731), (579105497, 580087501), (609135095, 610118112), (639172718, 640155119)]\r\n\r\nsmall_ranges = [(763550, 767001), (1804396, 1807849), (2841338, 2844775), (3876199, 3879621), (4909942, 4913388), (5947526, 5950949), (6982181, 6985620), (8017622, 8021059), (9051208, 9054618), (10093827, 10097286), (11133272, 11136742), (12174214, 12177650), (13214371, 13217818), (14248839, 14252272), (15283059, 15286520), (16319458, 16322900), (17359641, 17363087), (18399533, 18402948), (19435665, 19439126), (20472918, 20476359), (21504981, 21508429), (22541653, 22545100), (23579920, 23583345), (24614899, 24618368), (25649997, 25653449), (26687219, 26690652), (27724861, 27728287), (28764170, 28767607), (29801215, 29804672), (30078677, 30078803), (40631788, 40727687), (70671226, 70767231), (100702166, 100798350), (130711213, 130807092), (160718102, 160814105), (190719127, 190815017), (220756708, 220852664), (250794509, 250890543), (280836659, 280932741), (310882326, 310978216), (340904347, 341000207), (370942631, 371038613), (400973185, 401069273), (430991376, 431087406), (461009240, 461105040), (491024893, 491120734), (521054768, 521150783), (551075882, 551171818), (581092216, 581188223), (611123667, 611219647), (641159610, 641255457)]\r\n\r\nsmaller_ranges = [(772807, 774652), (1813736, 1815563), (2850540, 2852332), (3885380, 3887112), (4919154, 4920949), (5956756, 5958503), (6991442, 6993235), (8026871, 8028680), (9060438, 9062236), (10103111, 10104856), (11142606, 11144365), (12183446, 12185271), (13223601, 13225399), (14258050, 14259856), (15292365, 15294179), (16328741, 16330489), (17368942, 17370697), (18408741, 18410532), (19444949, 19446806), (20482189, 20484000), (21514212, 21516064), (22550930, 22552743), (23589146, 23590893), (24624144, 24625949), (25659294, 25661109), (26696405, 26698169), (27734078, 27735880), (28773424, 28775211), (29810507, 29812301), (30080955, 30081062), (51440513, 51488688), (81479133, 81527415), (111502489, 111550535), (141514554, 141562280), (171515261, 171563272), (201526246, 201574430), (231565878, 231614373), (261602704, 261650726), (291652263, 291700195), (321686124, 321734493), (351712836, 351761240), (381748386, 381796649), (411772830, 411821459), (441796872, 441844655), (471813913, 471861945), (501831558, 501879499), (531858756, 531906803), (561875773, 561923934), (591897787, 591945547), (621932752, 621980573), (651966963, 652015278)]\r\n```",
  "created_at":"2020-05-15T01:08:08Z",
  "id":628965885,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODk2NTg4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T01:08:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This script gives a diagnostic (not a summary) view of download speeds using various methods.\r\n\r\n```python\r\nimport time\r\nimport uproot4.source.xrootd\r\nimport uproot4.source.http\r\n\r\ndef time_profile(source, ranges):\r\n    starttime = time.time()\r\n\r\n    with source as managed:\r\n        print(f\"{time.time() - starttime:.3f} got file object\")\r\n\r\n        chunks = managed.chunks(ranges)\r\n        print(f\"{time.time() - starttime:.3f} obtained chunk futures\")\r\n\r\n        for chunk in chunks:\r\n            length = len(chunk.raw_data)\r\n            print(f\"{time.time() - starttime:.3f} chunk is ready ({length / 1024:.0f} kB)\")\r\n\r\n    used_fallback = \" (used fallback)\" if getattr(source, \"has_fallback\", False) else \"\"\r\n    print(f\"{time.time() - starttime:.3f} done{used_fallback}\")\r\n```\r\n\r\nPossible `sources` are\r\n\r\n   * XRootD in 12 parallel threads (each requests one basket): `uproot4.source.xrootd.XRootDSource(\"root://eospublic.cern.ch//eos/opendata/lhcb/AntimatterMatters2017/data/B2HHH_MagnetDown.root\", num_workers=12)`\r\n   * HTTP in 12 parallel threads (each requests one basket): `time_profile(uproot4.source.http.HTTPSource(\"https://scikit-hep-test-data.s3.cern.ch/B2HHH_MagnetDown.root\", num_workers=12), basic_ranges)`\r\n   * HTTP in 1 background multipart thread, makes 1 request and parses the 1 response as it streams in: `time_profile(uproot4.source.http.HTTPMultipartSource(\"https://scikit-hep-test-data.s3.cern.ch/B2HHH_MagnetDown.root\", num_fallback_workers=12), basic_ranges)`\r\n\r\nHowever, the last of those three doesn't actually do the multipart thing, so it falls back to `HTTPSource`. I haven't been able to find any servers that do this right other than `https://example.com`, which ICANN runs (so of course they'd do it right). See PR #3.",
  "created_at":"2020-05-15T01:27:00Z",
  "id":628971632,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODk3MTYzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T01:34:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Here's XRootD:\r\n\r\n```\r\n>>> time_profile(uproot4.source.xrootd.XRootDSource(\"root://eospublic.cern.ch//eos/opendata/lhcb/AntimatterMatters2017/data/B2HHH_MagnetDown.root\", num_workers=12), basic_ranges)\r\n0.000 got file object\r\n0.002 obtained chunk futures\r\n0.371 chunk is ready (28 kB)\r\n0.538 chunk is ready (28 kB)\r\n0.583 chunk is ready (28 kB)\r\n0.583 chunk is ready (28 kB)\r\n0.845 chunk is ready (28 kB)\r\n0.845 chunk is ready (28 kB)\r\n0.845 chunk is ready (28 kB)\r\n0.845 chunk is ready (28 kB)\r\n0.871 chunk is ready (28 kB)\r\n0.871 chunk is ready (28 kB)\r\n1.368 chunk is ready (28 kB)\r\n1.368 chunk is ready (28 kB)\r\n1.368 chunk is ready (28 kB)\r\n1.654 chunk is ready (28 kB)\r\n1.655 chunk is ready (28 kB)\r\n1.655 chunk is ready (28 kB)\r\n1.655 chunk is ready (28 kB)\r\n1.655 chunk is ready (28 kB)\r\n1.655 chunk is ready (29 kB)\r\n1.658 chunk is ready (28 kB)\r\n2.332 chunk is ready (28 kB)\r\n2.332 chunk is ready (28 kB)\r\n2.332 chunk is ready (28 kB)\r\n2.332 chunk is ready (28 kB)\r\n2.332 chunk is ready (28 kB)\r\n2.332 chunk is ready (28 kB)\r\n2.332 chunk is ready (28 kB)\r\n2.332 chunk is ready (28 kB)\r\n2.788 chunk is ready (28 kB)\r\n2.788 chunk is ready (28 kB)\r\n2.789 chunk is ready (28 kB)\r\n2.789 chunk is ready (28 kB)\r\n2.789 chunk is ready (28 kB)\r\n2.789 chunk is ready (28 kB)\r\n2.789 chunk is ready (28 kB)\r\n2.789 chunk is ready (28 kB)\r\n2.789 chunk is ready (28 kB)\r\n3.155 chunk is ready (28 kB)\r\n3.634 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n3.635 chunk is ready (28 kB)\r\n4.457 chunk is ready (28 kB)\r\n4.458 chunk is ready (28 kB)\r\n4.458 chunk is ready (28 kB)\r\n4.458 chunk is ready (28 kB)\r\n4.458 chunk is ready (28 kB)\r\n4.458 chunk is ready (28 kB)\r\n4.458 chunk is ready (28 kB)\r\n6.601 chunk is ready (1640 kB)\r\n9.882 chunk is ready (1640 kB)\r\n12.833 chunk is ready (1638 kB)\r\n65.131 chunk is ready (1638 kB)\r\n65.131 chunk is ready (1637 kB)\r\n65.132 chunk is ready (1639 kB)\r\n65.132 chunk is ready (1641 kB)\r\n113.069 chunk is ready (1639 kB)\r\n113.069 chunk is ready (1640 kB)\r\n113.069 chunk is ready (1638 kB)\r\n157.209 chunk is ready (1640 kB)\r\n212.204 chunk is ready (1639 kB)\r\n212.204 chunk is ready (1639 kB)\r\n212.204 chunk is ready (1639 kB)\r\n212.204 chunk is ready (1637 kB)\r\n212.204 chunk is ready (1639 kB)\r\n212.204 chunk is ready (1639 kB)\r\n212.204 chunk is ready (1638 kB)\r\n212.204 chunk is ready (1638 kB)\r\n212.204 chunk is ready (1640 kB)\r\n212.204 chunk is ready (1639 kB)\r\n215.273 done\r\n```\r\n\r\nRunning it again changes some of the intermediate times but not the final time (351 sec: order of magnitude), suggesting that the baskets are coming in a different order. The XRootD case has to cross the Atlantic (I'm testing this on a home ISP outside of Chicago).\r\n\r\nHere's HTTP:\r\n\r\n```\r\n>>> time_profile(uproot4.source.http.HTTPSource(\"https://scikit-hep-test-data.s3.cern.ch/B2HHH_MagnetDown.root\", num_workers=12), basic_ranges)\r\n0.000 got file object\r\n0.001 obtained chunk futures\r\n3.096 chunk is ready (28 kB)\r\n3.096 chunk is ready (28 kB)\r\n4.484 chunk is ready (28 kB)\r\n4.485 chunk is ready (28 kB)\r\n4.485 chunk is ready (28 kB)\r\n4.485 chunk is ready (28 kB)\r\n4.485 chunk is ready (28 kB)\r\n9.767 chunk is ready (28 kB)\r\n9.767 chunk is ready (28 kB)\r\n9.767 chunk is ready (28 kB)\r\n9.767 chunk is ready (28 kB)\r\n9.767 chunk is ready (28 kB)\r\n9.767 chunk is ready (28 kB)\r\n9.767 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (29 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.768 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n9.769 chunk is ready (28 kB)\r\n21.478 chunk is ready (1640 kB)\r\n25.131 chunk is ready (1640 kB)\r\n71.057 chunk is ready (1638 kB)\r\n71.057 chunk is ready (1638 kB)\r\n71.057 chunk is ready (1637 kB)\r\n71.057 chunk is ready (1639 kB)\r\n71.057 chunk is ready (1641 kB)\r\n71.057 chunk is ready (1639 kB)\r\n71.057 chunk is ready (1640 kB)\r\n79.842 chunk is ready (1638 kB)\r\n79.842 chunk is ready (1640 kB)\r\n83.116 chunk is ready (1639 kB)\r\n83.116 chunk is ready (1639 kB)\r\n83.116 chunk is ready (1639 kB)\r\n83.116 chunk is ready (1637 kB)\r\n83.116 chunk is ready (1639 kB)\r\n83.116 chunk is ready (1639 kB)\r\n83.116 chunk is ready (1638 kB)\r\n83.116 chunk is ready (1638 kB)\r\n83.116 chunk is ready (1640 kB)\r\n83.116 chunk is ready (1639 kB)\r\n83.118 done\r\n```\r\n\r\nRunning this a second time gives the same order of magnitude time (73.5 sec).\r\n\r\nThe `HTTPMultipartSource` behavior is similar, but only because it falls back to `HTTPSource`.",
  "created_at":"2020-05-15T01:48:39Z",
  "id":628977916,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODk3NzkxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T01:48:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@chrisburr @nsmith- @lgray and @PerilousApricot might be interested in this.\r\n\r\nI'm particularly surprised that HTTP GET multipart support is so bad: the servers don't even give you good hints about the fact that they don't support it (almost universally so far).\r\n\r\nIf you have any suggestions about what would be better remote file accessors, let me know. The first time I went through Uproot, I assumed that file access would be similar to memory mapping. However, most users are fetching remote files, so that goes out the window. With the above design, all of my interpretation code has to inform the physical layer about the byte ranges that it wants as early as possible to try to hide the latency by running the physical bytes fetch and the interpretation (mostly decompression) at the same time.\r\n\r\nNot very much has been moved into the uproot4 repo yet, but we do know which byte ranges of a file correspond to baskets (using old Uproot), and enough has been written to fetch those bytes, but not interpret them. At that level, the code is open for performance testing.\r\n\r\n(The small baskets, followed by large baskets pattern is typical.)",
  "created_at":"2020-05-15T01:55:57Z",
  "id":628980050,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODk4MDA1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T01:55:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@chrisburr has kindly hosted the file on a multipart-friendly HTTPS server, https://chrisburr.me/B2HHH_MagnetDown.root, so here are true multipart times from London to Chicago:\r\n\r\n```\r\n>>> time_profile(uproot4.source.http.HTTPMultipartSource(\"https://chrisburr.me/B2HHH_MagnetDown.root\", num_fallback_workers=12), basic_ranges)\r\n0.000 got file object\r\n0.453 obtained chunk futures\r\n0.760 chunk is ready (28 kB)\r\n0.884 chunk is ready (28 kB)\r\n0.971 chunk is ready (28 kB)\r\n1.023 chunk is ready (28 kB)\r\n1.050 chunk is ready (28 kB)\r\n1.103 chunk is ready (28 kB)\r\n1.157 chunk is ready (28 kB)\r\n1.208 chunk is ready (28 kB)\r\n1.717 chunk is ready (28 kB)\r\n1.717 chunk is ready (28 kB)\r\n1.717 chunk is ready (28 kB)\r\n1.717 chunk is ready (28 kB)\r\n1.717 chunk is ready (28 kB)\r\n1.717 chunk is ready (28 kB)\r\n1.717 chunk is ready (28 kB)\r\n1.717 chunk is ready (28 kB)\r\n1.724 chunk is ready (28 kB)\r\n1.724 chunk is ready (28 kB)\r\n1.725 chunk is ready (29 kB)\r\n1.782 chunk is ready (28 kB)\r\n1.834 chunk is ready (28 kB)\r\n1.889 chunk is ready (28 kB)\r\n1.916 chunk is ready (28 kB)\r\n1.969 chunk is ready (28 kB)\r\n2.252 chunk is ready (28 kB)\r\n2.273 chunk is ready (28 kB)\r\n2.273 chunk is ready (28 kB)\r\n2.319 chunk is ready (28 kB)\r\n2.319 chunk is ready (28 kB)\r\n2.319 chunk is ready (28 kB)\r\n2.320 chunk is ready (28 kB)\r\n2.350 chunk is ready (28 kB)\r\n2.403 chunk is ready (28 kB)\r\n2.462 chunk is ready (28 kB)\r\n2.517 chunk is ready (28 kB)\r\n2.545 chunk is ready (28 kB)\r\n2.603 chunk is ready (28 kB)\r\n2.660 chunk is ready (28 kB)\r\n2.712 chunk is ready (28 kB)\r\n2.742 chunk is ready (28 kB)\r\n2.794 chunk is ready (28 kB)\r\n2.850 chunk is ready (28 kB)\r\n2.904 chunk is ready (28 kB)\r\n2.961 chunk is ready (28 kB)\r\n2.987 chunk is ready (28 kB)\r\n3.050 chunk is ready (28 kB)\r\n3.102 chunk is ready (28 kB)\r\n3.158 chunk is ready (28 kB)\r\n3.186 chunk is ready (28 kB)\r\n3.240 chunk is ready (28 kB)\r\n3.294 chunk is ready (28 kB)\r\n3.352 chunk is ready (28 kB)\r\n3.379 chunk is ready (28 kB)\r\n3.432 chunk is ready (28 kB)\r\n3.484 chunk is ready (28 kB)\r\n3.542 chunk is ready (28 kB)\r\n3.595 chunk is ready (28 kB)\r\n3.622 chunk is ready (28 kB)\r\n6.450 chunk is ready (1640 kB)\r\n10.036 chunk is ready (1640 kB)\r\n13.425 chunk is ready (1638 kB)\r\n16.362 chunk is ready (1638 kB)\r\n19.577 chunk is ready (1637 kB)\r\n24.001 chunk is ready (1639 kB)\r\n27.406 chunk is ready (1641 kB)\r\n30.667 chunk is ready (1639 kB)\r\n33.486 chunk is ready (1640 kB)\r\n36.340 chunk is ready (1638 kB)\r\n39.177 chunk is ready (1640 kB)\r\n42.044 chunk is ready (1639 kB)\r\n44.975 chunk is ready (1639 kB)\r\n47.787 chunk is ready (1639 kB)\r\n50.691 chunk is ready (1637 kB)\r\n53.663 chunk is ready (1639 kB)\r\n63.379 chunk is ready (1639 kB)\r\n70.150 chunk is ready (1638 kB)\r\n74.136 chunk is ready (1638 kB)\r\n77.357 chunk is ready (1640 kB)\r\n80.215 chunk is ready (1639 kB)\r\n80.216 done\r\n```\r\n\r\nThe `num_fallback_workers=12` was superfluous because it didn't actually fall back on `HTTPSource`. Removing that doesn't affect the time (as it would if it fell back on a synchronous `HTTPSource`; synchronous is considerably slower than asynchronous).\r\n\r\n```\r\n>>> time_profile(uproot4.source.http.HTTPMultipartSource(\"https://chrisburr.me/B2HHH_MagnetDown.root\"), basic_ranges)\r\n0.000 got file object\r\n0.431 obtained chunk futures\r\n0.745 chunk is ready (28 kB)\r\n0.883 chunk is ready (28 kB)\r\n0.971 chunk is ready (28 kB)\r\n1.023 chunk is ready (28 kB)\r\n1.051 chunk is ready (28 kB)\r\n1.126 chunk is ready (28 kB)\r\n1.184 chunk is ready (28 kB)\r\n1.232 chunk is ready (28 kB)\r\n1.288 chunk is ready (28 kB)\r\n1.315 chunk is ready (28 kB)\r\n1.372 chunk is ready (28 kB)\r\n1.431 chunk is ready (28 kB)\r\n1.487 chunk is ready (28 kB)\r\n1.516 chunk is ready (28 kB)\r\n1.566 chunk is ready (28 kB)\r\n1.621 chunk is ready (28 kB)\r\n1.675 chunk is ready (28 kB)\r\n1.702 chunk is ready (28 kB)\r\n1.755 chunk is ready (29 kB)\r\n1.810 chunk is ready (28 kB)\r\n1.862 chunk is ready (28 kB)\r\n1.916 chunk is ready (28 kB)\r\n1.944 chunk is ready (28 kB)\r\n1.999 chunk is ready (28 kB)\r\n2.054 chunk is ready (28 kB)\r\n2.107 chunk is ready (28 kB)\r\n2.136 chunk is ready (28 kB)\r\n2.188 chunk is ready (28 kB)\r\n2.243 chunk is ready (28 kB)\r\n2.297 chunk is ready (28 kB)\r\n2.322 chunk is ready (28 kB)\r\n2.377 chunk is ready (28 kB)\r\n2.430 chunk is ready (28 kB)\r\n2.484 chunk is ready (28 kB)\r\n2.536 chunk is ready (28 kB)\r\n2.563 chunk is ready (28 kB)\r\n2.615 chunk is ready (28 kB)\r\n2.672 chunk is ready (28 kB)\r\n2.723 chunk is ready (28 kB)\r\n2.750 chunk is ready (28 kB)\r\n2.802 chunk is ready (28 kB)\r\n2.859 chunk is ready (28 kB)\r\n2.912 chunk is ready (28 kB)\r\n2.966 chunk is ready (28 kB)\r\n2.992 chunk is ready (28 kB)\r\n3.047 chunk is ready (28 kB)\r\n3.098 chunk is ready (28 kB)\r\n3.153 chunk is ready (28 kB)\r\n3.179 chunk is ready (28 kB)\r\n3.233 chunk is ready (28 kB)\r\n3.286 chunk is ready (28 kB)\r\n3.534 chunk is ready (28 kB)\r\n3.534 chunk is ready (28 kB)\r\n3.534 chunk is ready (28 kB)\r\n3.534 chunk is ready (28 kB)\r\n3.534 chunk is ready (28 kB)\r\n3.581 chunk is ready (28 kB)\r\n3.610 chunk is ready (28 kB)\r\n6.435 chunk is ready (1640 kB)\r\n9.238 chunk is ready (1640 kB)\r\n12.335 chunk is ready (1638 kB)\r\n15.153 chunk is ready (1638 kB)\r\n17.931 chunk is ready (1637 kB)\r\n20.832 chunk is ready (1639 kB)\r\n23.571 chunk is ready (1641 kB)\r\n26.457 chunk is ready (1639 kB)\r\n29.377 chunk is ready (1640 kB)\r\n32.407 chunk is ready (1638 kB)\r\n35.251 chunk is ready (1640 kB)\r\n38.194 chunk is ready (1639 kB)\r\n40.957 chunk is ready (1639 kB)\r\n43.802 chunk is ready (1639 kB)\r\n46.684 chunk is ready (1637 kB)\r\n49.649 chunk is ready (1639 kB)\r\n52.526 chunk is ready (1639 kB)\r\n55.345 chunk is ready (1638 kB)\r\n58.201 chunk is ready (1638 kB)\r\n61.182 chunk is ready (1640 kB)\r\n64.095 chunk is ready (1639 kB)\r\n64.096 done\r\n```\r\n\r\nCommunicating over the same distance with the same server, but switching `HTTPMultipartSource` with an explicit `HTTPSource` changes the time to\r\n\r\n```\r\n>>> time_profile(uproot4.source.http.HTTPSource(\"https://chrisburr.me/B2HHH_MagnetDown.root\", num_workers=12), basic_ranges)\r\n0.000 got file object\r\n0.001 obtained chunk futures\r\n1.453 chunk is ready (28 kB)\r\n1.734 chunk is ready (28 kB)\r\n1.734 chunk is ready (28 kB)\r\n1.931 chunk is ready (28 kB)\r\n1.931 chunk is ready (28 kB)\r\n1.931 chunk is ready (28 kB)\r\n1.931 chunk is ready (28 kB)\r\n1.931 chunk is ready (28 kB)\r\n1.931 chunk is ready (28 kB)\r\n1.931 chunk is ready (28 kB)\r\n1.931 chunk is ready (28 kB)\r\n2.251 chunk is ready (28 kB)\r\n3.394 chunk is ready (28 kB)\r\n3.394 chunk is ready (28 kB)\r\n3.394 chunk is ready (28 kB)\r\n3.394 chunk is ready (28 kB)\r\n3.394 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (29 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.395 chunk is ready (28 kB)\r\n3.774 chunk is ready (28 kB)\r\n4.018 chunk is ready (28 kB)\r\n4.018 chunk is ready (28 kB)\r\n4.054 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n4.567 chunk is ready (28 kB)\r\n5.989 chunk is ready (28 kB)\r\n5.989 chunk is ready (28 kB)\r\n5.989 chunk is ready (28 kB)\r\n5.989 chunk is ready (28 kB)\r\n5.989 chunk is ready (28 kB)\r\n36.149 chunk is ready (1640 kB)\r\n42.189 chunk is ready (1640 kB)\r\n42.189 chunk is ready (1638 kB)\r\n42.189 chunk is ready (1638 kB)\r\n46.050 chunk is ready (1637 kB)\r\n46.050 chunk is ready (1639 kB)\r\n48.438 chunk is ready (1641 kB)\r\n48.438 chunk is ready (1639 kB)\r\n48.438 chunk is ready (1640 kB)\r\n48.438 chunk is ready (1638 kB)\r\n48.438 chunk is ready (1640 kB)\r\n48.438 chunk is ready (1639 kB)\r\n68.315 chunk is ready (1639 kB)\r\n68.630 chunk is ready (1639 kB)\r\n72.884 chunk is ready (1637 kB)\r\n72.884 chunk is ready (1639 kB)\r\n72.884 chunk is ready (1639 kB)\r\n72.884 chunk is ready (1638 kB)\r\n72.884 chunk is ready (1638 kB)\r\n74.251 chunk is ready (1640 kB)\r\n74.360 chunk is ready (1639 kB)\r\n74.362 done\r\n```\r\n\r\nwhich is not significantly different. Let's do it once more for good measure.\r\n\r\n```\r\n>>> time_profile(uproot4.source.http.HTTPSource(\"https://chrisburr.me/B2HHH_MagnetDown.root\", num_workers=12), basic_ranges)\r\n0.000 got file object\r\n0.001 obtained chunk futures\r\n1.055 chunk is ready (28 kB)\r\n1.135 chunk is ready (28 kB)\r\n1.515 chunk is ready (28 kB)\r\n1.515 chunk is ready (28 kB)\r\n1.777 chunk is ready (28 kB)\r\n1.777 chunk is ready (28 kB)\r\n1.777 chunk is ready (28 kB)\r\n1.777 chunk is ready (28 kB)\r\n1.777 chunk is ready (28 kB)\r\n2.951 chunk is ready (28 kB)\r\n2.951 chunk is ready (28 kB)\r\n2.951 chunk is ready (28 kB)\r\n2.951 chunk is ready (28 kB)\r\n2.951 chunk is ready (28 kB)\r\n2.951 chunk is ready (28 kB)\r\n2.951 chunk is ready (28 kB)\r\n2.951 chunk is ready (28 kB)\r\n2.951 chunk is ready (28 kB)\r\n2.951 chunk is ready (29 kB)\r\n2.951 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n3.511 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.459 chunk is ready (28 kB)\r\n4.828 chunk is ready (28 kB)\r\n4.828 chunk is ready (28 kB)\r\n4.828 chunk is ready (28 kB)\r\n4.828 chunk is ready (28 kB)\r\n4.828 chunk is ready (28 kB)\r\n4.828 chunk is ready (28 kB)\r\n4.828 chunk is ready (28 kB)\r\n4.828 chunk is ready (28 kB)\r\n4.828 chunk is ready (28 kB)\r\n4.829 chunk is ready (28 kB)\r\n5.010 chunk is ready (28 kB)\r\n5.852 chunk is ready (28 kB)\r\n38.723 chunk is ready (1640 kB)\r\n38.723 chunk is ready (1640 kB)\r\n38.723 chunk is ready (1638 kB)\r\n51.667 chunk is ready (1638 kB)\r\n51.667 chunk is ready (1637 kB)\r\n51.667 chunk is ready (1639 kB)\r\n51.667 chunk is ready (1641 kB)\r\n51.667 chunk is ready (1639 kB)\r\n55.895 chunk is ready (1640 kB)\r\n55.895 chunk is ready (1638 kB)\r\n55.895 chunk is ready (1640 kB)\r\n55.895 chunk is ready (1639 kB)\r\n67.649 chunk is ready (1639 kB)\r\n71.511 chunk is ready (1639 kB)\r\n72.109 chunk is ready (1637 kB)\r\n72.109 chunk is ready (1639 kB)\r\n72.109 chunk is ready (1639 kB)\r\n73.113 chunk is ready (1638 kB)\r\n75.804 chunk is ready (1638 kB)\r\n75.804 chunk is ready (1640 kB)\r\n75.804 chunk is ready (1639 kB)\r\n75.808 done\r\n```\r\n\r\nAt least for this London \u2192 Chicago transfer, the difference between single background thread multipart and 12 background threads single-part each is in the noise of multiple retries. But multipart support is so variable\u2014and particularly, detecting failure is so variable\u2014that maybe it shouldn't be the default.",
  "created_at":"2020-05-15T13:36:03Z",
  "id":629239803,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTIzOTgwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T13:36:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"For XRootD, it is multi-thread but single-connection right? You are using the same pyxrootd File instance? Same question for HTTP multi-thread. I would say that if it is concurrently fetching through the same socket then practically no difference from multipart should be expected. And if not, only the connection spin-up time is paid when not using multipart.",
  "created_at":"2020-05-15T13:46:59Z",
  "id":629245354,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTI0NTM1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T13:46:59Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> For XRootD, it is multi-thread but single-connection right? You are using the same pyxrootd File instance? Same question for HTTP multi-thread. I would say that if it is concurrently fetching through the same socket then practically no difference from multipart should be expected.\r\n\r\n`XRootDSource` and `HTTPSource` are both multi-thread, single-connection each. Specifically, they are both thread pools and each chunk (eventually representing a TBasket) are tasks where the task performs the full request-response cycle for the one chunk. The `HTTPSConnection` object (a TCP connection) is tied to the thread pool, so it's really a threads-and-connections pool.\r\n\r\nThus, having fewer threads (\"workers\") synchronizes the process: only as many chunks/TBaskets can be in flight as the number of threads. Having more threads could cause contention on the network interface: it might context-switch too much. The ideal number of threads probably doesn't have much to do with the number of CPU cores but is a property of the network interface (and if there's more than one of them).\r\n\r\nThe `HTTPMultipartSource` is a single background thread that asks for all of the chunks/TBaskets in a single request. The single response is parsed by that thread. While it reads through the parts, it sends each result to a Chunk's Future, so interpretation can happen while the multipart data are being downloaded. I can see this making a difference for situations with many small chunks/TBaskets, but we haven't observed it in these cases yet.\r\n\r\nI have not investigated XRootD's `vector_read` yet, though that could be handled in a similar way to `HTTPMultipartSource`, with the exception that XRootD servers probably have better support for `vector_read` than HTTP servers have for multpart GETs. (You can find a lot of discussion online about _uploading_ form data as a multipart, but one of the few references I can find to multipart GET is a StackOverflow question asking why it even exists: see #3.)",
  "created_at":"2020-05-15T14:03:59Z",
  "id":629254343,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTI1NDM0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T14:09:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"So let me point out that when you first use callbacks within a pyxrootd File object, xrootd internally sets up its own threadpool to manage the number of connections and threads. I think you should probably let xrootd take care of that and have your thread pool share a common pyxrootd File object. In effect, you would just be using your threadpool to enable concurrent fetching.",
  "created_at":"2020-05-15T14:12:04Z",
  "id":629258212,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTI1ODIxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T14:13:12Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> And if not, only the connection spin-up time is paid when not using multipart.\r\n\r\nConnections are attached to the thread pool, which is part of the new file object's context manager. Unlike before, I'm now taking `__exit__` seriously as a way to close the file, and by \"close the file\" I mean shutdown a thread pool and all of its open TCP connections.",
  "created_at":"2020-05-15T14:12:06Z",
  "id":629258223,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTI1ODIyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T14:12:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Same actually for HTTP, I would let urllib decide how to open connections rather than try to force it myself.",
  "created_at":"2020-05-15T14:15:31Z",
  "id":629259925,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTI1OTkyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T14:15:31Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> So let me point out that when you first use callbacks within a pyxrootd File object, xrootd internally sets up its own threadpool to manage the number of connections and threads. I think you should probably let xrootd take care of that and have your thread pool share a common pyxrootd File object. In effect, you would just be using your threadpool to enable concurrent fetching.\r\n\r\nI agree that the multithreading should only be done in one place. I wish I understood pyxrootd better, but I seem to remember attempting to use pyxrootd's built-in multithreading and it didn't work.\r\n\r\n> Same actually for HTTP, I would let urllib decide how to open connections rather than try to force it myself.\r\n\r\nurllib does multithreaded HTTP requests?\r\n\r\nI'm in favor of using the prepackaged solutions, but I want to know that it really does what I need\u2014without error\u2014before I defer to it.\r\n\r\nThe main decision I need to make now is whether this interface: (1) ask for byte ranges and get them immediately as asynchronous Chunks and (2) they fill in the background while the interpretation code does such things as decompression and converting raw bytes into Python attributes or NumPy/Awkward arrays. I got into this situation in Uproot 1\u20123 by assuming that a memmap-like interface would be sufficient, in which we (1) ask for bytes as we need them and (2) let them be filled in equal-sized chunks if not in cache. This fails because requests can't be in flight (which was patched, so Uproot 3 can do that) and because the byte ranges couldn't be tailored to the actual sizes and positions of the TBaskets. I think that ServiceX is hitting that right now: ssl-hep/ServiceX#131 (the issue is more obvious in one of the plots that was presented; Uproot is reading a lot more bytes from the file than ROOT, likely because of this \"slop\" of equal-sized chunks around the actual TBaskets).\r\n\r\nSo I'm doing these tests now to find out if the current interface is also too naive in some way. If the same interface can be obtained, replacing my own background threads with pyxrootd's or urllib's, then that would be great. Also\u2014hint, hint\u2014there are people who are much more knowledgeable about these things than I am. `:)`",
  "created_at":"2020-05-15T14:36:00Z",
  "id":629270303,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTI3MDMwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T14:36:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I've been doing a lot of work around laurelin's I/O recently, so this is conveniently-timed.\r\n\r\nYou can get a pretty good speedup from xrootd's VectorRead call, particularly when the hosting server is exposing the storage as a plugin to xrootd (and not a POSIX filesystem), since the vectored-ness of the read can be passed all the way down to the FS. If you've got it set up to where you have the offset/lengths of all the TBaskets that need to be read collected in one place, then it's also worthwhile to coalesce adjacent reads. e.g. if baskets are separated by just the TKey, then do just one read and throw away the unneeded bytes instead of issuing two reads.\r\n\r\nIn XRootD's case, you should additionally look at the async versions. In pyxrootd, this is implemented by providing a callback function to be called when the I/O completes. It's not the best interface (the entire I/O has to complete before the callback is called, so you can't progressively dispatch the decompression as the bytes come in), but you can drop dealing with threads on the I/O end that way",
  "created_at":"2020-05-15T14:52:19Z",
  "id":629278568,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTI3ODU2OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-05-15T14:52:19Z",
  "user":"MDQ6VXNlcjkzMzU0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Regarding xrootd's threadpool, I used it without issue in https://github.com/nsmith-/aioroot/blob/master/aioroot/xrootd.py where I basically turn the callback-based async access to `pyxrootd.client.File` object into an async-await style. The callback-based `pyxrootd.client.File` object maintains a queue of requests and a pool of threads in the background. The only caution might be that since I used an event loop, I never had multiple threads trying to access `pyxrootd.client.File` at the same time. You might need to acquire a lock while calling into it, which is OK since it doesn't block if you use callbacks.\r\nAs for urllib, check out https://urllib3.readthedocs.io/en/1.4/pools.html\r\n\r\nI absolutely agree that uniform chunks cause lots of issues. See for example this plot where I scan the amount of bytes read from a file vs. the chunksize and cache size in uproot3, for the same amount of array data accessed:\r\n<img width=\"659\" alt=\"Screen Shot 2020-05-15 at 9 52 43 AM\" src=\"https://user-images.githubusercontent.com/6587412/82063965-ca7f8300-9691-11ea-8745-a14ab854e917.png\">\r\nNotice that this plot implies a \"slop factor\" of at least 2 even when the chunksize is only 100k for the particular (NanoAOD) file I was reading.\r\n",
  "created_at":"2020-05-15T14:52:40Z",
  "id":629278760,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTI3ODc2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T14:58:49Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Fortunately, the new interface doesn't require the `limitbytes` or `chunkbytes` parameters anymore: instead of `limitbytes` on an LRU cache, the lifetimes of chunks will be determined by the interpretation code (which is why the interpretation code needs to change) and instead of `chunkbytes`, we read the right number of bytes.\r\n\r\n`num_workers` remains a free parameter, though. (That used to be called `parallel`.)\r\n\r\nI'll look at your aioroot/xrootd.py, or maybe @chrisburr will, since he's also interested in vector_read in Python.",
  "created_at":"2020-05-15T15:04:50Z",
  "id":629285489,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTI4NTQ4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T15:04:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I spent a little time playing with pyxrootd vector_read but it appeared to massively leak memory for me, so I stopped investigating.",
  "created_at":"2020-05-15T15:19:32Z",
  "id":629300538,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTMwMDUzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T15:19:32Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Brian Bockelman pointed out that we get HTTP access directly from CERN's EOS servers so we can also access the file with:\r\n\r\n```python\r\nimport ssl\r\n# EOS uses untrusted certificates, use the CERN Grid CA or ssl._create_unverified_context\r\nssl._create_default_https_context = lambda *args, **kwargs: ssl.create_default_context(capath='/cvmfs/grid.cern.ch/etc/grid-security/certificates')\r\n\r\ntime_profile(uproot4.source.http.HTTPSource(\"https://eospublichttp.cern.ch/eos/opendata/lhcb/AntimatterMatters2017/data/B2HHH_MagnetDown.root\", num_workers=12), basic_ranges)\r\ntime_profile(uproot4.source.http.HTTPMultipartSource(\"https://eospublichttp.cern.ch/eos/opendata/lhcb/AntimatterMatters2017/data/B2HHH_MagnetDown.root\", num_fallback_workers=None), basic_ranges)\r\n```\r\n\r\nMultipart and HTTP2 are both supported and multipart requests are considerable faster than using a large number of threads. I think the lack of multipart support on S3 shouldn't influence the decision of default too much as the majority of servers do support multipart ranges.\r\n\r\nUsing EOS HTTP also exposes the issue that redirects result in an error due to the use of the standard library instead of `requests` (http redirects to https). I think the extra dependency on `requests` is probably worth it to avoid an endless stream of edge cases like this.",
  "created_at":"2020-05-15T18:05:01Z",
  "id":629401929,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTQwMTkyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T18:05:01Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"I think I've learned what I needed to know with these low-level tests.\r\n\r\nSomeday, I'll need to follow up with full-system tests.\r\n\r\nThe reading part of Uproot4 is more than half-way done, so that could be relatively soon.",
  "created_at":"2020-06-17T23:43:17Z",
  "id":645682973,
  "issue":2,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NTY4Mjk3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-17T23:43:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"My first example of how a multipart range request can fail (by sending a response code other than 206) came from github.io. This one came from Amazon S3 (through HTTPS). The [S3 documentation](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html) says that it's not supported:\r\n\r\n![Screenshot from 2020-05-14 20-01-12](https://user-images.githubusercontent.com/1852447/82000793-09c0bc00-961f-11ea-81e1-b6eb52f2dfdf.png)\r\n\r\nbut the specific way that it fails is by responding with response code 206 and _not_ sending multipart data; instead it sends _only one part_. So the heuristic for checking is if the actual length matches one of the ranges, we give up. (Note that even a multipart request with only one part would have a length that doesn't match because it has to have the separator and header.)\r\n\r\nThere are probably other ways that it can fail.\r\n\r\nI can't find a lot of documentation on multipart GET. [This guy on StackOverflow](https://stackoverflow.com/q/55665466/1623645) is even asking, \"What are they good for?\" and gets NO RESPONSE. That's not a good sign.",
  "created_at":"2020-05-15T01:15:47Z",
  "id":628968215,
  "issue":3,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyODk2ODIxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T01:15:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is definitely faster than the multithreaded version: `XRootDSource` typically returned after 200\u2012500 seconds while `XRootDVectorReadSource` returned after 60\u2012200 seconds. From the waterfall of times, I think that the multithreaded version is more affected by stragglers. Actually, it's the fact that the multithreaded version is partly synchronized by the fact that the number of workers is much smaller than the number of chunks... The vector_read is qualitatively better because the server gets the whole request up-front...\r\n\r\nI un-reversed the order of chunks (`while True: ranges.pop()` \u2192 `for start, stop in ranges`). I'm still figuring out how the interpretation code is going to use this\u2014which is my primary intent from these performance tests\u2014but it would be nice if I can expect the outputs to come out in roughly the order of request (which the `time_profile` test code explicitly assumes). Without that assumption, the interpretation of baskets (mostly decompression) would _have_ to be multithreaded and triggered by chunks finishing (which is an interface change from what I have now, so thinking about this is useful!).\r\n\r\nMaybe optimization comes in the form of distributing chunk sizes well among the `all_request_ranges`? Would it make sense to distribute `ranges` among the `all_request_ranges` such that the total number of bytes in each set is more uniform? The fact that these B2HHH.root tests all have small baskets followed by large baskets is a reminder that we often see patterns like that (because ROOT's basket-filling algorithm tunes itself as it goes forward).\r\n\r\nDo you want to make more changes to this before I commit it? I will probably remove the original `XRootDSource` and maybe rename this one, if vector_read is stable, available everywhere, and doesn't have memory leaking issues.",
  "created_at":"2020-05-15T20:34:31Z",
  "id":629469141,
  "issue":4,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTQ2OTE0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T20:34:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I would merge this if you have no other additions. Let me know!\r\n\r\nAfter this PR, there are a few things that will happen:\r\n   * The names will be changed so that this is `XRootDSource` and the other one, if it continues to exist at all, will have a highly specialized name. Unless there are problems with the vector_read version or we need the other one to exist as a fallback, the vector_read version will be the only XRootD source, as you suggest.\r\n   * The `chunks` method (on all Sources) will return a queue of completed chunks so that I don't have to rely on them being finished in order and can start decompressing/interpreting them in the order that they have been read.\r\n   * I will also have to add a simpler interface for single-chunk access, since there are many occasions in which we only want one contiguous block of bytes and we need it synchronously before we can look for the next one (e.g. an item fetched from a TDirectory or all of the streamers). That will probably be `chunk` vs `chunks`.",
  "created_at":"2020-05-15T21:29:04Z",
  "id":629498985,
  "issue":4,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTQ5ODk4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T21:29:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll reply tomorrow but go ahead and merge if you're ready \ud83d\udc4d ",
  "created_at":"2020-05-15T21:37:06Z",
  "id":629503716,
  "issue":4,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTUwMzcxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T21:37:06Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay! I figured it was late there and I'd catch you on the next cycle, but thanks!",
  "created_at":"2020-05-15T21:50:37Z",
  "id":629511412,
  "issue":4,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTUxMTQxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T21:50:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"hi @chrisburr -- how does this handle authentication to an xrootd server?",
  "created_at":"2020-05-15T22:19:25Z",
  "id":629527864,
  "issue":4,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTUyNzg2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T22:19:25Z",
  "user":"MDQ6VXNlcjIzMTgwODM="
 },
 {
  "author_association":"MEMBER",
  "body":"@lukasheinrich Primarily environment variables (`XRD_.*`/`X509_.*`) the same way as the command line client/C++ library/ROOT does.",
  "created_at":"2020-05-15T23:09:09Z",
  "id":629542864,
  "issue":4,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTU0Mjg2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T23:09:09Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"If pyxrootd works by forking a subprocess, we can perhaps control those environment variables programmatically from Python. (It's my understanding that setting `os.environ` only affects subprocesses.)",
  "created_at":"2020-05-15T23:43:44Z",
  "id":629550634,
  "issue":4,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTU1MDYzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-15T23:43:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the clean up, this looks much better now.\r\n\r\n> Maybe optimization comes in the form of distributing chunk sizes well among the all_request_ranges?\r\n\r\nI'd guess the optimisation comes from most storage being configured to work better when requesting larger chunks of data to minimise the overhead of each request. This matches the most common operations: copies of the entire file or vector reads from ROOT. This might be especially important considering the file is physically on a different server and might even be split across many servers (e.g. ECHO at RAL).\r\n\r\nThis also makes me realise I should run my grid tests with the file at different storage elements as there can be a lot of quirks between different setups.\r\n\r\n> Would it make sense to distribute ranges among the all_request_ranges such that the total number of bytes in each set is more uniform?\r\n\r\nThis feels like a premature optimisation, it could also be better to make requests which are spatially closer in the file. I think the requests should be optimised to return data in the most useful groups for uproot, e.g. small chunks of metadata in one request and sequential baskets in another.\r\n\r\n> If pyxrootd works by forking a subprocess, we can perhaps control those environment variables programmatically from Python.\r\n\r\nI think this is a bad idea. Everywhere I'm familiar with it supposed to \"just work\" (e.g. lxplus, local kerberos, local grid proxies) or these variables are intended to be set up without users knowing about it (e.g. grid jobs setting `X509_`). Keeping code independent of authentication is a feature that helps make collaborative working and long term preservation easier.\r\n\r\n> It's my understanding that setting os.environ only affects subprocesses.\r\n\r\nOn most systems the changes to `os.environ` affect everything except things that have already done what they were going to with the environment. From the Python docs:\r\n\r\n> When putenv() is supported, assignments to items in os.environ are automatically translated into corresponding calls to putenv(); however, calls to putenv() don\u2019t update os.environ, so it is actually preferable to assign to items of os.environ.\r\n",
  "created_at":"2020-05-16T08:25:04Z",
  "id":629608789,
  "issue":4,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTYwODc4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-16T08:25:04Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"Are the fallbacks accessible in the `XRootD.client.URL` object?\r\n\r\nRecovering from a nonexistent URL takes quite a while. The `timeout` has to be an integer number of seconds and passing \"1\" means as much as 5\u201210 seconds in practice. (Eventually, I'll be turning off unit test #1, or at least making it part of a \"long tests\" suite.)",
  "created_at":"2020-05-16T13:14:42Z",
  "id":629643933,
  "issue":5,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYyOTY0MzkzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-16T13:14:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That's great\u2014I didn't know that and will take advantage of it soon.",
  "created_at":"2020-05-22T17:45:08Z",
  "id":632825856,
  "issue":12,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMjgyNTg1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-22T17:45:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I learned about it from @chrisburr :)",
  "created_at":"2020-05-23T02:50:09Z",
  "id":632974498,
  "issue":12,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzMjk3NDQ5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-23T02:50:09Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Done in #18.",
  "created_at":"2020-06-02T13:59:02Z",
  "id":637561407,
  "issue":12,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNzU2MTQwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-02T13:59:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@oshadura This provides access to RNTuple in Uproot4. The test shows the \"right way\" to use it:\r\n* context manager for the file (`f` is actually a TDirectory that points back to the TFile through `f.file`)\r\n* get the ROOT::Experimental::RNTuple through `f[\"Staff\"]`\r\n* get its members through `obj.member(\"fWhatever\")`\r\n* read a single chunk of raw bytes (immediately), such as the data corresponding to the RNTuple header, with `f.file.source.chunk`\r\n* put in a request (might run in the background) for multiple chunks with `f.file.source.chunks` (for XRootD, this is a VectorRead; for HTTP, this is a multipart-GET)\r\n* print out bytes for debugging with `cursor.debug(chunk, length)`; shows their decimal values (more useful than hex, I've found), string interpretation, and optionally numerical interpretation (pass a dtype and offset)\r\n* actually interpret the header using `cursor.fields(struct)`, `cursor.string()`, etc.\r\n\r\nThe `Cursor` is a pointer to some position of the file, the `Source` is an object that delivers data (local file, XRootD, HTTP), and a `Chunk` is a range of bytes taken from the `Source`. `Cursors` only operate on a `Chunk`, not the `Source` (that's a change from old Uproot, allowing for more lazy file access). `Chunks` may be filled in the background (depending on the type of source), but when a `Cursor` asks for something from the `Chunk`, it waits until such data are available.\r\n\r\nWhen you get to the point of reading a bunch of chunks for RNTuple pages, note that you can [pass a notification queue](https://github.com/scikit-hep/uproot4/blob/52cc61a277773d22098090f2e5a2e5a39f684340/tests/test_0006-notify-when-downloaded.py#L196-L211) to `Source.chunks` so that you can initiate page-decompression as soon as the page is ready. I'll be doing the same for TTree baskets, so you may be able to follow my example.\r\n\r\nIf the data you're pointing to could be compressed, you'll probably want to follow [TKey.get_uncompressed_chunk_cursor](https://github.com/scikit-hep/uproot4/blob/52cc61a277773d22098090f2e5a2e5a39f684340/uproot4/reading.py#L660-L684). If you know that what you have is compressed, but haven't yet read the 9-byte compression header, call [uproot4.compression.decompress](https://github.com/scikit-hep/uproot4/blob/52cc61a277773d22098090f2e5a2e5a39f684340/uproot4/compression.py#L146) to turn a `Chunk` of compressed bytes into a `Chunk` of uncompressed bytes. Note that the returned `Chunk` starts at `Cursor(0)`, rather than the seek point in the file (like the `Cursor` you pass to this function), because there isn't a one-to-one relationship between uncompressed byte positions and compressed byte positions. The `context` is unused but included as a formality; it can be an empty dict. You do need to know the number of `compressed_bytes` and `uncompressed_bytes` so that the output array can be allocated, but I think you know this information from RNTuple's equivalent of `TKey::fNbytes` and `TKey::fObjlen`.\r\n\r\nDo you want me to give you write access to the uproot4 repo in case you need to make any upstream changes to get your things to work?",
  "created_at":"2020-05-28T12:54:31Z",
  "id":635331923,
  "issue":13,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNTMzMTkyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-28T12:54:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks Jim and thanks for an explanation ! (about access it will nice, but in case if you prefer I can jus t open PR and ping you there)",
  "created_at":"2020-05-28T13:54:18Z",
  "id":635364759,
  "issue":13,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzNTM2NDc1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-05-28T13:54:18Z",
  "user":"MDQ6VXNlcjcwMTI0MjA="
 },
 {
  "author_association":"MEMBER",
  "body":"You can set fail-fast to false to allow all the jobs to run, rather that quitting as soon as one fails.",
  "created_at":"2020-06-04T12:32:15Z",
  "id":638818244,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODgxODI0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T12:32:15Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Looks like conda may be installed on Linux only.",
  "created_at":"2020-06-04T12:33:08Z",
  "id":638818672,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODgxODY3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T12:33:08Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Conda is pre-installed in Windows too, just not macOS: https://github.com/actions/virtual-environments/blob/master/images/win/Windows2019-Readme.md",
  "created_at":"2020-06-04T12:34:28Z",
  "id":638819240,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODgxOTI0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T12:34:40Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"You'll probably need `shell: bash -l {0}`, and it will *really* slow down your tests to preinstall conda each time. How about just testing conda on linux and windows, and then having a pip-based test elsewhere?",
  "created_at":"2020-06-04T12:37:21Z",
  "id":638820664,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODgyMDY2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T12:37:21Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Keep in mind in multiple OS testing, Windows defaults to powershell, you need `shell: bash` to get the bash shell on Windows. (All OS's have powershell-core, so you can go all powershell, too)",
  "created_at":"2020-06-04T12:39:09Z",
  "id":638821491,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODgyMTQ5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T12:39:09Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"`conda init bash` is the wrong thing to use for the setup-miniconda action - that needs the `shell: bash -l {0}` instead. Also conda init bash requires that you restart your shell, IIRC.",
  "created_at":"2020-06-04T12:55:29Z",
  "id":638829372,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODgyOTM3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T12:55:29Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the help, @henryiii\u2014I haven't been ignoring you; I just noticed all of these messages where you warned me of just about everything I stumbled over.",
  "created_at":"2020-06-04T13:06:01Z",
  "id":638834832,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODgzNDgzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T13:06:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Conda is pre-installed on all OS's, including macOS. I'm discussing with GitHub Actions team, will have an update here soon.",
  "created_at":"2020-06-04T13:33:33Z",
  "id":638850349,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODg1MDM0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T13:33:33Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> You can set fail-fast to false to allow all the jobs to run, rather that quitting as soon as one fails.\r\n\r\nThe fail-fast is new to me in GitHub Actions (if it was possible in Travis or Azure, I didn't know about it), but I like it. Usually, when a commit is wrong, it's wrong in all platforms because it's a coding bug. (GHA's flake8 catches more errors than mine, and I need to figure out how to get that version! Oh, 3.7 vs 3.8... maybe that's it.)\r\n\r\nOccasionally, I really do need it to not fail-fast, such as a recent search on scikit-hep/uproot#493 for the `struct` syntax for that platform-dependent `long` type, and I'll turn off fail-fast if I'm in one of those situations.",
  "created_at":"2020-06-04T13:39:03Z",
  "id":638853720,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODg1MzcyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T13:39:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"They've changed the defaults and names, but otherwise, GHA is pretty much identical to Azure. And it was also possible to enable fail-fast in Travis, but don't remember the name; CI systems like fail-fast, after all. :)\r\n\r\nGitHub CI will fail on a multiline command if a command in the middle fails, which is *fantastic*, whereas Azure will fail only if the final command fails, which is awful - they said it was too late to change the default in Azure so they only changed it when they developed GHA.",
  "created_at":"2020-06-04T13:47:41Z",
  "id":638859118,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzODg1OTExOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T13:47:41Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"The Uproot HATS is done, so I'll close off this PR and give it a version number.",
  "created_at":"2020-06-04T18:24:41Z",
  "id":639027599,
  "issue":19,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzOTAyNzU5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T18:24:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Should this one be deleted now?",
  "created_at":"2020-06-04T19:00:48Z",
  "id":639056174,
  "issue":20,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzOTA1NjE3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T19:00:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think so - it's pretty easy to setup if we need more info, but most of the relevant errors have been copied to the GHA issue.",
  "created_at":"2020-06-04T19:18:14Z",
  "id":639064273,
  "issue":20,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzOTA2NDI3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T19:18:14Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I used to have a requirements-test.txt, but I needed XRootD to depend on Python version and platform. I suppose I could make it depend on Python version in the requirements-test.txt. Or maybe introduce requirements-test.txt and requirements-test-extra.txt, where the latter has only XRootD.",
  "created_at":"2020-06-04T19:00:17Z",
  "id":639055917,
  "issue":21,
  "node_id":"MDEyOklzc3VlQ29tbWVudDYzOTA1NTkxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-04T19:00:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"All of the old Uproot tests have been ported over except those in\r\n\r\n   * test_tree.py\r\n   * test_issues.py\r\n\r\nand some of the ones that were ported are skipped.",
  "created_at":"2020-06-17T23:39:56Z",
  "id":645682005,
  "issue":23,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NTY4MjAwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-17T23:39:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, this conversation is happening here: https://github.com/scikit-hep/uproot/issues/502#issuecomment-656864720",
  "created_at":"2020-07-11T18:44:26Z",
  "id":657110150,
  "issue":24,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NzExMDE1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-11T18:44:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"One of the design decisions in uproot 3's writing code that seemed wrong in hindsight was using high level functions to dictate the serialization for objects that uproot claims to be able to write instead of using separate classes with their own write methods. \r\n\r\nFor eg - \r\nWe have a `put_tleaf` function like here - https://github.com/scikit-hep/uproot/blob/master/uproot/write/objects/TTree.py#L588 instead of having a TLeaf class with its own write method. \r\n\r\nOne of the negative side effect of having done this was that we had to use hacky methods to handle TObjArrays where we had to pass the classnames as strings and have special conditions to handle different objects in the logic for TObjArray - https://github.com/scikit-hep/uproot/blob/master/uproot/write/objects/util.py#L33.",
  "created_at":"2020-06-18T06:15:52Z",
  "id":645802646,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY0NTgwMjY0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-06-18T06:15:52Z",
  "user":"MDQ6VXNlcjExNzc1NjE1"
 },
 {
  "author_association":"MEMBER",
  "body":"Done: https://pypi.org/project/uproot4/0.0.20/#files",
  "created_at":"2020-08-31T19:48:15Z",
  "id":684000288,
  "issue":32,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4NDAwMDI4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-31T19:48:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Currently,\r\n\r\n```python\r\nDIR = \"/home/pivarski/miniconda3/lib/python3.7/site-packages/skhep_testdata/data/\"\r\nimport uproot4, glob, pandas\r\ninterpretations = []\r\nfor filename in glob.glob(DIR + \"*.root\"):\r\n    with uproot4.open(filename) as file:\r\n        for path, classname in file.classnames().items():\r\n            if classname == \"TTree\":\r\n                for name, branch in file[path].items():\r\n                    interpretations.append({\r\n                        \"file\": filename[len(DIR):],\r\n                        \"tree\": path,\r\n                        \"branch\": name,\r\n                        \"interpretation\": repr(branch.interpretation),\r\n                        \"typename\": branch.typename,\r\n                    })\r\ndf = pandas.DataFrame(interpretations)\r\nprint(df.groupby(\"interpretation\").count().sort_values(\"branch\", ascending=False)[\"branch\"].to_string())\r\n```\r\n\r\n```\r\nAsJagged(AsDtype('>f4'), header_bytes=10)                                                              2347\r\nAsDtype('>f8')                                                                                         1781\r\nAsObjects(Model_BDSOutputROOTEventSampler_3c_float_3e_)                                                1242\r\nAsJagged(AsDtype('>i4'), header_bytes=10)                                                              1113\r\nAsDtype('>i4')                                                                                         1029\r\nAsDtype('bool')                                                                                        1011\r\nAsDtype('>f4')                                                                                          763\r\nAsJagged(AsDtype('>f4'))                                                                                762\r\nAsDtype('>u4')                                                                                          533\r\nAsJagged(AsDtype('>i4'))                                                                                529\r\nAsStrings()                                                                                             511\r\nAsJagged(AsDtype('>f8'))                                                                                464\r\nAsStrings(header_bytes=6)                                                                               429\r\nAsJagged(AsDtype('>u4'))                                                                                344\r\nNone                                                                                                    332\r\nAsDtype('>u2')                                                                                          236\r\nAsJagged(AsDtype('>f8'), header_bytes=10)                                                               234\r\nAsObjects(Unknown_BASE)                                                                                 230\r\n<UnknownInterpretation 'none of the rules matched'>                                                     171\r\nAsDtype('>u8')                                                                                          169\r\nAsJagged(AsDtype('bool'), header_bytes=10)                                                              116\r\nAsDtype('>i2')                                                                                           90\r\nAsDtype('>i8')                                                                                           86\r\nAsJagged(AsDtype('uint8'))                                                                               68\r\nAsObjects(AsVector(True, AsString(False)))                                                               65\r\nAsJagged(AsDtype('bool'))                                                                                58\r\nAsObjects(AsVector(True, AsVector(False, dtype('>i4'))))                                                 56\r\nAsJagged(AsDtype('>u8'))                                                                                 51\r\nAsJagged(AsDtype('uint8'), header_bytes=10)                                                              48\r\nAsObjects(Model_TXboxDAQChannel)                                                                         48\r\nAsDtype('int8')                                                                                          47\r\nAsJagged(AsDtype('int8'))                                                                                47\r\nAsDtype(\"('>f4', (3,))\")                                                                                 46\r\nAsObjects(AsVector(True, AsVector(False, dtype('>f8'))))                                                 46\r\nAsJagged(AsDtype('>i2'))                                                                                 45\r\nAsJagged(AsDtype('>u2'))                                                                                 43\r\nAsJagged(AsDtype('>i8'))                                                                                 42\r\nAsDtype('uint8')                                                                                         41\r\nAsDtype(\"('>f8', (3,))\")                                                                                 40\r\nAsDtype(\"('>i4', (3,))\")                                                                                 40\r\nAsDtype(\"('>i8', (3,))\")                                                                                 40\r\nAsDtype(\"('>i2', (3,))\")                                                                                 40\r\nAsDtype(\"('?', (3,))\")                                                                                   40\r\nAsDtype(\"('>u8', (3,))\")                                                                                 40\r\nAsDtype(\"('>u4', (3,))\")                                                                                 40\r\nAsDtype(\"('i1', (3,))\")                                                                                  40\r\nAsDtype(\"('>u2', (3,))\")                                                                                 40\r\nAsDtype(\"('u1', (3,))\")                                                                                  40\r\nAsJagged(AsStridedObjects(Model_TVector3_v3), header_bytes=10)                                           36\r\nAsJagged(AsStridedObjects(Model_TRotation_v1), header_bytes=10)                                          36\r\nAsObjects(Model_BDSOutputROOTEventLoss)                                                                  30\r\nAsObjects(AsVector(True, AsVector(False, Model_TVector3)))                                               22\r\nAsObjects(AsVector(True, Model_Trk))                                                                     22\r\nAsObjects(Model_TClonesArray)                                                                            18\r\nAsStridedObjects(Model_TVector3_v3)                                                                      18\r\nAsJagged(AsDouble32(0.0, 8.0, 4))                                                                        14\r\nAsJagged(AsDtype('>u4'), header_bytes=10)                                                                12\r\nAsObjects(Model_BDSOutputROOTEventLossWorld)                                                             12\r\nAsObjects(Model_BDSOutputROOTEventHistograms)                                                            12\r\nAsObjects(AsVector(True, Model_Hit))                                                                     12\r\nAsObjects(AsVector(True, AsPointer(Model_TH1D)))                                                         12\r\nAsObjects(AsVector(True, AsPointer(Unknown_TH2D)))                                                       12\r\nAsJagged(AsDouble32(0.0, 0.256, 8))                                                                      11\r\nAsObjects(AsFIXME('std::bitset<256>'))                                                                   11\r\nAsObjects(Model_Evt)                                                                                     11\r\nAsJagged(AsDtype('>f4', 'float64'))                                                                      10\r\nAsJagged(AsStridedObjects(Model_Hit_v104), header_bytes=10)                                              10\r\nAsObjects(AsVector(True, AsPointer(Unknown_TH3D)))                                                       10\r\nAsJagged(AsDouble32(-1.0, 1.0, 7))                                                                        8\r\nAsJagged(AsStridedObjects(Model_TLorentzVector_v4), header_bytes=10)                                      8\r\nAsObjects(Model_KM3NETDAQ_3a3a_JDAQTimesliceSN)                                                           7\r\nAsObjects(Model_KM3NETDAQ_3a3a_JDAQTimesliceL1)                                                           7\r\nAsObjects(AsVector(True, Model_KM3NETDAQ_3a3a_JDAQSuperFrame))                                            7\r\nAsObjects(AsVector(True, Unknown_KM3NETDAQ_3a3a_JDAQSummaryFrame))                                        7\r\nAsObjects(Model_KM3NETDAQ_3a3a_JDAQTimesliceL0)                                                           7\r\nAsObjects(Model_BDSOutputROOTEventCollimator)                                                             7\r\nAsObjects(Model_KM3NETDAQ_3a3a_JDAQTimeslice)                                                             7\r\nAsObjects(Model_KM3NETDAQ_3a3a_JDAQTimesliceL2)                                                           7\r\nAsObjects(Model_KM3NETDAQ_3a3a_JDAQSummaryslice)                                                          7\r\nAsJagged(AsDtype(\"('>f4', (2,))\"))                                                                        7\r\nAsObjects(Model_KM3NETDAQ_3a3a_JDAQEvent)                                                                 7\r\nAsJagged(AsDtype('>u2'), header_bytes=1)                                                                  7\r\nAsObjects(AsMap(True, dtype('>i4'), Unknown_BDSOutputROOTGeant4Data_3a3a_IonInfo))                        6\r\nAsObjects(Model_BDSOutputROOTEventRunInfo)                                                                6\r\nAsDouble32(-20.0, 20.0, 16)                                                                               6\r\nAsObjects(Model_BDSOutputROOTEventTrajectory)                                                             6\r\nAsObjects(Model_BDSOutputROOTGeant4Data)                                                                  6\r\nAsObjects(AsMap(True, dtype('>i4'), Model_BDSOutputROOTGeant4Data_3a3a_ParticleInfo))                     6\r\nAsJagged(AsDtype('>f8'), header_bytes=1)                                                                  6\r\nAsObjects(AsMap(True, AsString(True), dtype('>i4')))                                                      6\r\nAsJagged(AsDtype(\"('>f4', (4,))\"))                                                                        6\r\nAsJagged(AsDtype('>u2'), header_bytes=10)                                                                 6\r\nAsObjects(Model_BDSOutputROOTEventOptions)                                                                6\r\nAsObjects(Model_BDSOutputROOTEventAperture)                                                               6\r\nAsObjects(Model_BDSOutputROOTEventBeam)                                                                   6\r\nAsObjects(Model_BDSOutputROOTEventModel)                                                                  6\r\nAsObjects(Model_BDSOutputROOTEventCoords)                                                                 6\r\nAsObjects(Model_BDSOutputROOTEventHeader)                                                                 6\r\nAsDtype(\"('>f4', (10,))\")                                                                                 6\r\nAsObjects(AsMap(True, dtype('>i4'), dtype('>i4')))                                                        6\r\nAsObjects(Model_BDSOutputROOTEventInfo)                                                                   6\r\nAsJagged(AsDtype(\"('>f4', (3,))\"))                                                                        6\r\nAsObjects(AsVector(True, AsPointer(Model_allpix_3a3a_MCParticle)))                                        5\r\nAsObjects(AsVector(True, AsVector(False, dtype('bool'))))                                                 5\r\nAsObjects(AsVector(True, Unknown_BDSOutputROOTEventCollimatorInfo))                                       5\r\nAsDtype(\"('>f4', (2,))\")                                                                                  5\r\nAsDtype(\"('>i4', (2,))\")                                                                                  5\r\nAsObjects(AsVector(True, Unknown_JDAQSnapshotHit))                                                        5\r\nAsObjects(AsVector(True, Unknown_JDAQTriggeredHit))                                                       5\r\nAsJagged(AsDtype('>i2'), header_bytes=10)                                                                 5\r\nAsDtype(\"('>i8', (10,))\")                                                                                 4\r\nAsJagged(AsDtype('>u8'), header_bytes=10)                                                                 4\r\nAsDtype(\"('>f8', (10,))\")                                                                                 4\r\nAsJagged(AsDtype('>i8'), header_bytes=10)                                                                 4\r\nAsJagged(AsDouble32(-1.28, 1.28, 8))                                                                      4\r\nAsDtype(\"('>u8', (10,))\")                                                                                 4\r\nAsJagged(AsDouble32(0.0, 1.0, 8))                                                                         4\r\nAsJagged(AsDouble32(0.0, 10.24, 8))                                                                       4\r\nAsJagged(AsDouble32(0.0, 12.8, 8))                                                                        4\r\nAsJagged(AsDouble32(0.0, 2.56, 8))                                                                        4\r\nAsDtype(\"('>i4', (10,))\")                                                                                 4\r\nAsJagged(AsDouble32(0.9, 1.0, 16))                                                                        4\r\nAsObjects(Model_Event)                                                                                    4\r\nAsDtype(\"('>u4', (10,))\")                                                                                 4\r\nAsJagged(AsDtype('>i4'), header_bytes=1)                                                                  4\r\nAsStridedObjects(Model_nEXO_3a3a_SmartRef_v2)                                                             3\r\nAsObjects(AsMap(True, AsString(True), AsVector(True, dtype('>f8'))))                                      3\r\nAsStridedObjects(Model_TVector2_v3)                                                                       2\r\nAsDouble32(-1.0, 100.0, 16)                                                                               2\r\nAsObjects(AsVector(True, AsPointer(Model_nEXO_3a3a_SmartRef)))                                            2\r\nAsObjects(AsVector(True, AsPointer(Model_allpix_3a3a_MCTrack)))                                           2\r\nAsObjects(AsVector(True, Model_Lifetimes_3a3a_MiniV0))                                                    2\r\nAsObjects(AsVector(True, AsPointer(Model_TH3D)))                                                          2\r\nAsObjects(AsMap(True, AsString(True), dtype('>f8')))                                                      2\r\nAsObjects(AsVector(True, AsFIXME('std::bitset<9>')))                                                      2\r\nAsObjects(AsPointer(Unknown_AAny_3a3a_Holder_5f_base))                                                    2\r\nAsObjects(AsVector(True, Model_Lifetimes_3a3a_HyperTriton2Body))                                          2\r\nAsObjects(AsPointer(Model_TClonesArray))                                                                  2\r\nAsObjects(AsVector(True, Unknown_KM3NETDAQ_3a3a_JDAQKeyHit))                                              2\r\nAsStridedObjects(Model_TLorentzVector_v4)                                                                 2\r\nAsObjects(AsVector(True, Unknown_KM3NETDAQ_3a3a_JDAQTriggeredHit))                                        2\r\nAsJagged(AsDtype('>u4'), header_bytes=1)                                                                  2\r\nAsObjects(AsVector(True, AsVector(False, AsString(False))))                                               2\r\nAsJagged(AsDtype('>i8'), header_bytes=1)                                                                  2\r\nAsObjects(Model_Lifetimes_3a3a_MiniEvent)                                                                 2\r\nAsDtype(\"('>i4', (32,))\")                                                                                 2\r\nAsJagged(AsDtype(\"('>i4', (4,))\"))                                                                        2\r\nAsJagged(AsStridedObjects(Model_Lifetimes_3a3a_MCparticle_v1), header_bytes=10)                           2\r\nAsJagged(AsDtype('>f4'), header_bytes=1)                                                                  2\r\nAsDtype(\"('>i2', (10,))\")                                                                                 2\r\nAsJagged(AsDtype('>i2'), header_bytes=1)                                                                  2\r\nAsObjects(Model_MyEvent)                                                                                  2\r\nAsObjects(Model_nEXO_3a3a_SimEvent)                                                                       2\r\nAsDtype(\"('>f4', (13,))\")                                                                                 2\r\nAsDtype(\"('>u2', (10,))\")                                                                                 2\r\nAsObjects(Model_nEXO_3a3a_EvtNavigator)                                                                   2\r\nAsJagged(AsDtype('>u8'), header_bytes=1)                                                                  2\r\nAsObjects(Model_nEXO_3a3a_SimHeader)                                                                      2\r\nAsObjects(Model_SN_5f_Streaming_5f_All_5f_t)                                                              1\r\nAsObjects(Model_ND_3a3a_TND280Event)                                                                      1\r\nAsStrings(length_bytes='4')                                                                               1\r\nAsStridedObjects(Model_nEXO_3a3a_ElecHeader_v1)                                                           1\r\nAsObjects(Model_PWGJE_3a3a_EMCALJetTasks_3a3a_SubstructureTree_3a3a_JetSubstructureSplittings)            1\r\nAsObjects(Model_TH2F)                                                                                     1\r\nAsObjects(Model_SN_5f_SignalSimEvent_5f_t)                                                                1\r\nAsObjects(Model_SN_5f_OCandEvent_5f_t)                                                                    1\r\nAsObjects(AsVector(True, Model_BDSOutputROOTEventCollimatorInfo))                                         1\r\nAsObjects(Model_SN_5f_ORoutEvent_5f_t)                                                                    1\r\nAsDouble32(-2.71, 10.0, 10)                                                                               1\r\nAsObjects(Model_SN_5f_OMWatchEvent_5f_t)                                                                  1\r\nAsStridedObjects(Model_SN_5f_Streaming_5f_GPS_5f_t_v3)                                                    1\r\nAsStridedObjects(Model_mydata_v1)                                                                         1\r\nAsObjects(Model_TObjString)                                                                               1\r\nAsObjects(Model_JPetGeantEventPack)                                                                       1\r\nAsObjects(Model_baconhep_3a3a_TEventInfo)                                                                 1\r\nAsObjects(Model_DemoDouble32)                                                                             1\r\nAsObjects(AsVector(True, Model_nEXO_3a3a_ElecChannel))                                                    1\r\nAsDouble32(-2.71, 10.0, 3)                                                                                1\r\nAsObjects(Model_nEXO_3a3a_ElecEvent)                                                                      1\r\nAsObjects(Model_nEXO_3a3a_ElecSettings)                                                                   1\r\nAsObjects(Model_SN_5f_Streaming_5f_Range_5f_t)                                                            1\r\nAsObjects(AsVector(True, Unknown_ROOT_3a3a_Math_3a3a_LorentzVector_3c_ROOT_3a3a_Math_3a3a_PxPyPzE4D_3c_   1\r\nAsObjects(Model_AliReducedEventInfo)                                                                      1\r\nAsDouble32(-2.71, 10.0, 20)                                                                               1\r\nAsStridedObjects(Model_baconhep_3a3a_TGenEventInfo_v3)                                                    1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 28)                                                     1\r\nAsObjects(AsVector(True, AsSet(False, dtype('>i4'))))                                                     1\r\nAsDtype('>f4', 'float64')                                                                                 1\r\nAsDtype(\"('>i4', (4,))\")                                                                                  1\r\nAsDtype(\"('>i4', (8,))\")                                                                                  1\r\nAsDtype(\"('>u4', (6,))\")                                                                                  1\r\nAsDtype(\"('i1', (1024,))\")                                                                                1\r\nAsDtype(\"('u1', (9, 10))\")                                                                                1\r\nAsDtype(\"[('x', '>f8'), ('y', '>i4'), ('z', 'i1')]\")                                                      1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 26)                                                     1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 24)                                                     1\r\nAsFloat16(-2.71, 10.0, 3)                                                                                 1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 22)                                                     1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 20)                                                     1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 2)                                                      1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 18)                                                     1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 16)                                                     1\r\nAsFloat16(-2.71, 10.0, 10)                                                                                1\r\nAsFloat16(-2.71, 10.0, 10, to_dims=(3,))                                                                  1\r\nAsDtype(\"('>i4', (192,))\")                                                                                1\r\nAsDtype(\"('>i2', (2,))\")                                                                                  1\r\nAsDtype(\"('>f8', (9, 10, 2))\")                                                                            1\r\nAsDtype(\"('>f8', (5,))\")                                                                                  1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 32)                                                     1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 4)                                                      1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 6)                                                      1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 8)                                                      1\r\nAsDouble32(0.0, 0.0, 10)                                                                                  1\r\nAsDouble32(0.0, 0.0, 12)                                                                                  1\r\nAsDouble32(0.0, 0.0, 14)                                                                                  1\r\nAsDouble32(0.0, 0.0, 2)                                                                                   1\r\nAsDouble32(0.0, 0.0, 4)                                                                                   1\r\nAsDouble32(0.0, 0.0, 6)                                                                                   1\r\nAsDouble32(0.0, 0.0, 8)                                                                                   1\r\nAsDtype(\"('>f4', (26,))\")                                                                                 1\r\nAsDtype(\"('>f4', (6,))\")                                                                                  1\r\nAsDtype(\"('>f4', (64,))\")                                                                                 1\r\nAsDtype(\"('>f4', (7,))\")                                                                                  1\r\nAsFloat16(-2.71, 10.0, 16)                                                                                1\r\nAsFloat16(-2.71, 10.0, 5)                                                                                 1\r\nAsObjects(AsVector(True, AsSet(False, AsString(False))))                                                  1\r\nAsObjects(AsMap(True, dtype('>i4'), AsVector(True, AsVector(False, dtype('>i2')))))                       1\r\nAsObjects(AsMap(True, AsString(True), Model_TRotation))                                                   1\r\nAsObjects(AsMap(True, AsString(True), Model_TVector3))                                                    1\r\nAsObjects(AsMap(True, AsString(True), dtype('>i2')))                                                      1\r\nAsObjects(AsMap(True, dtype('>i4'), AsSet(True, AsString(False))))                                        1\r\nAsObjects(AsMap(True, dtype('>i4'), AsSet(True, dtype('>i2'))))                                           1\r\nAsObjects(AsMap(True, dtype('>i4'), AsVector(True, AsSet(False, dtype('>i2')))))                          1\r\nAsObjects(AsMap(True, dtype('>i4'), AsVector(True, AsString(False))))                                     1\r\nAsObjects(AsMap(True, dtype('>i4'), AsVector(True, dtype('>i2'))))                                        1\r\nAsJagged(AsDtype(\"('>f4', (21,))\"))                                                                       1\r\nAsObjects(AsMap(True, dtype('>i4'), dtype('>i2')))                                                        1\r\nAsObjects(AsPointer(Model_JPetGeantEventInformation))                                                     1\r\nAsObjects(AsPointer(Model_TH1D))                                                                          1\r\nAsObjects(AsSet(True, AsString(False)))                                                                   1\r\nAsObjects(AsSet(True, dtype('>i4')))                                                                      1\r\nAsObjects(AsVector(True, AsFIXME('std::bitset<10>')))                                                     1\r\nAsObjects(AsVector(True, AsPointer(Model_allpix_3a3a_PixelHit)))                                          1\r\nAsObjects(AsMap(True, AsString(True), AsVector(True, dtype('>i2'))))                                      1\r\nAsObjects(AsMap(True, AsString(True), AsVector(True, AsString(False))))                                   1\r\nAsObjects(AsMap(True, AsString(True), AsString(True)))                                                    1\r\nAsObjects(AsMap(True, AsString(True), AsString(False)))                                                   1\r\nAsJagged(AsDtype(\"('>f4', (6,))\"))                                                                        1\r\nAsJagged(AsDtype(\"('>u2', (2,))\"))                                                                        1\r\nAsJagged(AsDtype(\"('u1', (2,))\"))                                                                         1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 14)                                                     1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 12)                                                     1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 10)                                                     1\r\nAsDouble32(-2.71, 10.0, 5)                                                                                1\r\nAsDouble32(-2.71, 10.0, 32)                                                                               1\r\nAsDouble32(-2.71, 10.0, 30, to_dims=(3,))                                                                 1\r\nAsDouble32(-2.71, 10.0, 30)                                                                               1\r\nAsJagged(AsStridedObjects(Model_A_v1), header_bytes=10)                                                   1\r\nAsJagged(AsStridedObjects(Model_B_v1), header_bytes=10)                                                   1\r\nAsDouble32(-3.141592653589793, 3.141592653589793, 30)                                                     1\r\nAsObjects(AsMap(True, AsString(True), AsSet(True, AsString(False))))                                      1\r\nAsObjects(AsMap(True, AsString(True), AsSet(True, dtype('>i2'))))                                         1\r\n<UnknownInterpretation 'leaf-list with square brackets in the title'>                                     1\r\n```\r\n\r\n(107 chars wide)",
  "created_at":"2020-07-02T16:46:44Z",
  "id":653114941,
  "issue":35,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1MzExNDk0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-02T16:48:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is the [AsGrouped](https://github.com/scikit-hep/uproot4/blob/61d568b82c71936917fa2562c0a41ab23aaffeeb/uproot4/interpretation/grouped.py#L15-L33) interpretation. Its use may need to be expanded, to include the `std::map` example above, for instance, but that should be another issue.",
  "created_at":"2020-10-30T21:39:20Z",
  "id":719812095,
  "issue":37,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgxMjA5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T21:39:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Add to that one more: I think scikit-hep/uproot#510 is another example of that. Look at the second file, [uproot-issue510b.root](https://github.com/scikit-hep/scikit-hep-testdata/blob/master/src/skhep_testdata/data/uproot-issue510b.root):\r\n\r\n```python\r\n>>> import uproot4, skhep_testdata\r\n>>> t = uproot4.open(skhep_testdata.data_path(\"uproot-issue510.root\"))[\"EDepSimEvents\"]\r\n>>> b = t[\"Event\"][\"Trajectories.Points\"]\r\n>>> b.debug(0)\r\n```\r\n```\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64   0 101 102  64   9   0   1   0   0   0   2   0   1   0   0   0   0   2   0\r\n  @ ---   e   f   @ --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   1   0   0   0   0   2   0   0   0  64   0   0  60   0   4   0   1\r\n--- --- --- --- --- --- --- --- --- --- --- ---   @ --- ---   < --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   2   0   0   0  64   0   0  36   0   3   0   1   0   0   0   0\r\n--- --- --- --- --- --- --- ---   @ --- ---   $ --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  2   0   0   0  64 103 237  14  20  44 204 192  64  99 170 169 116  55  10  48\r\n--- --- --- ---   @   g --- --- ---   , --- ---   @   c --- ---   t   7 ---   0\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64 192  63 130 249   6 230 103  63 240   0   0   0   0   0   0  64   0   0  60\r\n  @ ---   ? --- --- --- ---   g   ? --- --- --- --- --- --- ---   @ --- ---   <\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   4   0   1   0   0   0   0   2   0   0   0  64   0   0  36   0   3   0   1\r\n--- --- --- --- --- --- --- --- --- --- --- ---   @ --- ---   $ --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   2   0   0   0  64 104 149  31 100 192  97 100  64  98 140 241\r\n--- --- --- --- --- --- --- ---   @   h --- ---   d ---   a   d   @   b --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n140  93 110   7  64 192  67  18 202 151 200 123  63 240 171 196  70 133 147  27\r\n---   ]   n ---   @ ---   C --- --- --- ---   {   ? --- --- ---   F --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64   0   0  36   0   3   0   1   0   0   0   0   2   0   0   0  64   4 167 135\r\n  @ --- ---   $ --- --- --- --- --- --- --- --- --- --- --- ---   @ --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n146 180 211 210 192  17 142 116  33 185 246 118  64  12   3 158  90 174 184  82\r\n--- --- --- --- --- --- ---   t   ! --- ---   v   @ --- --- ---   Z --- ---   R\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64   0   0  36   0   3   0   1   0   0   0   0   2   0   0   0   0   0   0   0\r\n  @ --- ---   $ --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0 128   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   2\r\n--- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+\r\n```\r\n\r\nThis is a _collection of_ `std::vector<TG4TrajectoryPoint>`, where `TG4TrajectoryPoint` is\r\n\r\n```python\r\n>>> tree.file.streamer_named(\"TG4TrajectoryPoint\").show()\r\nTG4TrajectoryPoint (v1): TObject (v1)\r\n    Position: TLorentzVector (TStreamerObject)\r\n    Momentum: TVector3 (TStreamerObject)\r\n    Process: int (TStreamerBasicType)\r\n    Subprocess: int (TStreamerBasicType)\r\n```\r\n\r\nThe first 6 bytes is header as usual: ` 64   0 101 102  64   9`. (That's the right `num_bytes` for the entry.)\r\n\r\nNext, we're looking at a _split_ `std::vector` header:\r\n\r\n```\r\n| 0   1 | 0   0   0   2 | 0   1 | 0   0   0   0 | 2   0   0   0 | 0   1 | 0   0   0   0 | 2   0   0   0 |\r\n|       |  two objects  |            bits for #1                |            bits for #2                |\r\n```\r\n\r\nThen follow two TLorentzVectors:\r\n\r\n```\r\n[191.4079686045643, 157.33318529844246, 8319.023224699498, 1.0]\r\n[196.6600822217398, 148.4044858765412, 8326.14680764474, 1.0419352297551032]\r\n```\r\n\r\nand two TVector3:\r\n\r\n```\r\n[2.5818015538629675, -4.389114882445588, 3.5017668804712594]\r\n[0.0, -0.0, 0.0]\r\n```\r\n\r\nand two integers, `0` and `2`.\r\n\r\nFollowing that is a header for 0 objects and then a header for 32 objects:\r\n\r\n```\r\n--+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0  14   0   0   0  32\r\n--- --- --- --- --- --- --- --- --- --- ---    \r\n--+---+---+---+---+---+---+---+---+---+---+---+-\r\n```\r\n\r\nand, indeed, there are 32 ten-byte `std::vector` headers:\r\n\r\n```\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   2   0   0   0   0   1   0   0   0   0   2   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n```\r\n\r\nRight after that, the TLorentzVectors start up again:\r\n\r\n```\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64   0   0  60   0   4   0   1   0   0   0   0   2   0   0   0  64   0   0  36\r\n  @ --- ---   < --- --- --- --- --- --- --- --- --- --- --- ---   @ --- ---   $\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   3   0   1   0   0   0   0   2   0   0   0  64 103 237  14  20  44 204 192\r\n--- --- --- --- --- --- --- --- --- --- --- ---   @   g --- --- ---   , --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64  99 170 169 116  55  10  48  64 192  63 130 249   6 230 103  63 240   0   0\r\n  @   c --- ---   t   7 ---   0   @ ---   ? --- --- --- ---   g   ? --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0\r\n--- --- --- ---\r\n--+---+---+---+\r\n```\r\n\r\nThis one is\r\n\r\n```\r\n[191.4079686045643, 157.33318529844246, 8319.023224699498, 1.0]\r\n```\r\n\r\nSimilarly, there's also a \"name\" field that claims to have type `std::string`:\r\n\r\n```python\r\n>>> t[\"Event\"][\"Trajectories.Name\"].streamer\r\n<TStreamerSTLstring at 0x7f33475eaf10>\r\n```\r\n\r\nbut it's clearly a _collection of_ strings (53 of them):\r\n\r\n```python\r\n>>> t[\"Event\"][\"Trajectories.Name\"].debug(0)\r\n```\r\n```\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64   0   0 214   0   9   5 103  97 109 109  97   3 109 117  45   6 112 114 111\r\n  @ --- --- --- --- --- ---   g   a   m   m   a ---   m   u   - ---   p   r   o\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n116 111 110   6 112 114 111 116 111 110   6 112 114 111 116 111 110   6 112 114\r\n  t   o   n ---   p   r   o   t   o   n ---   p   r   o   t   o   n ---   p   r\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n111 116 111 110   6 112 114 111 116 111 110   6 112 114 111 116 111 110   7 110\r\n  o   t   o   n ---   p   r   o   t   o   n ---   p   r   o   t   o   n ---   n\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n101 117 116 114 111 110   7 110 101 117 116 114 111 110   7 110 101 117 116 114\r\n  e   u   t   r   o   n ---   n   e   u   t   r   o   n ---   n   e   u   t   r\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n111 110   7 110 101 117 116 114 111 110   2 101  45   2 101  45   2 101  45   2\r\n  o   n ---   n   e   u   t   r   o   n ---   e   - ---   e   - ---   e   - ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n101  45   2 101  45   2 101  45   2 101  45   2 101  45   2 101  45   2 101  45\r\n  e   - ---   e   - ---   e   - ---   e   - ---   e   - ---   e   - ---   e   -\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  2 101  45   2 101  45   2 101  45   2 101  45   2 101  45   2 101  45   2 101\r\n---   e   - ---   e   - ---   e   - ---   e   - ---   e   - ---   e   - ---   e\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 45   2 101  45   2 101  45   2 101  45   2 101  45   2 101  45   2 101  45   2\r\n  - ---   e   - ---   e   - ---   e   - ---   e   - ---   e   - ---   e   - ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n101  45   2 101  45   2 101  45   2 101  45   2 101  45   2 101  45   2 101  45\r\n  e   - ---   e   - ---   e   - ---   e   - ---   e   - ---   e   - ---   e   -\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  2 101  45   2 101  45   2 101  45   2 101  45   2 101  43   7 110 101 117 116\r\n---   e   - ---   e   - ---   e   - ---   e   - ---   e   + ---   n   e   u   t\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n114 111 110   2 101  45   2 101  45   2 101  45   2 101  45   2 101  45\r\n  r   o   n ---   e   - ---   e   - ---   e   - ---   e   - ---   e   -\r\n```\r\n\r\nThe key: I think these are both TClonesArrays! They have a non-empty `fTClonesName` member.",
  "created_at":"2020-08-26T22:45:08Z",
  "id":681161557,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTE2MTU1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T15:12:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@tamasgal, I know that you're busy with Unroot.jl, but if you're ever interested in solving a mystery, we now have 6 issues that are due to cases in which ROOT writes the subentries `1`, `2`, `3` of structs with fields `a` `b` as\r\n\r\n```\r\na1 a2 a3 b1 b2 b3\r\n```\r\n\r\ninstead of\r\n\r\n```\r\na1 b1 a2 b2 a3 b3\r\n```\r\n\r\nIt's a lot like branch-splitting, but this happens _inside_ of each entry. I've been able to reverse engineer the fact that this is happening, but not _why_ it's happening: what information in the TBranch(Element), its parent branches, maybe the TTree itself, and associated streamers might tell us that we should deserialize it this way instead of the normal way. If you find anything that might shed some light on it (or even what this mode is named!), I'd be grateful.\r\n\r\nThe cases in this function:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/ccf790f9bb4c25363fdc9fabe455dd99c85d69d8/uproot4/interpretation/identify.py#L1180-L1186\r\n\r\nare guesses, based on a few examples, so don't copy them without care!\r\n\r\nAlso, if you know of anyone else who's inclined to dig into these details, let me know. I'm looking for help!",
  "created_at":"2020-08-26T23:30:07Z",
  "id":681175836,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTE3NTgzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T23:30:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Oh, that seems to be a tough one. I bookmarked it and try to see if I find something new! Currently I am more busy with my PhD than anything else though, but I am certainly interested :sweat_smile: \r\n\r\nI think I might have seen something similar in KM3NeT data, I have to dig in my notes, I hope I find it since for that I could also provide the source code...\r\n",
  "created_at":"2020-08-28T07:40:45Z",
  "id":682379283,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjM3OTI4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T07:40:45Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I just learned from Philippe that it's called \"memberwise streaming\" (as opposed to \"objectwise streaming\") and the ROOT code for deserializing these objects is here:\r\n\r\nhttps://github.com/root-project/root/blob/e87a6311278f859ca749b491af4e9a2caed39161/io/io/src/TStreamerInfoReadBuffer.cxx#L1220-L1277\r\n\r\nThe way to identify that a particular object is serialized this way is by checking for bit 14 (`2**14 == 16384`) in the instance version, [TBufferFile::kStreamedMemberWise](https://github.com/root-project/root/blob/8f19baf760fe3ff6e2a864bca0d96443127244eb/io/io/inc/TBufferFile.h#L69). Also, that bit has to be removed from the instance version before comparing it with the class version (second line of the quoted code above).\r\n\r\nIndeed, in the [uproot-issue510b.root](https://github.com/scikit-hep/scikit-hep-testdata/blob/master/src/skhep_testdata/data/uproot-issue510b.root) file I investigated above, the version number _does_ have bit 14 set:\r\n\r\n```python\r\n>>> import uproot4, skhep_testdata\r\n>>> t = uproot4.open(skhep_testdata.data_path(\"uproot-issue510b.root\"))[\"EDepSimEvents\"]\r\n>>> b = t[\"Event\"][\"Trajectories.Points\"]\r\n>>> b.debug(0, limit_bytes=6)\r\n```\r\n```\r\n--+---+---+---+---+---+\r\n 64   0 101 102  64   9\r\n  @ ---   e   f   @ ---\r\n--+---+---+---+---+---+\r\n```\r\n\r\nThe first four bytes is the size of this entry (with the `kByteCountMask` bit removed),\r\n\r\n```python\r\n>>> numpy.array([64, 0, 101, 102], \"u1\").view(\">u4\") & ~(2**30)\r\narray([25958])\r\n```\r\n\r\nand the next two bytes is the version number with a `kStreamedMemberWise` bit set,\r\n\r\n```python\r\n>>> numpy.array([64, 9], \"u1\").view(\">u2\") & ~(2**14)\r\narray([9], dtype=int32)\r\n```\r\n\r\nSo these things are identified one object at a time (even though a branch is likely to consist entirely of one type of serialization or the other).\r\n\r\nFor making tests, I think the way a class can be put into this mode is by calling `TClass::SetCanSplit(true)` on its TClass object (`TClass::GetClass(\"class name\")`). I'm not 100% certain whether this controls memberwise/objectwise splitting, ordinary branch splitting, or both. But it would be nice to see the same class written as memberwise and as objectwise, for confidence that we're doing it right.\r\n\r\n[root/test/bench.cxx](https://github.com/root-project/root/blob/8695045aeff4b2e606a5febdcd58a0a7e7f6c7af/test/bench.cxx) might make examples with and without memberwise splitting, but this is part of ROOT's benchmark tests and relies on other code that I haven't followed to its definitions. It might be possible to simply run this benchmark to generate files with memberwise and objectwise serialization.\r\n\r\nTVirtualStreamerInfo has a `SetStreamMemberWise(bool)` method, but I don't know if that means we can directly use it to make tests.\r\n\r\nI'm just writing these things here as notes, so that this information does not get lost.",
  "created_at":"2020-08-28T15:11:56Z",
  "id":682686940,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjY4Njk0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-31T17:56:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This should be considered a bug at least until we have a \"not implemented\" error message for this case, but fully implementing it is a feature. I think I'll put in one PR to add the \"not implemented\" message and then remove the \"bug\" label from this issue.",
  "created_at":"2020-08-28T15:15:45Z",
  "id":682692862,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjY5Mjg2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T15:15:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Oh wow, I didn't even have a chance \ud83d\ude05 \r\n\r\nNice to hear that the mystery is mostly solved.\r\n\r\nYou most certainly also found this thread (also from Philippe) https://root-forum.cern.ch/t/splitability-of-classes-with-custom-streamer/32974 for me especially this statement from Philippe was quite new:\r\n\r\n> If a class has a custom Streamer we have to assume that it is for a good reason :). When splitting is used, the custom Streamer is not used at all and thus we are (silently) not doing what the user (likely) intended often leading to corrupted results.\r\n\r\nBtw. I searched our codebase for `SetCanSplit` but have not found any use of that. I also have not found my notes which were about a strange split structure just like you described, only some sketches of the split-branch strategy which is well-known.",
  "created_at":"2020-08-30T17:45:26Z",
  "id":683449192,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzQ0OTE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-30T17:45:26Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I just asked him about it at the ROOT I/O meeting, which is every Friday:\r\n\r\nhttps://indico.cern.ch/category/526/",
  "created_at":"2020-08-30T17:49:48Z",
  "id":683449654,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzQ0OTY1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-30T17:49:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Oh nice, it seems to be open for externals, at least I was able to join the video room ;)",
  "created_at":"2020-08-30T17:52:13Z",
  "id":683449903,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzQ0OTkwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-30T17:52:13Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"This is helpful\u2014there's some inconsistency about how many entries a basket is believed to have (trying to get entry 12344 from a basket with 12344 entries... this might just be the first one that fails).\r\n\r\nCould you send the file to that I can run it and look for the reason why there's this disagreement about how many items the basket has? If it's large or private, you can send it to me by email: jpivarski at GMail. Thanks!",
  "created_at":"2020-07-04T11:40:57Z",
  "id":653755319,
  "issue":42,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1Mzc1NTMxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-04T11:40:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the file\u2014you helped me fix a super-dumb error. (See the correction: four if-cases need to assign `basket_array`, one didn't. This is a cogent argument for test-coverage: none of my sample files enter this case. Then again, it's hard to do test-coverage when your inputs are ROOT files; you have to _find_ files that exhibit each of the cases you want to test. Thanks for sending me yours!)\r\n\r\nYour original question was about performance. This branch takes 10 seconds to read on my computer. Since it's an inline object (vector of vectors), it's going through the slow Python code right now ([this](https://github.com/scikit-hep/uproot4/blob/b0dbc196cfe4e7536c490477712fca0ddd79718d/uproot4/containers.py#L471-L496)), though this will eventually be replaced by a compiled routine in Awkward 1. In the last plot of [this talk](https://figshare.com/articles/Vectorized_imperative_and_declarative_processing_of_Awkward_Arrays/11532795/1), your case is on the green line (original; no speedup) at the \"doubly jagged\" point, which is much lower than the \"singly jagged\" because of the difference in serialization format. The speedup shown in this plot from green line to orange line was made with experimental code that hasn't been integrated into the codebase, so it shows that we can expect a factor of 100 speedup when that's finished, bringing the \"doubly jagged\" case up to the same speed as \"singly jagged.\" This is consistent in your file, because your `std::vector<float>` branches take 0.05\u20120.08 seconds to load, which is that factor of 100 over 10 seconds.\r\n\r\nAs for when that will actually happen, it could be a matter of months, since getting Uproot 4 feature-complete is a higher priority. (Sorry!)",
  "created_at":"2020-07-05T14:57:29Z",
  "id":653899016,
  "issue":42,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1Mzg5OTAxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-05T14:57:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, there were two issues: the `file_path`, `object_path` was overzealous, interpreting port numbers as the start of the `object_path` because it's the rightmost colon (wouldn't happen if an actual `object_path` had been provided) and I was using `urlparse` wrong: I wanted the `hostname`, not `netloc` (see [pymotw](https://pymotw.com/2/urlparse/)).",
  "created_at":"2020-07-11T18:41:01Z",
  "id":657109656,
  "issue":47,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1NzEwOTY1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-11T18:41:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This doesn't even seem to be used as far as I can tell...",
  "created_at":"2020-07-16T23:37:23Z",
  "id":659735484,
  "issue":50,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY1OTczNTQ4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-16T23:37:23Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"The reason this was left half-implemented was to get input on how to make a sum-of-weights histogram in Boost. The code that's there goes into the ROOT minutiae of how to produce exactly the right errors for all the TProfile error modes; now I just need to know how to populate the boost-histogram `view`.\r\n\r\nThere are four error modes for TProfile, only one of which is the error on the mean. (The others are error estimates on the individual points that the bin was filled with, under different assumptions.) The user's choice of error mode is part of ROOT's metadata for the TProfile. By default, the `to_numpy` function returns `(values, errors)` for the error mode given in the metadata (and it's also a parameter of the `to_numpy` method).\r\n\r\nWhat should happen in `to_boost` and `to_hist`? Always pick the error on the mean? If so, perhaps it should be calculated using the `values_errors` method and then squared? ROOT's method of calculating errors, even the errors on the mean, is not simple.",
  "created_at":"2020-07-17T18:48:33Z",
  "id":660280685,
  "issue":50,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDI4MDY4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-17T18:48:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@jpivarski We have two \"profiles\", one computes the mean (storage.Mean), the other computes the weighted mean (storage.WeightedMean). The latter requires more memory per cell and is a bit slower, but handles the uncertainty of weighted samples properly. A conversion from TProfile may require storage.WeightedMean to be lossless, but it depends on the values that the user has fed into the TProfile.\r\n\r\nRegarding to your other question, Boost-Histogram always returns the value and the variance of that bin, so in case of a profile, it returns the variance of the samples in the bin, not the error on the mean.",
  "created_at":"2020-07-18T12:35:34Z",
  "id":660476834,
  "issue":50,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDQ3NjgzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-18T12:35:34Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"Our method of calculating errors is also not simple. The formulas are not simple, when you implement them in a numerically stable way.",
  "created_at":"2020-07-18T12:37:07Z",
  "id":660477022,
  "issue":50,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDQ3NzAyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-18T12:37:07Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"I meant we're going to have a hard time synchronizing them, such that we fill containers with values that mean the same thing, though they might be computed differently. I was hoping for a shortcut or something transparent.\r\n\r\nMaybe this will help, though: the ROOT documentation on TProfiles presents a high-level notational view: https://root.cern.ch/doc/master/classTProfile.html. The part that I've figured out is how to turn the data from C++ members into what they call `GetBinContent(j)` and `GetBinError(j)` on this page, for all of the four error options (`\"\"`, `\"s\"`, `\"i\"`, `\"g\"`). From what you wrote above, it sounds like `storage.WeightedMean` is `GetBinContent(j)` for the Boost mean and `GetBinError(j)**2` with option `\"s\"` for the Boost variances. I don't think ROOT's TProfile has an equivalent of an unweighted mean.\r\n\r\nI see no evidence for an adjustment for degrees of freedom in ROOT's derivation (what would typically be `N - 1`), so I think we're safe in squaring the \"spread\" to get true variances. If Boost histograms have a function for deriving display error bars from variances, that can be a good closure test that we've interpreted it right.\r\n\r\n(I've been calling it \"ROOT's calculation\" all over the place, but I distinctly remember it from the HBOOK documentation. There it is, on page 51 under `CALL HBPROF`: http://osksn2.hep.sci.osaka-u.ac.jp/~taku/doc/hbook.pdf)",
  "created_at":"2020-07-18T13:59:26Z",
  "id":660487112,
  "issue":50,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDQ4NzExMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-18T13:59:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The high-level notation seems useful. In Boost Histogram, I used the convention that variance really always means the variance in that bin. If you make a weighted histogram, it is at the variance of the weights. If you make mean a profile, it is the variance of the samples. I should add an explanation to the rationale https://www.boost.org/doc/libs/develop/libs/histogram/doc/html/histogram/rationale.html\r\n\r\nEdit: I actually have something there already. \"Some accumulators offer a value() method to return the cell value k and a variance() method, which returns an estimate v of the variance of that cell.\"",
  "created_at":"2020-07-19T09:21:51Z",
  "id":660615115,
  "issue":50,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDYxNTExNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-19T09:24:37Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"I think this has been implemented and it's just a matter of getting the Uproot \u2192 boost-histogram interface right, if it's not right now. Anyway, this is marked as a duplicate.",
  "created_at":"2020-10-30T21:40:47Z",
  "id":719812582,
  "issue":50,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgxMjU4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T21:40:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"All of these duplicates will all go into #167, where this interface will get figured out, once and for all.",
  "created_at":"2020-10-30T21:47:29Z",
  "id":719814978,
  "issue":50,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgxNDk3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T21:47:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, I'd consider this a success: the error message pointed directly at the issue (too often, the errors were downstream of the real error in Uproot3). It was the C++ type parser:\r\n\r\n```\r\nValueError: invalid C++ type name syntax at char 68\r\n\r\n    vector<ROOT::Math::PositionVector3D<ROOT::Math::Cartesian3D<double>,ROOT::Math::DefaultCoordinateSystemTag> >\r\n------------------------------------------------------------------------^\r\nin file uproot4-issue-51.root\r\n```\r\n\r\nand yeah, the parser wasn't handling commas (other than in `std::map`):\r\n\r\n```diff\r\n--- a/uproot4/interpretation/identify.py\r\n+++ b/uproot4/interpretation/identify.py\r\n@@ -629,6 +629,10 @@ def _parse_node(tokens, i, typename, file, quote, header, inner_header):\r\n             i, keys = _parse_node(\r\n                 tokens, i + 2, typename, file, quote, inner_header, inner_header\r\n             )\r\n+            while tokens[i].group(0) == \",\":\r\n+                i, keys = _parse_node(\r\n+                    tokens, i + 1, typename, file, quote, inner_header, inner_header\r\n+                )\r\n             _parse_expect(\">\", tokens, i, typename, file)\r\n             stop = tokens[i].span()[1]\r\n```\r\n\r\n(Why does Uproot4 have a type parser? Because the only reliable way to determine types in ROOT files is by parsing the type names. There are some integer tags in TStreamers, but they're not consistent: for instance, hadd can drop TStreamers. So if we're going to be interpreting types from strings, at least it should be a bone fide parser, with a tokenizer and parsing rules, like \"if you're in the template arguments (opened with a `<` token), fully parse an item, then expect a `>` token OR a `,` token and then fully parse another...\".)\r\n\r\nWe can now see the electrons:\r\n\r\n```python\r\n>>> t[\"Electrons\"].show()\r\nname                 | typename             | interpretation                    \r\n---------------------+----------------------+-----------------------------------\r\nElectrons            | std::vector<ROOT::Ma | AsObjects(AsVector(True, Unknown_R\r\nElectrons.fCoordinat | double[]             | AsJagged(AsDtype('>f8'))          \r\nElectrons.fCoordinat | double[]             | AsJagged(AsDtype('>f8'))          \r\nElectrons.fCoordinat | double[]             | AsJagged(AsDtype('>f8'))          \r\nElectrons.fCoordinat | double[]             | AsJagged(AsDtype('>f8'))          \r\n```\r\n\r\nand read their data through the subbranches:\r\n\r\n```python\r\n>>> t[\"Electrons.fCoordinates.fPt\"].array()\r\n<Array [[], [], [], [21.8, ... [], [], [56.5]] type='100 * var * float64'>\r\n>>> t[\"Electrons.fCoordinates.fEta\"].array()\r\n<Array [[], [], [], ... [], [], [-0.605]] type='100 * var * float64'>\r\n```\r\n\r\nbut I need to inhibit attempts to read from the `\"Electrons\"` branch, which doesn't really contain any data:\r\n\r\n```python\r\n>>> t[\"Electrons\"].array()\r\nuproot4.deserialization.DeserializationError: attempting to get bytes 0:6\r\noutside expected range 0:4 for this Chunk\r\nin file uproot4-issue-51.root\r\nin object /TreeMaker2/PreSelection\r\n```\r\n\r\nThe error message ought to be something like, \"This is a structure branch; its data is contained in its subbranches\" (listing them as a hint).\r\n",
  "created_at":"2020-07-17T14:29:30Z",
  "id":660138362,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDEzODM2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-17T14:29:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Cool, glad you understand the issue. Can't wait to give it a try once you have a version with the improved type parser. And yes, a note not to access a \"structure branch\" will keep people like me from bugging you with non-errors ;)\r\n\r\n@kpedro88, @nsmith-",
  "created_at":"2020-07-17T14:58:18Z",
  "id":660154143,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDE1NDE0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-17T14:58:18Z",
  "user":"MDQ6VXNlcjQ4MDgxMDY="
 },
 {
  "author_association":"MEMBER",
  "body":"On the roadmap is an idea for an attempted read of branches like `\"Electrons\"` as an [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html) of its four subbranches (issue #37).",
  "created_at":"2020-07-17T14:59:53Z",
  "id":660155082,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDE1NTA4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-17T14:59:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Oooo, I love that idea!\r\n+1",
  "created_at":"2020-07-17T15:03:45Z",
  "id":660157240,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDE1NzI0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-17T15:03:45Z",
  "user":"MDQ6VXNlcjQ4MDgxMDY="
 },
 {
  "author_association":"MEMBER",
  "body":"When this passes and gets merged, you should be able to read this TreeMaker file. I read in branches with `filter_name=\"Elect*\"` to see that it skipped the `AsGrouped` branch (just a stub/error message for now).",
  "created_at":"2020-07-17T16:30:16Z",
  "id":660206746,
  "issue":51,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MDIwNjc0Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-07-17T16:30:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Can you provide an example of the file, publicly (here) or privately (by email)?",
  "created_at":"2020-07-20T15:40:35Z",
  "id":661117991,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTExNzk5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T15:40:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I have uploaded the file I used to my cernbox\r\nhttps://cernbox.cern.ch/index.php/s/Upu7X3T8l7zdk8e",
  "created_at":"2020-07-20T15:47:29Z",
  "id":661122106,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTEyMjEwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T15:47:29Z",
  "user":"MDQ6VXNlcjI5NTQ1MDI0"
 },
 {
  "author_association":"MEMBER",
  "body":"Darn; I can't reproduce it. Can you tell me more about your environment? Like, operating system, what kind of file system the file is on, Python version, etc?",
  "created_at":"2020-07-20T15:53:50Z",
  "id":661125861,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTEyNTg2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T15:53:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hmm.. That is interesting. I use SL7, and the file is on the DUST system at DESY (I don't know if you are familiar with it but I am also not sure what kind of answer you want here tbh).\r\nThe python version is Python 3.8.3. I setup a conda environment and installed uproot4 using pip.\r\nEdit: Of course, if you need more info, please ask.",
  "created_at":"2020-07-20T15:59:25Z",
  "id":661129197,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTEyOTE5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T16:00:35Z",
  "user":"MDQ6VXNlcjI5NTQ1MDI0"
 },
 {
  "author_association":"MEMBER",
  "body":"I might have just found it. I switched from using `id(branch)` everywhere as a unique identifier (which should be okay as long as the branch that is being read doesn't get deleted while it's being read) to using `branch.cache_key` as a unique identifier (recently added, as part of caching). After doing so, I managed it get it to raise an error on the line you pointed out (i.e. something's wrong), though not with your file. Despite how transitory this seems, it's probably the same error.",
  "created_at":"2020-07-20T16:10:27Z",
  "id":661138661,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTEzODY2MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-07-20T16:10:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, no, it was just the way the test was written. (The test had been hard-coded to `id(branch)`; updating it to `branch.cache_key` fixed it.)\r\n\r\nI haven't been able to reproduce it. The following always works for me:\r\n\r\n```python\r\n>>> import uproot4\r\n>>> uproot4.open(\"uproot4-issue-54.root\")[\"Events\"][\"Muon_Pt\"].array()\r\n<Array [[], [], [], [], ... [113], [], []] type='561602 * var * float32'>\r\n```\r\n\r\nI'll look around for anything that might look like a race condition. Meanwhile, maybe you could try PR #55 to see if it fixes the problem for you? Your problem is a missing dict key; now the dict keys are strings associated with the file UUID, TTree path, TBranch name and location in the TTree. The `id(branch)` values are actually pointers, so this is probably safer, or at least might give better debugging information.",
  "created_at":"2020-07-20T16:19:42Z",
  "id":661148044,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTE0ODA0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T16:19:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I also see no problems when I use the `FileSource`, which would be the fallback if your filesystem doesn't support memory-mapping.\r\n\r\n```python\r\nuproot4.open(\"uproot4-issue-54.root\", file_handler=uproot4.source.file.FileSource)\r\n```",
  "created_at":"2020-07-20T16:22:50Z",
  "id":661151039,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTE1MTAzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T16:22:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"It works now. Thanks for the very quick help.",
  "created_at":"2020-07-20T16:30:46Z",
  "id":661159052,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTE1OTA1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T16:30:46Z",
  "user":"MDQ6VXNlcjI5NTQ1MDI0"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, then I'll keep it as `branch.cache_key` and always be suspicious of using `id(...)` for identifiers in the future.\r\n\r\nI don't know what _exactly_ went wrong, but perhaps the `branch` went out of scope and came back with a different `id`. At least the `cache_key` will always be the same (even between processes, not that it matters for a short-lived dict).\r\n\r\nAlso, there was no chance of a race condition: the dict is filled with a single item long before the machinery to start filling arrays is even invoked.",
  "created_at":"2020-07-20T16:36:21Z",
  "id":661164673,
  "issue":54,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MTE2NDY3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-20T16:36:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Do they also have a `__setstate__`/`__getstate__`? I would expect to be able to pickle a Histogram from ROOT, too.",
  "created_at":"2020-07-23T13:19:19Z",
  "id":663002251,
  "issue":56,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzAwMjI1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T13:19:19Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm planning on it, though that would definitely break the connection to the original file.",
  "created_at":"2020-07-23T14:55:01Z",
  "id":663054284,
  "issue":56,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzA1NDI4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T14:55:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Fixed in scikit-hep/uproot4#58; with a short list of exceptions (things like TDirectory, TTree, TBranch...), objects from a ROOT file are now detached from the original file. Thus, it wouldn't be possible to use these objects to read more data (which is why TDirectory, TTree, etc. are exceptions). But this means that the detached objects can be pickled and don't contain any transients, like locks or threads.\r\n\r\nYou can even save an object from a ROOT file into a pickle file and read it back in a new Python process, even though that object's class was derived from data in the ROOT file. (We pickle enough derived quantities from the TStreamerInfo to reconstitute the class object.)",
  "created_at":"2020-07-23T21:12:33Z",
  "id":663235055,
  "issue":56,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzIzNTA1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T21:12:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is done; somehow forgot to close it.",
  "created_at":"2020-08-26T19:48:46Z",
  "id":681088106,
  "issue":56,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTA4ODEwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T19:48:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It turns out this is caused by querying a redirector instead of the data server itself. I've found a similar setup within LHCb which I can use for testing a solution.",
  "created_at":"2020-07-24T08:51:09Z",
  "id":663421651,
  "issue":57,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzQyMTY1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-24T08:51:09Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"To clarify that last claim, it can be written and read back because of [PEP 562](https://www.python.org/dev/peps/pep-0562/), which means Python 3.7+ only.\r\n\r\nSince it can't be demonstrated in a unit test, I'll write it here. The writing process does\r\n\r\n```python\r\nimport uproot4, skhep_testdata, pickle\r\nf = uproot4.open(skhep_testdata.data_path(\"uproot-hepdata-example.root\"))\r\noriginal = f[\"hpx\"]\r\npickle.dump(original, open(\"tmp.pkl\", \"wb\"))\r\n```\r\n\r\nand the reading process does\r\n\r\n```python\r\nimport pickle\r\nreconstituted = pickle.load(open(\"tmp.pkl\", \"rb\"))\r\nreconstituted.to_numpy()\r\n...\r\n```\r\n\r\nThe class of `original` is dynamically generated from the TStreamerInfo in the ROOT file. Pickle doesn't like classes that don't exist in a module, but we put it in `uproot4.dynamic` for pickle to find when writing.\r\n\r\nWhen reading, we no longer have the dynamically generated class, but when pickle looks for it in `uproot4.dynamic`, it finds a (new, empty) class object, which it uses to fill the instance. Models have a custom `__getstate__`/`__setstate__` that updates the empty shell `uproot4.dynamic` produced with the `member_names`, `class_code`, `class_streamer`, etc. that it needs, as well as all of the class's behaviors.\r\n\r\nSo the `reconstituted` is a _work-alike_ of the original object. It lacks the methods needed for it to be read from a file (though they're in the `class_code`, we could re-evaluate that), but the object does not need to be read from a file again.",
  "created_at":"2020-07-23T21:10:40Z",
  "id":663234287,
  "issue":58,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzIzNDI4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T21:10:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`__slots__` interfere with pickle?\r\n\r\nAlso, why can't that be a unit test?",
  "created_at":"2020-07-23T21:28:45Z",
  "id":663241569,
  "issue":58,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI0MTU2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T21:28:45Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> Also, why can't that be a unit test?\r\n\r\nWhich can't be a unit test?",
  "created_at":"2020-07-23T21:29:52Z",
  "id":663242025,
  "issue":58,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI0MjAyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T21:29:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I just saw:\r\n> Since it can't be demonstrated in a unit test",
  "created_at":"2020-07-23T21:31:03Z",
  "id":663242518,
  "issue":58,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI0MjUxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T21:31:03Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Writing to a pickle file in one process and reading in another can't be demonstrated in a unit test because unit tests, at least the way pytest does it, are all in a single process. I could save the written file into the GitHub repo and read that as a test, but that wouldn't be a fair test unless I found a ROOT class that no other tests read in. For it to be a fair test, we have to have never seen this class before (and I'm using TH1F for it, which isn't uncommon).",
  "created_at":"2020-07-23T21:36:10Z",
  "id":663244574,
  "issue":58,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI0NDU3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-23T21:36:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> It's my understanding that the first XRootD version that includes your PR should not register the atexit handler.\r\n\r\nYes though it doesn't matter if both are registered, it will only have the effect of making the Python process exit slightly more slowly (but not noticeably so). If the fix is backported it might have to be a split check like: `if (version < 5.0.0 and version > 4.12.4) or (version > 5.0.2)`. I'll take care of adding this once we know what versions should be there.\r\n\r\n> Ordinarily, I squash and merge PRs (because I think of a PR as a single unit to add to a repo), and even though these commits are nicely curated, I'd like to do the same here\r\n\r\nAbsolutely! The commits only serve to make this page easier to understand, if anyone is ever tracing a bug back through the git history they should come here to read the comments and associated issues.",
  "created_at":"2020-07-24T12:46:48Z",
  "id":663521079,
  "issue":59,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzUyMTA3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-24T12:46:48Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This is great! Not only does it fix both of my issues - also by looking at these commits i learned that i can query the server i'm talking to which is very useful for testing. Thanks @chrisburr!",
  "created_at":"2020-07-24T14:44:10Z",
  "id":663574098,
  "issue":59,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2MzU3NDA5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-24T14:44:10Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"> ```\r\n> /usr/local/lib/python3.8/site-packages/uproot4/behaviors/TTree.py in aliases(self)\r\n>      21     @property\r\n>      22     def aliases(self):\r\n> ---> 23         aliases = self.member(\"fAliases\")\r\n>      24         if aliases is None:\r\n>      25             return {}\r\n> ```\r\n\r\nAt that point in the code, I must have thought that `self.member(..., none_if_missing=True)` was the default.\r\n\r\n> `KeyInFileError: not found: 'fAliases' because .Model_TTree_v5 has only`\r\n\r\nThe earliest version of ROOT that I use for testing is 5.24/00, which is about 5 years old. The TTree version in that ROOT version is 16. Your TTree is version 5! That's why it doesn't have some members, like `fAliases`, but that's also a good test. (Did it come from GEANT? GEANT has an independent ROOT-writer, which generates some very old versions of common ROOT objects.)\r\n\r\nSince this ought to be a simple fix, I committed it directly as d67d431. If your problem isn't fixed, either reopen issue (if possible) or comment here (I'll see it, either way).",
  "created_at":"2020-07-28T13:54:53Z",
  "id":665054620,
  "issue":61,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NTA1NDYyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-28T13:54:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Your TTree is version 5! That's why it doesn't have some members, like `fAliases`, but that's also a good test. \r\n\r\nI did notice the `v5` and the `dict` that starts in 16 but didn't think too much of it.\r\n\r\n>(Did it come from GEANT? GEANT has an independent ROOT-writer, which generates some very old versions of common ROOT objects.)\r\n\r\nYes, it's coming from GEANT; \ud83d\udcaf points for that alone.\r\n\r\n> Since this ought to be a simple fix, I committed it directly as [d67d431](https://github.com/scikit-hep/uproot4/commit/d67d431483e7b2b38d01305311438b2a5685566d). If your problem isn't fixed, either reopen issue (if possible) or comment here (I'll see it, either way).\r\n\r\nTested to be working. \ud83e\udd73\r\n\r\nThanks!\r\n\r\n",
  "created_at":"2020-07-28T14:18:52Z",
  "id":665068527,
  "issue":61,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2NTA2ODUyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-07-28T14:18:52Z",
  "user":"MDQ6VXNlcjQyNTY0Njk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"No more changes coming here for the moment. (Further significant improvements would require Numba I think.)",
  "created_at":"2020-08-03T18:32:06Z",
  "id":668176432,
  "issue":62,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY2ODE3NjQzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-03T18:32:06Z",
  "user":"MDQ6VXNlcjQ5MjA3OTg="
 },
 {
  "author_association":"MEMBER",
  "body":"Depending on what it is, you might not want to skip over it, regardless of the fact that it doesn't provide a `num_bytes` to be able to do that. There is some type that doesn't have a TStreamer and Uproot failed to even give up on it, labeling it \"Unknown,\" because it didn't know how big the instance is and where it can start reading next.\r\n\r\nNote to self: this error message should at least print out the name of the class\u2014that would be better diagnostic information.\r\n\r\nAbout your file, you could send it to my GMail address, which is jpivarski.",
  "created_at":"2020-08-10T12:38:19Z",
  "id":671329256,
  "issue":64,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTMyOTI1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T12:38:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for your quick response!\r\n\r\nOkay, I see, thanks - something must have been corrupted in the copying the tree from the other file (others succeeded, hence my surprise with this noe). For completeness I'll send you the file in case it helps with creating the error message, but since I can just regenerate it, I'll just do that",
  "created_at":"2020-08-10T12:53:14Z",
  "id":671335539,
  "issue":64,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTMzNTUzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T12:53:14Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"My conclusion above stands, but one other note: curiously, uproot3 can read it successfully (at least the number of entries, which is all I was trying to do at the moment)",
  "created_at":"2020-08-10T13:01:45Z",
  "id":671339644,
  "issue":64,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTMzOTY0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T13:01:45Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"I wouldn't call this \"corrupted,\" but ROOT used a different option for writing it (and perhaps we don't know why it chose that option). It comes in trying to skip over embedded baskets (what I call TBaskets that are serialized as part of a TBranch, rather than being located elsewhere in the file with a TKey), which for this one file lacked the number of bytes (didn't have the `kByteCountMask` bit set). In PR #65, I've added a fallback to actually reading those embedded baskets in this case\u2014at worst, it's extra reading, but in this case it's an empty TObjArray.\r\n\r\nJust from reading the code, I don't see why Uproot 3 succeeded on this file. (But I haven't gone to the trouble of running a test and figuring out why. There must be a reason; I just don't see it at a glance.)\r\n\r\nIf you need a workaround faster than I can get this PR merged (Windows is failing to get conda, and therefore the tests are failing), the `uproot4.open` parameter `minimal_ttree_metadata=False` switches off this attempt to skip embedded baskets and makes your file readable without any code updates.",
  "created_at":"2020-08-10T13:48:25Z",
  "id":671364943,
  "issue":64,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTM2NDk0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T13:48:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah, okay, now I think I understand - I read a bit too quickly before :-) Thanks for the explanation, the workaround, and the fix!\r\n\r\n",
  "created_at":"2020-08-10T15:11:39Z",
  "id":671414538,
  "issue":64,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTQxNDUzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T15:11:39Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"The problem seems to be \"archspec\" not being installable on windows, not that there's a problem with the GH Action.  This looks like a conda-forge issue possibly. Try using the action, but removing the conda-forge channel (you can add it manually to the install command)",
  "created_at":"2020-08-10T14:19:17Z",
  "id":671383843,
  "issue":65,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTM4Mzg0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T14:19:17Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> The problem seems to be \"archspec\" not being installable on windows, not that there's a problem with the GH Action. This looks like a conda-forge issue possibly. Try using the action, but removing the conda-forge channel (you can add it manually to the install command)\r\n\r\nThanks\u2014that was a big help. I just wanted to spot-fix one little issue and then a workflow thing that used to work took up an hour. (I think this is the gritting teeth one: `:E` Or else maybe three-toothed vampire?)\r\n\r\nAnyway, your help was invaluable! I was thinking that the \"goanpeca/setup-miniconda\" marketplace item had just gotten out of date or something and wanted it to work any way at all, even by manual installation (which is what I'd do to a VM).",
  "created_at":"2020-08-10T14:35:07Z",
  "id":671393469,
  "issue":65,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTM5MzQ2OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2020-08-10T14:35:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I am adding some tests and found `uproot-hepdata-example.root` has several baskets and the only common offset is the whole file. Now I am looking for another file with well-aligned baskets, any hints?",
  "created_at":"2020-08-10T18:15:01Z",
  "id":671509162,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTUwOTE2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T18:15:01Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"foriter.root has some small, oddly shaped baskets. The purpose was to check the correctness of stitching together partial baskets when an iteration size doesn't match it.",
  "created_at":"2020-08-10T18:28:40Z",
  "id":671516012,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTUxNjAxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T18:28:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The physics samples like HZZ.root and Zmumu.root might have the typical AutoFlush pattern (small baskets followed by big baskets as the algorithm learns how much memory it has).",
  "created_at":"2020-08-10T18:29:36Z",
  "id":671516485,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTUxNjQ4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T18:29:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This is ready",
  "created_at":"2020-08-11T13:49:01Z",
  "id":671958192,
  "issue":67,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTk1ODE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T13:49:01Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Do you have any other plans for this or should I merge it when the tests pass?",
  "created_at":"2020-08-10T18:25:25Z",
  "id":671514432,
  "issue":68,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTUxNDQzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T18:25:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"\nJim Pivarski writes:\n\n> Do you have any other plans for this or should I merge it when the tests pass?\n\nNothing additional from my end. Thanks!\n",
  "created_at":"2020-08-10T18:27:07Z",
  "id":671515252,
  "issue":68,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MTUxNTI1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-10T18:27:07Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the link\u2014this always confuses me.",
  "created_at":"2020-08-11T16:31:53Z",
  "id":672073206,
  "issue":69,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MjA3MzIwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T16:31:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Here, I must have been assuming (hoping?) that the RecordForm constructor would be like the RecordArray constructor, making it independent of dict order (Python 2.7 and 3.5-).\r\n\r\nBut I'll merge it if you're ready.",
  "created_at":"2020-08-11T19:55:16Z",
  "id":672244839,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MjI0NDgzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T19:55:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"ah I see why, ok maybe better to add that kind of form constructor instead?",
  "created_at":"2020-08-11T19:57:03Z",
  "id":672246262,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MjI0NjI2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T19:57:03Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"If you're willing. I should let you know that it's pybind11 work, not Python, so it will involve some compilation time. I can point out the line where it goes.",
  "created_at":"2020-08-11T20:00:18Z",
  "id":672248004,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MjI0ODAwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T20:00:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is RecordForm's only constructor so far: it takes a `std::map<std::string, ak::FormPtr>` as its only required argument.\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/f491da0adcd04fb6b07eb5616aa146c2ef737e32/src/python/forms.cpp#L555-L575\r\n\r\nThe `std::map` is presented in Python as dict, which loses order in Pythons 2.7 and 3.5. If there were a second constructor that took two lists (`std::vector<std::string>` and `std::vector<ak::FormPtr>`), then the keys and values could be passed to it in a way that preserves order.\r\n\r\nThe C++ RecordForm itself takes the list of `ak::FormPtr` (`contents`) separately from the list of keys (`util::RecordLookupPtr recordlookup`):\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/f491da0adcd04fb6b07eb5616aa146c2ef737e32/include/awkward/array/RecordArray.h#L22-L26\r\n\r\nso there wouldn't be anything to do after the pybind11 interface (i.e. not an exploding-in-size project).",
  "created_at":"2020-08-11T20:04:42Z",
  "id":672250454,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MjI1MDQ1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T20:04:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The RecordForm constructor described above has been added and is being published in 0.2.30. If that works, this PR may be unnecessary and should be closed.",
  "created_at":"2020-08-11T23:57:22Z",
  "id":672373366,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MjM3MzM2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-11T23:57:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to close this because I think it should work now. If it doesn't, then you can open it up again.",
  "created_at":"2020-08-12T00:55:30Z",
  "id":672411589,
  "issue":71,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3MjQxMTU4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-12T00:55:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> To obtain the same values in `uproot4` (0.0.16), I need to use `[:,0]`\r\n\r\nThe default `compute` environment for `arrays` is `uproot4.compute.python.ComputePython`, which treats the strings as Python code. This leaves room for a future \"`ComputeTTFormula`\", probably based on [formulate](https://github.com/scikit-hep/formulate), as well as the other analysis languages that are being developed. So this is intentional: the project of making `[0]` mean \"the first item of every list\" instead of \"the first list\" is an unstarted project. When such a thing exists, it would be a plug-in; perhaps the interface would be `compute=\"root\"` instead of `compute=\"python\"` (giving the compute environments string names, similar to the `library` argument).\r\n\r\n> I noticed that the above does not work with `library=np`\r\n\r\nWith `library=\"np\"`, you have even less functionality (and less performance) because NumPy does not have jagged arrays that can be indexed in such a way as to select the first item of each list without explicit loops. The `library=\"np\"` option means, \"I don't want to bring in any Awkward dependencies; give me a NumPy array, whatever it takes,\" and for a jagged array, that means\r\n\r\n```python\r\nnp.array([np.array([1, 2, 3]), np.array([]), np.array([4, 5])], dtype=\"O\")\r\n```\r\n\r\nWhen you slice this as `[:, 0]`, NumPy says, \"IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\" because NumPy doesn't see the nested arrays as being an irregular dimension, but as random Python objects that it doesn't know anything about. That's why Awkward Array exists.",
  "created_at":"2020-08-12T13:24:58Z",
  "id":672868609,
  "issue":72,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3Mjg2ODYwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-12T13:24:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the very clear explanation! Feel free to close, this provided all the information I was looking for.",
  "created_at":"2020-08-12T13:30:54Z",
  "id":672872030,
  "issue":72,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3Mjg3MjAzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-12T13:30:54Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay!",
  "created_at":"2020-08-12T13:33:58Z",
  "id":672873745,
  "issue":72,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3Mjg3Mzc0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-12T13:33:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks, I'll take a look! The UnROOT.jl issue you pointed to has examples of how to make the file.\r\n\r\nIt might be more instructive to look at files with more than one entry. Is also a good debugging technique to make the two factors we have to disambiguate relatively prime, or make every size a prime number (e.g. 2, 3, and 5)\u2014it helps to narrow down how what it's doing is different from what you want. Since we can create the files ourselves in this case, that's a possibility.",
  "created_at":"2020-08-22T15:11:53Z",
  "id":678652416,
  "issue":74,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODY1MjQxNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-22T15:11:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Evidently, it's expanding one of the string fields as a list, putting each letter in a separate row. I'll need to look into that, and the conversion to Pandas in general needs more careful thought. Meanwhile, you can also try reading the data as an Awkward Array and using the [ak.to_pandas](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_pandas.html) function on it.",
  "created_at":"2020-08-22T22:49:55Z",
  "id":678704933,
  "issue":75,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY3ODcwNDkzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-22T22:49:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"To start, I reproduced the error by creating a similar file:\r\n\r\n```c++\r\nroot [0] auto file = new TFile(\"issue-75.root\", \"RECREATE\")\r\n(TFile *) @0x7ffe052ea428\r\nroot [1] auto tree = new TTree(\"stuff\", \"\")\r\n(TTree *) @0x7ffe052ea428\r\nroot [2] struct stuff {double one; double two;}\r\nroot [3] stuff stuffy\r\n(stuff &) @0x7f731d8d9018\r\nroot [4] int64_t stuffo\r\n(long) 0\r\nroot [5] tree->Branch(\"stuffy\", &stuffy, \"one/D:two/D\")\r\n(TBranch *) 0x56180c6f9bc0\r\nroot [6] tree->Branch(\"stuffo\", &stuffo, \"stuffo/L\")\r\n(TBranch *) 0x56180c6f9400\r\nroot [7] stuffy.one = 1\r\n(double) 1.0000000\r\nroot [8] stuffy.two = 2\r\n(double) 2.0000000\r\nroot [9] stuffo = 3\r\n(long) 3\r\nroot [10] tree->Fill()\r\n(int) 24\r\nroot [11] stuffy.one = 1.1\r\n(double) 1.1000000\r\nroot [12] stuffy.two = 2.2\r\n(double) 2.2000000\r\nroot [13] stuffo = 33\r\n(long) 33\r\nroot [14] tree->Fill()\r\n(int) 24\r\nroot [15] stuffy.one = 1.2\r\n(double) 1.2000000\r\nroot [16] stuffy.two = 2.3\r\n(double) 2.3000000\r\nroot [17] stuffo = 333\r\n(long) 333\r\nroot [18] tree->Fill()\r\n(int) 24\r\nroot [19] tree->Write()\r\n(int) 484\r\nroot [20] file->Close()\r\nroot [21] .q\r\n```\r\n\r\nAnd it looks like yours:\r\n\r\n```python\r\n>>> import uproot4\r\n>>> f = uproot4.open(\"issue-75.root\")\r\n>>> f[\"stuff\"].show()\r\nname                 | typename             | interpretation                    \r\n---------------------+----------------------+-----------------------------------\r\nstuffy               | struct {double one;  | AsDtype(\"[('one', '>f8'), ('two', \r\nstuffo               | int64_t              | AsDtype('>i8')                    \r\n>>> f[\"stuff\"].arrays()\r\n<Array [{stuffy: {one: 1, ... stuffo: 333}] type='3 * {\"stuffy\": {\"one\": float64...'>\r\n>>> f[\"stuff\"].arrays().tolist()\r\n[{'stuffy': {'one': 1.0, 'two': 2.0}, 'stuffo': 3}, {'stuffy': {'one': 1.1, 'two': 2.2}, 'stuffo': 33}, {'stuffy': {'one': 1.2, 'two': 2.3}, 'stuffo': 333}]\r\n```\r\n\r\nand exhibits the error you're seeing:\r\n\r\n```python\r\n>>> f[\"stuff\"].arrays(library=\"pd\")\r\nEmpty DataFrame\r\nColumns: [(stuffy, one, nan, nan, nan, nan), (stuffy, two, nan, nan, nan, nan), (s, t, u, f, f, o)]\r\nIndex: []\r\n```\r\n\r\nNow I can dig into it.",
  "created_at":"2020-08-25T16:56:22Z",
  "id":680147873,
  "issue":75,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDE0Nzg3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T16:56:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Pulling out individual branches is fine:\r\n\r\n```bash\r\n% python -c 'import uproot4; f = uproot4.open(\"issue-75.root\"); print(f[\"stuff/stuffy\"].array(library=\"pd\"))'\r\n   one  two\r\n0  1.0  2.0\r\n1  1.1  2.2\r\n2  1.2  2.3\r\n% python -c 'import uproot4; f = uproot4.open(\"issue-75.root\"); print(f[\"stuff/stuffo\"].array(library=\"pd\"))'\r\n0      3\r\n1     33\r\n2    333\r\ndtype: int64\r\n```\r\n\r\nIt's just the combination that goes haywire. That's in [Pandas.group](https://github.com/scikit-hep/uproot4/blob/3827f967da9c1031c8b5de14a877184d1089470d/uproot4/interpretation/library.py#L636), rather than [Pandas.finalize](https://github.com/scikit-hep/uproot4/blob/3827f967da9c1031c8b5de14a877184d1089470d/uproot4/interpretation/library.py#L509).",
  "created_at":"2020-08-25T17:00:44Z",
  "id":680150204,
  "issue":75,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDE1MDIwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T17:00:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The problem was that some columns have multipart tuple structure (e.g. `(\"stuffy\", \"one\")`) and others didn't (e.g. `\"stuffo\"`). Pandas's `MultiIndex.from_tuples` iterated through the letters of the string, assuming it to be a tuple (i.e. Pandas didn't check). So I added a step that explicitly puts all column names into tuples of the same length if any of them are tuples.\r\n\r\n```python\r\n% python -c 'import uproot4; f = uproot4.open(\"issue-75.root\"); print(f[\"stuff\"].arrays(library=\"pd\"))'\r\n  stuffy      stuffo\r\n     one  two    NaN\r\n0    1.0  2.0      3\r\n1    1.1  2.2     33\r\n2    1.2  2.3    333\r\n```",
  "created_at":"2020-08-25T17:56:09Z",
  "id":680179890,
  "issue":75,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDE3OTg5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T17:56:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"And now I just realized that I committed that fix directly to master. I thought I was in a branch/PR! (Trying to work on too many things at the same time...)",
  "created_at":"2020-08-25T17:57:36Z",
  "id":680180640,
  "issue":75,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDE4MDY0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T17:57:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"EDIT: never mind, I delete this comment since I messed up my virtual environment... back on track: the behaviour above is with the recent versions of `uproot4` (0.0.19) and `awkward1` (0.2.33), just for future reference.",
  "created_at":"2020-08-25T15:24:17Z",
  "id":680094277,
  "issue":76,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDA5NDI3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T15:40:53Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"The way this works is not fundamentally different from before: the interpretation is still the first argument of the `array` method. I don't know why it should be saying that `AsJagged` is not an `Interpretation`: it definitely is!\r\n\r\n```python\r\n>>> interp\r\nAsJagged(AsDtype(\"[('dom_id', '<i4'), ('dq_status', '<u4'), ('hrv', '<u4'), ('fifo', '<u4'), ('status3', '<u4'), ('status4', '<u4'), ('ch0', 'u1'), ('ch1', 'u1'), ('ch2', 'u1'), ('ch3', 'u1'), ('ch4', 'u1'), ('ch5', 'u1'), ('ch6', 'u1'), ('ch7', 'u1'), ('ch8', 'u1'), ('ch9', 'u1'), ('ch10', 'u1'), ('ch11', 'u1'), ('ch12', 'u1'), ('ch13', 'u1'), ('ch14', 'u1'), ('ch15', 'u1'), ('ch16', 'u1'), ('ch17', 'u1'), ('ch18', 'u1'), ('ch19', 'u1'), ('ch20', 'u1'), ('ch21', 'u1'), ('ch22', 'u1'), ('ch23', 'u1'), ('ch24', 'u1'), ('ch25', 'u1'), ('ch26', 'u1'), ('ch27', 'u1'), ('ch28', 'u1'), ('ch29', 'u1'), ('ch30', 'u1')]\"))\r\n>>> isinstance(interp, uproot4.interpretation.Interpretation)\r\nTrue\r\n```\r\n\r\nEndianness works the same way: the endianness of the `from_dtype` is what you specify, but the `to_dtype` is the native endian version of that if `to_dtype` is not specified:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/3827f967da9c1031c8b5de14a877184d1089470d/uproot4/interpretation/numerical.py#L129-L135\r\n\r\nThe arguments involving `header_bytes` might have changed since they were a bit of a mess before and I tried to consolidate that. Updating your use of `header_bytes` options might involve some investigation of the code; sorry.\r\n\r\nOne of the new things that can help is that TBranch objects now have `debug` and `debug_array` methods built in, which we used to use the `uproot.asdebug` for. I've put a lot of bells and whistles into the debugging routines (centrally located in Cursor) because I had spent so much time working on these reverse engineering problems. You probably want to take a look at\r\n\r\n```python\r\nf[\"KM3NET_SUMMARYSLICE/KM3NET_SUMMARYSLICE/vector<KM3NETDAQ::JDAQSummaryFrame>\"].debug(0)\r\n```\r\n\r\nAnother new thing is \"[strided objects](https://github.com/scikit-hep/uproot4/blob/3827f967da9c1031c8b5de14a877184d1089470d/uproot4/interpretation/objects.py#L403),\" which were `asobj(astable(...))` before. Strided means that they are interpreted as interleaving strides of a buffer that was loaded all at once, not a Python for loop. The other kind of object interpretation, \"[as objects](https://github.com/scikit-hep/uproot4/blob/3827f967da9c1031c8b5de14a877184d1089470d/uproot4/interpretation/objects.py#L113),\" is a Python for loop. I think the only difference between a jagged, strided object interpretation and a jagged, recordarray-dtype interpretation is that the strided objects get labeled with a parameter that Awkward can pick up and use to add methods, such as adding boosting methods to Lorentz vectors. The structure of these two cases is the same (and parameters can always be added later).",
  "created_at":"2020-08-25T15:44:04Z",
  "id":680106291,
  "issue":76,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDEwNjI5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T15:44:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Alright thanks! My main confusion came from the fact that in `uproot3` I used `('dom_id', 'i4')` whereas in `uproot4` I had to specify `('dom_id', '>i4')`. Anyways, I had a busy day and should have spent a bit more time on investigating.\r\n\r\nThe `debug` stuff is really great, I really appreciate that!\r\n\r\nThe strided objects is an interesting feature, I have to explore it.\r\n\r\nBtw. I just stumbled upon another issue while I am trying to evaluate the performance of the new awkward1-arrays returned by `uproot4`. They seem to work really nice and intuitively, I am happy that I can get rid of the user-interface-code which I designed to propagate nested indexing over a hierarchy of numpy recarrays which were mapped to lazyarrays of ROOT locations (long ones, just as you saw before). Now I only need to take care of a user-friendly mapping and people can simply work with awkward1 arrays, very nice!\r\n\r\n...so the issue I stumbled upon is a memory-pileup when executing `branch.array(interp)` multiple times on a larger file. I'll create a new issue with an MWE.",
  "created_at":"2020-08-25T16:09:45Z",
  "id":680121758,
  "issue":76,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDEyMTc1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T16:09:45Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Just for the sake of completeness: the mentioned memory-pileup-issue was just a very lazy Python GC...",
  "created_at":"2020-08-25T16:37:07Z",
  "id":680137242,
  "issue":76,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDEzNzI0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T16:37:07Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Perfect, thanks for the follow-up!",
  "created_at":"2020-08-25T14:54:18Z",
  "id":680075308,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDA3NTMwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T14:54:18Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Ah\u2014they passed because CI isn't testing Dask. That's okay; I'm going to be removing the Dask experiment, anyway. (It puts restrictions on Awkward Arrays that I complained about in scikit-hep/awkward-1.0#350.)",
  "created_at":"2020-08-25T14:55:29Z",
  "id":680076011,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDA3NjAxMQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2020-08-25T14:55:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> which I guess is legal, so the regex has to allow for that.\r\n\r\nI just tried it when working on another issue, and ROOT _does_ complain:\r\n\r\n```\r\nroot [8] tree->Branch(\"stuffo\", &stuffo, \"/L\")\r\nWarning in <TBranch::TBranch>: No name was given to the leaf number '0' in the leaflist of the branch 'stuffo'.\r\n(TBranch *) 0x562798faa940\r\n```\r\n\r\nso it's not 100% allowed.",
  "created_at":"2020-08-25T16:49:13Z",
  "id":680143969,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDE0Mzk2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T16:49:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK interesting :wink: Do you plan to change it back or show a warning? At this very moment I don't see any danger in the relaxed regex but maybe encouraging a proper protocol is the better way, meaning that a warning/error is probably more appropriate than silently accepting it :thinking: ",
  "created_at":"2020-08-25T17:09:11Z",
  "id":680154611,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDE1NDYxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T17:09:11Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"No, ROOT's warning is in the creation of these nameless leaves, but since ROOT doesn't forbid it, there can be files out there that have this feature. We therefore need to be able to read it.\r\n\r\nThis regex was one consequence of that; there might be others. If, for instance, we identify leaves as fields by name, we'd have to introduce substitute names. (I haven't checked to see whether ROOT already does that at creation-time.)",
  "created_at":"2020-08-25T17:14:21Z",
  "id":680157258,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDE1NzI1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T17:14:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yep I see, you're right!",
  "created_at":"2020-08-25T18:11:58Z",
  "id":680187987,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDE4Nzk4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-25T18:11:58Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"NONE",
  "body":"This is indeed very interesting. \r\nI played around a little bit to look at the different responses by ROOT (v6.18/04).\r\n```\r\nroot [1] TTree *tree = new TTree(\"nums\",\"some numbers\");\r\nroot [2] float f = 1;\r\nroot [3] tree -> Branch(\"float\",&f,\"/F\");\r\nWarning in <TBranch::TBranch>: No name was given to the leaf number '0' in the leaflist of the branch 'float'.\r\nroot [4] tree -> Branch(\"float2\",&f,\"float2/F\");\r\nroot [5] float g[1] = {1};\r\nroot [8] tree -> Branch(\"float_arr\",&g,\"[1]/F\");\r\nroot [9] tree -> Branch(\"float_arr2\",&g,\"float2[1]/F\");\r\nroot [10] tree -> Print()\r\n******************************************************************************\r\n*Tree    :nums      : some numbers                                           *\r\n*Entries :        1 : Total =            2926 bytes  File  Size =          0 *\r\n*        :          : Tree compression factor =   1.00                       *\r\n******************************************************************************\r\n*Br    0 :float     : /F                                                     *\r\n*Entries :        1 : Total  Size=        652 bytes  One basket in memory    *\r\n*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    1 :float2    : float2/F                                               *\r\n*Entries :        1 : Total  Size=        655 bytes  One basket in memory    *\r\n*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    2 :float_arr : [1]/F                                                  *\r\n*Entries :        1 : Total  Size=        652 bytes  One basket in memory    *\r\n*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\n*Br    3 :float_arr2 : float2[1]/F                                           *\r\n*Entries :        1 : Total  Size=        673 bytes  One basket in memory    *\r\n*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\r\n*............................................................................*\r\nroot [11] tree -> Scan(\"*\",\"\",\"colsize=28\");\r\n****************************************************************************************************************************************\r\n*    Row   *        float.float.__noname0 *                       float2 *         float_arr.float_arr. * float_arr2.float_arr2.float2 *\r\n****************************************************************************************************************************************\r\n*        0 *                            1 *                            1 *                            1 *                            1 *\r\n****************************************************************************************************************************************\r\n```\r\n\r\nSo ROOT only throws a warning when using nameless leaves for basic variables (i.e. non-arrays) like in the Branch `float`. However, when using  unnamed arrays like in `float_arr` it seems to be no problem. \r\nI didn't notice this until now, because usually when I am saving basic variables I use only `tree->Branch(\"branchname\",&var)` and ROOT translates this automatically to  `tree->Branch(\"branchname\",&var,\"branchname/typeof(var)\")`\r\n\r\n> This regex was one consequence of that; there might be others. If, for instance, we identify leaves as fields by name, we'd have to introduce substitute names. (I haven't checked to see whether ROOT already does that at creation-time.)\r\n\r\nConcerning the naming of the leaves this seems to behave in a very interesting behavior. While in `tree->Print();` it looks like that the unnamed leave Branch (`*Br    0 :float     : /F                                                     *`) does not have something like a 'default'-name assigned to it. However, in `tree->Scan();` it seems like that there is some substructure to that. I haven't had time yet to look more into that but it thought already this small insight might be helpful.",
  "created_at":"2020-08-26T06:21:50Z",
  "id":680683409,
  "issue":77,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDY4MzQwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T06:21:50Z",
  "user":"MDQ6VXNlcjIwMTUxNTUz"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Btw. a minimal working example is also just\r\n\r\n```python\r\n>>> import uproot4\r\n>>> uproot4.open(\"foo:bar.root\")\r\n```\r\n\r\nThe problem is this line https://github.com/scikit-hep/uproot4/blob/b8828069b9ae52c742fb704f07e1a4e179fe7b30/uproot4/_util.py#L157\r\n\r\nThis simply assumes that whenever there is a colon, the first part is the path and the second part is the object path, which of course fails when there is a colon in the filename.\r\n\r\nEDIT: \r\nI played around with the test suite and found for example the pattern `file:/path/to/file : object path` as seen here https://github.com/scikit-hep/uproot4/blob/b8828069b9ae52c742fb704f07e1a4e179fe7b30/tests/test_0016-interpretations.py#L84\r\n\r\nThis would be another case to cover inside `file_object_path_split()`. I am not sure if this pattern is used in the wild, but I think restricting it to `file://` would probably be the better choice. But that's a minor issue.\r\n\r\nAnyways, the `file_object_path_split()` needs to be tweaked to cover all the cases when accepting `:` in filenames and respect the optional object path.\r\n\r\nIf the object path is always specified with leading and trailing spaces (`\" : \"`), then I think it's fairly easy.",
  "created_at":"2020-08-26T07:22:59Z",
  "id":680707569,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDcwNzU2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T07:43:28Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Funnily this tiny change let's quite a few tests pass but it obviously fails when the object path is appended without leading and trailing spaces around the `:`\r\n\r\n```python\r\n    try:\r\n        index = path.rindex(\" : \")\r\n    except ValueError:\r\n        return path, None\r\n```\r\n\r\nUnfortunately there is an ambiguity here, unless we assume that every root file ends with `.root`.",
  "created_at":"2020-08-26T07:59:03Z",
  "id":680723795,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDcyMzc5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T07:59:03Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I created a pull request (https://github.com/scikit-hep/uproot4/pull/80) which however comes with a grain of salt, so let's discuss further.",
  "created_at":"2020-08-26T08:23:33Z",
  "id":680735672,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDczNTY3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T08:23:33Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"I was going to say that there's an option that simply turns off the colon-splitting, but I look now and I can't find it. I'm sure it was there. Anyway, that was the intention: that a boolean parameter, by default true, would specify whether the filename gets split into filesystem/URL path and in-ROOT party or not.\r\n\r\nAlso, the splitting character used to be a vertical bar `|` because this is a very implausible character for a filename, given the amount of trouble it would cause in the shell. The reason I changed it was because it's not a natural separator\u2014nothing about it suggests that this should be the dividing line between paths. By contrast, a colon is used this way (in shell variable lists, such as `PATH`).\r\n\r\nI switched to a colon because it's an illegal character for filenames in MacOS and (if I remember right) URLs. Unix filenames can have almost any character in them, but considering the use of colons in file lists like `PATH`, that seemed implausible. It also seemed unlikely in Windows due to drive letter confusion.\r\n\r\nSo the colon had the right guessable meaning and didn't seem like it could be common in real file names. But ATLAS file names have colons in them? That's an important case. Maybe we should switch back to vertical bars?\r\n\r\nAt the very least, there should be a boolean parameter that just turns off the splitting. The `uproot4.open` function is already documented, so in principle it's discoverable, and anyone who doesn't discover it can be told pretty quickly, \"set `split_path_with_bar=False`.\" I'm surprised it's not there because I coulda sworn I added that. It's a two line change (adding the parameter default and the `if` statement).\r\n\r\nOn assuming ROOT files always end with \".root\", that's a stricter retirement than not including colons. I'd rather not make that assumption.",
  "created_at":"2020-08-26T11:36:50Z",
  "id":680825523,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDgyNTUyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T11:36:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah OK, with that additional option it's a bit better but still may cause confusion. I think switching to `|` is better since sticking to `:` and the current implementation means that at least in case of ATLAS files one could not load an object path via `open()`. Or do you consider adding an additional `object_path=` parameter?\r\n\r\nBtw. `:` in filenames work on macOS (it was an illegal character on class Mac OS and on the old carbon layer of Mac OS).",
  "created_at":"2020-08-26T11:45:42Z",
  "id":680829085,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDgyOTA4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T11:45:42Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Another possibility is to require paths-in-file to start with a slash (because they are absolute, after all), and then the real separator would be \"`:/`\", which works for ATLAS.\r\n\r\nOr to only split with a colon (a convenience) if the filename ends with \"`.root`\", while not requiring filenames to always end with \"`.root`\". Sorry if I misunderstood that that was your actual suggestion.\r\n\r\nIt drop it entirely. This is an elaboration on the idea of deep getitem (in which you can write a single path that selects through the TDirectory and TBranch hierarchy), but maybe it's an overextension? Maybe it's not bad to require those extra quotes and braces. They were intended for quick navigation.",
  "created_at":"2020-08-26T11:54:13Z",
  "id":680832721,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDgzMjcyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T11:54:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've been thinking about it all morning. An `object_path` parameter would defeat the purpose: it's easier to type\r\n\r\n```python\r\nuproot4.open(\"filename.root\")[\"pathname\"]\r\n```\r\n\r\nthan it is to type\r\n\r\n```python\r\nuproot4.open(\"filename.root\", object_path=\"pathname\")\r\n```\r\n\r\nWhile the `file_object[\"dir1/dir2/branch1/branch2\"]` feature was requested, the `uproot4.open(\"filename.root:dir1/dir2/branch1/branch2\")` feature was not requested\u2014it was an extrapolation of that request.\r\n\r\nSince there isn't a compelling reason to do otherwise, we should do the simplest thing of not having any colon/vertical bar syntax at all, since that would be the most guessable and wouldn't require reference to the documentation before getting started. Uproot 4 is still in a fluid enough state (no documentation yet) that I can remove something like that.",
  "created_at":"2020-08-26T12:56:03Z",
  "id":680861697,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDg2MTY5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T12:56:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I fully agree ;) I first thought that the column-syntax has also some special shortcuts to get to the data source even more efficiently but looking at the code it seems to me that it's just an alternative to `open()[]`... but correct me if I am wrong!",
  "created_at":"2020-08-26T13:26:29Z",
  "id":680878555,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDg3ODU1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T13:26:29Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"That's right: it's only syntactic sugar. The same is true of `file_object[\"dir1/dir2/branch1/branch2\"]`, but that syntactic sugar was helpful because people were stumbling over `file_object[\"dir1\"][\"dir2\"][\"branch1\"][\"branch2\"]`. I guess it's okay if the number of square brackets is reduce to one, rather than being reduced to zero.",
  "created_at":"2020-08-26T13:50:27Z",
  "id":680892227,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDg5MjIyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T13:50:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Now that I'm going in and deleting it, I find that it really is important: interfaces that take TTrees from multiple files need a way to express both succinctly.\r\n\r\n```python\r\nlazyarray = uproot4.lazy([\"batch1/*.root : treename1\", \"batch2/*.root : treename2\"])\r\n\r\nfor arrays in uproot4.iterate([\"batch1/*.root : treename1\", \"batch2/*.root : treename2\"]):\r\n    ...\r\n```\r\n\r\nWithout this, the object path could be a separate parameter (as it was in Uproot3, which was a stumbling block because `uproot.iterate(\"batch1/*.root\", \"treename1\")` led people to forget the tree name) but then it can't be different for different files, as above. Allowing it to be different for different files would require 2-tuples or something, if there isn't a syntactic delimiter.\r\n\r\nSo it is more important than quick and easy typing in the `uproot4.open` case... I have to think more about it... I'm open to suggestions.",
  "created_at":"2020-08-26T13:59:40Z",
  "id":680897602,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDg5NzYwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T13:59:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Maybe it's staring me in the face: the colon could be outside of quotation marks if the file name specifier is allowed to be a dict:\r\n\r\n```python\r\nlazyarray = uproot4.lazy([{\"batch1/*.root\": \"treename1\"}, {\"batch2/*.root\": \"treename2\"])\r\n```\r\n\r\nbut for the common case of all files having the same name, requiring people to know this weird syntax should be unnecessary:\r\n\r\n```python\r\nlazyarray = uproot4.lazy([\"batch1/*.root\", \"batch2/*.root\"], \"treename\")\r\n```\r\n\r\nor maybe pick the only TTree if the file has only one TTree (not counting cycle number).",
  "created_at":"2020-08-26T14:03:36Z",
  "id":680899864,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDg5OTg2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T14:03:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Also allowing\r\n\r\n```python\r\nlazyarray = uproot4.lazy([{\"batch1/*.root\": \"treename1\", \"batch2/*.root\": \"treename2\"])\r\n```\r\n\r\nThe fact that `uproot4.open` would accept dicts only if they have length 1 would be because `uproot4.open` is a special case that only opens one file. I'm trying it out and will post a few unit tests here as a suggestion.",
  "created_at":"2020-08-26T14:17:08Z",
  "id":680908186,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDkwODE4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T14:17:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I really like the dict approach, I think that's quite transparent.\r\n\r\nHowever, providing more logic is probably too much (like picking the only TTree if there is only one), I think that Python provides enough to make such things easily expressible...?\r\n\r\nWith a `dict` approach one could also write a generator style\r\n\r\n```python\r\nlazyarray = uproot4.lazy({fname: \"treename\"} for fname in [\"batch1/*.root\", \"batch2/*.root])\r\n```\r\n\r\nor even simplifying it to a single `dict`:\r\n\r\n```python\r\nlazyarray = uproot4.lazy({fname: \"treename\" for fname in [\"batch1/*.root\", \"batch2/*.root]})\r\n```\r\n\r\nbut notice that this is just one `dict` in contrast to your `list` of `dict`s example, so ordering is obviously not guaranteed for Python 3.5 and less, which might confuse one or the other. Anyways, that's just to throw in another point, the general gain is very low and the number of symbols to type is almost the same as above.",
  "created_at":"2020-08-26T15:39:06Z",
  "id":680958350,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MDk1ODM1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T15:39:06Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I don't think using the filename as a key in a dict is so great. What about instead a list of dicts with two specific keys: `[{\"file_path\": fname, \"object_path\": \"tree;1\"}, ...]`",
  "created_at":"2020-08-26T18:02:21Z",
  "id":681035996,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTAzNTk5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T18:02:21Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> I don't think using the filename as a key in a dict is so great. What about instead a list of dicts with two specific keys: `[{\"file_path\": fname, \"object_path\": \"tree;1\"}, ...]`\r\n\r\nBut then you'd have to remember the spellings of those keys: `\"file_path\"` and `\"object_path\"`. They become part of the API that has to be remembered or looked up. If I'm in a LISPy mood, I'd be inclined to expect hyphens, rather than underscores, and maybe the whole second word is unnecessary. What you suggest would be great for a REST API, but it's pretty verbose for Python interaction.\r\n\r\nAlso, it prevents multiple files in one dict, since there can only be one `\"file_path\"` key.",
  "created_at":"2020-08-26T18:24:53Z",
  "id":681047105,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTA0NzEwNQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2020-08-26T18:24:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"These are tests of the dict method: https://github.com/scikit-hep/uproot4/blob/96e99bc09ac45b1c306b54855e1b36060f23e096/tests/test_0081-dont-parse-colons.py\r\n\r\nExamples look like this:\r\n\r\n```python\r\none_file = uproot4.open(\"filename.root\")\r\none_tree = uproot4.open(\"filename.root\")[\"treename\"]\r\none_tree = uproot4.open({\"filename.root\": \"treename\"})\r\n\r\nmany_trees = uproot4.lazy(\"filenames*.root\")                 # if all files have exactly one TTree\r\nmany_trees = uproot4.lazy({\"filenames*.root\": \"treename\"})   # if they don't\r\n\r\nmany_trees = uproot4.lazy([\"dir1/*.root\", \"dir2/*.root\"])    # if all files have exactly one TTree\r\nmany_trees = uproot4.lazy({\"dir1/*.root: \"treename\", \"dir2/*.root\": treename})       # if they don't\r\nmany_trees = uproot4.lazy([{\"dir1/*.root: \"treename\"}, {\"dir2/*.root\": treename}])   # careful about order\r\n\r\nmany_trees = uproot4.lazy([already_open_tree1, already_open_tree2, ...])   # objects like one_tree above\r\n```\r\n\r\nI don't like magical things like guessing which TTree you mean, but it has been requested a user. (Don't ask me to find the reference: it's hard to search for these things.) However, it only guesses which TTree when there is exactly one in the file (in all nested TDirectories), and if two kinds of files are accidentally mixed, it would only be successful if the requested TBranch names _also_ overlap, which would be an even bigger coincidence than having different TTrees with the same name. (\"`Events`\" is a very popular name.)\r\n\r\nThis reminds me that I was also asked (in Uproot 3, in the mists of time) to make iteration skip over any files that are missing the requested TTree. That's a bit more dangerous, but the motivation is to not have to weed out badly written files. (Not what I'd call a careful analysis, but maybe a first look at the data.) Rather than porting that feature into Uproot 4, I've made it optional: `allow_missing=False` is the default, but if anyone has that problem again, I can point them to this option so they can opt into a more relaxed view.",
  "created_at":"2020-08-26T18:38:53Z",
  "id":681054061,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTA1NDA2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T18:47:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Since there have been a few differing suggestions about what to do here, I'm not going to merge PR #81 immediately. I'll wait a day or two to let others voice their opinions, to see if there is a majority view. (@henryiii, do you have an idea?)\r\n\r\nWe don't have a formal voting system, but I'll take your suggestions into account (as I did with a lot of the names in Awkward1, where I personally preferred fewer underscores).",
  "created_at":"2020-08-26T18:49:45Z",
  "id":681059186,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTA1OTE4Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-26T18:49:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"On a quick overview, I like the dict method that's just been merged. If one wanted to provide a colon based CLI interface, it would be easy to write:\r\n\r\n```python\r\n# arg might be filename or filename:treename or filename:treename:\r\nif len(x := arg.split(\":\",1)) > 1:\r\n    arg = {x[0]: x[1]}\r\n```\r\n(which obviously wouldn't work on filenames with colons, but that would not be inside uproot, either)",
  "created_at":"2020-08-26T19:09:11Z",
  "id":681069384,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTA2OTM4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T19:09:11Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> But ATLAS file names have colons in them? That's an important case.\r\n\r\nWell, I guess I should be more careful with my words so that I don't get anyone blamed for anything. It isn't that ATLAS software explicitly has colons in filepaths. However, one of the most widely used analysis frameworks that people in ATLAS have developed on their own is [`xAODAnaHelpers`](https://github.com/UCATLAS/xAODAnaHelpers), and the following [`xAH_run.py`](https://xaodanahelpers.readthedocs.io/en/latest/UsingUs.html#) example command\r\n\r\n```\r\nxAH_run.py \\\r\n    --config config/config.py \\\r\n    --files data16_13TeV:data16_13TeV.periodA.physics_Main.PhysCont.DAOD_JETM1.grp16_v01_p4061 \\\r\n    --inputRucio \\\r\n    --nevents 10000 \\\r\n    --force \\\r\n    direct\r\n```\r\n\r\nresults in an output file directory structure that includes (given the options in the `config/config.py`) a file at the path of\r\n\r\n```\r\nsumbitDir/data-tree/data16_13TeV:data16_13TeV.periodA.physics_Main.PhysCont.DAOD_JETM1.grp16_v01_p4061.root\r\n```\r\n\r\nSo it can/does happen (@kratsg please jump in and correct anything I've misrepresented).\r\n\r\n> I'll wait a day or two to let others voice their opinions, to see if there is a majority view.\r\n\r\nThanks. I don't have very strong API views at the moment (but I'll try to actually think on it now). cc the rest of `pyhf` for this (@lukasheinrich, @kratsg) and also @alexander-held and @masonproffitt.",
  "created_at":"2020-08-27T00:58:44Z",
  "id":681200168,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTIwMDE2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T01:08:53Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"fwiw, `ROOT`s `TFile::Open` seems to be able to handle colon in the filename and in the paths after the fact pretty well. You could split this up into trying to be smarter about things automatically:\r\n\r\n- check if file is valid path before splitting\r\n- check if colon split from the right is a valid path\r\n\r\nthen loudly complain you can't find the file. At least you handle both cases this way... but I'm still not really sure how many people use the `:/path/to/obj` functionality (it does exist in ROOT, but it doesn't seem like a common feature...)",
  "created_at":"2020-08-27T01:27:06Z",
  "id":681233471,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTIzMzQ3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T01:27:24Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"> I'm still not really sure how many people use the `:/path/to/obj` functionality (it does exist in ROOT, but it doesn't seem like a common feature...)\r\n\r\nReally? I had no idea! I guess that strengthens the case that colon is a natural character to use\u2014more easily guessable\u2014if it was independently chosen. (Do they require the leading slash? That effectively makes the delimiter \"`:/`\", which is less likely to conflict in real paths\u2014you'd need a directory to end with a colon.)\r\n\r\nAbout checking both with and without object path to see if the file exists, the remote protocols (HTTP and XRootD) would require an extra round trip for that. Also, what if (rare case, admittedly) _both_ exist?\r\n\r\nEarlier today, I wanted to remove the colon thing altogether because simple is better than clever. However, there are those functions that take many files, or more specifically, many file/tree combinations. The driving consideration is finding a syntax that simplifies the specification of many files/trees, and the single file/tree should be a special case of that. The dict thing works, but maybe it's strange looking or not guessable? If there are better ways to express many files/trees, let me know.",
  "created_at":"2020-08-27T03:03:23Z",
  "id":681315446,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTMxNTQ0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T03:03:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I find the existing colon functionality very convenient. One use case I have in mind are frameworks for histogram production. Users need to specify where all the inputs to their histograms are found. The existing syntax allows to do that in just one string, users could e.g. write\r\n\r\n```\r\nmc_samples/{SAMPLE_TYPE}.root:{SYSTEMATIC_VARIATION}\r\n```\r\n\r\nand specify a list of sample types and systematic variations to fill in the placeholders. If the paths inside and outside the ROOT file need to be specified separately, two such strings would be needed. Having it all in one string also reinforces the idea that for the structure of the data it does not matter much whether something is in a directory or a `TDirectory`.\r\n\r\nIf one takes the above example but puts everything into a big root file, the string would look like\r\n\r\n```\r\nmc_samples.root:{SAMPLE_TYPE}/{SYSTEMATIC_VARIATION}\r\n```\r\n\r\nand it is quickly evident that the structure of content is the same. This is harder to parse when split up.\r\n\r\nSince it came up above: I've had to deal with file structures before where folder names ended with `.root` (and those folders contained ROOT files), but due to the name the folders mistakenly were interpreted as a ROOT file (this was with ROOT, not `uproot`). Relying on this to parse might not always work.",
  "created_at":"2020-08-27T08:16:45Z",
  "id":681760243,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTc2MDI0Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-27T08:16:45Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ok, it seems I've mispoken and what I'm remembering is a feature that `hadd` supports but that `TFile` and `TFile::Open` does not:\r\n\r\n```\r\nhadd target_path.root:/pathtomerge in1.root in2.root\r\n```\r\n\r\nwill only hadd-merge everything under that path. I tried googling to find an actual example, but alas, cannot... In any case, the `TFile` [documentation](https://root.cern.ch/doc/master/classTFile.html#ad0377adf2f3d88da1a1f77256a140d60) indicates that they still strongly recommend suffixing with `.root` at the end of the file path... not sure why, but they rely on [`TUrl` to parse all file paths given](https://root.cern.ch/doc/master/TUrl_8cxx_source.html#l00108).\r\n\r\nAnyway, if it were me, I would want `pathlib.Path(...)` to be treated as a file path without parsing for the colon to get around this, rather than treating it as a string (which the current behavior seems to be).",
  "created_at":"2020-08-27T10:53:12Z",
  "id":681874938,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTg3NDkzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T10:53:12Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"There have been quite a few comments about the colon syntax being useful; one on Slack. Maybe it needs to be reinstated. (The syntax issues with URLs and Windows drive letters have been resolved; what remains is @matthewfeickert's issue about it being an unpleasant surprise when you don't know about it.)\r\n\r\n   * What do people think about allowing the colon syntax but requiring the object path to start with a `/`, and hence the delimiter is effectively `:/`, rather than just `:`?\r\n   * What about allowing both strings/Paths, which parse colons, and the dict syntax shown above, which do not? The dict syntax could then be the way to escape colon parsing, instead of `parse_colons=False` parameter.\r\n\r\nI can also add a detailed explanation of the syntax in all \"file not found\" error messages (local files, HTTP, etc.). If the colon parsing fails, the very likely error will be file not found, so that's when and how users can be informed. (Again, we don't _assume_ everybody's read the documentation!)",
  "created_at":"2020-08-27T12:48:32Z",
  "id":681927322,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTkyNzMyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T12:48:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"By the way, unrelated, but I finally understand why `vi my_broken_file.py:123` doesn't automatically try to open the file and go to line 123 (` +123` does, FYI); it's a valid filename on some systems.\r\n\r\nI like option 2, I think.",
  "created_at":"2020-08-27T13:43:50Z",
  "id":681957648,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTk1NzY0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T13:43:50Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> I like option 2, I think.\r\n\r\nWhich one is option 2?",
  "created_at":"2020-08-27T13:45:27Z",
  "id":681958546,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTk1ODU0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T13:45:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Colon parsing by default, but not in the dict syntax. Assumes that colons in filenames are somewhat rare, which I expect they are?",
  "created_at":"2020-08-27T13:46:41Z",
  "id":681959217,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTk1OTIxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T13:46:41Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Colon parsing by default, but not in the dict syntax. Assumes that colons in filenames are somewhat rare, which I expect they are?\r\n\r\nrucio relies on scopes for filenames, and scopes are separated with colons: `mc15_13TeV:{filename}` and when downloading, will include the scope as well. Dataset discovery relies on these scopes.",
  "created_at":"2020-08-27T13:48:50Z",
  "id":681960456,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTk2MDQ1Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-27T13:48:50Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"NONE",
  "body":"Repeating comment from Slack: I do use `:/path/to/tree` fairly often (when the CMSSW TFileService is used to create a TTree, that TTree resides in a TDirectory).",
  "created_at":"2020-08-27T13:49:03Z",
  "id":681960583,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTk2MDU4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T13:49:03Z",
  "user":"MDQ6VXNlcjQ2NzI4MDg="
 },
 {
  "author_association":"MEMBER",
  "body":"> Assumes that colons in filenames are somewhat rare, which I expect they are?\r\n\r\nI made the same assumption, based on some web-research. I saw somewhere that colons aren't allowed at all in MacOS filenames, but @kratsg said that this restriction only applied to MacOS 9 and below, and possibly OSX in a compatibility mode.\r\n\r\n(I remember MacOS 2\u20129 used `:` instead of `/` as a delimiter between directories, so the character was not allowed for the same reason that `/` is not allowed in filenames of modern filesystems. MacOS 1 didn't have directories.)\r\n\r\n> Well, I guess I should be more careful with my words so that I don't get anyone blamed for anything. It isn't that ATLAS software explicitly has colons in filepaths. However, one of the most widely used analysis frameworks that people in ATLAS have developed on their own is [`xAODAnaHelpers`](https://github.com/UCATLAS/xAODAnaHelpers)...\r\n\r\n...which leads naturally to colons in filenames, and that's how this all came up. @kratsg just mentioned above that Rucio can put colons in filenames, so we'll probably start seeing it in CMS, too.\r\n\r\nI'm leaning toward this \"option 2\" as well. I think the main open question for me is whether to require a `/` after the `:`, such that `:/` is the true delimiter.\r\n\r\nProbably the most important thing is that the precise rules are printed to the screen in every \"file not found\" error message, so that people can adjust after a first attempt.",
  "created_at":"2020-08-27T13:57:34Z",
  "id":681965744,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTk2NTc0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T13:57:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> > Colon parsing by default, but not in the dict syntax. Assumes that colons in filenames are somewhat rare, which I expect they are?\r\n> \r\n> rucio relies on scopes for filenames, and scopes are separated with colons: `mc15_13TeV:{filename}` and when downloading, will include the scope as well. Dataset discovery relies on these scopes.\r\n\r\nRight, this is exactly what I'm familiar with in dealing with my own analysis files. I think it's important for a filename like `foo:bar.root` to work in the standard string syntax. This is a very common case (for ATLAS at least). I like dropping the colon parsing entirely myself, but I think requiring `:/` is probably still good enough.",
  "created_at":"2020-08-27T14:01:32Z",
  "id":681968213,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTk2ODIxMw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-27T14:01:32Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"Since the colon parsing is a convenience and gaps in many-file functions can be filled in with the dict syntax, how about if it only works in limited circumstances, such as after `.root`? (@tamasgal originally made this suggestion.) So,\r\n\r\n   * `/path/to/filename.root:path/to/ttree` picks out an object at `path/to/ttree` (requiring `.root` makes requiring a leading `/` in the object path superfluous, since it's already quite unambiguous, and it's a potential \"gotcha\")\r\n   * `/path/to/file:name.root` would be interpreted as a file, not a \"`name.root`\" object path within a file, since \"`file`\" doesn't end in \"`.root`\"\r\n   * `/path/to/file:name.root:path/to/ttree` also picks out an object at `path/to/ttree` from a file named `file:name.root`; earlier colons are part of the filename\r\n   * use the dict syntax if you really need to ensure that colons are never parsed; `{\"/path/to/file.root:haha-this-is-the-filename.root\": None}` won't parse the colon and won't pick out an object within the file because of the `None` value\r\n   * use the dict syntax if you need to pick out objects and your files do not end with `.root`, or you're using wildcards that don't end in `.root` (a likely example: `/path/to/directory/*`). This is for functions that need to find many files, such as `uproot4.iterate`, `uproot4.concatenate`, and `uproot4.lazy`.\r\n\r\nThe colon parsing will be much simpler, based on the regex `\\.root\\s*:\\s*`, because I'll never need to worry about Windows drive letters or URL schemas with the `\\.root` disambiguator.\r\n\r\nThumbs-up this comment if you're in favor, write another comment below if you're not; I'm going to start coding up this solution and if we're in general agreement, I'll merge it.",
  "created_at":"2020-08-27T14:26:04Z",
  "id":681983249,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTk4MzI0OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-27T14:26:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"There's still a problem with requiring `.root:`. Another thing that I often see (again from rucio) is filenames ending with a number like `.root.1`. For a random public example, see <https://twiki.cern.ch/twiki/bin/view/Sandbox/GridNotes>.",
  "created_at":"2020-08-27T14:43:37Z",
  "id":681993971,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTk5Mzk3MQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2020-08-27T14:43:37Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski , I think you shouldn't change the parsing too much to try and support so much. What I think should happen is the following:\r\n\r\n- `str` or `bytes` passed in: be \"smart\" and parse out colons and figure out what the user wants\r\n- `pathlib.Path` -- assume this is openable via `path.open()` instead of trying to convert to string and parsing\r\n- `file pointer` (an already opened file) -- just load that up, no need to call `open` etc...\r\n\r\nIs this not at all possible to do? Then if users really start having more complicated files that, for whatever reason, is borking the magic of `uproot`, then can be more specific step by step, going from `str` to `Path` to a `filepointer` object. The nice thing with that last option, I hope, is that one can use `BytesIO` as input as well...",
  "created_at":"2020-08-27T14:53:42Z",
  "id":682000351,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjAwMDM1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T14:53:42Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"For files ending in `.root.1`, etc., I would have thought you're out of the easy case and have to start using dicts. The error message would say that. Would that be a big imposition?\r\n\r\nI hadn't considered having different behavior for `str`/`bytes` than for `pathlib.Path`. Would that be more expected?\r\n\r\nAlso, on accepting an already open Python file-handle (loosely interpreted as anything with `read` and `seek`, like `io.BytesIO`), Uproot has never had this capability, but it would be possible. It would exclude parallel reading of baskets, but you get what you pay for. That could become a new Source subclass, but it's beyond the scope of this issue.\r\n\r\n------------------------------\r\n\r\nSince there are no thumbs up on [the previous proposal](https://github.com/scikit-hep/uproot4/issues/79#issuecomment-681983249) and two objections thereafter, how about the following:\r\n\r\n   * No consideration for whether a `.root` substring is found.\r\n   * If a `str` or `bytes` is passed, do the colon parsing as before (many people have said that they want it, though it can be a stumbling block, as in @matthewfeickert's case that started this thread).\r\n   * If a `pathlib.Path` is passed, do not do colon parsing, which is a good way of enforcing what you want in `uproot4.open`, but would be limiting in functions that need to open many file-tree combinations, such as `uproot4.iterate`, `uproot4.concatenate`, and `uproot4.lazy`.\r\n   * Support the dict syntax, which also does not do colon parsing, since the key-value separation distinguishes between file path and object path. This is the most general way to express a file path and object path, particularly useful for expressing many flie-tree combinations, and its limit for `uproot4.open` is a length-1 dict.\r\n   * All \"file not found\" errors explain the rules clearly with examples.\r\n\r\nWhat does everybody think of this? Thumbs up or comment: thanks!",
  "created_at":"2020-08-27T15:26:16Z",
  "id":682020092,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjAyMDA5Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":4,
   "total_count":4
  },
  "updated_at":"2020-08-27T15:26:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Is there something wrong with using `:/` as the delimiter? That seems like the easiest way to (I think) satisfy everyone.\r\n\r\nIn any case, I'm confused as to what the `dict` syntax is when you just want to open a file with a colon in the filename. Is it:\r\n\r\n- `uproot.open({'foo:bar.root': None})`?\r\n- `uproot.open({'foo:bar.root': ''})`?\r\n- `uproot.open({'foo:bar.root': '/'})`?\r\n\r\nor some/all of the above? Yikes. Actually, I guess it can't even be `''`, since that's a valid key. My problem here is that this suddenly makes something as simple as opening a file more confusing and results in inconsistent behavior between different filenames. It's very not beginner-friendly, as you now need to know about dictionaries and their syntax and a bit about `TDirectory` before you can even look at anything in the ROOT file.",
  "created_at":"2020-08-27T16:45:08Z",
  "id":682064501,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjA2NDUwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T16:45:08Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"(I've actually been distracted by other work, so just getting back to this now.)\r\n\r\nOne difficulty with optimizing usability is that \"what is natural\" or \"what a novice might try\" is rather speculative. I would expect novices (including me, approaching a system I don't know) to get tripped up by the requirement that the object path has to be expressed as absolute, i.e. that it must start with `:/blah` and not just `:blah`, even though it is always going to be evaluated relative to `/`. (This was confusing in the [file URI scheme](https://en.wikipedia.org/wiki/File_URI_scheme#How_many_slashes?), for example.) I'm not of a strong opinion about this point, though, because I think the error message is where the wide array of users' guesses as to how it works will get narrowed down to how it actually does work\u2014assuming users read error messages. (Sometimes a problem, but not a deep one if they copy-paste the error message into the bug report for me to read.)\r\n\r\nThe other difficulty with optimizing usability is that expectations among advanced users become necessities. I hadn't known (I guessed right) that the colon syntax was an expected feature, and so losing it would be a stumbling block for users who expect it from ROOT.\r\n\r\nBut the thing that's driving the dict syntax is neither of the above. It's driven by the functions that access many file-object (actually, file-TTree) pairs. They need a way\u2014not a convenient way but _any_ way\u2014to express combined filesystem + ROOT object paths for hundreds or thousands of files. If we could have used the colon syntax for that, then that would have been great. But some filesystem paths have colons in them. In principle, a directory name could end with a colon, which would get incorrectly identified as `:/`. So the many file-object functions need to have a backup syntax for when the string syntax isn't expressive enough, and that's what the dicts are. Only [one other suggestion](https://github.com/scikit-hep/uproot4/issues/79#issuecomment-681035996) has been made that would be as unambiguous, but it's pretty verbose and introduces more things to remember.\r\n\r\nSo novice users do not need to know about the dict syntax; it only comes up if they can't express a group of filenames + object paths because the filenames have colons in them, and then it comes up as an error message because when they tried to do the naive thing, it couldn't find the file.\r\n\r\nI'm going to implement the [last proposal I made](https://github.com/scikit-hep/uproot4/issues/79#issuecomment-682020092). I considered not porting the dict syntax from the functions that need it (`uproot4.iterate`, `uproot4.concatenate`, `uproot4.lazy`) to the function that doesn't really need it (`uproot4.open`, for which only a length-1 dict would be allowed) because the \"file not found\" error messages don't know whether they're from an attempt to open a single file or many files, and tracking that provenance through everything so that they can give context-dependent error messages doesn't seem worthwhile. Making the same syntax apply to all functions equally, even if it's not really needed for one of them, is simpler.\r\n\r\n----------------------\r\n\r\n> In any case, I'm confused as to what the `dict` syntax is when you just want to open a file with a colon in the filename. Is it:\r\n> \r\n> - `uproot4.open({'foo:bar.root': None})`?\r\n> - `uproot4.open({'foo:bar.root': ''})`?\r\n> - `uproot4.open({'foo:bar.root': '/'})`?\r\n> \r\n> or some/all of the above?\r\n\r\nI think the first and third would work; the second wouldn't for the reason you mentioned. But the preferred way to do it, following @kratsg's advice about `pathlib.Path`, is\r\n\r\n- `uproot4.open(pathlib.Path(\"foo:bar.root\"))`\r\n\r\nNo colon parsing would be performed on a `pathlib.Path`, and since this is `uproot4.open`, there's no reason to bring in any fancy dicts because you don't have to specify a TTree path. For `uproot4.iterate`, `uproot4.concatenate`, `uproot4.lazy`, you do have to specify TTrees because you're iterating over, concatenating, or building lazy arrays from TTrees, not files. There wouldn't be a good reason to make the value in a dict `None`, though it would work, for consistency's sake.\r\n\r\nThe error message should not only say what's possible, but should say what's preferred. Now I'm finally going to go and implement that.",
  "created_at":"2020-08-27T18:09:50Z",
  "id":682108339,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjEwODMzOQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2020-08-27T18:09:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In the end, the error messages look like this:\r\n\r\n```\r\nFileNotFoundError: file not found\r\n\r\n    '/really/long/path/that:in:the:end/is/not-a-file.root'\r\n\r\nFiles may be specified as:\r\n   * str/bytes: relative or absolute filesystem path or URL, without any colons\r\n         other than Windows drive letter or URL schema.\r\n         Examples: \"rel/file.root\", \"C:\\abs\\file.root\", \"http://where/what.root\"\r\n   * str/bytes: same with an object-within-ROOT path, separated by a colon.\r\n         Example: \"rel/file.root:tdirectory/ttree\"\r\n   * pathlib.Path: always interpreted as a filesystem path or URL only (no\r\n         object-within-ROOT path), regardless of whether there are any colons.\r\n         Examples: Path(\"rel:/file.root\"), Path(\"/abs/path:stuff.root\")\r\n\r\nFunctions that accept many files (uproot4.iterate, etc.) also allow:\r\n   * glob syntax in str/bytes and pathlib.Path.\r\n         Examples: Path(\"rel/*.root\"), \"/abs/*.root:tdirectory/ttree\"\r\n   * dict: keys are filesystem paths, values are objects-within-ROOT paths.\r\n         Example: {\"/data_v1/*.root\": \"ttree_v1\", \"/data_v2/*.root\": \"ttree_v2\"}\r\n   * already-open TTree objects.\r\n   * iterables of the above.\r\n```",
  "created_at":"2020-08-27T20:46:13Z",
  "id":682182137,
  "issue":79,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MjE4MjEzNw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-08-27T20:46:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the suggestion, but this is now being dealt with in #81.",
  "created_at":"2020-08-26T17:41:15Z",
  "id":681025422,
  "issue":80,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTAyNTQyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T17:41:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> The only references to lazy-loading (of arrays) I could find in the code so far are those using uproot4.lazy(), which requires a file and an object path.\r\n\r\nI am trying to cut down on the number of entry points, so there's only one way to make lazy arrays, `uproot4.lazy`. However, it accepts already opened TTree objects in its `files` argument (something that definitely needs to be documented). As such, we do have the ability to get lazy arrays from already open files, but the fundamental interface to lazy arrays is multi-file. (For example, you can make lazy arrays from a _list_ of already opened TTree objects.)\r\n\r\n> Btw. I noticed that the .array() method is very fast in many of our files which require custom interpretations (which is really nice!) but still, the memory requirement is sometimes huge (due to large dtypes) compared to what a user usually extracts from our branches and the Python GC is going crazy.\r\n\r\nIf it's all in a single branch (single `AsDtype`), there's no way to read only part of it. If only a few parts are actually used by users, that's a problem with the file: the branch needs to be split. You'd have the same issue in ROOT. You can reduce the size of the lazily loaded chunks with `step_size`, which can in principle reduce your memory footprint, but if the problem is an over-active garbage collector, dividing the work into a larger number of smaller chunks would make the garbage collector busier.\r\n\r\nI always recommend `iterate` over `lazy` in situations where memory needs to be controlled: lazy arrays would approach problems like\r\n\r\n```python\r\narrays.branch1 + arrays.branch2 + arrays.branch3\r\n```\r\n\r\nby scanning through all chunks in `arrays.branch1` and `arrays.branch2` to produce `arrays.branch1 + arrays.branch2`, then scan through all chunks in that and `arrays.branch3` to produce the final sum. There's nowhere to insert the information that there's another `+` coming up and you'd really rather complete both sums in each chunk before moving on to the next chunk. The `iterate` method requires changes to user code:\r\n\r\n```python\r\nfor arrays in uproot.iterate(\"files*.root\", [\"branch1\", \"branch2\", \"branch3\"]):\r\n    arrays.branch1 + arrays.branch2 + arrays.branch3\r\n```\r\n\r\nbut those changes in user code are informative: now we know that everything inside the bounded loop should be completed before moving on to the next chunk. I've been looking at ways to use Dask to insert that information with lazy semantics (nothing happens until the user calls `.compute()`, which specifies the operations to be grouped per chunk in a different way), but I've been having some troubles with Dask's idea of what an array is (not jagged).\r\n\r\n> Also we usually deal with large amounts of branches and the overhead of the file opening might not be negligible.\r\n\r\nTo make a lazy array, we somehow have to know the number of entries in all files and names/types of branches to expect (in their intersection, at least). @nsmith- is considering an alternative to the `lazy` function that gets this information some other way, and thus does not have to preemptively open all the files. That solution would require some sort of database, though, and the goal of `lazy` is for convenience in exploring files, something that wouldn't be the case if you had to set up a database first.",
  "created_at":"2020-08-26T17:15:54Z",
  "id":681012652,
  "issue":82,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTAxMjY1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-26T17:15:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Alright, thanks for the explanation!\r\n\r\nRegarding large dtypes, I though that you maybe do something fancy inside awkward or the memory mapping of the files to skip fields, but I have not thought much about it. Surely, iteration makes more sense to keep the memory footprint low!\r\n\r\nSince we need to wrap `uproot` anyways in a class which creates a much more user-friend (and tab-completed) access to very long branch addresses, it's easy to maintain the old lazyarray functionality anyways, if still requested...",
  "created_at":"2020-08-27T06:56:28Z",
  "id":681640371,
  "issue":82,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTY0MDM3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T06:56:28Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"It's because the leaf values are interleaved in the TBasket buffer that gets compressed as a single unit. The whole thing has to be loaded to be decompressed. Then even selecting one field and letting the rest go out of scope still doesn't put the whole buffer out of scope because selecting a field is a view-slice in NumPy:, obtained by changing the strides, not by copying. So the original (all fields) becomes the `base` of the selected array.",
  "created_at":"2020-08-27T11:13:07Z",
  "id":681883514,
  "issue":82,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MTg4MzUxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-27T11:13:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This will be fixed in PR #86. To recognize a branch as having Double32_t type, we either have to parse its TLeaf's title (if it's a TLeafD32) or its streamer's title (as in your case). In your case, no streamer was attached, but we expected one because the branch is known to have Double32_t type and its leaf is not a TLeafD32.\r\n\r\nThe fact that no streamer was attached was a bigger issue, and I'm grateful that you caught it and reported it. In Uproot 3, the Python class hierarchy for user-defined types like RHyperTriton exactly mirrored the C++ class hierarchy, but that led to some issues that were better resolved by a class merely _containing_ instances of its superclass. (See [composition over inheritance](https://en.wikipedia.org/wiki/Composition_over_inheritance).) But that meant I was only looking in the RHyperTriton3O2 class for a field named `mppi_vert` when it was actually in the superclass, RHyperTriton. I had to extend the search to members of superclasses (which are now special members, rather than true Python superclasses) to find `mppi_vert`.\r\n\r\nI was going to say that explains why it found `m`, but `m` is in the superclass as well. Oh: `m` is not a Double32_t; it didn't need a streamer to figure out its interpretation. That's why it worked.\r\n\r\nThe PR's tests pass and I'm merging it now. If you need this to be deployed as a new version, let me know and I'll do it. Otherwise, it's available as the master branch.",
  "created_at":"2020-08-28T18:52:43Z",
  "id":683075573,
  "issue":85,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzA3NTU3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T18:52:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks a lot for the fix and the very accurate explanation! I don't need a new version for the fix urgently and I can easily use the master branch while waiting for the new release. Thanks again!",
  "created_at":"2020-08-28T19:15:10Z",
  "id":683096097,
  "issue":85,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4MzA5NjA5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-08-28T19:15:10Z",
  "user":"MDQ6VXNlcjQzNzQyMTk1"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, the quick answer to your question is that you can use `entry_start` and `entry_stop` parameters to limit which TBaskets are read by an `array` or `arrays` call. If you do this in little intervals through a dataset, it could be suboptimal because the TBasket boundaries might not line up with your chosen `entry_start`/`entry_stop` ranges and you would end up re-reading the TBasket that covers different entry intervals. (Note that there's `TBranch.entry_offsets` to find out where a branch's TBasket boundaries are and `HasBranches.common_entry_offsets` to find out where a set of branches have TBasket boundaries that line up, if at all.)\r\n\r\n(Uproot 4 doesn't have a `basket_cache` like Uproot 3: Uproot 3 had too many different types of caches that required the user to have deep understanding of how ROOT files work in order to use them effectively. Also, `iterate` solves this problem in a better way than a blind LRU cache.)\r\n\r\nA better way to iterate over intervals of entries is `TBranch.iterate` (for a single TTree) and `uproot4.iterate` (for a set of files). I think you know about this method and function; the advantage over calling `array` or `arrays` on intervals is that `iterate` knows it's sequential, so it knows to save the TBasket data from one iteration to use it in the next.\r\n\r\nLazy arrays are full of subtleties. As soon as I saw the issues they were causing, I regretted having introduced! However, they are popular, and the right thing to do is to figure out how to do them well, which will take some experience. While we work out these issues, I always recommend performance-minded users to use `array`, `arrays`, or `iterate` because at least you can control exactly when things get read. It requires more diligence to set the first argument properly (many users read all arrays, even when they plan to use only one or two), which is probably why `lazy` is so popular, but in ceding the responsibility of choosing what gets read to Uproot+Awkward, Uproot+Awkward will have to get better at not reading arrays prematurely.\r\n\r\nAs for the `lazy` Form bug you found, I've fixed it. It's because all `AsObjects` interpretations in Awkward Array are passed through [ak.from_iter](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_iter.html) and that function has only one integer type and one floating point type (like Python, which is appropriate because `ak.from_iter` is intended for Python data).\r\n\r\nThe performance issue with doubly jagged arrays is related: because of TTree serialization, more than one level of jaggedness has to be interpreted `AsObjects`, and so it needs to go through `ak.from_iter` (like Uproot 3). I've been setting up the infrastructure to write the `AsObjects` \u2192 Awkward interpretation in Awkward's C++ layer, like a faster version of [ak.ArrayBuilder](https://awkward-array.readthedocs.io/en/latest/_auto/ak.ArrayBuilder.html) because it knows its type at the outset. (That's the [ak.forms.Form](https://awkward-array.readthedocs.io/en/latest/ak.forms.Form.html) object.)\r\n\r\nHowever, that optimization likely won't happen until next year\u2014I'm setting things up to make it _possible_, but we have to do the `uproot4` \u2192 `uproot`, `uproot` \u2192 `uproot3` migration first. Also, before porting the `read` methods of all containers and models from Python to C++, I want them to fully stabilize in Python first. This includes understanding memberwise splitting (issue #38) because it comes up often enough. In assigning the `is_memberwise` attribute from `kStreamedMemberWise`, I found out that 100% of the `std::map` examples in our tests are memberwise\u2014we've never seen a non-memberwise `std::map`.\r\n\r\nSome more things you should know:\r\n\r\n   * If you're going to use `lazy` anyway, update to the latest Awkward, which has corrections from @nsmith- to avoid circular references between an array and its own cache. This PR #92 fixes some Uproot things related to that: the caches have to be explicitly attached to high-level Arrays because the deeply nested ArrayCache objects now only hold weak references, and fail when those weak references go out of scope.\r\n   * All of the public API modules, classes, functions, and methods have docstrings now.\r\n   * I'm going to deploy a version with PR #92 to make sure no one gets into trouble with weak references.",
  "created_at":"2020-09-08T16:25:49Z",
  "id":688991855,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4ODk5MTg1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-08T16:25:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks Jim for the detailed answer! Yes, I understand of course the decision regarding the caches and I think that the `uproot4` design is the right choice regarding caching and laziness. In our use-case the `TBranch.iterate` is the right choice and I also tried that (I forgot to add those examples) but I think I am either using them wrong, or there are some other issues which make it unusable in our case.\r\n\r\nGiven the file I uploaded (should still be online), the following shows that using `TBranch.iterate()` (assuming I am using it correctly) is painfully slow, it takes like 30 seconds per iteration although each entry is something like `~200 * ~10 * float64` and only retrieves 3 items (in the file there are 145000).\r\n\r\nSo in general, I am currently still struggling to find a way to somehow iteratively process these branches. Both `trks.fitinf` and `trks.rec_stages` are doubly jagged integer arrays and  are needed heavily in the analysis script I am currently trying to improve -- 4 hours per file, while I think it should be just a few minutes, given the theoretical I/O and processing times. This branch format of our ROOT file is really making me cry but it is what it is. Another problem, which I will post in a future issue that working with these arrays is far from the numpy performance (doing things like `arr > 0.5` takes ~2ms for 100 entries, while in numpy/Julia/C it should be more around a few hundred ns), but that's another story.\r\n\r\nHere is the example:\r\n\r\n\r\n```\r\nimport uproot4\r\nf = uproot4.open(\"doubly_jagged.root\")\r\n\r\ni = 0\r\nfor dir_z in f[\"E/Evt/trks\"].iterate(\"trks.fitinf\"):\r\n    print(i, dir_z)\r\n    i += 1\r\n    if i > 10:\r\n        break\r\n\r\n# 0 [{'trks.fitinf': [[0.00296, 0.002, -293, 139, 424, 242, 0.01, ... [], [], [], []]}]\r\n# 1 [{'trks.fitinf': [[0.00899, 0.00561, -188, 69, 14.4, 132, 4.88, ... [], [], [], []]}]\r\n# 2 [{'trks.fitinf': [[0.00701, 0.00372, -38.5, 33, 9.26, 44.7, ... [], [], []]}]\r\n\r\n# then it stops...\r\n\r\n# This one takes around 2 minutes but gives 10 entries (there are 145000 or so in this file)\r\nf[\"E/Evt/trks/trks.fitinf\"].array()[:10]\r\n# <Array [[[0.00296, 0.002, -293, ... [], []]] type='10 * var * var * float64'>\r\n```",
  "created_at":"2020-09-09T09:58:27Z",
  "id":689459702,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTQ1OTcwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T14:18:19Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Just to make sure this does not get lost, should I create a new issue or should we reopen this?\r\n\r\nAt least the fact that it stops after the third iteration (although there should be 145000) is a bug, which might be related to the extremely slow performance. Even if those are Python loops, I think it should be orders of magnitudes faster.\r\n\r\nI ran a line profiler with the following script:\r\n\r\n```python\r\nimport uproot4\r\n\r\nf = uproot4.open(\"doubly_jagged.root\")\r\n\r\nfor dir_z in f[\"E/Evt/trks\"].iterate(\"trks.fitinf\"):\r\n    print(dir_z)\r\n    # no need to break, it will stop after the third iteration\r\n```\r\n\r\nand here is the output of `kernprof -l -v ...` but it only revealed that 100% of the time is spent in `_ranges_or_baskets_to_arrays` and inside that function, 100% is spent on :\r\n\r\n```\r\nTotal time: 499.249 s\r\nFile: /home/tgal/Dev/uproot4/uproot4/behaviors/TBranch.py\r\nFunction: _ranges_or_baskets_to_arrays at line 2972\r\n...\r\n...\r\n  3086       463  498712540.0 1077132.9     99.9              interpretation_executor.submit(basket_to_array, basket)\r\n...\r\n...\r\n```\r\n\r\nwhich is spending 100% of the time in `basket_to_array` at line:\r\n\r\n```\r\n3062         6  490637778.0 81772963.0    100.0                  arrays[branch.cache_key] = interpretation.final_array(\r\n```",
  "created_at":"2020-09-10T10:59:35Z",
  "id":690156235,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDE1NjIzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T11:19:05Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"To classify it properly, this isn't a correctness bug: I was able to iterate through the whole file using a ridiculously small `step_size` (in a Zoom meeting, so my fans were blasting):\r\n\r\n```python\r\nfor x in tree.iterate(\"trks.fitinf\", step_size=100): print(x)\r\n```\r\n\r\n(The default is `tree.num_entries_for(\"100 MB\", \"trks.fitinf\")`, which is `63658`. That's 100 MB _read from disk_, which can translate into a lot of working memory with the current implementation because the current implementation has to construct Python objects, which are known to be memory hogs.)\r\n\r\nData of this type are known to have poor performance for understood reasons, and there's a plan in place to fix it (move the non-vectorized loops to C++). So it wouldn't add much to create an issue; I think there's already some roadmap somewhere saying that I need to do this next year.\r\n\r\n----------------------\r\n\r\nThe plan to fix it is a major project: I've already encoded the information about Uproot Interpretations into Awkward Forms, but now I've got to translate the Python Interpretation code (not entirely finalized yet!) into a machine in C++ that steps through the TBasket byte stream using the Form as a guide\u2014more likely a streamlined version of it because it will be runtime-interpreted for every entry. Technically, that's a virtual machine in the sense of the JVM or Python VM. (Very specific VMs can be fast, like the NumExpr VM, which is faster than precompiled NumPy. It should be highly specialized for speed, but that requires a thorough understanding of what it needs to do, including memberwise Interpretations.)\r\n\r\nBecause of your troubles, I was thinking of doing a quick, \"foot in the door\" implementation that handles only one type: `std::vector<std::vector<T>>` where `T` is a numeric type (bools, ints, and floats). That would force me to build the interface between Uproot and Awkward but not the full virtual machine. Since that's a very common type, it would be a good impact-to-effort ratio\u2014helping a lot of people with a relatively small investment.\r\n\r\nHowever, I see that what you actually have are not `std::vector<std::vector<T>>`, but\r\n\r\n```\r\ntrks.fitinf          | std::vector<double>* | AsObjects(AsArray(True, False, AsVector(False, dtype('>f8'))))\r\n```\r\n\r\nThat's odd enough that it wouldn't actually have a high impact. It would really just help your case and force me to set up the interface between Uproot and Awkward. I could do it for two very specific types:\r\n\r\n   * `std::vector<int32_t>*`, which is `AsObjects(AsArray(True, False, AsVector(False, dtype('>i4'))))` and applies to `trks.rec_stages` and `trks.hit_ids`\r\n   * `std::vector<double>*`, which is `AsObjects(AsArray(True, False, AsVector(False, dtype('>f8'))))` and applies to `trks.usr`, `trks.fitinf`, and `trks.error_matrix`.\r\n\r\nwhich wouldn't include any of the strings. The implementation on the Awkward side would check to see if a TBranch has exactly this type and run a canned algorithm to fill it\u2014no virtual machine. (In fact, the code that I write now would have to be _removed_ to create the virtual machine, so I can't invest much time into it.) If you have any other types (like 64-bit integers), let me know now because I'd have to explicitly include them.",
  "created_at":"2020-09-10T13:17:06Z",
  "id":690281082,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDI4MTA4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T13:17:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah, I should have tried much smaller step-sizes. I see indeed, it's working with that setting!\r\n\r\n> The plan to fix it is a major project: I've already encoded the information about Uproot Interpretations into Awkward Forms, but now I've got to translate the Python Interpretation code (not entirely finalized yet!) into a machine in C++ that steps through the TBasket byte stream using the Form as a guide\u2014more likely a streamlined version of it because it will be runtime-interpreted for every entry. Technically, that's a virtual machine in the sense of the JVM or Python VM.\r\n\r\nAbsolutely; that sounds like a massive undertaking!\r\n\r\nI feel a bit uncomfortable to steal your time -- also given that this code will be obsolete in future -- since you have a lot on the roadmap for `uproot4` already (and other things), but it would be certainly quite nice to see what's possible using your above-mentioned approach.\r\nRegarding other types, I could follow your patch and see if I can implement those if needed, but at this very moment these are the only problematic branch types we have.\r\n\r\nAnyways, many thanks in advance and If you find it takes too much time, just skip it and we can go with the small-step iteration or figure out some other way. Unfortunately this file format will likely not change in the upcoming years, so I guess we will have a few petabytes of them to process in the future :see_no_evil: ... ",
  "created_at":"2020-09-10T15:10:07Z",
  "id":690355211,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDM1NTIxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T15:10:07Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Maybe you can try harder to push the file format to change. Having non-splittable double-vectors is _probably_ giving worse performance and compression for classic ROOT analyses as well. Can you convince your collaborators to normalize to using two sets of singly-jagged arrays? See e.g. https://github.com/cms-nanoAOD/cmssw/issues/92#issuecomment-499284307",
  "created_at":"2020-09-14T16:08:32Z",
  "id":692157754,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjE1Nzc1NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-14T16:08:32Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Well, it will be a hard discussion but I will of course try. It will however not happen before next years and we have already quite a large amount of data stored this way, which is constantly analysed over and over.\r\n\r\nBut I appreciate your suggestions, I'd definitely go that way and try harder.",
  "created_at":"2020-09-14T16:21:45Z",
  "id":692165373,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjE2NTM3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T16:21:45Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"These data are already split\u2014the problem is that it's doubly jagged. See PR #96: it was the plan since [CHEP (page 26/29)](https://indico.cern.ch/event/773049/contributions/3473258) to pass non-vectorizable deserializations to Awkward's C++ layer\u2014this is the first example of that.\r\n\r\nI had been hoping that this doubly jagged array of numbers would be generalizable, but it's `std::vector<double>*`, not `std::vector<std::vector<double>>`, so it's probably not going to be usable by many. At the moment, [the implementation](https://github.com/scikit-hep/awkward-1.0/blob/master/src/libawkward/io/uproot.cpp) checks for that one type and handles it, but in the long run, it needs to be a translation of the Python (deserialization.py, containers.py, and models/*.py). Since Python is easier to modify and correct, I'd like to get a lot of experience with the Python before lowering it all to C++.",
  "created_at":"2020-09-14T17:31:19Z",
  "id":692202791,
  "issue":90,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjIwMjc5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T17:31:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The code is in\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/a5a2b1f7d8b50c7f0a85d53f271149b0e4317b67/uproot4/behaviors/TH2.py#L24-L53\r\n\r\nand the interface is still under discussion with @HDembinski and @henryiii. In fact, on the latter point, I don't know where we landed, but I'm willing to make this interface align with boost-histogram and hist, if those interfaces have settled (#50 and scikit-hep/uproot#502).\r\n\r\nSo @raymondEhlers, what you find in the linked code is basically a sketch: it can and should be changed to fit your expectations. The `to_numpy`, `to_boost`, and `to_hist` methods (and I've been reminded that we need a `to_pandas`, #91) all use the `edges`, `values`, and `values_errors` (which maybe needs to become `values_variances`). Some hidden helper functions are in the TH1.py file:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/a5a2b1f7d8b50c7f0a85d53f271149b0e4317b67/uproot4/behaviors/TH1.py#L16-L63\r\n\r\nI think I tested the 2D histogram by looking at a Gaussian blob with the same number of bins in X and Y, which would hide the kinds of transposition issues you're seeing (but it's a file I had available). If you see how to correct it with an application in mind, please do.",
  "created_at":"2020-09-09T17:48:43Z",
  "id":689718867,
  "issue":93,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY4OTcxODg2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-09T17:48:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the pointers and your detailed answer Jim!\r\n\r\nPerhaps my word choice was not great - I found the TH1 and TH2 behaviors and looked around the code, but was a bit confused as to how reading the hist correctly could lead to appearing to move the flow bins. However, looking more closely this morning, I realized that it's actually just a reshape with the wrong parameters. So it should be a straightforward change. I'll make a PR either later today or tomorrow.\r\n\r\nThanks!",
  "created_at":"2020-09-10T08:01:22Z",
  "id":690064506,
  "issue":93,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDA2NDUwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T08:01:22Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"@tamasgal The good news is that, with this PR and scikit-hep/awkward-1.0#448, the following code takes 3.6 seconds instead of 194 seconds (53\u00d7 faster):\r\n\r\n```python\r\nimport uproot4\r\nbranch = uproot4.open(\"issue-90.root:E/Evt/trks/trks.fitinf\")\r\nfor i in range(branch.num_baskets):\r\n    print(repr(branch.basket(i).array()))\r\n```\r\n\r\nIn the above, we're reading each TBasket individually. The bad news is that it still takes a long time to concatenate TBaskets because [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) has only been (internally) implemented for pairs: concatenating _n_ arrays means creating and throwing away _n - 2_ temporary arrays of quadratically increasing size. That was fine for examples of concatenating 2 or 3 arrays at a time, but when it comes to concatenating data from this file's 461 TBaskets, it's a problem.\r\n\r\nClearly, [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html) needs to be fixed anyway. There might already be an issue open about it. (It's been on my mind for a while...) Anyway, I'll tackle that next. We want the conclusion of the above story to be that the whole array is produced in 3.7 seconds!",
  "created_at":"2020-09-10T18:57:52Z",
  "id":690622622,
  "issue":96,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDYyMjYyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T18:57:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Awesome! Many thanks, Jim, that's really a huge leap.",
  "created_at":"2020-09-10T18:59:34Z",
  "id":690625052,
  "issue":96,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDYyNTA1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T18:59:34Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"For cases that aren't covered by the new interpret-by-Awkward mechanism, how about parallelizing the pure Python interpretation?\r\n\r\n   * **parallel interpretation in 'basket_array':** 259 seconds. 12 cores utilized at about 5-10% each. I'll blame the GIL. Reversing the order (putting the big TBaskets first) doesn't help\u2014it's not about stragglers.\r\n   * **sequential interpretation in 'basket_array', concatenate NumPy dtype=O arrays:** 190 seconds.\r\n   * **sequential interpretation in 'final_array' (old way):** 197 seconds. Moving the interpretation from `final_array` into `basket_array` doesn't hurt. Entries that would be trimmed (because they're at the ends of the first and last basket) are now unnecessarily interpreted, but that's probably better than not having the possibility to parallelize.\r\n   * **interpret-by-Awkward in 'basket_array':** 2.5 seconds.\r\n\r\nSo the baseline of 197 seconds wasn't unnecessarily harsh. It really is an 80\u00d7 speedup.\r\n",
  "created_at":"2020-09-12T13:44:35Z",
  "id":691490579,
  "issue":96,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTQ5MDU3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T13:44:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Making use of this will require Awkward 0.2.37 (but it won't _break_ with earlier versions), which is being deployed now. I'll do one last test after that deployment so that GHA pulls the new Awkward from PyPI.",
  "created_at":"2020-09-12T14:04:26Z",
  "id":691493982,
  "issue":96,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTQ5Mzk4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T14:04:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks, that's really nice and helps a lot!",
  "created_at":"2020-09-13T07:22:49Z",
  "id":691625975,
  "issue":96,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTYyNTk3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-13T07:22:49Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"When testing locally, I'm seeing that `test_numpy_2d` from `test_0046-histograms-bh-hist.py` fails (also as seen for the tests). However, when I create my test histogram in boost_histogram (based on my reproducer from #93) and compare it, my changes here agree with it. It also agrees with uproot3 (as expected - I crosschecked the code from there). So I'm not yet sure what's going on with that. It's certainly possible that I've missed something obvious - I'll look again tomorrow.",
  "created_at":"2020-09-10T19:08:33Z",
  "id":690638384,
  "issue":97,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDYzODM4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T19:12:42Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"The histogram bin values in the test were derived from a sample execution of the function, so when you fixed their order, it revealed the test was wrong. In this case, the test wasn't pinning the code to truth, but to an earlier state, so that we'd catch any unexpected changes.",
  "created_at":"2020-09-10T19:17:31Z",
  "id":690652095,
  "issue":97,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDY1MjA5NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-10T19:17:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah, okay, I was wondering if that was the case! (hence the PR now rather than further investigation at the moment) Thanks!",
  "created_at":"2020-09-10T19:19:46Z",
  "id":690654307,
  "issue":97,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDY1NDMwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T19:19:46Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Whenever you feel that it's done, let me know and I'll merge it. It looks good to me as it is.\r\n\r\n(I replaced `array.T` with `numpy.transpose(array)` because it makes the code easier to maintain. Other than that, I have no comments or requested changes.)",
  "created_at":"2020-09-10T19:26:32Z",
  "id":690660729,
  "issue":97,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDY2MDcyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T19:26:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think it's fine to merge now. There are probably other possible improvements here, but I tested a few variations with my reproducer and they seem to work okay, so it should be good enough for now. Thanks!",
  "created_at":"2020-09-10T20:18:32Z",
  "id":690701310,
  "issue":97,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDcwMTMxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T20:18:32Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thank you!",
  "created_at":"2020-09-10T20:20:40Z",
  "id":690703862,
  "issue":97,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MDcwMzg2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-10T20:20:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Amazing :)",
  "created_at":"2020-09-12T14:55:27Z",
  "id":691502296,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwMjI5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T14:55:27Z",
  "user":"MDQ6VXNlcjQwMDAwOTgx"
 },
 {
  "author_association":"MEMBER",
  "body":"What I'm amazed by is the fact that a file-like object can be used as a key of a dict (for the examples, not the implementation). Even in Python 2.7!",
  "created_at":"2020-09-12T14:59:10Z",
  "id":691502744,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwMjc0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T14:59:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"As long as it's hashable... right?",
  "created_at":"2020-09-12T14:59:37Z",
  "id":691502793,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwMjc5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T14:59:37Z",
  "user":"MDQ6VXNlcjQwMDAwOTgx"
 },
 {
  "author_association":"MEMBER",
  "body":"That's what I'm surprised by. A file-like object is by definition mutable, and usually mutable objects are not hashable because they could have a different meaning at a later time. However, the \"different meaning\" for a file-like object is a different position in the file, which ordinary people would call \"the same object,\" so it's sensible to make it hashable, hashing on the filename or something. Nevertheless, they _did_ it. Score +1 for Python!",
  "created_at":"2020-09-12T15:01:20Z",
  "id":691503023,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwMzAyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T15:01:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Awesome, working great!!! Much more streamlined and less writing to disk.",
  "created_at":"2020-09-12T15:12:42Z",
  "id":691504312,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwNDMxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T15:12:42Z",
  "user":"MDQ6VXNlcjQwMDAwOTgx"
 },
 {
  "author_association":"MEMBER",
  "body":"I'm glad it works for you!\r\n\r\nThere's a caveat written in the documentation that such objects can't be read in parallel because of the mutability thing. So only one reader-thread.",
  "created_at":"2020-09-12T15:18:42Z",
  "id":691505032,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwNTAzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T15:18:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ok, I was having some difficulty opening _different_ files in parallel. My workers would just hang.",
  "created_at":"2020-09-12T19:22:17Z",
  "id":691534415,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUzNDQxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-12T19:22:17Z",
  "user":"MDQ6VXNlcjQwMDAwOTgx"
 },
 {
  "author_association":"MEMBER",
  "body":"The thread-locality limits one thread to access one object\u2014if the object passed to Uproot is independent of any others, then Uproot will handle it correctly, though slowly. Since we assume that the given object has internal state, we spawn exactly one thread for it: a thread can't seek the file to the wrong place while another thread is using it because there is only one thread.\r\n\r\nHowever, if the object is not independent, then the behavior is undefined. If, for instance, the ExFileObject mutates the state of the TarFile and different threads are accessing different ExFileObjects from the same TarFile, they can mess each other up in unforeseeable ways. I don't know how Python's tarfile module is implemented. Its documentation says nothing about thread-safety, which is a crucial piece of information if you're going to be parallelizing access.\r\n\r\nI doubt there would be appreciable performance loss if each ExFileObject is accessed through a different TarFile object. Doing so should guarantee that each has a distinct file pointer and can safely be used in threads. Each TarFile object would have to re-read the same tarball header/info metadata, but I doubt this would make a significant impact, given that it only stores the name, size, permissions, etc. of the files, right?",
  "created_at":"2020-09-13T14:21:34Z",
  "id":691677717,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTY3NzcxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-13T14:21:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ah that makes sense! I will try using a different `TarFile` object for each process.",
  "created_at":"2020-09-13T19:11:01Z",
  "id":691711713,
  "issue":98,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MTcxMTcxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-13T19:11:01Z",
  "user":"MDQ6VXNlcjQwMDAwOTgx"
 },
 {
  "author_association":"MEMBER",
  "body":"Could it be the overflow bin? In Uproot 3, there's a distinction between \"values\" (no overflow bins) and \"allvalues\" (with overflow bins), and in Uproot 4, they just always have overflow bins.\r\n\r\nJust last week, Raymond Ehlers did a detailed study of the bin alignments in PR scikit-hep/uproot4#97.",
  "created_at":"2020-09-14T13:49:00Z",
  "id":692065027,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjA2NTAyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T13:49:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"To be sure it is not realted with the over- and underflow bins I tried to import the following image:\r\n\r\n![image](https://user-images.githubusercontent.com/11757284/93094999-deb8a880-f6a2-11ea-9759-d822e8ff2830.png)\r\n\r\n`myTH2histo.pandas()`\r\n\r\n```\r\nMultiIndex([([-inf, -1.5),                               [-inf, -1.5)),\r\n            ([-inf, -1.5),                [-1.5, -1.4863636363636363)),\r\n            ([-inf, -1.5), [-1.4863636363636363, -1.4727272727272727)),\r\n            ([-inf, -1.5),  [-1.4727272727272727, -1.459090909090909)),\r\n            ([-inf, -1.5),  [-1.459090909090909, -1.4454545454545455)),\r\n            ([-inf, -1.5), [-1.4454545454545455, -1.4318181818181819)),\r\n            ([-inf, -1.5), [-1.4318181818181819, -1.4181818181818182)),\r\n            ([-inf, -1.5), [-1.4181818181818182, -1.4045454545454545)),\r\n            ([-inf, -1.5), [-1.4045454545454545, -1.3909090909090909)),\r\n            ([-inf, -1.5), [-1.3909090909090909, -1.3772727272727272)),\r\n            ...\r\n            (  [1.5, inf),    [1.377272727272727, 1.3909090909090907)),\r\n            (  [1.5, inf),   [1.3909090909090907, 1.4045454545454543)),\r\n            (  [1.5, inf),    [1.4045454545454543, 1.418181818181818)),\r\n            (  [1.5, inf),    [1.418181818181818, 1.4318181818181817)),\r\n            (  [1.5, inf),   [1.4318181818181817, 1.4454545454545453)),\r\n            (  [1.5, inf),    [1.4454545454545453, 1.459090909090909)),\r\n            (  [1.5, inf),    [1.459090909090909, 1.4727272727272727)),\r\n            (  [1.5, inf),   [1.4727272727272727, 1.4863636363636363)),\r\n            (  [1.5, inf),                  [1.4863636363636363, 1.5)),\r\n            (  [1.5, inf),                                 [1.5, inf))],\r\n           names=['p_x [a.u.]', 'p_y [a.u.]'], length=49284)\r\n```\r\n\r\nbut here is the issue:\r\n\r\n```\r\npinumpy = myTH2Dhisto.numpy() \r\nz = pinumpy[0] # ndarray\r\nx = pinumpy[1][0][0] # ndarray\r\ny = pinumpy[1][0][1] # ndarray\r\nprint(z.shape, x.shape, y.shape)\r\n\r\n(220, 220) (221,) (221,)\r\n```\r\n\r\nit is INCREASED by 1 for both x and y compare to z. This makes it impossible to use directly some plotting libraries.",
  "created_at":"2020-09-14T14:12:36Z",
  "id":692079635,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjA3OTYzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T14:14:57Z",
  "user":"MDQ6VXNlcjExNzU3Mjg0"
 },
 {
  "author_association":"MEMBER",
  "body":"`pinumpy` is already a NumPy array, right? I'm having trouble seeing how you get the above: different elements of a single NumPy array can't have different shapes like that.\r\n\r\nCould you print out `type(pinumpy)` and `pinumpy.shape`?",
  "created_at":"2020-09-14T14:17:08Z",
  "id":692082571,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjA4MjU3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T14:17:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh! `pinumpy` is a tuple! Okay, I'm no longer confused about that.",
  "created_at":"2020-09-14T14:17:46Z",
  "id":692082969,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjA4Mjk2OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2020-09-14T14:17:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes, pinumpy is a `tuple` made of `ndarray` s. Have I stated the problem in the right way?",
  "created_at":"2020-09-14T14:19:26Z",
  "id":692084062,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjA4NDA2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T14:20:08Z",
  "user":"MDQ6VXNlcjExNzU3Mjg0"
 },
 {
  "author_association":"MEMBER",
  "body":"The format of `myTH2Dhisto.numpy()` is supposed to match the output of [np.histogram](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html), [np.histogram2d](https://numpy.org/doc/stable/reference/generated/numpy.histogram2d.html), and [np.histogramdd](https://numpy.org/doc/stable/reference/generated/numpy.histogramdd.html). There was some disagreement about whether 2d histograms should look like `np.histogram2d` or `np.histogramdd`; I don't remember where that landed.\r\n\r\nIn the NumPy examples, the array of edges is _supposed_ to be one longer than the number of bins because they represent the fenceposts surrounding bin content. Libraries that expect data from NumPy histograms should expect (require!) that.",
  "created_at":"2020-09-14T14:23:08Z",
  "id":692086249,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjA4NjI0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T14:23:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh\u2014you mean plotting libraries that don't expect the data to be histogram data, just x-y points to plot.\r\n\r\nIn that case, you want to use `(x[1:] - x[:-1])/2` and `(y[1:] - y[:-1])/2` instead of `x` and `y` because if you're plotting the histogram data as an x-y plot, you want to use the centers, not the edges, of the bins. The same would be true if you got the data from `np.histogram`, etc.",
  "created_at":"2020-09-14T14:27:18Z",
  "id":692092931,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjA5MjkzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T14:27:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If centering the edges is what you needed and it's working for you, just let me know so I can close the issue. Thanks!",
  "created_at":"2020-09-14T15:02:26Z",
  "id":692116444,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjExNjQ0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T15:02:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"The only plotting method directly working (i.e. having x,y,z as inputs) is `hep.hist2dplot(z,x,y)`.\r\n`plt.imshow()` works with` extent=[x[0],x[-1],y[0],y[-1]]` therefore specifying the x and y axis extent\r\n`plt.contourf(x,y,z)` requires the dimensions to match.\r\n\r\nThe problem is not to find the centre, it is to reduce the lenght of one value:\r\n\r\n`print(x)`\r\n\r\n```\r\n[-1.5        -1.48636364 -1.47272727 -1.45909091 -1.44545455 -1.43181818\r\n -1.41818182 -1.40454545 -1.39090909 -1.37727273 -1.36363636 -1.35\r\n -1.33636364 -1.32272727 -1.30909091 -1.29545455 -1.28181818 -1.26818182\r\n -1.25454545 -1.24090909 -1.22727273 -1.21363636 -1.2        -1.18636364\r\n -1.17272727 -1.15909091 -1.14545455 -1.13181818 -1.11818182 -1.10454545\r\n -1.09090909 -1.07727273 -1.06363636 -1.05       -1.03636364 -1.02272727\r\n -1.00909091 -0.99545455 -0.98181818 -0.96818182 -0.95454545 -0.94090909\r\n -0.92727273 -0.91363636 -0.9        -0.88636364 -0.87272727 -0.85909091\r\n -0.84545455 -0.83181818 -0.81818182 -0.80454545 -0.79090909 -0.77727273\r\n -0.76363636 -0.75       -0.73636364 -0.72272727 -0.70909091 -0.69545455\r\n -0.68181818 -0.66818182 -0.65454545 -0.64090909 -0.62727273 -0.61363636\r\n -0.6        -0.58636364 -0.57272727 -0.55909091 -0.54545455 -0.53181818\r\n -0.51818182 -0.50454545 -0.49090909 -0.47727273 -0.46363636 -0.45\r\n -0.43636364 -0.42272727 -0.40909091 -0.39545455 -0.38181818 -0.36818182\r\n -0.35454545 -0.34090909 -0.32727273 -0.31363636 -0.3        -0.28636364\r\n -0.27272727 -0.25909091 -0.24545455 -0.23181818 -0.21818182 -0.20454545\r\n -0.19090909 -0.17727273 -0.16363636 -0.15       -0.13636364 -0.12272727\r\n -0.10909091 -0.09545455 -0.08181818 -0.06818182 -0.05454545 -0.04090909\r\n -0.02727273 -0.01363636  0.          0.01363636  0.02727273  0.04090909\r\n  0.05454545  0.06818182  0.08181818  0.09545455  0.10909091  0.12272727\r\n  0.13636364  0.15        0.16363636  0.17727273  0.19090909  0.20454545\r\n  0.21818182  0.23181818  0.24545455  0.25909091  0.27272727  0.28636364\r\n  0.3         0.31363636  0.32727273  0.34090909  0.35454545  0.36818182\r\n  0.38181818  0.39545455  0.40909091  0.42272727  0.43636364  0.45\r\n  0.46363636  0.47727273  0.49090909  0.50454545  0.51818182  0.53181818\r\n  0.54545455  0.55909091  0.57272727  0.58636364  0.6         0.61363636\r\n  0.62727273  0.64090909  0.65454545  0.66818182  0.68181818  0.69545455\r\n  0.70909091  0.72272727  0.73636364  0.75        0.76363636  0.77727273\r\n  0.79090909  0.80454545  0.81818182  0.83181818  0.84545455  0.85909091\r\n  0.87272727  0.88636364  0.9         0.91363636  0.92727273  0.94090909\r\n  0.95454545  0.96818182  0.98181818  0.99545455  1.00909091  1.02272727\r\n  1.03636364  1.05        1.06363636  1.07727273  1.09090909  1.10454545\r\n  1.11818182  1.13181818  1.14545455  1.15909091  1.17272727  1.18636364\r\n  1.2         1.21363636  1.22727273  1.24090909  1.25454545  1.26818182\r\n  1.28181818  1.29545455  1.30909091  1.32272727  1.33636364  1.35\r\n  1.36363636  1.37727273  1.39090909  1.40454545  1.41818182  1.43181818\r\n  1.44545455  1.45909091  1.47272727  1.48636364  1.5       ]\r\n```\r\n\r\nand\r\n`\r\nprint(x.shape)`\r\n\r\n`(221,)`\r\n\r\nI need to do a for loop and use the centres of the bins, not the edge values (sadly) and this returns an array of lenght 220. I will update the stackoverflow thread too.\r\n",
  "created_at":"2020-09-14T17:50:44Z",
  "id":692213224,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjIxMzIyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-14T17:53:02Z",
  "user":"MDQ6VXNlcjExNzU3Mjg0"
 },
 {
  "author_association":"MEMBER",
  "body":"I mean if `x` has length `221`, `(x[1:] + x[:-1])/2` will have length `220`. The expression is implicitly a loop (and much faster than a for loop, though that may be irrelevant here).\r\n\r\nI didn't see a StackOverflow link, but that's a good place to start. The issue you're having is with NumPy, how to use slicing to get an array of the right dimensions. The bin edges have a length one greater than the bin contents because of the Fencepost Principle, and that's because it's qualitatively different than the thing you want (edges rather than centers). NumPy's syntax lets you express implicit loops simply, so that you can get from edges to centers in a tight expression like `(x[1:] + x[:-1])/2`. (The `x[1:]` drops the first edge, the `x[:-1]` drops the last edge, `x[1:] + x[:-1]` adds the remaining ones element by element (the implicit loop), and dividing by two gives the mean of each.)\r\n\r\nBy writing `(x[1:] + x[:-1])/2` out in words, I realized that I had meant a `+`, not a `-`. Sorry!",
  "created_at":"2020-09-14T18:11:10Z",
  "id":692224133,
  "issue":100,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MjIyNDEzMw==",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2020-09-14T18:11:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! I just went in and fixed this as part of PR #101.",
  "created_at":"2020-09-16T17:04:55Z",
  "id":693538665,
  "issue":102,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5MzUzODY2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-16T17:04:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hrmm @nsmith- and the Coffea team saw this in CoffeaTeam/coffea#115 but I thought this was a uproot3 specific issue, rather than a uproot4 issue.. I guess not.",
  "created_at":"2020-09-17T02:08:10Z",
  "id":693764138,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzc2NDEzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T02:25:19Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"There does seem to be a nicer API for this, using `file_handler` from https://uproot4.readthedocs.io/en/latest/uproot4.reading.open.html#uproot4.reading.open however:\r\n\r\n```\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.source.file.FileResource)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/gpfs/slac/atlas/fs1/u/gstark/collinearw/py3/lib/python3.6/site-packages/uproot4/reading.py\", line 142, in open\r\n    **options  # NOTE: a comma after **options breaks Python 2\r\n  File \"/gpfs/slac/atlas/fs1/u/gstark/collinearw/py3/lib/python3.6/site-packages/uproot4/reading.py\", line 537, in __init__\r\n    file_path, **self._options  # NOTE: a comma after **options breaks Python 2\r\nTypeError: __init__() got an unexpected keyword argument 'file_handler'\r\n```\r\n\r\nthis crashes because `uproot.source.chunk.Source` is inherited from `object`\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/117037a62a2ea8e6bbc5250326974df57a2f7190/uproot4/reading.py#L536-L538\r\n\r\nwhich doesn't allow keyword arguments\r\n\r\n```\r\n>>> uproot.source.chunk.Source(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.source.file.FileResource)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: object() takes no parameters\r\n```",
  "created_at":"2020-09-17T02:27:28Z",
  "id":693769713,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzc2OTcxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T02:31:14Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"> Hrmm @nsmith- and the Coffea team saw this in [CoffeaTeam/coffea#115](https://github.com/CoffeaTeam/coffea/issues/115) but I thought this was a uproot3 specific issue, rather than a uproot4 issue.. I guess not.\r\n\r\nThere is a similarity in that both memory per process and number of threads per process are limited resources that can be controlled by ulimit, so the reproducibility of this issue depends on whether your system's ulimit puts a cap on the number of threads.\r\n\r\nI'm taking Python at its word when it says\r\n\r\n```\r\nRuntimeError: can't start new thread\r\n```\r\n\r\nthat this is a number of threads issue. Since opening files spawns threads, that's entirely plausible.\r\n\r\nUproot 4 [Sources](https://uproot4.readthedocs.io/en/latest/uproot4.source.chunk.Source.html) handle parallelization internally.\r\n\r\n   * Some variants, like [MemmapSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.file.MemmapSource.html) (the default for local files), do not spawn any threads.\r\n   * Others, like [HTTPSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.http.HTTPSource.html) and [XRootDSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.xrootd.XRootDSource.html), spawn a single background thread because they try to do multi-part GETs or vector-reads, which bundle all TBasket-reads into a single request and therefore need a single thread to decode the response, but it must be a background thread because we can be decompressing and interpreting the first TBaskets while the others are still streaming in.\r\n   * The descendants of [MultithreadedSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.chunk.MultithreadedSource.html) ([MultithreadedFileSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.file.MultithreadedFileSource.html), [MultithreadedHTTPSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.http.MultithreadedHTTPSource.html), and [MultithreadedXRootDSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.xrootd.MultithreadedXRootDSource.html)) are fallbacks for when the default handlers can't be used, and they spawn `num_fallback_workers` (10) threads each with open file handles or remote connections attached to each thread.\r\n\r\nI believe that NFS doesn't support memory-mapping, and this would be a reason why it would fall back on [MultithreadedFileSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.file.MultithreadedFileSource.html). A way to check for this for sure would be\r\n\r\n```python\r\n>>> a.file.source.fallback\r\n```\r\n\r\nto see if the [MemmapSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.file.MemmapSource.html) (`a.file.source`) has a fallback or not. It's strange that Uproot 3 managed to open NFS files without complaining that it couldn't open the memory-map: I don't remember putting in fallback logic. (In Uproot 3, I think the equivalent of the above is `a._context.source`.)\r\n\r\nAnyway, the solution to the problem is to properly close the files. For these Uproot 4 [Sources](https://uproot4.readthedocs.io/en/latest/uproot4.source.chunk.Source.html), \"closing\" means closing all file handles and shutting down threads: the file handles and threads are glued to each other in a [ResourceThreadPoolExecutor](https://uproot4.readthedocs.io/en/latest/uproot4.source.futures.ResourceThreadPoolExecutor.html). The context-management semantics works at every level: if you do\r\n\r\n```python\r\n>>> with uproot4.open(\"/path/to/file.root:/path/to/tree\") as tree:\r\n...     do_something_with(tree)\r\n...\r\n>>> # the file and its threads are now gone\r\n```\r\n\r\nbecause file handles and threads are both limited resources that need to be fenced in user code to define their lifetimes, not reliant on the garbage collector, which only triggers when an unrelated and generally more plentiful resource (memory) runs out.\r\n\r\nAs for passing in an explicit `file_handler` (or `http_handler`, `xrootd_handler`, etc.), that seems to work:\r\n\r\n```python\r\n>>> f = uproot4.open(\"../uproot/tests/samples/simple.root\", file_handler=uproot4.MemmapSource)\r\n>>> f = uproot4.open(\"../uproot/tests/samples/simple.root\", file_handler=uproot4.MultithreadedFileSource)\r\n```\r\n\r\nYou were passing in a [Resource](https://uproot4.readthedocs.io/en/latest/uproot4.source.chunk.Resource.html) class instead of a [Source](https://uproot4.readthedocs.io/en/latest/uproot4.source.chunk.Source.html) class. Once again, the error message that Python generated (because the [Resource](https://uproot4.readthedocs.io/en/latest/uproot4.source.chunk.Resource.html) constructor doesn't take these arguments) doesn't say what the real problem is, which is that the user-supplied argument is of the wrong type. I'll have to add a type-guard to that. (I don't think I can go the route of MyPy-typing everything because so much of Uproot is dynamically generated, of necessity because we don't know what classes we'll find in each ROOT file.)",
  "created_at":"2020-09-17T13:10:50Z",
  "id":694221759,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDIyMTc1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T13:10:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and this `Source`:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/117037a62a2ea8e6bbc5250326974df57a2f7190/uproot4/reading.py#L533-L538\r\n\r\nis not the [Source](https://uproot4.readthedocs.io/en/latest/uproot4.source.chunk.Source.html) class:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/117037a62a2ea8e6bbc5250326974df57a2f7190/uproot4/source/chunk.py#L39-L48\r\n\r\nIt was failing because [FileResource](https://uproot4.readthedocs.io/en/latest/uproot4.source.file.FileResource.html) doesn't have those arguments.\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/117037a62a2ea8e6bbc5250326974df57a2f7190/uproot4/source/file.py#L26-L39\r\n\r\n-----------------\r\n\r\nThe give-away, if you knew my convention, is that I never use unqualified names in a codebase. I would never\r\n\r\n```python\r\nfrom uproot4.source.chunk import Source\r\n```\r\n\r\nat the top of a file like uproot4/reading.py, even if it used `Source` all over the place. I'd always write `uproot4.source.chunk.Source`, even though it makes the lines of code wide (and imposing a line width puts this under strain).\r\n\r\nThat's not a rule I've seen written down anywhere, but I gradually adopted it over the years because it's been incredibly useful to be able to trace any object back to its definition through its name. It's something I especially wish the Numba codebase did, as I've had to figure that out to write Numba extensions (with features beyond the documented examples).\r\n\r\nI should write that down as a rule in the CONTRIBUTING.md. I guess it's already a rule in [Awkward's CONTRIBUTING.md](https://github.com/scikit-hep/awkward-1.0/blob/master/CONTRIBUTING.md#fully-qualified-names).",
  "created_at":"2020-09-17T13:29:31Z",
  "id":694234404,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDIzNDQwNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-17T13:29:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> I believe that NFS doesn't support memory-mapping, and this would be a reason why it would fall back on [MultithreadedFileSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.file.MultithreadedFileSource.html). A way to check for this for sure would be\r\n> \r\n> ```python\r\n> >>> a.file.source.fallback\r\n> ```\r\n> \r\n> to see if the [MemmapSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.file.MemmapSource.html) (`a.file.source`) has a fallback or not. It's strange that Uproot 3 managed to open NFS files without complaining that it couldn't open the memory-map: I don't remember putting in fallback logic. (In Uproot 3, I think the equivalent of the above is `a._context.source`.)\r\n\r\nI'm not seeing a fallback here.\r\n\r\n```\r\n>>> import uproot4 as uproot\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\")\r\n>>> a.file.source.fallback\r\n>>> a.file.source\r\n<MemmapSource '...mc16a.root' at 0x7f8265715780>\r\n```\r\n\r\nTrying the fixed way of specifying the file_handler seems to work:\r\n\r\n```\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource)\r\n>>> a.file.source\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f825a1cdd68>\r\n```\r\n\r\nIs there a way to... open files without requiring threads explicitly? Or is this a change in uproot4 for the better(?)?",
  "created_at":"2020-09-17T14:05:22Z",
  "id":694259301,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDI1OTMwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T14:07:21Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"The MemmapSource (default) does not require threads. I'm a little confused as to why switching from MemmapSource, with no background threads, to MultithreadedFileSource, with 1 background thread (because `num_workers` is 1 by default) is _removing_ the number of threads limitation. The `num_fallback_workers` is by default 10, so if MemmapSource is falling back, then the 10 vs 1 would explain it...\r\n\r\nUproot 3 had a multithreaded physical layer as well. Uproot 4 gives you the new option of passing a file-like object, which uses [ObjectSource](https://uproot4.readthedocs.io/en/latest/uproot4.source.object.ObjectSource.html). That's one background thread, though. I think the MemmapSource is the only one that spawns zero threads.",
  "created_at":"2020-09-17T14:49:39Z",
  "id":694287386,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDI4NzM4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T14:49:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"7 threads and then crash.\r\n\r\n```\r\nPython 3.6.5 (default, Jun 15 2019, 23:43:55) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot4 as uproot\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\"); print(a.file.source)\r\n<MemmapSource '...mc16a.root' at 0x7f3670f1a780>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\"); print(a.file.source)\r\n<MemmapSource '...mc16a.root' with fallback at 0x7f3665723a58>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\"); print(a.file.source)\r\n<MemmapSource '...mc16a.root' with fallback at 0x7f366572beb8>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\"); print(a.file.source)\r\n<MemmapSource '...mc16a.root' with fallback at 0x7f3665746390>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\"); print(a.file.source)\r\n<MemmapSource '...mc16a.root' with fallback at 0x7f3665751828>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\"); print(a.file.source)\r\n<MemmapSource '...mc16a.root' with fallback at 0x7f366575bcc0>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\"); print(a.file.source)\r\n<MemmapSource '...mc16a.root' with fallback at 0x7f3664ce9198>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\"); print(a.file.source)\r\nTraceback (most recent call last):\r\n  File \"/gpfs/slac/atlas/fs1/u/gstark/collinearw/py3/lib/python3.6/site-packages/uproot4/source/file.py\", line 110, in __init__\r\n    self._file = numpy.memmap(self._file_path, dtype=self._dtype, mode=\"r\")\r\n  File \"/gpfs/slac/atlas/fs1/u/gstark/collinearw/py3/lib/python3.6/site-packages/numpy/core/memmap.py\", line 264, in __new__\r\n    mm = mmap.mmap(fid.fileno(), bytes, access=acc, offset=start)\r\nOSError: [Errno 12] Cannot allocate memory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/gpfs/slac/atlas/fs1/u/gstark/collinearw/py3/lib/python3.6/site-packages/uproot4/reading.py\", line 142, in open\r\n    **options  # NOTE: a comma after **options breaks Python 2\r\n  File \"/gpfs/slac/atlas/fs1/u/gstark/collinearw/py3/lib/python3.6/site-packages/uproot4/reading.py\", line 537, in __init__\r\n    file_path, **self._options  # NOTE: a comma after **options breaks Python 2\r\n  File \"/gpfs/slac/atlas/fs1/u/gstark/collinearw/py3/lib/python3.6/site-packages/uproot4/source/file.py\", line 117, in __init__\r\n    file_path, **opts  # NOTE: a comma after **opts breaks Python 2\r\n  File \"/gpfs/slac/atlas/fs1/u/gstark/collinearw/py3/lib/python3.6/site-packages/uproot4/source/file.py\", line 246, in __init__\r\n    [FileResource(file_path) for x in uproot4._util.range(num_workers)]\r\n  File \"/gpfs/slac/atlas/fs1/u/gstark/collinearw/py3/lib/python3.6/site-packages/uproot4/source/futures.py\", line 351, in __init__\r\n    worker.start()\r\n  File \"/cvmfs/sft.cern.ch/lcg/releases/Python/3.6.5-f74f0/x86_64-centos7-gcc8-opt/lib/python3.6/threading.py\", line 846, in start\r\n    _start_new_thread(self._bootstrap, ())\r\nRuntimeError: can't start new thread\r\n>>> exit()\r\n```\r\n\r\nversus MultiThreaded (no crash).\r\n\r\n```\r\nPython 3.6.5 (default, Jun 15 2019, 23:43:55) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot4 as uproot\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c0dfbe780>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027c7c18>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027c9898>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027d9518>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027e3198>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027e3dd8>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027eda58>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027ed390>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027e37b8>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027d9320>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027f1438>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027f1f98>\r\n>>> a = uproot.open(\"/nfs/slac/atlas/fs1/d/yuzhan/collinearw_files/June2020_Production/merged_files/Wj_AB212108_v2_mc16a.root\", file_handler=uproot.MultithreadedFileSource); print(a.file.source)\r\n<MultithreadedFileSource '...mc16a.root' (1 workers) at 0x7f2c027fbcf8>\r\n>>> exit()\r\n```\r\n\r\nThe conclusion, to me, seems that defaulting to MultiThreaded is better for us on the SLAC computers we're using.",
  "created_at":"2020-09-17T16:13:57Z",
  "id":694339535,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDMzOTUzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T16:13:57Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Aha! I missed earlier that the \"can't start new thread\" was a chained exception from \"cannot allocate memory\". In that case, it might not have anything to do with having a limited number of threads but the way memory-maps use memory. That can be OS dependent.\r\n\r\nAnyway, that's precisely why we have alternatives: the MultithreadedFileSource is precisely for cases where a MemoryMappedSource can't be used. (It was a late addition to old Uproot, in response to cases where memory-maps didn't work for some reason. I thought NFS was one of those reasons, but maybe that depends on NFS version.)\r\n\r\nSo if it works, use the MultithreadedFileSource. You might want to look at `num_workers` as an option to the `uproot4.open` function (and `uproot4.iterate`, etc.) because the issue really was about memory and maybe you can afford more threads. This directly affects parallelism on the physical layer (https://uproot4.readthedocs.io/en/latest/basic.html#parallel-processing), allowing multiple TBaskets to be in flight from disk to RAM while decompressing/interpreting the TBaskets that have already been loaded.\r\n\r\n(That's why I liked memory-maps in the first place: it's a rapid and stateless way to access bytes on disk, hiding disk latencies when parallelized with decompression/interpretation.)",
  "created_at":"2020-09-17T16:39:45Z",
  "id":694356474,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDM1NjQ3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T16:39:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> So if it works, use the MultithreadedFileSource. You might want to look at `num_workers` as an option to the `uproot4.open` function (and `uproot4.iterate`, etc.) because the issue really was about memory and maybe you can afford more threads. This directly affects parallelism on the physical layer (https://uproot4.readthedocs.io/en/latest/basic.html#parallel-processing), allowing multiple TBaskets to be in flight from disk to RAM while decompressing/interpreting the TBaskets that have already been loaded.\r\n\r\nI think this clarifies it, but also should probably be documented. For what it's worth, SLAC is a gpfs rather than a regular old nfs (and there's lots of peculiariaties with that anyway).",
  "created_at":"2020-09-17T21:00:01Z",
  "id":694497045,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQ5NzA0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T21:00:01Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, there's this in the Getting Started Guide: https://uproot4.readthedocs.io/en/latest/basic.html#parallel-processing\r\n\r\nAnd the `open` function talks about the parameters: https://uproot4.readthedocs.io/en/latest/uproot4.reading.open.html\r\n\r\nI'm hoping that all of these things are covered now.",
  "created_at":"2020-09-17T21:52:44Z",
  "id":694518763,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDUxODc2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T21:52:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think the original exception `cannot allocate memory` indicates it is the same issue that I ran into, as Giordon already identified. Each mmap counts against a ulimit, and if excessive ones are instantiated (and possibly held after use due to reference cycles) this may be the problem. You can watch your process's vsize in `htop` for example and see if it grows quickly.",
  "created_at":"2020-09-17T21:53:34Z",
  "id":694519133,
  "issue":103,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDUxOTEzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T21:53:34Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"The interface here will be made identical to the interface chosen by boost-histogram and hist. I'm waiting for @henryiii and @HDembinski to finalize a library-independent API for `edges`, `values`, (`errors` or `variances`), and `to_numpy`, and then I'll be putting exactly the same API into Uproot. Since this is a request for a feature in that API, I've mentioned them here so they're aware of it.",
  "created_at":"2020-09-17T13:42:02Z",
  "id":694243499,
  "issue":104,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDI0MzQ5OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-17T13:42:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Is this the relevant issue to keep track of https://github.com/scikit-hep/hist/issues/64 ?",
  "created_at":"2020-09-17T14:13:41Z",
  "id":694264862,
  "issue":104,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDI2NDg2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T14:13:41Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"> Is this the relevant issue to keep track of [scikit-hep/hist#64](https://github.com/scikit-hep/hist/issues/64) ?\r\n\r\nThis issue has been spread all over the place. It needs to be centralized, and ideally that would be in one of the two histogramming libraries. Maybe that would be a moot point by resolving it on a shorter timescale. (It sounded to me today like @henryiii was close.)",
  "created_at":"2020-09-17T14:39:33Z",
  "id":694281174,
  "issue":104,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDI4MTE3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T14:39:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"For example, #50 is another reference to this issue. So are scikit-hep/uproot#511 and scikit-hep/uproot#502.",
  "created_at":"2020-09-17T16:18:13Z",
  "id":694341949,
  "issue":104,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDM0MTk0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T16:18:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"All of these duplicates will all go into #167, where this interface will get figured out, once and for all.",
  "created_at":"2020-10-30T21:47:54Z",
  "id":719815116,
  "issue":104,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgxNTExNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T21:47:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"What happens if you\r\n\r\n```python\r\nimport gc\r\ngc.collect()\r\n```\r\n\r\nbefore or after the explicit `data` deletion? Any Python objects that are created will remain in the process's memory even if their reference counts are zero, until collected by the garbage collector. Fortunately, Python's `gc.collect()` does what you tell it to, allowing you to debug these things by explicitly invoking it. (Trying to debug Java's garbage collector is much harder because its equivalent command is a mere suggestion.)\r\n\r\nOne thing that Uproot 4 does differently than Uproot 3 is:\r\n\r\n   * Uproot 3 allocates a destination array and all interpreted TBaskets write into that array as they're read\r\n   * Uproot 4 collects a list of interpreted TBasket arrays and concatenates them when done.\r\n\r\nThe latter will use twice as much working memory, but it's less prone to error and more flexible (we can do more interpretation in parallel without knowing how long each variable-length structure will be before combining them; Uproot 3 had to delay interpretation of complex structures as [ObjectArrays](https://github.com/scikit-hep/awkward-array/blob/15a63d57659ef9e34baaf7654e54cd3fe5d0e5fc/awkward/array/objects.py#L33), which [notoriously limited functionality](https://github.com/scikit-hep/uproot/issues?q=is%3Aissue+objectarray)).\r\n\r\nI've just confirmed that these TBasket arrays are _not_ cached. They should go out of scope when the array is fully constructed. Specifically, the `basket_arrays` is a dict from basket number to the temporary basket array, and the `basket_arrays` is a value of the `branchid_arrays` dict, which maps from a TBranch identifier to the `basket_arrays` for that TBranch (for functions that fill multiple branches in one call). That `branchid_arrays` dict goes out of scope when this function finishes:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/28a867d4b2215069c6eeebebc46132ed11f8f8e7/uproot4/behaviors/TBranch.py#L3150-L3273\r\n\r\nThat's the only function that fills arrays; [TBranch.array](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TBranch.TBranch.html#array), [HasBranches.arrays](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TBranch.HasBranches.html), [uproot4.iterate](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TBranch.iterate.html), [uproot4.concatenate](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TBranch.concatenate.html), and [uproot4.lazy](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TBranch.lazy.html) all call into it.\r\n\r\nSo even though we (intentionally) use twice as much working memory now, those objects _should be_ out of scope and deleted on the next garbage collector pass, assuming that the garbage collector actually runs.",
  "created_at":"2020-09-17T16:08:46Z",
  "id":694336697,
  "issue":106,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDMzNjY5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T16:08:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim, thank you for the really fast and complete answer, I'm new to uproot I am still reading and understanding the code that you pointed.\r\n\r\nIn the mean time I tried to use the garbage collector as you asked and didn't see any change. \r\n\r\n```python\r\n>>> import uproot4\r\n>>> with uproot4.open('test.root') as file:\r\n...     data = file['tree/array'].array(library='np')\r\n...  \r\n>>> import gc    # first ps\r\n>>> gc.collect() # second ps\r\n0\r\n>>> del data     # third ps\r\n>>> gc.collect() # forth ps\r\n0\r\n```\r\n```bash\r\n\u276f ps -e -o command,pid,%mem | grep python  # after read the file\r\npython                        57623 10.6\r\n\u276f ps -e -o command,pid,%mem | grep python  # after the first gc.collect() call\r\npython                        57623 10.6\r\n\u276f ps -e -o command,pid,%mem | grep python # after the del data\r\npython                        57623  5.7\r\n\u276f ps -e -o command,pid,%mem | grep python # after the second gc.collect() call\r\npython                        57623  5.7\r\n```\r\nSo even after the deletion of the array and the garbage collection call the process seems to hold the TTree data in memory. That 5.7 % of memory is roughly the size of the TTree.\r\n",
  "created_at":"2020-09-17T18:29:58Z",
  "id":694419225,
  "issue":106,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQxOTIyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T18:29:58Z",
  "user":"MDQ6VXNlcjM5NjI3MzY2"
 },
 {
  "author_association":"MEMBER",
  "body":"PS: The garbage collector in CPython is usually set to run every line.",
  "created_at":"2020-09-17T18:31:43Z",
  "id":694420138,
  "issue":106,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQyMDEzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T18:31:43Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"As @jpivarski  said, the GC might mess up the memory measurements.\r\n\r\nAlternatively to the manual collection of `gc.collect()` you can set the garbage collector in \"aggressive mode\":\r\n\r\n```python\r\nimport gc\r\ngc.set_threshold((1, 1, 1))\r\n```\r\n\r\nor artificially decrease the available memory to see if the memory is still required.",
  "created_at":"2020-09-17T18:34:58Z",
  "id":694421897,
  "issue":106,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQyMTg5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T18:36:21Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"CPython doesn't collect all garbage on every line, and I was only suggesting `gc.collect()` as a debugging tool, not for production code.\r\n\r\nI'll look into this, to see if the memory is really leaking or just isn't being reclaimed.\r\n\r\nBy the way, although the basket arrays are not put in the cache, the final array is. Are you accounting for that?\r\n\r\nhttps://uproot4.readthedocs.io/en/latest/basic.html#caching-and-memory-management",
  "created_at":"2020-09-17T18:38:28Z",
  "id":694423756,
  "issue":106,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQyMzc1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T18:38:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> By the way, although the basket arrays are not put in the cache, the final array is. Are you accounting for that?\r\n\r\nNo, I wasnt aware of that. So the line `data = file['tree/array'].array(library='np')` fill the cache with the branch data and pass a reference of it to the `data` variable or I get a full copy of the cache, ending up with the two copies in memory?",
  "created_at":"2020-09-17T18:59:51Z",
  "id":694435458,
  "issue":106,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQzNTQ1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T18:59:51Z",
  "user":"MDQ6VXNlcjM5NjI3MzY2"
 },
 {
  "author_association":"MEMBER",
  "body":"Man, I keep meaning to get to this and I can't. Maybe an easy way to turn the cache off is\r\n\r\n```python\r\nuproot4.object_cache = None\r\nuproot4.array_cache = None\r\n```\r\n\r\nat the beginning of the program.",
  "created_at":"2020-09-17T19:01:21Z",
  "id":694436234,
  "issue":106,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQzNjIzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T19:01:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"One of the things that came out of this was that it forced me to think hard about the caching policy. Sharing a cache among files is not a good idea because you, the user, would expect that deleting the arrays you get from a file and deleting the file object would be sufficient to remove all trace of those arrays from memory. Not so if there's a shared cache, so I'm getting rid of that. Files still have individual caches for the reasons described in the above link, but when you get rid of a file, everything that had been stuck to it will now be gone.\r\n\r\nThe actual memory leak wasn't related to the caches, though. It was in the ThreadPool and ResourceThreadPool executors, which tie threads to the file handles they manage so that everything can be eliminated when a file's `close` or `__exit__` is called (leaves a `with` block).\r\n\r\nHere it is:\r\n\r\n```diff\r\n    def run(self):\r\n        \"\"\"\r\n        Listens to the :py:attr:`~uproot4.source.futures.ResourceWorker.work_queue`\r\n        and executes each :py:class:`~uproot4.source.futures.ResourceFuture` it\r\n        receives (with :py:attr:`~uproot4.source.futures.ResourceWorker.resource` as\r\n        its first argument) until it receives None.\r\n        \"\"\"\r\n+       future = None\r\n        while True:\r\n+           del future   # don't hang onto a reference while waiting for more work\r\n            future = self._work_queue.get()\r\n            if future is None:\r\n                break\r\n            assert isinstance(future, ResourceFuture)\r\n            future._run(self._resource)\r\n```\r\n\r\nWithout the new lines, the previous `future` is still in scope while the `self._work_queue.get()` waits for new work because the assignment can't happen until the function on the right-hand side finishes. The `future` contains a basket array from the previous task, and if you have enough threads, those baskets add up. They will be deleted as soon as new work comes in, so this is not a permanent memory leak (the memory usage would not grow without bound), but it is the cause of your surprising behavior.\r\n\r\nWith the fix, here's what happens:\r\n\r\n```python\r\n>>> import uproot4, gc\r\n>>> branch = uproot4.open(\"test.root:tree/array\", array_cache=None)\r\n>>> gc.collect()\r\n0\r\n>>> # RSS is at 35812\r\n>>> array = branch.array(library=\"np\")\r\n>>> # RSS is at 853356\r\n>>> gc.collect()\r\n57144\r\n>>> # RSS is at 452968\r\n>>> branch.close()\r\n>>> del branch\r\n>>> gc.collect()\r\n147\r\n>>> # RSS is at 448472\r\n>>> del array\r\n>>> gc.collect()\r\n0\r\n>>> # RSS is at 57588\r\n```\r\n\r\nImmediately after we read the array, there's 798 MiB of temporary data whose reference count is zero but the garbage collector hasn't gotten rid of it. I assume that's because of cyclic references, but I haven't investigated. Calling `gc.collect()` explicitly finds those objects and eliminates them, but in a memory-limited situation, the garbage collector would have been triggered automatically.\r\n\r\nClosing the file doesn't release much, 4.3 MiB of various things related to those threads and file handles; maybe also the memory-mapped file. (I don't know how that gets counted in the RSS.)\r\n\r\nBut when we delete the array itself (which goes away immediately; no cyclic references), the 381 MiB array goes away, and we're left with a baseline that's 21 MiB higher than when we started. It may be that some libraries were imported in the process. On another run, this was 11 MiB. If I step through this procedure several times in the same process, we come back to this new baseline (and bounce around by \u00b110 MiB). Since it bounces around, this is in the noise.",
  "created_at":"2020-09-17T21:47:49Z",
  "id":694516768,
  "issue":106,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDUxNjc2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-17T21:47:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks\u2014I'll fix it in about 15 minutes.",
  "created_at":"2020-09-18T12:11:30Z",
  "id":694831696,
  "issue":108,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDgzMTY5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T12:11:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nope\u2014you're faster than me! I'll approve your PR when the tests pass, and then deploy a new version.",
  "created_at":"2020-09-18T12:12:57Z",
  "id":694832296,
  "issue":108,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDgzMjI5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T12:12:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! I thought it was an easy enough change that I can just submit a PR directly, hope that's fine. :)",
  "created_at":"2020-09-18T12:16:17Z",
  "id":694833535,
  "issue":108,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDgzMzUzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T12:16:17Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"Recommendation (since I've had this happen to me **twice** now in boost-histogram): add a flake8 check for print statements in the source. https://scikit-hep.org/developer/style#flake8",
  "created_at":"2020-09-18T14:11:19Z",
  "id":694892048,
  "issue":108,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDg5MjA0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T14:11:19Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I was remembering your issue with that earlier and I was wondering if you were going to mention it.\r\n\r\n~I struggled through the instructions on that page because it didn't say that I need to install [flake8-print](https://pypi.org/project/flake8-print/).~ I've got that now and the PR is #110.\r\n\r\nAck! It _does_ say that I need flake8-print. How did I not see that? It was right in the spot where I was looking. Somehow, I was thinking that it was flake8-naming. That's the weirdest off-by-one error I've experienced. (flake8-naming was on the previous line: I scanned wrong.)\r\n\r\nFalse alarm: the documentation is fine. I can't read.",
  "created_at":"2020-09-18T14:56:24Z",
  "id":694917314,
  "issue":108,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDkxNzMxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T14:56:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's definitely fine, thanks!",
  "created_at":"2020-09-18T12:26:29Z",
  "id":694837736,
  "issue":109,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDgzNzczNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-18T12:26:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The `print` should be gone in [uproot 0.0.27](https://pypi.org/project/uproot4/).",
  "created_at":"2020-09-18T15:15:04Z",
  "id":694927383,
  "issue":109,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NDkyNzM4Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-18T15:15:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah, because the rules for YAML string escapes are complicated and I got burned by one of them. (I don't remember what it was, but there are things like `yes` meaning `true` unless it's quoted.) I wanted to make sure that everything was being interpreted as I thought it was because I was having troubles.\r\n\r\nMultiline YAML strings are a safer encoding\u2014only a deindent closes the quote.\r\n\r\nBut thanks for updating the setup! Do you foresee any other changes or can I merge it like this?",
  "created_at":"2020-09-19T02:30:45Z",
  "id":695152461,
  "issue":113,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTE1MjQ2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T02:30:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Not all your run's are multiline, though. Generally \": \" is the most common issue for an unquoted string. `uses`, branches, urls, etc are always safe. Totally up to you, though :)\r\n\r\nNope, as long as it passed (it did), it's safe to merge. If you wanted to move a little of the \r\n\r\n```bash\r\nconda config --add channels conda-forge\r\nconda update -n base -c defaults conda\r\n```\r\n\r\nup into the [action options](https://github.com/conda-incubator/setup-miniconda/blob/master/action.yml):\r\n\r\n```yaml\r\nchannels: conda-forge\r\n```\r\n\r\nThen you can just `conda list` / `conda info` to get some logging information, but you shouldn't need further configuration, just install lines. Conda is set to update itself whenever it installs packages.",
  "created_at":"2020-09-19T02:40:42Z",
  "id":695153314,
  "issue":113,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTE1MzMxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T02:40:42Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Applied the minor cleanup I mentioned above, seems to be fine.",
  "created_at":"2020-09-19T04:00:03Z",
  "id":695160101,
  "issue":113,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTE2MDEwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T04:00:03Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, thanks. I'll merge it now.",
  "created_at":"2020-09-19T11:10:23Z",
  "id":695200041,
  "issue":113,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5NTIwMDA0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-19T11:10:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know how `boost::histogram` objects are serialized in ROOT. What does the object you get look like? Does the class have \"Unknown\" in its name? (If so, that means it was skipped, not deserialized.)\r\n\r\nIf the object was deserialized and the data is somewhere in one of the [Model](https://uproot4.readthedocs.io/en/latest/uproot4.model.Model.html) attributes, then you just need to define a submodule in [uproot4.models](https://uproot4.readthedocs.io/en/latest/uproot4.models.html), just like [TH1](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TH1.TH1.html), etc.\r\n\r\nIf it was not deserialized (you have an \"Unknown\" model, a subclass of [UnknownClass](https://uproot4.readthedocs.io/en/latest/uproot4.model.UnknownClass.html)), then you'll need to add a custom model in [uproot4.models](https://uproot4.readthedocs.io/en/latest/uproot4.models.html).\r\n\r\nThe way it works is:\r\n\r\n   1. When we see that an object in a ROOT file has a certain C++ class, we look in `uproot4.classes` (a dict) or [ReadOnlyFile.custom_classes](https://uproot4.readthedocs.io/en/latest/uproot4.reading.ReadOnlyFile.html#custom-classes) to find a Model for it. If we find one directly, we call [Model.read](https://uproot4.readthedocs.io/en/latest/uproot4.model.Model.html#read), passing it a pointer to the raw data at that point in the file. The Model is responsible for deserializing the raw data into an object.\r\n   2. If there is no predefined Model, then we use the [ReadOnlyFile.streamers](https://uproot4.readthedocs.io/en/latest/uproot4.reading.ReadOnlyFile.html#streamers) to make such a class. The streamers have enough data to define the deserialization process.\r\n   3. If the C++ class has a behavior defined in [uproot4.behaviors](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.html) (a module), then the newly generated classes are subclasses of that behavior.\r\n\r\nSo for example, we have predefined Models for [TTree](https://uproot4.readthedocs.io/en/latest/uproot4.models.TTree.Model_TTree_v20.html), which is a subclass of the [TTree behavior](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TTree.TTree.html) by definition, so the TTree both knows how to deserialize itself and what user-facing methods it should have.\r\n\r\nFor another example, we don't have predefined Models for TH1*, so they're generated from the file's streamers. But we do have [behaviors for TH1](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TH1.TH1.html) with the right names in the right places, so when new model classes are created for TH1*, they get all the user-facing methods they're supposed to have.\r\n\r\n(To add just a little more complexity to this description, classes can have different versions. We only have predefined models for the most common TTree versions (from the last 5 years of ROOT versions). If we know about the class but not the version, we go to the file's streamers and generate a class with behaviors just like the case of not having any class information. In fact, we also need to fall back on this even for classes where we think we know the version because some files were generated with serialization patterns that don't agree with the standard definition of a given class version: most likely, they were made with custom compilations of ROOT. But it's the same mechanism: after failing with the first definition, we generate a new class from streamers and make it a subclass of the appropriate behaviors.)\r\n",
  "created_at":"2020-09-25T15:58:11Z",
  "id":699011078,
  "issue":115,
  "node_id":"MDEyOklzc3VlQ29tbWVudDY5OTAxMTA3OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-25T15:58:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This discussion moved to Gitter, but neither I nor @HDembinski could figure out how `boost::histogram` objects got saved to a ROOT file in the first place. If there's a standard serialization, then we can add Models to Uproot to read that serialization, but first we have to know what these objects are _supposed_ to look like.",
  "created_at":"2020-10-30T21:51:39Z",
  "id":719816358,
  "issue":115,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgxNjM1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T21:51:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a good idea. Since it's already in an exception, speed is not an issue (especially since the time it would take to sort a list of O(<1000) strings by similarity is not much compared to the other work that's going on).\r\n\r\nI looked around for hints on how to do this and found a variety of packages, but we don't want to _any_ new dependencies into Uproot, least of all for this task. (I'm imagining a dialogue: \"Why is it so hard to install Uproot in my weird DAQ environment?\" \"Because you need a fuzz package to sort key mismatches by similarity in KeyErrors.\") But I came across this recipe for manually calculating the Levenshtein similarity ratio:\r\n\r\nhttps://www.datacamp.com/community/tutorials/fuzzy-string-python\r\n\r\nIt's a 24 line function. That's not a significant maintenance burden for what it offers. There's a natural place for it in the `_util` module, which consists of private functions that we can always replace for something else later. Since the output is not programmatic, we don't have to worry that someone's going to depend on the exact order; if we want to change it later, that's still an option. (The exact character strings of an error message is not part of the API.) Also, I wouldn't implement it with a NumPy array as they have it; I'd use a Python list. There are no array-at-a-time function calls here; the NumPy array is nothing but an impediment that might complicate the use of Unicode.\r\n\r\nIf we were to go with this, I'd still show as many matches as fit on the screen (i.e. the current cut-off), but just sorted by similarity so that the one the user was looking for is unlikely to be after the cut-off.",
  "created_at":"2020-09-30T14:19:55Z",
  "id":701422221,
  "issue":119,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQyMjIyMQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-30T14:19:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yup, that's pretty much exactly the ask.",
  "created_at":"2020-09-30T14:46:28Z",
  "id":701438897,
  "issue":119,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQzODg5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T14:46:28Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"Just thinking it through and setting parameters (e.g. no new dependencies). I used conditional tense because I don't think I'll be doing it soon, but if it's still open someday when I'm looking for an hour-long project, I'll do it, or if someone wanted to take it up, they could. Actually, I'll label it as a \"good first project\" because it doesn't require deep knowledge of the machinery: it appears in only one place.\r\n\r\n(I do want to move the KeyInFileError (and all code, including `behavior_of`) out of the main `__init__` and just import them there, so that would interfere with this if they were being done at the same time. But that's not hard coordination.)",
  "created_at":"2020-09-30T14:51:12Z",
  "id":701441850,
  "issue":119,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQ0MTg1MA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-09-30T14:51:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Just going off the algorithm on wiki, this is pretty straightforward\r\n\r\n```\r\ndef damerau_levenshtein(a, b):\r\n    M = [[0]*(len(a)+1) for i in range(len(b)+1)]\r\n    \r\n    for i in range(len(a)):\r\n        M[i][0] = i\r\n    for j in range(len(b)):\r\n        M[0][j] = j\r\n        \r\n    for i in range(len(a)):\r\n        for j in range(len(b)):\r\n            if a[i] == b[j]:\r\n                cost = 0\r\n            else:\r\n                cost = 2\r\n            M[i][j] = min(M[i-1][j] + 1,\r\n                          M[i][j-1] + 1,\r\n                          M[i-1][j-1] + cost\r\n                             )\r\n            if i > 1 and j > 1 and a[i] == b[j-1] and a[i-1] == b[j]:\r\n                M[i][j] = min(M[i][j], M[i-2][j-2] + 1)\r\n        \r\n    return M[len(a)-1][len(b)-1]\r\n\r\nprint(damerau_levenshtein(\"Beer\", \"Love\"))\r\nprint(damerau_levenshtein(\"Beer\", \"Bear\"))\r\nprint(damerau_levenshtein(\"Beer\", \"Bee\"))\r\nprint(damerau_levenshtein(\"Beer\", \"Beers\"))\r\nprint(damerau_levenshtein(\"Beer\", \"Cola\"))\r\n\r\n>>> 3\r\n>>> 2\r\n>>> 1\r\n>>> 1\r\n>>> 4\r\n```\r\n\r\nAny concerns about putting this in a PR?",
  "created_at":"2020-10-19T12:55:27Z",
  "id":712136641,
  "issue":119,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjEzNjY0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T12:55:27Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"That looks good! As I said above, I think this should only be used to _sort_ the list of keys, but since the list gets truncated, sorting it puts the most likely candidates before the cut-off. (So the cut-off is still determined by screen space, not misspell-likelihood. There are other reasons for having the wrong key name than misspellings.)\r\n\r\nI also want to move KeyInFileError out of `uproot4/__init__.py` and into a named module. The `uproot4/__init__.py` file should only be for bringing classes and functions from submodules into a flat namespace that's easier for users to access. Perhaps as part of adding this, you could move the KeyInFileError to a new file called `uproot4/exceptions.py` and bring it into the flat namespace like everything else? That way, a future move doesn't cover up the attribution of your work.\r\n\r\nAlso, `damerau_levenshtein` belongs in `uproot4/_util.py`, since that's an implementation detail. We should leave the order of suggestions in this error message unspecified so that it's helpful, but not a backward-compatibility constraint on future versions.\r\n\r\nIf you want direct developer access to the repo, I can give you that, so that your PR is not across a fork.",
  "created_at":"2020-10-19T15:20:02Z",
  "id":712239796,
  "issue":119,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjIzOTc5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T15:20:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Just leaving a note for fixing this in the future; it's somewhere in here:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/7e42e2f71d9fb20dbfb12d177403b3b0839d0cc2/uproot4/behaviors/TBranch.py#L2707-L2786",
  "created_at":"2020-09-30T16:21:20Z",
  "id":701497138,
  "issue":120,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMTQ5NzEzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-09-30T16:21:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Adding my comment from Slack, for the record:\r\n\r\nThis is a missed case:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/d8a4b7203952bb7bd85cbf6d2ed481d610c83e67/uproot4/source/http.py#L163-L174\r\n\r\ndoesn't check for r`esponse.status == 302` (redirect) and use the connection to try again at `response.headers[\"location\"]`.\r\n\r\nSame for:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/d8a4b7203952bb7bd85cbf6d2ed481d610c83e67/uproot4/source/http.py#L244\r\n\r\nwhich only returns True or False based on whether the status is 206 or not. This should be an issue asking to support redirects.\r\n\r\nOn the second issue of not being able to access the redirected file: I wasn't able to the first time, then I was every subsequent time. I verify that I can read the data in it. I don't know if this was a one-time GitHub thing or if it always needs to \"warm up,\" or what.",
  "created_at":"2020-10-02T21:36:35Z",
  "id":702968977,
  "issue":121,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMjk2ODk3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-02T21:36:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"When you are at it, please don't forget the other redirect status codes:\r\n\r\nhttps://en.wikipedia.org/wiki/List_of_HTTP_status_codes#3xx_redirection",
  "created_at":"2020-10-27T08:53:12Z",
  "id":717087513,
  "issue":121,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNzA4NzUxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-27T08:53:12Z",
  "user":"MDQ6VXNlcjU0ODg0NDA="
 },
 {
  "author_association":"MEMBER",
  "body":"Copying from Slack again:\r\n\r\nThis \"probably\" is because I don't understand the \"max element size\" logic here; it was written by @chrisburr.\r\nHowever, you can test a non-vector read by setting `xrootd_handler=uproot4.MultithreadedXRootDSource`. This alternative XRootD source doesn't attempt a vector-read and makes separate XRootD connections in Python threads. The number of those Python threads is set by `num_workers` (by default, 1).\r\n\r\nHTTP sources do a fallback from multi-part GET to many threads each doing one GET; this would be the same thing for XRootD. I just wasn't sure if the \"max element size\" failure was reason enough to do the fallback and wanted to get real use-case experience first.",
  "created_at":"2020-10-02T21:46:37Z",
  "id":702971893,
  "issue":122,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMjk3MTg5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-02T21:46:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Another workaround would be using the http endpoint `https://eospublichttp.cern.ch/eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root` but there they don't have a host certificate that is trusted by default. Can I access the certificate verify option in the HTTP source somehow?",
  "created_at":"2020-10-02T21:56:05Z",
  "id":702974655,
  "issue":122,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMjk3NDY1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-02T21:56:05Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"That would be another requested feature. It would go into\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/7e42e2f71d9fb20dbfb12d177403b3b0839d0cc2/uproot4/source/http.py#L40\r\n\r\nas an argument. Specifically,\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/7e42e2f71d9fb20dbfb12d177403b3b0839d0cc2/uproot4/source/http.py#L52-L54\r\n\r\ntakes certificate as an argument: `cert_file` in https://docs.python.org/3/library/http.client.html#http.client.HTTPSConnection . (The use of positional arguments here had something to do with Python 2. That constructor now has a `*` in its argument list.)",
  "created_at":"2020-10-02T21:59:31Z",
  "id":702975760,
  "issue":122,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwMjk3NTc2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-02T22:02:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"#124 is chipping away at the assortment of issues this has raised. Try checking that out and seeing how far you get with it; post any other issues here. If it works, I'll merge it as-is.\r\n\r\n(They've all been little things, except AsGrouped, which I had thought was already implemented, but apparently not.)\r\n\r\nIf the PR doesn't get merged for a while, be sure to ping me because I'm likely to forget! Thanks!",
  "created_at":"2020-10-06T23:12:25Z",
  "id":704600894,
  "issue":123,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDYwMDg5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-06T23:12:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the quick fix! #124 seems to solve the issues on my second bullet point for reading the split branches from the MET association map. The errors are gone and  as far as i can see uproot now correctly reads the data from these branches.\r\n\r\nAbout the first bullet point with the double-jagged element links - i'm not sure if it is possible to split these (such that i would have instead of `vector<vector<ElementLink<something>>` two branches `vector<vector<unsigned int>` for the members `m_persIndex` and `m_persKey`). In case that does not work out - is there a way (possibly with custom deserialization code) to read these?",
  "created_at":"2020-10-07T08:15:49Z",
  "id":704773435,
  "issue":123,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDc3MzQzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T08:15:49Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"Right, I should look into why the ElementLink stranger send to be missing. Is that an ATLAS thing or a ROOT thing?\r\n\r\nMy first guess at a custom interpretation would be\r\n\r\nAsObjects(AsVector(AsVector(AsDtype([(\"persIndex\", \"u8\"), (\"persKey\", \"u8\")]))))\r\n\r\nbut that leaves undetermined whether one or both of those vectors have headers. (Need to guess or look at the raw bytes with `branch.debug(0)`.)",
  "created_at":"2020-10-07T11:46:09Z",
  "id":704880317,
  "issue":123,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDg4MDMxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T11:46:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"That is an ATLAS thing - i'm no expert on athena internals, but i guess the stuff is defined somewhere here https://gitlab.cern.ch/atlas/athena/-/tree/21.2/Control/AthLinks",
  "created_at":"2020-10-07T12:06:13Z",
  "id":704890087,
  "issue":123,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDg5MDA4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T12:06:13Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"So, a1f4c37220ae583a97ee60ffe8cdeff410ee774b fixes the problem with ElementLinks being \"unknown.\" If you'd believe it, it was because the name of the class sometimes has spaces before the closing `>` and sometimes doesn't (in the file).\r\n\r\n(The lookup for `\"ElementLink<DataVector<xAOD::TrackParticle_v1>>\"` was failing because it was named `\"ElementLink<DataVector<xAOD::TrackParticle_v1> >\"` or vice-versa. They're all forced to be `\"ElementLink<DataVector<xAOD::TrackParticle_v1>>\"` now.)\r\n\r\n```python\r\n>>> import uproot4\r\n>>> f = uproot4.open(\"issue-123a.root\")\r\n>>> t = f[\"CollectionTree\"]\r\n>>> arrays = t[\"AnalysisElectronsAuxDyn.trackParticleLinks\"].array(library=\"np\")\r\n>>> arrays[8]\r\n<STLVector [[<ElementLink<DataVector<xAOD::TrackParticle_v1>> (version 1) at 0x7f4eeeaf5cd0>, ...], ...]>\r\n>>> arrays[8][0][0]\r\n<ElementLink<DataVector<xAOD::TrackParticle_v1>> (version 1) at 0x7f4eeeaf5cd0>\r\n>>> arrays[8][0][0].all_members\r\n{'m_persKey': 776133387, 'm_persIndex': 0}\r\n>>> arrays[8][0][0].member(\"m_persKey\")\r\n776133387\r\n>>> arrays[8][0][0].member(\"m_persIndex\")\r\n0\r\n```",
  "created_at":"2020-10-07T14:03:00Z",
  "id":704957996,
  "issue":123,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDk1Nzk5Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-10-07T14:03:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks a lot! I think with these fixes i should be able to read everything that's needed - i'll continue annoying you when i find further issues.",
  "created_at":"2020-10-07T16:03:14Z",
  "id":705036024,
  "issue":123,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNTAzNjAyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-07T16:03:14Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Just for the record: We also have some branches where it seems they are affected by #38. E. g.\r\n\r\n```pycon\r\n>>> import uproot4\r\n>>>\r\n>>> f = uproot4.open(\"DAOD_PHYSLITE.art.pool.root\")\r\n>>> f[\"MetaData\"][\"EventFormat\"].array()\r\n[...]\r\n    xAOD::EventFormat_v1 version 9 as uproot4.dynamic.Model_xAOD_3a3a_EventFormat_5f_v1_v1 (46284 bytes)\r\nMembers for xAOD::EventFormat_v1: m_branchNames, m_classNames, m_parentNames, m_branchHashes\r\n\r\nattempting to get bytes 83840:84064\r\noutside expected range 0:83941 for this Chunk\r\nin file DAOD_PHYSLITE.art.pool.root\r\nin object /MetaData;2\r\n```\r\n\r\nUproot seems to get the members correctly, there should be 3 strings and then one integer hash. Seems they are also stored \"member wise\". The `.debug` output is a bit long, but if i do `.basket(0).data.tobytes()` i get first a long block of stuff that looks like the branch names, then a long block of stuff that looks like the class names, etc and in the end a long block of garbage which are probably the hashes.",
  "created_at":"2020-10-21T13:41:53Z",
  "id":713584946,
  "issue":123,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMzU4NDk0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-21T13:41:53Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the note; memberwise splitting is going to be important. (I think you're the 7th to report it.)\r\n\r\nThe `.basket(0).data.tobytes()` method of debugging is good, though you might find more information in `.debug(entryNum)` and `.debug_array(entryNum)`. One of the goals of Uproot 4 was to add more debugging tools because I think we'll always be encountering new types.",
  "created_at":"2020-10-21T14:30:02Z",
  "id":713619297,
  "issue":123,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMzYxOTI5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-21T14:30:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't think I know about this. Try printing out `ttree.all_members` or `ttree[\"some_branch\"].all_members`, maybe with `minimal_ttree_metadata=False` passed to `uproot4.open`. If you see what you're looking for, we can build convenience methods around it.",
  "created_at":"2020-10-06T20:41:29Z",
  "id":704540896,
  "issue":125,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNDU0MDg5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-06T20:41:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This has been quiet for a while, but I'm going through issues now.\r\n\r\nBy \"TTree indexes,\" do you mean any of the following?\r\n\r\n   * `fIndexValues`\r\n   * `fIndex`\r\n   * `fTreeIndex`\r\n\r\nCurrently, we're not reading these values because they come at the end of the structure and we're not using them. But the `minimal_ttree_metadata` parameter controls that:\r\n\r\n```python\r\n>>> import uproot4\r\n>>> import skhep_testdata\r\n>>> f = uproot4.open(skhep_testdata.data_path(\"uproot-Zmumu.root\"),\r\n...                  minimal_ttree_metadata=False)\r\n>>> f[\"events\"].member(\"fIndexValues\")\r\n<TArrayD [] at 0x7fc53d3e8df0>\r\n>>> f[\"events\"].member(\"fIndex\")\r\n<TArrayI [] at 0x7fc53d3e8e50>\r\n>>> f[\"events\"].member(\"fTreeIndex\")\r\nNone\r\n```\r\n\r\nIf these are useful, I'll consider them non-minimal. What would you do with these arrays? Can you show me an example?",
  "created_at":"2020-10-30T21:13:14Z",
  "id":719802287,
  "issue":125,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgwMjI4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T21:13:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know if you want to pursue this. By default, the `fIndexValues`, `fIndex`, and `fTreeIndex` quantities are not read; the `minimal_ttree_metadata=False` argument can turn them on.\r\n\r\nIf you know of a way that these quantities can be used, tell me and I'll (1) read them by default and (2) do whatever you have in mind.\r\n\r\nFor now, though, I'm going to close this issue because of the silence. (I use the open issues to look for work to do, so clutter is a problem. If you respond to an issue, even if it's closed, I do see it in my email inbox and will reopen the issue.)",
  "created_at":"2020-12-02T14:27:36Z",
  "id":737263418,
  "issue":125,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNzI2MzQxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-02T14:27:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As a temporary solution (which will last for the rest of this year, likely), complex objects are passed through ak.from_iter, which makes EmptyArray when there are no instances of data from which to derive the type (like `[[], [], []]`). EmptyArrays have unknown type.\r\n\r\nBut the type is known by the calling function. Actually, it's in the Form. After getting an array via ak.from_iter, it gets post-processed here:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/57fafcfd73c40aea21dd19a439c76c79fd370768/uproot4/interpretation/library.py#L326-L370\r\n\r\nAnd that post-processing should turn the EmptyArrays into relevant types, using information from the Form. I see that there aren't any cases catching EmptyArray, so this bug is due to missing code.\r\n\r\nIt could take me a few days to get to this.",
  "created_at":"2020-10-12T16:20:58Z",
  "id":707217915,
  "issue":126,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcwNzIxNzkxNQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-10-12T16:20:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know when you're done (no more changes expected), and if everything's in good shape, I'll merge it right away. Thanks!",
  "created_at":"2020-10-19T23:09:46Z",
  "id":712490399,
  "issue":127,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjQ5MDM5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T23:09:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@jpivarski should be it",
  "created_at":"2020-10-19T23:21:24Z",
  "id":712494265,
  "issue":127,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjQ5NDI2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T23:21:24Z",
  "user":"MDQ6VXNlcjEzMjI2NTAw"
 },
 {
  "author_association":"MEMBER",
  "body":"Since this has been implemented twice, once in Python and once in C++, maybe try to disable the C++ implementation to see if there's a difference? (Just for debugging, that is.) If it's the C++, the implementation is in here:\r\n\r\nhttps://github.com/scikit-hep/awkward-1.0/blob/master/src/libawkward/io/uproot.cpp\r\n\r\nYou can step through that with `std::cout`, [compiling Awkward Array with localbuild.py](https://github.com/scikit-hep/awkward-1.0/blob/master/CONTRIBUTING.md#building-and-teseting-locally).\r\n\r\n(This is why I want to get the deserialization 99% correct in Python first! Debugging in C++ is a pain!)",
  "created_at":"2020-10-19T23:30:10Z",
  "id":712497089,
  "issue":128,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxMjQ5NzA4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-19T23:30:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I fixed a bug in this part of the Awkward Array code in version 0.4.0. As part of a general clean-up, I noticed that it was emitting warnings on MacOS that pointed to a suspicious line. I fixed that and tried reading `trks.rec_stages`:\r\n\r\n```python\r\n>>> branch[\"trks.rec_stages\"].array(entry_stop=10)\r\n<Array [[[1, 2, 5, 3, 5, 4], ... 1], [1], [1]]] type='10 * var * var * int32'>\r\n```\r\n\r\nIt won't be easy for me to check the \"before\": if you upgrade Awkward, does it work for you now?",
  "created_at":"2020-10-30T18:14:06Z",
  "id":719716636,
  "issue":128,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcxNjYzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T18:14:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah thanks, I had no time to look into that until now but I will have a look now!",
  "created_at":"2020-10-30T21:14:33Z",
  "id":719802794,
  "issue":128,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgwMjc5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T21:14:33Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes, I can confirm that it works now, thank you, as always :)",
  "created_at":"2020-10-30T21:40:32Z",
  "id":719812519,
  "issue":128,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgxMjUxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T21:40:32Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"The colon syntax for a tree path and the way to select Pandas is:\r\n\r\n```python\r\ndf_generator = uproot4.iterate(\"data/*.root:my_TTree\", branches=branches, library=\"pd\")\r\n```\r\n\r\nAbsorbing the `treepath` into the file names is to allow different sets of files to have different tree paths: `[\"old-data/*.root:old_tree_path\", \"new-data/*.root:new_tree_path\"]` and if you have a colon in your file names, there's `{\"data/*.root\": \"my_TTree\"}` syntax. [All of this is documented.](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TBranch.iterate.html)\r\n\r\nUproot 3 has a `outputtype` to describe the type of output container\u2014dict, list, tuple, or Pandas DataFrame\u2014but there's a more important distinction: which array library are you using to describe the arrays? So `library=\"np\"` returns strict NumPy, even if inefficient for jagged arrays, `library=\"ak\"` (default) returns Awkward Arrays, and `library=\"pd\"` is for Pandas: Series or DataFrames depending on `array` vs `arrays` (in your case, it will be a DataFrame). There's also a `how` parameter for how you want that grouped: dict, list, tuple, or a merging strategy for Pandas if you have branches of different jaggedness. (`how` is passed straight into Pandas's merging `how`: it's a string like `\"inner\"` or `\"outer\"` for inner and outer joins.)\r\n\r\nGiven that your eventual goal is to concatenate everything, they'll have to fit into memory, but also, there's an [uproot4.concatenate](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TBranch.concatenate.html) function for that. Like iterate, it selects between Pandas and other libraries with a `library` parameter. I think it uses Pandas's own `concat` function to do the concatenation. If you've already found that that's too slow/memory hogging, maybe you want to pass `library=\"np\"` or `library=\"ak\"` to `uproot4.concatenate` and then convert the final result to Pandas with [ak.to_pandas](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_pandas.html) or (in NumPy's case) the Pandas DataFrame constructor.\r\n\r\nIf your data are flat (non-jagged, all of your selected branches are one-value-per-event, not `std::vector` or anything), then NumPy is the fastest thing around. Awkward Array and Pandas _use_ NumPy when they can and anything they need to implement separately can't match NumPy's decades of tuning. If your data are jagged, Awkward Array is your best bet, because it's the only one of the three that was designed for that. Pandas, generally speaking, is slow and memory-hogging. (See [its author's rant](https://wesmckinney.com/blog/apache-arrow-pandas-internals/).)",
  "created_at":"2020-10-23T16:08:20Z",
  "id":715434828,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNTQzNDgyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-23T16:08:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for the reply! I was actually really struggling to find the uproot4 docs as they're not in the git readme I just naively assumed they hadn't been done yet. \r\nI'll be taking this all into consideration. I think for now I'll stick with pandas as I need to be very flexible in which and how many branches I am reading in but if I reach a roadblock I now know I should probably be switching back to numpy. Thanks again",
  "created_at":"2020-10-26T17:46:56Z",
  "id":716716880,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNjcxNjg4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-26T17:46:56Z",
  "user":"MDQ6VXNlcjIwOTExOTg3"
 },
 {
  "author_association":"MEMBER",
  "body":"(I can't believe I forgot to put the full documentation into the GitHub README! Thanks!)",
  "created_at":"2020-10-26T17:50:11Z",
  "id":716718819,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxNjcxODgxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-26T17:50:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Fixed by #133.",
  "created_at":"2020-10-30T16:39:09Z",
  "id":719662064,
  "issue":129,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY2MjA2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T16:39:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It was because the `task` is used purely for its side-effect. (The `tasks` append to a `notifications` queue when they're done, so the return value is ignored.) This was fixed and accidentally pushed to master.\r\n\r\nAlso, NoFuture has been renamed TrivialFuture for consistency.",
  "created_at":"2020-10-30T14:20:01Z",
  "id":719580185,
  "issue":130,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTU4MDE4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T14:20:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't have authorization to access these files:\r\n\r\n```\r\n% xrdcp root://cmseos.fnal.gov//store/user/kdipetri/SUEP/Production_v0.2/2018/NTUP/Autumn18.QCD_HT1000to1500_TuneCP5_13TeV-madgraphMLM-pythia8_RA2AnalysisTree.root issue-131.root\r\n[0B/0B][100%][==================================================][0B/s]  \r\nRun: [FATAL] Auth failed:  (source)\r\n```",
  "created_at":"2020-10-29T17:38:29Z",
  "id":718911789,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODkxMTc4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T17:38:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ahh. Sorry about that. You probably need to have Kerberos initialized for FNAL. I am uploading them on CERNBox. I will post a link in a few minutes.",
  "created_at":"2020-10-29T17:42:46Z",
  "id":718914324,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODkxNDMyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T17:42:46Z",
  "user":"MDQ6VXNlcjczMDgxODM="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll only need one 9 GB and one 1 GB, so you can save space.",
  "created_at":"2020-10-29T17:50:04Z",
  "id":718918787,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODkxODc4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T17:50:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Alright, I think this should work: https://drive.google.com/drive/folders/1s_sX10H7pN2Xq1JK8QMuoS7i7gpSiOs2?usp=sharing",
  "created_at":"2020-10-29T17:58:11Z",
  "id":718923508,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODkyMzUwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T17:58:11Z",
  "user":"MDQ6VXNlcjczMDgxODM="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, I ran it on both and couldn't find a bug. Specifically, what I ran was a translation of your code to its columnar equivalent:\r\n\r\n```python\r\nimport numpy as np\r\nimport awkward1 as ak\r\nimport uproot4\r\n\r\n# f = uproot4.open(\"~/storage/data/uproot4-big/issue-131little.root\")\r\nf = uproot4.open(\"~/storage/data/uproot4-big/issue-131big.root\")\r\nt = f[\"TreeMaker2/PreSelection\"]\r\n\r\nevents = t.arrays([\r\n    \"HT\",\r\n    \"CrossSection\",\r\n    \"Tracks.fCoordinates.fX\",\r\n    \"Tracks.fCoordinates.fY\",\r\n    \"Tracks.fCoordinates.fZ\",\r\n    \"Tracks_fromPV0\",\r\n    \"Tracks_matchedToPFCandidate\",\r\n])  #, entry_start=99000)\r\n\r\ncut_events = events[events.HT > 1200]\r\nHT, CrossSection, x, y, z, num_fromPV0, matched_to_candidate = ak.unzip(cut_events)\r\n\r\npt = np.sqrt(x**2 + y**2 + z**2)\r\neta = np.arcsinh(z / np.sqrt(x**2 + y**2))\r\n\r\ntrack_cut = (pt > 1) & abs(eta < 2.5) & (num_fromPV0 >= 2) & matched_to_candidate\r\nmultiplicity = ak.sum(track_cut, axis=1)\r\n```\r\n\r\nwhich ought to be a significant boost in _efficiency_ (both speed and memory use), but it shouldn't cause or fix a bug. What you showed in StackOverflow had an actual error in deserialization. Perhaps something went wrong in the XRootD source?\r\n\r\n(I commented out the limit on `entry_start` and `entry_stop` because I could read all events into memory, for these 7 branches. It took about a minute: watching htop, a single Python thread popped up now and then while it was accessing disk, and then there was a burst of 10 threads while it decompressed. The math took almost no time after the I/O was done. Doing the `HT` cut early vs. late hardly mattered, either.)\r\n\r\n(Also, I assume that you wanted to cut on `abs(eta < 2.5)`, not `eta < 2.5`.)\r\n\r\nSince this ran without troubles from a local file, we should suspect the XRootD source. I no longer think it was a \"reading position\" issue (interpreting the wrong bytes as the TBasket header), because such a thing would affect a local source the same way. Maybe all the memory you were using confused PyXRootD? (If so, this would be a bug, and a subtle one, in PyXRootD's C++.) Or maybe Uproot's XRootDSource has a bug in it? If it's the former, then using the low-resource-usage code I wrote above would at least fix the symptom; if it's the latter, you'd get the same error either way, so try switching to the above with XRootD\u2014at least it would be a good diagnostic.",
  "created_at":"2020-10-29T18:59:06Z",
  "id":718956650,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODk1NjY1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T18:59:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, I should mention that this took 4.57 GB of memory; if you have less than that, you'll need to use `entry_start`/`entry_stop` to split it up. Actually, that's what [uproot4.iterate](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TBranch.iterate.html) is for: it repeatedly calls `arrays` with different `entry_start`/`entry_stop` values, though it's a little smarter than that in that it avoids re-reading TBaskets that are needed by more than one step in the iteration.",
  "created_at":"2020-10-29T19:09:52Z",
  "id":718962528,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODk2MjUyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T19:09:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"This real issue is that I don't think that I can go fully columnar. In fact, I need to recluster AK15 jets and to my knowledge the best approach so far is to use pyjet on an event loop. (Is there a way to pass jagged arrays with multiple events to pyjet or some equivalent?) My full code reproduces the error locally and looks like:\r\n\r\n```python\r\nimport uproot4 as uproot\r\nimport uproot_methods\r\nimport awkward1 as ak\r\nimport numpy as np\r\nfrom math import pi\r\nimport matplotlib.pyplot as plt\r\nimport mplhep as hep\r\nimport pyjet\r\nimport matplotlib.colors as colors\r\nimport matplotlib.cm as cmx\r\n\r\nplt.style.use(hep.style.ROOT)\r\n\r\ndef sphericityTensor(particles,r=2):\r\n    s = np.zeros((3,3))\r\n    s[0][0] = particles.x.dot(particles.x)\r\n    s[0][1] = particles.x.dot(particles.y)\r\n    s[0][2] = particles.x.dot(particles.z)\r\n    s[1][0] = particles.y.dot(particles.x)\r\n    s[1][1] = particles.y.dot(particles.y)\r\n    s[1][2] = particles.y.dot(particles.z)\r\n    s[2][0] = particles.z.dot(particles.x)\r\n    s[2][1] = particles.z.dot(particles.y)\r\n    s[2][2] = particles.z.dot(particles.z)\r\n    s = s/particles.p.dot(particles.p)\r\n    return s\r\n\r\ndef sphericity(s):\r\n    s_eigvalues, s_eigvectors = np.linalg.eig(s)\r\n    s_eigvalues = np.sort(s_eigvalues)\r\n    sphericity = 1.5*(s_eigvalues[0]+s_eigvalues[1])\r\n    return sphericity\r\n\r\ndef makeJets(tracks, R, p=-1):\r\n    # Cluster AK(R) jets\r\n    vectors = np.zeros(tracks.size, np.dtype([('pT', 'f8'), ('eta', 'f8'),\r\n                                              ('phi', 'f8'), ('mass', 'f8')]))\r\n    i = 0\r\n    for track in tracks:\r\n        vectors[i] = np.array((track.pt, track.eta, track.phi, track.mass),\r\n                              np.dtype([('pT', 'f8'), ('eta', 'f8'),\r\n                                        ('phi', 'f8'), ('mass', 'f8')]))\r\n        i += 1\r\n    sequence = pyjet.cluster(vectors, R=R, p=p)\r\n    jets = sequence.inclusive_jets()\r\n    return jets\r\n\r\ndef isrTagger(jets, warn=True, warnThresh=130):\r\n    mult0 = len(jets[0])\r\n    mult1 = len(jets[1])\r\n    if (mult0 > warnThresh) & (mult1 > warnThresh) & warn:\r\n        print(\"Warning: both multiplicities are above %d!\"%warnThresh)\r\n    elif (mult0 < warnThresh) & (mult1 < warnThresh) & warn:\r\n        print(\"Warning: both multiplicities are below %d!\"%warnThresh)\r\n    if mult0 < mult1:\r\n        return uproot_methods.TLorentzVectorArray.from_ptetaphim([jets[1].pt],\r\n                                                                 [jets[1].eta],\r\n                                                                 [jets[1].phi],\r\n                                                                 [jets[1].mass])\r\n    else:\r\n        return uproot_methods.TLorentzVectorArray.from_ptetaphim([jets[0].pt],\r\n                                                                 [jets[0].eta],\r\n                                                                 [jets[0].phi],\r\n                                                                 [jets[0].mass])\r\n\r\n# Get the file and import using uproot\r\nbase = 'root://cmseos.fnal.gov//store/user/kdipetri/SUEP/Production_v0.2/2018/NTUP/'\r\n#base = '/User/chrispap/QCD/'\r\ndatasets = {\r\n    base + 'Autumn18.QCD_HT1000to1500_TuneCP5_13TeV-madgraphMLM-pythia8_RA2AnalysisTree.root': 'TreeMaker2/PreSelection',\r\n    base + 'Autumn18.QCD_HT1500to2000_TuneCP5_13TeV-madgraphMLM-pythia8_RA2AnalysisTree.root': 'TreeMaker2/PreSelection',\r\n    base + 'Autumn18.QCD_HT2000toInf_TuneCP5_13TeV-madgraphMLM-pythia8_RA2AnalysisTree.root': 'TreeMaker2/PreSelection',\r\n}\r\n\r\nevents = uproot.lazy(datasets)\r\n\r\nevtShape = np.zeros(len(events['Tracks.fCoordinates.fX']))\r\nfor ievt in range(len(events['Tracks.fCoordinates.fX'])):\r\n    if ievt%1000 == 0:\r\n        print(\"Processing event %d. Progress: %.2f%%\"%(ievt,100*ievt/len(events['Tracks.fCoordinates.fX'])))\r\n    if events['HT'][ievt] < 1200:\r\n        evtShape[ievt] = -1\r\n        continue\r\n    tracks_x = events['Tracks.fCoordinates.fX'][ievt]\r\n    tracks_y = events['Tracks.fCoordinates.fY'][ievt]\r\n    tracks_z = events['Tracks.fCoordinates.fZ'][ievt]\r\n    tracks_E = np.sqrt(tracks_x**2+tracks_y**2+tracks_z**2+0.13957**2)\r\n    tracks = uproot_methods.TLorentzVectorArray.from_cartesian(ak.to_awkward0(tracks_x),\r\n                                                               ak.to_awkward0(tracks_y),\r\n                                                               ak.to_awkward0(tracks_z),\r\n                                                               ak.to_awkward0(tracks_E))\r\n    tracks_fromPV0 = events['Tracks_fromPV0'][ievt]\r\n    tracks_matchedToPFCandidate = events['Tracks_matchedToPFCandidate'][ievt]\r\n    tracks = tracks[(tracks.pt > 1.) & (abs(tracks.eta) < 2.5) &\r\n                    (ak.to_awkward0(tracks_fromPV0) >= 2) &\r\n                    (ak.to_awkward0(tracks_matchedToPFCandidate) > 0)]\r\n    jetsAK15 = makeJets(tracks, 1.5)\r\n    isrJet = isrTagger(jetsAK15, warn=False)\r\n    tracks_boosted_minusISR = tracks.boost(-isrJet.p3/isrJet.energy)\r\n    s = sphericityTensor(tracks_boosted_minusISR)\r\n    evtShape[ievt] = sphericity(s)\r\n\r\n# Plot results\r\nfig = plt.figure(figsize=(8,8))\r\nax = plt.gca()\r\n\r\nCrossSection = events['CrossSection'][events['HT'] > 1200]\r\nevtShape = evtShape[events['HT'] > 1200]\r\n\r\nax.hist(evtShape, bins=100, density=True, weights=CrossSection, histtype='step', color='b')\r\n\r\nplt.show()\r\n```",
  "created_at":"2020-10-29T19:18:00Z",
  "id":718966742,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODk2Njc0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T19:18:00Z",
  "user":"MDQ6VXNlcjczMDgxODM="
 },
 {
  "author_association":"NONE",
  "body":"Perhaps uproot4.iterate could solve this issue?",
  "created_at":"2020-10-29T19:24:00Z",
  "id":718969808,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODk2OTgwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T19:24:00Z",
  "user":"MDQ6VXNlcjczMDgxODM="
 },
 {
  "author_association":"MEMBER",
  "body":"As I said above, uproot4.iterate iterates over _chunks_ of events, so it doesn't change the for-loop vs. columnnarness. You can certainly use columnar techniques in some parts of your script, even if a restrictive interface prevents you from using it everywhere.\r\n\r\nFor a long time now, I've been wanting to give pyjet an interface that takes jagged arrays of Lorentz vectors, so that it wouldn't become a bottleneck by having to do Python for-loops over events. As it is, you'll need to do for loops in that one spot. Even this is much better than FastJet's own Python interface, which makes you do for loops over all the particles!",
  "created_at":"2020-10-29T19:50:49Z",
  "id":718983039,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxODk4MzAzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-29T19:50:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Is this issue still happening?\r\n\r\n```\r\nuproot4.deserialization.DeserializationError: while reading\r\n\r\n    TBasket version None as uproot4.models.TBasket.Model_TBasket (? bytes)\r\n        fNbytes: -1607368158\r\n        fObjlen: -1243566277\r\n        fDatime: 2634931141\r\n        fKeylen: -27664\r\n        fCycle: 21409\r\n\r\nattempting to get bytes 483015:483033\r\noutside expected range 510698:538758 for this Chunk\r\nin file /Users/chrispap/QCD/Autumn18.QCD_HT1500to2000_TuneCP5_13TeV-madgraphMLM-pythia8_RA2AnalysisTree.root\r\n```\r\n\r\nI can't test the XRootD access because of permissions, but we've narrowed it down to that: the bug does not occur with local files. If you can't reproduce it now; I'll close this issue.",
  "created_at":"2020-10-30T14:25:19Z",
  "id":719583256,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTU4MzI1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T14:25:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"The error message above has been produced with local files and the code that I posted when I created the issue. If you need the second large file I can upload it, too.",
  "created_at":"2020-10-30T15:36:28Z",
  "id":719625747,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTYyNTc0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T15:36:28Z",
  "user":"MDQ6VXNlcjczMDgxODM="
 },
 {
  "author_association":"NONE",
  "body":"You can see that the file is local in the last line of the error message, too.\r\n",
  "created_at":"2020-10-30T15:37:35Z",
  "id":719626442,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTYyNjQ0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T15:37:35Z",
  "user":"MDQ6VXNlcjczMDgxODM="
 },
 {
  "author_association":"MEMBER",
  "body":"Is the 9 GB file (`Autumn18.QCD_HT1000to1500_TuneCP5_13TeV-madgraphMLM-pythia8_RA2AnalysisTree.root`) not large enough to cause the error? Could you put a file that is large enough to cause the error on Google Drive?\r\n\r\n(This is good, because I'd much rather debug an Uproot issue than an XRootD one!)",
  "created_at":"2020-10-30T15:45:44Z",
  "id":719631591,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTYzMTU5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T15:45:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"In fact, I think that you need two of them since the issue happens exactly when the loop goes from ievt = 99999 to 100000. (i.e. when it goes to the next file). I will post a link a the second file in a few minutes.",
  "created_at":"2020-10-30T15:48:16Z",
  "id":719633046,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTYzMzA0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T15:48:16Z",
  "user":"MDQ6VXNlcjczMDgxODM="
 },
 {
  "author_association":"MEMBER",
  "body":"That fact will likely be important in the solution; thanks!",
  "created_at":"2020-10-30T15:55:41Z",
  "id":719637341,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTYzNzM0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T15:55:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Does it work if you try going from one file to itself, or from the small file to the large file, or from the large file to the small file? In any of those configurations, I would be able to reproduce it with the two files I have.",
  "created_at":"2020-10-30T15:56:24Z",
  "id":719637769,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTYzNzc2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T15:56:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Here's the link https://drive.google.com/drive/folders/1s_sX10H7pN2Xq1JK8QMuoS7i7gpSiOs2?usp=sharing\r\nThe other file is the QCD with HT1500to2000. I haven't tried these configurations. I will do it now.",
  "created_at":"2020-10-30T15:57:23Z",
  "id":719638307,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTYzODMwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T15:57:23Z",
  "user":"MDQ6VXNlcjczMDgxODM="
 },
 {
  "author_association":"NONE",
  "body":"So I tested a few different configurations and it turns out that the issue only occurs when trying to access the first entry of the HT1500to2000 file. Could it be a bad file? That's confusing though because I have run code on that file before and it worked.",
  "created_at":"2020-10-30T16:05:03Z",
  "id":719642555,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY0MjU1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T16:05:03Z",
  "user":"MDQ6VXNlcjczMDgxODM="
 },
 {
  "author_association":"MEMBER",
  "body":"It's entirely possible that it's a bad file. It's easy to make mistakes in bookkeeping: more than half the time, when I think I've found a test case that demonstrates an error in the main codebase, it's the test case that's wrong, not the main codebase. I'm downloading it and I'll test it soon. I'm going through a huge backlog of issues\u2014I might as well start with this one.",
  "created_at":"2020-10-30T16:31:04Z",
  "id":719657301,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY1NzMwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T16:31:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah, there's something wrong with that file. The `\"Tracks.fCoordinates.fX\"` and `\"Tracks.fCoordinates.fZ\"` branches have this junk in the first TBasket, and the `\"Tracks.fCoordinates.fY\"` has a \"count branch\" that points to `\"Tracks.fCoordinates.fX\"`. (It should be pointing to a branch of \"number of items per entry\", not \"x\" values.) I used `TBranch.debug` to search around nearby to see if the file pointer is just a little off, but I didn't see anything that looks like the beginning of a TBasket (which includes some easily recognizable strings, but I didn't see them).\r\n\r\nSo it's just that one file. Something went wrong while writing it. Also, your TBaskets are very small, like 7 entries each. Reading them would be more efficient if they were big enough that NumPy is doing more work than Python. For these 7 branches, loading 100 MB at a time (big, but can't possibly be a problem for the amount of RAM on your computer), you'd want 4000\u20127000 entries per basket:\r\n\r\n```python\r\n>>> branches = [\"HT\", \"CrossSection\", \"Tracks.fCoordinates.fX\", \"Tracks.fCoordinates.fY\", \"Tracks.fCoordinates.fZ\", \"Tracks_fromPV0\", \"Tracks_matchedToPFCandidate\"]\r\n>>> t = uproot4.open(\"~/storage/data/uproot4-big/issue-131-big1.root:TreeMaker2/PreSelection\")\r\n>>> t.num_entries_for(\"100 MB\", branches)\r\n7657\r\n>>> t = uproot4.open(\"~/storage/data/uproot4-big/issue-131-little.root:TreeMaker2/PreSelection\")\r\n>>> t.num_entries_for(\"100 MB\", branches)\r\n4689\r\n```\r\n\r\nFinally, for reference, you'll probably want a procedure like the following for gathering multiplicities (and anything else) from your set of files\u2014it's what I used in debugging:\r\n\r\n```python\r\nimport numpy as np\r\nimport awkward1 as ak\r\nimport uproot4\r\n\r\nmultiplicity_chunks = []\r\n\r\nfor events, report in uproot4.iterate(\r\n    [\r\n        \"~/storage/data/uproot4-big/issue-131-little.root:TreeMaker2/PreSelection\",\r\n        \"~/storage/data/uproot4-big/issue-131-big1.root:TreeMaker2/PreSelection\",\r\n    ], [\r\n        \"HT\",\r\n        \"CrossSection\",\r\n        \"Tracks.fCoordinates.fX\",\r\n        \"Tracks.fCoordinates.fY\",\r\n        \"Tracks.fCoordinates.fZ\",\r\n        \"Tracks_fromPV0\",\r\n        \"Tracks_matchedToPFCandidate\",\r\n    ], step_size=\"1 GB\", report=True\r\n):\r\n    print(report)\r\n\r\n    cut_events = events[events.HT > 1200]\r\n    HT, CrossSection, x, y, z, num_fromPV0, matched_to_candidate = ak.unzip(cut_events)\r\n\r\n    pt = np.sqrt(x**2 + y**2 + z**2)\r\n    eta = np.arcsinh(z / np.sqrt(x**2 + y**2))\r\n\r\n    track_cut = (pt > 1) & abs(eta < 2.5) & (num_fromPV0 >= 2) & matched_to_candidate\r\n    multiplicity_chunk = ak.sum(track_cut, axis=1)\r\n\r\n    multiplicity_chunks.append(multiplicity_chunk)\r\n\r\nmultiplicity = ak.concatenate(multiplicity_chunks)\r\nprint(multiplicity)\r\n```\r\n\r\nI'm going to close this because I think it's your file that's wrong. If you have any other problems, ping this issue or open a new one!",
  "created_at":"2020-10-30T20:48:24Z",
  "id":719791936,
  "issue":131,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTc5MTkzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T20:48:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @jpivarski for code, documentation, infrastructure, and maintainance.",
  "created_at":"2020-10-30T16:50:13Z",
  "id":719668436,
  "issue":134,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY2ODQzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T16:50:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/135) to add @jpivarski! :tada:",
  "created_at":"2020-10-30T16:50:22Z",
  "id":719668515,
  "issue":134,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY2ODUxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T16:50:22Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @jpivarski for maintenance. (Misspelled.)",
  "created_at":"2020-10-30T16:56:17Z",
  "id":719671977,
  "issue":134,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY3MTk3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T16:56:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/136) to add @jpivarski! :tada:",
  "created_at":"2020-10-30T16:56:24Z",
  "id":719672055,
  "issue":134,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY3MjA1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T16:56:24Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @reikdas for code and infrastructure.",
  "created_at":"2020-10-30T17:04:46Z",
  "id":719677032,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY3NzAzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:04:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/138) to add @reikdas! :tada:",
  "created_at":"2020-10-30T17:04:54Z",
  "id":719677117,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY3NzExNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:04:54Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"I hope this works!\r\n\r\n@all-contributors please add @chrisburr for code and infrastructure (58 commits: 737 ++ 439 --)\r\n\r\n@all-contributors please add @plexoos for code (32 commits: 2,407 ++ 1,219 --)\r\n\r\n@all-contributors please add @matthewfeickert for infrastructure (27 commits: 215 ++ 210 --)\r\n\r\n@all-contributors please add @tamasgal for code (18 commits: 626 ++ 393 --)\r\n\r\n@all-contributors please add @kreczko for code and test (9 commits: 85 ++ 34 --)\r\n\r\n@all-contributors please add @nsmith- for code (8 commits: 109 ++ 25 --)\r\n\r\n@all-contributors please add @nbiederbeck for code (7 commits: 91 ++ 27 --)\r\n\r\n@all-contributors please add @oshadura for code and infrastructure (7 commits: 52 ++ 17 --)\r\n\r\n@all-contributors please add @henryiii for code, infrastructure, and test (6 commits: 383 ++ 318 --)\r\n\r\n@all-contributors please add @masonproffitt for code and test (6 commits: 81 ++ 12 --)\r\n\r\n@all-contributors please add @guitargeek for code (4 commits: 88 ++ 105 --)\r\n\r\n@all-contributors please add @benkrikler for code (4 commits: 18 ++ 9 --)\r\n\r\n@all-contributors please add @HDembinski for doc (4 commits: 21 ++ 17 --)\r\n\r\n@all-contributors please add @riga for code (3 commits: 51 ++ 29 --)\r\n\r\n@all-contributors please add @wiso for code (1 commit: 2 ++ 1 --)\r\n\r\n@all-contributors please add @jrueb for code (1 commit: 7 ++ 23 --)\r\n\r\n@all-contributors please add @bfis for code (1 commit: 79 ++ 0 --)\r\n\r\n@all-contributors please add @raymondEhlers for code (1 commit: 865 ++ 863 --)\r\n\r\n@all-contributors please add @andrzejnovak for code (1 commit: 123 ++ 75 --)\r\n\r\n@all-contributors please add @bendavid for code (1 commit: 6 ++ 5 --)\r\n\r\n@all-contributors please add @douglasdavis for code (1 commit: 5 ++ 5 --)\r\n\r\n@all-contributors please add @asymmetry for code (1 commit: 6 ++ 1 --)\r\n\r\n@all-contributors please add @ast0815 for code (1 commit: 87 ++ 38 --)\r\n\r\n@all-contributors please add @HealthyPear for code (1 commit: 11 ++ 5 --)\r\n\r\n@all-contributors please add @EdoPro98 for code (1 commit: 36 ++ 10 --)\r\n\r\n@all-contributors please add @JMSchoeffmann for code (1 commit: 2 ++ 0 --)\r\n\r\n@all-contributors please add @alexander-held for code (1 commit: 0 ++ 2 --)\r\n",
  "created_at":"2020-10-30T17:28:51Z",
  "id":719690840,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5MDg0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:28:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI could not determine your intention.\n\nBasic usage: @all-contributors please add @someone for code, doc and infra\n\nFor other usages see the [documentation](https://allcontributors.org/docs/en/bot/usage)",
  "created_at":"2020-10-30T17:28:56Z",
  "id":719690876,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5MDg3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:28:56Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add chrisburr for code and infrastructure\r\n\r\n@all-contributors please add plexoos for code\r\n\r\n@all-contributors please add matthewfeickert for infrastructure\r\n\r\n@all-contributors please add tamasgal for code\r\n\r\n@all-contributors please add kreczko for code and test\r\n\r\n@all-contributors please add nsmith- for code\r\n\r\n@all-contributors please add nbiederbeck for code\r\n\r\n@all-contributors please add oshadura for code and infrastructure\r\n\r\n@all-contributors please add henryiii for code, infrastructure, and test\r\n\r\n@all-contributors please add masonproffitt for code and test\r\n\r\n@all-contributors please add guitargeek for code\r\n\r\n@all-contributors please add benkrikler for code\r\n\r\n@all-contributors please add HDembinski for doc\r\n\r\n@all-contributors please add riga for code\r\n\r\n@all-contributors please add wiso for code\r\n\r\n@all-contributors please add jrueb for code\r\n\r\n@all-contributors please add bfis for code\r\n\r\n@all-contributors please add raymondEhlers for code\r\n\r\n@all-contributors please add andrzejnovak for code\r\n\r\n@all-contributors please add bendavid for code\r\n\r\n@all-contributors please add douglasdavis for code\r\n\r\n@all-contributors please add asymmetry for code\r\n\r\n@all-contributors please add ast0815 for code\r\n\r\n@all-contributors please add HealthyPear for code\r\n\r\n@all-contributors please add EdoPro98 for code\r\n\r\n@all-contributors please add JMSchoeffmann for code\r\n\r\n@all-contributors please add alexander-held for code\r\n",
  "created_at":"2020-10-30T17:32:37Z",
  "id":719692983,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5Mjk4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:40:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI could not determine your intention.\n\nBasic usage: @all-contributors please add @someone for code, doc and infra\n\nFor other usages see the [documentation](https://allcontributors.org/docs/en/bot/usage)",
  "created_at":"2020-10-30T17:32:39Z",
  "id":719693005,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5MzAwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:32:39Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @chrisburr for code and infrastructure\r\n\r\n",
  "created_at":"2020-10-30T17:33:04Z",
  "id":719693446,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5MzQ0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:33:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/139) to add @chrisburr! :tada:",
  "created_at":"2020-10-30T17:33:12Z",
  "id":719693518,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5MzUxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:33:12Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @plexoos for code\r\n\r\n",
  "created_at":"2020-10-30T17:33:33Z",
  "id":719693700,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5MzcwMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:33:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/140) to add @plexoos! :tada:",
  "created_at":"2020-10-30T17:33:40Z",
  "id":719693784,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5Mzc4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:33:40Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @matthewfeickert for infrastructure\r\n\r\n",
  "created_at":"2020-10-30T17:34:06Z",
  "id":719694023,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NDAyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:34:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/141) to add @matthewfeickert! :tada:",
  "created_at":"2020-10-30T17:34:14Z",
  "id":719694109,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NDEwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:34:14Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @tamasgal for code\r\n\r\n",
  "created_at":"2020-10-30T17:34:36Z",
  "id":719694301,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NDMwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:34:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/142) to add @tamasgal! :tada:",
  "created_at":"2020-10-30T17:34:43Z",
  "id":719694373,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NDM3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:34:43Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @kreczko for code and test\r\n\r\n",
  "created_at":"2020-10-30T17:35:04Z",
  "id":719694585,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NDU4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:35:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/143) to add @kreczko! :tada:",
  "created_at":"2020-10-30T17:35:12Z",
  "id":719694655,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NDY1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:35:12Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"The CLI is not bad, probably better than a massive number of commits here and PRs. :) I'm surprised it's working so well, though, I have had issue getting to work all the time.",
  "created_at":"2020-10-30T17:35:13Z",
  "id":719694667,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NDY2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:35:13Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"NONE",
  "body":"I think you are spamming a lot of people's inboxes right now. Since being @ mentioned, I get a mail for each message in here. Is this intentional?",
  "created_at":"2020-10-30T17:35:44Z",
  "id":719694955,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NDk1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:35:44Z",
  "user":"MDQ6VXNlcjU4ODQwNjU="
 },
 {
  "author_association":"MEMBER",
  "body":"> The CLI is not bad, probably better than a massive number of commits here and PRs. :) I'm surprised it's working so well, though, I have had issue getting to work all the time.\r\n\r\nYeah, but I only have to do a big batch once.",
  "created_at":"2020-10-30T17:36:33Z",
  "id":719695899,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NTg5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:36:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I think you are spamming a lot of people's inboxes right now. Since being @ mentioned, I get a mail for each message in here. Is this intentional?\r\n\r\nAccording to my reading of the bot's documentation, the `@` is necessary. Sorry about the emails! It will be over soon\u2014if you \"unwatch,\" you won't get emails until I get to your name, since apparently I have to do them one at a time.",
  "created_at":"2020-10-30T17:37:45Z",
  "id":719696664,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NjY2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:37:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @nsmith- for code\r\n\r\n",
  "created_at":"2020-10-30T17:37:56Z",
  "id":719696748,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5Njc0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:37:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/144) to add @nsmith! :tada:",
  "created_at":"2020-10-30T17:38:03Z",
  "id":719696811,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NjgxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:38:03Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, let me try it without the `@`...\r\n\r\n@all-contributors please add nbiederbeck for code\r\n\r\n",
  "created_at":"2020-10-30T17:38:47Z",
  "id":719697233,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NzIzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:38:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/145) to add @nbiederbeck! :tada:",
  "created_at":"2020-10-30T17:38:56Z",
  "id":719697296,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5NzI5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:38:56Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"That worked! If anyone wants to opt out of all these emails, you can \"unsubscribe\" from this thread in GitHub and it will be permanent because I won't be using `@` anymore.",
  "created_at":"2020-10-30T17:41:18Z",
  "id":719698593,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5ODU5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:41:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add oshadura for code and infrastructure\r\n\r\n",
  "created_at":"2020-10-30T17:42:02Z",
  "id":719698948,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5ODk0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:42:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/146) to add @oshadura! :tada:",
  "created_at":"2020-10-30T17:42:09Z",
  "id":719699005,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5OTAwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:42:09Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add henryiii for code, infrastructure, and test\r\n\r\n",
  "created_at":"2020-10-30T17:42:30Z",
  "id":719699196,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5OTE5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:42:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/147) to add @henryiii! :tada:",
  "created_at":"2020-10-30T17:42:37Z",
  "id":719699256,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5OTI1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:42:37Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add masonproffitt for code and test\r\n\r\n",
  "created_at":"2020-10-30T17:43:01Z",
  "id":719699473,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5OTQ3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:43:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/148) to add @masonproffitt! :tada:",
  "created_at":"2020-10-30T17:43:10Z",
  "id":719699548,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5OTU0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:43:10Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add guitargeek for code\r\n\r\n",
  "created_at":"2020-10-30T17:43:34Z",
  "id":719699773,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5OTc3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:43:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/149) to add @guitargeek! :tada:",
  "created_at":"2020-10-30T17:43:41Z",
  "id":719699849,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTY5OTg0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:43:41Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add benkrikler for code\r\n\r\n",
  "created_at":"2020-10-30T17:44:05Z",
  "id":719700049,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMDA0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:44:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/150) to add @benkrikler! :tada:",
  "created_at":"2020-10-30T17:44:13Z",
  "id":719700110,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMDExMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:44:13Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add HDembinski for doc\r\n\r\n",
  "created_at":"2020-10-30T17:44:38Z",
  "id":719700358,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMDM1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:44:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/151) to add @HDembinski! :tada:",
  "created_at":"2020-10-30T17:44:46Z",
  "id":719700437,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMDQzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:44:46Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add riga for code\r\n\r\n",
  "created_at":"2020-10-30T17:45:06Z",
  "id":719700634,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMDYzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:45:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/152) to add @riga! :tada:",
  "created_at":"2020-10-30T17:45:13Z",
  "id":719700718,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMDcxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:45:13Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add wiso for code\r\n\r\n",
  "created_at":"2020-10-30T17:45:43Z",
  "id":719701015,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMTAxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:45:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/153) to add @wiso! :tada:",
  "created_at":"2020-10-30T17:45:51Z",
  "id":719701067,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMTA2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:45:51Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add jrueb for code\r\n\r\n",
  "created_at":"2020-10-30T17:46:16Z",
  "id":719701307,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMTMwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:46:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/154) to add @jrueb! :tada:",
  "created_at":"2020-10-30T17:46:23Z",
  "id":719701428,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMTQyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:46:23Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add bfis for code\r\n\r\n",
  "created_at":"2020-10-30T17:46:51Z",
  "id":719701699,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMTY5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:46:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/155) to add @bfis! :tada:",
  "created_at":"2020-10-30T17:47:00Z",
  "id":719701786,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMTc4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:47:00Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add raymondEhlers for code\r\n\r\n",
  "created_at":"2020-10-30T17:47:21Z",
  "id":719701976,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMTk3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:47:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/156) to add @raymondEhlers! :tada:",
  "created_at":"2020-10-30T17:47:28Z",
  "id":719702058,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMjA1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:47:28Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add andrzejnovak for code\r\n\r\n",
  "created_at":"2020-10-30T17:47:47Z",
  "id":719702229,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMjIyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:47:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/157) to add @andrzejnovak! :tada:",
  "created_at":"2020-10-30T17:47:54Z",
  "id":719702298,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMjI5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:47:54Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add bendavid for code\r\n\r\n",
  "created_at":"2020-10-30T17:48:12Z",
  "id":719702453,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMjQ1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:48:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/158) to add @bendavid! :tada:",
  "created_at":"2020-10-30T17:48:21Z",
  "id":719702536,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMjUzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:48:21Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add douglasdavis for code\r\n\r\n",
  "created_at":"2020-10-30T17:48:40Z",
  "id":719702723,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMjcyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:48:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/159) to add @douglasdavis! :tada:",
  "created_at":"2020-10-30T17:48:48Z",
  "id":719702787,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMjc4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:48:48Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add asymmetry for code\r\n\r\n",
  "created_at":"2020-10-30T17:49:05Z",
  "id":719702956,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMjk1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:49:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/160) to add @asymmetry! :tada:",
  "created_at":"2020-10-30T17:49:13Z",
  "id":719703043,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMzA0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:49:13Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add ast0815 for code\r\n\r\n",
  "created_at":"2020-10-30T17:49:30Z",
  "id":719703182,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMzE4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:49:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/161) to add @ast0815! :tada:",
  "created_at":"2020-10-30T17:49:38Z",
  "id":719703242,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMzI0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:49:38Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add HealthyPear for code\r\n\r\n",
  "created_at":"2020-10-30T17:49:56Z",
  "id":719703387,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMzM4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:49:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/162) to add @HealthyPear! :tada:",
  "created_at":"2020-10-30T17:50:04Z",
  "id":719703445,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMzQ0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:50:04Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add EdoPro98 for code\r\n\r\n",
  "created_at":"2020-10-30T17:50:21Z",
  "id":719703604,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMzYwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:50:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/163) to add @EdoPro98! :tada:",
  "created_at":"2020-10-30T17:50:29Z",
  "id":719703673,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMzY3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:50:29Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add JMSchoeffmann for code\r\n\r\n",
  "created_at":"2020-10-30T17:51:00Z",
  "id":719703952,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMzk1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:51:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/164) to add @JMSchoeffmann! :tada:",
  "created_at":"2020-10-30T17:51:09Z",
  "id":719704030,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwNDAzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:51:09Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add alexander-held for code\r\n\r\n",
  "created_at":"2020-10-30T17:51:29Z",
  "id":719704188,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwNDE4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:51:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/165) to add @alexander! :tada:",
  "created_at":"2020-10-30T17:51:37Z",
  "id":719704292,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwNDI5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:51:37Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"To everyone:\r\n\r\nI'm very sorry about all the emails, but it's over now!",
  "created_at":"2020-10-30T17:52:35Z",
  "id":719704782,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwNDc4Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "laugh":3,
   "total_count":5
  },
  "updated_at":"2020-10-30T17:52:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks Jim, I appreciate being added! \ud83d\ude42",
  "created_at":"2020-10-30T18:59:09Z",
  "id":719738810,
  "issue":137,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTczODgxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T18:59:09Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"This is cool @jpivarski. I'll have to checkout out this bot more.",
  "created_at":"2020-10-30T17:45:19Z",
  "id":719700787,
  "issue":141,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTcwMDc4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T17:45:19Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"I had no idea! Okay, I think I fixed it. Thanks for pointing this out.",
  "created_at":"2020-10-30T23:13:21Z",
  "id":719839547,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTgzOTU0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T23:13:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"me neither and was scratching my head \ud83d\ude48\r\n\r\nBtw. I created a pull request towards the docs. Do you prefer the wiki?",
  "created_at":"2020-10-30T23:15:18Z",
  "id":719840040,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg0MDA0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T23:15:18Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Can you check to see that you can put it in the wiki?\r\n\r\nActually, I would rather it in the wiki, then I can control how to mix it into the documentation. Passing custom Interpretations wouldn't be one of the \"basic\" items.",
  "created_at":"2020-10-30T23:33:03Z",
  "id":719844362,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg0NDM2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T23:33:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes sure!\r\n\r\nI just tried but I get the same error:\r\n\r\n```\r\n\u2591 tamasgal@greybox.local:uproot4.wiki \ue0a0 master \u21e1 uproot4_fork\r\n\u2591 00:34:42 > git push\r\nERROR: Permission to scikit-hep/uproot4.wiki.git denied to tamasgal.\r\nfatal: Could not read from remote repository.\r\n\r\nPlease make sure you have the correct access rights\r\nand the repository exists.\r\n\r\n\u2591 tamasgal@greybox.local:uproot4.wiki \ue0a0 master \u21e1 uproot4_fork\r\n\u2591 00:34:45 128 > cat .git/config\r\n[core]\r\n\trepositoryformatversion = 0\r\n\tfilemode = true\r\n\tbare = false\r\n\tlogallrefupdates = true\r\n\tignorecase = true\r\n\tprecomposeunicode = true\r\n[remote \"origin\"]\r\n\turl = git@github.com:scikit-hep/uproot4.wiki.git\r\n\tfetch = +refs/heads/*:refs/remotes/origin/*\r\n[branch \"master\"]\r\n\tremote = origin\r\n\tmerge = refs/heads/master\r\n\tpushRemote = origin\r\n```",
  "created_at":"2020-10-30T23:35:12Z",
  "id":719844857,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg0NDg1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T23:35:12Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"If you're doing it from a git client, it might matter what URLs it had at the time that you cloned it. Can you just paste it into the web form? That's the simplest way to find out if you have permissions.",
  "created_at":"2020-10-30T23:36:42Z",
  "id":719845152,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg0NTE1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T23:36:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yep, that works. I thought I tried both via HTTPS and SSH...",
  "created_at":"2020-10-30T23:37:58Z",
  "id":719845388,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg0NTM4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T23:37:58Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, let's do that instead of #170, then. Thanks!",
  "created_at":"2020-10-30T23:39:17Z",
  "id":719845682,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg0NTY4Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-10-30T23:39:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes, I closed the PR ;)",
  "created_at":"2020-10-30T23:40:58Z",
  "id":719846121,
  "issue":168,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTg0NjEyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-30T23:40:58Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Wow, this looks like a silly mistake. Does it fix #120?",
  "created_at":"2020-10-31T13:51:19Z",
  "id":719936972,
  "issue":171,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTkzNjk3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-31T13:51:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Oh, I forgot to mention that, yes of course. That was the main intention, to fix #120 \ud83d\ude06 ",
  "created_at":"2020-10-31T15:13:47Z",
  "id":719947035,
  "issue":171,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTk0NzAzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-31T15:13:47Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Actually I put it in the branch name, at least ;)",
  "created_at":"2020-10-31T15:14:30Z",
  "id":719947136,
  "issue":171,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTk0NzEzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-31T15:14:30Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"**Great, thanks for finding this!** I must have forgotten that this (internal) function returns an exception, rather than raising it. I think the reason for that was for the stack trace to end in a relevant spot, rather than the relevant spot being second-to-last in the stack trace.",
  "created_at":"2020-10-31T15:31:42Z",
  "id":719949381,
  "issue":171,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTk0OTM4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-31T15:31:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I see! : )",
  "created_at":"2020-10-31T15:33:28Z",
  "id":719949606,
  "issue":171,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcxOTk0OTYwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-10-31T15:33:28Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Does this file have an example that I can use for testing?\r\n\r\nhttps://github.com/donalrinho/uproot_fcc_edm4hep_test/blob/master/data/test.root\r\n\r\n(This kind of fix has to be test-driven.) Thanks!",
  "created_at":"2020-11-02T14:44:45Z",
  "id":720514983,
  "issue":172,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDUxNDk4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T14:44:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim,\r\n\r\nI'll prepare a test repo for you with a file!\r\n\r\nCheers,\r\nDonal",
  "created_at":"2020-11-02T14:46:21Z",
  "id":720515912,
  "issue":172,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDUxNTkxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T14:46:21Z",
  "user":"MDQ6VXNlcjg4Nzk5OTY="
 },
 {
  "author_association":"NONE",
  "body":"Hi Jim,\r\n\r\nPlease find a test file and notebook here:\r\n\r\nhttps://github.com/donalrinho/fcc_RAdoptAllocator_test\r\n\r\nThe ROOT file contains the same information stored both in `std::vector` format and the `RAdoptAllocator` format for comparison. `tree.show()` can read the `std::vector` branches, but not the `RAdoptAllocator` ones.\r\n\r\nCheers,\r\nDonal",
  "created_at":"2020-11-02T16:56:49Z",
  "id":720596499,
  "issue":172,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDU5NjQ5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T16:56:49Z",
  "user":"MDQ6VXNlcjg4Nzk5OTY="
 },
 {
  "author_association":"MEMBER",
  "body":"TIL `std::vector` can take more than one parameter:\r\n\r\n```python\r\n>>> import uproot4\r\n>>> t = uproot4.open(\"issue-172.root:events\")\r\n>>> t[\"rec_part_px_VecOps\"].member(\"fClassName\")\r\n<TString 'vector<float,ROOT::Detail::VecOps::RAdoptAllocator<float> >' at 0x7fa383f68510>\r\n>>> t[\"rec_part_px_VecOps\"].typename\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n...\r\nValueError: invalid C++ type name syntax at char 13\r\n\r\n    vector<float,ROOT::Detail::VecOps::RAdoptAllocator<float> >\r\n-----------------^\r\nin file issue-172.root\r\n```\r\n\r\nBut it [can take another argument](http://www.cplusplus.com/reference/vector/vector/): the allocator. First step is to add that to the type parsing (and perhaps all the rest of the STL containers should take an optional allocator as well).",
  "created_at":"2020-11-06T17:55:47Z",
  "id":723217067,
  "issue":172,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzIxNzA2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T17:55:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This was the issue:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/08f7df2e09f581c57a96433caf33713332061750/tests/test_0172-allow-allocators-in-vector-typenames.py#L15\r\n\r\nbut it's resolved now (by ignoring the allocators; we don't need them for read-only access in Python). There weren't any other issues in the way, since everything was just a `std::vector<float>`.",
  "created_at":"2020-11-06T18:25:03Z",
  "id":723230997,
  "issue":172,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzIzMDk5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T18:25:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for all the work here! Cheers, Donal",
  "created_at":"2020-11-09T11:38:45Z",
  "id":723959413,
  "issue":172,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzk1OTQxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T11:38:45Z",
  "user":"MDQ6VXNlcjg4Nzk5OTY="
 },
 {
  "author_association":"MEMBER",
  "body":"There aren't supposed to be any constraints. Uproot (both versions) spawn threads, but it's possible for objects in a multiprocessing environment to spawn their own threads. You don't want to be passing objects derived from one open file from one process into another (something you should avoid in general because processes don't share memory or file handles), but it should be perfectly okay for processes in `multiprocessing` to each open their own files and work with them independently.",
  "created_at":"2020-11-02T16:42:09Z",
  "id":720587214,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDU4NzIxNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T16:42:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for quick response as always. Yes indeed, I am not trying to pass any object from one thread to another, just to separate independent threads. \r\n\r\nI quote an example here but rootfile are in public area to do the test, if not possible I can share it somewhre on thw weblink, \r\n```\r\nimport uproot4\r\nimport awkward1 as ak\r\nimport multiprocessing as mp\r\nfrom functools import partial\r\n\r\nfile_=[\"/afs/cern.ch/user/k/khurana/public/test.root\", \"/afs/cern.ch/user/k/khurana/public/test.root\"]\r\n\r\ndef readfile(inputfile):\r\n    file_=uproot4.open(inputfile)\r\n    print (\"file opened\")\r\n    tree_ = file_[\"outTree\"].arrays()\r\n    print (\"number of events: \", len(tree_))\r\n```\r\n\r\nNow if I call this using \r\n```\r\nreadfile(file_[0])\r\n``` \r\n\r\nit works fine, \r\n\r\nhowever when tried to use the multiprocessing: \r\n\r\n```\r\niterable = file_                                                                                                                                                                                            \r\npool = mp.Pool(2)                                                                                                                                                                                           \r\nfunc = partial(readfile)                                                                                                                                                                                    \r\npool.map(func, iterable)                                                                                                                                                                                    \r\npool.close()                                                                                                                                                                                                \r\npool.join()                                                                                                                                                                                                 \r\n\r\n```\r\nIt printed \r\n```\r\n    print (\"file opened\")\r\n```\r\nand then it is stuck forever, \r\n\r\nDo you have any clue what can be a reason for this, or you see something missing in the snippet. I wonder this was working fine for another version of code in uproot3. \r\n\r\nThanks. \r\n",
  "created_at":"2020-11-02T17:22:33Z",
  "id":720611034,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDYxMTAzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T17:44:04Z",
  "user":"MDQ6VXNlcjQ5OTY2MDk="
 },
 {
  "author_association":"MEMBER",
  "body":"\"Stuck forever\" is a symptom of an error in Uproot 4 that has no equivalent in Uproot 3, so that's a hint for me.\r\n\r\n(Namely, Uproot 4 bundles byte intervals to read from a file into a single request and interprets them as they come in on a `queue.Queue`. If the number of file chunks that show up on this queue is less than the expected number, it's going to keep waiting for them to appear. Uproot 3 has no equivalent because it used a less efficient fetch-on-demand method. In your case, it looks like I got the expected number of chunks wrong, so I'll start by reproducing your example. Thanks!)",
  "created_at":"2020-11-02T17:45:38Z",
  "id":720624222,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDYyNDIyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T17:45:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for adding to the list, just a followup, can this be a reason for being stuck forever on an root file with a empty tree. The tree has the same structure as others, just that entries are zero so none of the  branch has any information. ",
  "created_at":"2020-11-02T19:58:38Z",
  "id":720691877,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDY5MTg3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T19:58:38Z",
  "user":"MDQ6VXNlcjQ5OTY2MDk="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll check that case, too.",
  "created_at":"2020-11-02T20:46:32Z",
  "id":720714974,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDcxNDk3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T20:46:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks Jim, btw is there a possibly some other option already in uproot4 while can allow parallel processing? ",
  "created_at":"2020-11-03T00:04:49Z",
  "id":720795115,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDc5NTExNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T00:04:49Z",
  "user":"MDQ6VXNlcjQ5OTY2MDk="
 },
 {
  "author_association":"MEMBER",
  "body":"All of the array-fetching functions (such as [HasBranches.arrays](https://uproot4.readthedocs.io/en/latest/uproot4.behaviors.TBranch.HasBranches.html#arrays)) have `decompression_executor` and `interpretation_executor` parameters, which take Python 3 executors. You don't want to make these process pools, because you don't want to be sending data between processes, but thread pools would make sense. The default `decompression_executor` has as many workers as the number of cores, but the default `interpretation_executor` is a trivial executor (no parallel processing). That's because decompression algorithms release the GIL and are good to parallelize, but interpretation is usually just casting with NumPy, which is not an expensive operation. However, there are cases in which interpretation is expensive Python code, though there's still not a lot of advantage because Python doesn't release the GIL.",
  "created_at":"2020-11-03T00:16:37Z",
  "id":720798835,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDc5ODgzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T00:16:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In fact, that's what's causing all the trouble: the default `uproot4.decompression_executor` and `uproot4.interpretation_executor` are global, and multiprocessing is trying to send this immobile objects to the other process.",
  "created_at":"2020-11-03T00:23:53Z",
  "id":720801817,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDgwMTgxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T00:23:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"So now I've removed the global default Executors; opened files and functions like `uproot4.iterate` and `uproot4.concatenate` now take `decompression_executor` and `interpretation_executor` arguments, like the other array-fetching functions. The problem was that multiprocessing tried to move these objects between processes, which cannot possibly work (because threads are bound to a process). Instead of raising some sort of error, Python froze.\r\n\r\nThis is different from the freezing that I was expecting, although attempting to read an empty file resulted in the more conventional type of freeze. Thanks for the tip: [I'm now testing for that](https://github.com/scikit-hep/scikit-hep-testdata/commit/1fe8cdcfda881385c1296ec3e5f7a04e274c2cb4).",
  "created_at":"2020-11-03T01:21:19Z",
  "id":720825817,
  "issue":173,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDgyNTgxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T01:21:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll let `step_size` be an iterable of `(start, stop)` pairs, like it was in Uproot 3. That's the simplest solution.",
  "created_at":"2020-11-02T18:05:47Z",
  "id":720635384,
  "issue":174,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDYzNTM4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-02T18:05:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've changed my mind about this one. I was just going to implement it because I was recently looking at the implementation to fix a memory leak (PR #187). But what I saw was that there's no advantage to passing a list of `(start, stop)` pairs through `iterate` over just doing\r\n\r\n```python\r\nfor start, stop in list_of_pairs:\r\n    arrays = ttree.arrays(entry_start=start, entry_stop=stop, array_cache=None)\r\n```\r\n\r\nThe performance advantages of using `ttree.iterate` (and `uproot4.iterate`) rely on it being sequential, since it saves `TBaskets` from the previous iteration to avoid rereading them on the next. (PR #187 fixes a broken implementation of that.) But if the `list_of_pairs` can skip entries, there's a (good) chance that the previously saved `TBaskets` won't be relevant, and if the `list_of_pairs` can skip around non-monotonically (not your case, I know), then the previously saved `TBaskets` definitely aren't relevant.\r\n\r\nI could add an interface that just does the above\u2014if (badly named) `step_size` is an iterable, then just call `arrays` a bunch of times\u2014but that interface wouldn't generalize to `uproot4.iterate`. Implementing it on `HasBranches.iterate` but not `uproot4.iterate` complicates the interface.\r\n\r\nSo I'd rather recommend that you write the for loop calling `HasBranches.arrays` a bunch of times. I think that is the most future-friendly way to go.",
  "created_at":"2020-11-06T22:44:07Z",
  "id":723332988,
  "issue":174,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzMzMjk4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T22:44:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I haven't tested it, but this _might_ be related to #121.",
  "created_at":"2020-11-03T01:23:55Z",
  "id":720827801,
  "issue":176,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDgyNzgwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T01:23:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"In fact, you're right! Thanks! closing\r\nUpdate: sorry, not quite",
  "created_at":"2020-11-03T01:28:31Z",
  "id":720833161,
  "issue":176,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDgzMzE2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T01:42:45Z",
  "user":"MDQ6VXNlcjE3NDU0ODQ4"
 },
 {
  "author_association":"NONE",
  "body":"Actually, the opening works, but then it fails again (while working for `uproot`)\r\n\r\ncommand: \r\n```uproot4.open('https://starterkit.web.cern.ch/starterkit/data/advanced-python-2019/dalitzdata.root')['tree'].arrays()```\r\n\r\n\r\nError: \r\n```\r\nOSError: found 0 of 156 expected headers in HTTP multipart\r\nfor URL https://starterkit.web.cern.ch/starterkit/data/advanced-python-2019/dalitzdata.root\r\n```\r\n",
  "created_at":"2020-11-03T01:44:02Z",
  "id":720850650,
  "issue":176,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDg1MDY1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-24T17:04:34Z",
  "user":"MDQ6VXNlcjE3NDU0ODQ4"
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like something with multipart HTTP requests was broken between v0.0.17 and v0.0.20",
  "created_at":"2020-11-03T06:42:25Z",
  "id":720934995,
  "issue":176,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMDkzNDk5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T06:42:25Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"NONE",
  "body":"Reopened, as it seems to me that this was not entirely fixed. The snippet above still errors:\r\nActually, the opening works, but then it fails again (while working for `uproot`)\r\n\r\n```\r\nfile = uproot4.open('https://starterkit.web.cern.ch/starterkit/data/advanced-python-2019/dalitzdata.root')\r\n_ = file['tree'].arrays()  # fails here\r\n```\r\n\r\nIt fails \"later\" though than the fix I assume, so I've rearranged the example.\r\n\r\nError: \r\n```\r\nOSError: found 0 of 156 expected headers in HTTP multipart\r\nfor URL https://starterkit.web.cern.ch/starterkit/data/advanced-python-2019/dalitzdata.root\r\n```\r\n",
  "created_at":"2020-11-24T17:06:34Z",
  "id":733113710,
  "issue":176,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzExMzcxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-24T17:07:04Z",
  "user":"MDQ6VXNlcjE3NDU0ODQ4"
 },
 {
  "author_association":"MEMBER",
  "body":"Darn: the test only checked\r\n\r\n```python\r\ndata = f[\"tree/Y1\"].array()\r\n```\r\n\r\nwhich works. I'm looking into the issue with multiple branches.",
  "created_at":"2020-11-24T17:15:55Z",
  "id":733119339,
  "issue":176,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzExOTMzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-24T17:15:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"So far as I can tell https://example.com returns an invalid response:\r\n```bash\r\n$ curl -L http://www.example.com -i -H \"Range: bytes=0-10, 15-20\"\r\nHTTP/1.1 206 Partial Content\r\nAccept-Ranges: bytes\r\nAge: 419053\r\nCache-Control: max-age=604800\r\nContent-Type: text/html; charset=UTF-8\r\nDate: Tue, 03 Nov 2020 10:00:47 GMT\r\nEtag: \"3147526947\"\r\nExpires: Tue, 10 Nov 2020 10:00:47 GMT\r\nLast-Modified: Thu, 17 Oct 2019 07:18:26 GMT\r\nServer: ECS (nyb/1D04)\r\nVary: Accept-Encoding\r\nX-Cache: HIT\r\nContent-Length: 225\r\n\r\n\r\n--3d6b6a416f9b5\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Range: bytes 0-10/1256\r\n\r\n<!doctype h\r\n--3d6b6a416f9b5\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Range: bytes 15-20/1256\r\n\r\n\r\n<html\r\n--3d6b6a416f9b5--\r\n```\r\n\r\nas the `Content-Type` header [should be](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests#Multipart_ranges) `Content-Type: multipart/byteranges; boundary=3d6b6a416f9b5`. I guess there could be other servers which do this so I've added a workaround in a8beb25.",
  "created_at":"2020-11-03T10:02:35Z",
  "id":721018778,
  "issue":177,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTAxODc3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T10:02:35Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"For #121, I was just going to add HTTP response numbers. I know that you brought up this point about the requests library being higher level, but I wanted to find out \"what we're buying\" with the dependency. Redirects is one thing, which doesn't seem so bad, but if you're willing to jump in and help like this and `http.client` is a terrible working environment, I can be persuaded to swallow the dependency just to be more friendly to contributors like you.\r\n\r\nI can't believe I missed the message about `urllib.request`; having a higher-level interface already in the standard library is a no-brainer. That comes with zero dependency cost.\r\n\r\nI wouldn't try to evolve this toward a completely different level of interface. HTTPSource is a small enough class that it can be rewritten to fit a new interface. I've already written it (and XRootDSource) from scratch twice: once at the start of Uproot 4, to establish a remote-file interface that minimizes round trips (to write the rest of Uproot around), and a second time after I identified a subset of the remote-file interface that I actually used (also, to clean up some spaghetti).\r\n\r\nI started a wiki where I'll document the [physical layer interface](https://github.com/scikit-hep/uproot4/wiki/Physical-layer-interface); what Uproot expects of a class like HTTPSource or XRootDSource. This document will be useful in the long run, to add additional interfaces in the future (Ceph? S3?), but if you want to make a clean break and write HTTPSource using `urllib.request` or the third-party requests library, I'll support that. I'll try to get that document written in the next few hours, in either case.\r\n\r\n@chrisburr, what do you think? If this is more than you want to get into right now, I'll be patching up #121 by handling all the redirect codes, but the possibility of redoing HTTPSource as a requests-based class is always available. (The requests library would become an optional dependency for PyPI, with a loader in [extras.py](https://github.com/scikit-hep/uproot4/blob/master/uproot4/extras.py), but a strict dependency in conda-forge, like awkward1.)",
  "created_at":"2020-11-03T14:06:36Z",
  "id":721136707,
  "issue":177,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTEzNjcwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T14:06:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've finished the above-mentioned documention at https://github.com/scikit-hep/uproot4/wiki/Physical-layer-interface\r\n\r\nIn the meantime, I'll mark #121 as \"in progress\" (in the [November projects](https://github.com/scikit-hep/uproot4/projects/6)), and I'll wait for your response about how you want to do this. If you want to rewrite HTTPSource with `urllib.request` or the requests library, I'll support that. If not, I'll take over and just add the redirect codes.",
  "created_at":"2020-11-03T16:51:40Z",
  "id":721249495,
  "issue":177,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMTI0OTQ5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-03T16:51:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@chrisburr Let me know what you want to do with this: (1) merge in as-is, (2) revamp HTTPSource as requests-based (either `urllib.requests` or the third-party `requests`), or (3) neither: I would address #121 by just adding more redirect codes.",
  "created_at":"2020-11-11T20:06:30Z",
  "id":725634312,
  "issue":177,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNTYzNDMxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-11T20:06:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@chrisburr Thanks for looking into this. I decided to finish it off, but after resolving a merge conflict, the tests failed. The helper function that checks for redirects is a good idea in principle, but I couldn't follow what was going wrong. (It wasn't just the helper function; the propagation of the \"boundary\" string also complicated things\u2014it might have been residual merge errors.)\r\n\r\nStarting from the main branch, I copied your tests and manually reimplemented what you developed here into PR #201. I didn't do a helper function because the calling structure for what to do in case of a redirect is different in the places where it appears. (Also, I put in a fix for redirects.)\r\n\r\nHere are the main things that I gleaned from your PR:\r\n\r\n   * Headers, including those that separate parts of a multipart response, are case-insensitive. That's what was breaking the LHCb StarterKit example.\r\n   * Although the \"`Content-Type: multipart/byteranges`\" is a part of the specification, and it provides a potentially useful string that could be used to cross-check the multipart boundaries, it isn't strictly necessary. We know exactly how many bytes are in each part and this information is reiterated in the \"`Content-Range:`\" (which _is_ strictly necessary, since the parts might come in a different order\u2014we need to know which one we're on). I've left the parsing of the multiparts minimal.\r\n   * Any response in the 300's is a redirect. As for the 200's, I'm still going to require exact status codes.\r\n\r\nMeanwhile, it has been [failing intermittently because of XRootD](https://github.com/scikit-hep/uproot4/runs/1433794743?check_suite_focus=true), though that's a separate thing. I noticed that XRootD went through a major version update (4 \u2192 5). I'll keep an eye on that.",
  "created_at":"2020-11-20T23:50:10Z",
  "id":731464832,
  "issue":177,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTQ2NDgzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T23:50:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Done.",
  "created_at":"2020-11-20T21:38:21Z",
  "id":731420431,
  "issue":179,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTQyMDQzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T21:38:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"ALso, I am a bit confused as what values I get from this.\r\n\r\nThis is the plot root draws for the `TProfile` ('meanetruevseest):\r\n\r\n![tprofile](https://user-images.githubusercontent.com/5488440/98346506-586f7180-2016-11eb-830a-292d4e379104.png)\r\n\r\n\r\nBut when reading with uproot3 and plotting, I get this:\r\n\r\n![uproot_tprofile](https://user-images.githubusercontent.com/5488440/98346793-c4ea7080-2016-11eb-98de-e2667e9c5988.png)\r\n\r\n",
  "created_at":"2020-11-06T08:59:35Z",
  "id":722962048,
  "issue":181,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMjk2MjA0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T08:59:47Z",
  "user":"MDQ6VXNlcjU0ODg0NDA="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks\u2014that was a bug in code generation, in which `fNoper` wasn't quoted. That's what PR #186 fixes.\r\n\r\nAs for your questions, TF1 doesn't have any behaviors defined, so all you can do with it at the moment is\r\n\r\n```python\r\n>>> f[\"correctedE\"].all_members\r\n{'@fUniqueID': 0,\r\n '@fBits': 50332672,\r\n 'fName': 'correctedE',\r\n 'fTitle': 'pol5',\r\n 'fNdim': 1,\r\n 'fNpar': 6,\r\n 'fNoper': 1,\r\n 'fNconst': 0,\r\n 'fNumber': 305,\r\n 'fNval': 0,\r\n 'fNstring': 0,\r\n 'fExpr': <TString 'pol5' at 0x7f52e19c8cf0>,\r\n 'fOper': array([1090519541], dtype=int32),\r\n 'fConst': array([], dtype=float64),\r\n 'fParams': array([ 7.96960054e-03,  9.95640665e-01,  8.74907519e-04,  3.23497673e-03,\r\n                   -3.36583446e-03,  1.35120768e-03]),\r\n 'fNames': <TString '' at 0x7f52e19290b0>,\r\n 'fFunctions': <TObjArray of 0 items at 0x7f52e191a4f0>,\r\n 'fLinearParts': <TObjArray of 0 items at 0x7f52e191a880>,\r\n 'fLineColor': 2,\r\n 'fLineStyle': 1,\r\n 'fLineWidth': 2,\r\n 'fFillColor': 19,\r\n 'fFillStyle': 0,\r\n 'fMarkerColor': 1,\r\n 'fMarkerStyle': 1,\r\n 'fMarkerSize': 1.0,\r\n 'fXmin': -1.899999976158142,\r\n 'fXmax': 2.2999999523162846,\r\n 'fNpx': 100,\r\n 'fType': 0,\r\n 'fNpfits': 418,\r\n 'fNDF': 412,\r\n 'fNsave': 103,\r\n 'fChisquare': 0.05494463858791303,\r\n 'fParErrors': array([0.09455276, 0.16740379, 0.15668286, 0.14833426, 0.04744522,\r\n                      0.0319801 ]),\r\n 'fParMin': array([0., 0., 0., 0., 0., 0.]),\r\n 'fParMax': array([0., 0., 0., 0., 0., 0.]),\r\n 'fSave': array([-1.98009905e+00, -1.92969130e+00, -1.87989590e+00, -1.83068014e+00,\r\n                 -1.78201250e+00, -1.73386258e+00, -1.68620112e+00, -1.63899996e+00,\r\n                 -1.59223201e+00, -1.54587127e+00, -1.49989275e+00, -1.45427250e+00,\r\n                 -1.40898757e+00, -1.36401596e+00, -1.31933666e+00, -1.27492957e+00,\r\n                 -1.23077552e+00, -1.18685621e+00, -1.14315424e+00, -1.09965304e+00,\r\n                 -1.05633687e+00, -1.01319080e+00, -9.70200691e-01, -9.27353164e-01,\r\n                 -8.84635586e-01, -8.42036043e-01, -7.99543325e-01, -7.57146903e-01,\r\n                 -7.14836906e-01, -6.72604099e-01, -6.30439868e-01, -5.88336191e-01,\r\n                 -5.46285621e-01, -5.04281264e-01, -4.62316759e-01, -4.20386254e-01,\r\n                 -3.78484389e-01, -3.36606269e-01, -2.94747449e-01, -2.52903909e-01,\r\n                 -2.11072032e-01, -1.69248587e-01, -1.27430706e-01, -8.56158578e-02,\r\n                 -4.38018359e-02, -1.98673020e-03,  3.98310912e-02,  8.16530039e-02,\r\n                  1.23480148e-01,  1.65313451e-01,  2.07153645e-01,  2.49001292e-01,\r\n                  2.90856804e-01,  3.32720463e-01,  3.74592443e-01,  4.16472832e-01,\r\n                  4.58361649e-01,  5.00258874e-01,  5.42164459e-01,  5.84078356e-01,\r\n                  6.26000535e-01,  6.67931008e-01,  7.09869847e-01,  7.51817206e-01,\r\n                  7.93773346e-01,  8.35738649e-01,  8.77713645e-01,  9.19699033e-01,\r\n                  9.61695698e-01,  1.00370474e+00,  1.04572747e+00,  1.08776549e+00,\r\n                  1.12982064e+00,  1.17189507e+00,  1.21399124e+00,  1.25611195e+00,\r\n                  1.29826036e+00,  1.34044000e+00,  1.38265482e+00,  1.42490917e+00,\r\n                  1.46720785e+00,  1.50955614e+00,  1.55195978e+00,  1.59442503e+00,\r\n                  1.63695868e+00,  1.67956806e+00,  1.72226107e+00,  1.76504621e+00,\r\n                  1.80793260e+00,  1.85092996e+00,  1.89404871e+00,  1.93729990e+00,\r\n                  1.98069532e+00,  2.02424744e+00,  2.06796950e+00,  2.11187549e+00,\r\n                  2.15598017e+00,  2.20029912e+00,  2.24484874e+00,  2.28964627e+00,\r\n                  2.33470981e+00, -1.89999998e+00,  2.29999995e+00]),\r\n 'fMaximum': -1111.0,\r\n 'fMinimum': -1111.0}\r\n```\r\n\r\nIf you have a suggestion for how this should be plotted or something, let me know. We should try to give it a similar interface to histograms, which is in flux: #167 and scikit-hep/boost-histogram#423.\r\n\r\nOne of the mistakes in Uproot 3 was to reproduce the ROOT C++ class hierarchy in the Python class hierarchy, which have the TProfile some odd behavior (because it descended from TH1). That has been separated in Uproot 4 and TProfiles have `to_boost` and `to_hist` methods for boost-histogram and hist, respectively. Unfortunately, it doesn't work:\r\n\r\n```\r\nAttributeError: 'WeightedMeanView' object has no attribute 'sum_of_weighted_deltas_squared'\r\n```\r\n\r\nI made some wrong assumption about the structure of a boost-histogram (#167 again). In the meantime, `values_errors` gives the profile values and their errors as two NumPy arrays.",
  "created_at":"2020-11-06T19:38:27Z",
  "id":723263090,
  "issue":181,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzI2MzA5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T19:38:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski thanks!\r\n\r\nAny idea about the `TProfile` business?",
  "created_at":"2020-11-06T20:32:22Z",
  "id":723286783,
  "issue":181,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzI4Njc4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T20:32:22Z",
  "user":"MDQ6VXNlcjU0ODg0NDA="
 },
 {
  "author_association":"MEMBER",
  "body":"This is what I meant:\r\n\r\n> One of the mistakes in Uproot 3 was to reproduce the ROOT C++ class hierarchy in the Python class hierarchy, which have the TProfile some odd behavior (because it descended from TH1).\r\n\r\nI don't think the output of TProfile was ever addressed in Uproot 3. But I do know that I made `TProfile.values_errors` exactly identical to ROOT's output in Uproot 4.",
  "created_at":"2020-11-06T20:39:29Z",
  "id":723289579,
  "issue":181,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzI4OTU3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T20:39:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ok, thanks again, will try",
  "created_at":"2020-11-06T22:42:16Z",
  "id":723332510,
  "issue":181,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzMzMjUxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T22:42:16Z",
  "user":"MDQ6VXNlcjU0ODg0NDA="
 },
 {
  "author_association":"MEMBER",
  "body":"Maybe if any string is not a glob, it should raise a FileNotFoundException when it doesn't match. This is already true of remote URLs (which don't support glob-matching).",
  "created_at":"2020-11-06T14:24:59Z",
  "id":723107543,
  "issue":182,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzEwNzU0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T14:24:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> Maybe if any string is not a glob, it should raise a FileNotFoundException when it doesn't match. This is already true of remote URLs (which don't support glob-matching).\r\n\r\nSounds good to me, I'll take a look if you don't get manage round to it :)",
  "created_at":"2020-11-06T15:23:38Z",
  "id":723138397,
  "issue":182,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzEzODM5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T15:23:38Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll do it because I know the spot where this should go, but I'll let you test the proposed solution, to be sure that it has the properties you want. I think it's supposed to be:\r\n\r\n   * any local filename (non-URL, except `\"file://\"`) with _no wildcard characters_ (`*`, `?`, `[`, `]`, `{`, `}`) must match an existent file or raise an error;\r\n   * any local filename with wildcard characters is allowed to match nothing.\r\n\r\nThe second rule is important for describing sets of directories where files might exist, according to a naming convention, but it's allowed for some directories to be empty. (That's a situation I think comes up a lot.)",
  "created_at":"2020-11-06T17:14:45Z",
  "id":723196994,
  "issue":182,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzE5Njk5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T17:14:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@BenGalewsky and @gordonwatts I don't have an example that will test this.\r\n\r\nIt was hard enough finding an example that I could use to verify that I understood the HTTPSConnection interface correctly\u2014I used this:\r\n\r\n```python\r\nconnection.request(\"GET\", \"/search/code?q=addClass+in:file+language:js+repo:jquery/jquery\", headers={\"user-agent\": \"Python\"})\r\n```\r\n\r\nto be sure that the query is added as part of the second argument, rather than being another argument (without the question mark, encoded differently, etc.). I also don't see anywhere else in the codebase where `ParseResult.path` is used in a context other than local files. If XRootD URLs take query parameters (I doubt it), I didn't fix that, either.\r\n\r\nCould you do a simple test against this branch to confirm that you can access the server that gives you ROOT files that depend on query parameters? (Or just reply here with some sample URLs, if they're publicly available and I can test them. Furthermore, if they can be guaranteed to exist, I could add them to the tests.)",
  "created_at":"2020-11-06T16:52:38Z",
  "id":723185222,
  "issue":183,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzE4NTIyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T16:52:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ah, thank you so much for tackling this so quickly! We'll see if we can run a test.",
  "created_at":"2020-11-06T16:56:54Z",
  "id":723187520,
  "issue":183,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzE4NzUyMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T16:56:54Z",
  "user":"MDQ6VXNlcjE3NzgzNjY="
 },
 {
  "author_association":"NONE",
  "body":"It works!\r\nThank you ",
  "created_at":"2020-11-06T17:19:51Z",
  "id":723199577,
  "issue":183,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzE5OTU3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T17:19:51Z",
  "user":"MDQ6VXNlcjgyMjk4NzU="
 },
 {
  "author_association":"MEMBER",
  "body":"> If XRootD URLs take query parameters (I doubt it), I didn't fix that, either.\r\n\r\nThey do and reading from at least one site fails in weird ways without them.",
  "created_at":"2020-11-06T19:26:12Z",
  "id":723257745,
  "issue":183,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzI1Nzc0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T19:26:12Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"Apparently, [uproot4/source/xrootd.py](https://github.com/scikit-hep/uproot4/blob/master/uproot4/source/xrootd.py) uses the whole `file_path` without decomposing it into a `parsed_url`, so I guess the query parameters go along for the ride.",
  "created_at":"2020-11-06T19:51:04Z",
  "id":723268932,
  "issue":183,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzI2ODkzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-06T19:51:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It fails because of something in the error message:\r\n\r\n```\r\n        TH1 version 7 as uproot4.dynamic.Model_TH1_v8 (522 bytes)\r\n```\r\n\r\nIt's trying to load a TH1 version 7 using a class named `Model_TH1_v8`. The difference between version 7 and version 8 is that the latter has one more 4 byte member, and that's why\r\n\r\n```\r\nexpected 522 bytes but cursor moved by 526 bytes (through TH1)\r\n```\r\n\r\nWhy's it using a version 8 class when it's a version 7 instance? _Because the streamer for TH1F told it to._\r\n\r\n```python\r\n>>> f = uproot4.open(\"issue-188.root\")\r\n>>> f.file.streamer_named(\"TH1F\").show()\r\nTH1F (v2): TH1 (v8), TArrayF (v1)\r\n```\r\n\r\nThat's wrong\u2014we certainly couldn't call this file \"not broken,\" but it works in Uproot 3 and presumably ROOT, so there should be a work-around. [PR #199](https://github.com/scikit-hep/uproot4/pull/199/files#diff-b9429ed79bcc66f23bf0f1637ee53386fe73c32932e7fcf0201f260f2b14263e) adds a fallback to rewind the deserialization and try again when it starts with the wrong class. That's how Uproot 3 worked throughout, because I hadn't thought through class versioning before writing it. Uproot 4 was meant to detangle that mess by dealing with class versions up-front, but sometimes the information given up-front is wrong.",
  "created_at":"2020-11-20T20:09:58Z",
  "id":731383766,
  "issue":188,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTM4Mzc2Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T20:09:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks very much @jpivarski. :+1: ",
  "created_at":"2020-11-21T01:56:49Z",
  "id":731488948,
  "issue":188,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTQ4ODk0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-21T01:56:49Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2020-11-09T03:26:16Z",
  "id":723732315,
  "issue":189,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyMzczMjMxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-09T03:26:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I was just about to address this, but `open` is the first thing listed under `help(uproot4)`. There's also a lot of introductory text in the [online documentation](https://uproot4.readthedocs.io/en/latest/basic.html); I think of docstrings as being more reference-oriented than tutorial-oriented.\r\n\r\n```python\r\n>>> help(uproot4)\r\n```\r\n```\r\nHelp on package uproot4:\r\n\r\nNAME\r\n    uproot4 - Uproot: ROOT I/O in pure Python and NumPy.\r\n\r\nDESCRIPTION\r\n    Nearly all of the functions needed for general use are imported here, but the\r\n    documentation gives fully qualified names. For example, the most frequently\r\n    used function in Uproot is\r\n    \r\n    .. code-block:: python\r\n    \r\n        uproot4.open(\"path/to/filename.root\")\r\n    \r\n    but we refer to it in the documentation as :py:func:`~uproot4.reading.open`.\r\n    \r\n    Typical entry points for file-reading are\r\n    \r\n    * :py:func:`~uproot4.reading.open`\r\n    * :py:func:`~uproot4.behaviors.TBranch.iterate`\r\n    * :py:func:`~uproot4.behaviors.TBranch.concatenate`\r\n    * :py:func:`~uproot4.behaviors.TBranch.lazy`\r\n    \r\n    though they would usually be accessed as ``uproot4.iterate``,\r\n    ``uproot4.concatenate``, and ``uproot4.lazy``.\r\n    \r\n    The most useful classes are\r\n    \r\n    * :py:class:`~uproot4.behaviors.TBranch.HasBranches` (``TTree`` or ``TBranch``)\r\n    * :py:class:`~uproot4.behaviors.TBranch.TBranch`\r\n    * :py:class:`~uproot4.behaviors.TH1`\r\n    * :py:class:`~uproot4.behaviors.TH2`\r\n    * :py:class:`~uproot4.behaviors.TProfile`\r\n    \r\n    though they would usually be accessed through instances that have been read\r\n    from files.\r\n...\r\n```\r\n\r\nThere are other documentation issues, and I'll be focusing on them next (ahead of the remaining feature requests).",
  "created_at":"2020-11-20T20:46:40Z",
  "id":731399145,
  "issue":190,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTM5OTE0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T20:46:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The `__len__` method is part of the [Mapping protocol](https://docs.python.org/3/library/collections.abc.html#collections-abstract-base-classes), and although I don't see it written down anywhere, I think the implication is that `__len__` must be the number of items that `__iter__` would yield. I struggled with that in Uproot 3, but TTree in Uproot 3 didn't officially support the Mapping protocol. (It just duck-typed relatively well.)\r\n\r\nYou're right that ReadOnlyDirectory has more reason to be a Mapping than HasBranches, but these two things are more fully integrated now\u2014you can \"getitem\" through directories into branches, a feature that partially worked by accident in Uproot 3, which I found out about because people used it in their examples. (That's clearly a desire path. And [explicitly requested](https://github.com/scikit-hep/uproot/issues/444) and [user-contributed](https://github.com/scikit-hep/uproot/pull/459).) For example, we can treat branches and subbranches as though they were directories in a single string with slashes:\r\n\r\n```python\r\n>>> import uproot4\r\n>>> import skhep_testdata\r\n>>> f = uproot4.open(skhep_testdata.data_path(\"uproot-nesteddirs.root\"))\r\n\r\n>>> f.keys()\r\n['one;1', 'one/two;1', 'one/two/tree;1', 'one/tree;1', 'three;1', 'three/tree;1']\r\n\r\n>>> f[\"one/two/tree\"].keys()\r\n['Int32', 'Int64', 'UInt32', 'UInt64', 'Float32', 'Float64', 'Str', 'ArrayInt32', 'ArrayInt64', 'ArrayUInt32',\r\n 'ArrayUInt64', 'ArrayFloat32', 'ArrayFloat64', 'N', 'SliceInt32', 'SliceInt64', 'SliceUInt32', 'SliceUInt64',\r\n 'SliceFloat32', 'SliceFloat64']\r\n\r\n>>> f[\"one/two/tree/Int32\"]\r\n<TBranch 'Int32' at 0x7fa5792740a0>\r\n```\r\n\r\nHaving both of these things, which are treated equivalently, both satisfy the Mapping protocol removes the friction I'd expect if one were a Mapping and the other were not. That's also why we don't have \"TTree\" behavior, we have \"HasBranches\" behavior, because a TTree with branches can be treated the same way as a branch with subbranches.\r\n\r\nNow, it is a pain point that `len(ttree)` returns the number of branches, rather than the number of entries. Without any of the motivations described above, I'd expect `len(ttree)` to be the number of entries (which is, I guess, why that was the case in Uproot 3). But if so, then what's `__iter__`? I think I made `__iter__` raise an exception in Uproot 3 because I know what users might be thinking, and it's not that.\r\n\r\nWe can see some of this cognitive dissonance here:\r\n\r\n```python\r\n>>> events = uproot4.open(skhep_testdata.data_path(\"uproot-Zmumu.root\"))[\"events\"]\r\n\r\n>>> len(events)\r\n20\r\n>>> events.num_entries\r\n2304\r\n\r\n>>> for branch in events:\r\n...     print(branch)\r\n... \r\n<TBranch 'Type' at 0x7fa5791ac4c0>\r\n<TBranch 'Run' at 0x7fa5791acc10>\r\n<TBranch 'Event' at 0x7fa5791b0370>\r\n<TBranch 'E1' at 0x7fa5791b0a90>\r\n<TBranch 'px1' at 0x7fa5791b0f40>\r\n<TBranch 'py1' at 0x7fa5791b6910>\r\n<TBranch 'pz1' at 0x7fa5791b6dc0>\r\n<TBranch 'pt1' at 0x7fa5791bc790>\r\n<TBranch 'eta1' at 0x7fa5791bceb0>\r\n<TBranch 'phi1' at 0x7fa5791c2610>\r\n<TBranch 'Q1' at 0x7fa5791c2d30>\r\n<TBranch 'E2' at 0x7fa5791c9490>\r\n<TBranch 'px2' at 0x7fa5791c9bb0>\r\n<TBranch 'py2' at 0x7fa5791ce310>\r\n<TBranch 'pz2' at 0x7fa5791cea30>\r\n<TBranch 'pt2' at 0x7fa5791ceee0>\r\n<TBranch 'eta2' at 0x7fa5791d48b0>\r\n<TBranch 'phi2' at 0x7fa5791d4fd0>\r\n<TBranch 'Q2' at 0x7fa5791d9730>\r\n<TBranch 'M' at 0x7fa5791d9e50>\r\n\r\n>>> for event in events.arrays():\r\n...     print(repr(event))\r\n... \r\n<Record ... phi2: -0.441, Q2: -1, M: 82.5} type='{\"Type\": string, \"Run\": int32, ...'>\r\n<Record ... phi2: 2.74, Q2: 1, M: 83.6} type='{\"Type\": string, \"Run\": int32, \"Ev...'>\r\n<Record ... phi2: 2.74, Q2: 1, M: 83.3} type='{\"Type\": string, \"Run\": int32, \"Ev...'>\r\n<Record ... phi2: 2.74, Q2: 1, M: 82.1} type='{\"Type\": string, \"Run\": int32, \"Ev...'>\r\n<Record ... phi2: -2.71, Q2: -1, M: 90.5} type='{\"Type\": string, \"Run\": int32, \"...'>\r\n<Record ... phi2: 0.583, Q2: 1, M: 89.8} type='{\"Type\": string, \"Run\": int32, \"E...'>\r\n...\r\n```\r\n\r\nIf we make `len(ttree)` be `ttree.num_entries`, then we should also make `iter(ttree)` be `iter(ttree.arrays())`, though I would not recommend the latter for performance\u2014I certainly wouldn't want to make it easy or suggest that it's the right way to go by making it `__iter__`.\r\n\r\nSo what do we do if one motivation (slicing through directories) implies one design and another motivation (Pythonic length/iteration) implies another? In this case, Pythonic iteration is also something I'd recommend against, so that's why the first motivation won.\r\n\r\n(Incidentally, in Julia, there would be nothing wrong with naive iteration over such an object. Some physical properties of the implementation\u2014like the performance of Python\u2014have interface consequences.)",
  "created_at":"2020-11-13T17:16:26Z",
  "id":726889311,
  "issue":191,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjg4OTMxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-13T17:16:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK, thanks for the detailed clarification, as always, it makes of course sense. I also agree with your last comment about performance subtleties dictating interface design.\r\n\r\nLet's close this then. I will put a link to this issue into the wiki!",
  "created_at":"2020-11-13T17:27:11Z",
  "id":726895544,
  "issue":191,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyNjg5NTU0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-13T17:27:11Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Two things that might be relevant for this: the `arrays`/`iterate`/etc. functions can take `aliases`. If the TTree has built-in `fAliases`, then these are used first, but you can also provide them as a user.\r\n\r\n```python\r\n>>> import uproot4\r\n>>> import skhep_testdata\r\n>>> t = uproot4.open(skhep_testdata.data_path(\"uproot-Zmumu.root\"))[\"events\"]\r\n>>> t.arrays([\"p1\", \"p2\"])\r\nTraceback (most recent call last):\r\n...\r\nKeyError: 'p1'\r\n>>> t.arrays([\"p1\", \"p2\"], aliases={\r\n...     \"p1\": \"sqrt(px1**2 + py1**2 + pz1**2)\",\r\n...     \"p2\": \"sqrt(px2**2 + py2**2 + pz2**2)\"\r\n... })\r\n<Array [{p1: 82.2, p2: 60.6}, ... p2: 171}] type='2304 * {\"p1\": float64, \"p2\": f...'>\r\n```\r\n\r\n(There's no magic here: this is just applying the string-based formula as Python code after loading the necessary arrays, but if you build up a dict of aliases, you can hide a lot of formulae in a package.)\r\n\r\nThe second thing is [NanoEvents](https://coffeateam.github.io/coffea/modules/coffea.nanoevents.html): @nsmith- has been writing a wrapper like yours for CMS data, which renames and restructures the TTree without reading it (i.e. lazy reading). It was originally designed for ROOT files with a NanoAOD structure, but it has been generalized to be parameterized by a YAML file, such that two different ROOT file structures have successfully been read with it.\r\n\r\nI doubt the NanoEvents infrastructure can apply custom Interpretations, but it might not be a huge extension.\r\n\r\nFor something like this, I'd rather build on top of Uproot than include it in Uproot, although there is this \"rename_fields\" like behavior through `aliases`, and ideally, Uproot should be coming up with the right Interpretations on its own, though I know you have some strange files.",
  "created_at":"2020-11-16T15:14:27Z",
  "id":728125342,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODEyNTM0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T17:34:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"It's a shame that I have not thought about using the alias functionality, thanks!\r\nI will also look into NanoEvents, it sounds like I can borrow from there, very promising.\r\n\r\nGiven that the core functionality of renaming fields is there, I definitely agree that building upon instead of integrating it is the better choice.",
  "created_at":"2020-11-16T15:47:00Z",
  "id":728145839,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODE0NTgzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T15:47:00Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"One correction, Jim linked to the awkward0 version of NanoEvents. The awkward1 version (which is also the first one to generalize) is https://coffeateam.github.io/coffea/modules/coffea.nanoevents.html\r\nThe idea is that a Schema class provides both the desired form of the awkward array (i.e. the thing one would pass to ak.from_arrayset) as well as the dictionary of behaviors that provide the necessary mixin methods for the named records found in that form. One such example is [TreeMakerSchema](https://github.com/CoffeaTeam/coffea/blob/a5583401173859878b52dea44b14ed6c613aea81/coffea/nanoevents/schemas.py#L297). Then an [underlying shim class](https://github.com/CoffeaTeam/coffea/blob/a5583401173859878b52dea44b14ed6c613aea81/coffea/nanoevents/mapping.py#L24) converts the form keys into uproot branches. The resulting awkward1 array then lazily accesses all branches.\r\nI had proposed to add this entirely string-based data access to uproot4 directly, since a similar mechanism of building lazy awkward arrays is already in uproot (but it keeps a reference to the python TBranch object, something I did not want, as I wanted to be able to put a persistent caching layer on top of the uproot shim). As mentioned by Jim, the Schema class instances (modulo behaviors) are pure-json objects and could be saved as yaml in principle.",
  "created_at":"2020-11-16T17:31:16Z",
  "id":728211656,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODIxMTY1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T17:35:18Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Jim linked to the awkward0 version of NanoEvents.\r\n\r\nYikes! I changed the link above to the new NanoEvents so that no one clicks through by mistake.",
  "created_at":"2020-11-16T17:34:53Z",
  "id":728213667,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyODIxMzY2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-16T17:34:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Alright, many thanks for both of you, I think I a have all the tools and inspiration to move forward. \r\n\r\nI'll close the issue now.",
  "created_at":"2020-11-20T17:49:38Z",
  "id":731312829,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTMxMjgyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T17:49:38Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@tamasgal any chance you can re-use NanoEvents for your case? Happy to collaborate on it.",
  "created_at":"2020-11-20T18:03:57Z",
  "id":731324085,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTMyNDA4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T18:03:57Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I agree that generalizing NanoEvents across experiments would be a great thing. It provides something unique that should sit at a layer above Awkward Array and Uproot, but still not be tied to a specific experiment (or TTree format, which it's already broken free of).",
  "created_at":"2020-11-20T18:08:38Z",
  "id":731328330,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTMyODMzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T18:08:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Indeed, I think that's a good idea in general. I just need to understand how NanoEvents works and how it maps to our use-case. We already have lots of code based on the current API and if it's possible to replace the inner parts, that would of course be great.\r\n\r\nDo you think that the nanoevents part of coffea could be extracted, so that we can minimise dependencies?",
  "created_at":"2020-11-20T18:49:39Z",
  "id":731347061,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTM0NzA2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T18:49:39Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"For sure in the long run I would like to (plus a name change...). In the mean time, I would imagine your existing dependencies mostly overlap https://github.com/CoffeaTeam/coffea/blob/master/setup.py#L58-L74 but if those look too onerous do let me know (and which ones! we could try to slim a bit)",
  "created_at":"2020-11-20T19:46:22Z",
  "id":731372937,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTM3MjkzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T19:46:22Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah ok, yes that seems quite huge compared to our low level I/O https://git.km3net.de/km3py/km3io/-/blob/master/requirements/install.txt \ud83d\ude09 \r\n\r\nThe library is really just a thin wrapper, in the sense that for a specific type of ROOT file, we only define the mappings for a branch and subbranches using a `BranchMapper` class (https://git.km3net.de/km3py/km3io/-/blob/6f893c07beae7addacb6b2db62e4e37391b04653/km3io/rootio.py#L13)  seen in action here:  https://git.km3net.de/km3py/km3io/-/blob/6f893c07beae7addacb6b2db62e4e37391b04653/km3io/offline.py#L25\r\nAnd after then initialse a subclass of `Branch` in our `*Reader` classes, like here https://git.km3net.de/km3py/km3io/-/blob/6f893c07beae7addacb6b2db62e4e37391b04653/km3io/offline.py#L205\r\n\r\nThat will then take over the mapping, tab-completion and index propagation as shown in the above example, by manipulating via `getattr`, `getitem`, `getkey` and alike, as you can see here https://git.km3net.de/km3py/km3io/-/blob/6f893c07beae7addacb6b2db62e4e37391b04653/km3io/rootio.py#L169\r\nThe indices are all collected and finally unfolded and passed to the actual branch, directly coming from uproot (vai `unfold_indices`: https://git.km3net.de/km3py/km3io/-/blob/6f893c07beae7addacb6b2db62e4e37391b04653/km3io/tools.py#L31)\r\n\r\nThis is basically what then provides the above mentioned API:\r\n\r\n```python\r\nr = WhateverReader(\"file.root\")\r\nr.events.hits.pmt_id   # returns a jagged array of PMT IDs for each event\r\nr.events[2:5].hits.dom_id  # a jagged array of PMT IDs for a slice of events\r\nr.events[:10].hits.dom_id[:5]  # a jagged array of the the first 5 DOM IDs of the first 10 events\r\n```\r\n\r\nSo in principle, everything is fairly simple and you see a lot of traces for some hacks, like the forced conversion to awkward arrays from uproot3 for specific branches (https://git.km3net.de/km3py/km3io/-/blob/6f893c07beae7addacb6b2db62e4e37391b04653/km3io/rootio.py#L37) or other cache/lazy related optimisations. Many of them are obsolete when transitioning to uproot4.\r\n\r\nIf I understood correctly, in `nanoevents` the mapper thing is basically the `TreeMakerSchema` (https://github.com/CoffeaTeam/coffea/blob/a5583401173859878b52dea44b14ed6c613aea81/coffea/nanoevents/schemas.py#L297) and I also see some mappings in `NanoAODSchema`: https://github.com/CoffeaTeam/coffea/blob/a5583401173859878b52dea44b14ed6c613aea81/coffea/nanoevents/schemas.py#L132 which are very similar to those we need, I guess.\r\nI am trying  to wrap my head around and find out how I could implement a scheme for our files.\r\n\r\nHere is a type of a ROOT file which does not use custom interpretations, so this should be I think easy to map in `NanoEvents`. This is how the file looks like in uproot4 (I shortened the list of keys, for the sake of simplicity):\r\n\r\n```python\r\n>>> import uproot4\r\n>>> from km3net_testdata import data_path\r\n>>> f = uproot4.open(data_path(\"offline/km3net_offline.root\"))\r\n>>> f[\"E/Evt\"].keys()\r\n['AAObject',\r\n 'AAObject/TObject',\r\n 'AAObject/TObject/fUniqueID',\r\n 'AAObject/TObject/fBits',\r\n 'AAObject/usr_data',\r\n 'AAObject/usr_data/usr_data.holder',\r\n 'AAObject/usr_names',\r\n 'id',\r\n 'det_id',\r\n 'mc_id',\r\n 'run_id',\r\n 'mc_run_id',\r\n 'frame_index',\r\n 'trigger_mask',\r\n 'trigger_counter',\r\n 'overlays',\r\n 't',\r\n 't/t.fSec',\r\n 't/t.fNanoSec',\r\n 'hits',\r\n 'hits/hits.id',\r\n 'hits/hits.dom_id',\r\n 'hits/hits.channel_id',\r\n 'hits/hits.tdc',\r\n 'hits/hits.tot',\r\n 'hits/hits.trig',\r\n 'hits/hits.pmt_id',\r\n 'hits/hits.t',\r\n 'hits/hits.a',\r\n 'hits/hits.pos.x',\r\n 'hits/hits.pos.y',\r\n 'hits/hits.pos.z',\r\n 'hits/hits.dir.x',\r\n 'hits/hits.dir.y',\r\n 'hits/hits.dir.z',\r\n 'hits/hits.pure_t',\r\n 'hits/hits.pure_a',\r\n 'hits/hits.type',\r\n 'hits/hits.origin',\r\n 'hits/hits.pattern_flags',\r\n 'trks',\r\n 'trks/trks.fUniqueID',\r\n 'trks/trks.fBits',\r\n 'trks/trks.usr_data',\r\n 'trks/trks.usr_names',\r\n 'trks/trks.id',\r\n 'trks/trks.pos.x',\r\n 'trks/trks.pos.y',\r\n 'trks/trks.pos.z',\r\n 'trks/trks.dir.x',\r\n 'trks/trks.dir.y',\r\n 'trks/trks.dir.z',\r\n 'trks/trks.t',\r\n 'trks/trks.E',\r\n 'trks/trks.len',\r\n 'trks/trks.lik',\r\n 'trks/trks.mother_id',\r\n 'trks/trks.fitinf',\r\n 'trks/trks.hit_ids',\r\n 'trks/trks.error_matrix',\r\n 'w',\r\n 'index',\r\n 'flags']\r\n```\r\n\r\nSo, the entry at `\"E/Evt\"` is our vector of events, which consist of `hits` and `trks` and other parameters. That is what we translate to `f.events` and `f.events.hits.dir_x` etc (which points to `f[\"E/Evt/hits/hits.dir.x\"].lazyarray(options...)` etc.).\r\n\r\nThis allows to do the fancy indexing like mentioned above and other stuff like handy iteration:\r\n\r\n```python\r\nfor event in f.events:\r\n    calib_hits = calibrate(event.hits, calibration)\r\n    ...\r\n```\r\n\r\nAs you can see, it's all about grouping things together and offering a user-friendly interface (tab completion,  hiding noisy stuff etc.):\r\n\r\n![Screenshot 2020-11-20 at 21 20 03](https://user-images.githubusercontent.com/1730350/99846340-7096d300-2b76-11eb-9eba-ae95d5a7733a.png)\r\n![Screenshot 2020-11-20 at 21 20 11](https://user-images.githubusercontent.com/1730350/99846350-71c80000-2b76-11eb-8ecc-175f5f3668ea.png)\r\n![Screenshot 2020-11-20 at 21 21 42](https://user-images.githubusercontent.com/1730350/99846353-71c80000-2b76-11eb-8b84-c931b4e8670b.png)\r\n\r\nDo you think that's easy to do with `NanoEvents`?",
  "created_at":"2020-11-20T20:24:34Z",
  "id":731390079,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTM5MDA3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T20:24:34Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I just realised, it's a flood of source code links, sorry for that \ud83d\ude48 but maybe it's easier to communicate the needs and implementations.",
  "created_at":"2020-11-20T20:25:20Z",
  "id":731390406,
  "issue":192,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTM5MDQwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T20:25:20Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"This is a reasonable feature request, but I don't see how that could have worked in Uproot 3. There was never any code for asking XRootD to match wildcards. Both versions pass the string with all characters unmodified to the server (except that \"`:Data`\" gets stripped off). Even if the server could resolve the wildcards to a set of files, both versions of Uproot have assumed that what you get back from the server is one file, not multiple. (I'm not sure how that would be represented.)\r\n\r\nAre you sure that this worked in Uproot 3? I'm happy to make it a feature request for Uproot 4, but I'm confused about how it might have accidentally worked in Uproot 3, if at all.",
  "created_at":"2020-11-18T15:45:03Z",
  "id":729766245,
  "issue":193,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTc2NjI0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-18T15:45:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"A temporary solution until this added as a feature you can get xrootd files paths with wildcarding using `XRootD.client.glob_funcs.glob` and then pass the resulting array of xrootd files paths into `uproot.iterate` and it works.\r\n",
  "created_at":"2020-11-18T17:06:38Z",
  "id":729818482,
  "issue":193,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTgxODQ4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-18T17:10:40Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "author_association":"MEMBER",
  "body":"Your workaround can be the basis of adding this feature in the future.\r\n\r\nBut as for Uproot 3, if it ever worked before, it was an accident\u2014\"broken\" doesn't describe the lack of that feature now.",
  "created_at":"2020-11-18T17:12:58Z",
  "id":729822587,
  "issue":193,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTgyMjU4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-18T17:12:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks! Sorry if I misremembered :-) I'll edit the OP.\r\nIt would be a very useful feature.",
  "created_at":"2020-11-18T18:10:22Z",
  "id":729861125,
  "issue":193,
  "node_id":"MDEyOklzc3VlQ29tbWVudDcyOTg2MTEyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-18T18:10:22Z",
  "user":"MDQ6VXNlcjYyOTMwMTE="
 },
 {
  "author_association":"NONE",
  "body":"I made the change bellow and plotted tree_sel[0]['D_MM'] and tree_sel[1]['D_MM'] histograms, the cuts were only applied in the first one,\r\n\r\n`\r\ntree_sel = []\r\nCuts = \"(D_MM>1910) & (D_MM<2030)\"\r\n\r\nfor x,report in tree.iterate(entry_stop=100000,step_size=50000,cut=Cuts,report=True):\r\n    print(report)    \r\n    tree_sel.append(x)\r\n\r\n`\r\n:/",
  "created_at":"2020-11-19T04:34:14Z",
  "id":730123217,
  "issue":194,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMDEyMzIxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-19T04:34:14Z",
  "user":"MDQ6VXNlcjM1NzQzNzc3"
 },
 {
  "author_association":"MEMBER",
  "body":"You found the issue: it's not \"`and`\", it's \"`&`\", which is a Python+NumPy thing that is simply something we'll have to get used to. In Python, we can't overload \"`and`\", so NumPy and Awkward Array overload \"`&`\". [This update](https://github.com/scikit-hep/awkward-1.0/pull/539/commits/cfde3c5490c87dc88c689a40ced749becdeb3f46) will improve the situation by explicitly overloading the `__bool__` method with an exception the way NumPy does:\r\n\r\n```python\r\n>>> # allowed\r\n>>> ak.Array([True, False, True]) & ak.Array([False, True, True])\r\n<Array [False, False, True] type='3 * bool'>\r\n\r\n>>> # not allowed\r\n>>> ak.Array([True, False, True]) and ak.Array([False, True, True])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/awkward-1.0/awkward1/highlevel.py\", line 1440, in __bool__\r\n    raise ValueError(\r\nValueError: the truth value of an array whose length is not 1 is ambiguous; use ak.any() or ak.all()\r\n```\r\n\r\nThis PR was included in `awkward1==0.4.4`, so upgrading to the latest version should provide this error message, at least.\r\n\r\nOn the second point, is the cut only being applied to the first file or the first step in iteration? Can you be explicit and provide a reproducer?\r\n\r\nHere's a test I just ran:\r\n\r\n```python\r\n>>> for arrays in uproot4.iterate(\"Zmumu*.root:events\", [\"px1\"]): print(arrays)\r\n... \r\n[{px1: -41.2}, {px1: 35.1}, {px1: 35.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: -41.2}, {px1: 35.1}, {px1: 35.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: -41.2}, {px1: 35.1}, {px1: 35.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: -41.2}, {px1: 35.1}, {px1: 35.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: -41.2}, {px1: 35.1}, {px1: 35.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: -41.2}, {px1: 35.1}, {px1: 35.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n>>> for arrays in uproot4.iterate(\"Zmumu*.root:events\", [\"px1\"], cut=\"px1 > 0\"): print(arrays)\r\n... \r\n[{px1: 35.1}, {px1: 35.1}, {px1: 34.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: 35.1}, {px1: 35.1}, {px1: 34.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: 35.1}, {px1: 35.1}, {px1: 34.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: 35.1}, {px1: 35.1}, {px1: 34.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: 35.1}, {px1: 35.1}, {px1: 34.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n[{px1: 35.1}, {px1: 35.1}, {px1: 34.1}, ... {px1: 32.4}, {px1: 32.4}, {px1: 32.5}]\r\n```\r\n\r\nThe cut applied to all of these files, eliminating the negative `px1` in all cases. (The files are different compressions of the same data, for tests.)",
  "created_at":"2020-11-19T15:59:08Z",
  "id":730470432,
  "issue":194,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMDQ3MDQzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-19T15:59:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski,\r\n\r\nEach step of the iterator produces a batch array with events selected by the cuts if I understand correctly the tutorial. I'm saving these batches in the tree_sel list and plotting the invariant mass distribution of each batch (2)  to check if the cut has been applied.\r\n\r\nThe problem is that the cuts seem to be only applied in the batch returned in the first step. The second batch still untouched.\r\n\r\nAs I'm using LHCb data, I think I can't provide a reproducer here. But, I can show you in a zoom call if it ok.\r\n\r\nThanks!",
  "created_at":"2020-11-19T16:25:04Z",
  "id":730486675,
  "issue":194,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMDQ4NjY3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-19T16:25:04Z",
  "user":"MDQ6VXNlcjM1NzQzNzc3"
 },
 {
  "author_association":"MEMBER",
  "body":"I'm in a Zoom meeting, and I think it's technically impossible to run two at once. (I _have_ used Zoom and Vidyo at the same time, but that gets complicated fast!)\r\n\r\nAlso, debugging through Zoom is going to be hard, since I wouldn't be able to touch the code. It sounds like your procedure for identifying this is complex\u2014the first thing we'd have to do anyway is break it down to focus just on Uproot itself.\r\n\r\nThe `cut` string does nothing more than putting the code from the string inside square brackets of the result, so\r\n\r\n```python\r\nCuts = \"(D_MM>1910) & (D_MM<2030)\"\r\nfor x,report in tree.iterate(entry_stop=100000,step_size=50000,cut=Cuts,report=True):\r\n```\r\n\r\nshould be entirely equivalent to\r\n\r\n```python\r\nfor arrays,report in tree.iterate(entry_stop=100000,step_size=50000,report=True):\r\n    x = arrays[(arrays.D_MM>1910) & (arrays.D_MM<2030)]\r\n```\r\n\r\nIf this isn't true, then there's some bug with `cut`.\r\n\r\n---------\r\n\r\nIn writing this response, I noticed a difference between my example and yours\u2014mine used `uproot4.iterate` and yours used `ttree.iterate`. Having created a reproducer, I found the issue and fixed it in PR #195. That will get deployed as a new version relatively soon, as we're doing the name transition Dec 1.\r\n",
  "created_at":"2020-11-19T17:29:40Z",
  "id":730525028,
  "issue":194,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMDUyNTAyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-19T17:29:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski ,\r\n\r\nThe equivalent way is working fine!\r\n\r\nThank you very much! :)\r\n",
  "created_at":"2020-11-19T17:47:34Z",
  "id":730535378,
  "issue":194,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMDUzNTM3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-19T17:47:34Z",
  "user":"MDQ6VXNlcjM1NzQzNzc3"
 },
 {
  "author_association":"MEMBER",
  "body":"@henryiii This is intended to generalize the histogram protocol.\r\n\r\nThese are tests, demonstrating everything with the histograms I have in scikit-hep-testdata:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/jpivarski/use-the-common-histogram-interface/tests/test_0167-use-the-common-histogram-interface.py\r\n\r\nThe TAxis behaviors are defined here, implementing this protocol and the Sequence protocol (because I think that was desired):\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/jpivarski/use-the-common-histogram-interface/uproot4/behaviors/TAxis.py\r\n\r\nHistograms are defined in files for each class, but an abstract class is defined here in the file for TH1, as a way of getting docstrings to all the others. (Uproot 4 does not follow ROOT's class hierarchy, giving us more freedom in behavior implementations, like not having TH2 inherit from TH1, etc.)\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/jpivarski/use-the-common-histogram-interface/uproot4/behaviors/TH1.py\r\n\r\nThis abstract class is mostly overwritten by the one for profiles, since the docstrings have to be very different.\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/jpivarski/use-the-common-histogram-interface/uproot4/behaviors/TProfile.py\r\n\r\nThere's a proliferation of variance methods: `errors`, `variances`, `values_errors`, and `values_variances`. This is because regular histograms use `values` as part of the calculation for `variances`, which can be used to derive `errors`, and profiles use `values` as part of the calculation of `errors`, which can be used to derive `variances`. The methods that return two arrays, values and something else, are primary; the others are conveniences derived from them.\r\n\r\nThere are two **FIXMEs** in the the profile implementation, one for filling `counts` (I'm unclear about what it means) and the other for building a Boost histogram representing the profile. I'm 100% certain of the calculation from ROOT's C++ members to values and variances, but I'm unsure how to fill Boost. To start with, shouldn't a profile be \"WeightedMeanVariance Storage,\" not \"WeightedMean Storage\"?",
  "created_at":"2020-11-19T23:16:27Z",
  "id":730697317,
  "issue":196,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMDY5NzMxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-19T23:16:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Upon further digging I found this magic function called uproot.numetries() in uproot3. Any reasons this was removed in this version?",
  "created_at":"2020-11-20T06:18:07Z",
  "id":730878326,
  "issue":197,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMDg3ODMyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T06:18:36Z",
  "user":"MDQ6VXNlcjQwODQxNDQ0"
 },
 {
  "author_association":"MEMBER",
  "body":"The difference between 0.7 seconds and 2 seconds would be considered \"in the noise,\" in the sense that the variation in opening times among different files is a lot more than a factor of 2. (Arguably, the means of these two overlapping distributions might be noticeably shifted, but we can't learn that from one example.)\r\n\r\nHowever, it sounds like you have been getting good use out of Uproot 3's `uproot.numentries` function, which is a hacked file+tree opening path that takes advantage of the fact that `fEntries` is serialized at the beginning of a TTree's byte stream to read that and bail out before getting to the TBranches. If you have a lot of TBranches and need to scan a lot of files to get each one's number of entries, then this can be a time-saver. (I don't know exactly how much, as that would depend on the file, particularly the number of TBranches.)\r\n\r\nI also didn't know that anyone was getting a lot of value out of this function, but now I do. I can take this issue to be a feature request for a `uproot4.num_entries` function, which I could implement the same way. In the meantime, you can use Uproot 3 to get the number of entries and Uproot 4 for everything else, until the function is ported. That's why they have different names, so that you can get the best of both while asking for features to be ported.",
  "created_at":"2020-11-20T13:46:43Z",
  "id":731179497,
  "issue":197,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTE3OTQ5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T13:46:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I also welcome this feature as we happen to do similar things (iterating over tons of files to gather some statistics and prepare some heavy processing). I also used that shortcut implementation in uproot3 \ud83d\ude09\r\n\r\nI am wondering if it's in general possible (in the sense that it's not too much work to implement at this stage) to somehow separate the initialisation in two parts, the one which just takes care of reading in the streamers and some metadata, and then the second part which actually initialises the streamer logic.\r\n\r\nAt least in our use-case, we often want to peek into a huge amount of files and extract a bit more information which do not require to read anything from baskets.\r\nOf course, the bootstrapping part of uproot will always \"suffer\" a bit from the Python overhead. I see e.g. a factor of ~10 compared to the Julia reader when accessing the `numentries` of the example file `HZZ.root` from the OP, compared to uproot3 which takes 37ms on the same machine. The `UnROOT` package however does not construct any streamer logic yet, so it's really just the parsing of the TKeys, streamers and meta stuff, so I only show it to set some kind of a lower limit. I think `uproot` can reach that performance since I/O in Python is generally quite good:\r\n\r\n```julia\r\njulia> using UnROOT, BenchmarkTools\r\njulia> @btime ROOTFile(\"/home/tgal/data/tmp/HZZ.root\")[\"events\"].fEntries\r\n  3.455 ms (17430 allocations: 1.40 MiB)\r\n2421\r\n```\r\n\r\n```pythonn\r\n>>> import uproot\r\n\r\n>>> %timeit uproot.open(\"/home/tgal/data/tmp/HZZ.root\")[\"events\"].numentries\r\n37.4 ms \u00b1 1.04 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\n...on the other hand, it might be that it's not worth to bother with this due to the rare use-cases and only introduce shortcuts on demand, like this. I have not ported that part of our processing to `uproot4` yet, but it might be that other things might have slowed down due to the redesign.",
  "created_at":"2020-11-20T18:22:28Z",
  "id":731334695,
  "issue":197,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTMzNDY5NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2020-11-20T18:22:28Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"One of the optimizations that `uproot3.numentries` did is to avoid reading streamers, which can be very large, taking advantage of the fact that `fEntries` is one of the first serialized values, before all the version-dependent differences that require knowledge from the streamers.\r\n\r\nUproot 4 skips reading the streamers until a particular streamer is needed, and it comes with pre-defined streamers for all the TTree versions from the past 5 years, so it already has that advantage. One additional thing that the `uproot3.numentries` function did (and `uproot4.num_entries` should do) is that it skipped reading all of the TBranch definitions, and that must be the difference @bsathian is seeing.\r\n\r\nBoth Uproots already skip reading any embedded TBaskets unless necessary, and neither would ever read TBasket data unless requested by the `array`, `arrays`, `iterate`, etc. methods. So if your interest is\r\n\r\n   * avoid reading streamers\r\n   * _do_ read TBranch metadata (names and types)\r\n   * avoid reading any TBasket data (embedded or otherwise)\r\n\r\nUproot 4 already does that. There isn't anything else to shave off. The best we can do for gathering TBranch names from a file is to open it like normal. If you have a large dataset and you want to get the number of entries in each file and the names and types of the TBranches, you could try `uproot3.numentries` on all the files but fully open only the first (_assuming_ they have consistent branch names and types).",
  "created_at":"2020-11-20T18:37:24Z",
  "id":731341420,
  "issue":197,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTM0MTQyMA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2020-11-20T18:37:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It is not a simple transpose, of course, the displacement gets more bizarre if `h` is not a square matrix.",
  "created_at":"2020-11-20T13:29:44Z",
  "id":731170833,
  "issue":198,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTE3MDgzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T13:29:44Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm working on the histogram interface right now (PR #196), so I wasn't sure that this would still be true. Here's my reproducer (using a non-square histogram\u2014unfortunately, all the examples in scikit-hep-testdata are square):\r\n\r\n```python\r\n>>> import ROOT\r\n>>> h = ROOT.TH2D(\"h\", \"\", 5, -10, 10, 7, -28, 28)\r\n>>> h.SetBinContent(1, 1, 1)\r\n>>> h.SetBinContent(1, 2, 2)\r\n>>> h.SetBinContent(2, 1, 3)\r\n>>> h.SetBinContent(2, 2, 4)\r\n>>> h.SetBinError(1, 1, 1)\r\n>>> h.SetBinError(1, 2, 2)\r\n>>> h.SetBinError(2, 1, 3)\r\n>>> h.SetBinError(2, 2, 4)\r\n>>> f = ROOT.TFile.Open(\"issue-198.root\", \"recreate\")\r\n>>> h.Write()\r\n302\r\n>>> f.Close()\r\n```\r\n\r\nReading it back in Uproot (PR #196, `flow=False` is now the default):\r\n\r\n```python\r\n>>> import uproot4\r\n>>> h = uproot4.open(\"issue-198.root:h\")\r\n>>> h.values()\r\narray([[1., 2., 0., 0., 0., 0., 0.],\r\n       [3., 4., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.]])\r\n>>> h.errors()\r\narray([[1., 2., 0., 0., 0., 0., 0.],\r\n       [3., 4., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.]])\r\n>>> hb = h.to_boost()\r\n>>> hb.view().value\r\narray([[1., 2., 0., 0., 0., 0., 0.],\r\n       [3., 4., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.]])\r\n>>> hb.view().variance**0.5\r\narray([[0., 0., 0., 0., 0., 2., 4.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.]])\r\n```\r\n\r\nYup. There it is. This is probably related to #93. After fixing this, I'll check across all the histogram types to be sure the transposition is applied generally.",
  "created_at":"2020-11-20T14:35:18Z",
  "id":731206039,
  "issue":198,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTIwNjAzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T14:35:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This commit, afe894056ef73dfca550da5e5f287534c2e6ecbf, results in:\r\n\r\n```python\r\n>>> import uproot4\r\n>>> h = uproot4.open(\"issue-198.root:h\")\r\n>>> h.values()\r\narray([[1., 2., 0., 0., 0., 0., 0.],\r\n       [3., 4., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.]])\r\n>>> h.errors()\r\narray([[1., 2., 0., 0., 0., 0., 0.],\r\n       [3., 4., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.]])\r\n>>> hb = h.to_boost()\r\n>>> hb.view().value\r\narray([[1., 2., 0., 0., 0., 0., 0.],\r\n       [3., 4., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.]])\r\n>>> hb.view().variance**0.5\r\narray([[1., 2., 0., 0., 0., 0., 0.],\r\n       [3., 4., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0.]])\r\n```\r\n",
  "created_at":"2020-11-20T14:54:21Z",
  "id":731216508,
  "issue":198,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTIxNjUwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T14:54:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I also made sure that all output arrays are in native endianness. (I think boost-histogram required that, but it's good for `values` and `errors` to return the same thing.)",
  "created_at":"2020-11-20T14:55:50Z",
  "id":731217374,
  "issue":198,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTIxNzM3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T14:55:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thank you for the immediate fix! Yes, we require native endianness, but so does ROOT itself, no? The memory buffer in a TH* is always in native endianness.\r\n\r\nAlso good to hear that flow=False is the default.",
  "created_at":"2020-11-21T09:36:50Z",
  "id":731554603,
  "issue":198,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTU1NDYwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-21T09:36:50Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"Should I try out the develop branch to see whether this fixed my problem before we close this?",
  "created_at":"2020-11-21T10:37:57Z",
  "id":731560946,
  "issue":198,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTU2MDk0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-21T10:37:57Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"This and #167 are both linked to #196, so they'll be closed when that gets merged. I'm attempting to get the histogram interface finalized\u2014it would be a generalization of what you and @henryiii are converging to (i.e. additional methods and arguments are allowed).\r\n\r\nYou can try out the development branch, but be aware that the interface has changed in it.",
  "created_at":"2020-11-21T14:55:46Z",
  "id":731590138,
  "issue":198,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTU5MDEzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-21T14:55:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The reason a test was removed is because it relied on the previous brokenness of its instance versions, which would have been corrected by reading its streamers. Now, though, we switch to the right version preemptively, without relying on a deserialization error to kick us back to the streamers. There are now fewer cases that read streamers, but the test was hard-coded to expect it to have read streamers. I don't know of a case to replace it with.",
  "created_at":"2020-11-20T20:14:14Z",
  "id":731385521,
  "issue":199,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTM4NTUyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T20:14:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This first commit fixes #121.",
  "created_at":"2020-11-20T22:31:19Z",
  "id":731440861,
  "issue":201,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMTQ0MDg2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-20T22:31:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Have to stop for now. Will finish this after the name change.",
  "created_at":"2020-11-30T14:02:38Z",
  "id":735804194,
  "issue":202,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNTgwNDE5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-30T14:02:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for finding this! Not only did fixing it patch an error, it reminded me of some cases that have to be considered as part of the string validity-checking (scikit-hep/awkward-1.0#404).\r\n\r\n```python\r\n>>> import uproot4\r\n>>> f = uproot4.open(\"sample.root\")\r\n>>> f[\"E/Evt/AAObject/usr_names\"].array()\r\n<Array [[], [], [], [], ... [], [], [], []] type='307 * var * string'>\r\n>>> f[\"E/Evt/AAObject/usr_names\"].array().layout\r\n<ListOffsetArray64>\r\n    <offsets><Index64 i=\"[0 0 0 0 0 ... 0 0 0 0 0]\" offset=\"0\" length=\"308\" at=\"0x55ec6df39d20\"/></offsets>\r\n    <content><ListOffsetArray64>\r\n        <parameters>\r\n            <param key=\"__array__\">\"string\"</param>\r\n        </parameters>\r\n        <offsets><Index64 i=\"[0]\" offset=\"0\" length=\"1\" at=\"0x55ec6dbe03e0\"/></offsets>\r\n        <content><NumpyArray format=\"B\" shape=\"0\" data=\"\" at=\"0x55ec6db2d980\">\r\n            <parameters>\r\n                <param key=\"__array__\">\"char\"</param>\r\n            </parameters>\r\n        </NumpyArray></content>\r\n    </ListOffsetArray64></content>\r\n</ListOffsetArray64>\r\n>>> import awkward1 as ak\r\n>>> ak.flatten(f[\"E/Evt/AAObject/usr_names\"].array())\r\n<Array [] type='0 * string'>\r\n```",
  "created_at":"2020-11-25T17:07:14Z",
  "id":733834419,
  "issue":205,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzgzNDQxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-25T17:07:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Lightning fast as alwas, thanks :)",
  "created_at":"2020-11-25T17:17:55Z",
  "id":733840332,
  "issue":205,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczMzg0MDMzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-25T17:17:55Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks, @henryiii! In general, I try to find data samples \"in the wild,\" which means that it may be some time before we have an instance of a TProfile2D or TProfile3D. When this does come up, we'll have examples to generalize.",
  "created_at":"2020-11-30T14:16:01Z",
  "id":735811632,
  "issue":207,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNTgxMTYzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-11-30T14:16:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This resolves a long-standing mystery. This file has what I call an \"embedded TBasket,\" which comes from the file-writing process closing early. I am unable to make these files on my own\u2014I don't know the circumstances that can lead to early file-closing\u2014but I've come across a few of them \"in the wild.\" Yours is the sixth; the others are\r\n\r\n   * uproot-issue21.root (not jagged)\r\n   * uproot-issue327.root (jagged)\r\n   * uproot-issue232.root (jagged)\r\n   * uproot-issue187.root (jagged)\r\n   * uproot-from-geant4.root (jagged)\r\n\r\nYours is only the second non-jagged example. The code I used to determine whether an embedded TBasket is jagged or not was therefore underdetermined: I knew it had something to do with `fNevBufSize` being a small value vs a large value, but I thought that `8` was the magic number (there are two 4-byte fields in a TBasket, so it's plausible). Yours was `12` and yet it clearly should not be jagged. (The information used to determine whether it's jagged is very clear elsewhere in the file, but that information is not available yet at the stage where this has to be deserialized.)\r\n\r\nYour `fNevBufSize` is `12` because your entry size is 12 bytes: `3 * sizeof(float)` for your lepton directions. That led me to the actual ROOT documentation:\r\n\r\nhttps://github.com/root-project/root/blob/0e6282a641b65bdf5ad832882e547ca990e8f1a5/tree/tree/inc/TBasket.h#L62-L65\r\n\r\nSo now I know that the threshold of `8` was very naive; it should be\r\n\r\n```diff\r\n--- a/uproot/models/TBasket.py\r\n+++ b/uproot/models/TBasket.py\r\n@@ -235,7 +235,12 @@ class Model_TBasket(uproot.model.Model):\r\n         cursor.skip(1)\r\n \r\n         if self.is_embedded:\r\n-            if self._members[\"fNevBufSize\"] > 8:\r\n+            maybe_entry_size = self._members[\"fNevBufSize\"]\r\n+            num_entries = self._members[\"fNevBuf\"]\r\n+            key_length = self._members[\"fKeylen\"]\r\n+\r\n+            if maybe_entry_size * num_entries + key_length != self._members[\"fLast\"]:\r\n                 raw_byte_offsets = cursor.bytes(\r\n                     chunk, 8 + self.num_entries * 4, context\r\n                 ).view(_tbasket_offsets_dtype)\r\n```\r\n\r\nAnd with that, we can read your data:\r\n\r\n```python\r\n>>> uproot.open(\"uproot-issue-208.root\")[\"truth/truelepton_dir\"].array(library=\"np\")\r\narray([[ 0.06063091, -0.48152107,  0.8743349 ],\r\n       [-0.05262217,  0.45292157,  0.8899961 ],\r\n       [-0.33558974, -0.09119339,  0.93758374],\r\n       ...,\r\n       [ 0.3563597 ,  0.4576636 ,  0.814587  ],\r\n       [ 0.01233674,  0.03180967,  0.99941784],\r\n       [ 0.41140354,  0.4344518 ,  0.80124825]], dtype=float32)\r\n```\r\n\r\nThanks!",
  "created_at":"2020-12-05T00:31:44Z",
  "id":739090482,
  "issue":208,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTA5MDQ4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T00:31:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hello jpivarski,\r\nMany many thanks for looking into this and implementing the solution. This has helped me a lot!\r\n\r\nThanks for your hard work!!",
  "created_at":"2020-12-17T15:12:15Z",
  "id":747498232,
  "issue":208,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzQ5ODIzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-17T15:12:15Z",
  "user":"MDQ6VXNlcjQwNzg3NDcx"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for that. I didn't realize that TEfficiency would have memberwise data in it, but that ties this issue in with #38. Memberwise deserialization will be implemented for `std::vector` first, using this file as a test-driven example.",
  "created_at":"2020-11-30T20:43:51Z",
  "id":736031687,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczNjAzMTY4Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-11-30T20:43:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I can reproduce it with:\r\n\r\n```python\r\n>>> file = uproot.open(\"uproot-issue-210.root\")\r\n>>> file[\"CollectionTree/VrtSecInclusive_SecondaryVertices\"].interpretation\r\n```\r\n\r\nIt's not in the array-reading, it's in determining the interpretation. I'm looking into it.",
  "created_at":"2020-12-04T20:39:47Z",
  "id":739007522,
  "issue":210,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTAwNzUyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-04T20:39:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've fixed the infinite recursion, but the branch still can't be read. At least you'll be able to `show` that TTree and read the other branches.\r\n\r\nThe issue is that the type technically contains itself:\r\n\r\n```python\r\n>>> f = uproot.open(\"uproot-issue-210.root\")\r\n>>> f.file.streamer_named(\"DataVector<xAOD::Vertex_v1>\").show()\r\nDataVector<xAOD::Vertex_v1> (v1)\r\n    This: DataVector<xAOD::Vertex_v1> (TStreamerSTL)\r\n```\r\n\r\nHowever, my interpretation of this as a class with one member named \"`This`\" whose type is the class is probably wrong. The single member named \"`This`\" is probably intended to be a signal to ROOT to interpret this some other way. Maybe \"`DataVector`\" is a typedef for \"`std::vector`\"? If so, then this is the wrong interpretation.\r\n\r\nI checked to see if Uproot 3 had a special interpretation for it that simply hasn't been ported over to Uproot 4: it doesn't.\r\n\r\nThis will probably have to be revisited. However, the first thing that happened when I tried reading this is that it complained about \"memberwise splitting\" not being implemented, so I'll probably need to do #38 first, at least for `std::vectors` (if that's what `DataVector` is). #209 also needs memberwise `std::vector` interpretation.",
  "created_at":"2020-12-04T21:50:59Z",
  "id":739040221,
  "issue":210,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTA0MDIyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-04T21:50:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I should have checked all the cases before. In principle, any type might be an EmptyArray.\r\n\r\nPR #217 ensures that every `if` branch in `_awkward_json_to_array` gets tested for EmptyArray. Also, if an `offsets` type differs from what the Form says it should be, it gets converted.\r\n\r\n```python\r\n>>> f = uproot.open(\"~/storage/data/uproot4-lazy/uproot-issue214.root\")\r\n>>> f[\"E/Evt/mc_trks/mc_trks.usr_names\"].array()\r\n<Array [[], [], [], [], ... [], [], [], []] type='10 * var * var * string'>\r\n```\r\n\r\nAll of this will get unraveled when we start interpreting the data in C++ (#90). Then `ak.from_iter`'s guessed types don't have to be converted into the actual types because they'd be read in the actual types in the first place.",
  "created_at":"2020-12-04T23:21:14Z",
  "id":739073364,
  "issue":214,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTA3MzM2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-04T23:21:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"```bash\r\n% # uproot-issue21.root (not jagged)\r\n% python -c 'import uproot, skhep_testdata; print(uproot.open(skhep_testdata.data_path(\"uproot-issue21.root\"))[\"nllscan/mH\"].array())'\r\n[124, 124, 124, 124, 124, 124, 125, 125, ... 125, 125, 126, 126, 126, 126, 126, 126]\r\n% \r\n% # uproot-issue327.root (jagged)\r\n% python -c 'import uproot, skhep_testdata; print(uproot.open(skhep_testdata.data_path(\"uproot-issue327.root\"))[\"DstTree/fTracks.fCharge\"].array())'\r\n[[1, -1], [1, 1, -1, -1, 1, -1, -1, -1, -1, ... [-1, -1, 1, -1, 1], [1, -1, 1, 1, 1]]\r\n% \r\n% # uproot-issue232.root (jagged)\r\n% python -c 'import uproot, skhep_testdata; print(uproot.open(skhep_testdata.data_path(\"uproot-issue232.root\"))[\"fTreeV0/V0s.fV0pt\"].array())'\r\n[[], [], [], [], [], [0.432], [], [1.17, ... [], [], [], [], [1.41, 1.46], [], []]\r\n% python -c 'import uproot, skhep_testdata; print(uproot.open(skhep_testdata.data_path(\"uproot-issue232.root\"))[\"fTreeV0/MCparticles.nbodies\"].array())'\r\n[[18], [135, 135, 135, 135, 135, 136, 135], ... [132, 132, 132], [243], [2, 2]]\r\n% \r\n% # uproot-issue187.root (jagged)\r\n% python -c 'import uproot, skhep_testdata; print(uproot.open(skhep_testdata.data_path(\"uproot-issue187.root\"))[\"fTreeV0/V0s.fV0pt\"].array())'\r\n[[], [], [], [], [], [0.432], [], [1.17, ... [], [], [], [], [1.41, 1.46], [], []]\r\n% python -c 'import uproot, skhep_testdata; print(uproot.open(skhep_testdata.data_path(\"uproot-issue187.root\"))[\"fTreeV0/MCparticles.nbodies\"].array())'\r\n[[18], [135, 135, 135, 135, 135, 136, 135], ... [132, 132, 132], [192], [2, 2]]\r\n% \r\n% # uproot-from-geant4.root (jagged)\r\n% python -c 'import uproot, skhep_testdata; print(uproot.open(skhep_testdata.data_path(\"uproot-from-geant4.root\"))[\"Details/numgood\"].array())'\r\n[224]\r\n% python -c 'import uproot, skhep_testdata; print(uproot.open(skhep_testdata.data_path(\"uproot-from-geant4.root\"))[\"TrackedRays/Event\"].array())'\r\n[2, 19, 21, 22, 26, 27, 35, 36, 37, ... 963, 983, 984, 985, 988, 990, 992, 993, 997]\r\n% python -c 'import uproot, skhep_testdata; print(uproot.open(skhep_testdata.data_path(\"uproot-from-geant4.root\"))[\"TrackedRays/phi\"].array())'\r\n[1.81, 3.78, 6.25, 4.76, 2, 4.28, 2.5, ... 2.45, 0.346, 2.42, 3.77, 2.24, 4.31, 5.13]\r\n```\r\n\r\n\r\n```python\r\n>>> uproot.open(\"uproot-issue-208.root\")[\"truth/truelepton_dir\"].array(library=\"np\")\r\narray([[ 0.06063091, -0.48152107,  0.8743349 ],\r\n       [-0.05262217,  0.45292157,  0.8899961 ],\r\n       [-0.33558974, -0.09119339,  0.93758374],\r\n       ...,\r\n       [ 0.3563597 ,  0.4576636 ,  0.814587  ],\r\n       [ 0.01233674,  0.03180967,  0.99941784],\r\n       [ 0.41140354,  0.4344518 ,  0.80124825]], dtype=float32)\r\n```\r\n",
  "created_at":"2020-12-05T00:28:44Z",
  "id":739089836,
  "issue":218,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTA4OTgzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-05T00:28:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@jpivarski I think it is fine. We are currently only debating whether an ordinary unweighted histogram should something .variances() or None, which does not really affect the interface, only how it should behave in the various cases.",
  "created_at":"2020-12-06T09:52:46Z",
  "id":739479781,
  "issue":219,
  "node_id":"MDEyOklzc3VlQ29tbWVudDczOTQ3OTc4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-06T09:52:46Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"The last branch, `\"mu_pt_sum\"`, has byte ranges like\r\n\r\n```\r\n[(308, 18671), (18671, 37017), (165181, 183507), (183507, 201850), (329993, 348355), (436552, 443498)]\r\n```\r\n\r\nbut the HTTP server returned ranges\r\n\r\n```\r\n[(308, 37017), (165181, 201850), (329993, 348355), (436552, 443498)]\r\n```\r\n\r\nThe first four were merged into two because they're contiguous. PR #221 adds a handler for that. Thanks for the report!",
  "created_at":"2020-12-07T22:19:30Z",
  "id":740215003,
  "issue":220,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDIxNTAwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-07T22:19:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks a lot for taking care of it!",
  "created_at":"2020-12-08T14:35:04Z",
  "id":740656264,
  "issue":220,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDY1NjI2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T14:35:04Z",
  "user":"MDQ6VXNlcjE3NDU0ODQ4"
 },
 {
  "author_association":"MEMBER",
  "body":"There was an off-by-one error in the check for variable bins (8d6ef20cafd5e49a6786f4d8064a7afc0defdc09). Thanks!\r\n\r\n```python\r\n>>> import uproot\r\n>>> h = uproot.open(\"uproot-issue-222.root:hist\")\r\n>>> h.axis().edges()\r\narray([0., 1., 3.])\r\n```",
  "created_at":"2020-12-08T16:06:57Z",
  "id":740714628,
  "issue":222,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MDcxNDYyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-08T16:06:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hanging is not good, so this needs to be addressed, but also XRootD with wildcards has not been implemented: #193. You're hitting some behavior that I didn't define in Uproot because one of my assumptions about what XRootD does is broken by its wildcard processing. I'll need to reproduce your issue and adjust.",
  "created_at":"2020-12-10T17:57:46Z",
  "id":742689872,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MjY4OTg3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-10T17:57:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nice! I never knew whether to use `object` or `numpy.object` (or `\"O\"`): this makes it clear, and I'll do the same in Awkward Array. It also clarifies the distinction between `numpy.bool` and `numpy.bool_`.\r\n\r\nAlthough Uproot doesn't formally target Python 2.7, I occasionally verify that it's not broken. The main reason for not targeting 2.7 is the difficulty of getting the testing framework to work. This policy of not officially supporting it, but making sure that it nevertheless works is a balance between helping people who have constrained, legacy systems and not being obliged to do so. Therefore, I'd rather replace `numpy.long` \u2192 `numpy.compat.long`. It's easy and it makes the intention more clear, anyway.",
  "created_at":"2020-12-11T23:34:24Z",
  "id":743481509,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ4MTUwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T23:34:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, let me know when you're done and I'll merge it. Thanks!",
  "created_at":"2020-12-11T23:36:10Z",
  "id":743482040,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ4MjA0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T23:36:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Jim Pivarski <notifications@github.com> writes:\n\n> This policy of not officially supporting it, but making sure that it\n> nevertheless works is a balance between helping people who have\n> constrained, legacy systems and not being obliged to do so.\n\nSounds good!\n\n> Oh, let me know when you're done and I'll merge it. Thanks!\n\nI did a quick search of the repository for the deprecations listed in\nthe NumPy release notes and everything I could find has been fixed. If I\nmissed any perhaps they'll surface when some more folks start testing\nwith the upcoming NumPy release :)\n\ncheers,\nDoug\n",
  "created_at":"2020-12-12T15:45:38Z",
  "id":743774659,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0Mzc3NDY1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-12T15:45:38Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"MEMBER",
  "body":"Great\u2014then it sounds like you're done. I'll merge it!\r\n\r\n(The merging step requires a \"lock\" because it's a concurrency problem\u2014I don't know if you're still working on your local branch and haven't pushed changes. That's why I ask!)",
  "created_at":"2020-12-12T17:11:19Z",
  "id":743785499,
  "issue":226,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0Mzc4NTQ5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-12T17:11:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Great, thanks for this! Since Uproot is an I/O library, we're limited in what we can test by the availability of files with certain features. I can (and have) made some files for testing, but mostly for corner cases. Files that come from real analyses are more valuable because there are a lot of knobs to turn in generating ROOT files, and we want to prioritize the ones that come from real analyses.\r\n\r\nTProfile2D isn't being tested. You caught some (after the fact) obvious bugs and I'd be grateful if you submit a PR. You're not missing the \"right way\" to do this: you're doing it the right way; it's just broken.\r\n\r\nIn this case, all of the descendants of TH1 were stubbed out with \"probably right\" implementations to make sure that they didn't accidentally inherit methods intended for TH1. (This was a problem in Uproot 3; now we have a mechanism to prevent it, but preventing it means stubbing out the classes.)\r\n\r\nI'll be available for any questions you have about the PR.",
  "created_at":"2020-12-11T23:54:37Z",
  "id":743486826,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ4NjgyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T23:54:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, I prefer fully qualified names everywhere, so instead of\r\n\r\n```python\r\nfrom uproot.behaviors import TProfile, TH2\r\n\r\nclass TProfile2D(TProfile.Profile):\r\n     no_inherit = (TH2.TH2,)\r\n```\r\n\r\ndo\r\n\r\n```python\r\nimport uproot.behaviors.TProfile\r\nimport uproot.behaviors.TH2\r\n\r\nclass TProfile2D(uproot.behaviors.TProfile.Profile):\r\n     no_inherit = (uproot.behaviors.TH2.TH2,)\r\n```",
  "created_at":"2020-12-11T23:57:05Z",
  "id":743487371,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ4NzM3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-11T23:57:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"What should we do for the unit-test?\r\n\r\nThe files I produced are very simple dummy objects filled with random numbers and I am not quite sure how to write a function to check for \"any\" error.\r\n\r\nUsually, a unit-test should assert or catch a specific error.\r\nIn this case was `AttributeError `, but in the future it could be anything.\r\n\r\nCould something like the following work in general?\r\n\r\n```\r\ndef test_read_TProfile(tmpdir):\r\n    filename = os.path.join(str(tmpdir), \"TProfile2D.root\")\r\n\r\n    with uproot.open(filename) as file:\r\n            file[\"hprof3d\"]\r\n```",
  "created_at":"2020-12-12T00:19:59Z",
  "id":743495870,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzQ5NTg3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-12T00:19:59Z",
  "user":"MDQ6VXNlcjE3ODM2NjEw"
 },
 {
  "author_association":"MEMBER",
  "body":"A bigger problem is how to include the file. The original Uproot repo became unwieldy because I added too many ROOT files\u2014too many large diffs. So for this one, I've been putting the files into the https://github.com/scikit-hep/scikit-hep-testdata repo. If you're willing to do two pull requests, you could add your files to that one as I have been. Otherwise, you can forgo testing for now and I'll follow up.\r\n\r\nAs for the content of the test\u2014after generation, the file has fixed contents, so you can check for exact numbers. Printing a Python list (not a NumPy array) includes enough decimals that you can paste those values into the text of a test and it will pass\u2014no need for approximation. I did that with some of these, particularly the original TProfile ones, in which the numbers came from PyROOT, so that I could verify that Uproot was calculating exactly the same thing as ROOT. I don't think we need to do a full ROOT-to-Uproot closure for the 2-D and 3-D ones, since the bins are calculated the same way, just reshaped into a rectangle. Checking to make sure that the rectangle isn't flipped or transposed is a good idea\u2014that was an error that we missed in the original TH2 tests (because our test samples were symmetric!).",
  "created_at":"2020-12-12T04:35:31Z",
  "id":743701256,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzcwMTI1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-12T04:35:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK then, I will start by opening the PR with this fix.\r\n\r\nMy files were generated with symmetric samples, so I will re-generate them as asymmetric (perhaps even smaller, we don't need 40x40 values...) and I will add them to that other repository with a different PR.\r\n\r\nI can maybe open a new PR here to add the unit-test when the data is ready, so this fix will be already available in the master?\r\n\r\nAny chance you have a server from which to download test data?\r\nIn the project I am working on, we were doing the same (a repo for extras) and eventually, we had to switch due to the need of bigger test data samples.",
  "created_at":"2020-12-12T09:42:46Z",
  "id":743731036,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzczMTAzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-12T09:46:11Z",
  "user":"MDQ6VXNlcjE3ODM2NjEw"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Oh sorry I saw now that in that repo also \"remote\" files are considered",
  "created_at":"2020-12-12T09:50:33Z",
  "id":743731798,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0MzczMTc5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-12T09:50:33Z",
  "user":"MDQ6VXNlcjE3ODM2NjEw"
 },
 {
  "author_association":"MEMBER",
  "body":"A server of test files would probably be a good idea. Most of our tests are files in scikit-hep-testdata, which after pip-installation are local files. The others, marked as \"pytest.mark.network\" attempt to access random servers that we don't control. If one of them stops serving, our tests will fail. However, those servers have special characteristics\u2014some don't support multi-part GET, which is good for verifying that our fall-back works.\r\n\r\nThere's a strong separation between the physical layer (getting data from local files, HTTP, and XRootD) and the interpretive layer (e.g. properly loading a TProfile2D) and their tests. Since you're adding to the latter, the file should be a local file in scikit-hep-testdata. I've turned on \"watching\" for that repo so I'll see it when you add the two files as a pull request, and I'll merge it and deploy a new version. Follow the naming convention of \"uproot-issue-227a.root\" and \"uproot-issue-227b.root\". (The distinction between old Uproot issue numbers and new Uproot issue numbers is a hyphen before the number. That convention wasn't carefully designed, but it has been maintained so far.)\r\n\r\nhttps://github.com/scikit-hep/scikit-hep-testdata/pulls\r\n\r\nThanks!",
  "created_at":"2020-12-12T17:26:26Z",
  "id":743787478,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0Mzc4NzQ3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-12T17:26:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"What is the naming convention for the unit-tests?\r\nI see that each of them should have a 4-digit number, but some of them are missing",
  "created_at":"2020-12-20T20:44:35Z",
  "id":748668498,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODY2ODQ5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-20T20:44:35Z",
  "user":"MDQ6VXNlcjE3ODM2NjEw"
 },
 {
  "author_association":"MEMBER",
  "body":"Are they issues numbers from (now a different) repo, `uproot3`?",
  "created_at":"2020-12-20T20:48:40Z",
  "id":748668947,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODY2ODk0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-20T20:48:40Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"All of the issue numbers/test names are from this repo.",
  "created_at":"2020-12-20T22:23:01Z",
  "id":748679113,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODY3OTExMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-20T22:23:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Ahh, the first digit is always 0. Okay, never mind about the different repo part. :)",
  "created_at":"2020-12-20T22:24:38Z",
  "id":748679281,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODY3OTI4MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-20T22:24:38Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I couldn't give a full explanation before, just trying to unblock @HealthyPear by deploying a new version of scikit-hep-testdata.\r\n\r\nThe tests in scikit-hep/awkward-1.0 and scikit-hep/uproot4 are named after the issue number OR the pull request number. (Sometimes, the test needs to be made before creating the pull request, and therefore the PR number isn't known yet, but the issue number is.) The numbers are all four digits with leading zeros so that they sort well (listing files is often alphabetical) and because I don't expect to reach 10000 soon. scikit-hep/awkward-0.x and scikit-hep/uproot3 don't have numbered tests, so the numbers correspond exclusively to the new repos.\r\n\r\nOf course, new PRs can update old tests, but they usually don't, at least not much. I'd like to add that I've found this method of organizing tests to be _great_ for finding things after the fact and for quickly running \"the last few months/weeks/days of tests\" (because they roughly correlate with number and numbers can be selected with glob patterns). It also frees me from worrying about organizing the tests by topic. I only worry about adding a test for the current PR's feature or bug fix, not how it fits into some sort of ontology.",
  "created_at":"2020-12-20T23:20:49Z",
  "id":748685795,
  "issue":227,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODY4NTc5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-20T23:20:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This looks good to me, it passes tests, and it addresses the original problem. I think it's ready to merge.\r\n\r\nLet me know if you're done and I'll merge it! (I don't want to merge it while you might have some unpushed updates in a local copy of the repo.) Thanks!",
  "created_at":"2020-12-21T16:01:08Z",
  "id":749048638,
  "issue":228,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0OTA0ODYzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-21T16:01:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes, I am done, I don't have further commits to push - if it's good for you then it's good also for me!",
  "created_at":"2020-12-21T16:29:14Z",
  "id":749062093,
  "issue":228,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0OTA2MjA5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-21T16:29:14Z",
  "user":"MDQ6VXNlcjE3ODM2NjEw"
 },
 {
  "author_association":"MEMBER",
  "body":"I don't think you build in your tests, but test against an uninstalled uproot. So this isn't really tested. I also can't directly patch the feedstock to apply this, so will try a version applied against 4.0.0 tomorrow.",
  "created_at":"2020-12-17T05:02:36Z",
  "id":747206685,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzIwNjY4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-17T05:02:36Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I've been thinking about setup.cfg vs setup.py for a while now. Uproot is a plain Python package without strange features, so there are no issues with this declarative approach, and it's much better for metadata.\r\n\r\nI'm not sure what you mean about it \"not being tested.\" It does test out of the local directory, not a pip-installed version, which verifies that the code does what it's supposed to do. Do you mean that we're not testing that all the files we intend to include in the package are included in the package? If Uproot had any `data_files`, then I can see how that would be an issue.\r\n\r\nOne thing about changes to Uproot's build procedure: I don't want to put all the source code into a `src` directory because that interferes with incremental development. Changing setup.py to setup.cfg is great, changing the CI tests to test a pip-installed version is fine, and you'll notice that I'm using black and flake8 now (not automatically changing the code, but flake8 is the gate-keeper and it usually doesn't pass if black hasn't been run). However, I want to keep the directory structure as `uproot`, rather than `src`, so that development/debugging has no intervening steps.\r\n\r\nThanks for your help, though! I appreciate it.",
  "created_at":"2020-12-17T15:09:03Z",
  "id":747495941,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzQ5NTk0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-17T15:09:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The problem with not having src is that it makes it hard to even test against an installed version, as it picks up the local directory when you run pytest on your tests. If it ever makes a difference - such as if the packaging is broken, then you can't tell in your tests. But I totally get it, I work on several packages this way, other builds systems like poetry and flit are structured this way, so would not push for a `/src` change unless someone really wanted it (or if you have binary components, maybe I'd push for it in that case). I believe it is a better structure due to the issue above, and it pushes you to always develop with virtual environments and `pip install -e .` rather than relying on directory based imports.\r\n\r\nYou can still keep the logic / funny stuff in setup.py for packages with strange features - it helps highlight the parts that are normal by pulling them out into setup.cfg, and then setup.py is _just_ the special stuff with logic. I left the version and extras there; the version extraction code in setuptools is a little picky on older versions, and special extras like \"all\" are really a bit nicer if you don't have to be so repetitive.\r\n\r\nFor really simple packages, I'm actually beginning to be tempted to remove setup.py and only have setup.cfg - people keep thinking `python setup.py ...` is valid API to put into linux repository packaging and such, and it's not, it's an implementation detail, and doesn't activate all the correct machinery for building/installing in some cases. Removing setup.py would teach them a lesson. :)\r\n\r\n\r\n",
  "created_at":"2020-12-17T20:38:24Z",
  "id":747688280,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzY4ODI4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-17T20:38:24Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I mean that if I totally broke `pip install uproot` I could not tell that from the tests. I prefer to consider that all packages require installing, and the fact Python happens to import directories in the working directory as packages (except for Windows + Python 3.8+, which will not import a DLL this way anymore) to be a trick that might be okay to use locally, but not in tests or production. 99% of your users get your package through pip - that should be the \"correct\" way to get it even from GitHub/local, and therefore the one tested. Tools like `install_requires` are _really_ handy to be able to depend on!",
  "created_at":"2020-12-17T20:43:05Z",
  "id":747690503,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzY5MDUwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-17T20:43:21Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to target v4.0.0 for a while, ignore that, I'm just trying to build a patch for conda-forge, and will come back to main once it starts working. That will test the install, because conda does a pip install. :)",
  "created_at":"2020-12-17T20:44:34Z",
  "id":747691169,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzY5MTE2OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-17T20:44:34Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Nevermind, GitHub says I can target a tag, but I can't. Admittedly, I have no idea what a \"merge\" would have been, but it would have been useful for the .patch file. I'll do it locally then.",
  "created_at":"2020-12-17T20:46:19Z",
  "id":747691986,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzY5MTk4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-17T20:46:19Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes! This fixed the encoding error in conda-forge. :)\r\n\r\nAdded a few notes on things that changed slightly or might be things you might be interested in changing.",
  "created_at":"2020-12-17T21:26:35Z",
  "id":747711910,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzcxMTkxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-17T21:26:35Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"It really does work for Python 2.6. Of course, I couldn't promise that\u2014it had to go under stealth mode... But okay, fine. 2.7.  `:)`\r\n\r\nIt's waaaay more important that it works in conda!",
  "created_at":"2020-12-18T02:14:45Z",
  "id":747823036,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzgyMzAzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T02:14:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm quite okay with it working in 2.6 - I just think these two numbers should match. `>=2.6` is fine. :)\r\n\r\nThough if we say it works in 2.6, it should have a test. I'm pretty sure GHA provides even 3.0 (?!).",
  "created_at":"2020-12-18T02:16:23Z",
  "id":747823507,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzgyMzUwNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T02:16:23Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"And the `python_requires` is the much more important number - it's the one that stops pip from installing if it doesn't match, and causes it to look for an older version. Though IIRC there might not be a version of pip that supports both 2.6 and `python_requires`, but still, on principle, it should match the trove classifiers!\r\n\r\nJust tell me what you want me to set it to or change it - either dropping the classifier or changing the `python_requires`. :)",
  "created_at":"2020-12-18T02:18:47Z",
  "id":747824176,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzgyNDE3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T02:19:39Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> It's waaaay more important that it works in conda!\r\n\r\nActually, I believe pip install uproot is broken on all Windows, not just conda. Your tests never pip install so you don't catch it, even though you test on Windows. Will open a quick PR adding the pip install to see.",
  "created_at":"2020-12-18T02:36:19Z",
  "id":747829361,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzgyOTM2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T02:36:19Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> I'm quite okay with it working in 2.6 - I just think these two numbers should match. `>=2.6` is fine. :)\r\n> Though if we say it works in 2.6, it should have a test. I'm pretty sure GHA provides even 3.0 (?!).\r\n\r\nThat's why I didn't want to _say_ it, but let it work if it's going to work for somebody. I even shied away from testing 2.7 because I couldn't get it to work in GHA.\r\n\r\n> Actually, I believe pip install uproot is broken on all Windows, not just conda.\r\n\r\nIf so, then you got me. I don't know how that could have happened, though.",
  "created_at":"2020-12-18T03:52:36Z",
  "id":747851324,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0Nzg1MTMyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T03:52:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> If so, then you got me.\r\n\r\nI added pip install to the tests in #233 and Windows breaks, as promised. Adding the same commit to this PR passes.\r\n\r\n> I don't know how that could have happened, though.\r\n\r\nAnything untested must be assumed to be broken. You never tested pip installs, so at some point you added a character to the readme that broke on the default codec on Windows, breaking your simple `open()`. The declarative config and `file: README.md` does the open correctly and assumes UTF-8, safely reading the readme.",
  "created_at":"2020-12-18T04:49:34Z",
  "id":747865729,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0Nzg2NTcyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T04:49:34Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Python is platform-independent... except for Windows. To think that reading text from a file without specifying a code breaks it\u2014okay, so your point is well-taken that if it's not doing a `pip install .`, it's not being tested.\r\n\r\nI'm having trouble following all of the changes, but I see that it is passing now, and it's doing a `pip install .`. The minimum NumPy would be much lower than 1.13.1, which is significant for people who get NumPy from the system distribution.\r\n\r\nAbout old Pythons, including 2.7, I wanted to only promise that it works for 3.5+ (as it says on the README), but not prevent it from being installed on arbitrarily old systems (where \"arbitrarily old\" is 2.6).\r\n\r\nI'll have to follow up later; a meeting is starting now.",
  "created_at":"2020-12-18T15:00:07Z",
  "id":748130467,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODEzMDQ2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T15:00:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry about the 2.6 / 2.7 confusion! I was totally confused because I changed it to 2.7 without realizing it, then was unhappy about the mismatch. \ud83e\udd26\r\n\r\nAnd, as requested, here it is: \ud83d\udc4d ",
  "created_at":"2020-12-18T17:25:25Z",
  "id":748217451,
  "issue":231,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODIxNzQ1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T17:25:25Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, you can't `pip install uproot 4.0.0` on Windows. this is why I'm such a pill about doing things the proper way and pip installing, etc. Pip install being broken should never pass tests!",
  "created_at":"2020-12-18T02:49:05Z",
  "id":747833459,
  "issue":233,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0NzgzMzQ1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T03:00:04Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Closing this because it's contained in #231, and was opened to show the current situation was broken without the fix contained there.",
  "created_at":"2020-12-18T17:31:39Z",
  "id":748220431,
  "issue":233,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODIyMDQzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T17:31:39Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"ah, this is fixed in uproot 4.0.1rc2.",
  "created_at":"2020-12-18T20:27:48Z",
  "id":748303179,
  "issue":234,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODMwMzE3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T20:27:48Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"I couldn't reproduce this; @lgray updated and the problem apparently went away.\r\n\r\nFor reference,\r\n\r\n```python\r\n>>> import uproot\r\n>>> import uproot3\r\n>>> uproot.__version__\r\n'4.0.1rc2'\r\n>>> uproot3.__version__\r\n'3.14.1'\r\n\r\n>>> hist_uproot4 = uproot.open(\"https://github.com/CoffeaTeam/coffea/raw/\"\r\n                               \"master/tests/samples/testSF2d.histo.root\")[\"scalefactors_Tight_Electron\"]\r\n>>> hist_uproot4.axes[0].edges()\r\narray([-2.5  , -2.   , -1.566, -1.444, -0.8  ,  0.   ,  0.8  ,  1.444,\r\n        1.566,  2.   ,  2.5  ])\r\n>>> hist_uproot4.axes[1].edges()\r\narray([ 10.,  20.,  35.,  50.,  90., 150., 500.])\r\n\r\n>>> hist_uproot3 = uproot3.open(\"https://github.com/CoffeaTeam/coffea/raw/\"\r\n                               \"master/tests/samples/testSF2d.histo.root\")[\"scalefactors_Tight_Electron\"]\r\n>>> hist_uproot3.edges[0]\r\narray([-2.5  , -2.   , -1.566, -1.444, -0.8  ,  0.   ,  0.8  ,  1.444,\r\n        1.566,  2.   ,  2.5  ])\r\n>>> hist_uproot3.edges[1]\r\narray([ 10.,  20.,  35.,  50.,  90., 150., 500.])\r\n```",
  "created_at":"2020-12-18T20:35:15Z",
  "id":748306189,
  "issue":234,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc0ODMwNjE4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-18T20:35:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks @beojan and @henryiii! That's a bizarre mistake. I'll merge the PR as soon as the tests pass.",
  "created_at":"2020-12-31T17:13:23Z",
  "id":753008430,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzAwODQzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-31T17:13:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"My standard reminder: MyPy catches things like this. ;)",
  "created_at":"2020-12-31T17:19:06Z",
  "id":753009371,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzAwOTM3MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2020-12-31T17:19:06Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Now I see how that happened. Most of the other methods on the abstract class `raise NotImplementedError`, but this one should return a value. Or, as in @henryiii's fix, it now raises NotImplementedError on the abstract class but returns `\"COUNT\"` for each concrete class. That's less prone to error (e.g. if a new concrete class is ever added, though that's unlikely since a new `TH1` descendent would have to be added to ROOT, and any new histogram types in ROOT will likely be in the ROOT 7 class hierarchy).",
  "created_at":"2020-12-31T17:20:45Z",
  "id":753009684,
  "issue":235,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1MzAwOTY4NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2020-12-31T17:20:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 }
]
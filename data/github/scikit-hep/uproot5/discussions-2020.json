[
 {
  "author":{
   "login":"bdrum"
  },
  "body":"Hi, guys! \r\nThanks again (I'll never tire to repeat it \ud83d\udc4d ) for the package!\r\n\r\nI've ported my scripts from uproot to uproot4 and noticed one effect, that I've not seen in the docs:\r\n\r\nuproot4 in case of library='pd' passed returns for me tuple of dataframes\r\n~~~python\r\nimport uproot4\r\nevents2 = uproot4.open(\"http://scikit-hep.org/uproot/examples/HZZ.root\")[\"events\"]\r\ndf2 = events2.arrays(entry_stop=10, library='pd')\r\ntype(df2)\r\n# tuple\r\n~~~\r\n\r\nbut previous version has returned dataframe:\r\n\r\n~~~python\r\nimport uproot\r\nevents2 = uproot.open(\"http://scikit-hep.org/uproot/examples/HZZ.root\")[\"events\"]\r\ndf = events2.pandas.df([\"MET_p*\", \"Muon_P*\"], entrystop=10)\r\ntype(df)\r\n# pandas.core.frame.DataFrame\r\n~~~\r\n\r\nSo what I can't realized is the reason of such behavior and moreover looks like that the data in each of dataframe from the tuple is not unique and have duplicates, so I can't merge it.",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"This is a change in behavior from Uproot 3, and if, after understanding the reason why, you have some suggested text for how it should be explained in the documentation, let me know.\r\n\r\nAlthough Pandas DataFrames can describe jagged data by putting the jaggedness into a MultiIndex, a DataFrame can have only one index. Thus, data with different multiplicities can't go into the same DataFrame.\r\n\r\nBelow, we ask for both jets and muons. The jet index (in each event) is unrelated to the muon index (in each event), so they really have to be in different DataFrames.\r\n\r\n```python\r\n>>> events.arrays(filter_name=[\"Jet_P*\", \"Muon_P*\"], library=\"pd\")\r\n(                   Jet_Px     Jet_Py      Jet_Pz\r\nentry subentry                                  \r\n1     0        -38.874714  19.863453   -0.894942\r\n3     0        -71.695213  93.571579  196.296432\r\n      1         36.606369  21.838793   91.666283\r\n      2        -28.866419   9.320708   51.243221\r\n4     0          3.880162 -75.234055 -359.601624\r\n...                   ...        ...         ...\r\n2417  0        -33.196457 -59.664749  -29.040150\r\n      1        -26.086025 -19.068407   26.774284\r\n2418  0         -3.714818 -37.202377   41.012222\r\n2419  0        -36.361286  10.173571  226.429214\r\n      1        -15.256871 -27.175364   12.119683\r\n\r\n[2773 rows x 3 columns],\r\n                   Muon_Px    Muon_Py     Muon_Pz\r\nentry subentry                                  \r\n0     0        -52.899456 -11.654672   -8.160793\r\n      1         37.737782   0.693474  -11.307582\r\n1     0         -0.816459 -24.404259   20.199968\r\n2     0         48.987831 -21.723139   11.168285\r\n      1          0.827567  29.800508   36.965191\r\n...                   ...        ...         ...\r\n2416  0        -39.285824 -14.607491   61.715790\r\n2417  0         35.067146 -14.150043  160.817917\r\n2418  0        -29.756786 -15.303859  -52.663750\r\n2419  0          1.141870  63.609570  162.176315\r\n2420  0         23.913206 -35.665077   54.719437\r\n\r\n[3825 rows x 3 columns])\r\n```\r\n\r\nUproot 3 merged these because that's similar to what ROOT's `TTree::Scan` does. But it's odd to say that the jet at index 0 is in the same row as the muon at index 0 (how are these two collections sorted? why associate e.g. the most energetic jet with the most energetic muon? they might not have anything to do with each other). If you want to do that, you can opt-in:\r\n\r\n```python\r\n>>> left, right = events.arrays(filter_name=[\"Jet_P*\", \"Muon_P*\"], library=\"pd\")\r\n>>> pd.merge(left, right, left_index=True, right_index=True, how=\"inner\")\r\n                   Jet_Px     Jet_Py  ...    Muon_Py     Muon_Pz\r\nentry subentry                        ...                       \r\n1     0        -38.874714  19.863453  ... -24.404259   20.199968\r\n3     0        -71.695213  93.571579  ... -85.835464  403.848450\r\n      1         36.606369  21.838793  ... -13.956494  335.094208\r\n4     0          3.880162 -75.234055  ...  67.248787  -89.695732\r\n      1          4.979580 -39.231731  ...  25.403667   20.115053\r\n...                   ...        ...  ...        ...         ...\r\n2414  0         33.961163  58.900467  ... -42.204014  -64.264900\r\n2416  0         37.071465  20.131996  ... -14.607491   61.715790\r\n2417  0        -33.196457 -59.664749  ... -14.150043  160.817917\r\n2418  0         -3.714818 -37.202377  ... -15.303859  -52.663750\r\n2419  0        -36.361286  10.173571  ...  63.609570  162.176315\r\n\r\n[2038 rows x 6 columns]\r\n>>> pd.merge(left, right, left_index=True, right_index=True, how=\"outer\")\r\n                   Jet_Px     Jet_Py  ...    Muon_Py     Muon_Pz\r\nentry subentry                        ...                       \r\n0     0               NaN        NaN  ... -11.654672   -8.160793\r\n      1               NaN        NaN  ...   0.693474  -11.307582\r\n1     0        -38.874714  19.863453  ... -24.404259   20.199968\r\n2     0               NaN        NaN  ... -21.723139   11.168285\r\n      1               NaN        NaN  ...  29.800508   36.965191\r\n...                   ...        ...  ...        ...         ...\r\n2417  1        -26.086025 -19.068407  ...        NaN         NaN\r\n2418  0         -3.714818 -37.202377  ... -15.303859  -52.663750\r\n2419  0        -36.361286  10.173571  ...  63.609570  162.176315\r\n      1        -15.256871 -27.175364  ...        NaN         NaN\r\n2420  0               NaN        NaN  ... -35.665077   54.719437\r\n\r\n[4560 rows x 6 columns]\r\n```\r\n\r\nThe first of these, the `how=\"inner\"` join, truncates the two collections in each event to the shorter of the two. The second, the `how=\"outer\"` join, pads the shorter collection to have the same length as the longer with NaN values.\r\n\r\nRelationally (i.e. in SQL-land), the right thing to do is to keep these DataFrames separate and join on the first level of their MultiIndex. The fact that that's such a pain has a lot to do with why Awkward Array exists (see [StackOverflow question from four years ago](https://stackoverflow.com/questions/38831961/what-declarative-language-is-good-at-analysis-of-tree-like-data)!). But I digress.\r\n\r\nIf you don't want data with different multiplicities, you get a single DataFrame, no tuple.\r\n\r\n```python\r\n>>> events.arrays(filter_name=[\"Muon_P*\"], library=\"pd\")\r\n                  Muon_Px    Muon_Py     Muon_Pz\r\nentry subentry                                  \r\n0     0        -52.899456 -11.654672   -8.160793\r\n      1         37.737782   0.693474  -11.307582\r\n1     0         -0.816459 -24.404259   20.199968\r\n2     0         48.987831 -21.723139   11.168285\r\n      1          0.827567  29.800508   36.965191\r\n...                   ...        ...         ...\r\n2416  0        -39.285824 -14.607491   61.715790\r\n2417  0         35.067146 -14.150043  160.817917\r\n2418  0        -29.756786 -15.303859  -52.663750\r\n2419  0          1.141870  63.609570  162.176315\r\n2420  0         23.913206 -35.665077   54.719437\r\n\r\n[3825 rows x 3 columns]\r\n```\r\n\r\nAs an interesting in-between case, if you want jagged and non-jagged data, the non-jagged data can be broadcasted to the jagged and that gives you a single DataFrame, too.\r\n\r\n```python\r\n>>> events.arrays(filter_name=[\"MET_p*\", \"Muon_P*\"], library=\"pd\")\r\n                  Muon_Px    Muon_Py     Muon_Pz     MET_px     MET_py\r\nentry subentry                                                        \r\n0     0        -52.899456 -11.654672   -8.160793   5.912771   2.563633\r\n      1         37.737782   0.693474  -11.307582   5.912771   2.563633\r\n1     0         -0.816459 -24.404259   20.199968  24.765203 -16.349110\r\n2     0         48.987831 -21.723139   11.168285 -25.785088  16.237131\r\n      1          0.827567  29.800508   36.965191 -25.785088  16.237131\r\n...                   ...        ...         ...        ...        ...\r\n2416  0        -39.285824 -14.607491   61.715790 -14.607650 -28.204895\r\n2417  0         35.067146 -14.150043  160.817917  22.208313  59.774940\r\n2418  0        -29.756786 -15.303859  -52.663750  18.101646  50.290718\r\n2419  0          1.141870  63.609570  162.176315  79.875191 -52.351452\r\n2420  0         23.913206 -35.665077   54.719437  19.713749  -3.595418\r\n\r\n[3825 rows x 5 columns]\r\n```\r\n\r\nArguably, maybe this should be two DataFrames as well, since the broadcasting means that MET values in events with no muons are dropped. But if that was an issue, you'd just call `events.arrays` multiple times, to make the DataFrames separate on purpose. The auto-broadcasting simplifies a common case (you don't have to explicitly `pd.merge`).\r\n\r\nArguably, maybe this function should always return a tuple, possibly a tuple of one item, so that types don't depend on values. The way it is now, whether you get a tuple of DataFrames or just a DataFrame depends on what kinds of branches exist in the ROOT file and whether you've asked for them. I'm on the fence about that: always returning the same type is nice for predictability, but it could be hard explaining to everyone why they're getting a tuple with only one item. It then becomes cumbersome to unpack (I like the `variable_name, = thing_that_returns_singleton()` syntax, but a lot of people don't like the meaningfulness of the trailing comma, including [black](https://pypi.org/project/black/).)\r\n\r\nSo that's why. I'm going to close this because it's not a bug/issue and label it as a question. If you want to start a discussion about changing the behavior, I'll reopen it as a policy question. If you have a suggested edit to [the documentation](https://github.com/scikit-hep/uproot4/blob/master/docs-sphinx/basic.rst), I'll take a PR. Thanks!",
     "createdAt":"2020-09-24T14:50:52Z",
     "number":167498,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2020-09-24T14:11:12Z",
  "number":114,
  "title":"The `library=\"pd\"` option sometimes returns a Pandas DataFrame, sometimes a tuple of DataFrames",
  "url":"https://github.com/scikit-hep/uproot5/discussions/114"
 },
 {
  "author":{
   "login":"tamasgal"
  },
  "body":"Help, I struggle again with the transitioning ;)\r\n\r\nWith `uproot3`, there was a weird interpretation (glitch) in one of our file formats, which I/we never fully understood, but the outcome was very useful. The data in `\"E/Evt/hits\"` below is split up into subbranches which one can read nicely with `array`/`lazyarray`, however, if you access the array of the main branch directly (`E/Evt/hits`), you get the number of entries per event instead of a large jagged array of structs, since as you can see below, uproot3 recognises it as a `vector<int>`.\r\nWe used this array to quickly identify e.g. empty or large events for further cuts since it loads extremely fast. The same behaviour is btw. observed in `E/Evt/mc_hits`, E/Evt/trks`, `E/Evt/mc_trks` and in other branches of different other formats we use as well:\r\n\r\n```python\r\n>>> import uproot3  # 3.14.0\r\n>>> from skhep_testdata import data_path\r\n>>> f = uproot3.open(data_path(\"uproot-issue431b.root\"))\r\n>>> f[\"E/Evt/hits\"].show()\r\nhits                       TStreamerSTL               asdtype('>i4')\r\nhits.id                    TStreamerBasicType         asjagged(asdtype('>i4'))\r\nhits.dom_id                TStreamerBasicType         asjagged(asdtype('>i4'))\r\nhits.channel_id            TStreamerBasicType         asjagged(asdtype('>u4'))\r\nhits.tdc                   TStreamerBasicType         asjagged(asdtype('>u4'))\r\nhits.tot                   TStreamerBasicType         asjagged(asdtype('>u4'))\r\nhits.trig                  TStreamerBasicType         asjagged(asdtype('>i4'))\r\nhits.pmt_id                TStreamerBasicType         asjagged(asdtype('>i4'))\r\nhits.t                     TStreamerObjectAny         asjagged(asdtype('>f8'))\r\nhits.a                     TStreamerBasicType         asjagged(asdtype('>f8'))\r\nhits.pos.x                 TStreamerBasicType         asjagged(asdtype('>f8'))\r\nhits.pos.y                 TStreamerBasicType         asjagged(asdtype('>f8'))\r\nhits.pos.z                 TStreamerBasicType         asjagged(asdtype('>f8'))\r\nhits.dir.x                 TStreamerBasicType         asjagged(asdtype('>f8'))\r\nhits.dir.y                 TStreamerBasicType         asjagged(asdtype('>f8'))\r\nhits.dir.z                 TStreamerBasicType         asjagged(asdtype('>f8'))\r\nhits.pure_t                TStreamerBasicType         asjagged(asdtype('>f8'))\r\nhits.pure_a                TStreamerBasicType         asjagged(asdtype('>f8'))\r\nhits.type                  TStreamerBasicType         asjagged(asdtype('>i4'))\r\nhits.origin                TStreamerBasicType         asjagged(asdtype('>i4'))\r\nhits.pattern_flags         TStreamerBasicType         asjagged(asdtype('>u4'))\r\n>>> f[\"E/Evt/hits/hits.tot\"].array()\r\n<JaggedArray [[24 30 22 ... 38 26 23] [29 26 22 ... 26 28 24] [27 19 13 ... 27 24 16] ... [22 22 9 ... 27 32 27] [30 32 17 ... 30 24 29] [27 41 36 ... 29 24 28]] at 0x00011421d640>\r\n>>> f[\"E/Evt/hits\"].array()\r\narray([176, 125, 318, 157,  83,  60,  71,  84, 255, 105], dtype=int32)\r\n```\r\n\r\nSo far so good... The \"problem\" is that `uproot4` is able to correctly figure out how to parse `E/Evt/hits` as an `awkward.Record` array (`vector<Hit>`) and the only way I have figured out so far to get the lengths of each sub-array is to use `ak.num` or `ak.count`, both of them however require to load a bunch of data into memory which takes time and... well, memory ;) other than that, they count all sub-arrays. I can reduce the footprint by only retrieving any of the subbranches, e.g. `ak.count(f[\"E/Evt/hits/hits.channel_id\"].array(), axis=1)` but that does not feel good (and still takes much longer compared to the uproot3 glitch.\r\n\r\nIt's btw. interesting that the interpretation is still  showing `AsJagged(AsDtype('>i4'))` and `int32_t[]`, so I am a bit confused why it works.\r\n\r\n```python\r\n>>> import uproot4\r\n\r\n>>> from skhep_testdata import data_path\r\n\r\n>>> import awkward1 as ak\r\n\r\n>>> f = uproot4.open(data_path(\"uproot-issue431b.root\"))\r\n\r\n>>> f[\"E/Evt/hits\"].show()\r\nname                 | typename                 | interpretation\r\n---------------------+--------------------------+-------------------------------\r\nhits                 | vector<Hit>              | AsGroup(<TBranchElement 'hits'hits.id              | int32_t[]                | AsJagged(AsDtype('>i4'))\r\nhits.dom_id          | int32_t[]                | AsJagged(AsDtype('>i4'))\r\nhits.channel_id      | uint32_t[]               | AsJagged(AsDtype('>u4'))\r\nhits.tdc             | uint32_t[]               | AsJagged(AsDtype('>u4'))\r\nhits.tot             | uint32_t[]               | AsJagged(AsDtype('>u4'))\r\nhits.trig            | int32_t[]                | AsJagged(AsDtype('>i4'))\r\nhits.pmt_id          | int32_t[]                | AsJagged(AsDtype('>i4'))\r\nhits.t               | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.a               | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.pos.x           | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.pos.y           | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.pos.z           | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.dir.x           | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.dir.y           | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.dir.z           | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.pure_t          | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.pure_a          | double[]                 | AsJagged(AsDtype('>f8'))\r\nhits.type            | int32_t[]                | AsJagged(AsDtype('>i4'))\r\nhits.origin          | int32_t[]                | AsJagged(AsDtype('>i4'))\r\nhits.pattern_flags   | uint32_t[]               | AsJagged(AsDtype('>u4'))\r\n\r\n>>> f[\"E/Evt/hits\"].array()  # correctly parses the whole dataset as a record array\r\n<Array [{'hits.id': [0, 0, 0, 0, ... 0, 0, 0]}] type='10 * {\"hits.id\": var * int...'>\r\n\r\n>>> ak.count(f[\"E/Evt/hits\"].array(), axis=1)  # this fails for a typical sized file due to excessive memory usage\r\n<Record ... 157, 83, 60, 71, 84, 255, 105]} type='{\"hits.id\": var * int64, \"hits...'>\r\n\r\n>>> ak.count(f[\"E/Evt/hits/hits.channel_id\"].array(), axis=1)  # kind of works, but still too slow\r\n<Record ... 157, 83, 60, 71, 84, 255, 105]} type='{\"hits.id\": var * int64, \"hits...'>\r\n```\r\n\r\nThe `f[\"E/Evt/hits\"].num_entries` gives the overall number of events, but I have not found any other shortcut to the subbranch lengths.\r\n\r\nDo you have a hint how to get to this information?",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"This is being read as a group (`uproot4.AsGroup` interpretation). _Sometimes_, a branch with subbranches has no data, _other times_, it has integer counts. I think that TClonesArrays do the latter.\r\n\r\nYou can check for this with\r\n\r\n```python\r\n>>> import uproot4\r\n>>> from skhep_testdata import data_path\r\n>>> f = uproot4.open(data_path(\"uproot-issue431b.root\"))\r\n>>> isinstance(f[\"E/Evt/hits\"].interpretation, uproot4.AsGrouped)\r\nTrue\r\n```\r\n\r\nand you can get the counts by supplying that interpretation:\r\n\r\n```python\r\n>>> f[\"E/Evt/hits\"].array(uproot4.AsDtype(\">i4\"))\r\n<Array [176, 125, 318, 157, ... 84, 255, 105] type='10 * int32'>\r\n```\r\n\r\nThat interpretation won't work in every AsGrouped case, but the only other case I've seen is when there's no data at all associated with the branch-with-subbranches. You can check for that with\r\n\r\n```python\r\n>>> f[\"E/Evt/hits\"].num_baskets\r\n1\r\n```\r\n\r\nor catch the failure of the `uproot4.AsDtype(\">i4\")` interpretation.\r\n\r\nAs for counting after the fact, prefer [ak.num](https://awkward-array.readthedocs.io/en/latest/_auto/ak.num.html) over [ak.count](https://awkward-array.readthedocs.io/en/latest/_auto/ak.count.html), since the latter is a full-fledged reducer. (The difference becomes observable when `axis < array.ndim - 1`, but it already costs performance in the cases where they have the same output.)",
     "createdAt":"2020-11-25T00:45:03Z",
     "number":167484,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"tamasgal"
     },
     "body":"Thanks Jim, the interpretation to integer is working perfectly! The error handling hints are also useful.\r\n I have the feeling I will never fully understand the ROOT intrinsics, even after spending so much time on them \ud83d\ude04\r\n",
     "createdAt":"2020-11-25T01:09:16Z",
     "number":167485,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2020-11-25T00:10:43Z",
  "number":204,
  "title":"Reading a branch that has subbranches (the AsGroup Interpretation)",
  "url":"https://github.com/scikit-hep/uproot5/discussions/204"
 },
 {
  "author":{
   "login":"DraTeots"
  },
  "body":"We have data structured like this:\r\n\r\n```\r\nhit_count            | uint64_t                        | AsDtype('>u8')\r\nhit_id               | std::vector<uint64_t>    | AsJagged(AsDtype('>u8'), he...\r\nhit_trk_id           | std::vector<uint64_t>    | AsJagged(AsDtype('>u8'), he...\r\nhit_x                | std::vector<double>      | AsJagged(AsDtype('>f8'), he...\r\nhit_y                | std::vector<double>      | AsJagged(AsDtype('>f8'), he...\r\nhit_z                | std::vector<double>      | AsJagged(AsDtype('>f8'), he...\r\n...\r\n\r\ntrk_count            | uint64_t                 | AsDtype('>u8')\r\ntrk_id               | std::vector<uint64_t>    | AsJagged(AsDtype('>u8'), he...\r\ntrk_pdg              | std::vector<int64_t>     | AsJagged(AsDtype('>i8'), he...\r\ntrk_mom              | std::vector<double>      | AsJagged(AsDtype('>f8'), he...\r\n...\r\n```\r\nFirst, the data is in aligned arrays, means, all hit_* arrays have the same size of hit_count and all trk_* arrays have the same size equal trk_count. \r\n\r\nThen hits have hit_trk_id which corresponds to trk_id - a track that made this hits. \r\n\r\nSo now imagine my task, I want to build a histogram with track momentums that have hits at certain subdetector (certain z). \r\n\r\n```python\r\nfor batch in tree.iterate(['hit_z', 'hit_trk_id', 'trk_mom', 'trk_id'], step_size=5, entry_stop=10):\r\n   # then I want all hits with certain z, so I do something like\r\n   good_tracks_id = batch.hit_trk_id[batch.hit_z>100]\r\n   # now, having good_tracks_id how do I select tracks momentum with ids in good_tracks_id?\r\n```\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"tamasgal"
     },
     "body":"You are almost there! I'd propose to give `batch.hit_z > 100` a name and reuse that boolean mask to also select the momentums.\r\nSince you are building a histogram, you may only be interested in the bare values of the momentums after masking, which you obtain by flattening the nested array (coming from the batch-wise operation), since it can contain empty (`[]`) elements.\r\n\r\n```python\r\nimport awkward as ak\r\n\r\nfor batch in tree.iterate(['hit_z', 'hit_trk_id', 'trk_mom', 'trk_id'], step_size=5, entry_stop=10):\r\n   # save your cut as a mask\r\n   mask = batch.hit_z > 100\r\n   good_tracks_id = batch.hit_trk_id[mask]\r\n   momentums = ak.flatten(batch.trk_mom[mask])  # ak.flatten() gets rid of empty elements\r\n```\r\n\r\n",
     "createdAt":"2020-12-16T08:19:42Z",
     "number":213144,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    },
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"Adding to @tamasgal's answer, I'll give some explicit examples. Suppose your data looks like\r\n\r\n```python\r\n>>> trk_mom = ak.Array([[1.1, 2.2], [], [3.3, 4.4], [5.5], [6.6, 7.7, 8.8]])\r\n>>> hit_id = ak.Array([[0, 0, 0, 1, 1], [], [1, 1, 0], [0, 0, 0], [2, 2, 1]])\r\n>>> hit_z = ak.Array([[50, 100, 100, 50, 100], [], [100, 50, 100], [100, 100, 50], [50, 100, 100]])\r\n\r\n>>> # hit_id and hit_z are aligned\r\n>>> ak.num(hit_id)\r\n<Array [5, 0, 3, 3, 3] type='5 * int64'>\r\n>>> ak.num(hit_z)\r\n<Array [5, 0, 3, 3, 3] type='5 * int64'>\r\n```\r\n\r\nAs you've seen, you can get a masking array of booleans from `hit_z`:\r\n\r\n```python\r\n>>> print(hit_z >= 100)\r\n[[False, True, True, False, True], [], ... [True, True, False], [False, True, True]]\r\n```\r\n\r\nand you can use it to select `hit_ids`:\r\n\r\n```python\r\n>>> print(hit_id[hit_z >= 100])\r\n[[0, 0, 1], [], [1, 0], [0, 0], [2, 1]]\r\n```\r\n\r\n_If these ids are the positions of trk values within their arrays,_ that is, if a `0` in the above means that it's the first track in a track list, then you can just pass this array of integers into square brackets, like this:\r\n\r\n```python\r\n>>> print(trk_mom[hit_id[hit_z >= 100]])\r\n[[1.1, 1.1, 2.2], [], [4.4, 3.3], [5.5, 5.5], [8.8, 7.7]]\r\n```\r\n\r\nIf the trk ids are not positions in the `trk_*` arrays, they need to be translated somehow to those positions before you can use it this way. (For more, see the [full documentation](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html#ak-array-getitem).)\r\n\r\nAnother thing that could be useful to know is that there's a way to apply cuts that does not change the shape of the array. If you use \"[mask](https://awkward-array.readthedocs.io/en/latest/_auto/ak.mask.html)\":\r\n\r\n```python\r\n>>> print(hit_id.mask[hit_z >= 100])\r\n[[None, 0, 0, None, 1], [], [1, None, 0], [0, 0, None], [None, 2, 1]]\r\n```\r\n\r\nthe True values in `hit_z >= 100` pass `hit_ids` through and the False values put in a placeholder \"None\". You can use this array of integers and Nones like an array of integers; the Nones pass through as None.\r\n\r\n```python\r\n>>> print(trk_mom[hit_id.mask[hit_z >= 100]])\r\n[[None, 1.1, 1.1, None, 2.2], [], ... None, 3.3], [5.5, 5.5, None], [None, 8.8, 7.7]]\r\n```\r\n\r\nAs @tamasgal said, you can use [ak.flatten](https://awkward-array.readthedocs.io/en/latest/_auto/ak.flatten.html) to get rid of the nested list structure, but it also gets rid of None values. (None is treated like an empty list for the purposes of flattening.) Note that the default `axis` for `ak.flatten` is `axis=1`, which only flattens the first level, but you might need to flatten all levels at once; `axis=None` does that:\r\n\r\n```python\r\n>>> print(ak.flatten(trk_mom[hit_id.mask[hit_z >= 100]], axis=1))\r\n[None, 1.1, 1.1, None, 2.2, 4.4, None, 3.3, 5.5, 5.5, None, None, 8.8, 7.7]\r\n>>> print(ak.flatten(trk_mom[hit_id.mask[hit_z >= 100]], axis=None))\r\n[1.1, 1.1, 2.2, 4.4, 3.3, 5.5, 5.5, 8.8, 7.7]\r\n```",
     "createdAt":"2020-12-16T15:58:57Z",
     "number":215769,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":2
  },
  "createdAt":"2020-12-16T06:25:34Z",
  "number":229,
  "title":"How to handle data related by id",
  "url":"https://github.com/scikit-hep/uproot5/discussions/229"
 },
 {
  "author":{
   "login":"DraTeots"
  },
  "body":"Actually, this relates to this question https://github.com/scikit-hep/uproot4/discussions/229 and the same data as there - hits and tracks connected by ids\r\n\r\nWe have a data field which is a vector of std::strings. \r\n\r\n```python\r\ntree['hit_vol_name'].interpretation\r\n```\r\n\r\ngives\r\n```\r\nAsObjects(AsVector(True, AsString(False)))\r\n```\r\n\r\nIn reality each hit has a string with the volume name. Not very effective in terms of performance, but beyond this discussion. These strings looks like **\"ci_EMCAL_section1_tower22\"** and the naming is organized so that if one wants to get all hits in central-ion-cap EMCAL, he just need a filter like \r\n\r\n```python\r\nhit_vol_name.startswith('ci_EMCAL')\r\n```\r\n\r\nI'm sure I could somehow flatten the data and those string would appear correctly in flattened arrays, where I could use ```numpy.char.startswith``` to select all hits from ci_EMCAL\r\n\r\nBut I'm wonder if one can do it inside iterate method without data flattening. Because as in #229 I would like to use this to get certain tracks:\r\n\r\n```python\r\nfor batch in tree.iterate(['hit_vol_name', 'hit_trk_id', 'trk_mom', 'trk_id'], step_size=5, entry_stop=10):\r\n   # then I want all hits in ci_EMCAL, so I do something like\r\n   good_tracks_id = batch.hit_trk_id[ak.char.startswith(batch.hit_vol_name, 'ci_EMCAL')]   # no such method!\r\n   # now, having good_tracks_id I select tracks momentums as it resolved in #229\r\n```\r\n\r\n\r\n\r\n",
  "comments":{
   "nodes":[
    {
     "author":{
      "login":"jpivarski"
     },
     "body":"There isn't an `ak.char.startswith` (or any other `ak.char.*` functions), which would be vectorized versions of the ordinary string methods. The motivations for this would be\r\n\r\n   * convenience (it's a one-liner) and\r\n   * performance: in principle, such a function could be specialized, compiled C or C++. If an `ak.char.*` suite of functions is implemented like all the other Awkward functions, they'd also be part of the CPU \u2192 GPU port: all Awkward operations will eventually be CPU/GPU agnostic.\r\n\r\nSo that would be nice, and it could be a feature request for Awkward Array (probably not addressed in the next month or so, to be honest).\r\n\r\nIf you main concern is performance, note that your interpretation, `AsObjects(AsVector(True, AsString(False)))`, cannot be vectorized, so it's interpreted with auto-generated Python code, rather than a NumPy cast. Before we get to nice touches like `ak.char.startswith`, I want to finish the job started with PR #96: moving that auto-generated Python code with a C++ state machine. In the first case that was tested, this was a factor of 80\u00d7 speedup, and I expect that to be typical. (It's still non-vectorized, but that's less of a performance hit in C++ than in Python.) A blocker for this is #38: I need to understand memberwise splitting in Python before carving it into stone (porting to C++). So if your concern is performance, there are other hotspots that are higher priority before getting to vectorizing string functions.\r\n\r\nIf your concern is convenience, you might want to consider a list comprehension.\r\n\r\nWait a minute\u2014(facepalm)\u2014there's a nice implementation for vectorized `startswith` that you can do right now:\r\n\r\n```python\r\n>>> strings = ak.Array([\r\n...     [\"ci_EMCAL-one\", \"ci_EMCAL-two\", \"something else\"],\r\n...     [],\r\n...     [\"yet another thing\", \"ci_EMCAL-three\"]])\r\n\r\n>>> print(strings[:, :, :len(\"ci_EMCAL\")])\r\n[['ci_EMCAL', 'ci_EMCAL', 'somethin'], [], ['yet anot', 'ci_EMCAL']]\r\n\r\n>>> print(strings[:, :, :len(\"ci_EMCAL\")] == \"ci_EMCAL\")\r\n[[True, True, False], [], [False, True]]\r\n```\r\n\r\nThere you go.  `:)`  Sorry for the digression.",
     "createdAt":"2020-12-16T15:31:52Z",
     "number":215556,
     "replies":{
      "nodes":[],
      "totalCount":0
     }
    }
   ],
   "totalCount":1
  },
  "createdAt":"2020-12-16T06:40:29Z",
  "number":230,
  "title":"Vector of std::strings starts with",
  "url":"https://github.com/scikit-hep/uproot5/discussions/230"
 }
]
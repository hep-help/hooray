[
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hello,\r\n\r\nI'm using uproot.dask routine to process root files.\r\nIt works fine with library='ak', and it works 'np' with 1D array.\r\nHowever, it breaks for multidimensional arrays.\r\nPlease see an example below.\r\n\r\n   --Yuri.\r\n\r\n```\r\n>>> uproot.__version__\r\n'5.0.2'\r\n\r\ndd_run = uproot.dask(file_list,  library='np')\r\ndd_run['temperature'].compute()\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[41], line 1\r\n----> 1 dd_run['temperature'].compute()\r\n\r\nFile ~/.conda/envs/MyEnv/lib/python3.10/site-packages/dask/base.py:315, in DaskMethodsMixin.compute(self, **kwargs)\r\n    291 def compute(self, **kwargs):\r\n    292     \"\"\"Compute this dask collection\r\n    293 \r\n    294     This turns a lazy Dask collection into its in-memory equivalent.\r\n   (...)\r\n    313     dask.base.compute\r\n    314     \"\"\"\r\n--> 315     (result,) = compute(self, traverse=False, **kwargs)\r\n    316     return result\r\n\r\nFile ~/.conda/envs/MyEnv/lib/python3.10/site-packages/dask/base.py:601, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\r\n    598     postcomputes.append(x.__dask_postcompute__())\r\n    600 results = schedule(dsk, keys, **kwargs)\r\n--> 601 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/.conda/envs/MyEnv/lib/python3.10/site-packages/dask/base.py:601, in <listcomp>(.0)\r\n    598     postcomputes.append(x.__dask_postcompute__())\r\n    600 results = schedule(dsk, keys, **kwargs)\r\n--> 601 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n\r\nFile ~/.conda/envs/MyEnv/lib/python3.10/site-packages/dask/array/core.py:1282, in finalize(results)\r\n   1280 while isinstance(results2, (tuple, list)):\r\n   1281     if len(results2) > 1:\r\n-> 1282         return concatenate3(results)\r\n   1283     else:\r\n   1284         results2 = results2[0]\r\n\r\nFile ~/.conda/envs/MyEnv/lib/python3.10/site-packages/dask/array/core.py:5291, in concatenate3(arrays)\r\n   5289         while arr.ndim < ndim:\r\n   5290             arr = arr[None, ...]\r\n-> 5291     result[idx] = arr\r\n   5293 return result\r\n\r\nValueError: could not broadcast input array from shape (31006,6,64) into shape (31006,)\r\n```",
  "closed_at":"2024-01-30T15:34:07Z",
  "comments":2,
  "created_at":"2023-01-05T06:00:26Z",
  "id":1520134873,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5am2rZ",
  "number":811,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot.dask with library='np' does not work as expected",
  "updated_at":"2024-01-30T15:34:08Z",
  "user":"MDQ6VXNlcjQ4OTI3MzA2"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"The `da[da.int_branch < 0]` expression should return an array with all fields, not just `\"int_branch\"`. An update in dask-awkward (2023.1.0) got it right, and older version (2022.12a3.dev6+g7fe448a) got it wrong, and presumably the old test was modeled on the old dask-awkward result.\r\n\r\nThere's still an error in these tests, which comes from scikit-hep/scikit-hep-testdata#111. A ROOT file was updated that the Uproot RNTuple test can no longer read. @Moelf?",
  "closed_at":"2023-01-06T19:57:02Z",
  "comments":1,
  "created_at":"2023-01-06T19:17:06Z",
  "draft":false,
  "id":1523027999,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5G1_MQ",
  "number":812,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-01-06T19:57:02Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: an uproot.dask test was wrong; revealed by new dask-awkward.",
  "updated_at":"2023-01-06T19:57:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"testing on something real simple:\r\n```python\r\nimport uproot\r\nimport awkward as ak\r\n\r\ndef test_writable():\r\n    filepath = \"/tmp/test.root\"\r\n\r\n    with uproot.recreate(filepath) as file:\r\n        akform = ak.forms.RecordForm(\r\n            [\r\n                ak.forms.NumpyForm(\"int32\"),\r\n            ],\r\n            [\"one\"],\r\n        )\r\n        file.mkrntuple(\"ntuple\", akform)\r\n        array = array = ak.from_iter([{\"one_integers\":1}])\r\n        a = file[\"ntuple\"]\r\n        a.extend(array)\r\n        assert type(file[\"ntuple\"]).__name__ == \"WritableNTuple\"\r\n\r\ntest_writable()\r\n```\r\n\r\nand can be read back from Julia\r\n```julia\r\njulia> f = ROOTFile(\"/tmp/test.root\")\r\nROOTFile with 1 entry and 0 streamers.\r\n/tmp/test.root\r\n\u2514\u2500 ntuple (ROOT::Experimental::RNTuple)\r\n\r\n\r\njulia> LazyTree(f, \"ntuple\")\r\n Row \u2502 one\r\n     \u2502 Int32\r\n\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n 1   \u2502 9\r\n 2   \u2502 8\r\n 3   \u2502 7\r\n 4   \u2502 6\r\n 5   \u2502 5\r\n 6   \u2502 4\r\n 7   \u2502 3\r\n 8   \u2502 2\r\n 9   \u2502 1\r\n 10  \u2502 0\r\n```\r\n\r\nBut ROOT doesn't like it:\r\n```python\r\n>>> import ROOT\r\n>>> ROOT.Experimental.RNTupleReader.Open(\"ntuple\", \"./test.root\")\r\n...<long error>\r\nbasic_string_view<char,char_traits<char> > storage, const ROOT::Experimental::RNTupleReadOptions& options = ROOT::Experimental::RNTupleReadOptions()) =>\r\n    RException: page list frame too short\r\n```\r\n\r\nwhich points to:\r\nhttps://github.com/root-project/root/blob/36bf93c68cbe581b22e6f7d31046860b7fddefc3/tree/ntuple/v7/src/RNTupleSerialize.cxx#L1426\r\n\r\nupon further re-reading of the specification:\r\n>The inner list is followed by a 64bit unsigned integer element offset and the 32bit compression settings (see Section \"Basic Types\"). \r\n\r\nwas something I missed before.",
  "closed_at":"2023-02-15T18:40:52Z",
  "comments":3,
  "created_at":"2023-01-07T06:41:24Z",
  "draft":false,
  "id":1523535126,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5G3o7J",
  "number":813,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-15T18:40:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: [WIP] RNTuple Basic Writing",
  "updated_at":"2023-02-15T18:40:53Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"The following example script generates two files with the same data in slightly different ways. `uproot` can successfully read the first file. But it fails to read the second file (created with `RDF::Snapshot`).\r\n```python\r\nimport ROOT\r\nimport uproot\r\n\r\ndef make_file_1_with_root():\r\n    tfile = ROOT.TFile(\"tfile_with_tvector3_1.root\", \"RECREATE\")\r\n    tree = ROOT.TTree(\"tree\", \"tree\")\r\n    tvector3 = ROOT.TVector3()\r\n    tree.Branch(\"tvector3\", tvector3)\r\n    for x in range(10):\r\n        tvector3.SetX(x)\r\n        tree.Fill()\r\n    tree.AutoSave()\r\n    tfile.Close()\r\n\r\ndef make_file_2_with_root():\r\n    tfile = ROOT.TFile(\"tfile_with_tvector3_1.root\", \"READ\")\r\n    tree = tfile.Get(\"tree\")\r\n    rdf = ROOT.RDataFrame(tree)\r\n    branchlist = ROOT.std.vector(ROOT.std.string)()\r\n    branchlist.push_back(\"tvector3\")\r\n    rdf.Snapshot(\"tree\", \"tfile_with_tvector3_2.root\", branchlist)\r\n    return\r\n\r\ndef read_file_1_with_uproot():\r\n    print(\"reading file 1\")\r\n    uproot.open(\"tfile_with_tvector3_1.root:tree\").arrays()\r\n\r\ndef read_file_2_with_uproot():\r\n    print(\"reading file 2\")\r\n    uproot.open(\"tfile_with_tvector3_2.root:tree\").arrays()\r\n\r\n\r\nmake_file_1_with_root()\r\nmake_file_2_with_root()\r\nread_file_1_with_uproot() # uproot successfully reads file 1\r\nread_file_2_with_uproot() # uproot fails to read file 2\r\n```\r\n\r\n`uproot.open(\"tfile_with_tvector3_2.root:tree\").arrays()` fails with the error message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/dave/work/reproduce_uproot_bug/reproduce_uproot_bug.py\", line 36, in <module>\r\n    read_file_2_with_uproot() # uproot fails to read file 2\r\n  File \"/home/dave/work/reproduce_uproot_bug/reproduce_uproot_bug.py\", line 30, in read_file_2_with_uproot\r\n    uproot.open(\"tfile_with_tvector3_2.root:tree\").arrays()\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 834, in arrays\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3124, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3093, in basket_to_array\r\n    arrays[branch.cache_key] = interpretation.final_array(\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/uproot/interpretation/objects.py\", line 397, in final_array\r\n    output = library.finalize(\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/uproot/interpretation/library.py\", line 592, in finalize\r\n    unlabeled = awkward.from_iter(\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/awkward/operations/ak_from_iter.py\", line 69, in from_iter\r\n    return _impl(iterable, highlevel, behavior, allow_record, initial, resize)\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/awkward/operations/ak_from_iter.py\", line 94, in _impl\r\n    builder.fromiter(iterable)\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/uproot/interpretation/library.py\", line 593, in <genexpr>\r\n    (_object_to_awkward_json(form, x) for x in array), highlevel=False\r\n  File \"/home/dave/work/reproduce_uproot_bug/env/uprootbug/lib/python3.10/site-packages/uproot/interpretation/library.py\", line 323, in _object_to_awkward_json\r\n    return [_object_to_awkward_json(subform, x) for x in obj]\r\nTypeError: 'Model_TVector3_v3' object is not iterable\r\n\r\n```\r\n\r\nROOT seems to be able to read the second file fine without problems so I don't think it's a bug in `RDF::Snapshot` causing a corrupt file.\r\n\r\nVersions:\r\n```\r\npython==3.10\r\nroot==6.26.10\r\nuproot==5.0.2\r\n```\r\n\r\nA small project that reproduces the issue:\r\n[reproduce_uproot_bug.zip](https://github.com/scikit-hep/uproot5/files/10383299/reproduce_uproot_bug.zip)\r\n\r\nThe two files:\r\n[tfiles_with_tvector3.zip](https://github.com/scikit-hep/uproot5/files/10383314/tfiles_with_tvector3.zip)\r\n",
  "closed_at":"2023-02-23T14:26:35Z",
  "comments":1,
  "created_at":"2023-01-10T13:58:27Z",
  "id":1527435206,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5bCs_G",
  "number":814,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot fails to read branch of TVector3 created with RDF::Snapshot",
  "updated_at":"2023-02-23T14:26:35Z",
  "user":"MDQ6VXNlcjk4NDUwMzQ="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjE4NTI0NDc=",
  "assignees":null,
  "author_association":"NONE",
  "body":"When generating arrays from multiple branches with different `vector<vector<T>>` types in uproot5, it seems that the type of the first such branch is inadvertently retained for any subsequent ones, causing them to be read incorrectly.\r\n\r\nE.g. reading `vector<vector<int32>>` first causes subsequent `vector<vector<float32>>` branches to be read as int (and similar incorrect behaviour if the ordering is changed). This also seems to corrupt the type of that branch permanently, so that e.g. calling `tree['branchname'].array()` breaks in the AwkwardForth generation. The output is detailed together with a demonstrator below. If instead the branches are made into arrays individually, the correct type determination occurs when later calling `arrays()` on the same branches.\r\n\r\nExpected behaviour: The type of each branch should be determined independently during execution of `TTree.arrays()`, equivalent to something like:\r\n```\r\nak.zip({bname:tree[bname].array() for bname in branchlist},depth_limit=1)\r\n```\r\nwhich gets all the types correct.\r\n\r\nSee a short demonstrator with the offending input file at:\r\nhttps://cernbox.cern.ch/s/MxDfDFVmYZTMfRf\r\n* Running this produces the output:\r\n```\r\n100 * {offline_akt4_pf_NOSYS_SumPtTrkPt500: var * var * float32, offline_akt4_pf_NOSYS_NumTrkPt1000: var * var * float32, offline_akt4_pf_NOSYS_TrackWidthPt1000: var * var * float32}\r\n100 * var * var * float32\r\nTraceback (most recent call last):\r\n  File \"/Users/khoo/anaconda3/envs/JVT/lib/python3.9/site-packages/uproot/interpretation/objects.py\", line 232, in basket_array_forth\r\n    context[\"forth\"].vm = awkward.forth.ForthMachine64(\r\nValueError: in AwkwardForth source code, line 7 col 1, input names, output names, variable names, and user-defined words must all be unique and not reserved words or integers:\r\n\r\n    output node0-offsets \r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob//src/libawkward/forth/ForthMachine.cpp#L1969)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/khoo/Work/ATLAS/JVT_retuning/run/vecvec_type_demonstrator.py\", line 14, in <module>\r\n    print(t[v].array().type)\r\n  File \"/Users/khoo/anaconda3/envs/JVT/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 1818, in array\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/Users/khoo/anaconda3/envs/JVT/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 3124, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/Users/khoo/anaconda3/envs/JVT/lib/python3.9/site-packages/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/Users/khoo/anaconda3/envs/JVT/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 3066, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/Users/khoo/anaconda3/envs/JVT/lib/python3.9/site-packages/uproot/interpretation/objects.py\", line 139, in basket_array\r\n    output = self.basket_array_forth(\r\n  File \"/Users/khoo/anaconda3/envs/JVT/lib/python3.9/site-packages/uproot/interpretation/objects.py\", line 236, in basket_array_forth\r\n    raise type(err)(\r\nValueError: in AwkwardForth source code, line 7 col 1, input names, output names, variable names, and user-defined words must all be unique and not reserved words or integers:\r\n\r\n    output node0-offsets \r\n\r\n(https://github.com/scikit-hep/awkward-1.0/blob//src/libawkward/forth/ForthMachine.cpp#L1969)\r\n\r\nForth code generated for this data type:\r\n\r\ninput stream\r\n    input byteoffsets\r\n    input bytestops\r\n    output node0-offsets int64\r\noutput node1-offsets int64\r\noutput node2-data float32\r\noutput node0-offsets int64\r\noutput node1-offsets int64\r\noutput node2-data float32\r\n\r\n    0 node0-offsets <- stack\r\n0 node1-offsets <- stack\r\n0 node0-offsets <- stack\r\n0 node1-offsets <- stack\r\n\r\n    0 do\r\n    byteoffsets I-> stack\r\n    stream seek\r\n    6 stream skip\r\nstream !I-> stack\r\n dup node0-offsets +<- stack\r\n0 do\r\nstream !I-> stack\r\n dup node1-offsets +<- stack\r\nstream #!f-> node2-data\r\nloop\r\n6 stream skip\r\nstream !I-> stack\r\n dup node0-offsets +<- stack\r\n0 do\r\nstream !I-> stack\r\n dup node1-offsets +<- stack\r\nstream #!f-> node2-data\r\nloop\r\n\r\n    loop\r\n```\r\n* Changing `do_hack = True` instead produces the expected output:\r\n```\r\n100 * var * var * float32\r\n100 * var * var * int32\r\n100 * var * var * float32\r\n100 * {offline_akt4_pf_NOSYS_SumPtTrkPt500: var * var * float32, offline_akt4_pf_NOSYS_NumTrkPt1000: var * var * int32, offline_akt4_pf_NOSYS_TrackWidthPt1000: var * var * float32}\r\n100 * var * var * float32\r\n100 * var * var * int32\r\n100 * var * var * float32\r\n```",
  "closed_at":"2023-01-27T09:48:06Z",
  "comments":9,
  "created_at":"2023-01-12T14:47:29Z",
  "id":1530843195,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5bPtA7",
  "number":816,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Incorrect type determined for vector<vector<T>> branches in TTree.arrays()",
  "updated_at":"2023-01-27T09:48:06Z",
  "user":"MDQ6VXNlcjE1MDk2NDk1"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjE4NTI0NDc=",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Reported by @nsmith-, problem identified by @agoose77.\r\n\r\n```python\r\n>>> fut.compute(scheduler=\"synchronous\")\r\n<Array [[469, 459, 188, 17.8], ..., [414, ...]] type='203939 * var * float32'>\r\n>>> fut.compute(scheduler=\"threads\")\r\nTraceback (most recent call last):\r\n...\r\nzstd.ZstdError: decompression error: Corrupted block detected\r\n```\r\n\r\nhttps://github.com/indygreg/python-zstandard/issues/166\r\n\r\nWe explicitly create one `ZstdDecompressor` object and use it indiscriminately.\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/afd8deb3a18aabbd26b162efa2dcdf640f4329e6/src/uproot/compression.py#L236-L247\r\n\r\nWhen multiple threads call this `_DecompressZSTD.decompress` method, they'll fail.\r\n\r\nWhat we ought to do instead is something like\r\n\r\n```python\r\n    def __init__(self):\r\n        self._decompressor = threading.local()\r\n        self._decompressor.obj = None\r\n\r\n    @property\r\n    def decompressor(self):\r\n        if self._decompressor.obj is None:\r\n            zstandard = uproot.extras.zstandard()\r\n            self._decompressor.obj = zstandard.ZstdDecompressor()\r\n        return self._decompressor.obj\r\n\r\n    def decompress(self, data, uncompressed_bytes=None):\r\n        return self.decompressor.decompress(data)\r\n```\r\n\r\nbecause `self._decompressor.obj` would be a distinct `ZstdDecompressor` in every thread.\r\n\r\nSee https://docs.python.org/3/library/threading.html#thread-local-data.",
  "closed_at":"2023-02-15T18:30:42Z",
  "comments":0,
  "created_at":"2023-01-20T23:59:03Z",
  "id":1551603092,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5ce5WU",
  "number":817,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Multithreaded ZSTD decompression is not thread-safe",
  "updated_at":"2023-02-15T18:30:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"The blue box under https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file does not list \"strings,\" though it is possible to get string data into ROOT as a variable-length array of `uint8`:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> import uproot\r\n>>> file = uproot.recreate(\"/tmp/some.root\")\r\n>>> array = ak.Array([\"one\", \"two\", \"three\", \"four\", \"five\"])\r\n>>> file[\"tree\"] = {\"branch\": ak.without_parameters(array)}\r\n>>> file[\"tree\"].show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nnbranch              | int32_t                  | AsDtype('>i4')\r\nbranch               | uint8_t[]                | AsJagged(AsDtype('uint8'))\r\n```\r\n\r\nOne can _use_ this as strings, perhaps by casting the `uint8_t*` array as a `char*` array in C++, or like this:\r\n\r\n```python\r\n>>> import ROOT\r\n>>> file = ROOT.TFile(\"/tmp/some.root\")\r\n>>> tree = file.Get(\"tree\")\r\n>>> for x in tree:\r\n...   print(\"\".join(chr(y) for y in x.branch))\r\n... \r\none\r\ntwo\r\nthree\r\nfour\r\nfive\r\n```\r\n\r\nBut this format is not the string format that ROOT users expect. This has a counter branch, and the strings above are not (necessarily) null-terminated. ROOT users expect bare strings to have leaf type `TLeafC` (and `std::string` is its own thing, with a more extensive header).\r\n\r\nThe current implementation is not very far away from one that can do that. `TLeafC` encoding consists of a 1-byte or 5-byte length (the latter for strings with 255 bytes or more) followed by the character data, which can be constructed from two or three arrays using `ak.concatenate`.",
  "closed_at":"2023-09-25T16:01:01Z",
  "comments":2,
  "created_at":"2023-01-22T21:16:21Z",
  "id":1552287859,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5chghz",
  "number":818,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Write Awkward Array's `string` and `bytestring` formats as TTrees with `TLeafC`",
  "updated_at":"2023-09-25T16:01:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2023-01-27T09:48:04Z",
  "comments":1,
  "created_at":"2023-01-26T19:52:50Z",
  "draft":false,
  "id":1558691390,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5InsAA",
  "number":819,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-01-27T09:48:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: separate AwkwardForth machine for each TBranch context.",
  "updated_at":"2023-01-27T09:48:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Does this fix the\r\n\r\n```python\r\n>>> fut.compute(scheduler=\"synchronous\")\r\n<Array [[469, 459, 188, 17.8], ..., [414, ...]] type='203939 * var * float32'>\r\n>>> fut.compute(scheduler=\"threads\")\r\nTraceback (most recent call last):\r\n...\r\nzstd.ZstdError: decompression error: Corrupted block detected\r\n```\r\n\r\nissue, @nsmith-?",
  "closed_at":"2023-02-15T18:30:41Z",
  "comments":2,
  "created_at":"2023-01-26T20:06:22Z",
  "draft":false,
  "id":1558708464,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Invs0",
  "number":820,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-15T18:30:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: separate ZstdDecompressor for each thread.",
  "updated_at":"2023-02-16T18:37:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/psf/black: 22.12.0 \u2192 23.1.0](https://github.com/psf/black/compare/22.12.0...23.1.0)\n- [github.com/PyCQA/isort: 5.11.4 \u2192 5.12.0](https://github.com/PyCQA/isort/compare/5.11.4...5.12.0)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-02-15T18:16:04Z",
  "comments":0,
  "created_at":"2023-01-31T00:49:46Z",
  "draft":false,
  "id":1563437935,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5I3jsU",
  "number":821,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-15T18:16:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"ci: pre-commit autoupdate",
  "updated_at":"2023-02-15T18:16:06Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This is an oversight in the implementation (both Uproot and Awkward). The policy is that `unknown` should become `float64` if it needs to be an array, because\r\n\r\n```python\r\nnp.array([])\r\n```\r\n\r\nmakes an array of `np.float64`. This issue is to say that Uproot should do that automatically, and there will be an issue on Awkward to say that `np.values_astype(empty_array, np.float64)` should turn the `EmptyArrays` into `NumpyArrays` with the specified type.\r\n\r\n### Discussed in https://github.com/scikit-hep/awkward/discussions/2186\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **fleble** February  1, 2023</sup>\r\nDear experts,\r\n\r\nI am writing ROOT files using uproot. One problem I face is when a branch is empty, thus having unknown type, which cannot be written down with uproot.\r\n\r\n```bash\r\n>>> ak.__version__\r\n1.5.1\r\n>>> uproot.__version__\r\n4.3.7\r\n```\r\n\r\nThis script\r\n```python\r\nak_array = ak.Array([[], [], []])\r\nak_array = ak.values_astype(ak_array, np.float64)\r\n\r\ntree = {\"branch\": ak_array}\r\n\r\nwith uproot.recreate(\"test.root\") as file:\r\n    file[\"test\"] = tree\r\n```\r\n terminates with the following error:\r\n ```\r\n TypeError: cannot write Awkward Array type to ROOT file:\r\n\r\n    var * unknown\r\n ```\r\n\r\nAnd:\r\n ```bash\r\n>>> ak_array = ak.Array([[], [], []])\r\n>>> ak.values_astype(ak_array, np.float64)\r\n <Array [[], [], []] type='3 * var * unknown'> \r\n ```\r\n \r\n Is it possible to give a type to empty to empty awkward array?</div>\r\n\r\n---------------\r\n\r\n> After some attempt, I found a trick by concatenating an ak array of the desired type, and filtering out the part that was concatenated.\r\n\r\nClever! But we'll fix it anyway.",
  "closed_at":"2023-04-06T16:18:38Z",
  "comments":5,
  "created_at":"2023-02-01T14:19:07Z",
  "id":1566204730,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5dWmM6",
  "number":822,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Writing Awkward Arrays of type unknown",
  "updated_at":"2023-04-06T16:18:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"- chore: move to using Ruff\n- style: refactor remaining issues\n",
  "closed_at":"2023-02-17T19:43:32Z",
  "comments":1,
  "created_at":"2023-02-16T14:28:40Z",
  "draft":false,
  "id":1587777599,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KIqO0",
  "number":825,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-17T19:43:32Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: move to Ruff",
  "updated_at":"2023-02-17T19:43:33Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"I also sorted the dict, to make it easier to spot when something is missing.",
  "closed_at":"2023-02-17T19:27:20Z",
  "comments":3,
  "created_at":"2023-02-16T17:33:59Z",
  "draft":false,
  "id":1588098873,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KJwKN",
  "number":827,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-17T19:27:19Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: `_awkward_forth.symbol_dict` was missing `np.dtype(\">i1\")`",
  "updated_at":"2023-02-17T19:27:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Continuation of #820",
  "closed_at":"2023-02-16T21:12:57Z",
  "comments":1,
  "created_at":"2023-02-16T18:43:01Z",
  "draft":false,
  "id":1588192404,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KKElY",
  "number":828,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-16T21:12:56Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: separate ZstdDecompressor for each thread (re-do)",
  "updated_at":"2023-02-16T21:12:57Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @renyhp as a contributor for code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot5/pull/827#issuecomment-1434897341)\n\n[skip ci]",
  "closed_at":"2023-02-17T16:42:38Z",
  "comments":0,
  "created_at":"2023-02-17T16:41:32Z",
  "draft":false,
  "id":1589649881,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KO-5-",
  "number":829,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-17T16:42:37Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add renyhp as a contributor for code",
  "updated_at":"2023-02-17T16:42:39Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"This PR implements the interface (to be merged) defined in https://github.com/douglasdavis/dask-awkward/pull/3 to implement form remapping in `uproot.dask`.\r\n\r\nI have confirmed this so far with local testing, ideas for a minimal test would be appreciated!\r\n\r\n@jpivarski ",
  "closed_at":"2023-02-17T19:49:06Z",
  "comments":4,
  "created_at":"2023-02-17T17:08:10Z",
  "draft":false,
  "id":1589695926,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KPJF0",
  "number":830,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-17T19:49:06Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: allow `uproot.dask` to re-map forms at the data source",
  "updated_at":"2023-03-01T22:49:03Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"I know, I know, we'll have to add a test runner without Awkward installed to test this. For now, I'm just getting it back into a can-run-without-Awkward state and we'll add the test runner later.",
  "closed_at":"2023-02-20T15:36:25Z",
  "comments":0,
  "created_at":"2023-02-17T19:54:52Z",
  "draft":false,
  "id":1589885540,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KPx7l",
  "number":831,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-20T15:36:25Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: Uproot should be able to run without Awkward.",
  "updated_at":"2023-02-20T15:36:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This is essentially completing #784, which added docstrings to materialized arrays, but not the typetracers that Dask uses.",
  "closed_at":"2023-02-18T16:12:39Z",
  "comments":1,
  "created_at":"2023-02-17T21:37:31Z",
  "draft":false,
  "id":1589989787,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KQH8m",
  "number":832,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-18T16:12:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: ak_add_doc should add docs to both the lazy and the materialized array.",
  "updated_at":"2023-02-18T16:12:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjE4NTI0NDc=",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"### Discussed in https://github.com/scikit-hep/uproot5/discussions/826\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **renyhp** February 16, 2023</sup>\r\nHi, I have just installed uproot, but I can't get it to read a tree.\r\n\r\nMy code is very simple:\r\n\r\n```python3\r\nimport uproot\r\nf = uproot.reading.open(\"myfile.root\")\r\nt = f.get(\"aTree\")\r\nt['TheOnlyBranch'].array()\r\n```\r\n\r\nThis just crashes with this stacktrace:\r\n```text\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 834, in arrays\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3124, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/behaviors/TBranch.py\", line 3066, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/interpretation/objects.py\", line 139, in basket_array\r\n    output = self.basket_array_forth(\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/interpretation/objects.py\", line 206, in basket_array_forth\r\n    output = self._discover_forth(\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/interpretation/objects.py\", line 286, in _discover_forth\r\n    output[i] = self._model.read(\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/containers.py\", line 609, in read\r\n    return uproot.deserialization.read_object_any(\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/deserialization.py\", line 274, in read_object_any\r\n    obj = cls.read(chunk, cursor, context, file, selffile, parent)\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/model.py\", line 1372, in read\r\n    versioned_cls.read(\r\n  File \"/tmp/venv/lib/python3.10/site-packages/uproot/model.py\", line 867, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"<dynamic>\", line 567, in read_members\r\nKeyError: dtype('int8')\r\n```\r\n\r\nThe branch does contain subbranches, some of them are of type `int8` and `uint8`.\r\n\r\nAm I doing anything wrong or is this a bug to be reported?</div>",
  "closed_at":"2023-02-21T15:59:41Z",
  "comments":0,
  "created_at":"2023-02-17T21:40:28Z",
  "id":1589992129,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5exVrB",
  "number":833,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Error in untested AwkwardForth case",
  "updated_at":"2023-02-21T15:59:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":null,
  "closed_at":"2023-03-01T22:40:53Z",
  "comments":3,
  "created_at":"2023-02-18T15:27:41Z",
  "draft":false,
  "id":1590377698,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KRV0L",
  "number":834,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-01T22:40:53Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: let form mappings apply a behavior",
  "updated_at":"2023-03-01T22:40:53Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Define top-level docstring for tree by kwarg.",
  "closed_at":"2023-02-18T16:53:42Z",
  "comments":0,
  "created_at":"2023-02-18T15:57:39Z",
  "draft":false,
  "id":1590385965,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KRXSO",
  "number":835,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: pass tree title as a docstr when remapping",
  "updated_at":"2023-02-18T16:53:42Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"for ```x = uproot.dask(ttree, ak_add_doc=True)``` this will cause x to have parameter `__doc__` to be set to `ttree.title`",
  "closed_at":"2023-02-21T15:39:52Z",
  "comments":1,
  "created_at":"2023-02-18T17:22:08Z",
  "draft":false,
  "id":1590409244,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KRbj-",
  "number":836,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-21T15:39:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: add ttree title to __doc__ of top level record",
  "updated_at":"2023-02-21T15:39:52Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This came up in Discussion #826 (Issue #833): the AwkwardForth errors occur on an Interpretation that could never have been Awkward anyway.\r\n\r\nAn early hint was that it was going through [read_object_any](https://github.com/scikit-hep/uproot5/blob/c24159917ea5732ada3bb0b03615707823194197/src/uproot/deserialization.py#L184-L319), which means arbitrary-typed pointers, circular references, and a whole host of things that are beyond the scope of Awkward Array, but the clearest indication was that we got the \"Cannot be Awkward\" error in TTrees with zero entries\u2014if it could have gotten past the AwkwardForth read, it would have encountered this error, anyway. (It didn't get past the AwkwardForth read because we don't generate AwkwardForth inside `read_object_any`.)\r\n\r\nSo this PR\r\n\r\n  * takes the check out of `Library.finalize`\r\n  * puts it into `_ranges_or_baskets_to_arrays`, on the main thread, before any reading starts[^1]\r\n  * fixes an unrelated error in `_util`, but it was uncovered by this PR.\r\n\r\n[^1]: Well, actually, `Source.chunks` has already been called, so the bytes have been read from local files or a remote server has been queried, but we haven't waited for any data to come back yet and won't be decompressing it or interpreting it if it does.",
  "closed_at":"2023-02-21T15:59:40Z",
  "comments":1,
  "created_at":"2023-02-20T17:12:45Z",
  "draft":false,
  "id":1592218435,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KXTYZ",
  "number":838,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-21T15:59:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: complain about CannotBeAwkward earlier, before reading data.",
  "updated_at":"2023-02-21T15:59:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/charliermarsh/ruff-pre-commit: v0.0.246 \u2192 v0.0.249](https://github.com/charliermarsh/ruff-pre-commit/compare/v0.0.246...v0.0.249)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-02-21T16:01:31Z",
  "comments":0,
  "created_at":"2023-02-21T00:08:20Z",
  "draft":false,
  "id":1592584318,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KYhGt",
  "number":839,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-21T16:01:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"ci: [pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2023-02-21T16:01:32Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR address issue #761 and adds support for `TLeafG`",
  "closed_at":"2023-02-23T18:45:37Z",
  "comments":1,
  "created_at":"2023-02-22T11:56:22Z",
  "draft":false,
  "id":1594975713,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Kgd1_",
  "number":840,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-23T18:45:37Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add support for TLeafG",
  "updated_at":"2023-02-23T18:45:39Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Fix for #814",
  "closed_at":"2023-02-23T14:26:33Z",
  "comments":2,
  "created_at":"2023-02-23T12:09:50Z",
  "draft":false,
  "id":1596744269,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5KmbSV",
  "number":841,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-02-23T14:26:33Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: models should not be wrapped in a ListOffsetForm",
  "updated_at":"2023-03-01T22:51:01Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/charliermarsh/ruff-pre-commit: v0.0.249 \u2192 v0.0.255](https://github.com/charliermarsh/ruff-pre-commit/compare/v0.0.249...v0.0.255)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-03-16T19:40:41Z",
  "comments":0,
  "created_at":"2023-02-28T00:22:00Z",
  "draft":false,
  "id":1602164303,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5K4ia2",
  "number":842,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-16T19:40:41Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"ci: [pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2023-03-16T19:40:42Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This is a code cleanup PR \u2014 we already define `awkward` as a core dependency, so we don't need to repeat this in the other dependency groups now.",
  "closed_at":"2023-03-02T14:23:32Z",
  "comments":1,
  "created_at":"2023-02-28T23:46:57Z",
  "draft":false,
  "id":1603985005,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5K-sUN",
  "number":843,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-02T14:23:32Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: don't repeat awkward dependency",
  "updated_at":"2023-03-02T14:23:34Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-03-09T20:51:08Z",
  "comments":2,
  "created_at":"2023-03-01T14:21:08Z",
  "draft":false,
  "id":1605068663,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5LCXqU",
  "number":844,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-09T20:51:08Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: unable to delete hist from ROOT file",
  "updated_at":"2023-03-09T20:51:10Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-03-02T14:38:21Z",
  "comments":0,
  "created_at":"2023-03-01T14:59:17Z",
  "draft":false,
  "id":1605132756,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5LClie",
  "number":845,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-02T14:38:21Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: test 0814 name",
  "updated_at":"2023-03-02T14:38:22Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-03-02T14:55:04Z",
  "comments":0,
  "created_at":"2023-03-01T16:56:28Z",
  "draft":false,
  "id":1605328319,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5LDPnA",
  "number":846,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-02T14:55:04Z"
  },
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "state":"closed",
  "state_reason":null,
  "title":"fix: update licence url for tests",
  "updated_at":"2023-03-02T14:55:05Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @lgray as a contributor for code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot5/pull/830#issuecomment-1450970600)\n\n[skip ci]",
  "closed_at":"2023-03-01T22:49:31Z",
  "comments":0,
  "created_at":"2023-03-01T22:49:00Z",
  "draft":false,
  "id":1605790782,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5LE0Rb",
  "number":847,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-01T22:49:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add lgray as a contributor for code",
  "updated_at":"2023-03-01T22:49:32Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @ioanaif as a contributor for code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot5/pull/841#issuecomment-1450972289)\n\n[skip ci]",
  "closed_at":"2023-03-01T22:51:19Z",
  "comments":0,
  "created_at":"2023-03-01T22:50:58Z",
  "draft":false,
  "id":1605792578,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5LE0pu",
  "number":848,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-01T22:51:19Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add ioanaif as a contributor for code",
  "updated_at":"2023-03-01T22:51:20Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Dear uproot,\r\n\r\nOne of my ROOT file contains `vector<vector<int>>` variables which are all produced in a very similar way.\r\nOut of 4, 2 of them are extremely slow to read with uproot (5.0.3) : \r\n```python\r\nt= uproot.open(\"test.root:gnnjet\")\r\n>>> timeit.timeit(\"a=t.arrays(['nnTruthReco' ])\", globals=globals(), number=1)\r\n6.898946930188686\r\n>>> timeit.timeit(\"a=t.arrays(['nnRecoReco' ])\", globals=globals(), number=1)\r\n0.03529514092952013\r\n>>> timeit.timeit(\"a=t.arrays(['nnRecoTruth' ])\", globals=globals(), number=1)\r\n5.870497648837045\r\n>>> timeit.timeit(\"a=t.arrays(['nnTruthTruth' ])\", globals=globals(), number=1)\r\n0.06448978511616588\r\n```\r\n\r\nAs you can see there seem to be a factor>100 in the read time for 2 of the variables. \r\nThis is a  test file with 1000 entries which can be found at : \r\nhttps://cernbox.cern.ch/s/rvpBMjWLCZfhatG\r\n\r\nMy complete files have ~100k entries  so they are practically unreadable...\r\nIs there something wrong with these files ?  Or is the slowness coming from uproot ?\r\n\r\nThanks for any hint !\r\n\r\nP-A\r\n\r\n```python\r\n# python version is 3.10.9\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'5.0.3'\r\n```\r\n",
  "closed_at":"2023-03-05T07:29:23Z",
  "comments":6,
  "created_at":"2023-03-02T15:44:17Z",
  "id":1607043323,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5fyYj7",
  "number":850,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Very slow reading of some (not all) vector<vector<int>> variables",
  "updated_at":"2023-03-05T07:29:23Z",
  "user":"MDQ6VXNlcjEyMTk4Njg="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"@mpad saw factors of 200\u00d7 speed difference between some TBranches in #850. It turned out to depend on whether any `std::vectors` are empty or not: TBranches that contain any empty `std::vectors` in all entries were slow, those that didn't were fast.\r\n\r\nIt's because the AwkwardForth code-generation step has to consider, at the end of each entry, whether the code that it has is complete or if it's not complete because it's never observed a given list as occupied (and therefore could learn how many bytes its headers are, if there had been any objects in it). The logic, as-written, would mark an entry incomplete (`var_set = True`) if any list anywhere within it is empty. However, data like this:\r\n\r\n```\r\n[[[]], [[]], [[]], [[]], [[]], [[123]]]\r\n```\r\n\r\nor this:\r\n\r\n```\r\n[[[123]], [[]], [[]], [[]], [[]], [[]]]\r\n```\r\n\r\nshould not be considered incomplete because the walk over data reached all parts of the corresponding type tree _at some point_. Rather than asking if there is _any_ empty list, the appropriate question would be whether they're _all_ empty lists.\r\n\r\nBut there's more than that: this determination must be made separately at each level of depth (and in multiple branches, possibly at the same level, if there are any C++ classes/Awkward record arrays). It needs to reach all points of the _type_ tree for any part of the _value_ tree. To extend the current logic, `var_set` would have to become a container with as many items as list nodes in the type tree, the walk over values would start pessimistic and would toggle to a good value if any list value for a given list type node has non-zero length, and the final check would require all items to be good values.\r\n\r\nPerhaps it could have been solved by using `ForthGenerator.get_keys(1)` to identify the current list node in the type tree during the walk over values, though I would first need to know the full set of keys to expect. (It could be done by examining the Form, generated before reading any data using `AsObjects.awkward_form(TBranch.file)`.)\r\n\r\nHowever, I think there's an easier way, implemented in this PR. The same walk that generates AwkwardForth code generates an Awkward Form. If we don't get to a particular node to discover whether how many bytes its header has (to put the right `skip` command into the AwkwardForth code), then we also don't get to make the corresponding Form node. They're built in tandem. (Right?) So instead of checking `var_set`, I added a check to the current state of the Form (which is made of Python dicts and lists), to see if there are any missing nodes. Missing nodes are all labeled by a string, `\"NULL\"`. (Right?) If the Form does not contain any `\"NULL\"`, then the Forth code must be complete, too, and we can consider the code-generation phase to be complete.\r\n\r\nAlso, this means we can remove the `var_set` entirely, which didn't have a good name, anyway. (It wasn't clear what it was supposed to mean by just looking at it.)\r\n\r\n@aryan26roy, could you check my assumptions? This PR fixes #850 and all of the tests still run. I also verified that one of the tests would have `var_set is False` after the first entry, so the new logic is being exercised to some degree. But if you can see any mistaken assumptions that wouldn't hold in general, I should fix them.",
  "closed_at":"2023-03-05T07:29:22Z",
  "comments":0,
  "created_at":"2023-03-02T20:11:05Z",
  "draft":false,
  "id":1607421248,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5LKVpW",
  "number":851,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-05T07:29:22Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: replaced incorrect AwkwardForth `var_set` logic with a check to see if the Form is complete",
  "updated_at":"2023-03-05T07:29:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-03-27T19:18:04Z",
  "comments":2,
  "created_at":"2023-03-07T11:05:56Z",
  "draft":false,
  "id":1613215258,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5LdozV",
  "number":852,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-27T19:18:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: strided interpretation for data with extra offsets",
  "updated_at":"2023-03-27T19:18:06Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Based on an interaction I just had with a user: the [getting started: writing objects to a file](https://uproot.readthedocs.io/en/latest/basic.html#writing-objects-to-a-file) page does include various examples of how to write histograms to a root file. It even includes weighted ones with `hist`, but only shows how to handle a histogram that is being filled on the spot. It is not obvious from there how a user would take an existing piece of histogram information (in the form of counts per bin + uncertainty) and turn that into a TH1 in a file.\r\n\r\nThe [`hist` documentation quick start]( https://hist.readthedocs.io/en/latest/user-guide/quickstart.html#setting-the-contents) also does not cover that. The pattern\r\n```python\r\nhist[...] = np.stack([yields, stdev**2], axis=-1)\r\n```\r\n\r\nis mentioned in the [`boost-histogram` docs](https://boost-histogram.readthedocs.io/en/latest/user-guide/histogram.html#views) (via https://github.com/scikit-hep/boost-histogram/issues/421) but a user is unlikely to get this far I imagine.\r\n\r\nPerhaps it makes sense to mention this pattern somewhere more prominently. I am not sure where the best spot would be, but wanted to raise this to see if others had ideas (and if you agree that this would be useful to do).\r\n",
  "closed_at":null,
  "comments":3,
  "created_at":"2023-03-09T10:05:48Z",
  "id":1616857164,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5gX0hM",
  "number":853,
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "state":"open",
  "state_reason":null,
  "title":"Writing a histogram with sumw2 information to a file in the getting started page",
  "updated_at":"2023-03-09T15:47:56Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Carrying on discussion here: https://matrix.to/#/!WwIPZwMtNCFuzXqrmf:gitter.im/$XZVjnFnoDutfxGE9GcfzDUToky0yiQGCKVvYvKy9s90?via=gitter.im&via=matrix.org\r\n\r\nIt'd be really interesting to get `uproot` running in `jupyter-lite` under the pyodide kernel, so far I have the following recipe:\r\n\r\n```py\r\nimport piplite\r\n\r\n%pip install -q numpy pandas pyodide-http\r\n\r\nimport pyodide_http\r\n\r\npyodide_http.patch_all()\r\nfrom http import client\r\nclass HTTPSConnection: pass\r\nclient.HTTPSConnection = HTTPSConnection\r\n\r\n%pip install --no-deps uproot\r\n```\r\n\r\nHowever, one cannot get around that `HTTPSource` relies on ThreadPools, leading to this exception:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[3], line 1\r\n----> 1 f=uproot.open(\"https://github.com/scikit-hep/scikit-hep-testdata/raw/main/src/skhep_testdata/data/uproot-hepdata-example.root\")\r\n\r\nFile /lib/python3.10/site-packages/uproot/reading.py:141, in open(path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\r\n    132 if not uproot._util.isstr(file_path) and not (\r\n    133     hasattr(file_path, \"read\") and hasattr(file_path, \"seek\")\r\n    134 ):\r\n    135     raise ValueError(\r\n    136         \"'path' must be a string, pathlib.Path, an object with 'read' and \"\r\n    137         \"'seek' methods, or a length-1 dict of {file_path: object_path}, \"\r\n    138         f\"not {path!r}\"\r\n    139     )\r\n--> 141 file = ReadOnlyFile(\r\n    142     file_path,\r\n    143     object_cache=object_cache,\r\n    144     array_cache=array_cache,\r\n    145     custom_classes=custom_classes,\r\n    146     decompression_executor=decompression_executor,\r\n    147     interpretation_executor=interpretation_executor,\r\n    148     **options,  # NOTE: a comma after **options breaks Python 2\r\n    149 )\r\n    151 if object_path is None:\r\n    152     return file.root_directory\r\n\r\nFile /lib/python3.10/site-packages/uproot/reading.py:581, in ReadOnlyFile.__init__(self, file_path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\r\n    576 self.hook_before_create_source()\r\n    578 Source, file_path = uproot._util.file_path_to_source_class(\r\n    579     file_path, self._options\r\n    580 )\r\n--> 581 self._source = Source(\r\n    582     file_path, **self._options  # NOTE: a comma after **options breaks Python 2\r\n    583 )\r\n    585 self.hook_before_get_chunks()\r\n    587 if self._options[\"begin_chunk_size\"] < _file_header_fields_big.size:\r\n\r\nFile /lib/python3.10/site-packages/uproot/source/http.py:564, in HTTPSource.__init__(self, file_path, **options)\r\n    562 self._fallback_options = options.copy()\r\n    563 self._fallback_options[\"num_workers\"] = self._num_fallback_workers\r\n--> 564 self._open()\r\n\r\nFile /lib/python3.10/site-packages/uproot/source/http.py:567, in HTTPSource._open(self)\r\n    566 def _open(self):\r\n--> 567     self._executor = uproot.source.futures.ResourceThreadPoolExecutor(\r\n    568         [HTTPResource(self._file_path, self._timeout)]\r\n    569     )\r\n\r\nFile /lib/python3.10/site-packages/uproot/source/futures.py:351, in ResourceThreadPoolExecutor.__init__(self, resources)\r\n    349     self._workers.append(ResourceWorker(self._work_queue, resource))\r\n    350 for worker in self._workers:\r\n--> 351     worker.start()\r\n\r\nFile /lib/python3.10/threading.py:928, in Thread.start(self)\r\n    926     _limbo[self] = self\r\n    927 try:\r\n--> 928     _start_new_thread(self._bootstrap, ())\r\n    929 except Exception:\r\n    930     with _active_limbo_lock:\r\n\r\nRuntimeError: can't start new thread\r\n```\r\n\r\nIt seems like the next step would be to create a pyodide-friendly `HTTPSource` that uses either `pyodide-http` or `js.fetch` interfaces.\r\n\r\ncc @chrisburr @agoose77\r\n",
  "closed_at":"2023-04-03T07:29:32Z",
  "comments":9,
  "created_at":"2023-03-10T17:17:36Z",
  "id":1619327280,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5ghPkw",
  "number":854,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Pyodide support for running in jupyter-lite browser-based notebooks",
  "updated_at":"2023-07-27T13:08:32Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"cc @nikoladze and @lgray\r\n\r\nTwo of the problematic TBranches in #798 have been fixed by a previous PR, probably this one: #851.\r\n\r\nThe other two had the same error: the first few `std::vectors` in the first entry were empty and the last few were not empty. Stepping through the empty ones created the ListOffsetArray Form node and Forth code multiple times, nesting it each time. (This is entirely in the code-generation stage, and the \"input names, output names, variable names, and user-defined words must all be unique\" error message was just the first symptom.)\r\n\r\nThe non-empty `std::vectors` already knew that they're not supposed to keep creating ListOffsetArray nodes. Despite over 3 hours of @nikoladze and me looking at this code, we didn't understand the code path of how the working part is supposed to work (in order to apply the same principle to the non-working part, the case of empty `std::vectors`). This code will _have to_ at some point be refactored into something maintainable; that will probably need to be another project in itself. (It would be an easier project than the first time around, because there is a working example to examine, but the whole concept of constantly changing `forth_obj.awkward_model` variables will have to be reconsidered. This code is functional, in that it works, but it is not functional enough, in the sense of avoiding mutable state.)\r\n\r\nNevertheless, we did manage to patch the current implementation. We couldn't simply skip the Form-and-Forth generation when `length == 0` because that broke existing tests (in multiple different ways; we didn't try to understand why and how). But if we're at a point in the codebase where we're considering a node key (`f\"node{key}\"`) that is equal to the current `forth_obj.awkward_model[\"name\"]`, nesting a new node like that within `forth_obj.awkward_model[\"content\"]` and adding to the Form would definitely be wrong, so the patch in this PR avoids doing that for all list types. (We only observed the issue in `AsVector`, but if it applies to `std::vector`, it ought to apply to `std::set`, `std::map`, etc.)\r\n\r\nThe patch does seem to break an abstraction layer: code outside of `ForthGenerator` is checking the value of `ForthGenerator.awkward_model` and making decisions based on it; it looks like the current code tries to hide this in `ForthGenerator.add_node`. But extending `add_node` and `should_add_form` to check a new `node_key` would require passing that in as an argument, and I didn't want to change all calls to these two functions, since they handle more cases than list-building and this is only an issue for list-building. (Only lists can be empty.)\r\n\r\nI think this is an improvement, but we'll definitely need an AwkwardForth-revamping project at some point in the future.",
  "closed_at":"2023-03-11T12:28:25Z",
  "comments":0,
  "created_at":"2023-03-10T20:17:35Z",
  "draft":false,
  "id":1619565885,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5LzE8J",
  "number":855,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-11T12:28:25Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: AwkwardForth was nesting the same ListOffsetArray (same node name) multiple times",
  "updated_at":"2023-03-11T12:28:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [pypa/gh-action-pypi-publish](https://github.com/pypa/gh-action-pypi-publish) from 1.6.4 to 1.7.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/pypa/gh-action-pypi-publish/releases\">pypa/gh-action-pypi-publish's releases</a>.</em></p>\n<blockquote>\n<h2>v1.7.1</h2>\n<h2>Regression?</h2>\n<p>There was a small setback with v1.7.0 \u2014 the snake_case fallbacks didn't work because the check for the kebab-case env vars with default values set was always truthy. This bugfix release promptly fixes that.</p>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.7.0...v1.7.1\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.7.0...v1.7.1</a></p>\n<h2>v1.7.0</h2>\n<h2>What should I care about?</h2>\n<p>TL;DR The action input names have been converted to use kebab-case and marked deprecated. But the old names still work.</p>\n<p>This is made to align the public API with the de-facto conventions in the ecosystem. We've used snake_case names, which the maintainer considers a historical mistake. New kebab-case inputs will make the end-users' workflows look more consistent and and visually distinguishable from other identifiers one may encounter in YAML.</p>\n<p>There is no timeline for removing the old names, but it will happen in v3 or later versions of the action. <em>If the maintainer doesn't forget to do this, that is.</em></p>\n<p>The patch is here: <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/125\">pypa/gh-action-pypi-publish#125</a>.</p>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.6.5...v1.7.0\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.6.5...v1.7.0</a></p>\n<h2>v1.6.5</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Added an explicit warning when the password passed into the action is empty \u2014 thanks <a href=\"https://github.com/colindean\"><code>@\u200bcolindean</code></a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/colindean\"><code>@\u200bcolindean</code></a> made their first contribution in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/122\">pypa/gh-action-pypi-publish#122</a></li>\n</ul>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.6.4...v1.6.5\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.6.4...v1.6.5</a></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/22b4d1f12511f2696162c08546dafbaa903448a2\"><code>22b4d1f</code></a> \ud83d\udc1b Make kebab options fall back for snake_case</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/7104b6e981edc64651f8fc454998a67914173bdb\"><code>7104b6e</code></a> Merge branch 'maintenance/kebab-case-inputs' into unstable/v1</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/f131721e84cbe1025d966f15e3a9c65be89d9396\"><code>f131721</code></a> \ud83c\udfa8 Convert action inputs to use kebab-case</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/32b5e9370928566935e085a9154f23bd662ef41c\"><code>32b5e93</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/122\">#122</a> from colindean/empty-token</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/efcb9babc8c755fa5eaf7e581749c61995268ba7\"><code>efcb9ba</code></a> \ud83c\udfa8 Warn about empty password/token action input</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/d2ce3ec872b8e7e5190b4b094dd0b7a447c0b266\"><code>d2ce3ec</code></a> \u21ea Bump isort to v5.12.0</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/0eaf3a11fdca4a989056b00eda351a21c1b0426e\"><code>0eaf3a1</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/121\">#121</a> from pypa/dependabot/pip/requirements/cryptography-39...</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/6a2da9bc3b4a932e2daeabc60613200d38c902f9\"><code>6a2da9b</code></a> Bump cryptography from 38.0.4 to 39.0.1 in /requirements</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/7eb3b701d11256e583f5b49899c5e7203deab573\"><code>7eb3b70</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/119\">#119</a> from pypa/pre-commit-ci-update-config</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/91e612128c6eb36bfa9bd31ddcae03eceb63616c\"><code>91e6121</code></a> Revert WPS flake8 hook version to 4.0.1</li>\n<li>Additional commits viewable in <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.6.4...v1.7.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pypa/gh-action-pypi-publish&package-manager=github_actions&previous-version=1.6.4&new-version=1.7.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2023-03-16T14:52:10Z",
  "comments":0,
  "created_at":"2023-03-13T10:02:04Z",
  "draft":false,
  "id":1621147052,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5L4EWo",
  "number":856,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-16T14:52:10Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump pypa/gh-action-pypi-publish from 1.6.4 to 1.7.1",
  "updated_at":"2023-03-16T14:52:11Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"\r\nI am trying to read a file.root using **uproot** but it's throwing back this error :-\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/siddharthkeshar/opt/anaconda3/lib/python3.9/site-packages/uproot/reading.py\", line 152, in open\r\n    return file.root_directory\r\n  File \"/Users/siddharthkeshar/opt/anaconda3/lib/python3.9/site-packages/uproot/reading.py\", line 785, in root_directory\r\n    return ReadOnlyDirectory(\r\n  File \"/Users/siddharthkeshar/opt/anaconda3/lib/python3.9/site-packages/uproot/reading.py\", line 1426, in __init__\r\n    ReadOnlyKey(keys_chunk, keys_cursor, {}, file, self, read_strings=True)\r\n  File \"/Users/siddharthkeshar/opt/anaconda3/lib/python3.9/site-packages/uproot/reading.py\", line 2266, in __init__\r\n    ) = cursor.fields(chunk, _key_format_small, context, move=False)\r\n  File \"/Users/siddharthkeshar/opt/anaconda3/lib/python3.9/site-packages/uproot/source/cursor.py\", line 194, in fields\r\n    return format.unpack(chunk.get(start, stop, self, context))\r\n  File \"/Users/siddharthkeshar/opt/anaconda3/lib/python3.9/site-packages/uproot/source/chunk.py\", line 417, in get\r\n    self.wait(insist=stop)\r\n  File \"/Users/siddharthkeshar/opt/anaconda3/lib/python3.9/site-packages/uproot/source/chunk.py\", line 370, in wait\r\n    raise OSError(\r\n[file.txt](https://github.com/scikit-hep/uproot5/files/10974931/file.txt)\r\n\r\nOSError: expected Chunk of length 395,\r\nreceived 0 bytes from MemmapSource\r\nfor file path /Users/siddharthkeshar/Desktop/file.root\r\n```\r\n\r\nHow can i resolve this error ?\r\n\r\nI am using uproot version : 5.0.4\r\n\r\n\r\n```python\r\n>>> import uproot\r\n>>> file = uproot.open(\"/Users/siddharthkeshar/Desktop/file.root\")\r\n```\r\n",
  "closed_at":"2023-03-15T16:22:16Z",
  "comments":1,
  "created_at":"2023-03-15T00:36:23Z",
  "id":1624513092,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5g1BpE",
  "number":858,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Not reading / opening of root file from uproot",
  "updated_at":"2023-03-15T16:22:17Z",
  "user":"U_kgDOBn1Lhw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Sometimes, ROOT produces TBranches with types like\r\n\r\n```\r\nleaf1[counter]/F:leaf2[counter]/I:leaf3[counter]/F\r\n```\r\n\r\nThat is, there are multiple TLeaves, the data are interleaved, but the number of items per entry is variable and determined by some other TBranch/TLeaf named `counter`. This is _only conceivably possible_ if all of the TLeaves in the leaf-list have the same counter, but presumably ROOT ensures that this is the case.\r\n\r\nWe just need an example file...",
  "closed_at":null,
  "comments":1,
  "created_at":"2023-03-15T14:54:21Z",
  "id":1625685615,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5g5f5v",
  "number":860,
  "performed_via_github_app":null,
  "reactions":{
   "eyes":1,
   "laugh":1,
   "total_count":2
  },
  "state":"open",
  "state_reason":null,
  "title":"Handle jagged leaf-list",
  "updated_at":"2024-01-30T16:13:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"As discussed in https://root-forum.cern.ch/t/ttime-saved-to-a-ttree-in-root5-and-root6/54031 and #859, the `TTime` class breaks pattern and includes headers because\r\n\r\n> the dictionary generated is requesting the \u2018very\u2019 old I/O for the class TTime.\r\n\r\nIn my reading of this, the only way to get it right is to add a custom/manual Model class in the models directory and have it read headers unconditionally (like some other old classes, `TList`, etc.).\r\n\r\nBefore I forget where I put the file, here is the baseline Model that needs to be copied and modified (to include `@num_bytes` and `@instance_version` regardless of the value of `header` in `strided_interpretation`). Also, the AwkwardForth code should start with a `6 stream skip`.\r\n\r\n```python\r\nclass Model_TTime_v2(uproot.model.VersionedModel):\r\n    def read_members(self, chunk, cursor, context, file):\r\n        import uproot._awkward_forth\r\n        if self.is_memberwise:\r\n            raise NotImplementedError(\r\n                f\"memberwise serialization of {type(self).__name__}\\nin file {self.file.file_path}\"\r\n            )\r\n\r\n        forth_stash = uproot._awkward_forth.forth_stash(context)\r\n        if forth_stash is not None:\r\n            forth_obj = forth_stash.get_gen_obj()\r\n            content = {}\r\n        if forth_stash is not None:\r\n            key = forth_obj.get_keys(1)\r\n            form_key = f\"node{key}-data\"\r\n            forth_stash.add_to_header(f\"output node{key}-data int64\\n\")\r\n            content['fMilliSec'] = { \"class\": \"NumpyArray\", \"primitive\": \"int64\", \"inner_shape\": [], \"parameters\": {}, \"form_key\": f\"node{key}\"}\r\n            forth_stash.add_to_pre(f\"stream !q-> node{key}-data\\n\")\r\n            if forth_obj.should_add_form():\r\n                forth_obj.add_form_key(form_key)\r\n        self._members['fMilliSec'] = cursor.field(chunk, self._format0, context)\r\n        if forth_stash is not None:\r\n            if forth_obj.should_add_form():\r\n                forth_obj.add_form({'class': 'RecordArray', 'contents': content, 'parameters': {'__record__': 'TTime'}}, len(content))\r\n            temp = forth_obj.add_node('dynamic', forth_stash.get_attrs(), \"i64\", 0, None)\r\n\r\n    def read_member_n(self, chunk, cursor, context, file, member_index):\r\n        if member_index == 0:\r\n            self._members['fMilliSec'] = cursor.field(chunk, self._format_memberwise0, context)\r\n\r\n    @classmethod\r\n    def strided_interpretation(cls, file, header=False, tobject_header=True, breadcrumbs=(), original=None):\r\n        if cls in breadcrumbs:\r\n            raise uproot.interpretation.objects.CannotBeStrided('classes that can contain members of the same type cannot be strided because the depth of instances is unbounded')\r\n        breadcrumbs = breadcrumbs + (cls,)\r\n        members = []\r\n        if header:\r\n            members.append(('@num_bytes', numpy.dtype('>u4')))\r\n            members.append(('@instance_version', numpy.dtype('>u2')))\r\n        members.append(('fMilliSec', numpy.dtype('>i8')))\r\n        return uproot.interpretation.objects.AsStridedObjects(cls, members, original=original)\r\n\r\n    @classmethod\r\n    def awkward_form(cls, file, context):\r\n        from awkward.forms import NumpyForm, ListOffsetForm, RegularForm, RecordForm\r\n        if cls in context['breadcrumbs']:\r\n            raise uproot.interpretation.objects.CannotBeAwkward('classes that can contain members of the same type cannot be Awkward Arrays because the depth of instances is unbounded')\r\n        context['breadcrumbs'] = context['breadcrumbs'] + (cls,)\r\n        contents = {}\r\n        if context['header']:\r\n            contents['@num_bytes'] = uproot._util.awkward_form(numpy.dtype('u4'), file, context)\r\n            contents['@instance_version'] = uproot._util.awkward_form(numpy.dtype('u2'), file, context)\r\n        contents['fMilliSec'] = uproot._util.awkward_form(numpy.dtype('>i8'), file, context)\r\n        return RecordForm(list(contents.values()), list(contents.keys()), parameters={'__record__': 'TTime' })\r\n\r\n    _format0 = struct.Struct('>q')\r\n    _format_memberwise0 = struct.Struct('>q')\r\n    base_names_versions=[]\r\n    member_names = ['fMilliSec']\r\n    class_flags = {}\r\n```\r\n\r\nAnd then these files, [root5_6_examples.zip](https://github.com/scikit-hep/uproot5/files/10978941/root5_6_examples.zip), should be used to make a test.",
  "closed_at":null,
  "comments":3,
  "created_at":"2023-03-15T18:10:13Z",
  "id":1626021550,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5g6x6u",
  "number":861,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"TTime needs a custom/manual Model class",
  "updated_at":"2023-11-27T17:29:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This would be a major project, but it's frequently asked for. Users want to be able to do this:\r\n\r\n```python\r\nwith uproot.update(\"existing_file.root\") as file:\r\n    assert file[\"existing_tree\"].num_entries == len(array)\r\n    file[\"existing_tree\"].new_branch(array)\r\n```\r\n\r\nsuch that the `\"existing_tree\"` has the same number of entries but a new TBranch.\r\n\r\nThis will involve overwriting the entire TTree metadata object (or creating one with a new cycle number), but it can reuse all of the TBasket data from the previous TTree, only adding new ones.",
  "closed_at":null,
  "comments":0,
  "created_at":"2023-03-16T13:27:31Z",
  "id":1627476442,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5hAVHa",
  "number":862,
  "performed_via_github_app":null,
  "reactions":{
   "+1":3,
   "total_count":3
  },
  "state":"open",
  "state_reason":null,
  "title":"Take an existing TTree and add a new TBranch",
  "updated_at":"2023-03-16T13:27:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [pypa/gh-action-pypi-publish](https://github.com/pypa/gh-action-pypi-publish) from 1.7.1 to 1.8.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/pypa/gh-action-pypi-publish/releases\">pypa/gh-action-pypi-publish's releases</a>.</em></p>\n<blockquote>\n<h2>v1.8.1</h2>\n<h2>\ud83d\udc1b What's Fixed</h2>\n<p>\ud83d\udc94 Unfortunately, a tiny mistake in v1.8.0 caused a far-reaching regression for the most used code path.\n\u2757 But don't worry, it's fixed now thanks to <a href=\"https://github.com/njzjz\"><code>@\u200bnjzjz</code></a> who promptly spotted it and <a href=\"https://github.com/zhongjiajie\"><code>@\u200bzhongjiajie</code></a> who sent a bugfix.</p>\n<h2>\ud83d\ude4c New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/zhongjiajie\"><code>@\u200bzhongjiajie</code></a> made their first contribution in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/131\">pypa/gh-action-pypi-publish#131</a></li>\n</ul>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.0...v1.8.1\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.0...v1.8.1</a></p>\n<h2>v1.8.0</h2>\n<h2>The Coolest Release Ever!</h2>\n<p>In this release, <a href=\"https://github.com/woodruffw\"><code>@\u200bwoodruffw</code></a> implemented support for secretless OIDC-based publishing to PyPI-like package indexes. The OIDC flow is activated when neither username nor password action inputs are set.</p>\n<p>The OIDC \u201ctoken exchange\u201d, is an authentication technique that PyPI (and TestPyPI, and hopefully some future others) supports as an alternative to long-lived username/password combinations or long-lived API tokens.</p>\n<blockquote>\n<p><strong>IMPORTANT:</strong> The PyPI-side configuration is only available to participants of the private beta test. Please, only try out the zero-config mode if you are a beta test participant having followed the PyPI configuration instructions.</p>\n</blockquote>\n<p>Setup prerequisites: <a href=\"https://github.com/marketplace/actions/pypi-publish#publishing-with-openid-connect\">https://github.com/marketplace/actions/pypi-publish#publishing-with-openid-connect</a>\nPyPI's documentation: <a href=\"https://pypi.org/help/#openid-connect\">https://pypi.org/help/#openid-connect</a>\nBeta test enrollment: <a href=\"https://redirect.github.com/pypi/warehouse/issues/12965\">pypi/warehouse#12965</a></p>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/woodruffw\"><code>@\u200bwoodruffw</code></a> made their first contribution in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/123\">pypa/gh-action-pypi-publish#123</a></li>\n</ul>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.7.1...v1.8.0\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.7.1...v1.8.0</a></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/a3a3bafbb3e5a75a854ae1bc53ae128cf22c4af4\"><code>a3a3baf</code></a> \ud83d\udc1b Merge PR <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/131\">#131</a> from into unstable/v1</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/d5417dc8be8a412b7bf487b09ad3997ad92f859b\"><code>d5417dc</code></a> \ud83d\udc1bCorrect default upload URL</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/8ef2b3d46c9ecba901fb2ae21d98e322c4089c4e\"><code>8ef2b3d</code></a> Merge PR <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/123\">#123</a> into unstable/v1</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/2b46bad8cbe9efe518cd7dd98a1610b22721a469\"><code>2b46bad</code></a> OIDC beta support</li>\n<li>See full diff in <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.7.1...v1.8.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pypa/gh-action-pypi-publish&package-manager=github_actions&previous-version=1.7.1&new-version=1.8.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2023-03-23T16:44:22Z",
  "comments":0,
  "created_at":"2023-03-20T10:00:21Z",
  "draft":false,
  "id":1631764800,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5MbsYn",
  "number":864,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-23T16:44:22Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump pypa/gh-action-pypi-publish from 1.7.1 to 1.8.1",
  "updated_at":"2023-03-23T16:44:23Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/charliermarsh/ruff-pre-commit: v0.0.255 \u2192 v0.0.257](https://github.com/charliermarsh/ruff-pre-commit/compare/v0.0.255...v0.0.257)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-03-23T16:55:04Z",
  "comments":0,
  "created_at":"2023-03-21T00:01:42Z",
  "draft":false,
  "id":1633035736,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Mf-1j",
  "number":865,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-03-23T16:55:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"ci: [pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2023-03-23T16:55:06Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [pypa/gh-action-pypi-publish](https://github.com/pypa/gh-action-pypi-publish) from 1.8.1 to 1.8.3.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/pypa/gh-action-pypi-publish/releases\">pypa/gh-action-pypi-publish's releases</a>.</em></p>\n<blockquote>\n<h2>v1.8.3</h2>\n<h2>What's New</h2>\n<p>This release improves the logging detalization of which authentication mode is selected when the action runs. It surfaces this detail to the workflow run summary page as annotations. The change was contributed by <a href=\"https://github.com/woodruffw\"><code>@\u200bwoodruffw</code></a> in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/136\">pypa/gh-action-pypi-publish#136</a>.</p>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.2...v1.8.3\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.2...v1.8.3</a></p>\n<h2>v1.8.2</h2>\n<h2>What's Changed</h2>\n<p>This release started printing out full OIDC error messages to console, instead of just one line -- by <a href=\"https://github.com/woodruffw\"><code>@\u200bwoodruffw</code></a> in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/134\">pypa/gh-action-pypi-publish#134</a>.</p>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.1...v1.8.2\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.1...v1.8.2</a></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/48b317d84d5f59668bb13be49d1697e36b3ad009\"><code>48b317d</code></a> Merge PR <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/136\">#136</a> into unstable/v1</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/ae295504b3468d3952767fa0d25a4a2996a5578f\"><code>ae29550</code></a> twine-upload: increase detail on console notices</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/f3ce18f69986f83a044b6e618425238f6c1bb8ef\"><code>f3ce18f</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/134\">#134</a> from trail-of-forks/tob-better-errors</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/ea29ccc08c5128e49d7b7744d6fe4fcac05e93de\"><code>ea29ccc</code></a> oidc-exchange: avoid splitting the error message</li>\n<li>See full diff in <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.1...v1.8.3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pypa/gh-action-pypi-publish&package-manager=github_actions&previous-version=1.8.1&new-version=1.8.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2023-04-03T10:00:27Z",
  "comments":1,
  "created_at":"2023-03-27T09:59:52Z",
  "draft":false,
  "id":1641792403,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5M9K3V",
  "number":866,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump pypa/gh-action-pypi-publish from 1.8.1 to 1.8.3",
  "updated_at":"2023-04-03T10:00:30Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/psf/black: 23.1.0 \u2192 23.3.0](https://github.com/psf/black/compare/23.1.0...23.3.0)\n- [github.com/charliermarsh/ruff-pre-commit: v0.0.257 \u2192 v0.0.260](https://github.com/charliermarsh/ruff-pre-commit/compare/v0.0.257...v0.0.260)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-04-06T16:55:36Z",
  "comments":0,
  "created_at":"2023-03-28T00:23:11Z",
  "draft":false,
  "id":1643013492,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5NBRQh",
  "number":867,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-04-06T16:55:36Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"ci: [pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2023-04-06T16:55:38Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-04-03T07:29:31Z",
  "comments":0,
  "created_at":"2023-03-31T08:30:54Z",
  "draft":false,
  "id":1648934845,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5NVIFs",
  "number":868,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-04-03T07:29:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add pyodide support for jupyter-lite for files opened via HTTP ",
  "updated_at":"2023-04-03T07:29:32Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [pypa/gh-action-pypi-publish](https://github.com/pypa/gh-action-pypi-publish) from 1.8.1 to 1.8.4.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/pypa/gh-action-pypi-publish/releases\">pypa/gh-action-pypi-publish's releases</a>.</em></p>\n<blockquote>\n<h2>v1.8.4</h2>\n<h2>What's Improved</h2>\n<ul>\n<li><a href=\"https://github.com/hugovk\"><code>@\u200bhugovk</code></a> cleaned up the double whitespaces in the OIDC flow logging in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/140\">pypa/gh-action-pypi-publish#140</a></li>\n<li><a href=\"https://github.com/woodruffw\"><code>@\u200bwoodruffw</code></a> added a title and a docs link to the OIDC error output in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/139\">pypa/gh-action-pypi-publish#139</a></li>\n</ul>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.3...v1.8.4\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.3...v1.8.4</a></p>\n<h2>v1.8.3</h2>\n<h2>What's New</h2>\n<p>This release improves the logging detalization of which authentication mode is selected when the action runs. It surfaces this detail to the workflow run summary page as annotations. The change was contributed by <a href=\"https://github.com/woodruffw\"><code>@\u200bwoodruffw</code></a> in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/136\">pypa/gh-action-pypi-publish#136</a>.</p>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.2...v1.8.3\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.2...v1.8.3</a></p>\n<h2>v1.8.2</h2>\n<h2>What's Changed</h2>\n<p>This release started printing out full OIDC error messages to console, instead of just one line -- by <a href=\"https://github.com/woodruffw\"><code>@\u200bwoodruffw</code></a> in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/134\">pypa/gh-action-pypi-publish#134</a>.</p>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.1...v1.8.2\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.1...v1.8.2</a></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/29930c9cf57955dc1b98162d0d8bc3ec80d9e75c\"><code>29930c9</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/139\">#139</a> from trail-of-forks/tob-improve-errors</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/9c859e9a775e2122ff3da8f3f5bdfc6ca175f996\"><code>9c859e9</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/140\">#140</a> from hugovk/oidc-whitespace</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/65bf8a81decf5c0966c65c2a2e4400e507008232\"><code>65bf8a8</code></a> Remove double spaces</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/486ec8dd234caaf3824a0662ee75bc66a778cab7\"><code>486ec8d</code></a> oidc-exchange: improve errors</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/48b317d84d5f59668bb13be49d1697e36b3ad009\"><code>48b317d</code></a> Merge PR <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/136\">#136</a> into unstable/v1</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/ae295504b3468d3952767fa0d25a4a2996a5578f\"><code>ae29550</code></a> twine-upload: increase detail on console notices</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/f3ce18f69986f83a044b6e618425238f6c1bb8ef\"><code>f3ce18f</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/134\">#134</a> from trail-of-forks/tob-better-errors</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/ea29ccc08c5128e49d7b7744d6fe4fcac05e93de\"><code>ea29ccc</code></a> oidc-exchange: avoid splitting the error message</li>\n<li>See full diff in <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.1...v1.8.4\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pypa/gh-action-pypi-publish&package-manager=github_actions&previous-version=1.8.1&new-version=1.8.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2023-04-06T16:42:05Z",
  "comments":0,
  "created_at":"2023-04-03T10:00:24Z",
  "draft":false,
  "id":1651740262,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5NeQFJ",
  "number":869,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-04-06T16:42:05Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump pypa/gh-action-pypi-publish from 1.8.1 to 1.8.4",
  "updated_at":"2023-04-06T16:42:06Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-04-06T16:18:37Z",
  "comments":1,
  "created_at":"2023-04-05T16:09:54Z",
  "draft":false,
  "id":1655921012,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5NsT8q",
  "number":870,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-04-06T16:18:37Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: have unknown type become float64 when in array context",
  "updated_at":"2023-04-06T16:18:38Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-04-06T16:02:15Z",
  "comments":2,
  "created_at":"2023-04-06T15:05:46Z",
  "draft":false,
  "id":1657601714,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5NxzwR",
  "number":871,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-04-06T16:02:15Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add support for pandas is_numeric API change",
  "updated_at":"2023-04-06T16:02:16Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [pypa/gh-action-pypi-publish](https://github.com/pypa/gh-action-pypi-publish) from 1.8.4 to 1.8.5.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/pypa/gh-action-pypi-publish/releases\">pypa/gh-action-pypi-publish's releases</a>.</em></p>\n<blockquote>\n<h2>v1.8.5</h2>\n<h2>What's Improved</h2>\n<p><a href=\"https://github.com/woodruffw\"><code>@\u200bwoodruffw</code></a> improved the user-facing documentation and logging to make use of the Trusted Publishing flow terminology cohesive with PyPI in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/143\">pypa/gh-action-pypi-publish#143</a>. Trusted Publishing used to be referred to as OpenID Connect (OIDC) \u2014 the underlying technology that is being used to make it work. He also made the action display the cause of the Trusted Publishing flow being selected by the action via <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/142\">pypa/gh-action-pypi-publish#142</a>.</p>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.4...v1.8.5\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.4...v1.8.5</a></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/0bf742be3ebe032c25dd15117957dc15d0cfc38d\"><code>0bf742b</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/143\">#143</a> from trail-of-forks/tob-rewrite-oidc-refs</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/30c382209e2edb18e26e0cd15ea4ddbb62e4d249\"><code>30c3822</code></a> oidc-exchange: another link</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/89ddbeae048cf21191cb26cde13abd949438b484\"><code>89ddbea</code></a> README: retitle, add note</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/a0f29a5690c3e59076bbc8d1349b7f872249ac83\"><code>a0f29a5</code></a> Apply suggestions from code review</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/0b567d5b015a5db43765f0ae2b3a215ea9d0150b\"><code>0b567d5</code></a> oidc-exchange, twine-upload: remove more OIDC refs</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/4372cb558524908cb34fcd57d7bc50d397daa875\"><code>4372cb5</code></a> README: replace OIDC with &quot;trusted publishing&quot;</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/69efb8cbfb5ff087b7145d695b90bba0c4a53029\"><code>69efb8c</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/142\">#142</a> from trail-of-forks/tob-indicate-oidc</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/dfde872acc38fce52eaf13748aaaa11845c89228\"><code>dfde872</code></a> Apply suggestions from code review</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/3d567f44ce3fb3bbcf9248d05726c6c9f811b67f\"><code>3d567f4</code></a> twine-upload: expound</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/67b747a9c83c5d2526d31259d1479f46210ffd0e\"><code>67b747a</code></a> oidc-exchange: more explanation</li>\n<li>See full diff in <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.4...v1.8.5\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pypa/gh-action-pypi-publish&package-manager=github_actions&previous-version=1.8.4&new-version=1.8.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2023-04-13T15:11:44Z",
  "comments":0,
  "created_at":"2023-04-10T09:59:01Z",
  "draft":false,
  "id":1660540145,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5N6_JP",
  "number":873,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-04-13T15:11:44Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump pypa/gh-action-pypi-publish from 1.8.4 to 1.8.5",
  "updated_at":"2023-04-13T15:11:46Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/charliermarsh/ruff-pre-commit: v0.0.260 \u2192 v0.0.261](https://github.com/charliermarsh/ruff-pre-commit/compare/v0.0.260...v0.0.261)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-04-13T15:12:40Z",
  "comments":0,
  "created_at":"2023-04-11T00:55:36Z",
  "draft":false,
  "id":1661607506,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5N-mnU",
  "number":874,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-04-13T15:12:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"ci: [pre-commit.ci] pre-commit autoupdate",
  "updated_at":"2023-04-13T15:12:41Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"To use EOS tokens, the token must be appended to the file name, so the URLs are typically of the style:\r\n\r\nroot://eosserver//eos/path/file.root?xrd.wantprot=unix&authz=zteos64:BASE64TOKEN\r\n\r\nCurrently EOS does not accept the \":\" to be escaped as %3A, and the semicolon is interpreted by uproot as the separator between the file name and the object name in the file.\r\n\r\nThis patch just ignores the semicolon if preceded by zteos64.\r\n\r\n\r\n\r\n",
  "closed_at":"2023-04-12T18:49:03Z",
  "comments":2,
  "created_at":"2023-04-12T13:37:39Z",
  "draft":false,
  "id":1664589360,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5OIrhg",
  "number":875,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: Ignore semicolon in EOS token when separating file name and object name",
  "updated_at":"2023-04-12T20:58:24Z",
  "user":"MDQ6VXNlcjcyMDgyODg="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Presently using the lazy file-opening mode of dask causes dask to map one file to one partition.\r\nFor the typical size of CMS NanoAOD files this is much too large and will exhaust memory on worker nodes.\r\nThis PR adds the possibility, in the case of lazy file opening, to blindly chunk the files and avoid this problem.\r\n\r\n@jpivarski following up from slack chat.",
  "closed_at":"2023-04-14T15:10:07Z",
  "comments":2,
  "created_at":"2023-04-12T20:33:33Z",
  "draft":false,
  "id":1665210064,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5OKxdR",
  "number":876,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-04-14T15:10:07Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add in capability for blindly splitting files into chunks for dask",
  "updated_at":"2023-04-14T15:10:07Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"In the issue, https://github.com/scikit-hep/uproot5/issues/438, it is reported that in root files Delphes created, `fBits` fields may be 6 bytes and desalinization processes may fail.\r\nAs an ad-hoc workaround, `fBits` field was decoded as a 1 byte field, but after the issue, https://github.com/scikit-hep/uproot5/issues/569, the change has reverted, and `kBits` type is decoded as the 4 bytes fields with a guess that Delphes does something wrong.\r\n\r\nHowever, this is not consistent with how the original root framework works.\r\nIn the original framework, in https://github.com/root-project/root/blob/61ea5a002198cd756bdce67e8499feded9dedc0b/io/io/src/TStreamerInfoReadBuffer.cxx#L1021, fields with type `kBits` is decoded as 4 bytes fields only if `kIsReferenced` is not set. If it is set, 2 additional bytes are read and used to get `TProcessID`. Then, `fUniqueID` on the decoded object is rewritten so that `fUniqueID` is really unique among all processes, I guess. In other words, `kBits` type is encoded to 4 or 6 bytes, depending on `kIsReferenced`.\r\nAlthough I could not find any document about this behavior, I now believe the original issue, https://github.com/scikit-hep/uproot5/issues/438, is NOT the bug of Delphes, but the bug of uproot and is to be fixed.\r\n\r\nAs long as `fUniqueID` is not used across the processes, just skipping 2 extra bytes when `kIsReferenced` should be fine, I guess.\r\nFor instance, the official JS implementation does in that way:\r\nhttps://github.com/root-project/root/blob/901c7cc0a986931bd771c0a873247f5708e54111/js/modules/io.mjs#L61\r\nI understand most people including me have never used `fBits` and this simpler way is rather time-consuming, but in principle, uproot should implement this. The best way is probably to adopt some form of lazy evaluation like an iterator,  but it would break backward compatibility.",
  "closed_at":null,
  "comments":2,
  "created_at":"2023-04-14T16:12:06Z",
  "id":1668541901,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5jc-3N",
  "number":878,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"fBits is not always 4 bytes",
  "updated_at":"2024-01-30T16:13:12Z",
  "user":"MDQ6VXNlcjEyNzkwNTA5"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"### Discussed in https://github.com/scikit-hep/uproot5/discussions/879\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **mattiasoldani** April 17, 2023</sup>\r\nHowdy!\r\n\r\nI'm using uproot 4.3.7 to open a ROOT file (of which a sample is attached: [TEST230417_LOCAL_SMALL.zip](https://github.com/scikit-hep/uproot5/files/11249405/TEST230417_LOCAL_SMALL.zip)) and access a tree (\"Z\") that is inside it. One of the branches of this tree, \"Cluster[6]\", should be an array of 6 custom zCluster objects for each event. Each zCluster object includes several members, mostly float and TVector3 entries. I checked with ROOT and all the branch dimensions are as expected, as seen in the TBrowser screenshots below: the tree contains 116 events, whereas all the Cluster[6] numerical subbranches contain 116*6 = 696 entries.\r\n\r\n![Z_Event_iEv](https://user-images.githubusercontent.com/62517661/232466982-a2d77f83-1c39-4ffc-a9e6-be7737ed16e1.jpg)\r\n![Z_Event_Cluster_nHits](https://user-images.githubusercontent.com/62517661/232466977-79dfddea-fc19-4205-aaf6-ebdcabb56498.jpg)\r\n\r\nIf I try to open the same file in uproot via\r\n```\r\nfile = uproot.open(\"TEST230417_LOCAL_SMALL.root\")\r\ntree = file[\"Z\"]\r\n```\r\nI get the correct tree, i.e. `tree.num_entries` outputs 116 and `tree.show()` outputs\r\n```\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nEvent                | zEvent                   | AsGroup(<TBranchElement 'Ev...\r\nEvent/TObject        | (group of fUniqueID:u... | AsGroup(<TBranchElement 'TO...\r\nEvent/TObject/fUn... | uint32_t                 | AsDtype('>u4')\r\nEvent/TObject/fBits  | uint32_t                 | AsDtype('>u4')\r\nEvent/iEv            | int32_t                  | AsDtype('>i4')\r\nEvent/W              | double                   | AsDtype('>f8')\r\nEvent/pK             | TLorentzVector           | AsStridedObjects(Model_TLor...\r\nEvent/xK             | TVector3                 | AsStridedObjects(Model_TVec...\r\nEvent/iPair          | int32_t                  | AsDtype('>i4')\r\nEvent/ig[6]          | int32_t[6]               | AsDtype(\"('>i4', (6,))\")\r\nEvent/igClus[6]      | int32_t[6]               | AsDtype(\"('>i4', (6,))\")\r\nEvent/pg[6]          | TLorentzVector           | AsStridedObjects(Model_TLor...\r\nEvent/Hit[6]         | zHit                     | AsStridedObjects(Model_zHit...\r\nEvent/nCluster       | int32_t                  | AsDtype('>i4')\r\nEvent/Cluster[6]     | zCluster                 | AsObjects(Model_zCluster)\r\nEvent/Vertex         | zVertex                  | AsGroup(<TBranchElement 'Ve...\r\nEvent/Vertex/Vert... | int32_t                  | AsDtype('>i4')\r\nEvent/Vertex/Vert... | int32_t[2]               | AsDtype(\"('>i4', (2,))\")\r\nEvent/Vertex/Vert... | int32_t                  | AsDtype('>i4')\r\nEvent/Vertex/Vert... | TVector3                 | AsStridedObjects(Model_TVec...\r\nEvent/Vertex/Vert... | TLorentzVector           | AsStridedObjects(Model_TLor...\r\nEvent/Vertex/Vert... | TVector3                 | AsStridedObjects(Model_TVec...\r\nEvent/Vertex/Vert... | TLorentzVector           | AsStridedObjects(Model_TLor...\r\nEvent/Vertex/Vert... | int32_t                  | AsDtype('>i4')\r\n```\r\n\r\nHowever, `tree.arrays([\"Event/Cluster[6]\"])` raises a `CannotBeAwkward: classes that can contain members of the same type cannot be Awkward Arrays because the depth of instances is unbounded` error and `tree.arrays([\"Event/Cluster[6]\"], library=\"np\")` outputs a dictionary of 116 entries whose values are single zCluster objects, i.e. one single zCluster per event. Accessing any of them via e.g. `tree[\"Event/Cluster[6]\"].arrays(library=\"np\")[\"Cluster[6]\"][0].all_members` seems to correctly output all the information regarding one single cluster, i.e.\r\n```\r\n{'Status': 1,\r\n 'Bin': 3,\r\n 'iSubdet': 0,\r\n 'Detected': 0,\r\n 'Primary': 0,\r\n 'Used': 0,\r\n 'Charge': 1,\r\n 'Eff': 1.0,\r\n 'xTrue': <TVector3 (version 3) at 0x7feb5830dc40>,\r\n 'xRec': <TVector3 (version 3) at 0x7feb58314280>,\r\n 'PosRes': 0.015788506169338613,\r\n 'xTrue1': <TVector3 (version 3) at 0x7feb58314190>,\r\n 'xTrue2': <TVector3 (version 3) at 0x7feb58314040>,\r\n 'xPre1': <TVector3 (version 3) at 0x7feb58314490>,\r\n 'xPre2': <TVector3 (version 3) at 0x7feb583145b0>,\r\n 'PreRes': 0.06397004001907128,\r\n 'ETrue': 0.07086701659225284,\r\n 'ERec': 0.03220946465343243,\r\n 'ERes': 0.09040274561889668,\r\n 'Converted': 1,\r\n 'dXdZTrue': 0.13310240197699721,\r\n 'dYdZTrue': -0.34894589067080145,\r\n 'nHits': 1,\r\n 'HitList': array([0, 1, 1, 1, 1, 1], dtype=int32),\r\n 'nConverted': 0}\r\n```\r\nbut where are the other 5 clusters? Am I missing something trivial?\r\n\r\nThank you very much!\r\n\r\n</div>",
  "closed_at":"2023-06-30T20:19:58Z",
  "comments":6,
  "created_at":"2023-04-17T13:49:12Z",
  "id":1671226929,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5jnOYx",
  "number":880,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Something wrong in deserialization of straightforward custom class",
  "updated_at":"2023-06-30T20:19:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"### Reproducing the issue\r\n\r\nI ran into the following behavior while reading a ~3 GB remote file over https. When trying to read a modest number of columns all in one go, the following results in a `http.client.RemoteDisconnected`:\r\n\r\n```python\r\nimport uproot\r\n\r\nfname = \"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV\"\\\r\n    \"-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic\"\\\r\n    \"_v12_ext3-v1_00000_0000.root\"\r\narrays_to_read = [\"Jet_mass\", \"nJet\", \"Muon_pt\", \"Jet_phi\", \"Jet_btagCSVV2\", \"Jet_pt\", \"Jet_eta\"]\r\n# arrays_to_read = [\"Jet_mass\", \"nJet\", \"Muon_pt\", \"Jet_phi\", \"Jet_btagCSVV2\", \"Jet_pt\"]  # this is ok\r\n\r\nwith uproot.open(fname) as f:\r\n    f[\"Events\"].arrays(arrays_to_read)\r\n```\r\n\r\nThis is with Python 3.11.3 and `uproot` 5.0.7. Full traceback in details:\r\n\r\n<details><summary>Details</summary>\r\n<p>\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"[...]/test.py\", line 31, in <module>\r\n    f[\"Events\"].arrays(arrays_to_read)\r\n  File \"[...]/python3.11/site-packages/uproot/behaviors/TBranch.py\", line 836, in arrays\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"[...]/python3.11/site-packages/uproot/behaviors/TBranch.py\", line 3147, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"[...]/python3.11/site-packages/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"[...]/python3.11/site-packages/uproot/behaviors/TBranch.py\", line 3060, in chunk_to_basket\r\n    basket = uproot.models.TBasket.Model_TBasket.read(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[...]/python3.11/site-packages/uproot/model.py\", line 860, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"[...]/python3.11/site-packages/uproot/models/TBasket.py\", line 228, in read_members\r\n    ) = cursor.fields(chunk, _tbasket_format1, context)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[...]/python3.11/site-packages/uproot/source/cursor.py\", line 194, in fields\r\n    return format.unpack(chunk.get(start, stop, self, context))\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[...]/python3.11/site-packages/uproot/source/chunk.py\", line 417, in get\r\n    self.wait(insist=stop)\r\n  File \"[...]/python3.11/site-packages/uproot/source/chunk.py\", line 354, in wait\r\n    self._raw_data = numpy.frombuffer(self._future.result(), dtype=self._dtype)\r\n                                      ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[...]/python3.11/site-packages/uproot/source/futures.py\", line 115, in result\r\n    delayed_raise(*self._excinfo)\r\n  File \"[...]/python3.11/site-packages/uproot/source/futures.py\", line 36, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"[...]/python3.11/site-packages/uproot/source/http.py\", line 304, in task\r\n    response = connection[0].getresponse()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[...]/python3.11/http/client.py\", line 1375, in getresponse\r\n    response.begin()\r\n  File \"[...]/python3.11/http/client.py\", line 318, in begin\r\n    version, status, reason = self._read_status()\r\n                              ^^^^^^^^^^^^^^^^^^^\r\n  File \"[...]/python3.11/http/client.py\", line 287, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n```\r\n\r\n</p>\r\n</details> \r\n\r\n### Other approaches and variations\r\n\r\nRemoving a column from the list, as in the commented out section, makes this run. It is surprisingly slow: these columns correspond to a few percent of the file content, and `[\"Jet_mass\", \"nJet\", \"Muon_pt\", \"Jet_phi\", \"Jet_btagCSVV2\", \"Jet_pt\"]` still took me almost 9 minutes to read. Downloading the whole file is much faster (<1 minute). The crash itself (when requesting too many columns) is fast, it takes about 5 seconds for that to happen for me.\r\n\r\nWhen running over the local file, reading all the initially intended columns (`[\"Jet_mass\", \"nJet\", \"Muon_pt\", \"Jet_phi\", \"Jet_btagCSVV2\", \"Jet_pt\", \"Jet_eta\"]`) gives me a maximum RSS of around 800 MB. The `f.file.source.num_requested_bytes` comes out to around 150 MB, I assume the difference is coming from decompression.\r\n\r\nI was wondering if somehow I run out of memory and that is what causes a crash that shows up as a misleading http error, but I would assume I have enough memory for that. I see similar behavior on a few different systems I have tried. I have also been in contact with the experts at UNL who see nothing suspicious on their end that might explain this. I was also wondering whether this is some kind of time-out, but 5 seconds total seems low for that.\r\n\r\nA way I found to successfully read the data is to batch via `uproot.iterate`:\r\n```python\r\nfor _ in uproot.iterate(f\"{fname}:Events\", expressions=arrays_to_read):\r\n    pass\r\n```\r\n\r\nThis works and is similarly slow to the non-batched version (~10 minutes). Setting `step_size` to a very large number to make this a single batch results in the same `http.client.RemoteDisconnected`.\r\n\r\nReading one column at a time in contrast is very fast, taking less than a minute with the remote file:\r\n```python\r\nwith uproot.open(fname) as f:\r\n    for arr in arrays_to_read:\r\n        f[\"Events\"].arrays(arr)\r\n```\r\nThis also does not produce a crash for the full column list `[\"Jet_mass\", \"nJet\", \"Muon_pt\", \"Jet_phi\", \"Jet_btagCSVV2\", \"Jet_pt\", \"Jet_eta\"]`.\r\n\r\n### Impact\r\n\r\nI am opening this as I am interested in better understanding the behavior, and to possibly flag it in case it is unexpected. This is not blocking me in any way, the `iterate` solution works fine (and should presumably generally be the better approach anyway). I am mainly curious about two things:\r\n- What causes the `http.client.RemoteDisconnected`?\r\n- Why is the reading of all columns at once (batched or not batched) so much slower than downloading the full file, or than reading one column at a time?",
  "closed_at":null,
  "comments":11,
  "created_at":"2023-04-24T11:11:01Z",
  "id":1681017337,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5kMkn5",
  "number":881,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Connection closed when reading big chunk of data over https",
  "updated_at":"2023-05-01T09:34:44Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/charliermarsh/ruff-pre-commit: v0.0.261 \u2192 v0.0.263](https://github.com/charliermarsh/ruff-pre-commit/compare/v0.0.261...v0.0.263)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-05-04T03:29:49Z",
  "comments":0,
  "created_at":"2023-04-25T01:07:16Z",
  "draft":false,
  "id":1682257570,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5PDkfW",
  "number":882,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-05-04T03:29:49Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: bugs caught by a Ruff update",
  "updated_at":"2023-05-04T03:29:50Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"When trying to make a Hist out of an geant4 histogram, you get the following error:\r\n\r\n```python\r\nimport uproot\r\nuproot_file = uproot.open(\"../scikit-hep-testdata/src/skhep_testdata/data/uproot-from-geant4.root\")\r\nuproot_file[\"cot_diff\"].to_hist()\r\n```\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/henryschreiner/git/scikit-hep/uproot-browser/.venv/lib/python3.11/site-packages/uproot/behaviors/TH1.py\", line 277, in to_hist\r\n    self.to_boost(metadata=metadata, axis_metadata=axis_metadata)\r\n  File \"/Users/henryschreiner/git/scikit-hep/uproot-browser/.venv/lib/python3.11/site-packages/uproot/behaviors/TH1.py\", line 226, in to_boost\r\n    axes = [\r\n           ^\r\n  File \"/Users/henryschreiner/git/scikit-hep/uproot-browser/.venv/lib/python3.11/site-packages/uproot/behaviors/TH1.py\", line 227, in <listcomp>\r\n    _boost_axis(self.member(name), axis_metadata)\r\n  File \"/Users/henryschreiner/git/scikit-hep/uproot-browser/.venv/lib/python3.11/site-packages/uproot/behaviors/TH1.py\", line 23, in _boost_axis\r\n    if axis.member(\"fLabels\") is not None:\r\n       ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/henryschreiner/git/scikit-hep/uproot-browser/.venv/lib/python3.11/site-packages/uproot/model.py\", line 556, in member\r\n    raise uproot.KeyInFileError(\r\nuproot.exceptions.KeyInFileError: not found: 'fLabels' because uproot.dynamic.Model_TAxis_v6 has only the following members:\r\n\r\n    '@fUniqueID', '@fBits', 'fName', 'fTitle', 'fNdivisions', 'fAxisColor', 'fLabelColor', 'fLabelFont', 'fLabelOffset', 'fLabelSize', 'fTickLength', 'fTitleOffset', 'fTitleSize', 'fTitleColor', 'fTitleFont', 'fNbins', 'fXmin', 'fXmax', 'fXbins', 'fFirst', 'fLast', 'fTimeDisplay', 'fTimeFormat'\r\n\r\nin file ../scikit-hep-testdata/src/skhep_testdata/data/uproot-from-geant4.root\r\n```\r\n\r\nYou can see this in the test data files, uproot-from-geant4.root. `cot_diff` is an example histogram that fails.\r\n\r\nUsing Uproot 5.0.7.\r\n\r\n\r\nI think the probe is `axis.member(\"fLabels\")` throws an error, not returns `None`, when it's not present, but this code assumes it returns None. This should probably have `none_if_missing=True`. (Just curious, why is this not `default=None`? That seems more pythonic, would look like `dict.get`).",
  "closed_at":"2023-06-01T07:47:18Z",
  "comments":3,
  "created_at":"2023-05-03T19:21:45Z",
  "id":1694660672,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5lAnhA",
  "number":883,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Can't make a hist out of a Geant4 histogram",
  "updated_at":"2023-06-01T07:47:18Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Possible fix for #883. Uproot-browser can now view histograms from Geant4.\n",
  "closed_at":"2023-05-04T03:13:50Z",
  "comments":0,
  "created_at":"2023-05-03T19:31:59Z",
  "draft":false,
  "id":1694673792,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5PtK6i",
  "number":884,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-05-04T03:13:50Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: histograms from Geant4",
  "updated_at":"2023-05-04T03:38:48Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"While trying to opening double nested root arrays with pandas, the array type remains as STLVector. The data I'm using has index, subindex and subsubindex. The double nested array opens fine with the default awkward opening. \r\n```python\r\nimport uproot\r\ndata = uproot.open(\"file.root\").arrays(['Double Nested', 'Single Nested'], library=\"pd\")\r\nprint(type(data.iloc[0]['Double Nested']), type(data.iloc[0]['Single Nested']))\r\n```\r\n```\r\n<class 'uproot.containers.STLVector'> <class 'awkward.highlevel.Array'>\r\n```\r\nWhile opening data with the normal awkward is the expected awkward array type\r\n```python\r\nimport uproot\r\ndata = uproot.open(\"file.root\").arrays(['Double Nested', 'Single Nested'])\r\nprint(type(data[0]['Double Nested']), type(data[0]['Single Nested']))\r\n```\r\n```\r\n<class 'awkward.highlevel.Array'> <class 'awkward.highlevel.Array'>\r\n```\r\nI'm using uproot  5.0.7, pandas 2.0.1 and awkward-pandas 2022.12a1. ",
  "closed_at":"2023-07-07T07:04:58Z",
  "comments":0,
  "created_at":"2023-05-04T21:55:08Z",
  "id":1696751984,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5lImFw",
  "number":885,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Double nested Awkward arrays are STL:Vector using library=\"pd\"",
  "updated_at":"2023-07-07T07:04:58Z",
  "user":"U_kgDOBw6OhA"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Addresses the first part of https://github.com/scikit-hep/uproot5/issues/880, where the recursion in `awkward_form` would fail when dealing with same class children (wrongly added to `breadcrumbs`) ",
  "closed_at":"2023-05-16T21:52:23Z",
  "comments":2,
  "created_at":"2023-05-05T07:57:12Z",
  "draft":false,
  "id":1697179829,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5P1xG0",
  "number":886,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-05-16T21:52:23Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: awkward_form breadcrumbs class issue 880",
  "updated_at":"2023-05-16T21:52:24Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [pypa/gh-action-pypi-publish](https://github.com/pypa/gh-action-pypi-publish) from 1.8.5 to 1.8.6.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/pypa/gh-action-pypi-publish/releases\">pypa/gh-action-pypi-publish's releases</a>.</em></p>\n<blockquote>\n<h2>v1.8.6</h2>\n<h2>What's Updated</h2>\n<ul>\n<li><a href=\"https://github.com/sponsors/woodruffw\"><code>@\u200bwoodruffw</code></a> dropped the references to a \u201cprivate beta\u201d from the project docs and runtime in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/147\">pypa/gh-action-pypi-publish#147</a>. He also clarified that the API tokens are still more secure than passwords in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/150\">pypa/gh-action-pypi-publish#150</a>.</li>\n<li><a href=\"https://github.com/sponsors/asherf\"><code>@\u200basherf</code></a> noticed that the action metadata incorrectly marked the <code>password</code> field as required and contributed a correction in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/151\">pypa/gh-action-pypi-publish#151</a></li>\n<li><a href=\"https://github.com/sponsors/webknjaz\"><code>@\u200bwebknjaz</code></a> moved the Trusted Publishing example to the top of the README in hopes that new users would default to using it via <a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/f47b34707fd264d5ddb1ef322ca74cf8e4cf351b\">https://github.com/pypa/gh-action-pypi-publish/commit/f47b34707fd264d5ddb1ef322ca74cf8e4cf351b</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/sponsors/asherf\"><code>@\u200basherf</code></a> made their first contribution in <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/pull/151\">pypa/gh-action-pypi-publish#151</a></li>\n</ul>\n<p><strong>Full Diff</strong>: <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.5...v1.8.6\">https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.5...v1.8.6</a></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/a56da0b891b3dc519c7ee3284aff1fad93cc8598\"><code>a56da0b</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/151\">#151</a> from asherf/trusted</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/e4b903174144ed1ae155796f72e117b95cf30c3f\"><code>e4b9031</code></a> password input is no longer required, since not specifying it implies trusted...</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/5a085bf49e449ba94cc551efdc03b14b2be3788c\"><code>5a085bf</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/150\">#150</a> from trail-of-forks/tob-doc-tweaks</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/0811f991bd3b72bc79a131736a1966d9df922f60\"><code>0811f99</code></a> README: small doc tweaks</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/f47b34707fd264d5ddb1ef322ca74cf8e4cf351b\"><code>f47b347</code></a> \ud83d\udcdd\ud83c\udfa8 Put OIDC on pedestal @ README</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/7a1a355fb5ad6afb4e8f748ad036708c1c61c396\"><code>7a1a355</code></a> \ud83c\udfa8 Show GH environments use in README examples</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/3b6670b0bd04d54039641fb3b2ac878aad9d70fc\"><code>3b6670b</code></a> Merge pull request <a href=\"https://redirect.github.com/pypa/gh-action-pypi-publish/issues/147\">#147</a> from trail-of-forks/tob-stabilize-oidc</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/c008c2f40abc7b85467b393f3b78e67391ffa7f8\"><code>c008c2f</code></a> README: re-add OIDC note</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/fe431ff9ad22d027a59d866e45c4e40d93d8ce57\"><code>fe431ff</code></a> README, oidc-exchange: remove beta references</li>\n<li><a href=\"https://github.com/pypa/gh-action-pypi-publish/commit/c542b72dc68d2280248f2d864ba901e0c31a3ee7\"><code>c542b72</code></a> Bump WPS flake8 plugin set to v0.17.0</li>\n<li>Additional commits viewable in <a href=\"https://github.com/pypa/gh-action-pypi-publish/compare/v1.8.5...v1.8.6\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pypa/gh-action-pypi-publish&package-manager=github_actions&previous-version=1.8.5&new-version=1.8.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2023-05-16T18:59:56Z",
  "comments":3,
  "created_at":"2023-05-08T09:58:30Z",
  "draft":false,
  "id":1699938801,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5P-8Rr",
  "number":887,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-05-16T18:59:56Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump pypa/gh-action-pypi-publish from 1.8.5 to release/v1",
  "updated_at":"2023-05-16T18:59:58Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"This is for every TH2 that I have thus far inspected. If it matters, the TH2 histograms were written into a TTree originally by C++ ROOT code. This is most easily seen if your TH2 is not a square histogram (number of bins along x-axis != number of bins along y-axis). Here's some below code to reproduce the behavior showing that the 3rd plot on the right is indeed incorrect:\r\n```\r\nimport uproot\r\nimport matplotlib\r\nfrom matplotlib import pyplot as plt\r\nimport mplhep as hep\r\n\r\nfig, ax = plt.subplots(1, 3, figsize=(30, 10))\r\n\r\nhep.hist2dplot(fileptr['th2_name'].variances(), norm=matplotlib.colors.LogNorm(), ax=ax[0])\r\nmyhist = fileptr['th2_name'].to_pyroot()\r\n\r\nvariances = [[myhist.GetBinError(i+1, j+1)**2 for j in range(768)] for i in range(192)]\r\nhep.hist2dplot(variances, norm=matplotlib.colors.LogNorm(), ax=ax[1])\r\nhep.hist2dplot(fileptr['th2_name'].to_hist().variances(), norm=matplotlib.colors.LogNorm(), ax=ax[2])\r\n```\r\n\r\n\r\nFigure showing differences:\r\n![th2_variances](https://user-images.githubusercontent.com/6516226/236842079-e9fccb7f-a4dc-4aa4-8a5f-69665b6b8a23.png)\r\n\r\nEdit: Added image showing example output of above code\r\n",
  "closed_at":"2023-10-03T19:15:35Z",
  "comments":3,
  "created_at":"2023-05-08T11:03:49Z",
  "id":1700044875,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5lVKBL",
  "number":888,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"TH2 variances returned by .to_hist().variances() are incorrect (swapped x and y axis and perhaps some counting issues)",
  "updated_at":"2023-10-03T19:15:35Z",
  "user":"MDQ6VXNlcjY1MTYyMjY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/charliermarsh/ruff-pre-commit: v0.0.263 \u2192 v0.0.265](https://github.com/charliermarsh/ruff-pre-commit/compare/v0.0.263...v0.0.265)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-05-15T20:36:40Z",
  "comments":0,
  "created_at":"2023-05-09T01:44:04Z",
  "draft":false,
  "id":1701172413,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5QDItW",
  "number":889,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-05-15T20:36:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-05-15T20:36:41Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"This addresses a sub-issue here: https://github.com/CoffeaTeam/coffea/issues/807\r\n\r\nWhere it would be nice indeed to focus in on buggy/interesting sub-ranges through uproot.dask, which is not possible right now in a simple way.\r\n\r\nI've included one implementation as example!\r\n\r\nIt composes nicely with `steps_per_file` and `step_size` as well.",
  "closed_at":"2023-05-15T22:41:44Z",
  "comments":2,
  "created_at":"2023-05-11T22:31:06Z",
  "draft":false,
  "id":1706657371,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5QVko0",
  "number":890,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: Allow uproot.dask to know about entry_start and entry_stop",
  "updated_at":"2023-05-15T23:04:01Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Extend the uproot.dask interface to allow specification of chunks along the lines of:\r\n```python3\r\nfiles = {\"/some/file/path.root\" : {\"objectpath\": \"Events\", \"chunks\": [[0, 10000], [15000, 20000], ...]}, ...}\r\n```\r\nor\r\n```python3\r\nfiles = {\"/some/file/path.root\" : {\"objectpath\": \"Events\", \"chunks\": [0, 10000, 15000, 20000, ...]}, ...}\r\n```\r\n\r\nWhere the first version allows discontinuous chunks and the second specifies chunk offsets.\r\n\r\nDiscussed with @jpivarski in the last two months or so.\r\n\r\nI'll provide an implementation example soon if it helps!",
  "closed_at":"2023-06-09T13:23:33Z",
  "comments":0,
  "created_at":"2023-05-11T22:38:45Z",
  "id":1706662722,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5luZtC",
  "number":891,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot.dask should be able to receive pre-defined chunks.",
  "updated_at":"2023-06-09T13:23:33Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Taken from https://github.com/scikit-hep/uproot5/pull/890#discussion_r1194429693 (originally 93e8a59eacbee64a23a21032d92582ef130233d1).",
  "closed_at":"2023-05-16T15:29:09Z",
  "comments":1,
  "created_at":"2023-05-15T23:03:07Z",
  "draft":false,
  "id":1710963558,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Qj1eG",
  "number":892,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-05-16T15:29:09Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: adapt to scikit-hep/awkward#2437.",
  "updated_at":"2023-05-16T15:29:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/charliermarsh/ruff-pre-commit: v0.0.265 \u2192 v0.0.270](https://github.com/charliermarsh/ruff-pre-commit/compare/v0.0.265...v0.0.270)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-06-02T17:05:49Z",
  "comments":0,
  "created_at":"2023-05-16T01:11:14Z",
  "draft":false,
  "id":1711053085,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5QkIFj",
  "number":893,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-06-02T17:05:49Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-06-02T17:05:50Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR \r\n- changes use of private APIs for public ones.[^note]\r\n- replaces use of dask-awkward's test-util with a call to `ak.almost_equal`\r\n- bumps the minimum dask-awkward version to the near-latest release\r\n\r\n\r\n[^note]: we don't explicitly pin awkward for these features, but they're used in a dask-awkward context, so we can rely on the pin imposed there. It's not ideal, but I don't think we need to worry too much.",
  "closed_at":"2023-09-14T17:04:35Z",
  "comments":1,
  "created_at":"2023-05-22T11:02:39Z",
  "draft":false,
  "id":1719446996,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5RASmh",
  "number":894,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-09-14T17:04:35Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"refactor: use public typetracer API",
  "updated_at":"2023-09-14T17:04:36Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"![image](https://github.com/scikit-hep/uproot5/assets/58833553/0c771a5d-7503-45be-a591-45e160055071)\r\n\r\n\r\n![image](https://github.com/scikit-hep/uproot5/assets/58833553/c0cf7467-67ea-403c-bc71-18939682ec3b)\r\n\r\n\r\nI hope my problem is self explanatory by images. Thank you.",
  "closed_at":"2023-05-23T20:00:28Z",
  "comments":7,
  "created_at":"2023-05-23T18:50:35Z",
  "id":1722608772,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5mrOyE",
  "number":895,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"'AwkwardExtensionArray' object is not callable",
  "updated_at":"2023-05-23T20:01:11Z",
  "user":"MDQ6VXNlcjU4ODMzNTUz"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Hello, I am using uproot 4.3.7 and it crashed when reading a remote file.\r\n\r\nActually this problem started on the ROOT forum, since when plotting a single variable from a TTree from a remote files it tooks several minutes, while locally less then a second. The file is really small 4.5M\r\n\r\nhttps://root-forum.cern.ch/t/reading-from-http-very-slow/53213/26\r\n\r\n```\r\ntime python -c \"import uproot; uproot.open('https://cernbox.cern.ch/remote.php/dav/public-files/1Cy1HIf03Ca76Dm/test_ntuples_200123.root').get('Electrons_All').arrays('pt__NOSYS')\"\r\n\r\n    raise uproot.deserialization.DeserializationError(\r\nuproot.deserialization.DeserializationError: while reading\r\n\r\n    TBasket version None as uproot.models.TBasket.Model_TBasket (? bytes)\r\n        fNbytes: 218759168\r\n        fObjlen: 65798144\r\n        fDatime: 293105760\r\n        fKeylen: 32314\r\n        fCycle: 85\r\nMembers for TBasket: fNbytes?, fObjlen?, fDatime?, fKeylen?, fCycle?\r\n\r\nattempting to get bytes 38380:38398\r\noutside expected range 6085:8333 for this Chunk\r\nin file https://cernbox.cern.ch/remote.php/dav/public-files/1Cy1HIf03Ca76Dm/test_ntuples_200123.root\r\n\r\n```\r\n\r\nit is curious that if I read if from elsewhere it works\r\n\r\n```\r\ntime python -c \"import uproot; uproot.open('http://rgw.fisica.unimi.it/test-ruggero/test_ntuples_200123.root?AWSAccessKeyId=M06HBTUGIKXVXYH1RES6&Signature=hpX%2FNzIKINZd825AWEGw%2FuVQ4nU%3D&Expires=1693581796').get('Electrons_All').arrays('pt__NOSYS')\"\r\n```\r\n\r\nyou can download the root file locally from these url.",
  "closed_at":"2023-08-03T13:58:06Z",
  "comments":5,
  "created_at":"2023-05-25T10:09:30Z",
  "id":1725516464,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5m2Uqw",
  "number":896,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Crash when reading remote file",
  "updated_at":"2023-08-03T13:58:06Z",
  "user":"MDQ6VXNlcjE0MzM4OQ=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"The function `uproot.from_pyroot` crashes for `TProfile3D` objects if the `uproot` package is not loaded in the main scope. As can be tested with the code snippet below, the execution produces a segmentation fault if `import uproot` remains commented. Surprisingly the code runs fine for `TH1D` objects for which it doesn't matter where the package is being imported. The code was tested with `uproot` version `5.0.7`.\r\n\r\n```python\r\nimport ROOT\r\n\r\n# import uproot # if we import uproot at this level then we can use TProfile3D\r\n\r\ndef function():\r\n\r\n    f = ROOT.TFile.Open('test.root', 'recreate')\r\n    # h = ROOT.TH1D() # if we use a TH1D object instead of TProfile3D everything runs fine in both cases\r\n    h = ROOT.TProfile3D()\r\n    h.Write('h')\r\n    f.Close()\r\n\r\n    f = ROOT.TFile.Open('test.root', 'read')\r\n    h = f.Get('h')\r\n\r\n    import uproot # this import must be defined in the main scope for TProfile3D\r\n    uproot.from_pyroot(h)\r\n\r\n\r\nfunction()\r\n```\r\n\r\nThe execution produces a long backtrace, which finishes with\r\n\r\n```\r\nmessage.WriteObject(obj)\r\ncppyy.ll.SegmentationViolation: void TBufferIO::WriteObject(const TObject* obj, bool cacheReuse = kTRUE) =>\r\nSegmentationViolation: segfault in C++; program state was reset\r\n```",
  "closed_at":"2023-08-08T14:04:45Z",
  "comments":4,
  "created_at":"2023-05-26T15:03:01Z",
  "id":1727797278,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5m_Bge",
  "number":897,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Class-dependent segmentation violation when converting PyROOT objects",
  "updated_at":"2023-08-08T14:04:45Z",
  "user":"MDQ6VXNlcjE0ODcyMDY2"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Fixes #891\r\n\r\nExample implemented for delayed-open.\r\n\r\n---------------\r\n\r\nFrom @jpivarski: implemented for four cases, `open_files=False`,`True` and `library=\"np\"`,`\"ak\"`.\r\n\r\nIf any user-specified steps (we're calling them \"steps,\" not \"chunks\" now) are out of bounds, this raises a `ValueError` in all four cases (though the `open_files=False` cases raise errors later than `open_files=True`).\r\n\r\nIf steps are specified, all four cases set Dask array `chunks` and dask-awkward `divisions`.\r\n\r\nAlso new: the `open_files=True` and `library=\"ak\"` case wasn't setting `divisions`, though it should have been.",
  "closed_at":"2023-06-09T13:23:32Z",
  "comments":5,
  "created_at":"2023-06-07T18:23:08Z",
  "draft":false,
  "id":1746446946,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5ScXNL",
  "number":898,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-06-09T13:23:32Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: chunk specification in uproot.dask",
  "updated_at":"2023-06-09T15:57:36Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR will add support for dask-contrib/dask-awkward#184 to the uproot input layer, so that unused fields can still be partially represented in the layouts of the loaded arrays.",
  "closed_at":"2023-06-30T14:32:18Z",
  "comments":10,
  "created_at":"2023-06-09T15:21:48Z",
  "draft":false,
  "id":1750093763,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5SomEH",
  "number":900,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-06-30T14:32:18Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add `unproject_layout` support",
  "updated_at":"2023-06-30T14:32:19Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":null,
  "closed_at":"2023-06-10T14:05:43Z",
  "comments":0,
  "created_at":"2023-06-09T16:33:12Z",
  "draft":false,
  "id":1750186154,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5So6SX",
  "number":901,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-06-10T14:05:43Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: unproject_layout support",
  "updated_at":"2023-06-10T14:05:43Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/charliermarsh/ruff-pre-commit: v0.0.270 \u2192 v0.0.275](https://github.com/charliermarsh/ruff-pre-commit/compare/v0.0.270...v0.0.275)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-06-29T22:26:12Z",
  "comments":1,
  "created_at":"2023-06-13T01:39:56Z",
  "draft":false,
  "id":1753873881,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5S1VCU",
  "number":902,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-06-29T22:26:12Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-06-29T22:26:13Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Logging it here since we discussed it over zoom / on friday.\r\n\r\nReplicating the text of where the problem is:\r\n\"\"\"\r\nso the issue is that the nanoevents keys do not get subbed in until project_columns is called https://github.com/scikit-hep/uproot5/blob/main/src/uproot/_dask.py#L789 and it's only called when we optimize columns https://github.com/dask-contrib/dask-awkward/blob/main/src/dask_awkward/lib/optimize.py#L124 (iiuc)\r\n\"\"\"\r\n\r\nSo I think we can make it so that when uproot.dask creates the layer we make sure to pass the correct dask-awkward form keys.",
  "closed_at":"2023-06-20T20:57:16Z",
  "comments":0,
  "created_at":"2023-06-19T00:18:58Z",
  "id":1762526105,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5pDgOZ",
  "number":904,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"uproot.dask form remapping does not work when dask-awkward column optimization turned off",
  "updated_at":"2023-06-20T20:57:16Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Fixes #904 ",
  "closed_at":"2023-06-20T20:57:15Z",
  "comments":13,
  "created_at":"2023-06-19T00:30:52Z",
  "draft":false,
  "id":1762531824,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5TSa2_",
  "number":905,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-06-20T20:57:15Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: if using form remapping start off with full list of remapped columns",
  "updated_at":"2023-06-27T21:50:50Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"For some branches i get an Exception when `.interpretation` is accessed. Noticed this when working on the coffea PHYSLITE interpretation (the NanoEvents factory tries to check whether a branch is readable in uproot by looking at the interpretation). It also happens when i call `tree.show()`.\r\n\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'5.0.8'\r\n```\r\n\r\nExample:\r\n\r\n```python\r\nfrom skhep_testdata import data_path\r\nimport uproot\r\nfilename = data_path(\"uproot-issue-798.root\") # PHYSLITE example file\r\nf = uproot.open(filename)\r\ntree = f[\"CollectionTree\"]\r\n\r\n>>> tree[\"EventInfo\"].interpretation\r\nTraceback (most recent call last):\r\n  File \"/home/nikolai/python/uproot4/src/uproot/interpretation/identify.py\", line 383, in interpretation_of\r\n    from_dtype = _leaf_to_dtype(leaf, getdims=False).newbyteorder(\">\")\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nikolai/python/uproot4/src/uproot/interpretation/identify.py\", line 102, in _leaf_to_dtype\r\n    return numpy.dtype((_ftype_to_dtype(leaf.member(\"fType\")), dims))\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nikolai/python/uproot4/src/uproot/interpretation/identify.py\", line 60, in _ftype_to_dtype\r\n    raise NotNumerical()\r\nuproot.interpretation.identify.NotNumerical\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/nikolai/python/uproot4/src/uproot/behaviors/TBranch.py\", line 1933, in interpretation\r\n    self._interpretation = uproot.interpretation.identify.interpretation_of(\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nikolai/python/uproot4/src/uproot/interpretation/identify.py\", line 460, in interpretation_of\r\n    return out.simplify()\r\n           ^^^^^^^^^^^^^^\r\n  File \"/home/nikolai/python/uproot4/src/uproot/interpretation/objects.py\", line 450, in simplify\r\n    return self._model.strided_interpretation(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/nikolai/python/uproot4/src/uproot/model.py\", line 1210, in strided_interpretation\r\n    return versioned_cls.strided_interpretation(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<dynamic>\", line 48, in strided_interpretation\r\n  File \"<dynamic>\", line 48, in strided_interpretation\r\n  File \"<dynamic>\", line 30, in strided_interpretation\r\n  File \"/home/nikolai/python/uproot4/src/uproot/interpretation/objects.py\", line 600, in __init__\r\n    while members[first_value_loc] == (None, None):\r\n          ~~~~~~~^^^^^^^^^^^^^^^^^\r\nIndexError: list index out of range\r\n```\r\n\r\nThis happens for a lot of branches that don't contain any numerical data and that we anyway want to skip, but i think it should not raise an Exception when accessing `.interpretation`.\r\n\r\n",
  "closed_at":"2023-06-29T21:55:41Z",
  "comments":1,
  "created_at":"2023-06-20T10:44:50Z",
  "id":1765133624,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5pNc04",
  "number":906,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Exception on accessing `.interpretation` member for some non-numeric branches",
  "updated_at":"2023-06-29T21:55:42Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"I'd like to request writing support for profile hists that are coming from hist/boost-histogram. The use case I ran into was merging some root files containing a variety of histograms. My procedure was roughly:\r\n\r\n1. Read file\r\n2. Retrieve hists, and convert using to_hist to take advantage of the `__add__` functionality already implemented\r\n3. Add the hists\r\n4. Write the merged hists to file\r\n\r\nThe issue is in step 4. This works fine for standard histograms, and from the docs I understand that it should work for TProfile objects that aren't converted to hist (eg. just read by uproot, and still contained in a Model), but it doesn't work after I've done the conversion. Looking into the exception that's raised\r\n\r\nhttps://github.com/scikit-hep/uproot5/blame/a9f4374607b7434ef89ebec82c784d34bfc31413/src/uproot/writing/identify.py#L248-L255\r\n\r\nthe `NotImplementedError` obviously suggests that profiles intentionally aren't supported at the moment, but the `ValueError` suggests that perhaps they were intended to be? In any case, adding support for writing profiles would be much appreciated, and would make merging hists a bit easier (this would be helpful because `hadd` isn't always available). Thanks!",
  "closed_at":"2024-01-18T16:37:43Z",
  "comments":4,
  "created_at":"2023-06-21T06:36:10Z",
  "id":1766856609,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5pUBeh",
  "number":908,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Request for support of writing hist-derived profiles",
  "updated_at":"2024-01-30T19:28:02Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This issue was created when adding a way to deal with data having extra offsets. \r\n\r\nFor that, members were set to have a (None, None) item before headers were added ([L234](https://github.com/scikit-hep/uproot5/blob/a9f4374607b7434ef89ebec82c784d34bfc31413/src/uproot/streamers.py#L234))\r\n\r\nThen, in `objects.py`, the `members` var was cleared of the `(None, None)`s. ([L600](https://github.com/scikit-hep/uproot5/blob/a9f4374607b7434ef89ebec82c784d34bfc31413/src/uproot/interpretation/objects.py#L600))\r\n\r\nHowever, it was only tested in the case where a header is added after the Nones tuple, thus the while loop would stop at the first actual header and continue to clear the members var of Nones. In the file that caught this issue, no headers are appended and the `members` var in the end is equal only to (None, None) which causes this while loop to complain of index out of range. \r\n\r\n\r\n*** I used `range` to fix the issue, because of the way the condition is imposed.  If `enumerate` is used, pre-commit will complain of not using `first_value_loc` in the loop body. ",
  "closed_at":"2023-06-29T21:55:40Z",
  "comments":0,
  "created_at":"2023-06-29T18:35:26Z",
  "draft":false,
  "id":1781300832,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5URFT0",
  "number":910,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-06-29T21:55:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: issues with members when dealing with non-numeric branches issue #906",
  "updated_at":"2023-06-29T21:55:41Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR addresses the second part of issue #880, where only one of the 6 Clusters were present into the final array. This was due to the interpretation being `AsObjects(Model_zCluster)` instead of `AsObjects(AsArray(False, False, Model_zCluster, (6,)))`.",
  "closed_at":"2023-06-30T20:19:57Z",
  "comments":0,
  "created_at":"2023-06-30T19:05:39Z",
  "draft":false,
  "id":1783035184,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5UW5z5",
  "number":911,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-06-30T20:19:57Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: interpretation for arrays of non-numerical objects issue 880 part2",
  "updated_at":"2023-06-30T20:19:58Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Double nested ROOT arrays when opened with pandas their type remained to be an `STLVector`. This happened because `AwkwardForth` was not being invoked for `pandas` and the processing result was a numpy array which was then added to a `series` without any other transformations. \r\n\r\n\r\n\r\n\r\n",
  "closed_at":"2023-07-07T07:04:57Z",
  "comments":0,
  "created_at":"2023-07-03T09:59:41Z",
  "draft":false,
  "id":1785818361,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5UgcUZ",
  "number":912,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-07-07T07:04:57Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: pandas and double nested vectors issue 885",
  "updated_at":"2023-07-07T07:04:58Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"See https://github.com/scientific-python/cookie/pull/200.\r\n\r\nCommitted via https://github.com/asottile/all-repos",
  "closed_at":"2023-07-06T14:20:57Z",
  "comments":0,
  "created_at":"2023-07-03T16:10:12Z",
  "draft":false,
  "id":1786470972,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5UiqMl",
  "number":913,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-07-06T14:20:57Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: ruff moved to astral-sh",
  "updated_at":"2023-07-06T14:20:59Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"See https://github.com/scientific-python/cookie/issues/201.\r\n\r\nCommitted via https://github.com/asottile/all-repos",
  "closed_at":"2023-07-06T14:24:46Z",
  "comments":0,
  "created_at":"2023-07-03T17:42:03Z",
  "draft":false,
  "id":1786583940,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5UjChR",
  "number":914,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-07-06T14:24:46Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: target-version no longer needed by Black or Ruff",
  "updated_at":"2023-07-06T14:24:48Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- https://github.com/charliermarsh/ruff-pre-commit \u2192 https://github.com/astral-sh/ruff-pre-commit\n- [github.com/astral-sh/ruff-pre-commit: v0.0.275 \u2192 v0.0.276](https://github.com/astral-sh/ruff-pre-commit/compare/v0.0.275...v0.0.276)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-07-06T13:15:04Z",
  "comments":0,
  "created_at":"2023-07-04T02:33:02Z",
  "draft":false,
  "id":1787040118,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Ukkj_",
  "number":915,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-07-06T13:15:03Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-07-06T13:15:07Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"This introduces support for for accessing files from S3. The `minio` client is used to interact with S3 service provider to obtain a presigned http URL that is valid for 7 days. The S3 method for getting objects is not used for simplicity, but also there is an assumption that the HTTP multipart queries might have a better performance.\r\n\r\nWith regards to choice of URI format, the `s3://<bucket>/<object>` schema as supported by the `aws` tool is implemented with some extensions to allow for non-AWS endpoints. Officially, there are also supposed to be another two formats[1]:\r\n\r\n - *virtual-hosted-style* `https://<bucket>.s3.<region>.amazonaws.com/<object>`\r\n - *path-style* `https://s3.<region>.amazonaws.com/<bucket>/<object>` which is \"deprecated\"\r\n\r\nFor these urls is not clear how to differentiate between the standard HTTPS and the S3 in the case when non-AWS endpoint is used.\r\nIt is worth noting that, in ROOT, `TS3WebFile` introduces some *path-style* schemas that don't fit any of the known tools.\r\n\r\n[1] https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-bucket-intro.html\r\n\r\n### TODO:\r\n - [x] update documentation\r\n - [x] add tests (need permanent public instance)\r\n\r\nResolves: #443\r\n\r\ncc @wdconinc @sly2j",
  "closed_at":"2023-08-10T19:44:13Z",
  "comments":8,
  "created_at":"2023-07-05T01:08:36Z",
  "draft":false,
  "id":1788609899,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Up4HA",
  "number":916,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-10T19:44:13Z"
  },
  "reactions":{
   "rocket":2,
   "total_count":2
  },
  "state":"closed",
  "state_reason":null,
  "title":"feat: Support reading from S3",
  "updated_at":"2023-08-10T19:44:14Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/psf/black: 23.3.0 \u2192 23.7.0](https://github.com/psf/black/compare/23.3.0...23.7.0)\n- [github.com/astral-sh/ruff-pre-commit: v0.0.276 \u2192 v0.0.282](https://github.com/astral-sh/ruff-pre-commit/compare/v0.0.276...v0.0.282)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-08-10T15:11:26Z",
  "comments":0,
  "created_at":"2023-07-11T02:36:41Z",
  "draft":false,
  "id":1797963841,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5VJour",
  "number":918,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-10T15:11:26Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-08-10T15:11:27Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Hi I have tried to open root file from DelphesSchema as\r\n\r\n\r\n\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'5.0.10'\r\n>>> \r\n\r\n\r\nimport awkward as ak\r\nimport cabinetry\r\nfrom coffea.nanoevents import *\r\nfrom coffea.nanoevents.schemas.delphes import DelphesSchema, zip_forms\r\n\r\n\r\nimport uproot # for reading .root files\r\nimport numpy as np # for numerical calculations such as histogramming\r\nimport utils  # contains code for bookkeeping and cosmetics, as well as some boilerplate\r\n\r\n\r\n\r\nfname = \"/Volumes/wd3TB/mg5Works/pp2zz_HLLHC_HELHC_LEFCC/production/dataFiles/cx_CBtWL4/E37.5/run-1/Events/cv3.5/zz_delphes_events.root\"\r\n\r\nevents = NanoEventsFactory.from_root(\r\n    fname, schemaclass=DelphesSchema, treepath=\"Delphes\"\r\n)\r\n\r\ndata = events.events()\r\nmask = ak.num(data.Electron) >= 1\r\n\r\nelectrons = data.Electron[mask]\r\nleading_electrons = electrons[:,0]\r\nprint(electrons.pt)\r\nprint(leading_electrons.pt)\r\n\r\nand it is failing with\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[11], line 3\r\n      1 fname = \"/Volumes/wd3TB/mg5Works/pp2zz_HLLHC_HELHC_LEFCC/production/dataFiles/cx_CBtWL4/E37.5/run-1/Events/cv3.5/zz_delphes_events.root\"\r\n----> 3 events = NanoEventsFactory.from_root(\r\n      4     fname, schemaclass=DelphesSchema, treepath=\"Delphes\"\r\n      5 )\r\n\r\nFile /usr/local/anaconda3/lib/python3.10/site-packages/coffea/nanoevents/factory.py:124, in NanoEventsFactory.from_root(cls, file, treepath, entry_start, entry_stop, runtime_cache, persistent_cache, schemaclass, metadata, uproot_options, access_log, iteritems_options)\r\n    117 mapping = UprootSourceMapping(\r\n    118     TrivialUprootOpener(uuidpfn, uproot_options),\r\n    119     cache={},\r\n    120     access_log=access_log,\r\n    121 )\r\n    122 mapping.preload_column_source(partition_key[0], partition_key[1], tree)\r\n--> 124 base_form = mapping._extract_base_form(\r\n    125     tree, iteritems_options=iteritems_options\r\n    126 )\r\n    128 return cls._from_mapping(\r\n    129     mapping,\r\n    130     partition_key,\r\n   (...)\r\n    135     metadata,\r\n    136 )\r\n\r\nFile /usr/local/anaconda3/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:105, in UprootSourceMapping._extract_base_form(cls, tree, iteritems_options)\r\n    101 if cls._fix_awkward_form_of_iter:\r\n    102     form = uproot._util.recursively_fix_awkward_form_of_iter(\r\n    103         awkward, branch.interpretation, form\r\n    104     )\r\n--> 105 form = uproot._util.awkward_form_remove_uproot(awkward, form)\r\n    106 form = json.loads(\r\n    107     form.tojson()\r\n    108 )  # normalizes form (expand NumpyArray classes)\r\n    109 try:\r\n\r\nAttributeError: module 'uproot._util' has no attribute 'awkward_form_remove_uproot'\r\n\r\nHow can I solve this issue, can you guide me about it, please?\r\n\r\nthank you\r\n\r\n",
  "closed_at":"2023-08-03T13:36:11Z",
  "comments":3,
  "created_at":"2023-07-11T13:31:30Z",
  "id":1798954793,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5rOd8p",
  "number":919,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"AttributeError: module 'uproot._util' has no attribute 'awkward_form_remove_uproot'",
  "updated_at":"2023-08-03T13:36:11Z",
  "user":"MDQ6VXNlcjc2NTMyMzQx"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"Uproot Version: 5.0.10\r\n\r\nI have a very long filename:\r\n```\r\nfilename = \"/servicey/output/7242feb5-db50-46b3-a3f7-9588c2a204ac/scratch/root:::atlasxrootd-kit.gridka.de:1094::pnfs:gridka.de:atlas:disk-only:atlasdatadisk:rucio:mc16_13TeV:05:ce:DAOD_PHYS.30899209._000097.pool.root.1\"\r\n```\r\n\r\nWhen I attempt to open the file with\r\n```python\r\nuproot.open(filenme)\r\n```\r\n\r\nIt can't open the file and it looks like someone is truncating the name:\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: '/servicey/output/7242feb5-db50-46b3-a3f7-9588c2a204ac/scratch/root:::atlasxrootd-kit.gridka.de:1094::pnfs:gridka.de:atlas:disk-only:atlasdatadisk:rucio:mc16_13TeV:05:ce'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.10/site-packages/uproot/reading.py\", line 141, in open\r\n    file = ReadOnlyFile(\r\n  File \"/usr/local/lib/python3.10/site-packages/uproot/reading.py\", line 581, in __init__\r\n    self._source = Source(\r\n  File \"/usr/local/lib/python3.10/site-packages/uproot/source/file.py\", line 108, in __init__\r\n    self._open()\r\n  File \"/usr/local/lib/python3.10/site-packages/uproot/source/file.py\", line 118, in _open\r\n    self._fallback = uproot.source.file.MultithreadedFileSource(\r\n  File \"/usr/local/lib/python3.10/site-packages/uproot/source/file.py\", line 250, in __init__\r\n    self._open()\r\n  File \"/usr/local/lib/python3.10/site-packages/uproot/source/file.py\", line 254, in _open\r\n    [FileResource(self._file_path) for x in range(self._num_workers)]\r\n  File \"/usr/local/lib/python3.10/site-packages/uproot/source/file.py\", line 254, in <listcomp>\r\n    [FileResource(self._file_path) for x in range(self._num_workers)]\r\n  File \"/usr/local/lib/python3.10/site-packages/uproot/source/file.py\", line 38, in __init__\r\n    raise uproot._util._file_not_found(file_path) from err\r\nFileNotFoundError: file not found\r\n```\r\n\r\nNote that the error message is showing the filename truncated to 168 characters.\r\n\r\nFor sanity checking purposes, I tried using python's open function:\r\n```python\r\nopen(filename)\r\n<_io.TextIOWrapper name='/servicey/output/7242feb5-db50-46b3-a3f7-9588c2a204ac/scratch/root:::atlasxrootd-kit.gridka.de:1094::pnfs:gridka.de:atlas:disk-only:atlasdatadisk:rucio:mc16_13TeV:05:ce:DAOD_PHYS.30899209._000097.pool.root.1' mode='r' encoding='UTF-8'>\r\n```\r\n\r\nIs there an arbitrary limit on the filename? Can it be longer?\r\n\r\n## Update\r\nOddly enough, this works if the argument is a `Path` instance instead of a string\r\n\r\n```python\r\np = Path(filename)\r\nuproot.open(p)\r\n<ReadOnlyDirectory '/' at 0x7f6efcea5660>\r\n```",
  "closed_at":"2023-07-12T18:56:23Z",
  "comments":1,
  "created_at":"2023-07-12T18:35:53Z",
  "id":1801551945,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5rYYBJ",
  "number":920,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Can't open files with long file names",
  "updated_at":"2023-07-12T18:56:37Z",
  "user":"MDQ6VXNlcjgyMjk4NzU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"## Short version\r\nuproot version: 5.0.10\r\nUsing the keyword `where` in an expression when loading with `library=\"pd\"` fails with\r\n```\r\nAttributeError: 'numpy.ndarray' object has no attribute 'to_frame'\r\n```\r\n(full stacktrace below)\r\n\r\n## longer version\r\nWhen loading a root file, using `where` in the expression fails for pandas (it should be supported [AFAI can see in PythonLanguage](https://github.com/scikit-hep/uproot5/blob/a8644df7a96ee2d788adb1e35f326ba160bedf48/src/uproot/language/python.py#L318). It seems to be only with that expression (I've just noticed that other ternary operators like min, max are not (yet?) supported, so maybe this is the reason?)\r\nI've tried to find the core issue, but didn't succeed.\r\n\r\nCode to reproduce (**creates file in current folder**):\r\n```python\r\n# create tmp file \r\nwith uproot.recreate(\"tmp.root\") as file:\r\n     file['tree'] = {'b1': [1, 5, 9], 'b2': [3, 6, 11]}\r\n\r\n# producing the bug\r\nwith uproot.open(\"tmp.root\") as file:\r\n     tree = file['tree']\r\n\t# different options\r\n     tree.arrays(['log(b1)'], library='pd')  # arbitrary expressions work, like log\r\n     tree.arrays(['where(b1 < b2, b1, b2)'], library='np')  # works with np\r\n     tree.arrays(['where(b1 < b2, b1, b2)'], library='pd')  # fails with pd\r\n     \r\n```\r\n\r\nFull stacktrace:\r\n\r\n<details>\r\n<summary>Details</summary>\r\n\r\n\r\n\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[8], line 3\r\n      1 with uproot.open(\"tmp.root\") as file:\r\n      2     tree = file['tree']\r\n----> 3     print(tree.arrays(['where(b1 < b2, b1, b2)'], library='pd'))\r\n\r\nFile ~/mambaforge/envs/zfit311/lib/python3.11/site-packages/uproot/behaviors/TBranch.py:898, in HasBranches.arrays(self, expressions, cut, filter_name, filter_typename, filter_branch, aliases, language, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library, ak_add_doc, how)\r\n    891 del arrays\r\n    893 expression_context = [\r\n    894     (e, c) for e, c in expression_context if c[\"is_primary\"] and not c[\"is_cut\"]\r\n    895 ]\r\n    897 return _ak_add_doc(\r\n--> 898     library.group(output, expression_context, how), self, ak_add_doc\r\n    899 )\r\n\r\nFile ~/mambaforge/envs/zfit311/lib/python3.11/site-packages/uproot/interpretation/library.py:867, in Pandas.group(self, arrays, expression_context, how)\r\n    865 elif uproot._util.isstr(how) or how is None:\r\n    866     arrays, names = _pandas_only_series(pandas, arrays, expression_context)\r\n--> 867     return _pandas_memory_efficient(pandas, arrays, names)\r\n    869 else:\r\n    870     raise TypeError(\r\n    871         \"for library {}, how must be tuple, list, dict, str (for \"\r\n    872         \"pandas.merge's 'how' parameter, or None (for one or more\"\r\n    873         \"DataFrames without merging)\".format(self.name)\r\n    874     )\r\n\r\nFile ~/mambaforge/envs/zfit311/lib/python3.11/site-packages/uproot/interpretation/library.py:793, in _pandas_memory_efficient(pandas, series, names)\r\n    791 for name in names:\r\n    792     if out is None:\r\n--> 793         out = series[name].to_frame(name=name)\r\n    794     else:\r\n    795         out[name] = series[name]\r\n\r\nAttributeError: 'numpy.ndarray' object has no attribute 'to_frame'\r\n```\r\n\r\n</details>",
  "closed_at":"2023-08-10T15:03:34Z",
  "comments":1,
  "created_at":"2023-07-19T10:00:44Z",
  "id":1811608062,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5r-vH-",
  "number":922,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"\"where\" in expression fails with library \"pandas\"",
  "updated_at":"2023-08-17T08:44:38Z",
  "user":"MDQ6VXNlcjE3NDU0ODQ4"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"In https://root.cern/doc/v628/release-notes.html#faster-reading-from-eos ROOT has used a local file's xattr to determine if an xrootd endpoint can provide the file. This is generally preferred over FUSE mounts. Since this is a bit of a niche use case, it seems best to keep it in uproot rather than lower-level libraries such as fsspec that would otherwise not want to waste time probing xattr when most of their customers do not use EOS. In python, I can find both `xattr` and `pyxattr` packages to query these, but I am not sure if we want to take that dependency or not.\r\ncc @sciaba",
  "closed_at":null,
  "comments":0,
  "created_at":"2023-07-21T14:16:19Z",
  "id":1815888950,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5sPEQ2",
  "number":923,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Automatically switch from FUSE mount to xrootd for EOS sites",
  "updated_at":"2023-07-21T14:16:19Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2023-08-08T14:04:44Z",
  "comments":4,
  "created_at":"2023-08-07T15:50:55Z",
  "draft":false,
  "id":1839766549,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5XWha3",
  "number":927,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-08T14:04:44Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: don't assume Uproot is in global scope in TPython::Eval",
  "updated_at":"2023-08-21T15:57:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"NONE",
  "body":"Due to some recent changes in RNTuple's format, uproot is not able to read RNTuples correctly anymore.\r\n\r\nA reproducer:\r\n\r\n```\r\npython -c 'import uproot; print(uproot.__version__); uproot.open(\"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoaod-rntuple/zstd/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root\")[\"Events\"].arrays([\"nTau\"])\r\n```\r\nresults in\r\n\r\n```\r\n5.0.10\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/blue/Repos/analysis-grand-challenge-iris-hep/analyses/cms-open-data-ttbar/venv-uproot5/lib/python3.11/site-packages/uproot/models/RNTuple.py\", line 394, in arrays\r\n    entry_stop = entry_stop or self._length\r\n                               ^^^^^^^^^^^^\r\n  File \"/home/blue/Repos/analysis-grand-challenge-iris-hep/analyses/cms-open-data-ttbar/venv-uproot5/lib/python3.11/site-packages/uproot/models/RNTuple.py\", line 180, in _length\r\n    return sum(x.num_entries for x in self.cluster_summaries)\r\n                                      ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/blue/Repos/analysis-grand-challenge-iris-hep/analyses/cms-open-data-ttbar/venv-uproot5/lib/python3.11/site-packages/uproot/models/RNTuple.py\", line 175, in cluster_summaries\r\n    return self.footer.cluster_summaries\r\n           ^^^^^^^^^^^\r\n  File \"/home/blue/Repos/analysis-grand-challenge-iris-hep/analyses/cms-open-data-ttbar/venv-uproot5/lib/python3.11/site-packages/uproot/models/RNTuple.py\", line 164, in footer\r\n    f = FooterReader().read(self._footer_chunk, cursor, context)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/blue/Repos/analysis-grand-challenge-iris-hep/analyses/cms-open-data-ttbar/venv-uproot5/lib/python3.11/site-packages/uproot/models/RNTuple.py\", line 697, in read\r\n    out.extension_links = self.extension_header_links.read(chunk, cursor, context)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/blue/Repos/analysis-grand-challenge-iris-hep/analyses/cms-open-data-ttbar/venv-uproot5/lib/python3.11/site-packages/uproot/models/RNTuple.py\", line 556, in read\r\n    assert num_bytes < 0, f\"num_bytes={num_bytes}\"\r\n           ^^^^^^^^^^^^^\r\nAssertionError: num_bytes=36\r\n```\r\n\r\nIIUC the Julia implementation already implemented the necessary changes at https://github.com/JuliaHEP/UnROOT.jl/pull/264/files \r\n\r\nEDIT:\r\nmaybe I should have marked it as a bug?",
  "closed_at":"2024-01-18T16:15:00Z",
  "comments":2,
  "created_at":"2023-08-09T15:20:17Z",
  "id":1843499960,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5t4ZO4",
  "number":928,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Add support for current RNTuple files",
  "updated_at":"2024-01-18T16:15:00Z",
  "user":"MDQ6VXNlcjEwOTk5MDM0"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Committed via https://github.com/asottile/all-repos",
  "closed_at":"2023-08-10T11:52:04Z",
  "comments":0,
  "created_at":"2023-08-10T02:25:39Z",
  "draft":false,
  "id":1844299586,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Xl6E6",
  "number":929,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-10T11:52:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: use 2x faster black mirror",
  "updated_at":"2023-08-10T11:52:05Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR fixes #922 \r\n\r\nThe issue was caused by an assumption that in the code path that series are made of other series which are then to be transformed in dataframes. ",
  "closed_at":"2023-08-10T15:03:30Z",
  "comments":0,
  "created_at":"2023-08-10T14:15:43Z",
  "draft":false,
  "id":1845283069,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5XpQnP",
  "number":930,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-10T15:03:30Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: expressions failing in pandas issue 922",
  "updated_at":"2023-08-10T15:03:31Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"An issue was introduced in between uproot 4.1.1 and 4.1.5, where ROOT .Map() shows errors for any TTrees with TBaskets (where the TBaskets can be empty or not). \r\n\r\n\r\n```python\r\n>>> import uproot\r\n>>> import ROOT\r\n>>> import numpy as np\r\n>>> uproot.__version__\r\n5.0.10\r\n>>> file = uproot.recreate(\"simple_reproducer.root\")\r\n>>> file[\"tree\"] = {\"some_array\": np.array([1,2,3])}\r\n>>> rf = ROOT.TFile(\"simple_reproducer.root\")\r\n>>> rf.Map()\r\n20230810/162540  At:100     N=142       TFile                     \r\n20230810/162557  At:242     N=101       TBasket                   \r\nAddress = 343\tNbytes = 0\t=====E R R O R=======\r\n0/000000  At:343     N=1         END           \r\n\r\n```\r\n",
  "closed_at":null,
  "comments":1,
  "created_at":"2023-08-10T14:34:13Z",
  "id":1845316181,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5t_UpV",
  "number":931,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"ROOT Map() showing errors for any written TTrees with TBaskets",
  "updated_at":"2024-01-30T16:13:07Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"The test introduced in #930 depends on pandas.\r\nLike other tests, it should be skipped if pandas cannot be imported.",
  "closed_at":"2023-08-14T13:10:49Z",
  "comments":3,
  "created_at":"2023-08-12T06:10:25Z",
  "draft":false,
  "id":1847788871,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Xx0Sw",
  "number":934,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-14T13:10:49Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: skip pandas test if pandas is not installed",
  "updated_at":"2023-08-14T14:48:11Z",
  "user":"MDQ6VXNlcjI1MDgzNzkw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"I noticed that https://uproot.readthedocs.io/en/latest/changelog.html currently doesn\u2019t list any releases past `5.0.0rc2`; all PRs in later releases are listed under \u201cUnreleased\u201d. This is due to a change in the tag schema: Tags for all releases from `v5.0.0rc3` onward start with `v`, while tags for previous releases didn\u2019t. This PR fixes the regex to accept both styles of tags.\r\n\r\nThis PR also updates the repo URL, since the old uproot4 repo URL now returns a redirect message instead of the expected releases:\r\n```Python\r\n>>> github_connection = http.client.HTTPSConnection(\"api.github.com\")\r\n>>> github_connection.request(\r\n        \"GET\",\r\n        rf\"/repos/scikit-hep/uproot4/releases?page={pageid + 1}&per_page=30\",\r\n        headers={\"User-Agent\": \"uproot4-changelog\"},\r\n    )\r\n>>> github_connection.getresponse().read()\r\nb'{\"message\":\"Moved Permanently\",\"url\":\"https://api.github.com/repositories/262422450/releases?page=3&per_page=30\",\"documentation_url\":\"https://docs.github.com/v3/#http-redirects\"}'\r\n```\r\n\r\n---\r\n\r\nNote: Releases in the v4.3.x series are still ignored by this script, since they are on the separate `main-v4` branch, not on `main`. Do you think it\u2019s worth adding something like\r\n```Python\r\noutfile.write(\"Note: Releases in the 4.3.x series were developed in parallel with v5.0 on a separate branch and are not included here. See https://github.com/scikit-hep/uproot5/releases for details on those releases.\\n\")\r\n```\r\nin line 78, after the \u201cRelease History\u201d header, to avoid confusion?",
  "closed_at":"2023-08-14T18:48:48Z",
  "comments":6,
  "created_at":"2023-08-13T00:33:27Z",
  "draft":false,
  "id":1848319867,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5XzmYe",
  "number":935,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-14T18:48:48Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: changelog script ignores releases past 5.0.0rc2",
  "updated_at":"2023-08-14T18:48:48Z",
  "user":"MDQ6VXNlcjE2MTg5NzQ3"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"I would like to read data from a class (`TRestAnalysisTree`) that inherits from the standard `TTree`.\r\n\r\nCurrently this does not work but uproot correctly reads the object as a tree so perhaps it can also read the branch contents with a few changes.\r\n\r\nUsing latest uproot (5.0.11).\r\n\r\n```\r\nimport uproot\r\n\r\nfile = uproot.open(\"simulation.analysis.root\")\r\n\r\nanalysis_tree = file[\"AnalysisTree\"] # is not a TTree but inherits from one\r\nprint(analysis_tree.keys()) # works fine\r\nevent_tree = file[\"EventTree\"] # event_tree is a TTree\r\nprint(analysis_tree) # <TTree 'AnalysisTree' (72 branches) at 0x02e3ec6540d0>\r\nprint(event_tree) # <TTree 'EventTree' (4 branches) at 0x02e39dbc5c90>\r\n\r\nanalysis_branch = analysis_tree[\"g4Ana_yOriginPrimary\"]\r\nanalysis_branch.array() # ERROR\r\n```\r\n\r\nThe error:\r\n\r\n```\r\n\r\ntests\\test_derived_tree.py:1 (test_derived_tree)\r\ndef test_derived_tree():\r\n        import uproot\r\n    \r\n        input_file = \"simulation.anaylsis.root\"\r\n    \r\n        file = uproot.open(input_file)\r\n    \r\n        analysis_tree = file[\"AnalysisTree\"]\r\n        event_tree = file[\"EventTree\"]\r\n    \r\n        print(analysis_tree.keys())\r\n        print(event_tree.keys())\r\n    \r\n        print(event_tree[\"TRestGeant4EventBranch/TRestEvent/TObject/fUniqueID\"].array(library=\"ak\"))\r\n    \r\n        analysis_branch = analysis_tree[\"g4Ana_yOriginPrimary\"]\r\n        event_branch = event_tree[\"TRestGeant4EventBranch/TRestEvent/fOk\"]\r\n    \r\n        #analysis_branch._file = event_branch._file\r\n        #analysis_branch._parent = event_branch._parent\r\n    \r\n    \r\n>       analysis_tree[\"g4Ana_yOriginPrimary\"].array()\r\n\r\nanalysis_branch = <TBranch 'g4Ana_yOriginPrimary' at 0x022534bbf0d0>\r\nanalysis_tree = <TTree 'AnalysisTree' (72 branches) at 0x022534ba61d0>\r\nevent_branch = <TBranchElement 'fOk' at 0x022534c8d990>\r\nevent_tree = <TTree 'EventTree' (4 branches) at 0x022534c29c10>\r\nfile       = <ReadOnlyDirectory '/' at 0x022534212c10>\r\ninput_file = 'simulation.anaylsis.root'\r\nuproot     = <module 'uproot' from 'C:\\\\Users\\\\lobis\\\\git\\\\uproot\\\\src\\\\uproot\\\\__init__.py'>\r\n\r\ntest_derived_tree.py:24: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n..\\src\\uproot\\behaviors\\TBranch.py:1768: in array\r\n    decompression_executor, interpretation_executor = _regularize_executors(\r\n        ak_add_doc = False\r\n        array_cache = 'inherit'\r\n        decompression_executor = None\r\n        entry_start = 0\r\n        entry_stop = 20\r\n        interpretation = AsDtype('>f8')\r\n        interpretation_executor = None\r\n        library    = 'ak'\r\n        self       = <TBranch 'g4Ana_yOriginPrimary' at 0x022534bbf0d0>\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ndecompression_executor = None, interpretation_executor = None\r\nfile = <uproot.reading.DetachedFile object at 0x000002253421D650>\r\n\r\n    def _regularize_executors(decompression_executor, interpretation_executor, file):\r\n        if file is None:\r\n            if decompression_executor is None:\r\n                decompression_executor = uproot.source.futures.TrivialExecutor()\r\n            if interpretation_executor is None:\r\n                interpretation_executor = uproot.source.futures.TrivialExecutor()\r\n        else:\r\n            if decompression_executor is None:\r\n>               decompression_executor = file.decompression_executor\r\nE               AttributeError: 'DetachedFile' object has no attribute 'decompression_executor'\r\n\r\ndecompression_executor = None\r\nfile       = <uproot.reading.DetachedFile object at 0x000002253421D650>\r\ninterpretation_executor = None\r\n\r\n..\\src\\uproot\\behaviors\\TBranch.py:2656: AttributeError\r\n```\r\n\r\nI think this come from the reading of the trees as there are some differences, for example:\r\n```\r\nprint(event_tree._file) # <ReadOnlyFile 'simulation.anaylsis.root' at 0x02e3ffa25fd0>\r\nprint(analysis_tree._file) # <uproot.reading.DetachedFile object at 0x000002E3EC656A10>\r\n```\r\n\r\nUpdating some of these private attributes (e.g. `_file`) fixes some of the errors but I could not get it working.\r\n\r\nSample root file (zip): [simulation.anaylsis.zip](https://github.com/scikit-hep/uproot5/files/12334414/simulation.anaylsis.zip)\r\n",
  "closed_at":null,
  "comments":0,
  "created_at":"2023-08-14T12:25:14Z",
  "id":1849675842,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5uP9BC",
  "number":936,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Reading from branches of extended TTree",
  "updated_at":"2024-01-30T16:02:36Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @natsukium as a contributor for test.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot5/pull/934#issuecomment-1677275776)\n\n[skip ci]",
  "closed_at":"2023-08-14T13:03:27Z",
  "comments":0,
  "created_at":"2023-08-14T13:02:55Z",
  "draft":false,
  "id":1849736720,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5X4XVf",
  "number":937,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-14T13:03:27Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add natsukium as a contributor for test",
  "updated_at":"2023-08-14T13:03:28Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @JostMigenda as a contributor for doc.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot5/pull/935#issuecomment-1677658055)\n\n[skip ci]",
  "closed_at":"2023-08-14T16:25:35Z",
  "comments":0,
  "created_at":"2023-08-14T16:25:16Z",
  "draft":false,
  "id":1850129386,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5X5qR0",
  "number":938,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-14T16:25:35Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add JostMigenda as a contributor for doc",
  "updated_at":"2023-08-14T16:25:36Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/astral-sh/ruff-pre-commit: v0.0.282 \u2192 v0.0.284](https://github.com/astral-sh/ruff-pre-commit/compare/v0.0.282...v0.0.284)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-08-17T13:12:50Z",
  "comments":0,
  "created_at":"2023-08-15T03:27:24Z",
  "draft":false,
  "id":1850852885,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5X8H9U",
  "number":939,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-17T13:12:50Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-08-17T13:12:51Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-09-25T16:00:03Z",
  "comments":1,
  "created_at":"2023-08-16T21:22:11Z",
  "draft":false,
  "id":1853923119,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5YGd73",
  "number":940,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-09-25T16:00:03Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add TLeafC - string - writing support",
  "updated_at":"2023-09-25T16:00:04Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Fixes case where the number of entries is less than the entry start in Dask.",
  "closed_at":"2023-08-17T14:31:31Z",
  "comments":0,
  "created_at":"2023-08-17T07:16:46Z",
  "draft":false,
  "id":1854410580,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5YIFeo",
  "number":941,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-17T14:31:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: clamp start and stop",
  "updated_at":"2023-08-17T14:31:32Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"It wasn't testing `library=\"ak\"` before. Also, it's a small enough file that we can test the whole thing.",
  "closed_at":"2023-08-17T14:43:34Z",
  "comments":0,
  "created_at":"2023-08-17T14:30:06Z",
  "draft":false,
  "id":1855126972,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5YKizC",
  "number":942,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-17T14:43:34Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: better test for vectorVectorDouble",
  "updated_at":"2023-08-17T14:43:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"tests/test_0637-setup-tests-for-AwkwardForth.py:\r\n\r\n- [x] test_00\r\n- [x] test_01\r\n- [x] test_02\r\n- [x] test_03\r\n- [x] test_04\r\n- [x] test_05\r\n- [x] test_06\r\n- [x] test_07\r\n- [x] test_08\r\n- [x] test_09\r\n- [x] test_10\r\n- [x] test_11\r\n- [x] test_12\r\n- [x] test_13\r\n- [x] test_14\r\n- [x] test_15\r\n- [x] test_16\r\n- [x] test_17\r\n- [x] test_18\r\n- [x] test_19\r\n- [x] test_20\r\n- [x] test_21\r\n- [x] test_22\r\n- [x] test_23\r\n- [x] test_24\r\n- [x] test_25\r\n- [x] test_27\r\n- [x] test_28\r\n- [x] test_30\r\n- [x] test_31\r\n- [x] test_32\r\n- [x] test_33\r\n- [x] test_34\r\n- [x] test_35\r\n- [x] test_36\r\n- [x] test_37\r\n- [x] test_38\r\n- [x] test_39\r\n- [x] test_40\r\n- [x] test_41\r\n- [x] test_42\r\n- [x] test_43\r\n- [x] test_44\r\n- [x] test_47\r\n- [x] test_51\r\n- [x] test_52\r\n- [x] test_53\r\n- [x] test_54\r\n- [x] test_55\r\n- [x] test_56\r\n- [x] test_57\r\n- [x] test_58\r\n- [x] test_59\r\n- [x] test_60\r\n- [x] test_61\r\n- [x] test_62\r\n- [x] test_63\r\n- [x] test_64\r\n- [x] test_65\r\n- [x] test_66\r\n- [x] test_67\r\n- [x] test_68\r\n- [x] test_69\r\n- [x] test_70\r\n- [x] test_71\r\n- [x] test_72\r\n- [x] test_73\r\n- [x] test_74\r\n- [x] test_75\r\n- [x] test_76\r\n- [x] test_77\r\n- [x] test_78\r\n- [x] test_79\r\n- [x] test_80\r\n- [x] test_81\r\n\r\ntests/test_0798_DAOD_PHYSLITE.py\r\n\r\n- [x] test_AnalysisJetsAuxDyn_GhostTrack\r\n- [x] test_TruthBosonAuxDyn_childLinks\r\n- [x] test_TruthPhotonsAuxDyn_parentLinks\r\n- [x] test_TruthTopAuxDyn_parentLinks\r\n",
  "closed_at":"2023-12-05T22:33:14Z",
  "comments":11,
  "created_at":"2023-08-17T14:45:47Z",
  "draft":false,
  "id":1855155480,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5YKpB2",
  "number":943,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-05T22:33:14Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: refactoring the AwkwardForth code-discovery process",
  "updated_at":"2023-12-12T22:36:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/astral-sh/ruff-pre-commit: v0.0.284 \u2192 v0.0.285](https://github.com/astral-sh/ruff-pre-commit/compare/v0.0.284...v0.0.285)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-08-24T13:30:07Z",
  "comments":0,
  "created_at":"2023-08-22T03:32:45Z",
  "draft":false,
  "id":1860483995,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5YcjUR",
  "number":944,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-24T13:30:07Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-08-24T13:30:08Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"It seems the `awkward-pandas` and the `dask-awkward` packages are now present on the conda-forge.\r\n\r\nMaybe one could change the message (in the [`src/uproot/extras.py`](https://github.com/scikit-hep/uproot5/blob/main/src/uproot/extras.py#L342) file):\r\n\r\n`pip install awkward-pandas # not on conda-forge yet`\r\n\r\ninto something like:\r\n\r\n`conda install awkward-pandas`\r\n\r\nAnd similar messages (in **two** files: [`src/uproot/extras.py`](https://github.com/scikit-hep/uproot5/blob/main/src/uproot/extras.py#L327) and [`src/uproot/_dask.py`](https://github.com/scikit-hep/uproot5/blob/main/src/uproot/_dask.py#L109)) could also be changed from:\r\n\r\n`pip install dask-awkward   # not on conda-forge yet`\r\n\r\ninto something like:\r\n\r\n`conda install dask-awkward`\r\n\r\nJust for the \"reference\": [TTree details -> Getting data into NumPy or Pandas ->missing awkward-pandas](https://github.com/hsf-training/hsf-training-scikit-hep-webpage/issues/49#issuecomment-1688214818)",
  "closed_at":"2023-08-23T13:16:09Z",
  "comments":1,
  "created_at":"2023-08-23T09:24:20Z",
  "id":1862928064,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5vCgbA",
  "number":946,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"awkward-pandas and dask-awkward are on conda-forge (no need to pip them)",
  "updated_at":"2023-08-23T13:16:09Z",
  "user":"MDQ6VXNlcjE1MTM5MDA1"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"dask-awkward and awkward-pandas are both on conda now.\r\n\r\nAlso consistently added `-c conda-forge` to the packages that are only on conda-forge (though users would have a problem if they mix conda channels, but we can't force conda channel hygene on them).\r\n\r\nRemoved the text from the `uproot.dask` docstring about installing Dask, just to ensure that the message is in only one place (error message when you attempt to use a package that isn't there), so that it can be updated in one place. That's the whole reason for having an extras.py, after all.",
  "closed_at":"2023-08-23T13:16:08Z",
  "comments":0,
  "created_at":"2023-08-23T12:19:52Z",
  "draft":false,
  "id":1863222500,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Yl5cX",
  "number":947,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-08-23T13:16:08Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: get package import messages up-to-date",
  "updated_at":"2023-08-23T13:16:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"### Discussed in https://github.com/scikit-hep/uproot5/discussions/948\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **JacekHoleczek** August 24, 2023</sup>\r\nI need to \"move\" (to here) my original StackOverflow topic: [How can I \"dump\" / get the content of objects of \"experiment specific\" / \"custom\" classes (from a TTree)](https://stackoverflow.com/questions/76938890/how-can-i-dump-get-the-content-of-objects-of-experiment-specific-custom)\r\nApparently, they do not allow any discussions there (they simply deleted my follow-up).\r\n\r\nMany thanks for your reply on the StackOverflow.\r\n\r\nThe standard ROOT provides the [`TFile::MakeProject`](https://root.cern/doc/master/classTFile.html#a5fdd58dba517dd7b70b43332295e529d) method, which can generate \"source code\" for classes present in the file (only \"data members\" are \"recovered\" from the \"streamer info\", of course).\r\nI understand `uproot` does not provide such functionality (i.e., I cannot get \"automatically generated\" Python classes).\r\n\r\nTo start, I'd now like to play with a small \"`header`\" tree with just a single branch:\r\n\r\n```text\r\nPOTInfo_v2                      | Header                               | AsObjects(Model_Header)\r\n```\r\n\r\nThe `root_directory.file.show_streamers('Header')` returns:\r\n\r\n```text\r\nHeader (v1)\r\n    _POT_CountedPerFile: double (TStreamerBasicType)\r\n    _POT_NoCut: double (TStreamerBasicType)\r\n    _POT_BadBeam: double (TStreamerBasicType)\r\n    _POT_BadND280: double (TStreamerBasicType)\r\n    _POT_GoodBeamGoodND280: double (TStreamerBasicType)\r\n    _POT_0KA: double (TStreamerBasicType)\r\n    _POT_200KA: double (TStreamerBasicType)\r\n    _POT_250KA: double (TStreamerBasicType)\r\n    _POT_m250KA: double (TStreamerBasicType)\r\n    _POT_OtherKA: double (TStreamerBasicType)\r\n    _POTInfo: double (TStreamerBasicType)\r\n    _Spill_NoCut: int (TStreamerBasicType)\r\n    _Spill_BadBeam: int (TStreamerBasicType)\r\n    _Spill_BadND280: int (TStreamerBasicType)\r\n    _Spill_GoodBeamGoodND280: int (TStreamerBasicType)\r\n    _SpillInfo: int (TStreamerBasicType)\r\n    _IsMC: bool (TStreamerBasicType)\r\n    _SoftwareVersion: string (TStreamerSTLstring)\r\n    _pot_def_counter: int (TStreamerBasicType)\r\n    _spill_def_counter: int (TStreamerBasicType)\r\n```\r\n\r\nSo, there are just some `double`, some `int`, one `bool`, and one `std::string` ordinary variables (no arrays at all).\r\n\r\nWhen I try to retrieve just the first entry:\r\n\r\n```python\r\nroot_directory['header']['POTInfo_v2'].array(entry_stop=1)\r\n```\r\n\r\nI get (note: it now thinks `_POTInfo` and `_SpillInfo` are arrays):\r\n\r\n```text\r\n(...)\r\nDeserializationError: while reading\r\n\r\n    Header version 6792 as uproot.dynamic.Model_Header_v1 (71320410 bytes)\r\n        _POT_CountedPerFile: 2.6612249000050942e-110\r\n        _POT_NoCut: 0.0\r\n        _POT_BadBeam: 0.0\r\n        _POT_BadND280: 0.0\r\n        _POT_GoodBeamGoodND280: 0.0\r\n        _POT_0KA: 0.0\r\n        _POT_200KA: 0.0\r\n        _POT_250KA: 0.0\r\n        _POT_m250KA: 0.0\r\n        _POT_OtherKA: 3.7076069415964e-310\r\n        _POTInfo: array([7.67844769e-239, 0.00000000e+000, 0.00000000e+000, 3.70760694e-310,\r\n       7.67844769e-239, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\r\n       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\r\n       0.00000000e+000], dtype='>f8')\r\n        _Spill_NoCut: 0\r\n        _Spill_BadBeam: 0\r\n        _Spill_BadND280: 96\r\n        _Spill_GoodBeamGoodND280: -1051394048\r\n        _SpillInfo: array([          0,           0,          96, -1051394048,         320,\r\n              2304], dtype='>i4')\r\n        _IsMC: True\r\nMembers for Header: (_POT_CountedPerFile), (_POT_NoCut), (_POT_BadBeam), (_POT_BadND280), (_POT_GoodBeamGoodND280), (_POT_0KA), (_POT_200KA), (_POT_250KA), (_POT_m250KA), (_POT_OtherKA), (_POTInfo), (_Spill_NoCut), (_Spill_BadBeam), (_Spill_BadND280), (_Spill_GoodBeamGoodND280), (_SpillInfo), (_IsMC), _SoftwareVersion, _pot_def_counter, _spill_def_counter\r\n\r\nattempting to get bytes 234:283\r\noutside expected range 0:246 for this Chunk\r\nin file NumuCCMultiPi_MCrun4.root\r\nin object /header;1\r\n```\r\n\r\nIt seems I'm missing something again (adding \"`library='ak'`\", \"`library='np'`\", or \"`library='pd'`\" does not help).</div>\r\n\r\nThere's nothing exotic about this class definition, so Uproot should be able to read it. I've labeled it \"bug (unverified)\" as a reminder to try to reproduce it and fix whatever the problem is.",
  "closed_at":null,
  "comments":0,
  "created_at":"2023-08-24T17:05:33Z",
  "id":1865561330,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5vMjTy",
  "number":949,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"How can I \"dump\" / get the content of objects of \"experiment specific\" / \"custom\" classes (from a TTree)",
  "updated_at":"2023-08-24T17:05:33Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/astral-sh/ruff-pre-commit: v0.0.285 \u2192 v0.0.287](https://github.com/astral-sh/ruff-pre-commit/compare/v0.0.285...v0.0.287)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-09-07T13:12:41Z",
  "comments":0,
  "created_at":"2023-08-29T03:06:46Z",
  "draft":false,
  "id":1870801599,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5Y_V_Z",
  "number":950,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-09-07T13:12:41Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-09-07T13:12:42Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"I found some more issues with ElementLink branches in newer DAOD_PHYSLITE files.\r\n\r\nAn example file can be found in [DAOD_PHYSLITE_24.0.2.art.pool.root.zip](https://github.com/scikit-hep/uproot5/files/12497262/DAOD_PHYSLITE_24.0.2.art.pool.root.zip)\r\n(produced with [produce_mc21_physlite_testfile_24.0.2_2023-09-01.sh](https://github.com/nikoladze/physlite_experiments/blob/bfd620f1c260cbf2eafecc0c625b6d32a61ab33b/physlite_experiments/scripts/produce_mc21_physlite_testfile_24.0.2_2023-09-01.sh))\r\n\r\n```python\r\n>>> import uproot\r\n>>> import awkward as ak\r\n>>> uproot.__version__\r\n'5.0.11'\r\n>>> ak.__version__\r\n'2.3.3'\r\n```\r\n\r\nTrying to read all ElementLink branches:\r\n\r\n```python\r\nimport uproot\r\n\r\ndef try_read_elementlinks(tree, use_forth=True):\r\n    for key, branch in tree.iteritems(filter_typename=\"*ElementLink*\"):\r\n        try:\r\n            if use_forth:\r\n                branch.interpretation._forth = True\r\n            else:\r\n                branch.interpretation._forth = False\r\n            branch.array()\r\n        except Exception as e:\r\n            print(f\"Problem with {key}:\\n{repr(e)}\\n\")\r\n\r\nwith uproot.open(\"DAOD_PHYSLITE_24.0.2.art.pool.root:CollectionTree\") as tree:\r\n    try_read_elementlinks(tree)\r\n```\r\n\r\ngives us\r\n\r\n```\r\nProblem with METAssoc_AnalysisMETAux./METAssoc_AnalysisMETAux.jetLink:\r\nValueError('basket 0 in tree/branch /CollectionTree;1:METAssoc_AnalysisMETAux./METAssoc_AnalysisMETAux.jetLink has the wrong number of bytes (4340) for interpretation AsStridedObjects(Model_ElementLink_3c_DataVector_3c_xAOD_3a3a_Jet_5f_v1_3e3e__v1)\\nin file DAOD_PHYSLITE_24.0.2.art.pool.root')\r\n\r\nProblem with EventInfoAuxDyn.hardScatterVertexLink:\r\n<UnknownInterpretation 'none of the rules matched'>\r\n\r\nProblem with TruthMuonsAuxDyn.parentLinks:\r\nTypeError('Awkward Array does not support arrays with object dtypes.')\r\n\r\nProblem with TruthTausAuxDyn.parentLinks:\r\nTypeError('Awkward Array does not support arrays with object dtypes.')\r\n\r\nProblem with TruthTausAuxDyn.childLinks:\r\nTypeError('Awkward Array does not support arrays with object dtypes.')\r\n\r\nProblem with GSFConversionVerticesAuxDyn.trackParticleLinks:\r\nTypeError('Awkward Array does not support arrays with object dtypes.')\r\n```\r\n\r\nPlaying a bit with this i believe there are 3 different issues - let's call them The Good, The Bad and The Ugly\r\n\r\n# The Good\r\n`METAssoc_AnalysisMETAux.jetLink` seems to fail because it is stored memberwise. Memberwise seems on by default for ATLAS and (normal) splitting for this branch has been deactivated due to some issues it caused in existing code. So, since this is a single-jagged vector we now seem to have this memberwise:\r\n```python\r\n>>> tree = uproot.open(\"DAOD_PHYSLITE_24.0.2.art.pool.root:CollectionTree\")\r\n>>> tree[\"METAssoc_AnalysisMETAux.jetLink\"].interpretation.typename\r\n'std::vector<ElementLink<DataVector<xAOD::Jet_v1>>>'\r\n>>> tree[\"METAssoc_AnalysisMETAux.jetLink\"].debug(0)\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64   0   0  92  64   9   0   0 167   6 112  11   0   0   0  10  26 253  25  25\r\n  @ --- ---   \\   @ --- --- --- --- ---   p --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 26 253  25  25  26 253  25  25  26 253  25  25  26 253  25  25  26 253  25  25\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 26 253  25  25  26 253  25  25  26 253  25  25   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   1   0   0   0   2   0   0   0   3   0   0   0   4   0   0   0   5\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   6   0   0   0   7   0   0   0   8 255 255 255 255\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n```\r\nIt seems this has 16 header bytes and then come all numbers for the `m_persKey`, followed by all numbers for the `m_persIndex`. If we wanted to put in a workaround for this we could abuse the fact that both these members are `int32` and read it as a single list\r\n```\r\n>>> tree[\"METAssoc_AnalysisMETAux.jetLink\"].debug(0, skip_bytes=16, dtype=\">i4\")\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 26 253  25  25  26 253  25  25  26 253  25  25  26 253  25  25  26 253  25  25\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n      452794649       452794649       452794649       452794649       452794649\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 26 253  25  25  26 253  25  25  26 253  25  25  26 253  25  25   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n      452794649       452794649       452794649       452794649               0\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   1   0   0   0   2   0   0   0   3   0   0   0   4\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n              0               1               2               3               4\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   5   0   0   0   6   0   0   0   7   0   0   0   8 255 255 255 255\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n              5               6               7               8              -1\r\n>>> interpretation = uproot.interpretation.jagged.AsJagged(uproot.interpretation.numerical.AsDtype(\">i4\"), header_bytes=16)\r\n>>> array = tree[\"METAssoc_AnalysisMETAux.jetLink\"].array(interpretation)\r\n```\r\n... and then do some tricks to split each list in half and rezip it into the two record fields.\r\n\r\n**But** it may be that it's actually a bug that this branch is not split (and splitting would cause it to be *not* memberwise). The discussion in [athena!61982](https://gitlab.cern.ch/atlas/athena/-/merge_requests/61982) seemed to have concluded that we can actually split these branches in newer ROOT versions, so i'll have to follow this up with the experts.\r\n\r\n# The Bad\r\nFor `TruthMuonsAuxDyn.parentLinks`, `TruthTausAuxDyn.parentLinks`, `TruthTausAuxDyn.childLinks` and `GSFConversionVerticesAuxDyn.trackParticleLinks` there seems to be a problem only in AwkwardForth mode\r\n\r\nWith\r\n```python\r\nwith uproot.open(\"DAOD_PHYSLITE_24.0.2.art.pool.root:CollectionTree\") as tree:\r\n    try_read_elementlinks(tree, use_forth=False)\r\n```\r\nThey don't show up anymore as having a problem.\r\n\r\nThe Exception that is thrown in forth-mode always complains about object dtypes in awkward array and it seems in AwkwardForth mode (at least some) `STLVector` instances are produced.\r\n\r\nThe reason why i put all the previous examples in `with uproot.open(...)` blocks is that trying the second time (no matter if forth was used the first time or not) it actually works for these branches:\r\n\r\n```python\r\nwith uproot.open(\"DAOD_PHYSLITE_24.0.2.art.pool.root:CollectionTree\") as tree:\r\n    print(\"First Try\")\r\n    print(\"=========\")\r\n    try_read_elementlinks(tree)\r\n    print(\"Second Try\")\r\n    print(\"==========\")\r\n    try_read_elementlinks(tree)\r\n```\r\n```\r\nFirst Try\r\n=========\r\nProblem with METAssoc_AnalysisMETAux./METAssoc_AnalysisMETAux.jetLink:\r\nValueError('basket 0 in tree/branch /CollectionTree;1:METAssoc_AnalysisMETAux./METAssoc_AnalysisMETAux.jetLink has the wrong number of bytes (4340) for interpretation AsStridedObjects(Model_ElementLink_3c_DataVector_3c_xAOD_3a3a_Jet_5f_v1_3e3e__v1)\\nin file DAOD_PHYSLITE_24.0.2.art.pool.root')\r\n\r\nProblem with EventInfoAuxDyn.hardScatterVertexLink:\r\n<UnknownInterpretation 'none of the rules matched'>\r\n\r\nProblem with TruthMuonsAuxDyn.parentLinks:\r\nTypeError('Awkward Array does not support arrays with object dtypes.')\r\n\r\nProblem with TruthTausAuxDyn.parentLinks:\r\nTypeError('Awkward Array does not support arrays with object dtypes.')\r\n\r\nProblem with TruthTausAuxDyn.childLinks:\r\nTypeError('Awkward Array does not support arrays with object dtypes.')\r\n\r\nProblem with GSFConversionVerticesAuxDyn.trackParticleLinks:\r\nTypeError('Awkward Array does not support arrays with object dtypes.')\r\n\r\nSecond Try\r\n==========\r\nProblem with METAssoc_AnalysisMETAux./METAssoc_AnalysisMETAux.jetLink:\r\nValueError('basket 0 in tree/branch /CollectionTree;1:METAssoc_AnalysisMETAux./METAssoc_AnalysisMETAux.jetLink has the wrong number of bytes (4340) for interpretation AsStridedObjects(Model_ElementLink_3c_DataVector_3c_xAOD_3a3a_Jet_5f_v1_3e3e__v1)\\nin file DAOD_PHYSLITE_24.0.2.art.pool.root')\r\n\r\nProblem with EventInfoAuxDyn.hardScatterVertexLink:\r\n<UnknownInterpretation 'none of the rules matched'>\r\n```\r\n\r\n# The Ugly\r\n`EventInfoAuxDyn.hardScatterVertexLink` is weird. It seems to have a sub branch with the same name\r\n```\r\n>>> tree = uproot.open(\"DAOD_PHYSLITE_24.0.2.art.pool.root:CollectionTree\")\r\n>>> tree[\"EventInfoAuxDyn.hardScatterVertexLink\"].keys()\r\n['EventInfoAuxDyn.hardScatterVertexLink']\r\n```\r\nFor which uproot doesn't know how to interpret it\r\n```\r\n>>> tree[\"EventInfoAuxDyn.hardScatterVertexLink/EventInfoAuxDyn.hardScatterVertexLink\"].interpretation\r\n<UnknownInterpretation 'none of the rules matched'>\r\n```\r\n`TTree.show` shows this is an ElementLink, this time not in a vector (makes sense, probably supposed to point to *the* hard scatter vertex of the event). We can see the key and index by skipping 10 bytes:\r\n```\r\n>>> tree[\"EventInfoAuxDyn.hardScatterVertexLink/EventInfoAuxDyn.hardScatterVertexLink\"].debug(0, skip_bytes=10, dtype=\">i4\")\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 55 209  69 151   0   0   0   0\r\n  7 ---   E --- --- --- --- ---\r\n      936461719               0\r\n```\r\n\r\nSorry for the long text - just wanted to write down what i found so far (some things i found while writing this, so writing it down was already helpful :slightly_smiling_face: )\r\n\r\nI guess for all these issues we have workarounds, but the AwkardForth ones are a bit worrisome (probably i should have called these \"the ugly\") since they seem to pop up a bit randomly and from our last debugging session i learned it can be quite difficult to find out what the issue is.\r\n",
  "closed_at":"2024-02-08T16:21:25Z",
  "comments":1,
  "created_at":"2023-09-01T11:11:15Z",
  "id":1877227420,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5v5Dec",
  "number":951,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"More issues with ElementLink branches in DAOD_PHYSLITE",
  "updated_at":"2024-02-15T11:44:39Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [actions/checkout](https://github.com/actions/checkout) from 3 to 4.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/checkout/releases\">actions/checkout's releases</a>.</em></p>\n<blockquote>\n<h2>v4.0.0</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Update default runtime to node20 by <a href=\"https://github.com/takost\"><code>@\u200btakost</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1436\">actions/checkout#1436</a></li>\n<li>Support fetching without the --progress option by <a href=\"https://github.com/simonbaird\"><code>@\u200bsimonbaird</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1067\">actions/checkout#1067</a></li>\n<li>Release 4.0.0 by <a href=\"https://github.com/takost\"><code>@\u200btakost</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1447\">actions/checkout#1447</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/takost\"><code>@\u200btakost</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/1436\">actions/checkout#1436</a></li>\n<li><a href=\"https://github.com/simonbaird\"><code>@\u200bsimonbaird</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/1067\">actions/checkout#1067</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/checkout/compare/v3...v4.0.0\">https://github.com/actions/checkout/compare/v3...v4.0.0</a></p>\n<h2>v3.6.0</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Mark test scripts with Bash'isms to be run via Bash by <a href=\"https://github.com/dscho\"><code>@\u200bdscho</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1377\">actions/checkout#1377</a></li>\n<li>Add option to fetch tags even if fetch-depth &gt; 0 by <a href=\"https://github.com/RobertWieczoreck\"><code>@\u200bRobertWieczoreck</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/579\">actions/checkout#579</a></li>\n<li>Release 3.6.0 by <a href=\"https://github.com/luketomlinson\"><code>@\u200bluketomlinson</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1437\">actions/checkout#1437</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/RobertWieczoreck\"><code>@\u200bRobertWieczoreck</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/579\">actions/checkout#579</a></li>\n<li><a href=\"https://github.com/luketomlinson\"><code>@\u200bluketomlinson</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/1437\">actions/checkout#1437</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/checkout/compare/v3.5.3...v3.6.0\">https://github.com/actions/checkout/compare/v3.5.3...v3.6.0</a></p>\n<h2>v3.5.3</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Fix: Checkout Issue in self hosted runner due to faulty submodule check-ins by <a href=\"https://github.com/megamanics\"><code>@\u200bmegamanics</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1196\">actions/checkout#1196</a></li>\n<li>Fix typos found by codespell by <a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1287\">actions/checkout#1287</a></li>\n<li>Add support for sparse checkouts by <a href=\"https://github.com/dscho\"><code>@\u200bdscho</code></a> and <a href=\"https://github.com/dfdez\"><code>@\u200bdfdez</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1369\">actions/checkout#1369</a></li>\n<li>Release v3.5.3 by <a href=\"https://github.com/TingluoHuang\"><code>@\u200bTingluoHuang</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1376\">actions/checkout#1376</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/megamanics\"><code>@\u200bmegamanics</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/1196\">actions/checkout#1196</a></li>\n<li><a href=\"https://github.com/DimitriPapadopoulos\"><code>@\u200bDimitriPapadopoulos</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/1287\">actions/checkout#1287</a></li>\n<li><a href=\"https://github.com/dfdez\"><code>@\u200bdfdez</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/1369\">actions/checkout#1369</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/checkout/compare/v3...v3.5.3\">https://github.com/actions/checkout/compare/v3...v3.5.3</a></p>\n<h2>v3.5.2</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Fix: Use correct API url / endpoint in GHES by <a href=\"https://github.com/fhammerl\"><code>@\u200bfhammerl</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1289\">actions/checkout#1289</a> based on <a href=\"https://redirect.github.com/actions/checkout/issues/1286\">#1286</a> by <a href=\"https://github.com/1newsr\"><code>@\u200b1newsr</code></a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/checkout/compare/v3.5.1...v3.5.2\">https://github.com/actions/checkout/compare/v3.5.1...v3.5.2</a></p>\n<h2>v3.5.1</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Improve checkout performance on Windows runners by upgrading <code>@\u200bactions/github</code> dependency by <a href=\"https://github.com/BrettDong\"><code>@\u200bBrettDong</code></a> in <a href=\"https://redirect.github.com/actions/checkout/pull/1246\">actions/checkout#1246</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/BrettDong\"><code>@\u200bBrettDong</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/checkout/pull/1246\">actions/checkout#1246</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/checkout/blob/main/CHANGELOG.md\">actions/checkout's changelog</a>.</em></p>\n<blockquote>\n<h1>Changelog</h1>\n<h2>v4.0.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1067\">Support fetching without the --progress option</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1436\">Update to node20</a></li>\n</ul>\n<h2>v3.6.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1377\">Fix: Mark test scripts with Bash'isms to be run via Bash</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/579\">Add option to fetch tags even if fetch-depth &gt; 0</a></li>\n</ul>\n<h2>v3.5.3</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1196\">Fix: Checkout fail in self-hosted runners when faulty submodule are checked-in</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1287\">Fix typos found by codespell</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1369\">Add support for sparse checkouts</a></li>\n</ul>\n<h2>v3.5.2</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1289\">Fix api endpoint for GHES</a></li>\n</ul>\n<h2>v3.5.1</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1246\">Fix slow checkout on Windows</a></li>\n</ul>\n<h2>v3.5.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1237\">Add new public key for known_hosts</a></li>\n</ul>\n<h2>v3.4.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1209\">Upgrade codeql actions to v2</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1210\">Upgrade dependencies</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1225\">Upgrade <code>@\u200bactions/io</code></a></li>\n</ul>\n<h2>v3.3.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1045\">Implement branch list using callbacks from exec function</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1050\">Add in explicit reference to private checkout options</a></li>\n<li>[Fix comment typos (that got added in <a href=\"https://redirect.github.com/actions/checkout/issues/770\">#770</a>)](<a href=\"https://redirect.github.com/actions/checkout/pull/1057\">actions/checkout#1057</a>)</li>\n</ul>\n<h2>v3.2.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/942\">Add GitHub Action to perform release</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/967\">Fix status badge</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1002\">Replace datadog/squid with ubuntu/squid Docker image</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/964\">Wrap pipeline commands for submoduleForeach in quotes</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1029\">Update <code>@\u200bactions/io</code> to 1.1.2</a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/1039\">Upgrading version to 3.2.0</a></li>\n</ul>\n<h2>v3.1.0</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/939\">Use <code>@\u200bactions/core</code> <code>saveState</code> and <code>getState</code></a></li>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/922\">Add <code>github-server-url</code> input</a></li>\n</ul>\n<h2>v3.0.2</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/checkout/pull/770\">Add input <code>set-safe-directory</code></a></li>\n</ul>\n<h2>v3.0.1</h2>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/actions/checkout/commit/3df4ab11eba7bda6032a0b82a6bb43b11571feac\"><code>3df4ab1</code></a> Release 4.0.0 (<a href=\"https://redirect.github.com/actions/checkout/issues/1447\">#1447</a>)</li>\n<li><a href=\"https://github.com/actions/checkout/commit/8b5e8b768746b50394015010d25e690bfab9dfbc\"><code>8b5e8b7</code></a> Support fetching without the --progress option (<a href=\"https://redirect.github.com/actions/checkout/issues/1067\">#1067</a>)</li>\n<li><a href=\"https://github.com/actions/checkout/commit/97a652b80035363df47baee5031ec8670b8878ac\"><code>97a652b</code></a> Update default runtime to node20 (<a href=\"https://redirect.github.com/actions/checkout/issues/1436\">#1436</a>)</li>\n<li>See full diff in <a href=\"https://github.com/actions/checkout/compare/v3...v4\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/checkout&package-manager=github_actions&previous-version=3&new-version=4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2023-09-18T15:06:44Z",
  "comments":0,
  "created_at":"2023-09-11T09:50:41Z",
  "draft":false,
  "id":1890120218,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5aAJ7k",
  "number":952,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-09-18T15:06:44Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump actions/checkout from 3 to 4",
  "updated_at":"2023-09-18T15:06:45Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/psf/black-pre-commit-mirror: 23.7.0 \u2192 23.9.1](https://github.com/psf/black-pre-commit-mirror/compare/23.7.0...23.9.1)\n- [github.com/astral-sh/ruff-pre-commit: v0.0.287 \u2192 v0.0.290](https://github.com/astral-sh/ruff-pre-commit/compare/v0.0.287...v0.0.290)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-09-21T15:05:25Z",
  "comments":0,
  "created_at":"2023-09-12T04:45:43Z",
  "draft":false,
  "id":1891662029,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5aFYy-",
  "number":953,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-09-21T15:05:25Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-09-21T15:05:26Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"> **Note**\r\n> This is a refactor request / place to put my thoughts after working on emscripten support for uproot. \r\n> I don't necessarily think we have the human-power to do this.\r\n\r\nOne of the challenges associated with integrating uproot into e.g. pyscript is that it blocks the main thread for IO, as pyjs et al. can't create background threads using the built-in threading models. There are things like webworkers which can offload work into a separate context, but these need helpers (trampolines), and work asynchronously via message passing. Blocking IO *can* be performed with fewer consequences in a jupyterlite kernel context (it's already _in_ a separate webworker!).\r\n\r\nAnother restriction posed by our existing IO handling is that integration into other async environments is nontrivial. To use asyncio requires uproot to be invoked behind `run_in_executor` contexts.\r\n\r\nIt would be nice if we were able to provide e.g. an additional async implementation of uproot that could safely wait for IO to complete, e.g.\r\n```python\r\nfrom uproot import async as up\r\n\r\nasync with up.open(\"../some/file.root\") as f:\r\n    data = await f['myTree'].arrays()\r\n```\r\n\r\nOf course, providing an async API might sound like a lot of duplication. The trick to doing this is to employ [sans-IO principles](https://sans-io.readthedocs.io/) that separate the business logic from the IO. This means that it's trivial to write blocking and non-blocking variants of an API.\r\n\r\nFor non request-response APIs, such as uproot's parsing routines (that have a lot of branching logic based upon buffer contents), the common model of \r\n```python\r\n# Build query\r\nrequest = sansio_api.build_request()\r\n\r\n# Submit query, receive response\r\nresponse = session.request(request)\r\n\r\n# Process response\r\nresult = sansio_api.handle_response(response)\r\n```\r\n\r\ndoesn't really apply. Instead one can implement a state machine that produces and consumes events such that there's a separate, simple event loop executor e.g. https://gist.github.com/agoose77/0e9b12a1c04afe61bce1c1c96ec5e3a1\r\n\r\nGiven that event loops are ultimately what powers asyncio et al., it might seem that what we're _really_ saying here is \"write async and call it synchronously\". However, it is important to keep a distinction between an async implementation and the concept of an event loop; there are multiple async frameworks, and the compatibility story is not perfect. Writing a state machine and associated event loop can make it trivial to port to new event-loops and frameworks.\r\n\r\n",
  "closed_at":null,
  "comments":0,
  "created_at":"2023-09-13T12:09:33Z",
  "id":1894391532,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5w6h7s",
  "number":954,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"open",
  "state_reason":null,
  "title":"Use sans-io principles to decouple IO from business logic",
  "updated_at":"2024-01-30T16:31:05Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"* Remove extra {} around the dictionary example.\r\n* Multiline literals are not a thing, convert that to a code block.",
  "closed_at":"2023-09-14T16:58:35Z",
  "comments":1,
  "created_at":"2023-09-13T18:21:22Z",
  "draft":false,
  "id":1895031534,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5aQxi-",
  "number":955,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-09-14T16:58:35Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: _dask.py: fix docstring formatting",
  "updated_at":"2023-09-14T16:58:35Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"#868 fixed some import errors, but we still used threads for non-HTTP sources. In this PR, we close the gap and make it possible to disable threads for all sources. By default, this is done for emscripten platforms.\r\n\r\nI didn't add HTTP support here \u2014 it's possible with e.g. `XMLHttpRequest`, but we'll need to spend some more cycles on it. Crucially, CORS means that this can be fiddly, but it would be more performant for emscripten platforms where `mmap` is not supported.\r\n\r\nSee https://agoose77.github.io/pyhep-23-jupyterlite/lab/index.html for a repo that uses this patch.",
  "closed_at":"2023-09-20T21:27:56Z",
  "comments":8,
  "created_at":"2023-09-13T21:22:56Z",
  "draft":false,
  "id":1895284312,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5aRpFu",
  "number":956,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-09-20T21:27:56Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: add minimimal emscripten support via non-HTTP sources",
  "updated_at":"2023-09-20T21:27:57Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This is an immediate fix, but when dask-contrib/dask-awkward#365 makes it into a release, we should remove the line\r\n\r\n```python\r\npytest.importorskip(\"pyarrow\")  # dask_awkward.lib.testutils needs pyarrow\r\n```\r\n\r\nfrom all of the following files:\r\n\r\n* test_0652_dask-for-awkward.py\r\n* test_0876-uproot-dask-blind-steps.py\r\n* test_0700-dask-empty-arrays.py\r\n* test_0755-dask-awkward-column-projection.py",
  "closed_at":"2023-09-14T16:38:37Z",
  "comments":1,
  "created_at":"2023-09-14T16:27:18Z",
  "draft":false,
  "id":1896923528,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5aXKwE",
  "number":957,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-09-14T16:38:37Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: tests were failing because dask_awkward.lib.testutils needs pyarrow",
  "updated_at":"2023-09-14T16:38:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"When dask-contrib/dask-awkward#365 makes it into a release, we should remove the line\r\n\r\n```python\r\npytest.importorskip(\"pyarrow\")  # dask_awkward.lib.testutils needs pyarrow\r\n```\r\n\r\nfrom all of the following files:\r\n\r\n* test_0652_dask-for-awkward.py\r\n* test_0876-uproot-dask-blind-steps.py\r\n* test_0700-dask-empty-arrays.py\r\n* test_0755-dask-awkward-column-projection.py",
  "closed_at":"2023-09-21T15:05:11Z",
  "comments":0,
  "created_at":"2023-09-14T16:30:08Z",
  "id":1896927310,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5xENBO",
  "number":958,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Remove soon-to-be unnecessary dependence of dask_awkward.lib.testutils on pyarrow",
  "updated_at":"2023-09-21T15:05:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Following the conventions in https://github.com/dask-contrib/dask-awkward/pull/368\r\n\r\n@agoose77 @douglasdavis ",
  "closed_at":"2023-10-04T10:51:24Z",
  "comments":2,
  "created_at":"2023-09-18T21:49:57Z",
  "draft":false,
  "id":1901801853,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5anZr0",
  "number":960,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: adapt to new shape touching in awkward/dask-awkward",
  "updated_at":"2023-10-04T10:51:24Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-09-21T15:05:10Z",
  "comments":0,
  "created_at":"2023-09-20T11:07:08Z",
  "draft":false,
  "id":1904752096,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5axZQD",
  "number":961,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-09-21T15:05:10Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: remove pyarrow import as a dependence coming from dask-awkward in tests",
  "updated_at":"2023-09-21T15:05:11Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-10-19T14:48:07Z",
  "comments":0,
  "created_at":"2023-09-20T11:34:32Z",
  "draft":false,
  "id":1904795582,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5axitt",
  "number":962,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-19T14:48:07Z"
  },
  "reactions":{
   "hooray":2,
   "total_count":2
  },
  "state":"closed",
  "state_reason":null,
  "title":"feat: add support for current RNTuple files",
  "updated_at":"2023-10-19T14:48:08Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/astral-sh/ruff-pre-commit: v0.0.290 \u2192 v0.0.292](https://github.com/astral-sh/ruff-pre-commit/compare/v0.0.290...v0.0.292)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-10-03T11:42:16Z",
  "comments":0,
  "created_at":"2023-09-26T04:27:40Z",
  "draft":false,
  "id":1912655028,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5bL4PA",
  "number":964,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-03T11:42:16Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-10-03T11:42:17Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-10-03T19:15:34Z",
  "comments":1,
  "created_at":"2023-10-03T10:15:48Z",
  "draft":false,
  "id":1923799955,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5bxeRu",
  "number":965,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-03T19:15:34Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: inverted axes for variances of 2D weighted histograms when transformed to hist",
  "updated_at":"2023-10-03T19:15:35Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"> **Warning**\r\n> Depends upon #980 \r\n\r\nThis PR adds support for the `shape_touched` tracking performed by Awkward typetracers in order to avoid reading all data buffers.\r\n\r\nHigh level overview of this PR:\r\n\r\n1. `_UprootRead` et al. implement the new buffer projection interface\r\n2. Implements non-remapped reads as a trivial form mapping\r\n3. Replaces rehydration call with `from_buffers`\r\n",
  "closed_at":"2023-10-11T13:15:08Z",
  "comments":5,
  "created_at":"2023-10-03T16:17:50Z",
  "draft":false,
  "id":1924493829,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5bz0w2",
  "number":966,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-11T13:15:08Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add support for shape touching in Dask",
  "updated_at":"2023-10-11T13:15:09Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR simply adds the changes from https://github.com/scikit-hep/uproot5/pull/692 with the necessary updates to make it compatible with current uproot.\r\n\r\nA very basic `Source` is implemented for `fsspec` allowing the user to optionally use `fsspec` by setting the appropiate handler in the read operation.\r\n\r\n```\r\nwith uproot.open(\r\n    \"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\",\r\n    http_handler=uproot.source.fsspec.FSSpecSource,\r\n) as f:\r\n    data = f[\"Events/MET_pt\"].array(library=\"np\")\r\n    assert len(data) == 40\r\n```",
  "closed_at":"2023-10-04T19:15:04Z",
  "comments":2,
  "created_at":"2023-10-03T16:32:17Z",
  "draft":false,
  "id":1924521101,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5bz6sX",
  "number":967,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-04T19:15:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: adding a very basic FSSpecSource",
  "updated_at":"2023-10-04T21:05:08Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"We missed this because all of the test-runners that have Dask installed also have dask-awkward installed. In principle, someone might run the tests in an environment that does not.",
  "closed_at":"2023-10-03T20:16:29Z",
  "comments":0,
  "created_at":"2023-10-03T18:05:15Z",
  "draft":false,
  "id":1924664176,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5b0ZdJ",
  "number":968,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-03T20:16:29Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: skip if dask-awkward, an optional dependency, is missing",
  "updated_at":"2023-10-03T20:16:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"We're going to need a stable set of tests for vetting #966. Perhaps it hasn't been happening in CI, but tests/test_0662-rntuple-stl-containers.py has been failing for me locally, presumably because #928 is not resolved (and CI is somehow skipping this one?).",
  "closed_at":"2023-10-03T20:16:16Z",
  "comments":0,
  "created_at":"2023-10-03T18:16:43Z",
  "draft":false,
  "id":1924680711,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5b0dAn",
  "number":969,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-03T20:16:16Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: skip RNTuple test until #928 is fixed",
  "updated_at":"2023-10-03T20:16:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Same as https://github.com/scikit-hep/awkward/pull/2044.\r\n\r\nHyphens in the test names breaks PyCharm for some reason, tests cannot be ran from the IDE.\r\n\r\nI can do the renaming but I will wait until people currently working on these files are done to avoid merge conflicts.\r\n\r\nThe following command can be used to rename the tests files (run from the tests dir):\r\n\r\n```\r\nfind . -type f -name \"*-*\" -exec sh -c 'mv \"$0\" \"${0//-/_}\"' {} \\;\r\n```",
  "closed_at":"2023-12-06T15:17:43Z",
  "comments":0,
  "created_at":"2023-10-03T23:53:57Z",
  "id":1925125185,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5yvxRB",
  "number":970,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"test: remove hyphens from test file names",
  "updated_at":"2023-12-06T15:17:43Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Currently the `uproot.reading.open` method takes optional arguments such as `http_handler`, `file_handler` etc. instead of a single argument `handler`. This allows to declare multiple handlers for different file schemas.\r\n\r\nThis PR falls under the umbrella of the `fsspec` integration (issue incoming). The goal of this integration would be that there is a single source `fsspec` that can handle all schemas. Optionally the user is able to define a custom handler. Existing handlers should work as before (may be deprecated in the future once fsspec integration is validated).\r\n\r\nA good first step would be to refactor the current code to only allow a single `handler` argument and if not defined automatically infer this from the file path string.",
  "closed_at":"2023-10-12T19:52:20Z",
  "comments":11,
  "created_at":"2023-10-04T18:41:11Z",
  "draft":false,
  "id":1926776850,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5b7kXi",
  "number":971,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-12T19:52:20Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: Use a single `handler` argument on `uproot.reading.open`",
  "updated_at":"2023-10-12T19:52:21Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This issue is to track the progress of the `fsspec` integration as it's going to be consisting of multiple PRs.\r\n\r\nThis work is part of an IRIS-HEP Fellowship. Here are some of the PRs performed during this period:\r\n\r\n- https://github.com/scikit-hep/uproot5/pull/692 (extensive discussion)\r\n- https://github.com/scikit-hep/uproot5/pull/967\r\n- https://github.com/scikit-hep/uproot5/pull/971\r\n- https://github.com/scikit-hep/uproot5/pull/976\r\n- https://github.com/scikit-hep/uproot5/pull/979\r\n- https://github.com/scikit-hep/uproot5/pull/983\r\n- https://github.com/scikit-hep/uproot5/pull/984 (TODO: remember to merge this in the future when the deprecation warning expires).\r\n- https://github.com/scikit-hep/uproot5/pull/992\r\n- https://github.com/scikit-hep/uproot5/pull/996\r\n- https://github.com/CoffeaTeam/fsspec-xrootd/pull/26\r\n- https://github.com/scikit-hep/uproot5/pull/999\r\n- https://github.com/scikit-hep/uproot5/pull/1001\r\n- https://github.com/scikit-hep/uproot5/pull/1002\r\n- https://github.com/scikit-hep/uproot5/pull/1007\r\n- https://github.com/scikit-hep/uproot5/pull/1009\r\n- https://github.com/scikit-hep/uproot5/pull/1010\r\n- https://github.com/scikit-hep/uproot5/pull/1012\r\n- https://github.com/scikit-hep/uproot5/pull/1014\r\n- https://github.com/scikit-hep/uproot5/pull/1015\r\n- https://github.com/scikit-hep/uproot5/pull/1016\r\n- https://github.com/scikit-hep/uproot5/pull/1018\r\n- https://github.com/scikit-hep/uproot5/pull/1020\r\n- https://github.com/fsspec/filesystem_spec/pull/1426\r\n- https://github.com/scikit-hep/uproot5/pull/1021\r\n- https://github.com/scikit-hep/uproot5/pull/1022\r\n- https://github.com/scikit-hep/uproot5/pull/1023\r\n- https://github.com/scikit-hep/uproot5/pull/1024\r\n- https://github.com/scikit-hep/uproot5/pull/1026\r\n- https://github.com/scikit-hep/uproot5/pull/1027\r\n",
  "closed_at":"2023-12-14T15:01:45Z",
  "comments":0,
  "created_at":"2023-10-04T18:47:14Z",
  "id":1926785477,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5y2GnF",
  "number":972,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Integration of `fsspec`",
  "updated_at":"2023-12-14T15:01:45Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-10-05T21:04:44Z",
  "comments":2,
  "created_at":"2023-10-04T21:19:53Z",
  "draft":false,
  "id":1926999342,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5b8Ura",
  "number":973,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-05T21:04:44Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: use file in skhep-testdata for issue #121",
  "updated_at":"2023-10-05T21:27:42Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"When i have a key in a root file with a colon in the name, e.g.\r\n\r\n```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'5.0.12'\r\n\r\nf = uproot.recreate(\"test.root\")\r\nf[\"bla:bla\"] = \"blub\"\r\n```\r\n\r\nThen reading this seems to run in some kind of infinite (?) recursion\r\n\r\n<details>\r\n\r\n```python\r\nIn [2]: f[\"bla:bla\"]\r\n---------------------------------------------------------------------------\r\nRecursionError                            Traceback (most recent call last)\r\nCell In[2], line 1\r\n----> 1 f[\"bla:bla\"]\r\n\r\nFile ~/.local/lib/python3.11/site-packages/uproot/writing/writable.py:968, in WritableDirectory.__getitem__(self, where)\r\n    966 if self._file.sink.closed:\r\n    967     raise ValueError(\"cannot get data from a closed file\")\r\n--> 968 return self._get_del_search(where, True)\r\n\r\nFile ~/.local/lib/python3.11/site-packages/uproot/writing/writable.py:927, in WritableDirectory._get_del_search(self, where, isget)\r\n    925     else:\r\n    926         last = step\r\n--> 927         step = step[item]\r\n    929 elif isinstance(step, WritableTree):\r\n    930     rest = items[i:]\r\n\r\nFile ~/.local/lib/python3.11/site-packages/uproot/writing/writable.py:968, in WritableDirectory.__getitem__(self, where)\r\n    966 if self._file.sink.closed:\r\n    967     raise ValueError(\"cannot get data from a closed file\")\r\n--> 968 return self._get_del_search(where, True)\r\n\r\nFile ~/.local/lib/python3.11/site-packages/uproot/writing/writable.py:927, in WritableDirectory._get_del_search(self, where, isget)\r\n    925     else:\r\n    926         last = step\r\n--> 927         step = step[item]\r\n    929 elif isinstance(step, WritableTree):\r\n    930     rest = items[i:]\r\n\r\n    [... skipping similar frames: WritableDirectory.__getitem__ at line 968 (1489 times), WritableDirectory._get_del_search at line 927 (1489 times)]\r\n\r\nFile ~/.local/lib/python3.11/site-packages/uproot/writing/writable.py:968, in WritableDirectory.__getitem__(self, where)\r\n    966 if self._file.sink.closed:\r\n    967     raise ValueError(\"cannot get data from a closed file\")\r\n--> 968 return self._get_del_search(where, True)\r\n\r\nFile ~/.local/lib/python3.11/site-packages/uproot/writing/writable.py:927, in WritableDirectory._get_del_search(self, where, isget)\r\n    925     else:\r\n    926         last = step\r\n--> 927         step = step[item]\r\n    929 elif isinstance(step, WritableTree):\r\n    930     rest = items[i:]\r\n\r\nFile ~/.local/lib/python3.11/site-packages/uproot/writing/writable.py:966, in WritableDirectory.__getitem__(self, where)\r\n    965 def __getitem__(self, where):\r\n--> 966     if self._file.sink.closed:\r\n    967         raise ValueError(\"cannot get data from a closed file\")\r\n    968     return self._get_del_search(where, True)\r\n\r\nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\n</details>\r\n\r\nAlso happens for `ReadOnlyDirectory`\r\n\r\n<details>\r\n\r\n```python\r\nIn [3]: f.close()\r\n\r\nIn [4]: f = uproot.open(\"test.root\")\r\n\r\nIn [5]: f[\"bla:bla\"]\r\n[...]\r\nFile ~/.local/lib/python3.11/site-packages/uproot/reading.py:2076, in ReadOnlyDirectory.__getitem__(self, where)\r\n   2074 if item != \"\":\r\n   2075     if isinstance(step, ReadOnlyDirectory):\r\n-> 2076         if \":\" in item and item not in step:\r\n   2077             index = item.index(\":\")\r\n   2078             head, tail = item[:index], item[index + 1 :]\r\n\r\nFile ~/.local/lib/python3.11/site-packages/uproot/reading.py:1925, in ReadOnlyDirectory.__contains__(self, where)\r\n   1923 def __contains__(self, where):\r\n   1924     try:\r\n-> 1925         self.key(where)\r\n   1926     except KeyError:\r\n   1927         return False\r\n\r\nFile ~/.local/lib/python3.11/site-packages/uproot/reading.py:2017, in ReadOnlyDirectory.key(self, where)\r\n   2003 def key(self, where):\r\n   2004     \"\"\"\r\n   2005     Returns a ``TKey`` (:doc:`uproot.reading.ReadOnlyKey`) for the object\r\n   2006     selected by ``where``.\r\n   (...)\r\n   2015     Note that this does not read any data from the file.\r\n   2016     \"\"\"\r\n-> 2017     where = uproot._util.ensure_str(where)\r\n   2019     if \"/\" in where:\r\n   2020         items = where.split(\"/\")\r\n\r\nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\n</details>\r\n\r\nI can workaround it by removing the check for `\":\" in where` here, but i don't know if this has other negative implications.\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/546b0cbb8595d1775f53f938c9bc6bda6c2d5931/src/uproot/reading.py#L2069 \r\n\r\nNoticed this when i tried to read the current ATLAS PHYSLITE RNtuple prototype (currently has `\"RNT:CollectionTree\"` as key name), but wanted to post this as an isolated issue.",
  "closed_at":"2024-02-15T17:15:20Z",
  "comments":1,
  "created_at":"2023-10-05T11:51:44Z",
  "id":1928101173,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5y7H01",
  "number":974,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Can't read keys in root files with colon in name",
  "updated_at":"2024-02-15T17:15:20Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"We are starting to have RNtuple prototypes for PHYSLITE. This is all in a very early stage, so things might change. However i still wanted to start looking how we can read them from uproot in parallel to the developments.\r\n\r\nHere is a small example file, produced with [produce_rntuple.sh](https://github.com/nikoladze/physlite_experiments/blob/9ce36623d5cd21b6ff748a0231f63fb4d882b397/physlite_experiments/scripts/produce_rntuple.sh):\r\n[DAOD_PHYSLITE_2023-09-13T1230.art.rntuple.root.zip](https://github.com/scikit-hep/uproot5/files/12819286/DAOD_PHYSLITE_2023-09-13T1230.art.rntuple.root.zip)\r\n\r\nFirst, i run into #974, since the RNtuple in that file is under the key `\"RNT:CollectionTree\"` which has a colon in the name. But when i work around this i can at least read the RNtuple object:\r\n\r\n```python\r\nimport uproot\r\nrnt = uproot.open({\"DAOD_PHYSLITE_2023-09-13T1230.art.rntuple.root\": \"RNT:CollectionTree\"})\r\n```\r\n\r\n`rnt.field_names` and `rnt.column_records` seem to work as well, but when i try to read data i get an Exception, e.g.\r\n\r\n<details>\r\n\r\n```python\r\nIn [2]: rnt.arrays([\"AnalysisJetsAux::pt\"])\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\nCell In[2], line 1\r\n----> 1 rnt.arrays([\"AnalysisJetsAux::pt\"])\r\n\r\nFile ~/code/uproot/src/uproot/models/RNTuple.py:392, in Model_ROOT_3a3a_Experimental_3a3a_RNTuple.arrays(self, filter_names, filter_typenames, entry_start, entry_stop, decompression_executor,\r\n array_cache)\r\n    381 def arrays(\r\n    382     self,\r\n    383     filter_names=\"*\",\r\n   (...)\r\n    388     array_cache=None,\r\n    389 ):\r\n    390     ak = uproot.extras.awkward()\r\n--> 392     entry_stop = entry_stop or self._length\r\n    394     clusters = self.cluster_summaries\r\n    395     cluster_starts = numpy.array([c.num_first_entry for c in clusters])\r\n\r\nFile ~/code/uproot/src/uproot/models/RNTuple.py:178, in Model_ROOT_3a3a_Experimental_3a3a_RNTuple._length(self)\r\n    176 @property\r\n    177 def _length(self):\r\n--> 178     return sum(x.num_entries for x in self.cluster_summaries)\r\n\r\nFile ~/code/uproot/src/uproot/models/RNTuple.py:173, in Model_ROOT_3a3a_Experimental_3a3a_RNTuple.cluster_summaries(self)\r\n    171 @property\r\n    172 def cluster_summaries(self):\r\n--> 173     return self.footer.cluster_summaries\r\n\r\nFile ~/code/uproot/src/uproot/models/RNTuple.py:162, in Model_ROOT_3a3a_Experimental_3a3a_RNTuple.footer(self)\r\n    159 cursor = self._footer_cursor.copy()\r\n    160 context = {}\r\n--> 162 f = FooterReader().read(self._footer_chunk, cursor, context)\r\n    163 assert (\r\n    164     f.header_crc32 == self.header.crc32\r\n    165 ), f\"crc32={self.header.crc32}, header_crc32={f.header_crc32}\"\r\n    166 assert f.crc32 == zlib.crc32(self._footer_chunk.raw_data[:-4])\r\n\r\nFile ~/code/uproot/src/uproot/models/RNTuple.py:695, in FooterReader.read(self, chunk, cursor, context)\r\n    692 out.feature_flag = cursor.field(chunk, _rntuple_feature_flag_format, context)\r\n    693 out.header_crc32 = cursor.field(chunk, struct.Struct(\"<I\"), context)\r\n--> 695 out.extension_links = self.extension_header_links.read(chunk, cursor, context)\r\n    696 out.col_group_records = self.column_group_record_frames.read(\r\n    697     chunk, cursor, context\r\n    698 )\r\n    699 out.cluster_summaries = self.cluster_summary_frames.read(chunk, cursor, context)\r\n\r\nFile ~/code/uproot/src/uproot/models/RNTuple.py:554, in ListFrameReader.read(self, chunk, cursor, context)\r\n    550 local_cursor = cursor.copy()\r\n    551 num_bytes, num_items = local_cursor.fields(\r\n    552     chunk, _rntuple_frame_header, context\r\n    553 )\r\n--> 554 assert num_bytes < 0, f\"num_bytes={num_bytes}\"\r\n    555 cursor.skip(-num_bytes)\r\n    556 return [\r\n    557     self.payload.read(chunk, local_cursor, context) for _ in range(num_items)\r\n    558 ]\r\n\r\nAssertionError: num_bytes=37077\r\n```\r\n\r\n</details>\r\n\r\nThe exception seems to occur when reading the footer, so maybe it's the same issue as reported in #928, let's see.\r\n\r\nFor Information: also in ROOT this is currently bleeding edge and to read that file one needs features currently not in a release yet. But it works with a nightly version of root, e.g\r\n\r\n<details>\r\n\r\n```\r\n# setup in centos7 environment\r\nsource /cvmfs/sft.cern.ch/lcg/views/dev3/latest/x86_64-centos7-gcc11-opt/setup.sh\r\n```\r\n\r\n```python\r\nimport ROOT\r\nrnt = ROOT.Experimental.RNTupleReader.Open(\"RNT:CollectionTree\", \"DAOD_PHYSLITE_2023-09-13T1230.art.rntuple.root\")\r\nview = rnt.GetView[ROOT.std.vector[ROOT.float]](\"AnalysisJetsAux::pt\")\r\n\r\n>>> view(0)\r\nvector<float>{ 155946.f, 140229.f, 120520.f, 119895.f, 69683.3f, 28485.4f, 18776.9f, 10522.2f, 11948.9f }\r\n```\r\n\r\n(To read all branches, e.g. also those involving custom classes one needs to setup Athena)\r\n\r\n</details>\r\n",
  "closed_at":"2024-01-18T16:15:00Z",
  "comments":6,
  "created_at":"2023-10-05T14:28:33Z",
  "id":1928439183,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5y8aWP",
  "number":975,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Reading the RNtuple PHYSLITE prototype",
  "updated_at":"2024-01-18T16:15:00Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Currently the `uproot._util.py` file contains some helper methods for things such as url parsing, split root object from file path etc.\r\n\r\nBesides the splitting of the root object from a URI which is non-standard, fsspec should be able to handle everything else and we could remove or simplify some helper methods.\r\n\r\nSince this uses fsspec but fsspec is not currently a dependency my solution was to do the new url split using fsspec only if its installed, otherwise do the old one, which may cause some problems.",
  "closed_at":"2023-10-19T17:05:52Z",
  "comments":7,
  "created_at":"2023-10-05T14:55:48Z",
  "draft":false,
  "id":1928498028,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5cBZNl",
  "number":976,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-19T17:05:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: refactor url - object split (motivated by `fsspec` integration)",
  "updated_at":"2023-10-19T17:05:53Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR adds a non-blocking implementation of the `chunks` interface.\r\n\r\nIt uses the `AbstractFileSystem.cat_file` api to get a byte range of the file. This api should only request the selected range (as opposed to the whole file, then slicing). In order to make sure this is the case I opened a discussion [here](https://github.com/fsspec/filesystem_spec/discussions/1388).\r\n\r\nIt also adds some type annotations to the `uproot.source` files.\r\n",
  "closed_at":"2023-10-12T20:33:39Z",
  "comments":7,
  "created_at":"2023-10-06T21:09:23Z",
  "draft":false,
  "id":1930936237,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5cJkpJ",
  "number":979,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-12T20:33:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: `fsspec` source non-blocking chunks",
  "updated_at":"2023-10-12T20:33:40Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR introduces `pyupgrade`, bumps the oldest supported Python interpreter from 3.7 to 3.8, and adds support for 3.12 in the metadata.\r\n\r\nIt also applies a fix for the removal of `NumpyArray.__array__`.",
  "closed_at":"2023-10-11T13:50:31Z",
  "comments":6,
  "created_at":"2023-10-06T22:19:48Z",
  "draft":false,
  "id":1930998332,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5cJyTI",
  "number":980,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-11T13:50:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: bump Python version",
  "updated_at":"2023-10-11T13:50:32Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"\r\nUproot 5 isn't correctly handling boost_histogram to root object conversion. Just for refence I've attached 4.3.7 which works as expected. (edit: updating to boost_histogram 1.4.0 fixed this, was previously on 1.3.1)\r\n```python\r\n>>>import uproot, boost_histogram as bh, numpy as np\r\n>>>print(uproot.__version__)\r\n>>>hist= bh.Histogram(bh.axis.Regular(20, 5, 30))\r\n>>>hist.fill(np.arange(10))\r\n>>>with uproot.recreate(\"hist.root\") as root_file:\r\n>>>root_file[\"hist\"] = hist\r\n5.0.12\r\nAttributeError: 'Histogram' object has no attribute 'storage_type'\r\n```\r\nFor reference this works as expected on 4.3.7\r\n```python\r\n>>>import uproot, boost_histogram as bh\r\n>>>print(uproot.__version__)\r\n>>>hist= bh.Histogram(bh.axis.Regular(20, 5, 30))\r\n>>>hist.fill([1]*5)\r\n>>>with uproot.recreate(\"hist.root\") as root_file:\r\n>>>root_file[\"hist\"] = hist\r\n4.3.7\r\n```\r\n",
  "closed_at":"2023-10-09T13:35:00Z",
  "comments":1,
  "created_at":"2023-10-07T19:35:13Z",
  "id":1931476461,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5zH_3t",
  "number":981,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Issue converting boost_histogram to root histogram",
  "updated_at":"2023-10-09T13:35:01Z",
  "user":"U_kgDOBw6OhA"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/pre-commit/pre-commit-hooks: v4.4.0 \u2192 v4.5.0](https://github.com/pre-commit/pre-commit-hooks/compare/v4.4.0...v4.5.0)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-10-13T18:58:27Z",
  "comments":0,
  "created_at":"2023-10-09T19:46:14Z",
  "draft":false,
  "id":1933687105,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5cSwDv",
  "number":982,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-13T18:58:27Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-10-13T18:58:28Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"\r\n- The `uproot.source.chunk.notifier` method is used as a callback for a future but it's not directly compatible with the `concurrent.futures.Future.add_done_callback` api as it does not take a `future` argument. I modified it to add an optional argument so it's compatible (previously one could wrap it in a function to achieve the compatibility, but I think this makes more sense).\r\n\r\n- The `uproot.futures.ThreadPoolExecutor` does not have the same constructor arguments as the `concurrent.futures.ThreadPoolExecutor`. uproot's has `num_workers` and concurrent's has `max_workers` instead. I guess when uproot's was created the argument was called `num_workers` then it got renamed (altough its not just a renaming, it means different things).\r\n\r\nI think this PR can also serve as an opportunity for a discussion regarding the `uproot.futures` module. I understand that it was originally motivated by the need to support Python 2 but now that this is deprecated maybe it makes sense to replace the usage of some of these classes such as `uproot.futures.Future` or `uproot.futures.ThreadPoolExecutor` by the `concurrent` equivalent?",
  "closed_at":"2023-10-12T13:30:14Z",
  "comments":1,
  "created_at":"2023-10-10T22:49:20Z",
  "draft":false,
  "id":1936399920,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5ccPNz",
  "number":983,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-12T13:30:13Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: improve `uproot.futures` compatibility with `concurrent.futures`",
  "updated_at":"2023-10-12T13:30:15Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"After https://github.com/scikit-hep/uproot5/pull/971 the `*_handler` option is deprecated but the options remained for backwards compatibility.\r\n\r\nThis PR removes the backwards compatible options and sets the `fsspec` source as the default handler, which should be able to handle anything. The `handler` option is kept for to allow the user to specify a custom source or one in the `uproot.source` module. These sources are kept but I think there shouldn't be a reason to use them.",
  "closed_at":"2023-10-12T15:55:56Z",
  "comments":1,
  "created_at":"2023-10-11T16:46:26Z",
  "draft":true,
  "id":1938245309,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5cil4D",
  "number":984,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: remove `*_handler` option, fsspec source as default handler",
  "updated_at":"2023-10-12T15:55:57Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-10-12T18:18:31Z",
  "comments":3,
  "created_at":"2023-10-12T15:52:37Z",
  "draft":false,
  "id":1940286514,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5cpmBW",
  "number":985,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-12T18:18:31Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: pull out `.data` from `NumpyArray`",
  "updated_at":"2023-10-12T18:18:32Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-10-12T18:54:39Z",
  "comments":3,
  "created_at":"2023-10-12T17:47:56Z",
  "draft":false,
  "id":1940495068,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5cqUvn",
  "number":986,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-12T18:54:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: support `Content` objects in writing",
  "updated_at":"2023-10-12T18:54:40Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This is going to be a commit merge, not a squash-and-merge, so that all of the individual commits in `main-v510`, each of which is a PR, will appear as individual commits in `main`.",
  "closed_at":"2023-10-13T18:36:08Z",
  "comments":0,
  "created_at":"2023-10-13T18:08:48Z",
  "draft":false,
  "id":1942408772,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5cw1LD",
  "number":988,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-13T18:36:08Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: merge `main-v510` into `main`, ending the two-main branch era",
  "updated_at":"2023-10-13T18:36:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"I scanned all of @lobis's PRs for \"XRootDSource\" and \"source.root\" and it only appeared in #971.\r\n\r\nWe need a test for it, though.",
  "closed_at":"2023-10-13T20:24:49Z",
  "comments":1,
  "created_at":"2023-10-13T20:03:02Z",
  "draft":false,
  "id":1942565141,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5cxUDk",
  "number":990,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-13T20:24:49Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: typo in `uproot.source.xrootd.XRootDSource` name",
  "updated_at":"2023-10-13T20:24:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR adds a new type of executor exclusive to the fsspec source that allows to run tasks asynchronously in a single thread using the fsspec event loop (for async-capable backends).\r\n\r\nThe list of commits for this PR is very long because the design changed a few times:\r\n\r\nFirst we added a generic `LoopExecutor` capable of submitting tasks to a loop running in a different thread. This class was responsible for managing the lifetime of the loop and thread. There were some issues when submitting fsspec coroutines (I still don't really understand why).\r\n\r\nFinally I realised that fsspec provides it's own loop, so we avoid having to manage all this and just use the fsspec's implementation of a \"loop in a can\" (@nsmith- ). This executor is defined in the `fsspec.py` source file as currently it only makes sense to use it from the fsspec source since you have to provide a loop. The executor does nothing, it's just a wrapper for compatibility.\r\n\r\nWhen implementing this thin executor I thought that implementing an ABC executor class would be a good idea (IMHO).",
  "closed_at":"2023-10-19T18:29:50Z",
  "comments":7,
  "created_at":"2023-10-15T01:43:05Z",
  "draft":false,
  "id":1943634061,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5c0T6q",
  "number":992,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-19T18:29:50Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: `asyncio` LoopExecutor and async fsspec source",
  "updated_at":"2023-10-19T18:29:51Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/asottile/pyupgrade: v3.13.0 \u2192 v3.15.0](https://github.com/asottile/pyupgrade/compare/v3.13.0...v3.15.0)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-10-19T13:40:37Z",
  "comments":0,
  "created_at":"2023-10-16T19:22:59Z",
  "draft":false,
  "id":1945932152,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5c8C8B",
  "number":993,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-19T13:40:37Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-10-19T13:40:38Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Test 0965 fails:\r\n```\r\n==================================== ERRORS ====================================_____ ERROR collecting tests/test_0965-inverted-axes-variances-hist-888.py _____ImportError while importing test module '/build/source/tests/test_0965-inverted-axes-variances-hist-888.py'.\r\nHint: make sure your test modules/packages have valid Python names.\r\nTraceback:\r\n/nix/store/pzf6dnxg8gf04xazzjdwarm7s03cbrgz-python3-3.10.12/lib/python3.10/importlib/__init__.py:126: in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\ntests/test_0965-inverted-axes-variances-hist-888.py:5: in <module>\r\n    import hist\r\nE   ModuleNotFoundError: No module named 'hist'\r\n```\r\n\r\nIt seems that the other tests relying on `hist` import it using `hist = pytest.importorskip(\"hist\")`.\r\nI did the same in this problematic test.",
  "closed_at":"2023-10-18T14:20:36Z",
  "comments":4,
  "created_at":"2023-10-18T13:54:51Z",
  "draft":false,
  "id":1949821364,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dJUQe",
  "number":994,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-18T14:20:36Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: remove unused hist import in test_0965",
  "updated_at":"2023-10-18T14:20:37Z",
  "user":"MDQ6VXNlcjMzMDU4NzQ3"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @GaetanLepage as a contributor for test.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot5/pull/994#issuecomment-1768555613)\n\n[skip ci]",
  "closed_at":"2023-10-18T14:15:09Z",
  "comments":0,
  "created_at":"2023-10-18T14:14:16Z",
  "draft":false,
  "id":1949865263,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dJdze",
  "number":995,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-18T14:15:09Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add GaetanLepage as a contributor for test",
  "updated_at":"2023-10-18T14:15:10Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Add types to most of the sources module.\r\n\r\nOnly added types where I was sure of the type.\r\n\r\nI left things such as urls, file paths, etc. untyped instead of typing them as `str` since user may pass a `pathlib` path or similar.",
  "closed_at":"2023-10-18T18:16:57Z",
  "comments":1,
  "created_at":"2023-10-18T17:00:47Z",
  "draft":false,
  "id":1950237992,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dKusg",
  "number":996,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-18T18:16:57Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: add types to most of the `uproot.source` module",
  "updated_at":"2023-10-18T19:35:03Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Restrict more stateful parts of code to client side of dask.\r\n\r\nThis fixes stickiness of configuration data for NanoEvents since that interface is class driven.\r\nWithout caching the rendered form setting cross references or branch behavior would not make it to remote dask workers!\r\n\r\n@agoose77 If there's a better way to do this please alter the code!\r\n\r\nOnce this is reviewed and merged a prompt new release would be appreciated!",
  "closed_at":"2023-10-19T15:45:22Z",
  "comments":4,
  "created_at":"2023-10-18T17:38:37Z",
  "draft":false,
  "id":1950304741,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dK9dG",
  "number":997,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: cache rendered Form and ImplementedFormMappingInfo",
  "updated_at":"2023-10-19T15:45:22Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Replaces #997 with a PR that exclusively caches the remapped form.\r\n\r\nIn the original dask-awkward refactoring PR, I treated form remapping as a cheap operation that could be performed multiple times. However, it appears that this was too strong an assumption, and incidentally brings other consequences when performed at worker time, not just at scheduler time.\r\n\r\nThis PR caches the form remapping, changing our private interface to store the remapping information rather than its factory.",
  "closed_at":"2023-10-19T18:03:43Z",
  "comments":3,
  "created_at":"2023-10-18T21:12:57Z",
  "draft":false,
  "id":1950670897,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dMOqs",
  "number":998,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-19T18:03:43Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: cache form remapping to avoid per-chunk workload",
  "updated_at":"2023-10-19T18:03:44Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR removes the support for different types of executors for the `fsspec` source. The `use_threads` and `num_workers` options are also dropped.\r\n\r\nThe loop executor exists only for compatibility with the remaining sources and does not hold any resources. The loop is accessed directly from `fsspec.asyn` and `submit` is basically just `asyncio.run_coroutine_threadsafe`.\r\n\r\nWith this PR, whenever the `fsspec` filesystem does not support `async` calls, the methods are wrapped into a coroutine that runs them in a separate thread so they are not blocking. This may spawn too many short-lived threads but from limited testing I haven't found this to be an issue. The alternative would be to run them as coroutines in the `fsspec` loop but they wouldn't run concurrently as they are blocking calls. Another alternative would be to use something like `https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor` to run the coroutines in an executor. Edit: https://github.com/scikit-hep/uproot5/pull/999#discussion_r1377699608\r\n\r\nThis PR took a long time to finish mainly due to an intermittent error in the CI. After much testing I am still not sure what the cause of this is, but I think it has to do with `s3fs` specifically. I could not reproduce this error outside of pytest running on the uproot repo, I even created another repository to test this but could not reproduce it using the same pytest code and uproot version. Since it's proven so hard to trigger and I don't think this PR causes this error (since it can also be reproduced on the main branch of uproot), I think we can merge this PR and I will try to fix / understand the error in another PR where I enable the `s3fs` tests.\r\n\r\nThe code I used to reproduce this:\r\n\r\n```\r\ndef test_open_fsspec_s3_issue():\r\n    fs, path = fsspec.core.url_to_fs(\"s3://pivarski-princeton/pythia_ppZee_run17emb.picoDst.root\")\r\n    # fs, path = fsspec.core.url_to_fs(\"github://scikit-hep:scikit-hep-testdata@v0.4.33/src/skhep_testdata/data/uproot-issue121.root\")\r\n    # fs, path = fsspec.core.url_to_fs(\"https://github.com/scikit-hep/scikit-hep-testdata/raw/main/src/skhep_testdata/data/uproot-issue121.root\")\r\n    data = fs.cat_file(path, start=0, end=100)\r\n\r\n    url = \"s3://pivarski-princeton/pythia_ppZee_run17emb.picoDst.root:PicoDst\"\r\n\r\n    for handler in [\r\n        uproot.source.s3.S3Source,\r\n        uproot.source.s3.S3Source,\r\n        uproot.source.s3.S3Source,\r\n        uproot.source.s3.S3Source,\r\n        uproot.source.s3.S3Source,\r\n        uproot.source.s3.S3Source,\r\n    ]:\r\n        with uproot.open(\r\n                url,\r\n                anon=True,\r\n                handler=handler,\r\n        ) as f:\r\n            data = f[\"Event/Event.mEventId\"].array(library=\"np\")\r\n            assert len(data) == 8004\r\n```\r\n\r\nNotice in the start where I have defined multiple pairs of `fs, path` then I get some bytes from the file. Only when I run this line with the `s3` backend it produces the error. Using a sync-only backend such as `github` or another async one such as `https` does not cause the error to trigger, so this is why I think `s3fs` is the culprit. I disabled the test that used `s3fs` and the error also disappeared.",
  "closed_at":"2023-10-31T14:10:28Z",
  "comments":2,
  "created_at":"2023-10-19T03:55:42Z",
  "draft":false,
  "id":1951148467,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dN0ra",
  "number":999,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-31T14:10:28Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: use only loop executor for `fsspec` source",
  "updated_at":"2023-10-31T15:06:39Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR aims to add a writing feature for hist derived profiles; As requested in #908 \r\n",
  "closed_at":"2024-01-18T16:37:42Z",
  "comments":3,
  "created_at":"2023-10-19T10:43:35Z",
  "draft":false,
  "id":1951861761,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dQP9M",
  "number":1000,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-18T16:37:42Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: support for writing hist derived profiles",
  "updated_at":"2024-01-18T16:37:43Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"We were passing arguments to the tasks as positional, this makes some of the fsspec sources behave differently because they may have different number / order of arguments.\r\n\r\nI also modified the definitions to keep the same as `concurrent.futures.ThreadPoolExecutor`.\r\n\r\nI noticied after debugging https://github.com/scikit-hep/uproot5/pull/999 failure.",
  "closed_at":"2023-10-19T20:05:25Z",
  "comments":0,
  "created_at":"2023-10-19T18:39:02Z",
  "draft":false,
  "id":1952851028,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dTkg8",
  "number":1001,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-19T20:05:25Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: update the executor submit interface to take keyword arguments and be compatible with `concurrent.futures.ThreadPoolExecutor`",
  "updated_at":"2023-10-19T20:05:27Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Added pretty formatters for yaml and toml after accidentaly formatting a file in another PR. The configuration should roughly follow our conventions and it can be further tuned.\r\n\r\nI did not run the formatter in all files in order not to pollute the contribution history, I think the pre-commit bot should do it if possible.",
  "closed_at":"2023-10-19T20:30:19Z",
  "comments":0,
  "created_at":"2023-10-19T19:40:28Z",
  "draft":false,
  "id":1952941527,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dT4de",
  "number":1002,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-19T20:30:19Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: add pre-commit formatters for toml and yaml",
  "updated_at":"2023-10-20T01:14:50Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Fixes #928 and #975",
  "closed_at":"2024-01-18T16:14:59Z",
  "comments":1,
  "created_at":"2023-10-20T06:06:44Z",
  "draft":false,
  "id":1953598082,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dWIKJ",
  "number":1004,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-18T16:14:59Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add the ability to read RNTuple alias columns",
  "updated_at":"2024-01-19T14:32:52Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/psf/black-pre-commit-mirror: 23.9.1 \u2192 23.10.1](https://github.com/psf/black-pre-commit-mirror/compare/23.9.1...23.10.1)\n- [github.com/astral-sh/ruff-pre-commit: v0.0.292 \u2192 v0.1.4](https://github.com/astral-sh/ruff-pre-commit/compare/v0.0.292...v0.1.4)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-11-09T14:23:10Z",
  "comments":0,
  "created_at":"2023-10-23T19:43:28Z",
  "draft":false,
  "id":1957903163,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dkdfm",
  "number":1005,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-09T14:23:10Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-11-09T14:23:11Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"In #976, the parsing that determines where a colon is between a file name and an object path was changed incorrectly: a basic (and important) use-case fails:\r\n\r\n```python\r\n>>> import uproot\r\n>>> tree = uproot.open(\"00376186-543E-E311-8D30-002618943857.root:Events\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/reading.py\", line 143, in open\r\n    file = ReadOnlyFile(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/reading.py\", line 584, in __init__\r\n    Source, file_path = uproot._util.file_path_to_source_class(\r\n  File \"/home/jpivarski/irishep/uproot5/src/uproot/_util.py\", line 508, in file_path_to_source_class\r\n    raise ValueError(f\"URI scheme not recognized: {file_path}\")\r\nValueError: URI scheme not recognized: 00376186-543E-E311-8D30-002618943857.root:Events\r\n```\r\n\r\nbut\r\n\r\n```python\r\n>>> tree = uproot.open(\"/tmp/00376186-543E-E311-8D30-002618943857.root:Events\")\r\n```\r\n\r\npasses. The failing case is a relative local path; the successful case is an absolute local path.\r\n\r\nI verified that 6b1952ea572471c7b64da3650566d6ce7ff6a4bc (just before #976) passes but fdcbb8e94aea734390ffe6babc9df380e97bf117 (just after #976) fails.",
  "closed_at":"2023-10-24T23:13:23Z",
  "comments":0,
  "created_at":"2023-10-24T15:59:52Z",
  "id":1959588110,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss50zPEO",
  "number":1006,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"closed",
  "state_reason":"completed",
  "title":"File-object separator colon parsing PR #976 must be fixed before the next release",
  "updated_at":"2023-10-24T23:13:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Fixes https://github.com/scikit-hep/uproot5/issues/1006.\r\n\r\nAdd additional tests to cover more cases.",
  "closed_at":"2023-10-24T23:13:22Z",
  "comments":0,
  "created_at":"2023-10-24T19:02:44Z",
  "draft":false,
  "id":1959897309,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5drLTd",
  "number":1007,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-24T23:13:22Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: url and object splitting for local files",
  "updated_at":"2023-10-24T23:13:23Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"When a TTree has zero elements, calling `array` when the branch is of type `std::vector<std::vector<T>>` raises KeyError.\r\nWhen the branch type is simpler, it just returns an empty array as expected.\r\n<details><summary>Stack trace</summary>\r\n<p>\r\n\r\n```python\r\nKeyError                                  Traceback (most recent call last)\r\n/workspaces/cmssw/analyzer/uproot-bug.ipynb Cell 2 line 1\r\n----> [1]uproot.open(\"empty-tree.root\")[\"tree/br\"].array()\r\n\r\nFile /opt/miniforge3/envs/supercls-analyzer/lib/python3.12/site-packages/uproot/behaviors/TBranch.py:1821, in TBranch.array(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library, ak_add_doc)\r\n   1818                 ranges_or_baskets.append((branch, basket_num, range_or_basket))\r\n   1820 interp_options = {\"ak_add_doc\": ak_add_doc}\r\n-> 1821 _ranges_or_baskets_to_arrays(\r\n   1822     self,\r\n   1823     ranges_or_baskets,\r\n   1824     branchid_interpretation,\r\n   1825     entry_start,\r\n   1826     entry_stop,\r\n   1827     decompression_executor,\r\n   1828     interpretation_executor,\r\n   1829     library,\r\n   1830     arrays,\r\n   1831     False,\r\n   1832     interp_options,\r\n   1833 )\r\n   1835 _fix_asgrouped(\r\n   1836     arrays,\r\n   1837     expression_context,\r\n   (...)\r\n   1841     ak_add_doc,\r\n   1842 )\r\n   1844 if array_cache is not None:\r\n\r\nFile /opt/miniforge3/envs/supercls-analyzer/lib/python3.12/site-packages/uproot/behaviors/TBranch.py:3051, in _ranges_or_baskets_to_arrays(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays, update_ranges_or_baskets, interp_options)\r\n   3047     # check for CannotBeAwkward errors on the main thread before reading any data\r\n   3048     if isinstance(library, uproot.interpretation.library.Awkward) and isinstance(\r\n   3049         interpretation, uproot.interpretation.objects.AsObjects\r\n   3050     ):\r\n-> 3051         branchid_to_branch[cache_key]._awkward_check(interpretation)\r\n   3053 hasbranches._file.source.chunks(ranges, notifications=notifications)\r\n   3055 def replace(ranges_or_baskets, original_index, basket):\r\n\r\nKeyError: '7bbc7f8a-7332-11ee-bbd0-020011acbeef:/tree;1:nested(1)'\r\n``` \r\n</p>\r\n</details> \r\n\r\n\r\nA ROOT macro to build an empty TTree\r\n```c++\r\nTFile f(\"empty-tree.root\", \"recreate\");\r\nTTree tree(\"tree\", \"Tree\");\r\nint simple;\r\ntree.Branch(\"simple\", &simple);\r\nstd::vector<std::vector<int>> nested;\r\ntree.Branch(\"nested\", &nested);\r\ntree.Write();\r\n``` \r\nThe corresponding reproducer :\r\n```python\r\nimport uproot\r\nuproot.open(\"empty-tree.root\")[\"tree/simple\"].array()\r\nuproot.open(\"empty-tree.root\")[\"tree/nested\"].array()\r\n``` \r\n",
  "closed_at":"2023-10-26T14:06:37Z",
  "comments":1,
  "created_at":"2023-10-25T12:34:41Z",
  "id":1961319056,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss5051qQ",
  "number":1008,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Calling array() with branch type std::vector<std::vector<T>> on TTree with zero elements raises KeyError",
  "updated_at":"2023-10-26T14:06:38Z",
  "user":"U_kgDOBhn17g"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Use `fsspec` to get the list of supported uri schemes. Direct the user to this list in case of an invalid scheme.\r\n\r\n**Changes**: paths such as `file.root://object-with-too-many-slashes` will now throw a `ValueError` saying that the `file.root://` scheme is not in the list of valid schemes.",
  "closed_at":"2023-10-25T20:38:09Z",
  "comments":2,
  "created_at":"2023-10-25T16:11:51Z",
  "draft":false,
  "id":1961780062,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dxhGb",
  "number":1009,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-25T20:38:09Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: improve uri scheme parsing with list of available schemes from `fsspec`",
  "updated_at":"2023-10-25T20:43:28Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Add a local http server pytest fixture. The http server is the python built-in but the request handler comes from https://github.com/danvk/RangeHTTPServer since the python one does not handle range requests. This handler also supports multipart range header requests!\r\n\r\nThe current implementation serves files from the `skhep_testdata` cache directory and downloads them on demand (only once) via calls to `skhep_testdata.data_path`.\r\n\r\nThere is an open issue to add this to python: https://github.com/python/cpython/issues/86809.\r\n\r\nI replaced most of the previous instances of remote http access by this local server but not all of them.\r\n\r\nThis PR also enables back some tests that were marked as skip due to network issues.",
  "closed_at":"2023-10-26T16:58:47Z",
  "comments":1,
  "created_at":"2023-10-25T17:00:36Z",
  "draft":false,
  "id":1961855244,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5dxxZU",
  "number":1010,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-10-26T16:58:47Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: local http server for tests",
  "updated_at":"2023-10-26T16:58:48Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR doesn't fix the underlying cause of this error in pytest but it makes it go away from the CI.\r\n\r\n- Add back previosly skipped `S3Source` tests. There is an issue with the test that checks for exception when the file does not exist: many times this throws a timeout that doesn't go away with retries, I will just add the timeout exceptions to the list of valid exceptions.\r\n- Add the infamous `s3fs` test but only on python 3.11 and above. In older python version we have this mysterious error.\r\n- (named some older test functions that were matching the substring \"s3\" by mistake and was slightly annoying).\r\n\r\nI'm 90% confident that this mysterious error won't cause a problem when using uproot with s3 and fsspec but it's the reason I didn't set it as default in https://github.com/scikit-hep/uproot5/pull/1023. Now that I think about it, I don't think it was a good idea to not set the s3 default to s3fs. It probably works fine and doing so will probably let us know if this is the case, so I will do it (in another PR).",
  "closed_at":"2023-11-16T20:47:19Z",
  "comments":2,
  "created_at":"2023-10-31T13:09:51Z",
  "draft":false,
  "id":1970441233,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5eO0by",
  "number":1012,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-16T20:47:19Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: s3fs pytest unraisable exception",
  "updated_at":"2023-11-16T20:47:20Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR adds an ssh server only to the ubuntu build of the CI to test accessing files over ssh via fsspec.\r\n\r\nAfter testing it I realised that `sshfs` does not implement some interface required by uproot for this to work (`cat_file`) so it raises a `NotImplementedError`. I'm not sure if this is just not implemented and can be implemented or if there is a limitation why this has not been implemented (you cannot request a chunk of a file, you need to request it all?).\r\n\r\nWe could handle each `fsspec` case separately having a fallback in case some interface is not implemented, but I think it would complicate things too much.\r\n\r\nAfter writing this PR I think it makes sense to raise a more detailed exception if fsspec does not implemented the required interface, so the user can post an issue in the corresponding fsspec repo. `fsspec` probably has a way to list the available methods (or it should!)",
  "closed_at":"2023-11-02T15:22:53Z",
  "comments":2,
  "created_at":"2023-10-31T14:32:08Z",
  "draft":false,
  "id":1970607863,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5ePZHO",
  "number":1013,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-02T15:22:53Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: testing sshfs with local ssh server",
  "updated_at":"2023-11-02T15:22:54Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"After https://github.com/scikit-hep/uproot5/pull/1013 I opened an issue in fsspec https://github.com/fsspec/filesystem_spec/issues/1411 to discuss the issue of not having `cat_files` as part of the interface for the ssh backend.\r\n\r\nI realised that I was using the wrong package: `sshfs` which is a third party implementation of an ssh backend (to my defense `s3fs` is the name for the `fsspec` `s3` backend...). `fsspec` already has an ssh backend that can be used if the `paramiko` package is installed. And it implements the `cat_file` interface so it works!\r\n\r\nThis PR updates the tests to reflect this (now the tests actually access the file over ssh).\r\n\r\n**Minor feature**:\r\n\r\nI noticed that if the fsspec handler was not being explicitly specified the ssh would faild because it would default to the old sources. I modified the behaviour so that instead of failing when an scheme is not recognized by the old sources, it will now use fsspec instead (if possible). When fsspec becomes default this can be removed.",
  "closed_at":"2023-11-03T16:47:27Z",
  "comments":0,
  "created_at":"2023-11-02T17:59:02Z",
  "draft":false,
  "id":1974789043,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5edkr-",
  "number":1014,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-03T16:47:27Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: use `paramiko` for ssh instead of `sshfs`",
  "updated_at":"2023-11-03T16:47:29Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Add test for more fsspec backends: memory, zip, tar.\r\n\r\nI also modifed the github uri test to run but skip if the server returns 403 error (too many requests) this way it's tested some of the time.\r\n\r\nProbably nobody will ever use uproot to open `.root.zip` files but in any case it's a good test to check the parsing of complex uris.\r\n\r\nFrom adding this tests I noticed that the `uproot._util.file_object_path_split` fails when chaining fsspec protocols on windows paths. I modified the function so it would work (and added additional tests).",
  "closed_at":"2023-11-07T23:46:50Z",
  "comments":0,
  "created_at":"2023-11-02T21:47:25Z",
  "draft":false,
  "id":1975089058,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5eemD4",
  "number":1015,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-07T23:46:50Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: cover more fsspec backends",
  "updated_at":"2023-11-07T23:46:51Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR adds writing of root files via fsspec which allows writing files to some interesting backends such as ssh, memory, s3, etc. Some are not yet supported such as http(s). In principle all the fsspec backends that support \"wb\" mode should work fine.\r\n\r\nIf the backend requires some special option, it can be passed in the `uproot.recreate` call such as `uproot.recreate(\"ssh:///tmp/file.root\", host=\"localhost\", user=\"user\")`.\r\n\r\nThis is a basic implementation. File is writen on close.\r\n\r\nThe implementation is as follows:\r\n- If the path to write is a file-like object, behaviour is as before.\r\n- If it is a string, it will check if it has some scheme. If it doesn't behavour is as before.\r\n- If it does have some scheme, it will use fsspec to create a file-like object and produce a sink from it, in a similar way to passing a file-like object to the `uproot.recreate` method. The sink will hold a reference to the object and is responsible for closing it.\r\n\r\nMinor features:\r\n- When writing to a local path, the parent directories of the file will be created if they don't exist instead of throwing an exception.\r\n\r\nAs mentioned this is a basic writer using fsspec. When PR becomes a requirement I will refactor the sink class and the surrounding code to make this cleaner.",
  "closed_at":"2023-11-15T20:44:50Z",
  "comments":7,
  "created_at":"2023-11-06T18:00:25Z",
  "draft":false,
  "id":1979741162,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5euGiD",
  "number":1016,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-15T20:44:50Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: basic fsspec writing",
  "updated_at":"2023-11-16T19:50:41Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"NONE",
  "body":"Currently our collaboration keeps part of data in a TTree holding CalibEvent class type objects. The CalibEvent class holds its member in std::list<CalibChannel>, and std::list is not parsed by uproot5. It is not easy to update the software from our side since it's used by many packages. \r\n\r\nIf uproot5 can add support for std::list it would be nice. If there is not enough manpower I'd like to help, and some hints would be very useful where to start.",
  "closed_at":null,
  "comments":1,
  "created_at":"2023-11-10T02:18:30Z",
  "id":1986723254,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss52av22",
  "number":1017,
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "state":"open",
  "state_reason":null,
  "title":"support for std::list",
  "updated_at":"2024-02-09T20:59:10Z",
  "user":"MDQ6VXNlcjMyMDU5Mzg="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"After doing some benchmarks for the http and fsspec sources, I noticed a significant difference between my \"requests-only\" implementation and using uproot http source. Mine was correctly doing the multipart request while uproot wasn't. After some debugging I found the reason why: uproot separates the byte ranges with a \", \" (comma + space) while I was just using a single comma. This space in the header apparently results in some servers not responding as expected.\r\n\r\nAt first I though that this failure in the request was due to the significantly large header (just adding the space results in 150 additional characters when retrieving 151 tbaskets as my use-case). But then I realised that the server just didn't understand the request if it had the space.\r\n\r\nI am using the following endpoint: `https://ncsmith.web.cern.ch/rootio/Run2012B_DoubleMuParked.root`\r\n\r\nNot using space: `curl -H \"Range: bytes=0-100,200-300\" https://ncsmith.web.cern.ch/rootio/Run2012B_DoubleMuParked.root --output /tmp/output`. Works fine.\r\n\r\nUsing space: `curl -H \"Range: bytes=0-100, 200-300\" https://ncsmith.web.cern.ch/rootio/Run2012B_DoubleMuParked.root --output /tmp/output`. Doesn't work, server never replies.\r\n\r\nIn this case uproot doesn't hang and will fallback to not use multipart (after the request times out). This may have caused uproot to work slower as expected since we were not aware that it was falling back in some cases. I am using a cern server here (@nsmith- knows the details of the server better than I do).\r\n\r\nI could not clear information about how the actual \"Range\" header should be formatted. In https://svn.apache.org/repos/asf/labs/webarch/tags/draft-fielding-http/draft00/p5-range.html it appears without the space, but in https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests it appears with the space...\r\n\r\nThis PR removes the space from the header, which may result in some servers not understanding the request, but removing the space is necessary for other servers to understand it. Removing the space has some advantages: the request will be smaller (which may be the differece between accepting it or not for some servers that have a header length limit).\r\n\r\nWe could also handle both cases: right now a multipart range request is sent, if it fails we assume server doesn't support multipart. We could send two requests instead (one with space, one without) and if both fails, assume multipart is not supported).\r\n\r\n@jpivarski please let me know what do you prefer:\r\n1 - Request with space in range, then without space in range, then fallback\r\n2 - Request without space in range, then with space in range, then fallback\r\n3 - Request without space in range, then fallback\r\n4 - Request with space in range, then fallback (current)\r\n\r\nIf we want to support servers that can only parse the comma-only separator and servers that can only parse the comma and space separator then we should do (2 -). This would however slightly increase the time to trigger the fallback when multipart is not supported.\r\n\r\nIf we assume there are no servers that do not allow the comma-only separator (or we just don't care to support them), then we should do (3 -). I would be inclined to choose this at the moment.",
  "closed_at":"2023-11-13T15:46:26Z",
  "comments":4,
  "created_at":"2023-11-13T00:06:53Z",
  "draft":false,
  "id":1989647156,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fPwGa",
  "number":1018,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-13T15:46:26Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: modify how multipart bytes header is built (no space) on http source",
  "updated_at":"2023-11-13T15:46:27Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/psf/black-pre-commit-mirror: 23.10.1 \u2192 23.11.0](https://github.com/psf/black-pre-commit-mirror/compare/23.10.1...23.11.0)\n- [github.com/astral-sh/ruff-pre-commit: v0.1.4 \u2192 v0.1.6](https://github.com/astral-sh/ruff-pre-commit/compare/v0.1.4...v0.1.6)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-11-28T21:49:08Z",
  "comments":0,
  "created_at":"2023-11-13T19:46:39Z",
  "draft":false,
  "id":1991360023,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fVmfi",
  "number":1019,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-28T21:49:08Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-11-28T21:49:09Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR does the following refactoring:\r\n\r\n- Remove old comments related to Python2\r\n- Remove `lzma` helper function\r\n- Remove `isstr` helper function",
  "closed_at":"2023-11-14T17:05:41Z",
  "comments":0,
  "created_at":"2023-11-13T21:28:11Z",
  "draft":false,
  "id":1991514635,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fWIaJ",
  "number":1020,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-14T17:05:41Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: replace some old code (python 2)",
  "updated_at":"2023-11-14T17:05:42Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Notice that this PR merges into `main-fsspec` not into `main`. The `main-fsspec` will exist along main for a while. I will make a PR to merge `main-fsspec` into `main`.",
  "closed_at":"2023-11-14T22:23:39Z",
  "comments":8,
  "created_at":"2023-11-14T16:28:02Z",
  "draft":false,
  "id":1993103658,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fbhU8",
  "number":1021,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-14T22:23:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: add fsspec as required dependency",
  "updated_at":"2023-11-14T22:23:40Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"`main-fsspec` will exist alongside `main` and release candidate distributions will be produced from this branch until it's deemed ready to merge into `main`. It will be merged without squashing. (changes from `main` should be rabased instead of merged in order to avoid merge commits in `main` since we're not squashing).\r\n",
  "closed_at":"2023-12-13T23:56:43Z",
  "comments":0,
  "created_at":"2023-11-14T22:26:58Z",
  "draft":false,
  "id":1993681498,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fdgGp",
  "number":1022,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-13T23:56:43Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: fsspec as required dependency",
  "updated_at":"2023-12-13T23:56:44Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"- Removes support for the `*_handler` arguments (`s3_handler`, `file_handler`, etc.) which were already on a deprecation notice.\r\n\r\n- Sets the fsspec source as default for all cases except:\r\n    - file-like objects \r\n    - s3 due to https://github.com/scikit-hep/uproot5/pull/1012 (if set as default pytest will fail)",
  "closed_at":"2023-11-15T22:33:42Z",
  "comments":3,
  "created_at":"2023-11-14T23:16:06Z",
  "draft":false,
  "id":1993735372,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fdr80",
  "number":1023,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-15T22:33:42Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: set fsspec as default source",
  "updated_at":"2023-11-16T09:47:35Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"When reviewing tests I noticed the `uproot.source.S3.S3Source` has some issues not catched in the current test.\r\n\r\n- Test was checking for a generic exception instead of a `FileNotFoundError`\r\n- class `repr` failed due to not having correctly set `self._file_path`\r\n- default options were not correctly loaded\r\n\r\nI also added a skip to the test for failure (file not found) which apparently is causing the CI to fail (https://github.com/scikit-hep/uproot5/pull/1012). Not sure if the error comes from the `s3.S3Source` or `s3fs` source but it's definitely s3 related.",
  "closed_at":"2023-11-15T21:02:21Z",
  "comments":0,
  "created_at":"2023-11-15T00:51:25Z",
  "draft":false,
  "id":1993825832,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fd_xP",
  "number":1024,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-15T21:02:21Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: s3 source options and repr",
  "updated_at":"2023-11-15T21:02:22Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"This comes from https://github.com/scikit-hep/uproot5/pull/1016#pullrequestreview-1730396600, but I've copied it here to make it a real issue.\r\n\r\n@lobis:\r\n\r\n> Right now I like this solution (not 100% sure it's going to work though and not implemented in this PR):\r\n> \r\n> - Uproot can only be used to open files with `.root` extension.\r\n> - Object is specified after the root file in the urlpath (not necessarily at the end). For example: `            \"simplecache::zip://uproot-issue121.root:Events/MET_pt::file:///tmp/pytest-of-runner/pytest-0/test_fsspec_zip0/uproot-issue121.root.zip\"`. In this case the object is `Events/MET_pt`.\r\n> - Find the object by searching for a filename ending in `.root` followed by single `:`. (Will this be robust enough?).\r\n\r\n@jpivarski:\r\n\r\n> We can't assume that all ROOT files end with a `.root` extension, but we _could_ limit the applicability of the colon-parsing to just those files that do. (The colon-parsing is supposed to be a convenience. If you have a weird situation\u2014a ROOT file not ending in `.root`\u2014you don't deserve a convenience!)\r\n> \r\n> The rule for what we do has to be simple enough to communicate and hopefully guess or intuit. Although the rules are complicated now, they're made that way to correspond to intuition. But how about the following?\r\n> \r\n> 1. If the string contains `.root:`, the colon of the _last_ `.root:` is used to split between URI-path and object-path.\r\n> 2. Otherwise, the whole string is interpreted as a URI-path, every time.\r\n> \r\n> This breaking change would have to go in when the minor version changed. Ideally, we would need to have warned users about this ahead of time, but I don't see a way to do that.\r\n> \r\nThe above would not remove the existing rules that `pathlib.Path` is entirely interpreted as a URI-path, every time, and the `{\"uri-path\": \"object-path\"}` syntax would still be recognized.\r\n\r\n@lobis:\r\n\r\n> I agree with those rules, they are simple enough and should be robust (we'll see if they can handle all cases, thankfully you cannot name a windows drive \".root\"...).\r\n> \r\n> - Find `.root` followed by single `:` (but not by `::`!)\r\n> - There shouldn't be any restriction to object paths except they cannot contain `:` (in that case use the dict notation).\r\n> - I think it makes more sense to apply the rule to the first `.root:` instead of the last given how fsspec chains urls, but I cannot think of any reasonable case where it would trigger more than once (a zip file having a .root extension instead of .zip...). In this case for instance matching the first appearance would make it work.\r\n> \r\n> In any case I will make a separate PR for this into `main-fsspec` (which will be live next minor version).\r\n> \r\n> Otherwise do you @nsmith- @jpivarski have any other comments regarding this PR? I'm not planning on further changes.\r\n\r\nI'll add that the reason I suggested the _last_ `.root:` is because it might be part of a directory name. If someone's working on a system with a strangely named directory, there's not a lot they can do about it (manually creating symlinks is more work than I'm considering reasonable). Since `:` aren't allowed to appear in object-paths (for the sake of colon-parsing), it's the last colon that matters.",
  "closed_at":"2023-11-16T18:53:00Z",
  "comments":1,
  "created_at":"2023-11-15T19:57:33Z",
  "id":1995470191,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss528HVv",
  "number":1025,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Simplify rule for colon parsing between URI-path and object-path",
  "updated_at":"2023-11-16T18:53:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Based on https://pyodide.org/en/stable/development/building-and-testing-packages.html and https://github.com/numpy/numpy/blob/main/.github/workflows/emscripten.yml.\r\n\r\n(Currently doesn't work since awkward-cpp is not available in the right version but it'll be fixed soon).\r\n\r\n- https://github.com/pyodide/pyodide/pull/4298\r\n\r\nThis action will build the uproot wheel and run the tests using nodejs.\r\n\r\nI will label and skip those tests that don't make sense to run on the browser (local files, etc.).",
  "closed_at":null,
  "comments":2,
  "created_at":"2023-11-15T20:21:39Z",
  "draft":true,
  "id":1995505080,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fjtZZ",
  "number":1026,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"test: Emscripten build and test on GitHub Actions",
  "updated_at":"2024-01-22T11:56:15Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"In this PR I review the skipped tests due to \"flakiness\". They are all xrootd tests.\r\n\r\nAfter removing the skip some of them failed for various reasons, I made changes so tests pass. It's not the cleanest fix but this xrootd source is going to be replaced by fsspec (atleast for the default behaviour).\r\n\r\nOcasionaly some test will fail due to server issues, we should retry those. I think raising a specific error when this happens and configuring pytest to retry on this error is the way to go (this is what we done at the moment, but it doesn't work since we don't see `requests` errors). I updated the `http` source to throw a `http.client.HTTPException` and xrootd to throw a `TimeoutError`. This currently works for the ocasional xrootd and s3 fails. \r\n\r\nProbably these errors can be improved (not sure if it's correct to throw a `TimeoutError`). Before this changes we were throwing an `OSError` which imho is too generic to do retries on.",
  "closed_at":"2023-11-16T18:51:22Z",
  "comments":0,
  "created_at":"2023-11-15T22:33:16Z",
  "draft":false,
  "id":1995686008,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fkU7-",
  "number":1027,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-16T18:51:22Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: review skipped tests (networking timeouts)",
  "updated_at":"2023-11-16T18:51:23Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Closes #1025 \r\n\r\nBonus features:\r\n- Able to parse object paths with colons in them e.g. file.root:dir/tree:with:colon -> file.root, dir/tree:with:colon\r\n\r\nLimitations:\r\n- Uses `.root` file extension as anchor, it's only able to split objects if the file has exactly a `.root` extension. This is technically a breaking change.",
  "closed_at":"2023-11-16T18:51:04Z",
  "comments":1,
  "created_at":"2023-11-16T04:33:25Z",
  "draft":false,
  "id":1996038045,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5flgMH",
  "number":1028,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-16T18:51:04Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: simplify object path split",
  "updated_at":"2023-11-21T20:21:44Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"The changes introduced in #1016 (as tested via an installation of `uproot5` at 28b0f7bd) break  the use of `pathlib.Path` when writing files:\r\n\r\n```python\r\nimport pathlib\r\nimport uproot\r\n\r\nuproot.recreate(pathlib.Path(\"f.root\"))\r\n```\r\nresults in\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"[...]/debug.py\", line 4, in <module>\r\n    uproot.recreate(pathlib.Path(\"f.root\"))\r\n  File \"[...]/uproot5/src/uproot/writing/writable.py\", line 148, in recreate\r\n    sink = _sink_from_path(file_path, **storage_options)\r\n  File \"[...]/uproot5/src/uproot/writing/writable.py\", line 80, in _sink_from_path\r\n    return uproot.sink.file.FileSink.from_object(file_path_or_object)\r\n  File \"[...]/uproot5/src/uproot/sink/file.py\", line 55, in from_object\r\n    raise TypeError(\r\nTypeError: writable file can only be created from a file path or an object\r\n\r\n    * that has 'read', 'write', 'seek', and 'tell' methods\r\n    * is 'readable() and writable() and seekable()'\r\n```\r\n\r\nWe ran into this via nightly tests in `pyhf` (cc @matthewfeickert) and `cabinetry`.\r\n\r\nThe above still works fine in 113c58c7.",
  "closed_at":"2023-11-16T20:29:59Z",
  "comments":0,
  "created_at":"2023-11-16T07:48:53Z",
  "id":1996264283,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss52_JNb",
  "number":1029,
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "state":"closed",
  "state_reason":"completed",
  "title":"New `fsspec` writing breaks for `pathlib.Path`",
  "updated_at":"2023-11-16T20:29:59Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Fixes https://github.com/scikit-hep/uproot5/issues/1029\r\n\r\nAdd new test",
  "closed_at":"2023-11-16T20:29:57Z",
  "comments":1,
  "created_at":"2023-11-16T19:07:21Z",
  "draft":false,
  "id":1997561764,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fqvZa",
  "number":1031,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-16T20:29:57Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: processing of `pathlib.Path` argument for writing",
  "updated_at":"2023-11-16T21:22:22Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This wasn't added in https://github.com/scikit-hep/uproot5/pull/1023 due to concerns regarding the pytest exception due to `s3fs` (https://github.com/scikit-hep/uproot5/pull/1012).\r\n\r\nThe issue is not understood yet but from my experience with this issue (it's readlly hard to reproduce) I don't think it's going to result in any problem. I think the issue comes from `s3fs` (but not 100% sure) as we are using this filesystem as any other but the issue only appears here. It'll probably be fixed on some newer release. Besides, this only appears on older version of python (in 3.11 or newer this doesn't appear) so I think it's safe to set the new s3 default to fsspec.\r\n\r\n- Increased version to `5.2.0rc1`\r\n- Fix xrootd pickle tests (were not being run before due to the parallel branch not being in sync with main, it should've tripped in another PR).",
  "closed_at":"2023-11-17T00:16:55Z",
  "comments":3,
  "created_at":"2023-11-16T20:09:44Z",
  "draft":false,
  "id":1997684205,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5frKr2",
  "number":1032,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-17T00:16:55Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: set `fsspec` (`s3fs`) as default handler for s3 paths",
  "updated_at":"2023-11-17T00:16:56Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"As discovered in https://github.com/scikit-hep/uproot5/pull/1032 the serialization of the fsspec source wasn't being done correctly.",
  "closed_at":"2023-11-16T23:35:56Z",
  "comments":1,
  "created_at":"2023-11-16T23:18:46Z",
  "draft":false,
  "id":1997952946,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5fsGLn",
  "number":1033,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-16T23:35:56Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: correct fsspec source serialization",
  "updated_at":"2023-11-16T23:35:57Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"- Make all writing go through fsspec (when requesting to write to a urlpath, if it's a file-like object, then fsspec doesn't apply).\r\n- When `uproot.create` fails due to a file already existing, a `FileExistsError` (derived from `OSError`) will be thrown instead of a generic `OSError`\r\n- Simplified how the source class is chosen based on the urlpath\r\n- Remove windows specific code and test (no longer makes sense imo). I tried to cover the same cases (uri in path) in new test functions in the fsspec writing tests. Since these tests write local files and also run in windows, they automatically parse the full uri with the drive letter without issue so no need to explicitly test it. https://github.com/scikit-hep/uproot5/issues/325\r\n\r\n**How urlpath strings are processed now:**\r\n\r\nUproot is only responsible for stripping the object if specified (under the `path:object` notation). Then the path is passed to fsspec. `fsspec` is responsible of correctly resolving the path).\r\n\r\nI updated the current tests to cover all cases in https://github.com/scikit-hep/uproot5/issues/325 regarding windows paths and all it's variations. I had to delete the older test file and include these tests in another file because the path resolution is now done by fsspec so we cannot test it before opening the file.\r\n\r\nAll previous cases appear to be covered by fsspec.\r\n\r\n**There is one breaking change however:**\r\n\r\nBefore this PR, uproot was decoding %-encoded urls when they had the `file://` scheme. Because now everything goes through fsspec it's not a good idea to distinguish between having a `file://` or not (fsspec will infer the scheme in case it's not present, which will correspond to `file://`).\r\n\r\nNow we are not decoding %-encoded urls, they are passed as they come to fsspec (and fsspec won't decode them either). **Now it is the user's responsibility to decode these urls should they want to**.\r\n\r\n",
  "closed_at":"2023-11-20T20:32:58Z",
  "comments":5,
  "created_at":"2023-11-17T03:54:30Z",
  "draft":false,
  "id":1998227869,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5ftAUG",
  "number":1034,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-20T20:32:58Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: fsspec for all non-object writing - %-encoded urls no longer decoded",
  "updated_at":"2023-11-20T20:32:59Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Described here: https://github.com/CoffeaTeam/coffea/pull/930.\r\n\r\nAt first this was treated as an fsspec issue introduced at some point during the integration. However I found out it's not fsspec related and it's reproducible by other multithreaded sources such as `uproot.source.file.MultithreadedFileSource` (when using more than one thread). I guess this is a race condition and it's more likely to happen with `fsspec` since it launches all requests at once.",
  "closed_at":"2023-11-24T20:27:07Z",
  "comments":1,
  "created_at":"2023-11-20T22:28:17Z",
  "id":2003121460,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss53ZTU0",
  "number":1035,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"multithreaded file source breaks interpretation",
  "updated_at":"2023-11-24T20:27:08Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"I added a test that is able to reproduce this.\r\n\r\nIt's not a fsspec issue, its a race condition triggered when using multiple threads (also with the `uproot.source.file.MultithreadedFileSource`).\r\n\r\nWhen a branch with subbranches is interpreted, the subbranches are submitted first. If this is the order in which they are interpreted it works fine. When using a multithreaded source, this order may not be maintained and it breaks.",
  "closed_at":"2023-11-24T20:27:07Z",
  "comments":2,
  "created_at":"2023-11-20T22:46:12Z",
  "draft":false,
  "id":2003142389,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5f9eXL",
  "number":1036,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-24T20:27:06Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: multithreaded file source breaks interpretation",
  "updated_at":"2023-11-24T20:27:07Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"plotly's graph is interactive, making it easier to see the data directly when analyzing it.\n```[tasklist]\n### Tasks\n```\n",
  "closed_at":null,
  "comments":2,
  "created_at":"2023-11-21T00:33:44Z",
  "id":2003242371,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss53Zw2D",
  "number":1037,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"You can import plots to plotly.",
  "updated_at":"2024-01-31T05:09:56Z",
  "user":"MDQ6VXNlcjczMDU4MDYy"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"Hi. So this might be more of an XRootD issue than an `upoort` one, but from the [\"Opening a file\" docs](https://uproot.readthedocs.io/en/latest/basic.html#opening-a-file) I'm not sure what the limits are so I thought I'd ask.\r\n\r\nI don't have a great public minimal reproducible example yet, so I'll just share the example that @ivukotic found today and sent me. In this example there is one (of many) ROOT files on the UChicago Analysis Facility that we're able to read using XRootD from inside of a Docker container (`sslhep/analysis-dask-base:latest`) that provides the base environment for the k8 pod that is serving the user a Jupyter Lab environment. This environment has [`uproot` `v5.1.2`](https://github.com/scikit-hep/uproot5/releases/tag/v5.1.2) and XRootD Python bindings in it but when we try to open the file in the following `test.py`\r\n\r\n```python\r\n# test.py\r\nimport importlib.metadata\r\n\r\nimport uproot\r\nimport XRootD\r\n\r\nprint(f\"uproot version: {uproot.__version__}\")\r\nprint(f\"XRootD Python bindings version: {importlib.metadata.version('XRootD')}\")\r\n\r\nxrootd_uri = \"root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1\"\r\nfile = uproot.open(xrootd_uri)\r\nfile\r\n```\r\n\r\nwe error out with\r\n\r\n```pytb\r\nValueError: too many colons in file path: root://xcache.af.uchicago.edu...\r\n```\r\n\r\nFull example here, but this requires the user to have ATLAS credentials here:\r\n\r\n```console\r\n$ docker run --rm -ti sslhep/analysis-dask-base:latest /bin/bash\r\nConfigured GCC from: /opt/lcg/gcc/11.2.0-8a51a/x86_64-centos7/bin/gcc\r\nConfigured AnalysisBase from: /usr/AnalysisBase/24.2.26/InstallArea/x86_64-centos7-gcc11-opt\r\nConfigured PyColumnarPrototype from: /usr/tools/PyColumnarPrototypeDemo/1.0.0/InstallArea/x86_64-centos7-gcc11-opt\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > kinit feickert@CERN.CH\r\nPassword for feickert@CERN.CH:\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > klist\r\nTicket cache: FILE:/tmp/krb5cc_1000\r\nDefault principal: feickert@CERN.CH\r\n\r\nValid starting     Expires            Service principal\r\n11/21/23 06:48:13  11/22/23 07:47:43  krbtgt/CERN.CH@CERN.CH\r\n\trenew until 11/26/23 06:47:43\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > xrdcp -f root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1  .\r\n[252.2MB/252.2MB][100%][==================================================][3.655MB/s]\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > file DAOD_PHYSLITE.34858087._000001.pool.root.1\r\nDAOD_PHYSLITE.34858087._000001.pool.root.1: ROOT file Version 62608 (Compression: 505)\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > cat test.py\r\nimport importlib.metadata\r\n\r\nimport uproot\r\nimport XRootD\r\n\r\nprint(f\"uproot version: {uproot.__version__}\")\r\nprint(f\"XRootD Python bindings version: {importlib.metadata.version('XRootD')}\")\r\n\r\nxrootd_uri = \"root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1\"\r\nfile = uproot.open(xrootd_uri)\r\nfile\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis > python test.py\r\nuproot version: 5.1.2\r\nXRootD Python bindings version: 5.4.3\r\nTraceback (most recent call last):\r\n  File \"/analysis/test.py\", line 10, in <module>\r\n    file = uproot.open(xrootd_uri)\r\n  File \"/venv/lib/python3.9/site-packages/uproot/reading.py\", line 126, in open\r\n    file_path, object_path = uproot._util.file_object_path_split(path)\r\n  File \"/venv/lib/python3.9/site-packages/uproot/_util.py\", line 314, in file_object_path_split\r\n    raise ValueError(f\"too many colons in file path: {path} for url {parsed_url}\")\r\nValueError: too many colons in file path: root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1 for url ParseResult(scheme='root', netloc='xcache.af.uchicago.edu:1094', path='//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1', params='', query='', fragment='')\r\n(venv) [bash][atlas AnalysisBase-24.2.26]:analysis >\r\n```\r\n\r\nCan you provide any details on if there's revisions to the naming scheme that we'll need to use or if there's something we did wrong?\r\n\r\n> Please attach a small ROOT file that reproduces the issue! If small and public, you can drag-and-drop it into the issue\u2014rename the extension to \"txt\" so that GitHub allows it. If large, you can put it on some large-file service (e.g. Dropbox). In general, we can't access XRootD URLs (most are not public).\r\n\r\nLet me work on this. Maybe @ivukotic can temporarily move the file to a public area?\r\n\r\ncc @alexander-held and @oshadura in the event that they came across similar issues with the [IRIs-HEP Analysis Grand challenge](https://github.com/iris-hep/analysis-grand-challenge/blob/65db0d69db962b0505e3bb97bb26ca3eb40c3e15/analyses/cms-open-data-ttbar/utils/file_input.py) before.",
  "closed_at":"2023-12-13T23:56:44Z",
  "comments":7,
  "created_at":"2023-11-21T06:25:47Z",
  "id":2003550362,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss53a8Ca",
  "number":1038,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"ValueError: too many colons in file path",
  "updated_at":"2023-12-13T23:56:44Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Adds test cases discussed in https://github.com/scikit-hep/uproot5/issues/1038.\r\n\r\nAdd pytest fixture to access local test files.",
  "closed_at":"2023-11-27T22:57:28Z",
  "comments":0,
  "created_at":"2023-11-21T16:43:54Z",
  "draft":false,
  "id":2004729451,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5gC4_I",
  "number":1039,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-27T22:57:28Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: improve path object split tests",
  "updated_at":"2023-11-27T22:57:29Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"MEMBER",
  "body":"> Another issue is opening more than one file:\r\n>\r\n> ```python\r\n> import uproot\r\n>\r\n> xc='root://xcache.af.uchicago.edu:1094//'\r\n> fname_data = xc+\"root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1\"\r\n> fname_dat1 = xc+\"root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/6c/67/DAOD_PHYSLITE.34858087._000002.pool.root.1\"\r\n>\r\n> tree_data = uproot.iterate(\r\n>     {fname_data: \"CollectionTree\"}, {fname_dat1: \"CollectionTree\"}\r\n> )\r\n> next(tree_data)  # trigger error\r\n> ```\r\n>\r\n> ```pytb\r\n> ValueError: cannot produce Awkward Arrays for interpretation AsObjects(Unknown_xAOD_3a3a_MissingETAssociationMap_5f_v1) because\r\n>\r\n>     xAOD::MissingETAssociationMap_v1\r\n>\r\n> instead, try library=\"np\" rather than library=\"ak\" or globally set uproot.default_library\r\n>\r\n> in file root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1\r\n> in object /CollectionTree;1:METAssoc_AnalysisMET\r\n> ```\r\n>\r\n> The same happens even if I try to open single file using iterator...\r\n\r\n_Originally posted by @ivukotic in https://github.com/scikit-hep/uproot5/issues/1038#issuecomment-1821190003_\r\n            \r\n\r\n@lobis as you started looking at this I thought I would open up a seperate Issue as these seem distinct enough. As a second example, following the setup in Issue #1038, that follows the [`uproot.iterate` docs example](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html#uproot-iterate) to some extent ([for a single file](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.HasBranches.html#iterate))\r\n\r\n```python\r\n# test.py\r\nimport uproot\r\n\r\nxc = \"root://xcache.af.uchicago.edu:1094//\"\r\nfile_name = (\r\n    xc\r\n    + \"root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1\"\r\n)\r\ntree_name = \"CollectionTree\"\r\nbranch_name = \"AnalysisTrigMatch_HLT_e20_lhvloose\"\r\n\r\nwith uproot.open(file_name) as read_file:\r\n    print(read_file[tree_name])\r\n    print(read_file[tree_name][branch_name])\r\n\r\n    _tree = read_file[tree_name]\r\n    tree_data = _tree.iterate([branch_name], step_size=100)\r\n    next(tree_data)  # trigger error\r\n```\r\n\r\nwhen executed\r\n\r\n```\r\npython test.py\r\n```\r\n\r\nerrors out with\r\n\r\n```pytb\r\n<TTree 'CollectionTree' (864 branches) at 0x7fe67d753f10>\r\n<TBranchElement 'AnalysisTrigMatch_HLT_e20_lhvloose' at 0x7fe67c13f3d0>\r\nTraceback (most recent call last):\r\n  File \"/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 2478, in _awkward_check\r\n    interpretation.awkward_form(self.file)\r\n  File \"/venv/lib/python3.9/site-packages/uproot/interpretation/objects.py\", line 111, in awkward_form\r\n    return self._model.awkward_form(self._branch.file, context)\r\n  File \"/venv/lib/python3.9/site-packages/uproot/model.py\", line 684, in awkward_form\r\n    raise uproot.interpretation.objects.CannotBeAwkward(\r\nuproot.interpretation.objects.CannotBeAwkward: DataVector<xAOD::TrigComposite_v1>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/analysis/test.py\", line 17, in <module>\r\n    next(tree_data)  # trigger error\r\n  File \"/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 1076, in iterate\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 3041, in _ranges_or_baskets_to_arrays\r\n    branchid_to_branch[cache_key]._awkward_check(interpretation)\r\n  File \"/venv/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\", line 2480, in _awkward_check\r\n    raise ValueError(\r\nValueError: cannot produce Awkward Arrays for interpretation AsObjects(Unknown_DataVector_3c_xAOD_3a3a_TrigComposite_5f_v1_3e_) because\r\n\r\n    DataVector<xAOD::TrigComposite_v1>\r\n\r\ninstead, try library=\"np\" rather than library=\"ak\" or globally set uproot.default_library\r\n\r\nin file root://xcache.af.uchicago.edu:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/data18_13TeV/df/a4/DAOD_PHYSLITE.34858087._000001.pool.root.1\r\nin object /CollectionTree;1:AnalysisTrigMatch_HLT_e20_lhvloose\r\n```\r\n\r\nI'm not sure if we need to be using the `xrootd_handler` option here, but that seems like it shouldn't be needed for the single file case(?).",
  "closed_at":"2023-11-22T06:31:22Z",
  "comments":3,
  "created_at":"2023-11-21T21:27:25Z",
  "id":2005165506,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss53hGXC",
  "number":1040,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"ValueError with uproot.iterate over xrootd",
  "updated_at":"2023-11-22T06:31:22Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":null,
  "closed_at":"2023-11-27T20:25:47Z",
  "comments":3,
  "created_at":"2023-11-22T20:40:29Z",
  "draft":false,
  "id":2007085076,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5gK8w4",
  "number":1042,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-27T20:25:47Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: use ruff to import annotations",
  "updated_at":"2023-11-27T20:25:48Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Hi,\r\n\r\nI noticed that serializing an object with root containing a `const std::string` member like:\r\n\r\n```cpp\r\nstruct Foo {\r\nconst std::string bar;\r\n};\r\n```\r\n\r\nproduces the following error:\r\n\r\n```\r\nValueError: invalid C++ type name syntax at char 6\r\n\r\n    const string\r\n----------^\r\nin file foobar.root\r\n```\r\n\r\nThe \"fix\" adds another explicit check for `const std::string`, not sure though whether this is the best solution.",
  "closed_at":"2023-11-28T21:30:07Z",
  "comments":3,
  "created_at":"2023-11-23T10:22:14Z",
  "draft":false,
  "id":2007897784,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5gNqT2",
  "number":1043,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-28T21:30:07Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: `const std::string` identification",
  "updated_at":"2023-11-28T21:30:07Z",
  "user":"MDQ6VXNlcjgwNTAyOTI="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"NONE",
  "body":"On the [anaconda.org/conda-forge](https://anaconda.org/conda-forge/uproot) the `uproot` version is outdated and points to v4.3.7.\r\n \r\nCould this be updated?\r\n\r\nThank you in advance.\r\n",
  "closed_at":"2024-01-18T15:04:11Z",
  "comments":9,
  "created_at":"2023-11-25T02:12:01Z",
  "id":2010436385,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss531NMh",
  "number":1044,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"conda-forge points to v4.3.7",
  "updated_at":"2024-01-18T15:04:27Z",
  "user":"MDQ6VXNlcjE5NTU2OTM0"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"It would be interesting to read files written using the [`rest-for-physics` framework](https://github.com/rest-for-physics/framework). Currently this is not possible as Uproot is unable to produce a valid interpretation of many branches. In principle, even though the `TTree` is writen using custom classes, the relevant branches are fully split so it should be possible to read them. They were readable in the past with uproot but this stopped working at some point (most likely due to changes in `rest-for-physics`).\r\n\r\nSome branches are readable as shown in the example below, while some are not. In particular I would be interested in reading the Geant4 data from branhes such as `TRestGeant4EventBranch/fTracks/fTracks.fHits.fY` in the `EventTree`.\r\n\r\nThe Geant4 data is stored in a branch named `TRestGeant4EventBranch` in the `EventTree` `TTree`. This branch is fully split. Using custom classes and dictionaries we replicated the hierarchical structure of a Geant4 simulation (Event > Track > Hit). [Here](https://github.com/rest-for-physics/geant4lib/blob/master/inc/TRestGeant4Track.h) is the source code for these classes.\r\n\r\nThis issue is related to https://github.com/scikit-hep/uproot5/issues/936.\r\n\r\nThe code snippet below uses the new `fsspec` url chain to open a zip file since GitHub doesn't allow uploading root files directly. It needs uproot `>= 5.2.0` (currently pre-release version).\r\n\r\n```\r\nzip_urlpath = \"https://github.com/scikit-hep/uproot5/files/12334414/simulation.anaylsis.zip\"\r\nroot_filename = \"simulation.anaylsis.root\"\r\ntree_name = \"EventTree\"\r\nwith uproot.open(f\"zip://{root_filename}:{tree_name}::{zip_urlpath}\") as tree:\r\n    print(tree.keys())\r\n\r\n    for key in [\"TRestGeant4EventBranch/TRestEvent/TObject/fUniqueID\",\r\n                \"TRestGeant4EventBranch/TRestEvent/fSubRunOrigin\"]:\r\n        branch = tree[key]\r\n        data = branch.array()\r\n\r\n    # other keys do not work (unknown interpretation)\r\n    for key in [\"TRestGeant4EventBranch/fPrimaryEnergies\", ]:\r\n        branch = tree[key]\r\n        data = branch.array()\r\n```\r\n\r\nI'll tag @jgalan in case he wants to follow this up.",
  "closed_at":null,
  "comments":0,
  "created_at":"2023-11-26T19:46:05Z",
  "id":2011178970,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss534Cfa",
  "number":1045,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"Reading `rest-for-physics` files",
  "updated_at":"2024-01-30T16:01:58Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"The current behavior of `uproot.open` and `uproot.dask` differs when dealing with files that are partially unreadable by `uproot`. In my concrete example, I am dealing with ATLAS PHYSLITE files. The following snippet:\r\n```python\r\nimport uproot\r\n\r\ntree = uproot.open({\"DAOD_PHYSLITE.34857549._000351.pool.root.1\": \"CollectionTree\"})\r\ntree[\"AnalysisElectronsAuxDyn.pt\"].array()\r\n```\r\nworks just fine to access this specific array. A similar version with Dask fails (before a `.compute()`):\r\n```python\r\ntree = uproot.dask({\"DAOD_PHYSLITE.34857549._000351.pool.root.1\": \"CollectionTree\"})\r\ntree[\"AnalysisElectronsAuxDyn.pt\"]\r\n```\r\nbecause parts of the file are not understandable to `uproot`:\r\n```pytb\r\nUnknownInterpretation: none of the rules matched\r\nin file DAOD_PHYSLITE.34857549._000351.pool.root.1\r\nin object /CollectionTree;1:xTrigDecisionAux./xTrigDecisionAux.xAOD::AuxInfoBase\r\n```\r\n\r\n@lgray pointed out that this is expected behavior and `coffea` removes branches to address this ([`_remove_not_interpretable`](https://github.com/CoffeaTeam/coffea/blob/729e79d982f4e45cf1bc1aee422f1e2ae4e96f39/src/coffea/nanoevents/factory.py#L29-L55)).\r\n\r\nWhat I would like to raise for discussion here is making the `uproot.dask` behavior match more closely that of `uproot.open`. As long as I only need data that `uproot` can read, the Dask interface should be able to supply it without too much additional effort for the user. Concretely that might mean for example:\r\n- automatically apply something like `_remove_not_interpretable`, possibly with an accompanying warning, or\r\n- provide a new keyword argument and accompany the `UnknownInterpretation` error with information of how to use it to achieve `_remove_not_interpretable`-like behavior.\r\n\r\nI do not know what the worst case scenario looks like with partially unreadable files: can this ever imply that the interpretation of the other (to `uproot` appearing as \"readable\") columns becomes wrong? If so, it is dangerous to automatically handle such files without the user potentially being aware of course.",
  "closed_at":null,
  "comments":3,
  "created_at":"2023-11-28T14:15:44Z",
  "id":2014541834,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss54E3gK",
  "number":1046,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"open",
  "state_reason":null,
  "title":"`uproot.dask` behavior for partially readable files",
  "updated_at":"2024-01-25T16:08:25Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":"While experimenting with ATLAS PHYSLITE files (we might be able to find a way to share something for debugging purposes, details to be figured out privately) I ran into a setup that does not make sense on paper: filling a histogram with a jagged array without flattening it. I did not at first realize that this is the case and the error message is somewhat misleading. Full details are available in https://gist.github.com/alexander-held/56e203690c0d7f67218a4c67d46c586f, for `uproot` this is using the current version of `main` (4112159).\r\n\r\nI am opening this issue to inquire if the error message for such a case can be improved.\r\n\r\nTwo errors are raised in my trace (top of the notebook):\r\n\r\n```pytb\r\nValueError: When changing to a larger dtype, its size must be a divisor of the total size in bytes of the last axis of the array.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n```\r\n\r\nand\r\n\r\n```pytb\r\nValueError: basket 323 in tree/branch /CollectionTree;1:METAssoc_AnalysisMETAux./METAssoc_AnalysisMETAux.jetLink has the wrong number of bytes (3318) for interpretation AsStridedObjects(Model_ElementLink_3c_DataVector_3c_xAOD_3a3a_Jet_5f_v1_3e3e__v1)\r\nin file DAOD_PHYSLITE.34857549._000351.pool.root.1\r\n```\r\n\r\nNeither of these to me sound obviously related to the actual issue and in particular the second one is confusing as I do not need to access this object at all.\r\n\r\nIn a setup without Dask (further down in the notebook), the error is more understandable:\r\n```pytb\r\nValueError: cannot convert to RegularArray because subarray lengths are not regular (in compiled code: https://github.com/scikit-hep/awkward/blob/awkward-cpp-26/awkward-cpp/src/cpu-kernels/awkward_ListOffsetArray_toRegularArray.cpp#L22)\r\n\r\nThis error occurred while calling\r\n\r\n    numpy.asarray(\r\n        <Array [[], [], [], ..., [], [9.61e+03]] type='163363 * var * float32'>\r\n        dtype = None\r\n    )\r\n```\r\n\r\nIn particular, in this case the trace also includes `boost_histogram` to suggest that the histogramming is a problem here, while in the Dask case the trace fully stays within uproot + Dask.\r\n\r\nIn both case (Dask or no Dask), flattening the array (commented out in the notebook) makes things work as expected.",
  "closed_at":"2024-01-25T16:40:25Z",
  "comments":4,
  "created_at":"2023-11-28T17:47:34Z",
  "id":2014982604,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss54GjHM",
  "number":1048,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Cryptic error message when filling histogram in the wrong way with Dask",
  "updated_at":"2024-01-25T16:40:26Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Adds @HaarigerHarald as a contributor for code.\n\nThis was requested by jpivarski [in this comment](https://github.com/scikit-hep/uproot5/pull/1043#issuecomment-1830768616)\n\n[skip ci]",
  "closed_at":"2023-11-28T21:23:18Z",
  "comments":0,
  "created_at":"2023-11-28T21:22:24Z",
  "draft":false,
  "id":2015339726,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5gmlRg",
  "number":1049,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-11-28T21:23:18Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"docs: add HaarigerHarald as a contributor for code",
  "updated_at":"2023-11-28T21:23:19Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"This PR pairs with https://github.com/dask-contrib/dask-awkward/pull/415. It adds a new `report` argument to `uproot.dask`. It causes the call to return two collections, the actual array of interest represented by the first collection, and a report collection as the second return.\r\n\r\nAs it stands if `report` is not `None` we create a report collection; it's designed to be computed simultaneously with whatever collection of the interest is. An example where we have two field access calls and then a `dak.max` call:\r\n\r\n```python\r\ndataset, report = uproot.dask({\"/path/to/files/*.root\": \"tree\"}, report=True)\r\nmeasurement = dak.max(dataset.thing1 + datasetset.thing2, axis=1)\r\nresult, report = dask.compute(measurement, report)\r\n```\r\nIf any of the files at that path end up as failure-to-read (raise an exception), the the `measurement` collection will have those partitions get computed to an empty array, so the `result` array object will be pieced together from a concatenation where some of the ingredients are empty, and the `report` array object will be an awkward array of length `dataset.npartitions`, (one element in the array per partition)\r\n\r\n---\r\n\r\nWe have some flexibility here with the `report=` argument. The `from_map` API in dask-awkward via https://github.com/dask-contrib/dask-awkward/pull/415 has 4 arguments that can steer the behavior here\r\n1. which exceptions should be absorbed to send back an empty array at that failed node\r\n2. which backend for the empty array (for now I think is is always \"cpu\")\r\n3. a callback function to run on nodes where the read was successful (this is an entry point for customizing the report for successful reads). (The default is `None`)\r\n4. a callback function to run on nodes where the read was a failure (this is an entry point for customizing the report for failed reads). (The default is construct a record array which hold information about the error and the function arguments passed to the read function that failed)\r\n\r\ncc @lgray ",
  "closed_at":"2023-12-08T15:26:50Z",
  "comments":16,
  "created_at":"2023-11-29T21:14:14Z",
  "draft":true,
  "id":2017425687,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5gttJy",
  "number":1050,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat(draft): add `report=` argument to `uproot.dask`; triggers report collection for failed reads",
  "updated_at":"2023-12-08T15:26:51Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [conda-incubator/setup-miniconda](https://github.com/conda-incubator/setup-miniconda) from 2 to 3.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/conda-incubator/setup-miniconda/releases\">conda-incubator/setup-miniconda's releases</a>.</em></p>\n<blockquote>\n<h2>Version 3.0.0</h2>\n<h3>Features</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/308\">#308</a> Update to node20</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/291\">#291</a> Add conda-solver option (defaults to libmamba)</li>\n</ul>\n<h3>Fixes</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/299\">#299</a> Fix condaBasePath when useBundled is false, and there's no pre-existing conda</li>\n</ul>\n<h3>Documentation</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/309\">#309</a> Switch to main branch based development</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/313\">#313</a> Specify team conda-incubator/setup-miniconda as codeowners</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/318\">#318</a> README: update actions in examples, add security section, similar actions</li>\n</ul>\n<h3>Tasks and Maintenance</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/307\">#307</a> Run dependabot against main branch and also update node packages</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/311\">#311</a> Bump actions/checkout from 2 to 4</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/310\">#310</a> Bump actions/cache from 1 to 3</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/314\">#314</a> Strip/update dependencies</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/315\">#315</a> Split lint into check and build, switch from <code>npm install</code> to <code>npm ci</code></li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/317\">#317</a> Bump normalize-url from 4.5.1 to 8.0.0</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/316\">#316</a> Faster workflow response / saving resources via timeout/concurrency policy</li>\n</ul>\n<p><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/308\">#308</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/308\">conda-incubator/setup-miniconda#308</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/291\">#291</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/291\">conda-incubator/setup-miniconda#291</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/299\">#299</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/299\">conda-incubator/setup-miniconda#299</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/309\">#309</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/309\">conda-incubator/setup-miniconda#309</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/313\">#313</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/313\">conda-incubator/setup-miniconda#313</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/318\">#318</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/318\">conda-incubator/setup-miniconda#318</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/307\">#307</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/307\">conda-incubator/setup-miniconda#307</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/311\">#311</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/311\">conda-incubator/setup-miniconda#311</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/310\">#310</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/310\">conda-incubator/setup-miniconda#310</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/314\">#314</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/314\">conda-incubator/setup-miniconda#314</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/315\">#315</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/315\">conda-incubator/setup-miniconda#315</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/317\">#317</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/317\">conda-incubator/setup-miniconda#317</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/316\">#316</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/316\">conda-incubator/setup-miniconda#316</a></p>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/isuruf\"><code>@\u200bisuruf</code></a> made their first contribution in <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/299\">conda-incubator/setup-miniconda#299</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/conda-incubator/setup-miniconda/compare/v2...v3.0.0\">https://github.com/conda-incubator/setup-miniconda/compare/v2...v3.0.0</a></p>\n<h2>Version 2.3.0</h2>\n<h3>Documentation</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/263\">#263</a> Update links to GitHub shell docs</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/conda-incubator/setup-miniconda/blob/main/CHANGELOG.md\">conda-incubator/setup-miniconda's changelog</a>.</em></p>\n<blockquote>\n<h2><a href=\"https://github.com/conda-incubator/setup-miniconda/releases/tag/v3.0.1\">v3.0.1</a> (2023-11-29)</h2>\n<h3>Fixes</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/325\">#325</a> Fix environment activation on windows (a v3 regression) due to\nhard-coded install PATH</li>\n</ul>\n<p><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/325\">#325</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/325\">conda-incubator/setup-miniconda#325</a></p>\n<h2><a href=\"https://github.com/conda-incubator/setup-miniconda/releases/tag/v3.0.0\">v3.0.0</a> (2023-11-27)</h2>\n<h3>Features</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/308\">#308</a> Update to node20</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/291\">#291</a> Add conda-solver option (defaults to libmamba)</li>\n</ul>\n<h3>Fixes</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/299\">#299</a> Fix condaBasePath when useBundled is false, and there's no pre-existing\nconda</li>\n</ul>\n<h3>Documentation</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/309\">#309</a> Switch to main branch based development</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/313\">#313</a> Specify team conda-incubator/setup-miniconda as codeowners</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/318\">#318</a> README: update actions in examples, add security section, similar\nactions</li>\n</ul>\n<h3>Tasks and Maintenance</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/307\">#307</a> Run dependabot against main branch and also update node packages</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/311\">#311</a> Bump actions/checkout from 2 to 4</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/310\">#310</a> Bump actions/cache from 1 to 3</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/314\">#314</a> Strip/update dependencies</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/315\">#315</a> Split lint into check and build, switch from <code>npm install</code> to <code>npm ci</code></li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/317\">#317</a> Bump normalize-url from 4.5.1 to 8.0.0</li>\n<li><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/316\">#316</a> Faster workflow response / saving resources via timeout/concurrency\npolicy</li>\n</ul>\n<p><a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/308\">#308</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/308\">conda-incubator/setup-miniconda#308</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/291\">#291</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/291\">conda-incubator/setup-miniconda#291</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/299\">#299</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/299\">conda-incubator/setup-miniconda#299</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/309\">#309</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/309\">conda-incubator/setup-miniconda#309</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/313\">#313</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/313\">conda-incubator/setup-miniconda#313</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/318\">#318</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/318\">conda-incubator/setup-miniconda#318</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/307\">#307</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/307\">conda-incubator/setup-miniconda#307</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/311\">#311</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/311\">conda-incubator/setup-miniconda#311</a>\n<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/310\">#310</a>: <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/pull/310\">conda-incubator/setup-miniconda#310</a></p>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/11b562958363ec5770fef326fe8ef0366f8cbf8a\"><code>11b5629</code></a> Prepare 3.0.1 (<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/326\">#326</a>)</li>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/8706aa744e0162a2264dc938c759d2b51f9b5e10\"><code>8706aa7</code></a> Fix env activation on win (a v3 regression) due to hard-coded install PATH (#...</li>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/c585a970973750f68d4d5c75284051b3f0167afe\"><code>c585a97</code></a> Bump conda-incubator/setup-miniconda from 2.3.0 to 3.0.0 (<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/321\">#321</a>)</li>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/2defc80cc6f4028b1780c50faf08dd505d698976\"><code>2defc80</code></a> Prepare release (<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/320\">#320</a>)</li>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/0d5a56b9eb002ebca917fd24dbbb168694a769e3\"><code>0d5a56b</code></a> Bump actions/checkout from 2 to 4 (<a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/319\">#319</a>)</li>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/45fd3f90899ca98b9fe9ed48921c9edbb341a570\"><code>45fd3f9</code></a> Merge pull request <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/316\">#316</a> from dbast/timeout</li>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/d1e04fc2677b38349d15832651d5d3c485c8411a\"><code>d1e04fc</code></a> Merge pull request <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/299\">#299</a> from isuruf/condaBasePath</li>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/fab00738406b3d3854b3b57791e274f16ac95ab4\"><code>fab0073</code></a> Merge pull request <a href=\"https://redirect.github.com/conda-incubator/setup-miniconda/issues/318\">#318</a> from dbast/readme</li>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/fa6bdf964307f648fedc0f07787e550f08eb2812\"><code>fa6bdf9</code></a> Update with npm run build</li>\n<li><a href=\"https://github.com/conda-incubator/setup-miniconda/commit/d42f8b884a951a7e5af53888bd6101e0fd1dc916\"><code>d42f8b8</code></a> Fix condaBasePath when useBundled is false, and there's no pre-existing conda</li>\n<li>Additional commits viewable in <a href=\"https://github.com/conda-incubator/setup-miniconda/compare/v2...v3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=conda-incubator/setup-miniconda&package-manager=github_actions&previous-version=2&new-version=3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2023-12-07T14:24:52Z",
  "comments":0,
  "created_at":"2023-12-04T09:33:01Z",
  "draft":false,
  "id":2023406353,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5hB6wf",
  "number":1051,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-07T14:24:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump conda-incubator/setup-miniconda from 2 to 3",
  "updated_at":"2023-12-07T14:24:53Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"This PR closes https://github.com/scikit-hep/uproot5/issues/970.",
  "closed_at":"2023-12-06T15:17:41Z",
  "comments":0,
  "created_at":"2023-12-05T23:10:21Z",
  "draft":false,
  "id":2027288156,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5hPL8s",
  "number":1053,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-06T15:17:41Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: remove hyphens from test names (PyCharm compatibility)",
  "updated_at":"2023-12-06T15:17:42Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"The current implementation of the `fsspec` source relies on being able to handle files with `:` in it's name (this is a way to specify which objects to read).\r\n\r\nUp to `fsspec` version `v2023.10.0` this worked. `fsspec` is able to read a file such as `local-file-with:colons.root` or `local-file-with-colons.root:object` and treat them as local files (`local` protocol). Now this results in an error.\r\n\r\nDepending on wether this is a intended or unintended change in fsspec behaviour we'll modify how the fsspec uproot source reads local files. If this was unintentional a test could be added to `fsspec` to prevent this from happening in the future.\r\n\r\n- https://github.com/fsspec/filesystem_spec/issues/1447",
  "closed_at":"2023-12-13T18:39:23Z",
  "comments":1,
  "created_at":"2023-12-05T23:41:50Z",
  "id":2027321967,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss541npv",
  "number":1054,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"Local files with colons not correclty handled by newest version of fsspec",
  "updated_at":"2023-12-13T18:39:24Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"I add an explicit test for this issue. This is currently also catched by other tests in the `main-fsspec` branch but I wouldn't be caught if we didn't make the switch to use fsspec for default local files (which is not clear yet).\r\n\r\nCloses https://github.com/scikit-hep/uproot5/issues/1054\r\n\r\nI will use this PR to fix this issue in case it needs some code change. I am hoping that https://github.com/fsspec/filesystem_spec/issues/1447 will address this (I assume it's a bug on fsspec part).\r\n\r\nCurrently this PR is failing until either fsspec publishes a new release with a fix, we set an upper limit for supported fsspec versions, or we modify how we handle paths with colons in our code. This should be handled before the next release.\r\n\r\nI also added an explicit test for windows since colons cannot be part of a filename in windows.",
  "closed_at":"2023-12-11T22:36:49Z",
  "comments":3,
  "created_at":"2023-12-06T20:07:36Z",
  "draft":false,
  "id":2029301092,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5hWEv0",
  "number":1055,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-11T22:36:49Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: add test for issue 1054 (newer fsspec failing to parse files with colons in name)",
  "updated_at":"2023-12-11T22:36:50Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"MEMBER",
  "body":null,
  "closed_at":"2023-12-07T14:51:17Z",
  "comments":0,
  "created_at":"2023-12-07T14:36:29Z",
  "draft":false,
  "id":2030890386,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5hbgqF",
  "number":1056,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-07T14:51:17Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: zstandard should be a test dependency, and xxhash goes with lz4.",
  "updated_at":"2023-12-07T14:51:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Supersedes #1050 \r\n\r\nThis again boils down to adding a `report=` argument, but now the implementation to actually build the report is significant changed and based on PR https://github.com/dask-contrib/dask-awkward/pull/433 in dask-awkward instead of the combination of #1050 + https://github.com/dask-contrib/dask-awkward/pull/415\r\n\r\n@lgray if you could take this for a spin on something that you know will potentially have `OSError` raised, that would be great! ",
  "closed_at":"2023-12-12T00:35:20Z",
  "comments":25,
  "created_at":"2023-12-07T18:10:31Z",
  "draft":false,
  "id":2031285686,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5hc4gJ",
  "number":1058,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-12T00:35:20Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat(draft): add `report=` argument for `uproot.dask`; trigger report collection (take 2!)",
  "updated_at":"2023-12-12T00:35:20Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [actions/setup-python](https://github.com/actions/setup-python) from 4 to 5.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/setup-python/releases\">actions/setup-python's releases</a>.</em></p>\n<blockquote>\n<h2>v5.0.0</h2>\n<h2>What's Changed</h2>\n<p>In scope of this release, we update node version runtime from node16 to node20 (<a href=\"https://redirect.github.com/actions/setup-python/pull/772\">actions/setup-python#772</a>). Besides, we update dependencies to the latest versions.</p>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/setup-python/compare/v4.8.0...v5.0.0\">https://github.com/actions/setup-python/compare/v4.8.0...v5.0.0</a></p>\n<h2>v4.8.0</h2>\n<h2>What's Changed</h2>\n<p>In scope of this release we added support for GraalPy (<a href=\"https://redirect.github.com/actions/setup-python/pull/694\">actions/setup-python#694</a>). You can use this snippet to set up GraalPy:</p>\n<pre lang=\"yaml\"><code>steps:\n- uses: actions/checkout@v4\n- uses: actions/setup-python@v4 \n  with:\n    python-version: 'graalpy-22.3' \n- run: python my_script.py\n</code></pre>\n<p>Besides, the release contains such changes as:</p>\n<ul>\n<li>Trim python version when reading from file by <a href=\"https://github.com/FerranPares\"><code>@\u200bFerranPares</code></a> in <a href=\"https://redirect.github.com/actions/setup-python/pull/628\">actions/setup-python#628</a></li>\n<li>Use non-deprecated versions in examples by <a href=\"https://github.com/jeffwidman\"><code>@\u200bjeffwidman</code></a> in <a href=\"https://redirect.github.com/actions/setup-python/pull/724\">actions/setup-python#724</a></li>\n<li>Change deprecation comment to past tense by <a href=\"https://github.com/jeffwidman\"><code>@\u200bjeffwidman</code></a> in <a href=\"https://redirect.github.com/actions/setup-python/pull/723\">actions/setup-python#723</a></li>\n<li>Bump <code>@\u200bbabel/traverse</code> from 7.9.0 to 7.23.2 by <a href=\"https://github.com/dependabot\"><code>@\u200bdependabot</code></a> in <a href=\"https://redirect.github.com/actions/setup-python/pull/743\">actions/setup-python#743</a></li>\n<li>advanced-usage.md: Encourage the use actions/checkout@v4 by <a href=\"https://github.com/cclauss\"><code>@\u200bcclauss</code></a> in <a href=\"https://redirect.github.com/actions/setup-python/pull/729\">actions/setup-python#729</a></li>\n<li>Examples now use checkout@v4 by <a href=\"https://github.com/simonw\"><code>@\u200bsimonw</code></a> in <a href=\"https://redirect.github.com/actions/setup-python/pull/738\">actions/setup-python#738</a></li>\n<li>Update actions/checkout to v4 by <a href=\"https://github.com/dmitry-shibanov\"><code>@\u200bdmitry-shibanov</code></a> in <a href=\"https://redirect.github.com/actions/setup-python/pull/761\">actions/setup-python#761</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/FerranPares\"><code>@\u200bFerranPares</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/setup-python/pull/628\">actions/setup-python#628</a></li>\n<li><a href=\"https://github.com/timfel\"><code>@\u200btimfel</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/setup-python/pull/694\">actions/setup-python#694</a></li>\n<li><a href=\"https://github.com/jeffwidman\"><code>@\u200bjeffwidman</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/setup-python/pull/724\">actions/setup-python#724</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/setup-python/compare/v4...v4.8.0\">https://github.com/actions/setup-python/compare/v4...v4.8.0</a></p>\n<h2>v4.7.1</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Bump word-wrap from 1.2.3 to 1.2.4 by <a href=\"https://github.com/dependabot\"><code>@\u200bdependabot</code></a> in <a href=\"https://redirect.github.com/actions/setup-python/pull/702\">actions/setup-python#702</a></li>\n<li>Add range validation for toml files by <a href=\"https://github.com/dmitry-shibanov\"><code>@\u200bdmitry-shibanov</code></a> in <a href=\"https://redirect.github.com/actions/setup-python/pull/726\">actions/setup-python#726</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/setup-python/compare/v4...v4.7.1\">https://github.com/actions/setup-python/compare/v4...v4.7.1</a></p>\n<h2>v4.7.0</h2>\n<p>In scope of this release, the support for reading python version from pyproject.toml was added (<a href=\"https://redirect.github.com/actions/setup-python/pull/669\">actions/setup-python#669</a>).</p>\n<pre lang=\"yaml\"><code>      - name: Setup Python\n        uses: actions/setup-python@v4\n&lt;/tr&gt;&lt;/table&gt; \n</code></pre>\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/actions/setup-python/commit/0a5c61591373683505ea898e09a3ea4f39ef2b9c\"><code>0a5c615</code></a> Update action to node20 (<a href=\"https://redirect.github.com/actions/setup-python/issues/772\">#772</a>)</li>\n<li><a href=\"https://github.com/actions/setup-python/commit/0ae58361cdfd39e2950bed97a1e26aa20c3d8955\"><code>0ae5836</code></a> Add example of GraalPy to docs (<a href=\"https://redirect.github.com/actions/setup-python/issues/773\">#773</a>)</li>\n<li><a href=\"https://github.com/actions/setup-python/commit/b64ffcaf5b410884ad320a9cfac8866006a109aa\"><code>b64ffca</code></a> update actions/checkout to v4 (<a href=\"https://redirect.github.com/actions/setup-python/issues/761\">#761</a>)</li>\n<li><a href=\"https://github.com/actions/setup-python/commit/8d2896179abf658742de432b3f203d2c2d86a587\"><code>8d28961</code></a> Examples now use checkout@v4 (<a href=\"https://redirect.github.com/actions/setup-python/issues/738\">#738</a>)</li>\n<li><a href=\"https://github.com/actions/setup-python/commit/7bc6abb01e0555719edc2dbca70a2fde309e5e56\"><code>7bc6abb</code></a> advanced-usage.md: Encourage the use actions/checkout@v4 (<a href=\"https://redirect.github.com/actions/setup-python/issues/729\">#729</a>)</li>\n<li><a href=\"https://github.com/actions/setup-python/commit/e8111cec9d3dc15220d8a3b638f08419f57b906a\"><code>e8111ce</code></a> Bump <code>@\u200bbabel/traverse</code> from 7.9.0 to 7.23.2 (<a href=\"https://redirect.github.com/actions/setup-python/issues/743\">#743</a>)</li>\n<li><a href=\"https://github.com/actions/setup-python/commit/a00ea43da65e7c04d2bdae58b3afecd77057eb9e\"><code>a00ea43</code></a> add fix for graalpy ci (<a href=\"https://redirect.github.com/actions/setup-python/issues/741\">#741</a>)</li>\n<li><a href=\"https://github.com/actions/setup-python/commit/8635b1ccc5934e73ed3510980fd2e7790b85839b\"><code>8635b1c</code></a> Change deprecation comment to past tense (<a href=\"https://redirect.github.com/actions/setup-python/issues/723\">#723</a>)</li>\n<li><a href=\"https://github.com/actions/setup-python/commit/f6cc428f535856f9c23558d01765a42a4d6cf758\"><code>f6cc428</code></a> Use non-deprecated versions in examples (<a href=\"https://redirect.github.com/actions/setup-python/issues/724\">#724</a>)</li>\n<li><a href=\"https://github.com/actions/setup-python/commit/5f2af211d616f86005883b44826180b21abb4060\"><code>5f2af21</code></a> Add GraalPy support (<a href=\"https://redirect.github.com/actions/setup-python/issues/694\">#694</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/actions/setup-python/compare/v4...v5\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/setup-python&package-manager=github_actions&previous-version=4&new-version=5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2023-12-14T00:18:50Z",
  "comments":0,
  "created_at":"2023-12-11T09:11:32Z",
  "draft":false,
  "id":2035184972,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5hp9jQ",
  "number":1059,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-14T00:18:49Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump actions/setup-python from 4 to 5",
  "updated_at":"2023-12-14T00:18:50Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/astral-sh/ruff-pre-commit: v0.1.6 \u2192 v0.1.7](https://github.com/astral-sh/ruff-pre-commit/compare/v0.1.6...v0.1.7)\n<!--pre-commit.ci end-->",
  "closed_at":"2023-12-12T14:05:25Z",
  "comments":0,
  "created_at":"2023-12-11T19:41:17Z",
  "draft":false,
  "id":2036401949,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5huKXb",
  "number":1060,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-12T14:05:25Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2023-12-12T14:05:26Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Implement globbing support using fsspec.\r\n\r\nCurrently not working for all protocols due to https://github.com/fsspec/filesystem_spec/issues/1459. In Related to https://github.com/CoffeaTeam/fsspec-xrootd/pull/40 we implement the necessary changes to make it work for xrootd.\r\n\r\nA new release of `fsspec-xrootd` is necessary for this to work. (DONE!)\r\n\r\nBefore this PR, glob expressions were ignored for all but local file paths.\r\n\r\nThis PR tries to expand them using fsspec, but this may not always work due to https://github.com/fsspec/filesystem_spec/issues/1459. We made sure it worked for xrootd. Not working here means that the glob expression will be expanded but the full path will be wrong due to the protocol prefix being incomplete (missing auth information, etc.). uproot will just throw an exception as if it tried to open a bad path.\r\n\r\nAdded tests for `xrootd` and `s3` globs (http glob does not work at the moment).\r\n\r\nIn order to correclty expand the glob expression fsspec may need additional `storage_options`. These are part of the `options` directionary that needs to be passed to the `regularize_files` method.\r\n",
  "closed_at":"2023-12-13T18:41:24Z",
  "comments":11,
  "created_at":"2023-12-12T05:54:02Z",
  "draft":false,
  "id":2037035744,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5hwUjm",
  "number":1061,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-13T18:41:24Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"feat: globbing with fsspec",
  "updated_at":"2023-12-13T18:41:25Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"This may help in streamlining releases in the future, but it's up to opinion on operations.\r\nIt also makes it easier for users to see if they are using a build installed from source.\r\n\r\n@jpivarski @lobis ",
  "closed_at":"2024-01-03T22:01:46Z",
  "comments":4,
  "created_at":"2023-12-12T15:29:46Z",
  "draft":false,
  "id":2038014682,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5hzppA",
  "number":1062,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-03T22:01:46Z"
  },
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"closed",
  "state_reason":null,
  "title":"build: change build to autogen version info",
  "updated_at":"2024-01-03T22:10:14Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjM1ODAzMjgw",
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"uproot version: 5.2.0rc4 (but also exists in 5.2.0rc1-3)\r\n\r\n@lobis @iasonkrom\r\n\r\nThe following code:\r\n```python3\r\nimport dask\r\nfrom distributed import Client\r\nimport uproot\r\n\r\nif __name__ == \"__main__\":\r\n    with Client() as _:\r\n        events = uproot.dask(\"tests/samples/nano_dy.root:Events\")\r\n\r\n        print(dask.compute(events.Muon_pt))\r\n```\r\n\r\nfails with a very lengthy error message, the gist of which is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/lgray/miniforge3/envs/coffea-rc-test/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 63, in dumps\r\n    result = pickle.dumps(x, **dump_kwargs)\r\nAttributeError: Can't pickle local object 'unpack_collections.<locals>.repack'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/lgray/miniforge3/envs/coffea-rc-test/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 68, in dumps\r\n    pickler.dump(x)\r\nAttributeError: Can't pickle local object 'unpack_collections.<locals>.repack'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/lgray/miniforge3/envs/coffea-rc-test/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 81, in dumps\r\n    result = cloudpickle.dumps(x, **dump_kwargs)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-rc-test/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\r\n    cp.dump(obj)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-rc-test/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\r\n    return super().dump(obj)\r\n  File \"/Users/lgray/miniforge3/envs/coffea-rc-test/lib/python3.10/site-packages/uproot/source/fsspec.py\", line 68, in __getstate__\r\n    state.pop(\"_fh\")\r\nKeyError: '_fh'\r\n2023-12-13 08:39:47,143 - distributed.protocol.core - CRITICAL - Failed to Serialize\r\nTraceback (most recent call last):\r\n  File \"/Users/lgray/miniforge3/envs/coffea-rc-test/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 63, in dumps\r\n    result = pickle.dumps(x, **dump_kwargs)\r\nAttributeError: Can't pickle local object 'unpack_collections.<locals>.repack'\r\n```\r\nThe key error above appears to be causing the pickling error that gets reported first. A bit confusing but makes sense in the end.\r\n\r\nIf we use `open_files=False`:\r\n```\r\nimport dask\r\nfrom distributed import Client\r\nimport uproot\r\n\r\nif __name__ == \"__main__\":\r\n    with Client() as _:\r\n        events = uproot.dask(\"tests/samples/nano_dy.root:Events\", open_files=False)\r\n\r\n        print(dask.compute(events.Muon_pt))\r\n```\r\n\r\nit successfully executes:\r\n```\r\n(coffea-rc-test) lgray@Lindseys-MacBook-Pro coffea % python uproot_distributed_test.py\r\n(<Array [[], [], [], [], [...], ..., [], [], [], []] type='40 * var * float32'>,)\r\n```\r\n\r\nThis should be fixed before we release uproot 5.2.0.",
  "closed_at":"2023-12-13T23:56:44Z",
  "comments":7,
  "created_at":"2023-12-13T12:46:24Z",
  "id":2039641115,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss55knQb",
  "number":1063,
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "state":"closed",
  "state_reason":"completed",
  "title":"urgent: uproot.dask(..., open_files=True) not usable with dask distributed client",
  "updated_at":"2024-01-06T21:23:36Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"`aiohttp` appears to be available for python 3.12 so we can remove the skip.\r\n\r\nI also increased the number of reruns in the pytest command to avoid failure due to flaky xrootd.",
  "closed_at":"2023-12-13T18:00:20Z",
  "comments":1,
  "created_at":"2023-12-13T16:44:28Z",
  "draft":false,
  "id":2040101647,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5h6wFp",
  "number":1064,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-13T18:00:20Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: do not skip aiohttp tests for Python 3.12 - update pytest rerun settings",
  "updated_at":"2023-12-13T18:00:21Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"- https://github.com/scikit-hep/uproot5/issues/1063",
  "closed_at":"2023-12-13T21:23:17Z",
  "comments":2,
  "created_at":"2023-12-13T18:59:11Z",
  "draft":false,
  "id":2040297101,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5h7a3Z",
  "number":1065,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-13T21:23:17Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: dask distributed fsspec issue",
  "updated_at":"2023-12-13T21:23:18Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"uproot version: 5.2.0rc5\r\n\r\ndict-style file + object path globbing works fine:\r\n```\r\n>>> import uproot\r\n>>> events = uproot.dask({\"root://cmsdcadisk.fnal.gov//dcache/uscmsdisk/store/data/Run2023C/EGamma0/NANOAOD/PromptNanoAODv11p9_v1-v1/*/*.root\":\"Events\"}, open_files=False)\r\n>>> events.Muon_pt\r\ndask.awkward<Muon-pt, npartitions=71>\r\n```\r\n\r\nhowever, in-string style does not:\r\n```\r\n>>> events = uproot.dask(\"root://cmsdcadisk.fnal.gov//dcache/uscmsdisk/store/data/Run2023C/EGamma0/NANOAOD/PromptNanoAODv11p9_v1-v1/*/*.root:Events\", open_files=False)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/srv/.env/lib/python3.10/site-packages/uproot/_dask.py\", line 164, in dask\r\n    files = uproot._util.regularize_files(files, steps_allowed=True, **options)\r\n  File \"/srv/.env/lib/python3.10/site-packages/uproot/_util.py\", line 944, in regularize_files\r\n    raise _file_not_found(files)\r\nFileNotFoundError: file not found\r\n\r\n    'root://cmsdcadisk.fnal.gov//dcache/uscmsdisk/store/data/Run2023C/EGamma0/NANOAOD/PromptNanoAODv11p9_v1-v1/*/*.root:Events'\r\n\r\nFiles may be specified as:\r\n   * str/bytes: relative or absolute filesystem path or URL, without any colons\r\n         other than Windows drive letter or URL schema.\r\n         Examples: \"rel/file.root\", \"C:\\abs\\file.root\", \"http://where/what.root\"\r\n   * str/bytes: same with an object-within-ROOT path, separated by a colon.\r\n         Example: \"rel/file.root:tdirectory/ttree\"\r\n   * pathlib.Path: always interpreted as a filesystem path or URL only (no\r\n         object-within-ROOT path), regardless of whether there are any colons.\r\n         Examples: Path(\"rel:/file.root\"), Path(\"/abs/path:stuff.root\")\r\n\r\nFunctions that accept many files (uproot.iterate, etc.) also allow:\r\n   * glob syntax in str/bytes and pathlib.Path.\r\n         Examples: Path(\"rel/*.root\"), \"/abs/*.root:tdirectory/ttree\"\r\n   * dict: keys are filesystem paths, values are objects-within-ROOT paths.\r\n         Example: {\"/data_v1/*.root\": \"ttree_v1\", \"/data_v2/*.root\": \"ttree_v2\"}\r\n   * already-open TTree objects.\r\n   * iterables of the above.\r\n\r\n```\r\n",
  "closed_at":"2023-12-14T15:00:53Z",
  "comments":0,
  "created_at":"2023-12-14T12:16:37Z",
  "id":2041579171,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss55sAaj",
  "number":1066,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"xrootd file globbing only works for some file + object path specifications ",
  "updated_at":"2023-12-14T15:00:53Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Fixes #1066 ",
  "closed_at":"2023-12-14T15:00:52Z",
  "comments":1,
  "created_at":"2023-12-14T12:27:26Z",
  "draft":false,
  "id":2041598672,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5h_zLY",
  "number":1067,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-14T15:00:52Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"fix: correct typo in fsspec globbing",
  "updated_at":"2023-12-14T15:00:53Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "active_lock_reason":null,
  "assignee":"MDQ6VXNlcjk3NTE4NzE=",
  "assignees":null,
  "author_association":"NONE",
  "body":"I am using the latest uproot version 5.2.0. \r\nWhen reading a ROOT file with `uproot.iterate` into `pandas.DataFrame`, a PerformanceWarning is triggered:\r\n```\r\nPerformanceWarning: DataFrame is highly fragmented.\r\nThis is usually the result of calling `frame.insert` many times, which has poor performance. \r\nConsider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\r\n```\r\nThe issue seems to be in the function `_pandas_memory_efficient` in the file `interpretation/library.py`.",
  "closed_at":"2024-01-22T10:45:25Z",
  "comments":4,
  "created_at":"2023-12-15T11:41:22Z",
  "id":2043577053,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"I_kwDOD6Q_ss55zoLd",
  "number":1070,
  "performed_via_github_app":null,
  "reactions":{},
  "state":"closed",
  "state_reason":"completed",
  "title":"`uproot.iterate` throws a `pandas` PerformanceWarning",
  "updated_at":"2024-01-22T10:45:25Z",
  "user":"U_kgDOBtvh2Q"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [actions/upload-artifact](https://github.com/actions/upload-artifact) from 3 to 4.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/upload-artifact/releases\">actions/upload-artifact's releases</a>.</em></p>\n<blockquote>\n<h2>v4.0.0</h2>\n<h2>What's Changed</h2>\n<p>The release of upload-artifact@v4 and download-artifact@v4 are major changes to the backend architecture of Artifacts. They have numerous performance and behavioral improvements.</p>\n<p>For more information, see the <a href=\"https://github.com/actions/toolkit/tree/main/packages/artifact\"><code>@\u200bactions/artifact</code></a> documentation.</p>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/vmjoseph\"><code>@\u200bvmjoseph</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/upload-artifact/pull/464\">actions/upload-artifact#464</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/upload-artifact/compare/v3...v4.0.0\">https://github.com/actions/upload-artifact/compare/v3...v4.0.0</a></p>\n<h2>v3.1.3</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>chore(github): remove trailing whitespaces by <a href=\"https://github.com/ljmf00\"><code>@\u200bljmf00</code></a> in <a href=\"https://redirect.github.com/actions/upload-artifact/pull/313\">actions/upload-artifact#313</a></li>\n<li>Bump <code>@\u200bactions/artifact</code> version to v1.1.2 by <a href=\"https://github.com/bethanyj28\"><code>@\u200bbethanyj28</code></a> in <a href=\"https://redirect.github.com/actions/upload-artifact/pull/436\">actions/upload-artifact#436</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/upload-artifact/compare/v3...v3.1.3\">https://github.com/actions/upload-artifact/compare/v3...v3.1.3</a></p>\n<h2>v3.1.2</h2>\n<ul>\n<li>Update all <code>@actions/*</code> NPM packages to their latest versions- <a href=\"https://redirect.github.com/actions/upload-artifact/issues/374\">#374</a></li>\n<li>Update all dev dependencies to their most recent versions - <a href=\"https://redirect.github.com/actions/upload-artifact/issues/375\">#375</a></li>\n</ul>\n<h2>v3.1.1</h2>\n<ul>\n<li>Update actions/core package to latest version to remove <code>set-output</code> deprecation warning <a href=\"https://redirect.github.com/actions/upload-artifact/issues/351\">#351</a></li>\n</ul>\n<h2>v3.1.0</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Bump <code>@\u200bactions/artifact</code> to v1.1.0 (<a href=\"https://redirect.github.com/actions/upload-artifact/pull/327\">actions/upload-artifact#327</a>)\n<ul>\n<li>Adds checksum headers on artifact upload (<a href=\"https://redirect.github.com/actions/toolkit/pull/1095\">actions/toolkit#1095</a>) (<a href=\"https://redirect.github.com/actions/toolkit/pull/1063\">actions/toolkit#1063</a>)</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/c7d193f32edcb7bfad88892161225aeda64e9392\"><code>c7d193f</code></a> Merge pull request <a href=\"https://redirect.github.com/actions/upload-artifact/issues/466\">#466</a> from actions/v4-beta</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/13131bb095770b4070a7477c3cd2d96e1c16d9f4\"><code>13131bb</code></a> licensed cache</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/4a6c273b9834f66a1d05c170dc3f80f9cdb9def1\"><code>4a6c273</code></a> Merge branch 'main' into v4-beta</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/f391bb91a3d3118aeca171c365bb319ece276b37\"><code>f391bb9</code></a> Merge pull request <a href=\"https://redirect.github.com/actions/upload-artifact/issues/465\">#465</a> from actions/robherley/v4-documentation</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/9653d03c4b74c32144e02dae644fea70e079d4b3\"><code>9653d03</code></a> Apply suggestions from code review</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/875b63076402f25ef9d52c294c86ba4f97810575\"><code>875b630</code></a> add limitations section</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/ecb21463e93740a6be75c3116242169bfdbcb15a\"><code>ecb2146</code></a> add compression example</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/5e7604f84a055838f64ed68bb9904751523081ae\"><code>5e7604f</code></a> trim some repeated info</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/d6437d07581fe318a364512e6cf6b1dca6b4f92c\"><code>d6437d0</code></a> naming</li>\n<li><a href=\"https://github.com/actions/upload-artifact/commit/1b561557037b4957d7d184e9aac02bec86c771eb\"><code>1b56155</code></a> s/v4-beta/v4/g</li>\n<li>Additional commits viewable in <a href=\"https://github.com/actions/upload-artifact/compare/v3...v4\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/upload-artifact&package-manager=github_actions&previous-version=3&new-version=4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2024-01-12T16:17:53Z",
  "comments":0,
  "created_at":"2023-12-18T09:08:37Z",
  "draft":false,
  "id":2046112336,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5iPBIR",
  "number":1071,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-12T16:17:53Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump actions/upload-artifact from 3 to 4",
  "updated_at":"2024-01-12T16:17:54Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"Bumps [actions/download-artifact](https://github.com/actions/download-artifact) from 3 to 4.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/actions/download-artifact/releases\">actions/download-artifact's releases</a>.</em></p>\n<blockquote>\n<h2>v4.0.0</h2>\n<h2>What's Changed</h2>\n<p>The release of upload-artifact@v4 and download-artifact@v4 are major changes to the backend architecture of Artifacts. They have numerous performance and behavioral improvements.</p>\n<p>For more information, see the <a href=\"https://github.com/actions/toolkit/tree/main/packages/artifact\"><code>@\u200bactions/artifact</code></a> documentation.</p>\n<h2>New Contributors</h2>\n<ul>\n<li><a href=\"https://github.com/bflad\"><code>@\u200bbflad</code></a> made their first contribution in <a href=\"https://redirect.github.com/actions/download-artifact/pull/194\">actions/download-artifact#194</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/actions/download-artifact/compare/v3...v4.0.0\">https://github.com/actions/download-artifact/compare/v3...v4.0.0</a></p>\n<h2>v3.0.2</h2>\n<ul>\n<li>Bump <code>@actions/artifact</code> to v1.1.1 - <a href=\"https://redirect.github.com/actions/download-artifact/pull/195\">actions/download-artifact#195</a></li>\n<li>Fixed a bug in Node16 where if an HTTP download finished too quickly (&lt;1ms, e.g. when it's mocked) we attempt to delete a temp file that has not been created yet <a href=\"hhttps://redirect.github.com/actions/toolkit/pull/1278\">actions/toolkit#1278</a></li>\n</ul>\n<h2>v3.0.1</h2>\n<ul>\n<li><a href=\"https://redirect.github.com/actions/download-artifact/pull/178\">Bump <code>@\u200bactions/core</code> to 1.10.0</a></li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/actions/download-artifact/commit/7a1cd3216ca9260cd8022db641d960b1db4d1be4\"><code>7a1cd32</code></a> Merge pull request <a href=\"https://redirect.github.com/actions/download-artifact/issues/246\">#246</a> from actions/v4-beta</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/8f32874a49903ea488c5e7d476a9173e8706f409\"><code>8f32874</code></a> licensed cache</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/b5ff8444b1c4fcec8131f3cb1ddade813ddfacb1\"><code>b5ff844</code></a> Merge pull request <a href=\"https://redirect.github.com/actions/download-artifact/issues/245\">#245</a> from actions/robherley/v4-documentation</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/f07a0f73f51b3f1d41667c782c821b9667da9d19\"><code>f07a0f7</code></a> Update README.md</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/7226129829bb686fdff47bd63bbd0d1373993a84\"><code>7226129</code></a> update test workflow to use different artifact names for matrix</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/ada9446619b84dd8a557aaaec3b79b58c4986cdf\"><code>ada9446</code></a> update docs and bump <code>@\u200bactions/artifact</code></li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/7eafc8b729ba790ce8f2cee54be8ad6257af4c7c\"><code>7eafc8b</code></a> Merge pull request <a href=\"https://redirect.github.com/actions/download-artifact/issues/244\">#244</a> from actions/robherley/bump-toolkit</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/3132d12662b5915f20cdbf449465896962101abf\"><code>3132d12</code></a> consume latest toolkit</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/5be1d3867182a382bc59f2775e002595f487aa88\"><code>5be1d38</code></a> Merge pull request <a href=\"https://redirect.github.com/actions/download-artifact/issues/243\">#243</a> from actions/robherley/v4-beta-updates</li>\n<li><a href=\"https://github.com/actions/download-artifact/commit/465b526e63559575a64716cdbb755bc78dfb263b\"><code>465b526</code></a> consume latest <code>@\u200bactions/toolkit</code></li>\n<li>Additional commits viewable in <a href=\"https://github.com/actions/download-artifact/compare/v3...v4\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=actions/download-artifact&package-manager=github_actions&previous-version=3&new-version=4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n</details>",
  "closed_at":"2024-01-12T16:17:40Z",
  "comments":0,
  "created_at":"2023-12-18T09:08:40Z",
  "draft":false,
  "id":2046112497,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5iPBKk",
  "number":1072,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-12T16:17:40Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore(deps): bump actions/download-artifact from 3 to 4",
  "updated_at":"2024-01-12T16:17:41Z",
  "user":"MDM6Qm90NDk2OTkzMzM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"CONTRIBUTOR",
  "body":"<!--pre-commit.ci start-->\nupdates:\n- [github.com/psf/black-pre-commit-mirror: 23.11.0 \u2192 23.12.1](https://github.com/psf/black-pre-commit-mirror/compare/23.11.0...23.12.1)\n- [github.com/astral-sh/ruff-pre-commit: v0.1.7 \u2192 v0.1.11](https://github.com/astral-sh/ruff-pre-commit/compare/v0.1.7...v0.1.11)\n- [github.com/macisamuele/language-formatters-pre-commit-hooks: v2.11.0 \u2192 v2.12.0](https://github.com/macisamuele/language-formatters-pre-commit-hooks/compare/v2.11.0...v2.12.0)\n<!--pre-commit.ci end-->",
  "closed_at":"2024-01-12T16:18:06Z",
  "comments":0,
  "created_at":"2023-12-18T19:48:13Z",
  "draft":false,
  "id":2047341559,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5iTMt7",
  "number":1073,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2024-01-12T16:18:06Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"chore: update pre-commit hooks",
  "updated_at":"2024-01-12T16:18:07Z",
  "user":"MDM6Qm90NjY4NTMxMTM="
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Update docs with fsspec integration usage.",
  "closed_at":null,
  "comments":1,
  "created_at":"2023-12-20T15:15:08Z",
  "draft":false,
  "id":2050756532,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5ie3cv",
  "number":1074,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":null
  },
  "reactions":{},
  "state":"open",
  "state_reason":null,
  "title":"docs: fsspec documentation",
  "updated_at":"2024-01-22T19:19:58Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"test `simplecache`. Made it work with xrootd in https://github.com/CoffeaTeam/fsspec-xrootd/pull/42.",
  "closed_at":"2023-12-25T11:09:36Z",
  "comments":0,
  "created_at":"2023-12-20T15:26:30Z",
  "draft":false,
  "id":2050785965,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5ie-KF",
  "number":1075,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-25T11:09:36Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: fsspec cache",
  "updated_at":"2023-12-25T11:09:37Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "active_lock_reason":null,
  "assignee":null,
  "assignees":null,
  "author_association":"COLLABORATOR",
  "body":"Add a pytest fixture for xrootd server which can be used to serve files locally over xrootd. We might want to switch to use this for all xrootd tests to avoid networking / server issues.\r\n\r\nI updated one xrootd test to use this new fixture.\r\n\r\nI also renamed the old \"server\" fixture to \"http_server\" since now we have more than one server fixture.\r\n",
  "closed_at":"2023-12-21T14:51:39Z",
  "comments":1,
  "created_at":"2023-12-20T17:12:10Z",
  "draft":false,
  "id":2050963220,
  "labels":null,
  "locked":false,
  "milestone":null,
  "node_id":"PR_kwDOD6Q_ss5iflUF",
  "number":1076,
  "performed_via_github_app":null,
  "pull_request":{
   "merged_at":"2023-12-21T14:51:39Z"
  },
  "reactions":{},
  "state":"closed",
  "state_reason":null,
  "title":"test: xrootd server fixture",
  "updated_at":"2023-12-21T14:51:40Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 }
]
[
 {
  "author_association":"COLLABORATOR",
  "body":"Is there a chance that simpler object writing like TH1* could be supported in short term, or is the overall design not yet finalized to be able to implement it?",
  "created_at":"2021-01-14T20:20:29Z",
  "id":760452967,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MDQ1Mjk2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-14T20:20:29Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"The structure for writing anything is not yet started. See the [Write methods](https://github.com/scikit-hep/uproot4/projects/3#card-40292604) project.",
  "created_at":"2021-01-14T21:18:45Z",
  "id":760482516,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MDQ4MjUxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-14T21:18:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Just to say, now that **uproot3** _and_ [**root_pandas**](https://github.com/scikit-hep/root_pandas) have both been deprecated, I am unaware of any currently supported project that allows for writing pandas DataFrames to ROOT TTrees; to my knowledge, the [Write methods](https://github.com/scikit-hep/uproot4/projects/3#card-40292604) project is the only existing effort that might solve this.",
  "created_at":"2021-03-19T15:10:46Z",
  "id":802903838,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkwMzgzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:10:46Z",
  "user":"MDQ6VXNlcjE1Nzc0ODM4"
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't mention it here, but I'm scheduled to work on Uproot 4 writing in April. (March is [Vector](https://github.com/scikit-hep/vector), April is Uproot-writing.)\r\n\r\nBut on the point of turning Pandas DataFrames into ROOT files, we're also missing an `ak.from_pandas` to reverse [ak.to_pandas](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_pandas.html). It's not hard to turn a Pandas DataFrame with a simple index into equivalent Awkward Arrays and therefore ROOT files, but if the DataFrame has a MultiIndex, I think you'd want that to be turned into a jagged array (reversing what [ak.to_pandas](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_pandas.html) does), and therefore arrays-per-event in ROOT.\r\n\r\nBut I don't think that root_pandas did the MultiIndex \u2192 jagged ROOT, either, so maybe I'm thinking about more complex use-cases than people have.",
  "created_at":"2021-03-19T15:31:18Z",
  "id":802917934,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkxNzkzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:31:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Comment following from https://github.com/scikit-hep/root_pandas/commit/5b174a4bb119573bd120c04a636db9e2d9aca874#commitcomment-48451554.",
  "created_at":"2021-03-19T15:33:57Z",
  "id":802919757,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkxOTc1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:33:57Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski Thanks for the update. This is certainly a more complicated use-case than I have at the moment, but obviously I cannot speak for the broader community. \r\n\r\nIf it is easier to support the simple case while development for the complicated case is ongoing, I would certainly appreciate an earlier release, though for now I can continue with the deprecated **root_pandas**.",
  "created_at":"2021-03-19T15:36:27Z",
  "id":802921396,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkyMTM5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:36:27Z",
  "user":"MDQ6VXNlcjE1Nzc0ODM4"
 },
 {
  "author_association":"MEMBER",
  "body":"Hi @goi42, on these matters - yes root_pandas is deprecated for good reasons. But point is taken that coverage in replacement is not 100% yet. This and many other packages really are community efforts and you should feel free and urged to help out, especially when you are a user. In short, if you have been a user of root_pandas for a while, would you consider patching root_pandas e.g. for a more recent ROOT or Python version? From what I've seen in messages every now and then, there are various colleagues out there that would be keen on that. Maybe that's not much work to you? Thanks.",
  "created_at":"2021-03-19T15:44:38Z",
  "id":802926797,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkyNjc5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:44:38Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"> would you consider patching root_pandas \r\n\r\nThe real problem here is `root_numpy` and it's test suite has been failing since ROOT 6.16.00 was released. If someone is willing to revive `root_numpy` then the continued maintenance of `root_pandas` is relatively simple.",
  "created_at":"2021-03-19T15:48:30Z",
  "id":802929583,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkyOTU4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:48:30Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"BTW @goi42, if you got out of ROOT with root_pandas, what is your use-case \"for writing pandas DataFrames to ROOT TTrees\" back again? It is useful to know to better understand the needs out there :-).",
  "created_at":"2021-03-19T15:48:55Z",
  "id":802929894,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkyOTg5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:48:55Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"> > would you consider patching root_pandas\r\n> \r\n> The real problem here is `root_numpy` and it's test suite has been failing since ROOT 6.16.00 was released. If someone is willing to revive `root_numpy` then the continued maintenance of `root_pandas` is relatively simple.\r\n\r\nTotally, @chrisburr. I'm aware of that. My general comment on maintenance of community packages still holds :-).",
  "created_at":"2021-03-19T15:57:46Z",
  "id":802936119,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkzNjExOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:57:46Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"NONE",
  "body":"@eduardo-rodrigues `root_pandas` allowed me to do some pythonic calculations and write out a friend `TTree` with the results, which I could then use for histogramming, etc., within `ROOT`. \r\n\r\nAs I mentioned in the other comment you linked above, I may find time to contribute to the improvement of these features in the future; part of community development is drawing attention to use-cases and finding out what other work is being done.",
  "created_at":"2021-03-19T15:57:53Z",
  "id":802936199,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkzNjE5OQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-03-19T15:57:53Z",
  "user":"MDQ6VXNlcjE1Nzc0ODM4"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The ability to write a **friend** `TTree` as @goi42 mentioned would be very useful. I assume that setting a `TTree` as a friend would be easier than writing arbitrarily complex `TTree`s?",
  "created_at":"2021-04-14T14:07:31Z",
  "id":819546661,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxOTU0NjY2MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-14T14:07:31Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"MEMBER",
  "body":"Writing arbitrarily complex `TTrees` would have open-ended difficulty*, so assigning a friend should be easier than that. I don't see how there's an equivalence between these two problems, but I can look into how to assign friend relationships, likely after all other work (including jagged arrays in `TTrees`) is done.\r\n\r\n*If the space of types were enumerable, like \"primitive, list, union, record, and all combinations thereof,\" then this would be a finite problem: implement the generators of the space and test them in combinations, at least two deep. But the kinds of objects that can go into `TTrees` are \"all C++ types,\" a space with a lot of generators and new ones being added with each new standard. Such a problem would involve understanding how ROOT encodes them (and ROOT doesn't include all C++ types, either\u2014there are a few recent ones that are still pending) and how to represent them as Python data. RNTuple is simplifying this considerably by defining a suite of language-independent types (e.g. \"primitive, list, union, record\") at one layer, and then describing C++ types at a layer above that. We would target the language-independent types and leave it to ROOT to interpret them as C++ types in C++.\r\n\r\nBut yeah: friend `TTrees`, I'll look into that when the rest is done.",
  "created_at":"2021-04-14T14:25:03Z",
  "id":819559409,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxOTU1OTQwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-14T14:25:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Happy to hear that you'll look into it! And what you say about treating the language-independent types and leaving the C++ types up to ROOT sounds very reasonable. I didn't quite understand if this is on the roadmap or not.\r\n\r\nLet me comment on how the two problems are connected. Our use-case is that a user supplies a ROOT file with a `TTree`. We run certain calculations and want to add the results of these calculations as branches to the user's `TTree`. Since the user `TTree` is out of our control (and can in principle be quite complicated), writing the whole thing using Uproot seems unrealistic (and inefficient as most of the data is already on disk). What we can do using PyROOT is add a `TTree` with the new branches to the user file and make it a friend of the original `TTree`. This allows the user to work as if their `TTree` had all the branches.",
  "created_at":"2021-04-14T16:18:35Z",
  "id":819644164,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxOTY0NDE2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-14T16:18:35Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"MEMBER",
  "body":"Just to add - this is related to official LHCb PID work for run 3 :-).",
  "created_at":"2021-04-14T16:24:14Z",
  "id":819647898,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxOTY0Nzg5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-14T16:24:14Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Is it a hard requirement to use ROOT file format to store the additional data? Because otherwise there are many options already available, e.g. saving the derived awkward arrays in [parquet](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_parquet.html) or even pickle and then open them and attach with the original tree using `ak.zip()`.",
  "created_at":"2021-04-14T16:29:57Z",
  "id":819651733,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxOTY1MTczMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-14T16:29:57Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Although you're right that friend trees are the right solution here (less data to write), this particular problem could be solved without Uproot actually understanding the branches it's copying. Suppose, for the moment, that Uproot doesn't do friend trees, but you want to make a new TTree that's just like the old one with extra branches added. We could have functionality that copies all TStreamerInfo, the original TBranches with their hierarchical structure, and all their associated TBaskets (maybe without uncompressing them, just copying them as binary blobs, or with uncompressing and merging\u2014understanding the data is not necessary for concatenation), and then also add the new TBranches. That would solve the problem, and maybe it would be good to have that functionality also: \"blind copy.\"\r\n\r\nWith an uncompress-merge-recompress option, this could be useful for repartitioning data (independently of adding new TBranches), as there are a lot of cases with TBaskets or TFiles that are too small. The issues I described above, of needing Python types to mirror the C++ types and needing to know how each is serialized, don't apply to this case, since we can just take the raw byte stream as-is (and also not touch the hierarchical structure of the TBranches, since part of the type information is encoded in the nesting of TBranches within TBranches).",
  "created_at":"2021-04-14T16:38:42Z",
  "id":819658797,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxOTY1ODc5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-14T16:38:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@nsmith- Yes, this is the output of our tool - the end product that the users use. We can't ask them to deal with another file format.\r\n\r\n@jpivarski That definitely sounds interesting and feasible as a solution to our problem.",
  "created_at":"2021-04-14T16:44:01Z",
  "id":819662119,
  "issue":25,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxOTY2MjExOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-14T16:44:01Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"MEMBER",
  "body":"See #321; this is done.  `:)`",
  "created_at":"2021-08-30T21:59:25Z",
  "id":908730405,
  "issue":25,
  "node_id":"IC_kwDOD6Q_ss42KiAl",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-08-30T21:59:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"So nicely, #209 has a TEfficiency which looks like a good vector of attack (pun intended) for memberwise serialization. This can be easily made like so\r\n\r\n```python\r\nimport ROOT\r\n\r\nfp = ROOT.TFile.Open(\"test-efficiency.root\", \"RECREATE\")\r\n\r\nnbins = 11\r\n\r\nh_den = ROOT.TH1F('h_den', 'h_den', nbins, 0, 100)\r\nh_num = ROOT.TH1F('h_num', 'h_num', nbins, 0, 100)\r\n\r\nfor i in range(1, nbins):\r\n    h_num.SetBinContent(i, 2**i)\r\n    h_den.SetBinContent(i, 2**(i+1))\r\n\r\neff = ROOT.TEfficiency(h_num, h_den)\r\neff.SetName('TEfficiencyName')\r\neff.SetTitle('TEfficiencyTitle')\r\n\r\nh_den.Write()\r\nh_num.Write()\r\neff.Write()\r\nfp.Close()\r\n```\r\n\r\nto get a small ROOT file to play around with. This tefficiency does indicate a crash when doing\r\n\r\n```python\r\nwith uproot.open('test-efficiency.root') as fp:\r\n    eff = fp['TEfficiencyName']\r\n```\r\n\r\nlike so\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 12, in <module>\r\n    tree = fp['TEfficiencyName']\r\n  File \"/Users/kratsg/uproot4/uproot/reading.py\", line 1979, in __getitem__\r\n    return self.key(where).get()\r\n  File \"/Users/kratsg/uproot4/uproot/reading.py\", line 2364, in get\r\n    out = cls.read(chunk, cursor, context, self._file, selffile, parent)\r\n  File \"/Users/kratsg/uproot4/uproot/model.py\", line 1181, in read\r\n    versioned_cls.read(\r\n  File \"/Users/kratsg/uproot4/uproot/model.py\", line 800, in read\r\n    self.read_members(chunk, cursor, context, file)\r\n  File \"<dynamic>\", line 12, in read_members\r\n  File \"/Users/kratsg/uproot4/uproot/containers.py\", line 798, in read\r\n    raise NotImplementedError(\r\nNotImplementedError: memberwise serialization of AsVector\r\nin file test-efficiency.root\r\n```\r\n\r\nso we can start here. This is also quick to iterate and make new ROOT files with different values to determine that we have the right offsets.\r\n\r\nThe reason for the `SetName` and `SetTitle` is to match the file in #209 that has the issue. So at least it's just trying to match the structure there as much as possible.",
  "created_at":"2021-03-03T21:25:29Z",
  "id":790065521,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDA2NTUyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-03T21:26:52Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Here's the full sequence of that TEfficiency with the following histograms stored:\r\n\r\n```bash\r\n$ cat tefficiency.py \r\nimport ROOT\r\n\r\n#fp = ROOT.TFile.Open('uproot4-issue209.root')\r\nfp = ROOT.TFile.Open('test-efficiency.root')\r\neff = fp.TEfficiencyName\r\n\r\nnum = eff.GetPassedHistogram()\r\nden = eff.GetTotalHistogram()\r\n\r\nprint([num.GetBinContent(i) for i in range(len(num)+1)])\r\nprint([den.GetBinContent(i) for i in range(len(den)+1)])\r\n\r\n$ roopython3 tefficiency.py \r\n[0.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0, 128.0, 256.0, 512.0, 1024.0, 0.0, 0.0, 0.0]\r\n[0.0, 4.0, 8.0, 16.0, 32.0, 64.0, 128.0, 256.0, 512.0, 1024.0, 2048.0, 0.0, 0.0, 0.0]\r\n```\r\n\r\nas below.\r\n\r\n<details>\r\n  <summary>Click to expand the full byte content of TEfficiency.</summary>\r\n  \r\n```\r\nAsVector::num_bytes=        16\r\nAsVector::instance_version= 9\r\nAsVector::is_memberwise=    0b100_0000_0000_0000 (16384)\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0 215 190 210   0   0   0   0  63 229 216 151 162  65 163 245  64   0\r\n--- --- --- --- --- --- --- --- --- ---   ? --- --- --- ---   A --- ---   @ ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0  17   0   5   0   1   0   0   0   0   3   0   0   0   0   0   0   0   0  64\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---   @\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   2 120 255 255 255 255  84  72  49  70   0  64   0   2 107   0   3  64   0\r\n--- ---   x --- --- --- ---   T   H   1   F ---   @ --- ---   k --- ---   @ ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  2  45   0   8  64   0   0  61   0   1   0   1   0   0   0   0   3   0   0   8\r\n---   - --- ---   @ --- ---   = --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 22  84  69 102 102 105  99 105 101 110  99 121  78  97 109 101  95 112  97 115\r\n---   T   E   f   f   i   c   i   e   n   c   y   N   a   m   e   _   p   a   s\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n115 101 100  25  84  69 102 102 105  99 105 101 110  99 121  84 105 116 108 101\r\n  s   e   d ---   T   E   f   f   i   c   i   e   n   c   y   T   i   t   l   e\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 32  40 112  97 115 115 101 100  41  64   0   0   8   0   2   2  90   0   1   0\r\n      (   p   a   s   s   e   d   )   @ --- --- --- --- --- ---   Z --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  1  64   0   0   6   0   2   0   0   3 233  64   0   0  10   0   2   0   1   0\r\n---   @ --- --- --- --- --- --- --- --- ---   @ --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  1  63 128   0   0   0   0   0  13  64   0   0 109   0  10  64   0   0  19   0\r\n---   ? --- --- --- --- --- --- ---   @ --- ---   m --- ---   @ --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  1   0   1   0   0   0   0   3   0   0   0   5 120  97 120 105 115   0  64   0\r\n--- --- --- --- --- --- --- --- --- --- --- ---   x   a   x   i   s ---   @ ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0  36   0   4   0   0   1 254   0   1   0   1   0  42  59 163 215  10  61  15\r\n---   $ --- --- --- --- --- --- --- --- --- --- ---   *   ; --- --- ---   = ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 92  41  60 245 194 143  63 128   0   0  61  15  92  41   0   1   0  42   0   0\r\n  \\   )   < --- --- ---   ? --- --- ---   = ---   \\   ) --- --- ---   * --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0  11   0   0   0   0   0   0   0   0  64  89   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- ---   @   Y --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0  64   0   0 109   0  10  64   0   0  19   0   1   0   1   0   0   0   0\r\n--- ---   @ --- ---   m --- ---   @ --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  3   0   0   0   5 121  97 120 105 115   0  64   0   0  36   0   4   0   0   1\r\n--- --- --- --- ---   y   a   x   i   s ---   @ --- ---   $ --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n254   0   1   0   1   0  42  59 163 215  10  61  15  92  41  60 245 194 143   0\r\n--- --- --- --- --- ---   *   ; --- --- ---   = ---   \\   )   < --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0  61  15  92  41   0   1   0  42   0   0   0   1   0   0   0   0   0\r\n--- --- ---   = ---   \\   ) --- --- ---   * --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0  63 240   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- ---   ? --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  64   0   0 109   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- ---   @ --- ---   m ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 10  64   0   0  19   0   1   0   1   0   0   0   0   3   0   0   0   5 122  97\r\n---   @ --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---   z   a\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n120 105 115   0  64   0   0  36   0   4   0   0   1 254   0   1   0   1   0  42\r\n  x   i   s ---   @ --- ---   $ --- --- --- --- --- --- --- --- --- --- ---   *\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 59 163 215  10  61  15  92  41  60 245 194 143  63 128   0   0  61  15  92  41\r\n  ; --- --- ---   = ---   \\   )   < --- --- ---   ? --- --- ---   = ---   \\   )\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0  42   0   0   0   1   0   0   0   0   0   0   0   0  63 240   0   0\r\n--- --- ---   * --- --- --- --- --- --- --- --- --- --- --- ---   ? --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   3 232  64  36   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- ---   @   $ --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0 192 145  92   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- ---   \\ --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n192 145  92   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- ---   \\ --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0  64   0   0  17   0   5   0   1   0   0   0   0   3   1   0\r\n--- --- --- --- ---   @ --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0  13   0   0   0   0  64   0   0   0  64 128   0   0  65   0   0   0  65\r\n--- --- --- --- --- --- ---   @ --- --- ---   @ --- --- ---   A --- --- ---   A\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n128   0   0  66   0   0   0  66 128   0   0  67   0   0   0  67 128   0   0  68\r\n--- --- ---   B --- --- ---   B --- --- ---   C --- --- ---   C --- --- ---   D\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0  68 128   0   0   0   0   0   0   0   0   0   0   0   0   0   0  64\r\n--- --- ---   D --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---   @\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   2 113 128   0   0 229  64   0   2 105   0   3  64   0   2  43   0   8  64\r\n--- ---   q --- --- --- ---   @ --- ---   i --- ---   @ --- ---   + --- ---   @\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0  59   0   1   0   1   0   0   0   0   3   0   0   8  21  84  69 102 102\r\n--- ---   ; --- --- --- --- --- --- --- --- --- --- --- --- ---   T   E   f   f\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n105  99 105 101 110  99 121  78  97 109 101  95 116 111 116  97 108  24  84  69\r\n  i   c   i   e   n   c   y   N   a   m   e   _   t   o   t   a   l ---   T   E\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n102 102 105  99 105 101 110  99 121  84 105 116 108 101  32  40 116 111 116  97\r\n  f   f   i   c   i   e   n   c   y   T   i   t   l   e       (   t   o   t   a\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n108  41  64   0   0   8   0   2   2  90   0   1   0   1  64   0   0   6   0   2\r\n  l   )   @ --- --- --- --- --- ---   Z --- --- --- ---   @ --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   3 233  64   0   0  10   0   2   0   1   0   1  63 128   0   0   0   0\r\n--- --- --- ---   @ --- --- --- --- --- --- --- --- ---   ? --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0  13  64   0   0 109   0  10  64   0   0  19   0   1   0   1   0   0   0   0\r\n--- ---   @ --- ---   m --- ---   @ --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  3   0   0   0   5 120  97 120 105 115   0  64   0   0  36   0   4   0   0   1\r\n--- --- --- --- ---   x   a   x   i   s ---   @ --- ---   $ --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n254   0   1   0   1   0  42  59 163 215  10  61  15  92  41  60 245 194 143  63\r\n--- --- --- --- --- ---   *   ; --- --- ---   = ---   \\   )   < --- --- ---   ?\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n128   0   0  61  15  92  41   0   1   0  42   0   0   0  11   0   0   0   0   0\r\n--- --- ---   = ---   \\   ) --- --- ---   * --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0  64  89   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- ---   @   Y --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  64   0   0 109   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- ---   @ --- ---   m ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 10  64   0   0  19   0   1   0   1   0   0   0   0   3   0   0   0   5 121  97\r\n---   @ --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---   y   a\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n120 105 115   0  64   0   0  36   0   4   0   0   1 254   0   1   0   1   0  42\r\n  x   i   s ---   @ --- ---   $ --- --- --- --- --- --- --- --- --- --- ---   *\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 59 163 215  10  61  15  92  41  60 245 194 143   0   0   0   0  61  15  92  41\r\n  ; --- --- ---   = ---   \\   )   < --- --- --- --- --- --- ---   = ---   \\   )\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0  42   0   0   0   1   0   0   0   0   0   0   0   0  63 240   0   0\r\n--- --- ---   * --- --- --- --- --- --- --- --- --- --- --- ---   ? --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0  64   0   0 109   0  10  64   0   0  19   0   1\r\n--- --- --- --- --- --- --- ---   @ --- ---   m --- ---   @ --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   1   0   0   0   0   3   0   0   0   5 122  97 120 105 115   0  64   0   0\r\n--- --- --- --- --- --- --- --- --- --- ---   z   a   x   i   s ---   @ --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 36   0   4   0   0   1 254   0   1   0   1   0  42  59 163 215  10  61  15  92\r\n  $ --- --- --- --- --- --- --- --- --- --- ---   *   ; --- --- ---   = ---   \\\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 41  60 245 194 143  63 128   0   0  61  15  92  41   0   1   0  42   0   0   0\r\n  )   < --- --- ---   ? --- --- ---   = ---   \\   ) --- --- ---   * --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  1   0   0   0   0   0   0   0   0  63 240   0   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- ---   ? --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   3 232  64  36   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- --- --- ---   @   $ --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0 192 145  92   0   0   0   0   0 192 145  92   0   0   0   0\r\n--- --- --- --- --- --- ---   \\ --- --- --- --- --- --- ---   \\ --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  64   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---   @ ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0  17   0   5   0   1   0   0   0   0   3   1   0   0   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   2   0   0   0  13   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64 128   0   0  65   0   0   0  65 128   0   0  66   0   0   0  66 128   0   0\r\n  @ --- --- ---   A --- --- ---   A --- --- ---   B --- --- ---   B --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 67   0   0   0  67 128   0   0  68   0   0   0  68 128   0   0  69   0   0   0\r\n  C --- --- ---   C --- --- ---   D --- --- ---   D --- --- ---   E --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0  63 240   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- ---   ? --- --- --- --- --- --- ---\r\n```\r\n</details>\r\n\r\n\r\n--------------------\r\n\r\nthe numerator is at `603` and then +52 bytes for the full array contents\r\n\r\n```\r\n# offset=623, dtype=\">f4\"\r\n\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0  13   0   0   0   0  64   0   0   0  64 128   0   0  65   0   0   0  65\r\n--- --- --- --- --- --- ---   @ --- --- ---   @ --- --- ---   A --- --- ---   A\r\n                        0.0             2.0             4.0             8.0\r\n    --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n    128   0   0  66   0   0   0  66 128   0   0  67   0   0   0  67 128   0   0  68\r\n    --- --- ---   B --- --- ---   B --- --- ---   C --- --- ---   C --- --- ---   D\r\n           16.0            32.0            64.0           128.0           256.0\r\n    --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n      0   0   0  68 128   0   0   0   0   0   0   0   0   0   0   0   0   0   0  64\r\n    --- --- ---   D --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---   @\r\n          512.0          1024.0             0.0             0.0             0.0\r\n    --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n```\r\n\r\nand the denominator is at `1256` and then +52 bytes for the full array contents\r\n\r\n```\r\n# offset=1256, dtype=\">f4\"\r\n\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0   0   0   0   2   0   0   0  13   0   0   0   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n                                                                            0.0\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64 128   0   0  65   0   0   0  65 128   0   0  66   0   0   0  66 128   0   0\r\n  @ --- --- ---   A --- --- ---   A --- --- ---   B --- --- ---   B --- --- ---\r\n            4.0             8.0            16.0            32.0            64.0\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 67   0   0   0  67 128   0   0  68   0   0   0  68 128   0   0  69   0   0   0\r\n  C --- --- ---   C --- --- ---   D --- --- ---   D --- --- ---   E --- --- ---\r\n          128.0           256.0           512.0          1024.0          2048.0\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0   0   0   0   0   0  63 240   0   0   0   0   0   0\r\n--- --- --- --- --- --- --- ---   ? --- --- --- --- --- --- ---\r\n            0.0             0.0           1.875             0.0\r\n```",
  "created_at":"2021-03-03T21:37:44Z",
  "id":790077349,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDA3NzM0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-03T22:32:03Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Continuing further using this python, it seems to be that the memberwise is complaining about the `AsVector::read` call. So if we look at the streamer for `TEfficiency` here and the corresponding class code\r\n\r\n```python\r\n>>> fp.file.streamer_named('TEfficiency').class_code()\r\n```\r\n\r\nthe `read_members` has this line:\r\n\r\n```python\r\nself._members['fBeta_bin_params'] = self._stl_container0.read(chunk, cursor, context, file, self._file, self._concrete)\r\n```\r\n\r\n<details>\r\n<summary>(click to expand)</summary>\r\n\r\n```python\r\n    def read_members(self, chunk, cursor, context, file):\r\n        if self.is_memberwise:\r\n            raise NotImplementedError(\r\n                \"memberwise serialization of {0}\\nin file {1}\".format(type(self).__name__, self.file.file_path)\r\n            )\r\n        self._bases.append(c('TNamed', 1).read(chunk, cursor, context, file, self._file, self._parent, concrete=self._concrete))\r\n        self._bases.append(c('TAttLine', 2).read(chunk, cursor, context, file, self._file, self._parent, concrete=self._concrete))\r\n        self._bases.append(c('TAttFill', 2).read(chunk, cursor, context, file, self._file, self._parent, concrete=self._concrete))\r\n        self._bases.append(c('TAttMarker', 2).read(chunk, cursor, context, file, self._file, self._parent, concrete=self._concrete))\r\n        self._members['fBeta_alpha'], self._members['fBeta_beta'] = cursor.fields(chunk, self._format0, context)\r\n        self._members['fBeta_bin_params'] = self._stl_container0.read(chunk, cursor, context, file, self._file, self._concrete)\r\n        self._members['fConfLevel'] = cursor.field(chunk, self._format1, context)\r\n        self._members['fFunctions'] = c('TList').read(chunk, cursor, context, file, self._file, self._concrete)\r\n        self._members['fPassedHistogram'] = read_object_any(chunk, cursor, context, file, self._file, self)\r\n        self._members['fStatisticOption'] = cursor.field(chunk, self._format2, context)\r\n        self._members['fTotalHistogram'] = read_object_any(chunk, cursor, context, file, self._file, self)\r\n        self._members['fWeight'] = cursor.field(chunk, self._format3, context)\r\n```\r\n</details>\r\n\r\nwhich indicates that the memberwise is failing for the fBeta_bin_params. So then going back to the python code for making the `TEfficiency`, we do the following\r\n\r\n```python\r\neff.SetBetaBinParameters(0, -1.0, -2.0)\r\nfor i in range(1, nbins):\r\n    eff.SetBetaBinParameters(i, 2**i, 2**(i+1))\r\n\r\neff.SetBetaBinParameters(nbins, -1.0, -2.0)\r\n```\r\n\r\nwhich bookends the alpha parameters by `-1.0` and the beta parameters by `-2.0` to make them easier to identify. Dumping out the chunk again and playing with the offset a bit (finding double-precision values for these parameters, using `>f8`), we find them:\r\n\r\n```\r\n(Pdb) cursor.debug(chunk, dtype=\">f8\", offset=2, limit_bytes=240)\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n  0   0   0 215 190 210   0   0   0  13 191 240   0   0   0   0   0   0  64   0\r\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- ---   @ ---\r\n                1.3525824167906353e-304                            -1.0\r\n        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n          0   0   0   0   0   0  64  16   0   0   0   0   0   0  64  32   0   0   0   0\r\n        --- --- --- --- --- ---   @ --- --- --- --- --- --- ---   @     --- --- --- ---\r\n                            2.0                             4.0\r\n                        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n                          0   0  64  48   0   0   0   0   0   0  64  64   0   0   0   0   0   0  64  80\r\n                        --- ---   @   0 --- --- --- --- --- ---   @   @ --- --- --- --- --- ---   @   P\r\n                            8.0                            16.0                            32.0\r\n        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n          0   0   0   0   0   0  64  96   0   0   0   0   0   0  64 112   0   0   0   0\r\n        --- --- --- --- --- ---   @   ` --- --- --- --- --- ---   @   p --- --- --- ---\r\n                           64.0                           128.0\r\n                        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n                          0   0  64 128   0   0   0   0   0   0  64 144   0   0   0   0   0   0 191 240\r\n                        --- ---   @ --- --- --- --- --- --- ---   @ --- --- --- --- --- --- --- --- ---\r\n                          256.0                           512.0                          1024.0\r\n        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n          0   0   0   0   0   0  63 240   0   0   0   0   0   0 192   0   0   0   0   0\r\n        --- --- --- --- --- ---   ? --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n                           -1.0                             1.0\r\n                        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n                          0   0  64  16   0   0   0   0   0   0  64  32   0   0   0   0   0   0  64  48\r\n                        --- ---   @ --- --- --- --- --- --- ---   @     --- --- --- --- --- ---   @   0\r\n                           -2.0                             4.0                             8.0\r\n        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n          0   0   0   0   0   0  64  64   0   0   0   0   0   0  64  80   0   0   0   0\r\n        --- --- --- --- --- ---   @   @ --- --- --- --- --- ---   @   P --- --- --- ---\r\n                           16.0                            32.0\r\n                        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n                          0   0  64  96   0   0   0   0   0   0  64 112   0   0   0   0   0   0  64 128\r\n                        --- ---   @   ` --- --- --- --- --- ---   @   p --- --- --- --- --- ---   @ ---\r\n                           64.0                           128.0                           256.0\r\n        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n          0   0   0   0   0   0  64 144   0   0   0   0   0   0  64 160   0   0   0   0\r\n        --- --- --- --- --- ---   @ --- --- --- --- --- --- ---   @ --- --- --- --- ---\r\n                          512.0                          1024.0\r\n                        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n                          0   0 192   0   0   0   0   0   0   0  63 240   0   0   0   0   0   0  63 229\r\n                        --- --- --- --- --- --- --- --- --- ---   ? --- --- --- --- --- --- ---   ? ---\r\n                         2048.0                            -2.0                             1.0\r\n        --+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n        216 151 162  65 163 245  64   0   0  17   0   5   0   1   0   0   0   0   3   0\r\n        --- --- ---   A --- ---   @ --- --- --- --- --- --- --- --- --- --- --- --- ---\r\n                 0.682689492137              2.0000324250722774\r\n```\r\n\r\nSo to summarize so far, we have (for the chunk + cursor):\r\n- 10 byte header `0  0  0 215 190 210  0  0  0 13 `\r\n- 96 bytes data (12 entries of double-precision floats bookended by `-1.0`)\r\n- 8 byte header `63 240   0   0   0   0   0   0`\r\n- 96 bytes data (12 entries of double-precision floats bookended by `-2.0`)\r\n- 8 byte header `63 240   0   0   0   0   0   0` (unclear if this is part of the fBeta_bin_params` or not)\r\n- [and other stuff in the rest of the chunk]",
  "created_at":"2021-03-03T23:44:56Z",
  "id":790156745,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDE1Njc0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-03T23:44:56Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ok, getting further, we have a slight issue that I think `uproot` might need to be refactored since I think memberwise streaming breaks the current `Model.read` assumptions. Let me step through what I see happen.\r\n\r\n1. Cursor(113, origin=-71)\r\n-> _something_else = cursor.field(chunk, struct.Struct(\">H\"), context)\r\n1. Cursor(117, origin=-71)\r\n-> length = cursor.field(chunk, _stl_container_size, context)\r\n1. Cursor(119, origin=-71)\r\n-> values = _read_nested(\r\n1. Cursor(123, origin=-71)\r\n\r\nwith this initial \"preprocessing\" sequence, we have:\r\n\r\n- `_num_memberwise_bytes=215` (maybe? there's actually 214 bytes left to read the rest of the structure, not 215.. coincidence?)\r\n- `_something_else=48850` (maybe a version of some sort? unsure. still seems _very_ odd)\r\n- `length=13` (this at least makes sense)\r\n\r\nSo far, so good. `length` here refers to the length of the `std::vector` (which we always want as we allocate that many values when reading). The `_num_memberwise_bytes` is interesting, as it seems to be off-by-one perhaps (will remake this file but with more entries in the `std::vector` to see...)\r\n\r\nNow, the problem is the following. At this point, we make a call to `_read_nested`:\r\n\r\n```python\r\n        values = _read_nested(\r\n            self._values, length, chunk, cursor, context, file, selffile, parent\r\n        )\r\n```\r\n\r\nwhich goes into this loop\r\n\r\n```python\r\n            for i in uproot._util.range(length):\r\n                values[i] = model.read(chunk, cursor, context, file, selffile, parent)\r\n                print(cursor, values[i])\r\n```\r\n\r\nwhere `values` is a 13-element array allocated correctly. However, the values coming out of this are nonsensical... But it was clearly fine! In fact, the `model.read` is shifting the cursor another 2 bytes to read a version. Here's a portion of `model.read`:\r\n\r\n```python\r\n        self.hook_before_read(chunk=chunk, cursor=cursor, context=context, file=file)\r\n\r\n        self.read_numbytes_version(chunk, cursor, context)\r\n```\r\n\r\nwhere `self.read_numbytes_version` will read in 2 bytes to grab the version. Problematically, as you can see, we have what might be a version number _before_ the length instead of after. This makes things difficult. So my idea is one of two ways:\r\n\r\n- add a hook before read to shift the cursor index back two (and hope that the version number being read in being gibberish doesn't break anything)\r\n- shift back the cursor manually before starting the call [although this means we need to do it for every `model.read` call I think]\r\n\r\nI found the `hook` to be too hard to figure out (seems that it's something that ROOT might have to hook into before or after creating) so I ended up implementing the second option using `rollback_nbytes` like so:\r\n\r\n```python\r\ndef _read_nested(\r\n    model, length, chunk, cursor, context, file, selffile, parent, header=True, rollback_nbytes=0\r\n):\r\n    if isinstance(model, numpy.dtype):\r\n        return cursor.array(chunk, length, model, context)\r\n\r\n    else:\r\n        values = numpy.empty(length, dtype=_stl_object_type)\r\n        if isinstance(model, AsContainer):\r\n            for i in uproot._util.range(length):\r\n                cursor._index = cursor._index - rollback_nbytes\r\n                values[i] = model.read(\r\n                    chunk, cursor, context, file, selffile, parent, header=header\r\n                )    \r\n        else:\r\n            for i in uproot._util.range(length):\r\n                cursor._index = cursor._index - rollback_nbytes\r\n                values[i] = model.read(chunk, cursor, context, file, selffile, parent)\r\n                print(cursor, values[i])\r\n        return values\r\n```\r\n\r\nwhich is CLEARLY hacky but I'm ok with this for right now. I'm able to read out the memberwise object without an error, but I need to now teach the `Cursor` to read in a memberwise fashion (jumping around the cursor for me instead).",
  "created_at":"2021-03-05T04:28:57Z",
  "id":791144309,
  "issue":38,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MTE0NDMwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-05T04:28:57Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I rediscovered this while working on _writing_ compressed data, and included a quick test of reading compressed data for all algorithms here:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/42d078b53c9a456ec6bb4b0d04d42c7a166c1e08/tests/test_0416-writing-compressed-data.py#L16-L75",
  "created_at":"2021-08-30T22:01:11Z",
  "id":908731400,
  "issue":39,
  "node_id":"IC_kwDOD6Q_ss42KiQI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-30T22:01:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm using this now on a MiniAOD file, which has 2299 branches, and it takes a human-perceptible amount of time, like a few seconds. Fortunately, it shows the \"`raise uproot.KeyInFileError(`\" text while it computes all those Damerau-Levenshtein distances, so at least I know why it's pausing.\r\n\r\n(And searching through those 2299 branches by hand to find my misspelling would be much worse! So thanks again for this!)",
  "created_at":"2021-03-19T13:46:43Z",
  "id":802845011,
  "issue":127,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjg0NTAxMQ==",
  "performed_via_github_app":null,
  "reactions":{
   "rocket":1,
   "total_count":1
  },
  "updated_at":"2021-03-19T13:46:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Wait a minute\u2014I've written that: https://uproot.readthedocs.io/en/latest/uproot3-to-4.html\r\n\r\nI can close this!",
  "created_at":"2021-02-18T15:01:54Z",
  "id":781404858,
  "issue":169,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTQwNDg1OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T15:01:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm closing this as a duplicate of #38, which is still a high-priority item. Consolidation makes it easier for me to see what needs to be done.",
  "created_at":"2021-02-18T15:00:59Z",
  "id":781404163,
  "issue":209,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTQwNDE2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T15:00:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've tried this out, and it is as you say:\r\n\r\n```python\r\ninput_data = glob(\"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\")\r\n```\r\n\r\nmakes it hang (I waited about a minute) and\r\n\r\n```python\r\ninput_data = [\"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\"]\r\n```\r\n\r\nworks normally. However, if we inspect the return type of XRootD's `glob`,\r\n\r\n```python\r\n>>> type(input_data)\r\n<class 'list'>\r\n>>> type(input_data[0])\r\n<class 'str'>\r\n>>> type(input_data) is list\r\nTrue\r\n>>> type(input_data[0]) is str\r\nTrue\r\n```\r\n\r\nit's a totally normal list of Python strings. There's nothing in this data object that could cause a difference in Uproot.\r\n\r\nBy process of elimination, then, simply calling `glob` is putting XRootD into a state that determines whether it hangs. This function evidently has a side-effect. To test that, I ran your code with one small change:\r\n\r\n```python\r\nfrom concurrent import futures\r\nfrom XRootD.client.glob_funcs import glob\r\nimport uproot\r\n\r\ndef simple_func(input_path, tree):\r\n    print(\"starting\", input_path, tree)\r\n    return [uproot.open(input_path)[tree].num_entries]\r\n\r\n# Call it and forget about it.\r\nglob(\"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\")\r\n\r\ninput_data = [\"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\"]\r\ntree = 'Events'\r\n\r\nprint(\"Input Data: {}\".format(input_data))\r\nprint(\"Starting ProcessPoolExectutor\")\r\nwith futures.ProcessPoolExecutor(max_workers=1) as executor:\r\n    tasks = set()\r\n    tasks.update(executor.submit(simple_func, path, tree) for path in input_data)\r\n    complete_tasks = futures.as_completed(tasks)\r\n\r\n    for job in complete_tasks:\r\n        print(job.result())\r\n```\r\n\r\nThis hangs, too. In fact, I can toggle it by commenting out the completely unused `glob` call.\r\n\r\nHere's a minimally reproducing example to report to the pyxrootd group: it fails if glob is called in the main process _and_ the file is read from a child process. With the eight cases you can test below, only the two `(ProcessPoolExecutor OR multiprocessing) AND (call_glob)` actually hang.\r\n\r\n```python\r\nimport XRootD.client\r\nfrom concurrent import futures\r\nimport threading\r\nimport multiprocessing\r\nfrom XRootD.client.glob_funcs import glob\r\n\r\ndef simple_func(input_path, tree):\r\n    file = XRootD.client.File()\r\n    status, _ = file.open(input_data[0])\r\n    print(status)\r\n\r\n    status, some_raw_bytes = file.read(0, 100)\r\n    print(status)\r\n    print(some_raw_bytes)\r\n\r\n    status, some_raw_bytes = file.read(100, 200)\r\n    print(status)\r\n    print(some_raw_bytes)\r\n\r\n    return \"yay\"\r\n\r\n# Call it and forget about it.\r\nglob(\"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\")\r\n\r\ninput_data = [\"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\"]\r\ntree = 'Events'\r\n\r\nwhich_test = \"multiprocessing\"\r\n\r\nif which_test == \"ProcessPoolExecutor\":\r\n    with futures.ProcessPoolExecutor(max_workers=1) as executor:\r\n        future = executor.submit(simple_func, input_data[0], tree)\r\n        print(future.result())\r\n\r\nelif which_test == \"ThreadPoolExecutor\":\r\n    with futures.ThreadPoolExecutor(max_workers=1) as executor:\r\n        future = executor.submit(simple_func, input_data[0], tree)\r\n        print(future.result())\r\n\r\nelif which_test == \"multiprocessing\":\r\n    class MyProcess(multiprocessing.Process):\r\n        def run(self):\r\n            print(\"before\")\r\n            print(simple_func(input_data[0], tree))\r\n            print(\"after\")\r\n    myprocess = MyProcess()\r\n    myprocess.start()\r\n    myprocess.join()\r\n\r\nelse:\r\n    class MyThread(threading.Thread):\r\n        def run(self):\r\n            print(\"before\")\r\n            print(simple_func(input_data[0], tree))\r\n            print(\"after\")\r\n    mythread = MyThread()\r\n    mythread.start()\r\n    mythread.join()\r\n```\r\n\r\nSince this is not an Uproot bug or an Uproot failure mode triggered by a pyxrootd behavior, I'm closing it. (I hadn't noticed before that you are only asking for the number of entries, which only does an `XRootD.client.File.read`, not any fancy vector-reads, like the TBasket-fetching code does. Thinking it was the latter, I had some ideas of how Uproot might be hanging, but it's just failing in `XRootD.client.File.open`.)",
  "created_at":"2021-02-18T16:46:01Z",
  "id":781480175,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTQ4MDE3NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-18T16:46:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks Jim for looking into this, I'll let the pyxrootd group know about the issue",
  "created_at":"2021-02-19T10:12:08Z",
  "id":781976527,
  "issue":224,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTk3NjUyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-19T10:12:08Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah\u2014all this PR does is ignore XRootD's error.\r\n\r\nI've used weak references once (so that Awkward Array's virtual array caches don't have circular references through C++, which Python's garbage collector can't detect), and I have to say, they're really hard to get right. Also, `atexit` is a strange time in the Python process when a lot of things you expect to be present are not. It's not surprising that they're having trouble with it, but if they need it (because they go through C++, like Awkward Array does), then they'll have to work it out.\r\n\r\nThe good news is that it happens after all of your code is done, so it's something of a cosmetic problem, unless the failure to close some files prevents other files from being closed, and that somehow has consequences beyond the lifetime of the process.",
  "created_at":"2021-01-08T14:47:06Z",
  "id":756792965,
  "issue":237,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Njc5Mjk2NQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-08T14:47:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The reason for the Uproot 3 \u2192 Uproot 4 transition was to change the interface. Special functions for Pandas are not gone, but any function that produces an array (like `mytree.arrays`) can produce a Pandas DataFrame or Series if you pass `library=\"pd\"` as an argument. See\r\n\r\nhttps://uproot.readthedocs.io/en/latest/\r\n\r\nfor more details.\r\n\r\nIf you have a large script that would be difficult to transition to the new interface, you can install the `uproot3` package instead. This package is the last version with the old interface. Replacing\r\n\r\n```python\r\nimport uproot\r\n```\r\n\r\nwith\r\n\r\n```python\r\nimport uproot3 as uproot\r\n```\r\n\r\nwill give you all the old behavior, with a warning about the version not being supported anymore. (Hint: warnings can be [suppressed for one line of code](https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings) if you don't want to see this message.)",
  "created_at":"2021-01-08T13:39:17Z",
  "id":756760248,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Njc2MDI0OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-08T13:39:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for a quick reply",
  "created_at":"2021-01-08T13:44:33Z",
  "id":756762551,
  "issue":238,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Njc2MjU1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-08T13:44:33Z",
  "user":"MDQ6VXNlcjE1ODAyMTA5"
 },
 {
  "author_association":"MEMBER",
  "body":"I haven't heard back, but to be sure this doesn't get forgotten, I'm going to merge it now. (It works and is definitely an improvement.)",
  "created_at":"2021-01-11T23:33:44Z",
  "id":758291375,
  "issue":239,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1ODI5MTM3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T23:33:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"HI, sorry I just saw it. I upgraded to 4.0.11 and now it works!",
  "created_at":"2021-07-08T15:47:40Z",
  "id":876549218,
  "issue":239,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NjU0OTIxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-08T15:47:40Z",
  "user":"MDQ6VXNlcjExNzU3Mjg0"
 },
 {
  "author_association":"MEMBER",
  "body":"> here's a test script to work with\r\n\r\nOh, and if that URL is stable, it can be included as a test in Uproot's test suite (numbered with \"0240\" to reference this PR). It just needs to be marked as \"network\" ([like this](https://github.com/scikit-hep/uproot4/blob/434f51f0e3f3d48fd890d071edafabbba35729a4/tests/test_0001-source-class.py#L105)) so that it can be turned off if someone doesn't have network access.",
  "created_at":"2021-01-08T19:33:13Z",
  "id":756953247,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Njk1MzI0Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-08T19:33:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Oh, and if that URL is stable, it can be included as a test in Uproot's test suite (numbered with \"0240\" to reference this PR). It just needs to be marked as \"network\" ([like this](https://github.com/scikit-hep/uproot4/blob/434f51f0e3f3d48fd890d071edafabbba35729a4/tests/test_0001-source-class.py#L105)) so that it can be turned off if someone doesn't have network access.\r\n\r\nI've gone ahead and added a full-fledged network-based test for this. We definitely should do something similar for pyhf for network-related tests (if we don't already @matthewfeickert ).",
  "created_at":"2021-01-09T01:59:24Z",
  "id":757077344,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzA3NzM0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T01:59:24Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski maybe I'll leave this here for now. I thought you would've had python `requests` as part of the test suite. HEPData doesn't allow streaming or chunking ROOT files (I tried) and you just need to download the file entirely which is how I wrote the test... do you want to add `requests` as a test dependency? \ud83e\udd37\u200d\u2642\ufe0f",
  "created_at":"2021-01-09T02:04:24Z",
  "id":757077954,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzA3Nzk1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T02:04:24Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"I suppose `requests` could be a test dependency. I had been avoiding it in the main codebase because I wanted to find out what we're paying for with that dependency. (One thing is the automated handling of redirects, but that was easy enough to add to the built-in Python HTTP client.)\r\n\r\nThe primary place for test files is `scikit-hep-testdata`, which get installed as local files, but the process of adding a file to that repo and pushing a new version of it all before you can test a PR here is a bit arduous. We should probably find a way to distribute local files for tests in a way that doesn't involve PyPI. But that's a longer term project.\r\n\r\nAs far as I know, Uproot has been the only project using `scikit-hep-testdata`, but if you have similar needs in pyhf, we could maybe band together to make a common test file source (accessible as both HTTP and local, maybe even XRootD, without file size limits and without the steps involved in publishing versions, since the files rarely/never change, they just get added to).\r\n\r\nI have a big collection of files on my computer named \"uproot4-lazy\", which are files associated to Uproot 4 issues that I have been too lazy to add to `scikit-hep-testdata`. They're each about a MB or less.",
  "created_at":"2021-01-09T17:12:37Z",
  "id":757337004,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzMzNzAwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T17:12:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> As far as I know, Uproot has been the only project using `scikit-hep-testdata`, but if you have similar needs in pyhf, we could maybe band together to make a common test file source (accessible as both HTTP and local, maybe even XRootD, without file size limits and without the steps involved in publishing versions, since the files rarely/never change, they just get added to).\r\n\r\nWe certainly do have similar needs in pyhf. I think we're going to start seeing this needed for ServiceX as well and various other IRIS-HEP projects. I wonder if it's possible to talk with the Chicago folks here about having something like faxbox available for use. I know we've used CVMFS in the past to unpack and store various docker images so that grid sites only need CVMFS in order to access a subset of docker images (if they are registered there). One concern i've had generally is that this decouples the files from the codes that use it, and it's much harder to figure out who depends on a file when it needs to get updated or changed, or what that policy would be.\r\n\r\nThis file isn't abundantly large and I could get a pure-python only variation of the setUp fixture here in order to make this work (based on how I factorized the tests) so it's not the end of the world here. That said, i've always thought test dependencies were mostly not as restrictive since only the CI and developers would need to grab them. I guess you're trying to improve the developer experience here.",
  "created_at":"2021-01-09T21:33:39Z",
  "id":757370391,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzM3MDM5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T21:33:39Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"> That said, i've always thought test dependencies were mostly not as restrictive since only the CI and developers would need to grab them.\r\n\r\nThat's what I'm saying: there's a much lower bar to including a dependency for the tests, and I'm definitely willing to make `requests` a test dependency.",
  "created_at":"2021-01-09T22:04:56Z",
  "id":757374095,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzM3NDA5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T22:04:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Requests is the forth most popular package https://pythonwheels.com , I certainly wouldn't worry about it as a dependency, especially for tests. :)",
  "created_at":"2021-01-09T22:23:43Z",
  "id":757376457,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzM3NjQ1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-09T22:23:43Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"> As far as I know, Uproot has been the only project using scikit-hep-testdata, but if you have similar needs in pyhf, we could maybe band together to make a common test file source (accessible as both HTTP and local, maybe even XRootD, without file size limits and without the steps involved in publishing versions, since the files rarely/never change, they just get added to).\r\n\r\nFor completeness - also `pylhe` uses it. For now I believe it only contains in there a single test file, though. So that's 3 org packages using the test package, which is nice.",
  "created_at":"2021-01-11T08:25:53Z",
  "id":757706312,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1NzcwNjMxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T08:25:53Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> For completeness - also `pylhe` uses it. For now I believe it only contains in there a single test file, though. So that's 3 org packages using the test package, which is nice.\r\n\r\nIndeed. My only issue is that this file is not a good candidate for a common test utility for multiple packages. `uproot` has the slightly special case that while it's great for it to test lots of \"common\" ROOT files, it also needs to handle edge cases as well as minimal cases which are not of as-interest for other packages using the `scikit-hep-testdata`. There's a similar issue with `pyhf` because we only want `HistFactory` objects in these files which isn't that useful for something like `uproot` at the moment.\r\n\r\nI tend to agree with @jpivarski that it feels a lot more cumbersome to add to the `scikit-hep-testdata` repo for something like this.",
  "created_at":"2021-01-11T14:05:37Z",
  "id":757972833,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Nzk3MjgzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T14:05:37Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes. Do keep in mind that the test files can also be handy for tutorials, sometimes. And some (a very small subset) *have* been used in tutorials and hands-on. For example the pylhe test file could also be useful for pyhepmc ...\r\n\r\nI'm not saying the situation of having to make a release is perfect.",
  "created_at":"2021-01-11T14:11:55Z",
  "id":757976372,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Nzk3NjM3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T14:11:55Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"Ideally, we should haven an online repo of files, and scikit-hep-testdata would just be an interface to that online repository, save for a few, tutorial-ready files locally. ",
  "created_at":"2021-01-11T14:33:05Z",
  "id":757989246,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Nzk4OTI0Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-11T14:33:05Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"It could be modeled after importlib.resouces.files in Python 3.9, where it returns an object that behaves like a Path opens the remote resource.",
  "created_at":"2021-01-11T14:34:51Z",
  "id":757990356,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1Nzk5MDM1Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2021-01-11T14:34:51Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"fyi @jpivarski this is ready to go.",
  "created_at":"2021-01-11T22:26:37Z",
  "id":758264401,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1ODI2NDQwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T22:26:37Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"> fyi @jpivarski this is ready to go.\r\n\r\nThanks! I just added some formatting and documentation and will merge it when it passes. (I also manually tested Python 2.7.)",
  "created_at":"2021-01-11T23:07:25Z",
  "id":758281103,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1ODI4MTEwMw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-11T23:07:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @kratsg  for code",
  "created_at":"2021-01-11T23:31:14Z",
  "id":758290339,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1ODI5MDMzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T23:31:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/241) to add @kratsg! :tada:",
  "created_at":"2021-01-11T23:31:23Z",
  "id":758290392,
  "issue":240,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1ODI5MDM5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-11T23:31:23Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for pointing this out\u2014the `library=\"cp\"` interface has not been consistently tested, as it can't be included in CI that runs on machines without GPUs. I'll take this as a note to give the `library=\"cp\"` interface more attention (or remove it if I'm unable to do so).\r\n\r\nThat bug looks like an easy one to fix: it looks like a NumPy array is being assigned into a CuPy array. I doubt it's related to the specific file that you're using, but thanks for including it in case it is. Uproot's `library=\"cp\"` interface is limited to simple data types (rectilinear arrays of numbers) because that's what CuPy supports. Thus, there aren't very many types to test. (For more complex data on GPUs, Awkward Array's GPU support is coming along...)\r\n\r\nIf you need a way to get this to work, you can definitely use `library=\"np\"` and then transfer the data to CuPy in a subsequent step. The data begin life in main memory, so there's always going to be _some_ transfer from main memory to the GPU, whether that happens inside of Uproot or in your code, one step afterward. There isn't a major performance benefit to having that step be in Uproot, maybe just the fact that copying from individual TBasket arrays into a single output array can also be the copy from main memory to GPU. There might also be a convenience to getting a dict of CuPy arrays without having to iterate that dict to copy them yourself. Okay, that's sounding like reason enough to not just remove the `library=\"cp\"` option.\r\n\r\n(The `library=\"cp\"` option was originally added to convince myself of the generality of this `library` argument as a concept.)",
  "created_at":"2021-01-12T18:25:54Z",
  "id":758850024,
  "issue":242,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc1ODg1MDAyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-12T18:25:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the quick response. That's cool that Awkward Array GPU support is coming, that will be very helpful as some of the data I use is jagged so requires Awkward arrays anyways. \r\n\r\nShould I just close this issue as Awkward Array GPU support is coming and that's more likely to be preferred than cupy?",
  "created_at":"2021-01-14T16:55:23Z",
  "id":760324537,
  "issue":242,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MDMyNDUzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-14T16:55:23Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "author_association":"MEMBER",
  "body":"No, there's still a real bug here: this issue is a reminder to do something about it. I either have to fix it or remove the `library=\"cp\"` support. Maintenance of this feature will be difficult if it can't be tested regularly in CI. I'll have to think about that.",
  "created_at":"2021-01-14T17:21:08Z",
  "id":760340482,
  "issue":242,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MDM0MDQ4Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-14T17:21:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As you can see in #279, I've decided to remove CuPy support. It wouldn't have been a huge performance advantage to copy the data to the GPU in the TBasket-to-concatenated-array stage, rather than one step later in your own code (i.e. you read the array into main memory and then copy it to the GPU yourself). Without a way to regularly test it in CI, we can't say that we really \"supported\" the feature, anyway. At least now we're not leading you on.",
  "created_at":"2021-02-18T14:59:09Z",
  "id":781402764,
  "issue":242,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTQwMjc2NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-18T14:59:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! I'm glad that you're taking this on\u2014you likely know a lot more about XRootD than I do.\r\n\r\nAs for merging Chunks, the thing to do would be for the Chunk's `future` to be a function that initiates the next step in the operation. The HTTP service used to do this when it was handling a case that gets the length of the file _and then_ gets the data, though I've removed that because that optimization of fetching the last bytes in the file didn't provide a benefit to outweigh its complexity. I also tried replacing the `future` object from the relevant Chunks, which is perhaps more complicated than writing the `future` function in such a way that it could do either operation.\r\n\r\nDownstream of this, Uproot only uses the single-Chunk interface (for everything except TBaskets) and the `notifications` queue of the multi-Chunk interface (for TBaskets), so it can deal with Chunks in any order and only starts working on a Chunk when its `future` function finishes.\r\n\r\n   * [This wiki](https://github.com/scikit-hep/uproot4/wiki/Physical-layer-interface) explains that in detail.\r\n   * Early tests, [like this one](https://github.com/scikit-hep/uproot4/blob/main/tests/test_0001-source-class.py), access the physical layer in isolation, which is probably how you want to do it when debugging.\r\n\r\nIf having the `future` of each Chunk deal with the exceptional case by putting extra logic in that function is too complex, then you might want to instead consider a two-phase approach: in the `XRootDSource.chunks` method, do one request that will determine how it _ought_ to be chunked, then do another that requests XRootD-friendly chunks and if a single Uproot Chunk requires more than one of these XRootD-friendly chunks, then it has to wait on them all.\r\n\r\nAnother possibility: it might actually simplify things to have two ResourceExecutors, one that the Uproot sets up to wait for its Chunks, and another that the XRootDSource sets up to wait for XRootD-friendly chunks that feed into the Uproot Chunks.\r\n\r\nThese are just some initial hints. I don't understand the problem well enough to make a definitive suggestion, but you can contact me on Gitter ([Scikit-HEP/uproot](https://gitter.im/Scikit-HEP/uproot)) to talk it over in real time.",
  "created_at":"2021-01-14T15:06:41Z",
  "id":760255599,
  "issue":243,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MDI1NTU5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-14T15:06:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks @jpivarski again for the detailed explanation! That wiki page was also very helpful.\r\n\r\nI tried it now with a variant similar to the suggestion of having 2 separate Executors. The chunks in the `XRootDSource` are actually retrieved using the internal threads from the xrootd library and receive a python callback that calls `_run` for the futures to trigger the notifications. I added a `ResourceThreadPoolExecutor` to `XRootDSource` that executes potential tasks for merging sub-chunks. Those tasks receive the futures of the sub-chunks, wait for them and merge the buffers. Maybe i should also add a `num_workers` argument since this now probably only uses one thread.\r\n\r\n@jpivarski  you mentioned that uproot also sets up a ResourceExecutor (meaning outside of the Source?). Can i access this from within the Source and use that instead of the Executor i added to `XRootDSource`?",
  "created_at":"2021-01-15T13:11:03Z",
  "id":760933398,
  "issue":243,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MDkzMzM5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-15T13:11:03Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"> @jpivarski you mentioned that uproot also sets up a ResourceExecutor (meaning outside of the Source?). Can i access this from within the Source and use that instead of the Executor i added to `XRootDSource`?\r\n\r\nAll I meant was that Uproot maintains a single ResourceExecutor (passed in by the user or a default of `array`/`arrays`/etc.), but if it helps for XRootDSource to have a second ResourceExecutor to manage splitting and grouping of XRootD-friendly chunks into the Chunks that Uproot actually asked for, then do so.\r\n\r\nThere isn't another ResourceExecutor beyond Uproot's.",
  "created_at":"2021-01-15T16:18:17Z",
  "id":761037519,
  "issue":243,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MTAzNzUxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-15T16:18:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think this works now. The `XRootDSource` now has a `ResourceExecutor` to which it submits futures that either wait for the xrootd chunks if they are the same as requested or waits for several of them and merges them if they had to be split up. First i tried to only submit futures in case of merging and use the futures that are given to the xrootd `vector_read` function in case of no merging, but doing so i made the mistake to call `_set_notify` after passing the callback to xrootd which subsequently calls `_run`. That lead to deadlocks in case the thread that reads the queue was faster (took me quite a long time to debug this and i only noticed it by running a larger scale test with 1000 files on 100 workers). So now all chunks in the notifications queue come from futures submitted to the `ResourceExecutor` of `XRootDSource` (and not from the ones passed to `vector_read`).\r\n\r\nThe `ResourceExecutor` of `XRootDSource` actually doesn't need a resource, since it only forwards or merges chunks that are retrieved via the main xrootd File (and xrootd has it's own threads ...). So i passed `None` as a Resource in the constructor - is this fine? I still wanted to have the rest of the `ResourceExecutor` and `ResourceFuture` methods like `_set_notify`.",
  "created_at":"2021-01-18T18:19:05Z",
  "id":762405957,
  "issue":243,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjQwNTk1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-18T18:19:05Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"If XRootDSource's `ResourceThreadPoolExcutor` doesn't actually need a resource, then it might as well be a plain [ThreadPoolExecutor](https://github.com/scikit-hep/uproot4/blob/06a17bea7ffb3b9cb17553435daa85ad5b94d421/uproot/source/futures.py#L178-L250). The difference between these is that the \"resource\" one passes a resource as the first argument to the future, so it sounds like you can use the simple one.\r\n\r\nUproot's `ThreadPoolExecutor` is functionally identical to a subset of Python 3's `ThreadPoolExecutor`, but I explicitly defined that subset so that it can be used in Python 2 (and to make the generalization to `ResourceThreadPoolExcutor` explicit by putting them side-by-side).",
  "created_at":"2021-01-18T18:28:31Z",
  "id":762410286,
  "issue":243,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjQxMDI4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-18T18:28:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The example from @nsmith- in #122 fails without the fix and works now:\r\n\r\n```pycon\r\n>>> import uproot\r\n>>> file = uproot.open(\"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root\")\r\n>>> array1 = file[\"Events/Muon_eta\"].array(entry_stop=244800)\r\n```\r\n\r\nwe can also cross check this against `MultithreadedXRootDSource`\r\n\r\n```pycon\r\n>>> file2 = uproot.open(\r\n...     \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root\",\r\n...     xrootd_handler=uproot.MultithreadedXRootDSource\r\n... )\r\n>>> array2 = file2[\"Events/Muon_eta\"].array(entry_stop=244800)\r\n>>> import awkward as ak\r\n>>> assert ak.all(array1 == array2)\r\n```\r\n\r\nI chose `ResourceThreadPoolExecutor`/`ResourceFuture` over `ThreadPoolExecutor`/`Future` because it has this mechanism of adding the chunk to the notifications queue (via `_notify`, `_set_notify`), which `ThreadPoolExecutor`/`Future` lack.",
  "created_at":"2021-01-18T19:25:26Z",
  "id":762432956,
  "issue":243,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjQzMjk1Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-18T19:25:26Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for the explicit check!\r\n\r\n> I chose `ResourceThreadPoolExecutor`/`ResourceFuture` over `ThreadPoolExecutor`/`Future` because it has this mechanism of adding the chunk to the notifications queue (via `_notify`, `_set_notify`), which `ThreadPoolExecutor`/`Future` lack.\r\n\r\nThat makes sense. Okay, I guess this is ready!",
  "created_at":"2021-01-18T19:45:12Z",
  "id":762440622,
  "issue":243,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjQ0MDYyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-18T19:45:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This should be fixed in PR #245. The reason this happened is because [ak.zip](https://awkward-array.readthedocs.io/en/latest/_auto/ak.zip.html) had a surprising behavior if its arguments were regular arrays (like your `covariance`). They were \"[right broadcasted](https://awkward-array.readthedocs.io/en/latest/_auto/ak.broadcast_arrays.html),\" a feature intended for consistency with NumPy when computing ufuncs, but it doesn't really belong in `ak.zip` (though they both use the same function to do broadcasting\u2014a consistency that is unwanted in this case!).\r\n\r\nI ~~changed~~ fixed `ak.zip` in the Awkward Array library (scikit-hep/awkward-1.0#656) and also avoided using `ak.zip` in Uproot where an [ak.Array dict constructor](https://awkward-array.readthedocs.io/en/latest/_auto/ak.Array.html) would do the same thing. That way, you get the right behavior regardless of whether you've updated Uproot, Awkward Array, or both.",
  "created_at":"2021-01-18T14:28:08Z",
  "id":762284601,
  "issue":244,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjI4NDYwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-18T14:28:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Thanks for the lightning fast response on this Jim, it's much appreciated! :tada: ",
  "created_at":"2021-01-18T14:56:55Z",
  "id":762301452,
  "issue":244,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MjMwMTQ1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-18T14:56:55Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Is the behavior you see similar to #244? If so, that was a bug, fixed in PR #245.\r\n\r\nIf it is the same issue, then I'd better push a new release soon because multiple people are running into it!",
  "created_at":"2021-01-19T16:33:53Z",
  "id":762962433,
  "issue":248,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Mjk2MjQzMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-19T16:33:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ah, yes, I suspect that is probably the same thing. Sorry for the duplicate report - I just searched \"concatenate\" to look for previous issues. \r\n\r\nI'll wait for the next release then!",
  "created_at":"2021-01-19T16:36:36Z",
  "id":762964264,
  "issue":248,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Mjk2NDI2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-19T16:36:36Z",
  "user":"MDQ6VXNlcjg1OTM0ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"In a few minutes, [4.0.2rc1](https://github.com/scikit-hep/uproot4/releases/tag/4.0.2rc1) will be available [on PyPI](https://pypi.org/project/uproot/#history). Then you'll be able to\r\n\r\n```bash\r\npip install -U \"uproot>=4.0.2rc1\"\r\n```\r\n\r\nand try it out. Another test to see if #244 is the same thing would be to try using the `arrays` method, rather than `concatenate`.",
  "created_at":"2021-01-19T16:43:55Z",
  "id":762969027,
  "issue":248,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Mjk2OTAyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-19T16:43:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> In a few minutes, [4.0.2rc1](https://github.com/scikit-hep/uproot4/releases/tag/4.0.2rc1) will be available [on PyPI](https://pypi.org/project/uproot/#history).\r\n\r\nIt's ready: https://pypi.org/project/uproot/4.0.2rc1/",
  "created_at":"2021-01-19T16:44:22Z",
  "id":762969321,
  "issue":248,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Mjk2OTMyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-19T16:44:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks! That did indeed fix the issue.",
  "created_at":"2021-01-19T17:25:48Z",
  "id":762996311,
  "issue":248,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2Mjk5NjMxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-19T17:25:48Z",
  "user":"MDQ6VXNlcjg1OTM0ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"It was a one-word fix! Uproot 3 automatically reads all streamers (instructions for how to deserialize the classes in a ROOT file) as soon as a file is opened, but Uproot 4 only reads streamers needed for the classes that are actually read. That's why it worked in Uproot 3.\r\n\r\nFor some reason, this object doesn't have a `num_bytes`. I don't know why that is, but it's not necessary to deserialize it. I think that \"if\" condition is supposed to be checking for the existence of a `version`, which _is_ necessary (unless we just guess that it's the maximum version in the streamers: there's rarely more than one of them).\r\n\r\n```diff\r\n--- a/uproot/model.py\r\n+++ b/uproot/model.py\r\n@@ -1160,11 +1161,12 @@ class DispatchByVersion(object):\r\n         if versioned_cls is not None:\r\n             pass\r\n \r\n-        elif num_bytes is not None:\r\n+        elif version is not None:\r\n             versioned_cls = cls.new_class(file, version)\r\n \r\n         elif context.get(\"in_TBranch\", False):\r\n             versioned_cls = cls.new_class(file, \"max\")\r\n \r\n         else:\r\n             raise ValueError(\r\n```\r\n",
  "created_at":"2021-01-21T14:24:06Z",
  "id":764676588,
  "issue":250,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NDY3NjU4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-21T14:24:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for this thread, which gave me the pointers needed to interpret a more complicated structure.\r\n\r\nIn case it's useful, here is the function I wrote to transform the branch title into a dType for interpretation:\r\n\r\n```\r\ndef get_Python_dType_from_root_branch_title(title):\r\n    '''\r\n    Given a root branch's title, find the dType needed to interpret it\r\n    Input:\r\n        title: root branch title\r\n    return: \r\n        dtype_list: to be used with interpretation = uproot.AsDtype(np.dtype(dtype_list))\r\n    Throws an error if the type in Root isn't one of those already included in the translation\r\n    By Michael Punch... MP 20210117\r\n    '''\r\n    import re\r\n\r\n    # Dictionary to translate Root types to Python dTypes\r\n    # Note: if no type included with the name, then assume it's a float\r\n    trans_dict ={\"\": \">f4\", \"I\": \">i4\", \"i\": \">u4\", \"F\": \">f4\", \"S\": \">i2\"}\r\n\r\n    dtype_list = []\r\n\r\n    leaves = title.split(':')\r\n    for leaf in leaves:\r\n        # Split into variable name + possible array lengths, and field type\r\n        # If there is no field type, leave this empty\r\n        extname,field = leaf.split(\"/\") if '/' in leaf else (leaf,'')\r\n        \r\n        # Take the part of variable name before any possible array lengths as the name\r\n        name = extname.split('[')[0]\r\n\r\n        arr_lens = [int(_) for _ in re.findall('\\[(\\d+?)\\]',extname)]\r\n\r\n        try:\r\n            pfield = trans_dict[field] # will give keyerror if unknown key found\r\n        except KeyError:\r\n            print(\"Error: Field type '\"+field+\"' in Root not yet translated to Python type. Add correct type to 'trans_dict'\")\r\n\r\n        line = (name,pfield,(arr_lens)) if len(arr_lens) else (name,pfield)\r\n\r\n        dtype_list.append(line)\r\n        \r\n    return dtype_list\r\n```",
  "created_at":"2021-01-17T18:07:32Z",
  "id":761854297,
  "issue":252,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2MTg1NDI5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-18T16:21:42Z",
  "user":"MDQ6VXNlcjE3ODUzNDI2"
 },
 {
  "author_association":"NONE",
  "body":"Ping @jpivarski (sorry if that's not the correct procedure), if you think this is useful / needs work ?",
  "created_at":"2021-01-22T10:22:46Z",
  "id":765304740,
  "issue":252,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTMwNDc0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T10:22:46Z",
  "user":"MDQ6VXNlcjE3ODUzNDI2"
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't realize that I needed to take an action on this: I had to go back to the StackOverflow post to see that I had asked for the algorithm that you just gave. Sorry about that!\r\n\r\nI'll move this issue to Uproot 4 with a to-do item to see if your algorithm can be integrated into [uproot/interpretation/identify.py](https://github.com/scikit-hep/uproot4/blob/main/uproot/interpretation/identify.py).\r\n\r\nThanks for offering the algorithm, and sorry that I didn't recognize it as exactly the thing I'd asked for! (I thought you were just showing something you were using in your analysis, and I didn't think I needed to comment on it.)",
  "created_at":"2021-01-22T15:07:50Z",
  "id":765469142,
  "issue":252,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTQ2OTE0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T15:07:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I guess I wasn't clear.  Thanks for taking it into account.\r\n\r\nTwo worries I would have on the routine as-is is that:\r\n\r\n- I have only taken a handful of the root data types into account, so it might fail with other datatypes.\r\n  - so, I have 4 of the data types (I guess the most common) from the list of 12 here: https://root.cern/manual/root_classes_data_types_and_global_variables/#machine-independent-data-types \r\n- I am assuming that root is giving everything as big-endian (which works for me on x86_64 machine to produce the root files, and for this uproot reading)\r\n  - But, I suppose this is okay, \"It also is stored in machine independent format (ASCII, IEEE floating point, Big Endian byte ordering)\" from the root docs https://root.cern.ch/root/htmldoc/guides/users-guide/InputOutput.html\r\n\r\nIf anyone has to hand the list of the data type descriptors that root uses in the branch titles with their equivalent as a Python (numpy) dtype, that would be sufficient... and especially if this is already somewhere in the uproot codebase, it would be better to leverage that.\r\n\r\nBut I would be happy to do some reverse engineering on that, if needed.\r\n\r\n",
  "created_at":"2021-01-22T17:42:35Z",
  "id":765577854,
  "issue":252,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTU3Nzg1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T17:42:35Z",
  "user":"MDQ6VXNlcjE3ODUzNDI2"
 },
 {
  "author_association":"MEMBER",
  "body":"Information about how to interpret a branch is scattered around, and some of it is only available in titles that have to be parsed as strings, such as the [parameters of Double32/Float16](https://github.com/scikit-hep/uproot4/blob/20d8575e941c32559c7b5e62b0ed5f92bc4927d0/uproot/interpretation/identify.py#L206-L214).\r\n\r\nEverything in ROOT is big-endian\u2014that, at least, is easy to state as an absolute rule for now. The new RTNtuple will be little-endian, but everything in the RNTuple class (apart from the [RNTuple \"anchor\"](https://github.com/scikit-hep/uproot4/blob/main/uproot/models/RNTuple.py), which is an entry point) is distinct from the rest of ROOT I/O.\r\n\r\nManually providing an Interpretation is certainly allowed. That was my initial, quick [answer on StackOverflow](https://stackoverflow.com/a/62520558/1623645). Sometimes, that's what I recommend when I can't figure out why someone's classes have internal headers or are missing internal headers, but it's plain from the serialized file whether those headers exist or not, and I give them an Interpretation they can plug in. [Automatically determining the Interpretation](https://github.com/scikit-hep/uproot4/blob/20d8575e941c32559c7b5e62b0ed5f92bc4927d0/uproot/interpretation/identify.py#L275-L291) is, of course, better, and I'll see if your code can be merged in to improve that.",
  "created_at":"2021-01-22T17:56:19Z",
  "id":765585429,
  "issue":252,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTU4NTQyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T17:56:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Okay, thanks!",
  "created_at":"2021-01-22T18:00:28Z",
  "id":765587909,
  "issue":252,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2NTU4NzkwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-22T18:00:28Z",
  "user":"MDQ6VXNlcjE3ODUzNDI2"
 },
 {
  "author_association":"MEMBER",
  "body":"I can't believe I lost track of this again! So I did it right away; PR #280 adds the leaf dimension interpretation.",
  "created_at":"2021-02-18T15:51:59Z",
  "id":781441305,
  "issue":252,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTQ0MTMwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T15:51:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The PR #254 just changes the check slightly to make sure the length of any sublist in `all_request_ranges` never exceeds the limit.",
  "created_at":"2021-01-27T14:07:03Z",
  "id":768308705,
  "issue":253,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMwODcwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:07:03Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The CI failure is caused by a HTTP 500 error from a remote server in one of the tests https://github.com/scikit-hep/uproot4/pull/254/checks?check_run_id=1777200916#step:9:441",
  "created_at":"2021-01-27T14:02:04Z",
  "id":768305504,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMwNTUwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:02:04Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "author_association":"MEMBER",
  "body":"That HTTP issue was unrelated to the XRootD thing you fixed, so I restarted the jobs.",
  "created_at":"2021-01-27T14:06:58Z",
  "id":768308643,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMwODY0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:06:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Now it's failing because pip is using syntax that's not legal in Python 3.5: \"`sys.stderr.write(f\"ERROR: {exc}\")`\". I wonder why it's pulling in a version of pip that's incompatible with the Python that's running.",
  "created_at":"2021-01-27T14:16:18Z",
  "id":768314465,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMxNDQ2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:16:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks! Yes, I saw that, odd. Something wrong with `conda-incubator/setup-miniconda` ?",
  "created_at":"2021-01-27T14:20:59Z",
  "id":768317352,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMxNzM1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:20:59Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "author_association":"MEMBER",
  "body":"Python 3.5 hasn't been supported by conda in a long time. Normally I'd say this sounds like SLC6; `pip install -U pip` from an old pip that doesn't know about the Requires-Python metadata slot will upgrade to pip 21.0 and break.",
  "created_at":"2021-01-27T14:24:04Z",
  "id":768319288,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMxOTI4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:24:04Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Ahh, I think the problem is pip on conda is a no-arch package, meaning it's not tied to Python version. It's listed as >=3, not >=3.6. If I can manage to get anyone to care about 3.5, I might be able to get it fixed. But not sure I can get anyone to care.",
  "created_at":"2021-01-27T14:27:53Z",
  "id":768321932,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMyMTkzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:27:53Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't know if it's conda, but maybe. I know that a lot of things have been dropping Python 3.5 support.\r\n\r\nUproot technically works in Python 3.5, just as it works in Python 2.7 (and even Python 2.6 in some tests I performed personally), but we can't claim support if it's not regularly tested in CI, and it's hard to run CI on old platforms.\r\n\r\nI have less of an attachment to Python 3.5 than 2.7 because use-cases are more likely to be stuck in Python 2.7 than 3.5\u2014any system on a Python 3 branch can update rather easily, but Python 2 has locked in some code with large, messy codebases. So, having given up on trying to keep Python 2.7 in CI, I might as well give up on 3.5.",
  "created_at":"2021-01-27T14:30:31Z",
  "id":768323721,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMyMzcyMQ==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-27T14:30:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi, @henryiii! Our messages crossed. I agree that it's hard to care about Python 3.5: if you're already on Python 3, you might as well upgrade already. (Also, its non-deterministic dict order has been a pain for me. I don't mind seeing it go.)\r\n\r\n@ryuwd, if you don't have any other changes, give me a thumbs-up and I'll squash-and-merge. Thanks!",
  "created_at":"2021-01-27T14:33:36Z",
  "id":768325786,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMyNTc4Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-01-27T14:33:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @ryuwd for code",
  "created_at":"2021-01-27T14:35:56Z",
  "id":768327490,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMyNzQ5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:35:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/255) to add @ryuwd! :tada:",
  "created_at":"2021-01-27T14:36:04Z",
  "id":768327588,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODMyNzU4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T14:36:04Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks @jpivarski !",
  "created_at":"2021-01-27T15:06:06Z",
  "id":768348045,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM0ODA0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T15:06:06Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "author_association":"MEMBER",
  "body":"I've opened a PR to fix the issue you saw with `pip`: https://github.com/conda-forge/pip-feedstock/pull/68",
  "created_at":"2021-01-27T15:21:40Z",
  "id":768359008,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM1OTAwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T15:21:40Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"@chrisburr Thanks! I didn't think there would be this interest in Python 3.5. (Although now that I think about it, the failure should at least be prevented because it can be hard to understand the error message: \"f-strings are not correct syntax.\")",
  "created_at":"2021-01-27T15:33:55Z",
  "id":768368023,
  "issue":254,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2ODM2ODAyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-27T15:33:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If it does work, I'll merge it!",
  "created_at":"2021-01-29T14:19:36Z",
  "id":769832244,
  "issue":256,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTgzMjI0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-29T14:19:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If we say we support 3.5, I want to have at least one CI job running 3.5 (same goes for 2.7, actually). (Though Linux only would be just fine). If we claim to support 3.5 but actually don't, that's _worse_ than just changing our minimum requires to remove 3.5, causing pip to pull old versions instead. This is exactly the same issue that pip's conda feedstock had, they didn't change what they claimed to support, so it broke!",
  "created_at":"2021-01-29T14:20:42Z",
  "id":769832899,
  "issue":256,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTgzMjg5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-29T14:22:14Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"```\r\n  OSError: XRootD error: [ERROR] Operation expired\r\nE               in file root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\r\n```",
  "created_at":"2021-01-29T14:21:14Z",
  "id":769833178,
  "issue":256,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc2OTgzMzE3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-29T14:21:14Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll mark this as ready for review as the issue it's claiming to fix is sorted.\r\n\r\nI've left a script running to see if I can get enough of a reproducer to open a ticket against EOS...",
  "created_at":"2021-01-30T09:32:35Z",
  "id":770184173,
  "issue":256,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MDE4NDE3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-30T09:32:35Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"Ah I've finally figured it out, unauthenticated reads are _**often**_ failing but authenticated ones are _**always**_ fine.\r\n\r\nI've opened a ticket: https://cern.service-now.com/service-portal?id=ticket&n=INC2684948\r\n\r\n* With kerberos credentials:\r\n```bash\r\n$ ssh lxplus 'echo \"Start\" && time XRD_REQUESTTIMEOUT=20 xrdcp root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root - | head -c1000 > /dev/null'\r\nStart\r\nXRD_REQUESTTIMEOUT=20 xrdcp  -  0.01s user 0.03s system 39% cpu 0.106 total\r\nhead -c1000 > /dev/null  0.00s user 0.00s system 1% cpu 0.103 total\r\n```\r\n\r\n* Destroy kerberos credentials before reading:\r\n```bash\r\n$ ssh lxplus 'kdestroy && echo \"Start\" && time XRD_REQUESTTIMEOUT=20 xrdcp root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root - | head -c1000 > /dev/null'\r\nStart\r\n[0B/0B][100%][==================================================][0B/s]\r\nRun: [ERROR] Operation expired:  (source)\r\nXRD_REQUESTTIMEOUT=20 xrdcp  -  0.03s user 0.01s system 0% cpu 15.016 total\r\nhead -c1000 > /dev/null  0.00s user 0.00s system 0% cpu 15.016 total\r\n```",
  "created_at":"2021-01-30T11:30:09Z",
  "id":770197979,
  "issue":256,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MDE5Nzk3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-30T11:30:09Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"@jpivarski The EOS issue is resolved now so this is ready to merge.\r\n\r\nIf anyone is interested there is a nice explanation about the root cause on the support ticket.",
  "created_at":"2021-01-30T20:44:48Z",
  "id":770277962,
  "issue":256,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MDI3Nzk2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-01-30T20:44:48Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"Writing ROOT files (at all) is a goal for this spring. Unsigned integers will definitely be included as will jagged arrays\u2014meaning a variable number of integers per entry. These are also available in Uproot 3: [documentation on writing basic TTrees](https://github.com/scikit-hep/uproot3#writing-ttrees) and writing jagged arrays, unfortunately only documented by a PR #477.\r\n\r\nHowever, if you really need the data type to be `std::vector<unsigned int>`, rather than `unsigned int[]` (which is variable), that isn't planned. They differ in that the latter has an extra 10 bytes per entry that would have to be understood and written correctly (4 of those bytes are the length of the vector, but the other 6 are a kind of header). If your goal is to have accessible jagged array data in a ROOT file, then `unsigned int[]` would be sufficient. If your project actually needs `std::vector<unsigned int>`, then we'll have to look at that.",
  "created_at":"2021-02-01T20:40:38Z",
  "id":771143457,
  "issue":257,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MTE0MzQ1Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-01T20:40:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In addition to Uproot 3, another option that exists right now is RDataFrame in PyROOT:\r\n\r\n   1. install ROOT with the Python bindings\r\n   2. open Python and `import ROOT`\r\n   3. create an [RNumpyDS](https://root.cern/doc/master/classROOT_1_1Internal_1_1RDF_1_1RNumpyDS.html) to point to NumPy arrays derived from `h5py` (sorry that I can't give more detailed instructions on that\u2014I haven't done it)\r\n   4. create an [RDataFrame](https://root.cern/doc/master/classROOT_1_1RDataFrame.html) (lots of tutorials on this)\r\n   5. its [snapshot](https://root.cern/doc/master/classROOT_1_1RDF_1_1RInterface.html#a233b7723e498967f4340705d2c4db7f8) method writes to ROOT files.",
  "created_at":"2021-02-01T20:47:34Z",
  "id":771147116,
  "issue":257,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MTE0NzExNg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-01T20:47:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I am also working on something like this.  I am in need of the `vector<unsigned int>` type.  Is it possible to get this on the list of to-do items?",
  "created_at":"2021-02-01T22:56:24Z",
  "id":771219093,
  "issue":257,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MTIxOTA5Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-01T22:59:26Z",
  "user":"MDQ6VXNlcjc4Mzg4NTc4"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I'll look into the `std::vector` headers when I work on file-writing.",
  "created_at":"2021-02-01T23:24:13Z",
  "id":771231097,
  "issue":257,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3MTIzMTA5Nw==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-01T23:24:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski Do you know if `std::vector`  was added?",
  "created_at":"2021-06-09T17:50:48Z",
  "id":857910208,
  "issue":257,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzkxMDIwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-09T17:50:48Z",
  "user":"MDQ6VXNlcjY4MzQyMzI3"
 },
 {
  "author_association":"MEMBER",
  "body":"Uproot4-writing (in general) development is stalled while I get through all the summer conferences/workshops/tutorials. It is still my top-priority programming project. Updates are here:\r\n\r\nhttps://github.com/scikit-hep/uproot4/discussions/321",
  "created_at":"2021-06-09T18:16:19Z",
  "id":857938521,
  "issue":257,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzkzODUyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-09T18:16:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Great, thanks!\r\n\r\n(Grumble, grumble... pyxrootd can't handle `np.int64`? C extensions should call `__int__` on any arguments intended to be integers\u2014it's the sort of thing pybind11 does automatically.)",
  "created_at":"2021-02-08T16:10:44Z",
  "id":775256938,
  "issue":258,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTI1NjkzOA==",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2021-02-08T16:10:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"No, `__index__`, actually. `__int__` means \"convert to integer even if you lose something\", while `__index__` means \"this is exactly this integer\". But the argument still applies. ;)",
  "created_at":"2021-02-08T20:00:32Z",
  "id":775407009,
  "issue":258,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTQwNzAwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T20:00:32Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"(And it was slightly incorrectly used in the CPython API until 3.8+, a few things did call `__int__` instead of `__index__`)",
  "created_at":"2021-02-08T20:01:30Z",
  "id":775407630,
  "issue":258,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTQwNzYzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T20:01:30Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, though this is based on my interpretation of [PEP 357](https://www.python.org/dev/peps/pep-0357/). (That's one that I've _read_.) The `__index__` prevents floating point numbers from being cast as integers, which you wouldn't want as an index in a slice, but I think you might want that as an argument to a function. But maybe enough bugs have led to the conclusion that lossless integer conversions should always be required.\r\n\r\nSo, anyway, a C extension wanting integer or floating point arguments should call `__index__` and `__float__` (asymmetrically, but if you want a float, you're already allowing for imprecision).",
  "created_at":"2021-02-08T20:27:38Z",
  "id":775429219,
  "issue":258,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTQyOTIxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T20:27:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is based on the C-API; https://docs.python.org/3/c-api/long.html#c.PyLong_AsLong. In CPython 3.8, `__index__` is called first if you ask for `PyLong_AsLong(obj)` and falling back on `__int__` is deprecated.\r\n\r\nAnyway, `__float__` converts lossily to a float, `__str__` converts lossily to a string, and `__int__` converts lossily to an int. Generally, you would try `__index__`, and dispatch an int version, and then fall back on `__float__` if you have a float API. If you want (possibly) lossy conversion to ints, of course, then `__int__` is completely correct. In this case, it looks like this _is_ an index (into an array), so `__index__` is correct (and is exactly what pybind11 2.6.2 would call).",
  "created_at":"2021-02-08T20:47:44Z",
  "id":775449246,
  "issue":258,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTQ0OTI0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T20:48:30Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi David,\r\n\r\nthe syntax has changed in `uproot` v4 to access multiple arrays. Try this:\r\n\r\n```python\r\nimport uproot\r\ntest = uproot.open(\"onepinhole.root\")\r\nTestDf = test['Hits'].arrays()\r\n```\r\n\r\nThis will give you an instance of `awkward.Array` including all branches (no need for the `\"*\"` filter in this case):\r\n\r\n```\r\n>>> TestDf\r\n<Array [] type='0 * {\"PDGEncoding\": int32, \"trackID\": int32, \"parentID\": int32, ...'>\r\n\r\n>>> type(TestDf)\r\nawkward.highlevel.Array\r\n\r\n>>> TestDf[\"PDGEncoding\"]\r\n<Array [] type='0 * int32'>\r\n\r\n>>> TestDf.fields\r\n['PDGEncoding',\r\n 'trackID',\r\n 'parentID',\r\n 'trackLocalTime',\r\n 'time',\r\n 'edep',\r\n 'stepLength',\r\n 'trackLength',\r\n 'posX',\r\n 'posY',\r\n 'posZ',\r\n 'localPosX',\r\n 'localPosY',\r\n 'localPosZ',\r\n 'momDirX',\r\n 'momDirY',\r\n 'momDirZ',\r\n 'headID',\r\n 'crystalID',\r\n 'pixelID',\r\n 'unused3ID',\r\n 'unused4ID',\r\n 'unused5ID',\r\n 'photonID',\r\n 'nPhantomCompton',\r\n 'nCrystalCompton',\r\n 'nPhantomRayleigh',\r\n 'nCrystalRayleigh',\r\n 'primaryID',\r\n 'sourcePosX',\r\n 'sourcePosY',\r\n 'sourcePosZ',\r\n 'sourceID',\r\n 'eventID',\r\n 'runID',\r\n 'axialPos',\r\n 'rotationAngle',\r\n 'volumeID',\r\n 'processName',\r\n 'comptVolName',\r\n 'RayleighVolName']\r\n```\r\n\r\nTo use a filter pattern, you now have to provide it with the `filter_name=` keyword:\r\n\r\n```python\r\n>>> test['Hits'].arrays(filter_name=\"sourcePos*\")\r\n<Array [] type='0 * {\"sourcePosX\": float32, \"sourcePosY\": float32, \"sourcePosZ\":...'>\r\n\r\n>>> test['Hits'].arrays(filter_name=\"*\")  # your original example\r\n<Array [] type='0 * {\"PDGEncoding\": int32, \"trackID\": int32, \"parentID\": int32, ...'>\r\n```\r\n\r\nYou can find some more hints regarding the uproot 3 -> 4 transition here: https://uproot.readthedocs.io/en/latest/uproot3-to-4.html#reading-arrays \r\n\r\nEdit: sorry, accidentally hit the submit button too early ;)",
  "created_at":"2021-02-08T17:37:39Z",
  "id":775317915,
  "issue":259,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTMxNzkxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T17:44:20Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"NONE",
  "body":"I am really sorry, I should have read the documentation on the transition. It does now work as expected. Thanks a lot @tamasgal for your quick response!\r\n\r\n",
  "created_at":"2021-02-08T18:24:22Z",
  "id":775347942,
  "issue":259,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTM0Nzk0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T18:24:22Z",
  "user":"MDQ6VXNlcjYyMjYyMDgx"
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"No worries and don't hesitate to ask, we all are busy and overlook things here and there \ud83d\ude09 ",
  "created_at":"2021-02-08T18:26:08Z",
  "id":775349159,
  "issue":259,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTM0OTE1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T18:26:08Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"NONE",
  "body":"Thanks! \ud83d\ude0a",
  "created_at":"2021-02-08T18:34:25Z",
  "id":775354192,
  "issue":259,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTM1NDE5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-08T18:34:25Z",
  "user":"MDQ6VXNlcjYyMjYyMDgx"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks @tamasgal!",
  "created_at":"2021-02-08T18:35:22Z",
  "id":775354694,
  "issue":259,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTM1NDY5NA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-08T18:35:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is actually because this is a binary or being misused instead of a logical or. Due to short circuiting, logical or's cannot be overloaded, so numeric Python had to use the binary operators instead for these logical operations, but the precedence is wrong. There have been a couple of PEPs to fix this in Python, but they have been deferred and everyone seems too busy trying to add pattern matching.\n\nI think it's a good idea, just thought I'd throw in some context.",
  "created_at":"2021-02-09T12:53:42Z",
  "id":775914830,
  "issue":260,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTkxNDgzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T12:53:42Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sure, some context is always interesting. Feel free to modify my PR to provide more explanation, I just wanted to make clear for anyone else that would face the same issue how to proceed. I tried to include && and || operators in the example so that people would have a better clue of the syntax.",
  "created_at":"2021-02-09T13:57:17Z",
  "id":775955998,
  "issue":260,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NTk1NTk5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T13:57:17Z",
  "user":"MDQ6VXNlcjI4MTAxMjAx"
 },
 {
  "author_association":"MEMBER",
  "body":"I really dislike the use of `&` and `|` instead of `and` and `or`, too, and it's good to show in the docs that it is necessary (as well as the parentheses).\r\n\r\nThe syntax of the strings is governed by a `language=uproot.language.python.PythonLanguage` that may someday be expanded to include TTreeFormula (through the `formulate` package or a new parser in Lark). Then the documentation examples might use `&&` and `||` without parentheses, but also include `language=\"root\"` (simplifying that parameter setting in a similar way as `language=\"np\"`).\r\n\r\nThanks for this PR!",
  "created_at":"2021-02-09T16:09:58Z",
  "id":776054318,
  "issue":260,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjA1NDMxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T16:09:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors, please add @ChristopheRappold for documentation",
  "created_at":"2021-02-11T16:31:36Z",
  "id":777622316,
  "issue":260,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzYyMjMxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T16:31:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors, please add @ChristopheRappold for doc",
  "created_at":"2021-02-11T16:33:04Z",
  "id":777623385,
  "issue":260,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzYyMzM4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T16:33:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @ChristopheRappold for doc",
  "created_at":"2021-02-11T16:33:47Z",
  "id":777623877,
  "issue":260,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzYyMzg3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T16:33:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/264) to add @ChristopheRappold! :tada:",
  "created_at":"2021-02-11T16:33:56Z",
  "id":777623970,
  "issue":260,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzYyMzk3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T16:33:56Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"(That's why I dislike tools configured by natural language...)\r\n\r\nThanks, @ChristopheRappold for your new contribution!",
  "created_at":"2021-02-11T16:35:01Z",
  "id":777624730,
  "issue":260,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NzYyNDczMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-11T16:35:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The goal of `allow_missing` was to allow for files that don't have a TTree with the given name. This one has the TTree, but not the requested TBranches. That's a different case, but it does sound like \"allow missing\" should apply. I'll look at where it fails.",
  "created_at":"2021-02-09T20:25:44Z",
  "id":776220539,
  "issue":261,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3NjIyMDUzOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-09T20:25:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"After much back-and-forth, the histogram protocol settled on not including under/overflow bins, and _going beyond_ the protocol, we are allowed to include a `flow=True/False` option to turn it on and off. See, for instance, [TH1.values](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TH1.TH1.html#values) and [TH1.edges](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TAxis.TAxis.html#edges).\r\n\r\nBut I think what you're running up against is not under/overflow bin handling, but the [fencepost principle](https://en.wikipedia.org/wiki/Off-by-one_error#Fencepost_error). If you do `__getitem__`, you get a section of the fence defined by the two posts that border it. If you do `edges()`, you see all the posts. It's like the difference between `offsets` and `starts/stops`.\r\n\r\nIf anything, this should be categorized as \"policy,\" rather than a \"bug,\" but I really don't want to revisit the histogram protocol definitions again. I'd like to let those dogs lie.",
  "created_at":"2021-02-15T16:39:49Z",
  "id":779336887,
  "issue":265,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTMzNjg4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-15T16:39:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Uh.. I am fine with fenceposts or whatever but why don't I get the first (not overflow) bin `(-2.5, -2)` when I ask for bin index 0? Surely such behaviors are supposed to be specified by the protocol. @henryiii ?",
  "created_at":"2021-02-15T17:10:56Z",
  "id":779354198,
  "issue":265,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTM1NDE5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-15T17:14:55Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, that is strange, now that I look at the actual numbers. This `edges` doesn't look consistent with the `__getitem__`. If you had asked for `flow=True`, it would add infinity fenceposts to the ends:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/ea0456cd4d101eed8c8e20207560e6057bf063c3/uproot/behaviors/TAxis.py#L215-L223\r\n\r\nSo that means that the `__getitem__` should return `fXbins[where], fXbins[where + 1]` instead of\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/ea0456cd4d101eed8c8e20207560e6057bf063c3/uproot/behaviors/TAxis.py#L61-L75\r\n\r\nwhich, I guess, only affects irregularly spacaed bins. This is looking more like a bug to me, too (and not a histogram protocol thing).\r\n",
  "created_at":"2021-02-15T17:47:10Z",
  "id":779371473,
  "issue":265,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTM3MTQ3Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-15T17:47:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've checked and we just don't have any test files with histograms containing irregularly spaced bins. The fact that we have to find files to do proper tests is part of a larger problem: scikit-hep/uproot4#269 and scikit-hep/uproot4#240.\r\n\r\nIt looks to me like replacing\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/bfc1380ca304c5b0108417954946b1d793cfeed6/uproot/behaviors/TAxis.py#L74-L75\r\n\r\nwith\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/714f86309059d708c180956cd14f1acf6849052d/uproot/behaviors/TAxis.py#L74-L75\r\n\r\nis all we have to do. The `TAxis.__getitem__` has to be coordinated with `TAxis.intervals` (same file), but that looks correct.",
  "created_at":"2021-02-15T19:47:58Z",
  "id":779419321,
  "issue":265,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTQxOTMyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-15T19:47:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"These are variable-length arrays of Double32, but that ought to be supported already. Please include an example ROOT file and I'll look into what's preventing it from being identified.\r\n\r\nYou can attach a small ROOT file to a GitHub issue by renaming the extension to `.txt` first. (GitHub doesn't like `.root` as a filename extension, but doesn't complain about text files containing binary data.)",
  "created_at":"2021-02-15T18:55:20Z",
  "id":779399109,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTM5OTEwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-15T18:55:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"okay I have attached a similar file, where an analysis tree has under the VtxTracks branch channels_ \r\n![Screenshot from 2021-02-16 10-10-15](https://user-images.githubusercontent.com/47216877/108041772-4c487680-703f-11eb-98ca-1047ff2d395f.png)\r\n\r\nbut uproot doesn't recognize it at all\r\n![Screenshot from 2021-02-16 10-10-54](https://user-images.githubusercontent.com/47216877/108041918-7732ca80-703f-11eb-8691-110c98a05bcd.png)\r\n\r\nThe file is more than 50mb so I have stored it on the following google drive, kindly download it there\r\nhttps://drive.google.com/drive/folders/1dXSFcRlWXvzeIUbNMSZQEUaeWSBy75_J?usp=sharing",
  "created_at":"2021-02-16T09:14:55Z",
  "id":779696553,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTY5NjU1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-16T09:14:55Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, first off, you're using Uproot 3 and Uproot 4 is more complete in terms of type coverage. (I should have recognized Uproot 3's `show` format from your screenshots.) `pip install -U uproot` should do it, or maybe you need to require `\"uproot>=4.0.0\"` to convince pip to give you the latest version. There are [significant differences between 3.x and 4.x](https://uproot.readthedocs.io/en/latest/uproot3-to-4.html).\r\n\r\nMost of the examples with `None` in your screenshots are actually groups of branches that do not have any data in themselves\u2014ROOT only puts them there for structure. Nevertheless, Uproot 4 interprets these non-data branches by reading all of their subbranches and presenting them as some kind of group. (I.e. if `library=\"ak\"` (default), then the group is an Awkward record array, if `library=\"np\"`, then the group is a dict of NumPy arrays, if `library=\"pd\"`, then the group is a Pandas DataFrame instead of a Series.)\r\n\r\nI did manage to find a bug: some of your `vector<float>` and `vector<bool>` were being interpreted as `vector<int>` because they all have the same name, `field_`, in their C++ structure and the interpretation took the first name that matched. I've tightened that rule by requiring the field name and the parent name to both match, and that tighter rule is satisfied by all other ROOT files in my tests. (Fixed in #272.)\r\n\r\nFinally, scanning all the branches with\r\n\r\n```python\r\n>>> for x in tree.keys():\r\n...   try:\r\n...     a = tree[x].array()\r\n...   except Exception as err:\r\n...     print(x, type(err), str(err))\r\n...   else:\r\n...     print(x, a)\r\n```\r\n\r\nthe only failures left are branches with ROOT's \"memberwise splitting.\" This is a feature I only found out about after the transition to Uproot 4, and it's a big to-do item: #38. For years, I had occasionally run into ROOT files with this strange serialization and it took a long time to even realize that it's a ROOT feature\u2014I thought people were using custom streamers. One was (scikit-hep/uproot3#373); they were using Boost.Serialization, which biased me to think that they all were.\r\n\r\nNow, though, I've found the bit that indicates that an object is serialized in a memberwise format so that I can figure out how to deserialize it, and in the meantime raise NotImplementedError. Your file has _a lot_ of memberwise data, mostly `std::vectors`, which would be a good start because they're fairly simple. However (see #38), it's only the 7th file I've ever encountered with this feature. It's rare!",
  "created_at":"2021-02-16T23:00:29Z",
  "id":780172898,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDE3Mjg5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-16T23:00:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski thank you very much for your in-depth overview of my file.\r\n\r\n1. upgraded to uproot 4 using  pip install -U uproot , although it was already there but somehow wasn't being used\r\n2. These sub-branches have leaves inside them and I want to upload them using uproot, could you tell me how can I do that? For example the leaf px_\r\n![Screenshot from 2021-02-17 10-28-17](https://user-images.githubusercontent.com/47216877/108184190-356f5600-710b-11eb-9d06-b8197ab16933.png)\r\n\r\n3. tried with this code\r\n![Screenshot from 2021-02-17 10-40-55](https://user-images.githubusercontent.com/47216877/108185561-b9760d80-710c-11eb-8b97-bc9e5a09cb0f.png)\r\n\r\ngives the following error\r\n\r\n![Screenshot from 2021-02-17 10-41-10](https://user-images.githubusercontent.com/47216877/108185542-b2e79600-710c-11eb-8dab-6e1b408da271.png)\r\n\r\n4. This also doesn't help\r\n![Screenshot from 2021-02-17 10-43-10](https://user-images.githubusercontent.com/47216877/108185813-fc37e580-710c-11eb-9bff-21f11fcede7e.png)\r\n",
  "created_at":"2021-02-17T09:43:39Z",
  "id":780432725,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDQzMjcyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T09:43:39Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "author_association":"NONE",
  "body":"@viktorklochkov What do you think?",
  "created_at":"2021-02-17T09:59:39Z",
  "id":780442162,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDQ0MjE2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T09:59:39Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "author_association":"NONE",
  "body":"Maybe you could also try to update CbmRoot and test with some new files. I've simplified the format in the last version, maybe that will help.",
  "created_at":"2021-02-17T12:06:35Z",
  "id":780512422,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDUxMjQyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T12:06:35Z",
  "user":"MDQ6VXNlcjcyNjYzNjM="
 },
 {
  "author_association":"MEMBER",
  "body":"\"awkward has no attribute 'layout'\" is suggestive that you have Awkward version 0.x, when you want Awkward 1.x. try `pip install -U awkward` to update Awkward as you already have updated Uproot.",
  "created_at":"2021-02-17T13:03:11Z",
  "id":780541716,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDU0MTcxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T13:03:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski also what happened to parallel processing in uproot 4 arrays?\r\n![Screenshot from 2021-02-17 14-04-30](https://user-images.githubusercontent.com/47216877/108208308-2b5c5000-7129-11eb-9c71-ef73f915f19d.png)\r\n",
  "created_at":"2021-02-17T13:05:26Z",
  "id":780543075,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDU0MzA3NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T13:05:26Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski installing awkward only changed the error message\r\n![Screenshot from 2021-02-17 14-14-29](https://user-images.githubusercontent.com/47216877/108209434-978b8380-712a-11eb-9dec-fa3990e78c58.png)\r\n![Screenshot from 2021-02-17 14-14-42](https://user-images.githubusercontent.com/47216877/108209446-9a867400-712a-11eb-8ce9-75d179a57a8d.png)\r\n ",
  "created_at":"2021-02-17T13:15:38Z",
  "id":780548451,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDU0ODQ1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T13:15:38Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "author_association":"MEMBER",
  "body":"For parallel processing, the arguments are named `decompression_executor` and `interpretation_executor`:\r\n\r\nhttps://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays\r\n\r\nYou can also pass in `decompression_executor` and `interpretation_executor` when you open a file:\r\n\r\nhttps://uproot.readthedocs.io/en/latest/uproot.reading.open.html\r\n\r\nso that it applies to all arrays from that file.\r\n\r\nThat website has all of the argument lists of all functions. The `decompression_executor` is used to parallelize decompression, which is often useful because the `zlib`, `lz4`, `lzma`, and `zstd` libraries release the [Python GIL](https://wiki.python.org/moin/GlobalInterpreterLock) when they run. The `interpretation_executor` is used to parallelize interpretation, which is converting an uncompressed buffer of bytes into arrays. In most cases, this uses Python code and doesn't parallelize well.\r\n\r\nAs for the \"NotImplementedError: memberwise serialization\" error message, that's what I was talking about above: issue #38 is for me to solve and implement memberwise deserialization. Uproot 3 couldn't read that data layout, either\u2014in fact, it was a mystery to me, as it appeared in only a small number of files over the years. It wasn't until July 2020 (the date of that issue) that I found out it's a ROOT feature, an alternate way of writing files that only a few people have ever switched on.",
  "created_at":"2021-02-17T15:56:57Z",
  "id":780656906,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDY1NjkwNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T15:56:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"@jpivarski then I have been playing with the right parameters but the problem is that before I was able to do parallel processing and it was also memory efficient. \r\n\r\n`from concurrent.futures import ThreadPoolExecutor\r\nexecutor = ThreadPoolExecutor(8)\r\n\r\nbranches = pd.DataFrame.from_dict(uproot.open(''+file_with_path+'')[''+tree_name+''].arrays(namedecode='utf-8', executor = executor))`\r\n\r\n\r\n\r\nBut now it consumes all my memory, may be I am not doing it properly. Could you please have a look at it?\r\n\r\n\r\n`from concurrent.futures import ThreadPoolExecutor\r\nexecutor = ThreadPoolExecutor(8)\r\n\r\ninput_tree = uproot.open('/home/shahid/cbmsoft/Data/10k_events_PFSimplePlainTree.root:PlainTree', decompression_executor=executor)\r\n\r\nbranches = input_tree.arrays(library='pd', decompression_executor=executor)`",
  "created_at":"2021-02-17T16:32:28Z",
  "id":780681825,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDY4MTgyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T16:33:00Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "author_association":"MEMBER",
  "body":"In this thread, we've been talking about several different files. The one you provided couldn't have been read by Uproot 3, so you're not talking about that one. (The one you posted above has trees named \"Configuration,\" \"aTree,\" and \"DataHeader,\" not \"PlainTree.\")\r\n\r\nBut anyway, I assume that what you mean by \"memory efficient\" is that it worked in Uproot 3 but is running out of memory in Uproot 4. From all that I know right now, the difference might be some 10% but it's 10% more than you have available. The whole idea of parallel processing is to trade memory for speed, since you're asking for 8 times the working space to be used at once. Maybe Uproot 3 wasn't as well parallelized\u2014the biggest 3 \u2192 4 difference is that the low-level physical layer (getting bytes from files) was streamlined to use knowledge of which TBaskets you will be reading to request all the bytes while others are in flight\u2014Uproot 3 could have been _prevented_ from using 8 times the working memory, as requested, because it was waiting for data from the file. (Without replicating the process in some performance-tuning diagnostic, we can only speculate about what's actually happening, but using more memory while parallel processing can be good news, rather than bad.)\r\n\r\nThe [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) method that you're calling isn't specifying `filter_name` or `expressions`, so it's reading everything into memory\u2014do you need all branches in the DataFrame? That kind of question is often more fruitful than carefully tuning performance with a memory profiler. If you only use half the variables, that's an easy factor of two.\r\n\r\nThis is becoming a discussion that is unrelated to your original problem of not being able to read certain branches. Maybe ask on a GitHub Discussion or StackOverflow with the `[uproot]` tag? (I'm hoping other users will be able to help with these usage questions.)",
  "created_at":"2021-02-17T19:07:56Z",
  "id":780782563,
  "issue":268,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDc4MjU2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T19:07:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I have to merge this now to be able to move forward on other issues, but we should definitely reinstate these tests when the files become accessible again.",
  "created_at":"2021-02-15T19:38:09Z",
  "id":779415588,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTQxNTU4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-15T19:38:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hrmm, we should probably ping the hepdata folx on this. I'll do it now and cc you. I'm a little surprised.",
  "created_at":"2021-02-15T20:54:02Z",
  "id":779443242,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTQ0MzI0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-15T20:54:02Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the alert.  It looks like a general failure of the HEPData converter (e.g. https://converter.hepdata.net/ping should return OK).  We share a Kubernetes cluster with https://inspirehep.net and the INSPIRE people made some changes to the Ingress today, which also affects https://hepdata.net.  I'll let you know when it's fixed (probably tomorrow).",
  "created_at":"2021-02-15T21:26:51Z",
  "id":779455104,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTQ1NTEwNA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-02-15T21:26:51Z",
  "user":"MDQ6VXNlcjExNTQ0MjA0"
 },
 {
  "author_association":"NONE",
  "body":"The issue with the HEPData converter certificate has now been fixed.  The bug (HEPData/hepdata#312) only affects table names ending in a `/` followed by an integer, so it's not relevant here.",
  "created_at":"2021-02-17T10:29:16Z",
  "id":780459834,
  "issue":269,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDQ1OTgzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T10:29:16Z",
  "user":"MDQ6VXNlcjExNTQ0MjA0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"At least for my test example:\r\n```python\r\nh = uproot.open('https://raw.githubusercontent.com/CoffeaTeam/coffea/master/tests/samples/testSF2d.histo.root:scalefactors_Tight_Electron')\r\naxis = h.axes[0]\r\nlen(axis)\r\naxis.edges()\r\naxis.edges().shape\r\naxis[0]\r\n```\r\n~still gives back `(-2.0, -1.566)`~ Sorry did not have the branch checked out properly. Looks OK now `(-2.5, -2.0)`",
  "created_at":"2021-02-16T15:56:02Z",
  "id":779929911,
  "issue":270,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTkyOTkxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-16T16:07:28Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, then this is probably correct and necessary. I've been staring at it long enough that I don't see how it could be anything but this. Your test confirms it.\r\n\r\nBy the way, you see the right thing for `intervals`, too, right? `intervals` gives all the `__getitem__`s in one batch, and therefore it's a different code path, but it needs to be consistent. The default `flow=False` corresponds to `__getitem__`, which doesn't take options like `flow`.",
  "created_at":"2021-02-16T16:12:59Z",
  "id":779941925,
  "issue":270,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTk0MTkyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-16T16:12:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Seems so:\r\n```\r\nIn [16]: numpy.array([numpy.array(item) for item in axis])\r\nOut[16]:\r\narray([[-2.5  , -2.   ],\r\n       [-2.   , -1.566],\r\n       [-1.566, -1.444],\r\n       [-1.444, -0.8  ],\r\n       [-0.8  ,  0.   ],\r\n       [ 0.   ,  0.8  ],\r\n       [ 0.8  ,  1.444],\r\n       [ 1.444,  1.566],\r\n       [ 1.566,  2.   ],\r\n       [ 2.   ,  2.5  ]])\r\nIn [18]: numpy.all(_16 == axis.intervals())\r\nOut[18]: True\r\n```",
  "created_at":"2021-02-16T16:21:14Z",
  "id":779947638,
  "issue":270,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTk0NzYzOA==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-02-16T16:21:14Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Though this seems a bit weird:\r\n```\r\nIn [21]: axis[-1]\r\nOut[21]: (2.5, -2.5)\r\n```\r\nconsidering\r\n```\r\nIn [22]: axis[len(axis)]\r\n...\r\nIndexError: index 11 is out of bounds for axis 0 with size 11\r\n```",
  "created_at":"2021-02-16T16:21:48Z",
  "id":779948018,
  "issue":270,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTk0ODAxOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-16T16:22:31Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> Though this seems a bit weird:\r\n> \r\n> ```\r\n> In [21]: axis[-1]\r\n> Out[21]: (2.5, -2.5)\r\n> ```\r\n> \r\n> considering\r\n> \r\n> ```\r\n> In [22]: axis[len(axis)]\r\n> ...\r\n> IndexError: index 11 is out of bounds for axis 0 with size 11\r\n> ```\r\n\r\nThis looks okay. That `axis[-1]` is normal Python indexing, which would be the same as `axis[len(axis) - 1]`.",
  "created_at":"2021-02-16T16:24:59Z",
  "id":779950085,
  "issue":270,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTk1MDA4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-16T16:24:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Creating a new environment seems to work:\r\n\r\n```bash\r\n$ conda create --name test uproot\r\n$ conda activate test\r\n$ python -c 'import uproot; print(uproot.__version__)'\r\n4.0.3\r\n```\r\n\r\nConda is probably avoiding upgrading too many packages and uproot 3 is a valid solution to the install command. Have you tried `conda install -c conda-forge uproot=4`?",
  "created_at":"2021-02-16T15:16:07Z",
  "id":779903468,
  "issue":271,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc3OTkwMzQ2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-16T15:16:07Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"@naidoo88, respond here if @chrisburr's solution doesn't work for you. I'm closing this now, but I can reopen it if it's still an issue.",
  "created_at":"2021-02-18T14:45:49Z",
  "id":781393170,
  "issue":271,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTM5MzE3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T14:45:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Apologies for the slow response! I got sucked down a whole other environment hell.\r\nSpecifying uproot=4 indeed worked.  Must be as @chrisburr  said.\r\n\r\nThank you for your help! ",
  "created_at":"2021-02-18T15:00:09Z",
  "id":781403509,
  "issue":271,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTQwMzUwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T15:00:09Z",
  "user":"MDQ6VXNlcjQ1ODE2ODYw"
 },
 {
  "author_association":"MEMBER",
  "body":"One suggestive thing is that the split version has a `whatever/TObject` that couldn't be interpreted. I wonder if that's some kind of header.\r\n\r\n```python\r\n>>> tree = uproot.open(\"split_1.objectwise.root:T\")\r\n>>> tree.show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nwhatever             | TWhatever                | AsGroup(<TBranchElement 'wh...\r\nwhatever/TObject     | unknown                  | <UnknownInterpretation 'non...\r\nwhatever/a           | double                   | AsDtype('>f8')\r\nwhatever/b           | int32_t                  | AsDtype('>i4')\r\n```\r\n\r\nAnother hint is that the unsplit version can be interpreted `AsObjects` but not `AsStridedObjects`. The auto-determined interpretation is `AsStridedObjects`:\r\n\r\n```python\r\n>>> tree = uproot.open(\"nosplit.objectwise.root:T\")\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\nwhatever             | TWhatever                | AsStridedObjects(Model_TWha...\r\n```\r\n\r\nwhich we can do explicitly using [uproot.interpretation.identify.interpretation_of](https://uproot.readthedocs.io/en/latest/uproot.interpretation.identify.interpretation_of.html).\r\n\r\n```python\r\n>>> uproot.interpretation.identify.interpretation_of(tree[\"whatever\"], {})\r\nAsStridedObjects(Model_TWhatever_v5)\r\n```\r\n\r\n(I'm passing `{}` as the `context` because this object isn't deep; it probably doesn't need all the information about how we got to this point in deserialization. Oh, I could have just passed [TBranch.context](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.TBranch.html#context). That would have been better, but this is okay.)\r\n\r\nNow let's remove the simplification step that replaces `AsObjects` with `AsStridedObjects`.\r\n\r\n```python\r\n>>> uproot.interpretation.identify.interpretation_of(tree[\"whatever\"], {}, simplify=False)\r\nAsObjects(Model_TWhatever)\r\n>>> tree[\"whatever\"].array(uproot.interpretation.identify.interpretation_of(tree[\"whatever\"], {}, simplify=False))\r\n<Array [{a: 0, b: 0}, ... b: -5000}] type='5001 * TWhatever[\"a\": float64, \"b\": i...'>\r\n```\r\n\r\nAha! We can deserialize it! It's slow (there's a noticeable lag with 5001 elements), but it does bracket the error between `AsObjects` and `AsStridedObjects`.\r\n\r\nThe difference between [AsObjects](https://uproot.readthedocs.io/en/latest/uproot.interpretation.objects.AsObjects.html) and [AsStridedObjects](https://uproot.readthedocs.io/en/latest/uproot.interpretation.objects.AsStridedObjects.html) is that `AsObjects` walks through Python loops, element by element, byte by byte, and `AsStridedObjects` casts the buffer as a [NumPy structured array](https://numpy.org/doc/stable/user/basics.rec.html), then pulls each field out using field-access. Objects with different field lengths, like some bools, some 32-bit integers, and some 64-bit floats, can be interpreted by this striding, and we use that to read it much more quickly.  Objects with variable-length fields, such as strings or `std::vector`, can't, and we have to fall back to `AsObjects`. This might become moot when AwkwardForth is introduced (AwkwardForth can deal with variable-length data and _might_ be as fast as striding), but it isn't yet.\r\n\r\nIt could be that, because of the TObject header that we don't know how to interpret when split, it is incorrect to simplify this particular `AsObjects` to `AsStridedObjects`. In other words, the bug could be in the rules that decide whether to simplify it.\r\n\r\nOn the other hand, it could be that we _can_ read this by striding, but are currently doing it incorrectly. That would require more research.",
  "created_at":"2021-02-17T18:12:54Z",
  "id":780748083,
  "issue":275,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDc0ODA4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T18:12:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"In https://github.com/CoffeaTeam/coffea/issues/377, @nsmith- pointed to https://github.com/scikit-hep/uproot4/pull/243 as possibly relevant to this issue. I'm not sure if it actually eliminates the issue, though.",
  "created_at":"2021-02-17T19:43:10Z",
  "id":780804659,
  "issue":276,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MDgwNDY1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-17T19:43:10Z",
  "user":"MDQ6VXNlcjQ2NzI4MDg="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> If so, what's the XRootD error that causes it?\r\n\r\n(a guess) It could be any error that may occur when requesting chunks in this block of code:\r\nhttps://github.com/xrootd/xrootd/blob/ced6cdeccc889d3943e4d5e93d5ce24c2b88512f/src/XrdXrootd/XrdXrootdXeq.cc#L2426-L2581\r\n\r\nIn #253 the cause was uproot4 asking for too many chunks per request \r\nhttps://github.com/xrootd/xrootd/blob/ced6cdeccc889d3943e4d5e93d5ce24c2b88512f/src/XrdXrootd/XrdXrootdXeq.cc#L2453-L2458\r\n\r\nFor #243, probably this code:\r\nhttps://github.com/xrootd/xrootd/blob/ced6cdeccc889d3943e4d5e93d5ce24c2b88512f/src/XrdXrootd/XrdXrootdXeq.cc#L2464-L2478\r\n\r\nIf an uproot version earlier than 4.0.2 (e.g. 4.0.1) was being used by coffea 0.7 (which is possible when looking at the `setup.py` for that tag), then coffea maybe needs to use the latest version of uproot to avoid #253 which was fixed in 4.0.2.\r\n\r\nUnless the `response is None` case is caught in `callback` and `status` is printed to show what `XrdXrootdProtocol::do_ReadV` is complaining about then it's hard to be certain about what's going wrong.",
  "created_at":"2021-02-22T11:27:26Z",
  "id":783306611,
  "issue":276,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MzMwNjYxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-24T00:45:34Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "author_association":"MEMBER",
  "body":"Same clarification as [on StackOverflow](https://stackoverflow.com/q/66257420/1623645): I said that for all I know, it could be a small difference, like 10%. I didn't say that it is or should be 10%. The statement was about my lack of knowledge.",
  "created_at":"2021-02-18T13:40:51Z",
  "id":781351084,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTM1MTA4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T13:40:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I guess a GitHub discussion would have been a better choice than StackOverflow since it's more closer to an actual discussion than to a problem with a definite answer :wink: \r\n\r\nI had a quick look and tried the code above (with 8 `executors`). On my machine\u2122 it takes 2m 12s and ~18.29GB of peak memory. Interestingly I get the same numbers when I run with a single `executor`.\r\n\r\nThe minimum (uncompressed) data size of the resulting `pandas.Dataframe` is ~5GB:\r\n\r\n```python\r\n>>> sum(dt.itemsize for dt in branches.dtypes) * branches.shape[0] / 1024**3                                                                        \r\n4.912072330713272\r\n```\r\n\r\nYou should in principle be able to do the decompression and `Dataframe` creation with less than double the memory (~10GB) in worst case, assuming that each branch's data is independently stored, so that there is no overlapping data occupying the memory in each `executor`.\r\nBtw. you get this number when you simply use `.arrays()` (which takes 10s):\r\n\r\n```python\r\nimport sys\r\nimport resource\r\nimport uproot\r\n\r\ndef peak_memory_usage():\r\n    \"\"\"Return peak memory usage in MB\"\"\"\r\n    mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n    factor_mb = 1 / 1024\r\n    if sys.platform == \"darwin\":\r\n        factor_mb = 1 / (1024 * 1024)\r\n    return mem * factor_mb\r\n\r\nf = uproot.open('10k_events_PFSimplePlainTree.root')\r\nf[\"PlainTree\"].arrays()\r\nprint(peak_memory_usage())\r\n```\r\n\r\nAlso please note that Python is garbage collected and sometimes it's a bit lazy of cleaning things up, so your mileage may vary and peak memory usage might give weird results. **Do you have an actual comparison of the peak memory compared to your `uproot3` code other than guess the percentage?** That would certainly be interesting...\r\n\r\nI don't know much about the internals of the `uproot`'s `arrays(library=\"pd\")` mechanics, but if you want to go crazy, you could preallocate the underlying numpy array and fill it in place using different threads to be as memory efficient as one can be. The overhead would then be the cache which is used to grab a chunk of compressed data, uncompress it and write it directly to the target memory location. Which means you could get away with 5GB + cache overhead, given that you don't read the same data multiple times. That however might be much more complicated than it sounds `;)` I think that all the tools are there but you'll probably not find a copy&pasteable example out there.\r\n\r\nAnyways...\r\n\r\nIf memory is an issue, I'd definitely cite Jim who already pointed out: parallel I/O usually trades memory for speed. This could explain the higher memory footprint (however, still a bit of a mystery why the footprint is the same for 1 or 8 executors) and regarding the speed: you might not gain much benefit from parallel processes due to a GIL fight, which might even be responsible for a slowdown in some cases, see for example the wonderful work of David Beazley (http://www.dabeaz.com/GIL/) which is from 2010 but still valid.\r\n\r\nMy current conclusion is: don't use parallel executors at all and simply go with `.arrays(libary=\"pd\")` which takes 11GB of memory and 10 seconds.\r\n\r\nI am still curious about the footprint of the uproot3 parallel approach.",
  "created_at":"2021-02-18T15:00:29Z",
  "id":781403767,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTQwMzc2Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T15:00:29Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"NONE",
  "body":"> Same clarification as [on StackOverflow](https://stackoverflow.com/q/66257420/1623645): I said that for all I know, it could be a small difference, like 10%. I didn't say that it is or should be 10%. The statement was about my lack of knowledge.\r\n\r\nI am sorry for misquoting you, it is due to my English. It is my 3rd language and sometimes I may confuse may and should. I edited again my posts.",
  "created_at":"2021-02-18T15:23:35Z",
  "id":781420319,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTQyMDMxOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T15:23:35Z",
  "user":"MDQ6VXNlcjQ3MjE2ODc3"
 },
 {
  "author_association":"MEMBER",
  "body":"> You should in principle be able to do the decompression and `Dataframe` creation with less than double the memory (~10GB) in worst case, assuming that each branch's data is independently stored, so that there is no overlapping data occupying the memory in each `executor`.\r\n\r\nThere's another doubling because all of the little arrays from the TBaskets have to be concatenated into a big array representing the whole TBranch. That particular doubling would not scale with the number of executor, since each task is responsible for a disjoint subset of the TBaskets and the single output array is common to all. So that's 1\u00d7 5GB for the individual TBaskets, 1\u00d7 5GB for the resulting array, and 1\u00d7 5GB for the Pandas DataFrame = 15GB. That's pretty close to 18GB.\r\n\r\n> Also please note that Python is garbage collected and sometimes it's a bit lazy of cleaning things up, so your mileage may vary and peak memory usage might give weird results.\r\n\r\nIf you have the memory available, the garbage collector won't bother cleaning it up, so you could get different results on a machine with a lower ceiling. It's not smart enough to delay tasks until previous ones are done and their garbage gets collected.\r\n\r\n> I don't know much about the internals of the uproot's `arrays(library=\"pd\")` mechanics\r\n\r\nI took a quick look at what `library=\"pd\"` does differently (since it's such a big difference in this case). Assuming that the data are simple types, it just wraps each TBranch's fully concatenated array as a Series,\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/446da9f5e9417a8aa2dc36958b80b9ee195f713b/uproot/interpretation/library.py#L899-L900\r\n\r\nand puts those Series (called `arrays` here) into a new DataFrame using the standard constructor.\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/446da9f5e9417a8aa2dc36958b80b9ee195f713b/uproot/interpretation/library.py#L932-L933\r\n\r\nWhile writing this comment, I got sucked into it and investigated. (There's a few hours between this sentence and the previous.) This `pandas.DataFrame` constructor makes a copy of all the Series it is given, but the total memory usage can be minimized by appending one column at a time:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/38ee68bfd4f0c7d90fdf5d12a2d894fb42f629ad/uproot/interpretation/library.py#L765-L778\r\n\r\nThis is the _first time_ I've ever used `gc.collect()` in production code, but without it, Python doesn't notice that it can garbage collect before running out of memory. It could be because Pandas's memory allocation is unconnected to the Python garbage collector. I would normally recommend strongly against such a thing, but this is a very limited use and only associated with making flat Pandas DataFrames. (Last month, I wrote my [first goto in decades](https://github.com/scikit-hep/awkward-1.0/blob/6cdc4d4e57fc41f6409b93895a3a825d6abb92a4/src/libawkward/forth/ForthMachine.cpp#L3138-L3143), so anything's possible.)\r\n\r\nThere were [a few places](https://github.com/scikit-hep/uproot4/pull/281/commits/38ee68bfd4f0c7d90fdf5d12a2d894fb42f629ad) where TBaskets (with associated raw data), arrays from TBaskets, and arrays before computing expressions could be deleted, which lowers the overall memory use before approaching Pandas, and these are the data that need to be collected with `gc.collect()`.\r\n\r\nNone of this had anything to do with parallelizing execution. The parallel processing was completely done and we were back on a single thread before any unnecessary data could be deleted. Parallelizing the decompression does make it considerably faster, so this file is in the regime of spending most of its time in the GIL-released decompression routines.",
  "created_at":"2021-02-18T19:49:20Z",
  "id":781594102,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTU5NDEwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T19:49:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The PR that trims memory usage is #281, so if that works for you, this issue can be closed.",
  "created_at":"2021-02-18T19:50:09Z",
  "id":781594517,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTU5NDUxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T19:50:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK that's a nice wrap-up!\r\n\r\n> If you have the memory available, the garbage collector won't bother cleaning it up, so you could get different results on a machine with a lower ceiling. It's not smart enough to delay tasks until previous ones are done and their garbage gets collected.\r\n\r\nExactly. Thanks for the better explanation. I had issues with this in past quite often where users were stuck in debugging memory leaks, instead you could easily limit the (V)RAM of the process and everything was \"fine\", so it was a no-problem. It is definitely a (let's call it) Python feature which can confuse and mislead people.\r\n\r\nYou made my day with `gc.collect()` and your `goto` \ud83d\ude06 \r\n\r\nNow it's time for @shahidzk1 to try #281. Unfortunately I cannot redo the test on the same machine currently because I am running a processing chain and need every possible memory address...",
  "created_at":"2021-02-18T20:01:15Z",
  "id":781600592,
  "issue":277,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MTYwMDU5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-18T20:01:15Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 },
 {
  "author_association":"NONE",
  "body":"I spoke too soon. When trying to address specific indices, it fails again:\r\n\r\n```\r\n$ python -c \"import uproot as up; up.open('Event.root')['T'].arrays(['event/fTracks/fTracks.fYfirst[...:0]'])\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 1133, in arrays\r\n    arrays,\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3460, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/behaviors/TBranch.py\", line 3414, in basket_to_array\r\n    library,\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/interpretation/objects.py\", line 141, in basket_array\r\n    form = self.awkward_form(branch.file, index_format=\"i64\")\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/interpretation/objects.py\", line 122, in awkward_form\r\n    self._branch.file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/containers.py\", line 645, in awkward_form\r\n    self._values, file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/_util.py\", line 426, in awkward_form\r\n    file, index_format, header, tobject_header, breadcrumbs\r\n  File \"/home/koch/Projekte/histoprint/env/lib/python3.7/site-packages/uproot/containers.py\", line 369, in awkward_form\r\n    raise uproot.interpretation.objects.CannotBeAwkward(self.message)\r\nuproot.interpretation.objects.CannotBeAwkward: Double32_t in array (note: Event.root fClosestDistance has an example)\r\n```\r\n\r\nSlicing does work for another file I have though: [test.root.zip](https://github.com/scikit-hep/uproot4/files/6022713/test.root.zip)\r\n\r\n```\r\n$ python -c \"import uproot as up; up.open('test.root')['truth'].arrays(['truelepton_dir[...,0]'])\"\r\n```",
  "created_at":"2021-02-22T14:49:00Z",
  "id":783427263,
  "issue":282,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MzQyNzI2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-22T14:49:00Z",
  "user":"MDQ6VXNlcjU4ODQwNjU="
 },
 {
  "author_association":"MEMBER",
  "body":"Full paths or relative paths are not the issue: what it's trying to do here is load the whole `event` array as a single object to _divide it_ by `fTracks`, and _divide that_ by `fTracks.fYfirst[...:0]`. The first argument of [uproot.TTree.arrays](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#arrays) is \"`expressions`\", quantities that are computed by Python. (You are using the fact that this is a computed expression by slicing `fTracks.fYfirst` with `..., :0`, which couldn't be done in Uproot 3.)\r\n\r\nThe nested path names (with `/` not meaning \"divide\") and pattern matching (with `*` not meaning \"multiply\") are in the `filter_name` parameter. What you meant to say in your first example is:\r\n\r\n```python\r\n>>> uproot.open(\"/mnt/storage/data/misc/Event.root\")[\"T\"].arrays(\r\n...     [\"fTracks.fYfirst[..., :0]\"],\r\n...     filter_name=\"event/fTracks/fTracks.fYfirst\")\r\n<Array [{'fTracks.fYfirst[..., :0]': [, ... ] type='1000 * {\"fTracks.fYfirst[......'>\r\n```\r\n\r\nor you could hide that unwieldy field name as an alias (which work just like ROOT's built-in fAliases):\r\n\r\n```python\r\n>>> uproot.open(\"/mnt/storage/data/misc/Event.root\")[\"T\"].arrays(\r\n...     [\"first\"],\r\n...     filter_name=\"event/fTracks/fTracks.fYfirst\",\r\n...     aliases={\"first\": \"fTracks.fYfirst[..., :0]\"})\r\n<Array [{first: []}, ... {first: []}] type='1000 * {\"first\": var * float32}'>\r\n```\r\n\r\nAlso, if you're after just one branch, you can navigate to that branch explicitly and avoid all this searching and renaming.\r\n\r\n```python\r\n>>> uproot.open(\"/mnt/storage/data/misc/Event.root\")[\"T/event/fTracks/fTracks.fYfirst\"].array()[..., :0]\r\n<Array [[], [], [], [], ... [], [], [], []] type='1000 * var * float32'>\r\n```\r\n\r\nNow I'm wondering if you really meant to slice it with \"`..., :0`\", since this would give you all empty lists, but the slicing is independent of the distinction between `expressions` (computable expressions, possibly aliased), `filter_name` (picking out the branches you want), and paths (navigating through TDirectories and TBranches).\r\n\r\nI don't see a bug here, and the above would be good for everybody to know, so I'm going to make this a Discussion. If I'm wrong and there's still a bug, open a new Issue. (Unfortunately, we can't turn Discussions back into Issues, but a second iteration on your problem would likely be more specific, anyway, justifying a new Issue.)",
  "created_at":"2021-02-22T15:56:52Z",
  "id":783475779,
  "issue":282,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4MzQ3NTc3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-22T15:56:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks for looking into this! I'm looking at the CI error, and it might be something that expects one error but is getting another.\r\n\r\nInstead of raising `OSError`, maybe this should call `self._xrd_error(status)`?\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/c719f858e775d8bd04b05a69552610e51cdb0047/uproot/source/xrootd.py#L96-L110\r\n\r\nThe trouble then is that the `callback` is defined in one of `XRootDResource`'s static methods, rather than a normal method. That could be fixed by changing `callbacker` from a static method into a normal method:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/c719f858e775d8bd04b05a69552610e51cdb0047/uproot/source/xrootd.py#L214-L215\r\n\r\n(dropping `@staticmethod` and adding `self` as the first argument) and calling it as such:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/c719f858e775d8bd04b05a69552610e51cdb0047/uproot/source/xrootd.py#L337\r\n\r\n(replacing `self.ResourceClass` with `self._resource`).\r\n\r\nThat ought to do it. I know that the callback/callbacker is not used outside of this file, and these are all the find-matches.",
  "created_at":"2021-02-24T01:24:54Z",
  "id":784663613,
  "issue":286,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4NDY2MzYxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-24T01:24:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks @jpivarski! I made the change you suggested.",
  "created_at":"2021-02-24T01:42:48Z",
  "id":784670724,
  "issue":286,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4NDY3MDcyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-24T01:42:48Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "author_association":"MEMBER",
  "body":"And it seems that solved the error, great!\r\n\r\nThanks, again!",
  "created_at":"2021-02-24T13:00:43Z",
  "id":785058322,
  "issue":286,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4NTA1ODMyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-02-24T13:00:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, if it says `NotImplementedError(\"memberwise serialization of AsArray\")`, that's a duplicate of issue #38. I need to investigate and add deserialization code for ROOT's \"memberwise\" option\u2014it simply doesn't exist at the moment.\r\n\r\nAlthough the `\"AsObjects(AsArray(True,False,None\"` string also looks wrong: an Interpretation ought to return its string representation but is just returning (the string) `\"None\"` for some reason. I'll look at that, too.",
  "created_at":"2021-03-02T13:41:46Z",
  "id":788917289,
  "issue":288,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4ODkxNzI4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-02T13:41:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry that I lost track of this one!\r\n\r\nI've verified that you get the \"memberwise serialization\" error, but that is a duplicate of #38.\r\n\r\nThe other thing I was worried about, the interpretation name with `\"None\"` in it, is apparently gone:\r\n\r\n```python\r\n>>> uproot.open(\"uproot-issue-288.root:Events/event/channels/channels.pulses\").interpretation\r\nAsObjects(AsArray(True, False, AsVector(False, Model_Pulse)))\r\n```",
  "created_at":"2021-05-18T09:50:19Z",
  "id":843026741,
  "issue":288,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MzAyNjc0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-18T09:50:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm okay with this. Merge when you're done!",
  "created_at":"2021-03-03T19:10:15Z",
  "id":789985183,
  "issue":289,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4OTk4NTE4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-03T19:10:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As long as the new test line runs on at least one job, I'm happy with it",
  "created_at":"2021-03-03T19:31:36Z",
  "id":789998616,
  "issue":289,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc4OTk5ODYxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-03T19:31:36Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"MemoryError and `std::bad_alloc` are pretty similar: the first is Python's exception, the second is C++'s exception. The allocation that actually fails to allocate memory, whether it's in Python or C++, is not where the real problem lies. It's a matter of other things using up the memory.\r\n\r\nIf it's running out of memory \"legitimately,\" which is to say, you just don't have enough memory to do what you're trying to do, then it is difficult to diagnose on a different computer with a different amount of memory available. If it's a memory leak (which #281 was not), then it happens on any computer if you wait long enough.\r\n\r\nWhat you're doing in your code doesn't involve Pandas, just some masking, so you can remove a big performance and memory hog by replacing `library=\"pd\"` with `library=\"np\"` and do everything with plain NumPy arrays. (What you've named \"`df`\" would come out as a dict of NumPy arrays, which can be selected with square brackets and strings, just as you have it.)\r\n\r\nThe Python garbage collector is _supposed to_ get invoked when you start running low on memory, though it could only do that if the limit is approached while the Python code is active (i.e. a MemoryError, not `std::bad_alloc`). In #281, however, it wasn't getting invoked before a Pandas DataFrame construction, but that could be because Pandas has its own C++ code to hide allocations from the garbage collector. So removing Pandas might help that, or you could `import gc` and `gc.collect()` at the end of your loop.\r\n\r\nActually, thinking about this more deeply, the arrays you make in one step of iteration (`mask`, `df`, etc.) are still in scope when the next iteration starts, so either put all the logic in the body of the loop or explicitly `del mask`, `del df`, etc. when done, maybe in addition to `gc.collect()`. (The garbage collector can only remove things that are out of scope.) As it is, there has to be two iterations' worth of arrays in memory at the time that the next iteration makes arrays\u2014the old and the new ones. So that can also help.\r\n\r\nBeyond that, the resource utilization of XRootD are a bit of a mystery to me. @nsmith- recommends\r\n\r\n```python\r\nuproot.open.defaults[\"xrootd_handler\"] = uproot.source.xrootd.MultithreadedXRootDSource\r\n```\r\n\r\nand this should probably become the default. The current default does a vector-read, which has caused a number of problems.\r\n\r\nBeyond that, maybe break your work up into smaller processes. (That's what I'd do if I run out of all other options.)\r\n\r\nI'm going to convert this into a Discussion because it's not really a bug, that I know of.",
  "created_at":"2021-03-03T22:14:35Z",
  "id":790111094,
  "issue":290,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDExMTA5NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-03T22:14:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Since `self` in the first code block is a XRootDSource, which has a `self._timeout`, is there any downside to calling the equivalent of `_xrd_timeout` and passing it to `vector_read`? \r\n\r\n`_xrd_timeout` is unfortunately not defined for XRootDSource, but XRootDResource, though it could become a free function:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/7e3353cfdec91c340c7a0e26a31a695d2db3bcf1/uproot/source/xrootd.py#L90-L94",
  "created_at":"2021-03-04T18:54:28Z",
  "id":790848234,
  "issue":292,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDg0ODIzNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T18:54:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think it should be as simple as adding `timeout=self._resource._xrd_timeout()` to the call. I'll make a PR soon",
  "created_at":"2021-03-04T18:57:32Z",
  "id":790850009,
  "issue":292,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDg1MDAwOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T18:57:32Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"If the resource has the same `_timeout` (i.e. the data is simply copied). I would think that it is.",
  "created_at":"2021-03-04T18:59:14Z",
  "id":790850960,
  "issue":292,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDg1MDk2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T18:59:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The answer to your last question is yes it does just copy the timeout currently.",
  "created_at":"2021-03-04T19:27:36Z",
  "id":790870291,
  "issue":292,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDg3MDI5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T19:27:36Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Once these tests pass, I'll merge it. Thanks!",
  "created_at":"2021-03-04T19:16:39Z",
  "id":790862895,
  "issue":293,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDg2Mjg5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T19:16:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Please do make it non-default. I'll release a new version of Uproot when that's merged into main.\r\n\r\nThanks!",
  "created_at":"2021-03-04T20:40:21Z",
  "id":790926238,
  "issue":294,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDkyNjIzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T20:40:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"More python objects that are not being held:\r\n\r\n```python\r\nimport numpy\r\nsum(isinstance(o, numpy.ndarray) for o in gc.get_objects())\r\n```\r\nis 2\r\n\r\n```python\r\nimport XRootD\r\nsum(isinstance(o, XRootD.client.responses.ChunkInfo) for o in gc.get_objects())\r\n```\r\nis 0",
  "created_at":"2021-03-04T20:40:42Z",
  "id":790926426,
  "issue":294,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDkyNjQyNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T20:40:42Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"FYI @chrisburr may be interested",
  "created_at":"2021-03-04T20:45:02Z",
  "id":790928945,
  "issue":294,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDkyODk0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T20:45:02Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The example works for MultithreadedXRootDSource but the `notifications.join()` was not sufficient synchronization to wait for the futures somehow. I rewrote it to block properly and indeed the multithreaded source memory does not grow.\r\nThat said, there is still a bit of an issue in the delayed gc on the Chunk objects. If I remove `gc.collect()` I end up with something like this:\r\n![image](https://user-images.githubusercontent.com/6587412/110038359-42906580-7d05-11eb-97a7-c762bc8035e0.png)\r\n\r\nWhich is a bit unfortunate because in principle none of this data needs to be around any longer than handing the chunk to whatever decompression executor the basket has.",
  "created_at":"2021-03-04T22:18:37Z",
  "id":790984804,
  "issue":294,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDk4NDgwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T22:18:37Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Is it sufficient to set `MultithreadedXRootDSource` as the default xrootd source to avoid the leak, or does something else need to be done (e.g. adding a `gc.collect()` call somewhere?)",
  "created_at":"2021-03-04T22:28:42Z",
  "id":790989738,
  "issue":294,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDk4OTczOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T22:30:18Z",
  "user":"MDQ6VXNlcjU2NDEwOTc4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@ryuwd yeah that avoids the leak described here. And that is what was done in PR #295.",
  "created_at":"2021-03-04T22:53:14Z",
  "id":791005711,
  "issue":294,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MTAwNTcxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T22:53:14Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like this has already been reported https://github.com/xrootd/xrootd/issues/1400 and the cause the leak has been found. I'll try to find time to make a PR if nobody beats me to it.",
  "created_at":"2021-03-05T06:48:00Z",
  "id":791197484,
  "issue":294,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MTE5NzQ4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-05T06:48:00Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hmm should I have also adjusted `num_workers` ? Because `num_fallback_workers` is what was used for MultithreadedXRootDSource.",
  "created_at":"2021-03-04T21:53:55Z",
  "id":790972130,
  "issue":295,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDk3MjEzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T21:53:55Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"When MultithreadedXRootDSource is the default and not the fallback, it uses `num_workers` to decide how many threads to run. That's `1` because multithreading should be opt-in.\r\n\r\nNow I wonder why the same argument doesn't hold for `num_fallback_workers`. Maybe it should.\r\n\r\nBut anyway, I think `num_workers=1` by default is a good idea. It's easy to set a different number when passing arguments to the `uproot.open` function (and `uproot.iterate`, etc).",
  "created_at":"2021-03-04T21:59:35Z",
  "id":790975298,
  "issue":295,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MDk3NTI5OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-04T21:59:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"A nice way to watch object lifetime is to put this somewhere in the constructor:\r\n```python\r\nweakref.finalize(self, lambda s: print(f\"Deleting {s} from thread {threading.get_ident()}\"), repr(self))\r\n```\r\nThe cycles were found with the help of https://code.activestate.com/recipes/523004-find-cyclical-references/",
  "created_at":"2021-03-05T05:25:22Z",
  "id":791164044,
  "issue":297,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MTE2NDA0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-05T05:25:22Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Summary of what this PR does:\r\n\r\n   * Adds a `update_ranges_or_baskets` (bool) parameter to `_ranges_or_baskets_to_arrays`, which is only True for `iterate`. The `ranges_or_baskets` are only are only replaced (see nested function `replace`) if True.\r\n   * Objects like `basket` and `obj` (chunk or basket) that are no longer needed after a loop are decref'ed so that they don't hold memory while the `_ranges_or_baskets_to_arrays` blocks and waits for the next datum.\r\n   * Instead of the concrete object holding a reference `_concrete = self`, it's `_concrete = None` and the `concrete` property returns `self` in case of `None`.\r\n   * A million references to `_concrete` (attribute) have been changed to `concrete` (property) to get this benefit.\r\n\r\nEverything here looks perfectly safe.",
  "created_at":"2021-03-05T14:41:14Z",
  "id":791461142,
  "issue":297,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MTQ2MTE0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-05T14:41:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"make the ROOT file\r\n\r\n```python\r\n$ cat make_tefficiency.py \r\nimport ROOT\r\n\r\nfp = ROOT.TFile.Open(\"test-efficiency.root\", \"RECREATE\")\r\n\r\nnbins = 11\r\n\r\nh_den = ROOT.TH1F('h_den', 'h_den', nbins, 0, 100)\r\nh_num = ROOT.TH1F('h_num', 'h_num', nbins, 0, 100)\r\n\r\nfor i in range(1, nbins):\r\n    h_num.SetBinContent(i, 2**i)\r\n    h_den.SetBinContent(i, 2**(i+1))\r\n\r\neff = ROOT.TEfficiency(h_num, h_den)\r\neff.SetName('TEfficiencyName')\r\neff.SetTitle('TEfficiencyTitle')\r\n\r\neff.SetBetaBinParameters(0, -1.0, -2.0)\r\nfor i in range(1, nbins):\r\n    eff.SetBetaBinParameters(i, 2**i, 2**(i+1))\r\n\r\neff.SetBetaBinParameters(nbins, -1.0, -2.0)\r\n\r\nh_den.Write()\r\nh_num.Write()\r\neff.Write()\r\nfp.Close()\r\n```\r\n\r\nthen run it\r\n\r\n```python\r\n$ cat debug.py \r\nimport uproot\r\n\r\nwith uproot.open('test-efficiency.root') as fp:\r\n    eff = fp['TEfficiencyName']\r\n```",
  "created_at":"2021-03-05T05:54:35Z",
  "id":791175732,
  "issue":298,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5MTE3NTczMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-05T05:54:35Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"This looks like the right thing to do. (There are earlier references to `bcnt` having a `kByteCountMask`, but that's in a logical-or with `kNewClassTag`, so it won't always be true.)\r\n\r\nIf you mark this as \"ready for review,\" I'll merge it right away. The fact that the tests pass is strong evidence that we weren't relying on `bcnt` being huge numbers. (`kByteCountMask` is 1073741824, which isn't ever a plausible number of bytes to skip.)",
  "created_at":"2021-03-10T21:03:35Z",
  "id":796102528,
  "issue":301,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5NjEwMjUyOA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-03-10T21:03:35Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> If you mark this as \"ready for review,\" I'll merge it right away.\r\n\r\ndone\r\n\r\n\r\n\r\n> The fact that the tests pass is strong evidence that we weren't relying on `bcnt` being huge numbers. (`kByteCountMask` is 1073741824, which isn't ever a plausible number of bytes to skip.)\r\n\r\nRight. I think unless a single object somehow manages to be that large in size, it's just not happening.",
  "created_at":"2021-03-10T21:04:54Z",
  "id":796104751,
  "issue":301,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5NjEwNDc1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-10T21:04:54Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"That's right: you should never see this `JaggedArray` class; it's used internally when collecting data, before converting it to the appropriate NumPy/Awkward/Pandas output. Fixed in PR #304.\r\n\r\nThanks for reporting it!",
  "created_at":"2021-03-12T18:55:47Z",
  "id":797684054,
  "issue":303,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5NzY4NDA1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-12T18:55:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We might need to update to the updated metadata system from (IIRC) boost-histogram 0.12+.",
  "created_at":"2021-03-12T19:23:15Z",
  "id":797701446,
  "issue":305,
  "node_id":"MDEyOklzc3VlQ29tbWVudDc5NzcwMTQ0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-12T19:23:15Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"The reason is that not all data types have been reverse-engineered; when a data type is not recognized, it is presented as \"unknown.\"\r\n\r\nI found a random MiniAOD file (`Run2016G/SingleMuon/MINIAOD/23Sep2016-v1/90000/1C603D88-C597-E611-A0BE-008CFAF2931E.root`) and tried it out. These branches don't have \"`PAT`\" in their names, but \"`RECO`\", though I suspect that they're the same types of objects because CMSSW's naming convention puts the type first, then collection name, etc. The branch I'm looking in is\r\n\r\n   * `\"patMETs_slimmedMETs__RECO./patMETs_slimmedMETs__RECO.obj/patMETs_slimmedMETs__RECO.obj.m_state.p4Polar_.fCoordinates.fPt\"`\r\n\r\nand I find that the branch is readable:\r\n\r\n```python\r\n>>> t[\"patMETs_slimmedMETs__RECO./patMETs_slimmedMETs__RECO.obj/patMETs_slimmedMETs__RECO.obj.m_state.p4Polar_.fCoordinates.fPt\"].array()\r\n<Array [[36.6], [28.6], ... [28.1], [67.7]] type='97194 * var * float64'>\r\n```\r\n\r\nMoreover, this whole group of branches (everything under the same parent branch) has an interpretation:\r\n\r\n```\r\n>>> t[\"patMETs_slimmedMETs__RECO./patMETs_slimmedMETs__RECO.obj\"].show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\npatMETs_slimmedMETs_ | (group of patMETs_slimme | AsGroup(<TBranchElement 'patME\r\npatMETs_slimmedME... | float[]                  | AsJagged(AsDtype('>f4', 'fl...\r\npatMETs_slimmedME... | float[]                  | AsJagged(AsDtype('>f4', 'fl...\r\npatMETs_slimmedME... | float[]                  | AsJagged(AsDtype('>f4', 'fl...\r\npatMETs_slimmedME... | float[]                  | AsJagged(AsDtype('>f4', 'fl...\r\npatMETs_slimmedME... | float[]                  | AsJagged(AsDtype('>f4', 'fl...\r\npatMETs_slimmedME... | float[]                  | AsJagged(AsDtype('>f4', 'fl...\r\npatMETs_slimmedME... | float[]                  | AsJagged(AsDtype('>f4', 'fl...\r\npatMETs_slimmedME... | int32_t[]                | AsJagged(AsDtype('>i4'))\r\npatMETs_slimmedME... | int32_t[]                | AsJagged(AsDtype('>i4'))\r\npatMETs_slimmedME... | int32_t[]                | AsJagged(AsDtype('>i4'))\r\npatMETs_slimmedME... | double[]                 | AsJagged(AsDtype('>f8'))\r\npatMETs_slimmedME... | double[]                 | AsJagged(AsDtype('>f8'))\r\npatMETs_slimmedME... | double[]                 | AsJagged(AsDtype('>f8'))\r\npatMETs_slimmedME... | double[]                 | AsJagged(AsDtype('>f8'))\r\npatMETs_slimmedME... | double[]                 | AsJagged(AsDtype('>f8'))\r\npatMETs_slimmedME... | double[]                 | AsJagged(AsDtype('>f8'))\r\npatMETs_slimmedME... | std::vector<CorrMETDa... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | edm::RefCore*            | AsJagged(AsStridedObjects(M...\r\npatMETs_slimmedME... | uint64_t[]               | AsJagged(AsDtype('>u8'))\r\npatMETs_slimmedME... | std::vector<pat::Trig... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<pat::Look... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<std::stri... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<edm::Ref<... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<reco::Gen... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<std::stri... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<edm::PtrV... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<std::stri... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<pat::User... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<std::stri... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<float>*      | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<std::stri... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<int32_t>*    | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<std::stri... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<edm::Ptr<... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<pat::Cand... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<std::stri... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<reco::Gen... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<SpecificC... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<SpecificP... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | double[]                 | AsJagged(AsDtype('>f8'))\r\npatMETs_slimmedME... | std::vector<pat::MET:... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<pat::MET:... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<pat::MET:... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<pat::MET:... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | std::vector<pat::MET:... | AsObjects(AsArray(True, Fal...\r\npatMETs_slimmedME... | uint16_t[]               | AsJagged(AsDtype('>u2'))\r\npatMETs_slimmedME... | uint16_t[]               | AsJagged(AsDtype('>u2'))\r\npatMETs_slimmedME... | uint16_t[]               | AsJagged(AsDtype('>u2'))\r\n```\r\n\r\nSo maybe you have an old version of Uproot? It could be that the type you want wasn't interpreted at the time your Uproot version was released, but now it's done. (It's a continual process...)\r\n\r\nOh, and one other thing: a name ending with parentheses, such as \"`patMETs_slimmedMETs__PAT.obj.m_state.p4Polar_.Pt()`\", is not a branch name but a C++ method executed on the branch. (The C++ method is \"`Pt`\" and the class instance is `p4Polar_` in this case.) C++ methods do not exist in the ROOT files; they are a part of CMSSW that aren't available if you take the file out of the ROOT+CMSSW world. If the C++ method only extracts the `fCoordinates.fPt` (member data) and returns it, then you just have to do that manually instead. If it applied some sort of corrections, then you have to reproduce those corrections\u2014if they were expressed as code in the CMSSW codebase and you're not using the CMSSW codebase, then you can't get them except by looking up that code and reproducing it.",
  "created_at":"2021-03-19T14:01:13Z",
  "id":802855122,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjg1NTEyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T14:01:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hmm... okay. I also initially thought it was an issue with the file because most of the branches were loading fine. But there's no problem accessing them with ROOT.\r\n\r\nI guess I can upload one here since it's a MC (private sample).\r\n\r\n[output_1.txt](https://github.com/scikit-hep/uproot4/files/6171768/output_1.txt) \r\n\r\nIt will be great if you can take a quick look. Thanks!",
  "created_at":"2021-03-19T14:25:57Z",
  "id":802871713,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjg3MTcxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T14:27:37Z",
  "user":"MDQ6VXNlcjY0MTA4ODUw"
 },
 {
  "author_association":"MEMBER",
  "body":"For the main intention of getting the `pt`, there's\r\n\r\n```python\r\n>>> t[\"patJets_slimmedJets__PAT./patJets_slimmedJets__PAT.obj/patJets_slimmedJets__PAT.obj.m_state.p4Polar_.fCoordinates.fPt\"].array()\r\n<Array [[71.6, 53.3, 49.6, ... 10.8, 10.8]] type='250 * var * float64'>\r\n```\r\n\r\nalthough I do see a few unknown interpretations in this file. I'm looking into what those are now.",
  "created_at":"2021-03-19T14:37:25Z",
  "id":802880016,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjg4MDAxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T14:37:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The unknown interpretations are all \"`product_`\" and \"`keys_`\", which are not physics data anyway. (They have something to do with the CMSSW infrastructure.)\r\n\r\n```\r\npatJets_slimmedJets__PAT.obj.genJetRef_.refVector_.product_                      | unknown\r\npatJets_slimmedJets__PAT.obj.genJetRef_.refVector_.keys_                         | unknown\r\npatJets_slimmedJets__PAT.obj.genJetFwdRef_.ref_.product_                         | unknown\r\npatJets_slimmedJets__PAT.obj.genJetFwdRef_.backRef_.product_                     | unknown\r\npatJets_slimmedJets__PAT.obj.jetFlavourInfo_.m_bHadrons.refVector_.product_      | unknown\r\npatJets_slimmedJets__PAT.obj.jetFlavourInfo_.m_bHadrons.refVector_.keys_         | unknown\r\npatJets_slimmedJets__PAT.obj.jetFlavourInfo_.m_cHadrons.refVector_.product_      | unknown\r\npatJets_slimmedJets__PAT.obj.jetFlavourInfo_.m_cHadrons.refVector_.keys_         | unknown\r\npatJets_slimmedJets__PAT.obj.jetFlavourInfo_.m_partons.refVector_.product_       | unknown\r\npatJets_slimmedJets__PAT.obj.jetFlavourInfo_.m_partons.refVector_.keys_          | unknown\r\npatJets_slimmedJets__PAT.obj.jetFlavourInfo_.m_leptons.refVector_.product_       | unknown\r\npatJets_slimmedJets__PAT.obj.jetFlavourInfo_.m_leptons.refVector_.keys_          | unknown\r\n```\r\n\r\nI'm going to mark this as \"won't fix\" because you can get access to the physics data. Just be sure to check to find out whether the `Pt()` (method call) that you want is applying important corrections that aren't present in the `fPt` (member data).",
  "created_at":"2021-03-19T15:07:53Z",
  "id":802901880,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkwMTg4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:07:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"But you can see there's an entire branch under `patMETs_slimmedMETs__PAT.` with TBrowser, and it contains different pt from the branch you mentioned because it stores missing-eT(MET) of events. The () was a typo because I copied the path from somewhere else. Sorry if that caused some confusion.",
  "created_at":"2021-03-19T15:18:28Z",
  "id":802909180,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkwOTE4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:18:28Z",
  "user":"MDQ6VXNlcjY0MTA4ODUw"
 },
 {
  "author_association":"NONE",
  "body":"![jetPt](https://user-images.githubusercontent.com/64108850/111808581-7ed3e080-8917-11eb-9001-9b8ea782f6a0.png)\r\n![met](https://user-images.githubusercontent.com/64108850/111808585-80050d80-8917-11eb-998c-505094a91325.png)\r\n",
  "created_at":"2021-03-19T15:59:20Z",
  "id":802937189,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkzNzE4OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T15:59:20Z",
  "user":"MDQ6VXNlcjY0MTA4ODUw"
 },
 {
  "author_association":"MEMBER",
  "body":"Ah, somewhere between the first file and the second I swapped \"MET\" for \"Jet\". (I know what it was! I used the \"nearest branch name\" KeyError to pick what I thought was a misspelling, without reading it carefully.)\r\n\r\nThe `patMETs_slimmedMETs__PAT.obj` branch is uninterpreted and it contains no subbranches\u2014it must be \"unsplit.\"\r\n\r\n```\r\n>>> t[\"patMETs_slimmedMETs__PAT.obj\"].show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\npatMETs_slimmedMETs_ | unknown                  | <UnknownInterpretation 'none o\r\n>>> t[\"patMETs_slimmedMETs__PAT.obj\"].keys()\r\n[]\r\n>>> t[\"patMETs_slimmedMETs__PAT.obj\"].interpretation\r\n<UnknownInterpretation 'none of the rules matched'>\r\n```\r\n\r\nThe TBrowser shows \"split\" and \"unsplit\" branches the same way, but they're very different serializations. In order to read an unsplit branch, you need to be able to read every field because they're contiguous (i.e. you can't get to the Nth field if you haven't successfully read the N\u20121th field), whereas split branches can each be read independently. Not only does that mean that Uproot's support for split data is better, because if you want floating-point `pt` values, you only need to be able to read floating-point numbers, but even if it is possible to read an unsplit branch, the corresponding split branch will always be faster.\r\n\r\n(It would be faster for two reasons: first, because if you only one `pt` and a few other fields, you don't have to read and decompress unwanted fields, which affects both Uproot and ROOT. Second, because complex unsplit types have to be deserialized with a \"for\" loop, which is implemented in Python, rather than NumPy, in Uproot. That latter issue is going to be fixed with AwkwardForth, but the first issue will always be there.)",
  "created_at":"2021-03-19T16:00:43Z",
  "id":802938136,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkzODEzNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T16:00:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Our comments crossed in the mail.",
  "created_at":"2021-03-19T16:01:08Z",
  "id":802938437,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjkzODQzNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T16:01:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Ah, that makes sense. So you mean the unsplit branches aren't supported?",
  "created_at":"2021-03-19T16:08:33Z",
  "id":802943612,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjk0MzYxMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T16:08:33Z",
  "user":"MDQ6VXNlcjY0MTA4ODUw"
 },
 {
  "author_association":"MEMBER",
  "body":"\"Fewer\" unsplit branches are supported. Some do work because all fields are interpretable and of course we're not going to prevent it from being read. But then, I always give the performance-related warning about requests to implement unsplit types because after all that work, it won't be very fast, and that might be a show-stopper for actually using it. Splitting is preferable in most end-user applications, and that's why ROOT's new RNTuple is 100% split by design.",
  "created_at":"2021-03-19T16:19:01Z",
  "id":802950751,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjk1MDc1MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T16:19:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Well then guess I'll have to use ROOT. With that said, I'm much more comfortable in the python environment and uproot has been awesome so far. I think that numpy+uproot approach will become mainstream in the near future.\r\n\r\nThanks for the quick response!",
  "created_at":"2021-03-19T16:41:27Z",
  "id":802964879,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjk2NDg3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T16:41:27Z",
  "user":"MDQ6VXNlcjY0MTA4ODUw"
 },
 {
  "author_association":"MEMBER",
  "body":"Maybe try RDataFrame in Python. We hope to integrate that with Awkward Array (https://github.com/scikit-hep/awkward-1.0/issues/588), which would provide more avenues for integration with the Python ecosystem.",
  "created_at":"2021-03-19T17:10:20Z",
  "id":802983623,
  "issue":308,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwMjk4MzYyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-19T17:10:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"\"Support\" in this case means, \"has special high-level methods for easier access.\" All of the data can be accessed as `all_members` (dict) and `member` (method to access one).\r\n\r\n`TGraphAsymmErrors` has these high-level methods because @kratsg had an idea about how he wanted to access `TGraphAsymmErrors` and implemented it (#240). The equivalent for `TGraph` would probably be very similar: a `values` property without the `errors`.\r\n\r\nIt's enough to add a submodule with the class name containing a class of the same name in `uproot.behaviors`, similar to\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/main/uproot/behaviors/TGraphAsymmErrors.py\r\n\r\nSince there are (to a first approximation) infinitely many classes in ROOT and the automated interpretation gets all of the private data members into `all_members`, I'd like to encourage users of Uproot to define how they want to access their favorite classes. That's why \"uproot-methods\" used to be a separate module (so that it could change more rapidly than the main codebase, and demonstrably didn't break core functioning), but keeping it as a separate module led to it getting ignored and out-of-date. So the methods/behaviors are part of the main Uproot codebase again, but I'd still like users of these classes to take an active part in shaping them.",
  "created_at":"2021-03-22T11:48:19Z",
  "id":804001705,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNDAwMTcwNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-22T11:48:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I did implement being able to read memberwise versions of TGraph recently in #298 . I think until that point, it wasn't quite easy for one to be able to read it in. One could use the test file in scikit-hep to write a test for it. I'm guessing because it's a TGraph, you'd only want the x/y values and not much else?",
  "created_at":"2021-03-22T12:12:31Z",
  "id":804015649,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNDAxNTY0OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-22T12:12:31Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"In my case I'm actually reading a `RooCurve`. I'll work on a PR for this, the `RooCurve` could use some extra functions to disentangle the strange way RooFit plots histograms and errors on histograms.",
  "created_at":"2021-03-22T12:15:39Z",
  "id":804017387,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNDAxNzM4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-22T12:15:39Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "author_association":"MEMBER",
  "body":"Wait, are we talking about deserializing (to get an object at all) or accessing the object's methods in an easier way? Deserialization is a more significant effort (and @kratsg's work on memberwise deserialization is beyond what I'd expect most people to be able to do).\r\n\r\nI thought we were talking about the fact that there's a `TGraphAsymmErrors` submodule in [uproot.behaviors](https://github.com/scikit-hep/uproot4/tree/main/uproot/behaviors) but not a `TGraph`.",
  "created_at":"2021-03-22T12:26:15Z",
  "id":804023444,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNDAyMzQ0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-22T12:26:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I was talking about the behaviour. I can see that we have a filled `all_members` already.",
  "created_at":"2021-03-22T12:31:08Z",
  "id":804026195,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNDAyNjE5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-22T12:31:08Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Well, I believe up until that PR -- most TGraphs were not able to be read in since it seems a lot of them tend to be memberwise-serialized. Now that they can be read in, it shouldn't be too hard to add in the behavior. Just mostly explaining why the behavior never quite existed before for a common object.",
  "created_at":"2021-03-22T12:35:22Z",
  "id":804027004,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNDAyNzAwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-22T12:35:22Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, \"most\" depends on what kinds of ROOT files you tend to look at (i.e. the measure of your space). I remember that uproot-methods had a behavior defined for TGraph, indicating that Uproot 3 could access TGraphs, even though Uproot 3 didn't do any memberwise deserialization.\r\n\r\nIt was scikit-hep/uproot3-methods#7, added by @marinang. Looking at that now, I see that there's actually three classes, TGraph, TGraphErrors, and TGraphAsymmErrors. His implementation had a lot of methods for finding the maximum and such, though I'd rather have these objects have smaller interfaces that go straight to the library with all of that functionality now. That is, with a `values` method to get a NumPy array, things like the maximum can be computed in NumPy. There was also a `matplotlib` method; it wouldn't be bad to have something that immediately returns an \"`ax`\" to be mixed into a plot or implicitly drawn by Jupyter.\r\n\r\nActually, TGraph and TGraphErrors are the only two behaviors that haven't been reimplemented in Uproot 4 or are going to be in [Vector](https://github.com/scikit-hep/vector). That set\u2014histograms/profiles, graphs, TParameter, and 2D/3D/Lorentz vectors\u2014are the ones that are desirable enough that they were requested again after the Uproot 3 \u2192 4 transition. THnSparse is in the old [uproot3-methods/classes](https://github.com/scikit-hep/uproot3-methods/tree/master/uproot3_methods/classes) directory, but it's just a placeholder (says \"hello world\") as an illustration of how to get started.",
  "created_at":"2021-03-22T13:17:10Z",
  "id":804053046,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNDA1MzA0Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-22T13:17:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I've made a PR [here](https://github.com/scikit-hep/uproot4/pull/350) with a draft implementation. No tests yet, I think those need to go in scikit-hep-testdata, right?",
  "created_at":"2021-05-04T11:15:47Z",
  "id":831864328,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMTg2NDMyOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-04T11:15:47Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "author_association":"MEMBER",
  "body":"@beojan Thank you!\r\n\r\nAny files that are needed as input to a test would have to go into [scikit-hep-testdata/src/skhep_testdata/data](https://github.com/scikit-hep/scikit-hep-testdata/tree/main/src/skhep_testdata/data), which has a normal PR/release process. If you have any files that you plan to use to test #350, you can make the PR now and I'll merge-and-deploy scikit-hep-testdata as soon as possible. It should be publicly sharable and not much larger than a megabyte. (The largest so far is 64 MB, but the median is 68 kB.)",
  "created_at":"2021-05-04T17:28:21Z",
  "id":832114063,
  "issue":309,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMjExNDA2Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-04T17:28:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In fact I'm scheduled to be working on the writing aspect in a few days (depending on whether I finish [Vector](https://github.com/scikit-hep/vector) on time): https://github.com/scikit-hep/uproot4/issues/25#issuecomment-802917934",
  "created_at":"2021-03-24T19:40:23Z",
  "id":806114538,
  "issue":310,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNjExNDUzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-24T19:40:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is done or moot now; see #321.",
  "created_at":"2021-08-30T22:19:47Z",
  "id":908743806,
  "issue":310,
  "node_id":"IC_kwDOD6Q_ss42KlR-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-30T22:19:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I actually don't know _how_ you're using uproot 4.0.6 with uproot-methods\u2014they are formally incompatible.\r\n\r\nYou're right that there was a bug fix in uproot-methods a week ago (scikit-hep/uproot3-methods#98), but that was related to writing histograms to ROOT files and that's the one feature that Uproot 4 doesn't have yet\u2014the only recourse is to use Uproot 3. (See #310 and https://github.com/scikit-hep/uproot4/issues/25#issuecomment-802917934).\r\n\r\nOn Lorentz vectors, the [Vector](https://github.com/scikit-hep/vector) library is almost done, so it makes more sense to finish that than patch uproot-methods. On the specific issue with negative masses, Vector is following ROOT's prescription of copying the sign through a square root: `mass-squared = -4e-12 \u21d2 mass = -2e-6`. This maps the spacelike region of Minkowski space onto negative mass values (instead of imaginary mass values, but we don't want to introduce complex number types just for round-off errors).",
  "created_at":"2021-03-25T14:34:25Z",
  "id":806861678,
  "issue":311,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNjg2MTY3OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-25T14:34:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hi. Note also that single floating point precision (for 4-vectors) is regularly a source of issues when calculating invariant masses, just because of the calculation involved when the energy and momentum are much larger than their difference E-|p|. Using double precision can be a good solution. Else avoid using the energy and rather use mass and momentum in single precision, switching to double precision if you need E for some calculation?",
  "created_at":"2021-03-25T15:12:28Z",
  "id":806925016,
  "issue":311,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNjkyNTAxNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-25T15:12:28Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"Vector is letting the component dtype be completely unconstrained, governed by the backend. But that means that it can be changed using the backend's standard idiom: NumPy arrays of vectors can be changed with `.astype(np.float64)`, Awkward Arrays of vectors can be changed with [ak.values_astype](https://awkward-array.readthedocs.io/en/latest/_auto/ak.values_astype.html), etc.\r\n\r\n(That sort of thing shouldn't be automatically changed by some operations and not others. The `E**2 - p**2` difference is often numerically sensitive, but so are others, and some only in certain situations. Anyway, adopting ROOT's spacelike-to-negative-mass representation solves this issue: no more `NaN`s for masses near zero.)",
  "created_at":"2021-03-25T15:23:13Z",
  "id":806942253,
  "issue":311,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNjk0MjI1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-25T15:23:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Agreed, @jpivarski. Also some calculations involving angles in multi-body decays can trigger numerical instabilities ... The fact that dtype is unconstrained is a great thing, of course.\r\n\r\nI am not suggesting to change precision automaticall, BTW, no worries. Just noting that in some situations that's the way to get around problems, I mean when you're stuck ... ;-)",
  "created_at":"2021-03-25T15:27:30Z",
  "id":806949215,
  "issue":311,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgwNjk0OTIxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-25T15:27:30Z",
  "user":"MDQ6VXNlcjUwMTM1ODE="
 },
 {
  "author_association":"MEMBER",
  "body":"That's right\u2014thanks!",
  "created_at":"2021-03-31T12:52:01Z",
  "id":811043332,
  "issue":315,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxMTA0MzMzMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-31T12:52:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> I found this solution; typing pip install . slows down the debugging loop \r\n\r\nFor pure python packages, please use `pip install -e .`; this installs in \"editable\" mode, which basically means it makes a symlink to the directory, but _also_ supports all the niceties that require an installed package, like the version file from setuptools_scm, importlib.metadata, importlib.resouces, etc.\r\n\r\nThe version number won't update until you rerun this line, and if you have compiled components, they also don't update until you run this line, but otherwise, it's how one develops with packages, especially `/src` packages.",
  "created_at":"2021-03-31T18:51:54Z",
  "id":811341504,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxMTM0MTUwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-31T18:51:54Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll give `pip install -e .` a try. That would have to be after I move `uproot` to `src/uproot`, though.",
  "created_at":"2021-03-31T18:53:04Z",
  "id":811342511,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxMTM0MjUxMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-31T18:53:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is also the main reason to keep `setup.py`, as pip still requires a setup.py to enable editable mode, since it calls `python setup.py develop` somewhere. Eventually editable mode will make it into PEP 517 hopefully. Poetry and flit each have a special method to activate editable mode, since they don't have a `setup.py` to call develop on.\r\n\r\nYou can use editable mode on a non-src package, though then it's a little arbitrary on whether you get the \"package\" version or the \"local\" version - in general, it _mostly_ doesn't matter. Except when it does.",
  "created_at":"2021-03-31T18:54:40Z",
  "id":811343886,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxMTM0Mzg4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-31T18:54:40Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I bumped up the `uproot \u2192 src/uproot` task to try out `pip install -e .`. I was expecting that it would be an additional step before every debugging step, but one time sets up a link/not copy permanently (even after closing and opening a new terminal). So this is a good way to work and I'll use it from now on.\r\n\r\nSince I moved all the files, I'll be closing this PR and doing the rest of the work in another PR. That way, the \"move files\" commit is kept separate from the \"change file contents\" commit and I can keep using the \"Squash and merge\" button.",
  "created_at":"2021-03-31T19:16:04Z",
  "id":811363680,
  "issue":316,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxMTM2MzY4MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-03-31T19:16:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"From this listing, it's clear that you're using Uproot 4. If you're looking for `uproot.recreate`, the file-writing feature is currently being added to Uproot 4: see #321. (Your timing is almost perfect, actually.) Watch that discussion to keep informed about when file-writing is ready to test.\r\n\r\nBeyond file-writing, you'll find [quite a few differences between Uproot 3 and 4](https://uproot.readthedocs.io/en/latest/uproot3-to-4.html). If you need something to work just like Uproot 3, you can install the package that is now named [uproot3](https://pypi.org/project/uproot3/):\r\n\r\n```bash\r\npip install -U uproot3 uproot3-methods awkward0\r\n```",
  "created_at":"2021-04-05T21:20:30Z",
  "id":813657365,
  "issue":324,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgxMzY1NzM2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-05T21:20:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"```python\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'4.0.7'\r\n```",
  "created_at":"2021-04-16T16:26:33Z",
  "id":821292892,
  "issue":333,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTI5Mjg5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T16:26:33Z",
  "user":"MDQ6VXNlcjM3MDcyMjU="
 },
 {
  "author_association":"MEMBER",
  "body":"This wouldn't happen with XRootD because it was leaking through the HTTP fallback mechanism: a lot of servers HTTP servers don't support multi-part GET, so the optimistic default of asking for all TBaskets in one request has to fallback to one TBasket per request with `num_fallback_workers` threads (10) to collect the TBaskets as the server provides them.\r\n\r\nI looked up prmon, decided that I didn't want to custom-install something (no conda), and checked the thread count with print statements. I'm going to try to find some way to quantify the memory usage, since that's something psutil ought to be able to do. I suspect, though, that the memory leak was due to the thread leak: there were Python objects (instances of a ResourceWorker class and possibly open HTTP connections) attached to each thread.",
  "created_at":"2021-04-16T17:19:48Z",
  "id":821322099,
  "issue":333,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTMyMjA5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T17:19:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nevermind: prmon [does have a conda installer](https://anaconda.org/conda-forge/prmon).",
  "created_at":"2021-04-16T17:21:03Z",
  "id":821322759,
  "issue":333,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTMyMjc1OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T17:21:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"First, the number of threads now:\r\n\r\n![PrMon_wtime_vs_nthreads](https://user-images.githubusercontent.com/1852447/115061333-af734f80-9eae-11eb-93f8-d721e984e1f1.png)\r\n\r\nThere was a lucky/unlucky moment when the interval of sampling happened to be during a time when the fallback threads were active. 12 = 2 (main and HTTPSource thread) + 10 (fallback worker threads).",
  "created_at":"2021-04-16T17:25:50Z",
  "id":821325279,
  "issue":333,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTMyNTI3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T17:25:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This is from another run because I didn't realize that the same temporary files would be used for memory profiling, too, before I deleted them. The new number of threads is a flat line at 2 and the memory usage is:\r\n\r\n![PrMon_wtime_vs_pss_rss](https://user-images.githubusercontent.com/1852447/115061995-738cba00-9eaf-11eb-9e70-cc3eca364da7.png)\r\n",
  "created_at":"2021-04-16T17:30:15Z",
  "id":821327554,
  "issue":333,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTMyNzU1NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T17:30:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"See #333: this fixes the thread leak and the memory leak.\r\n\r\nIt only applies to HTTP servers that don't support multi-part GET, since the threads and Python objects being leaked were the fallback MultithreadedHTTPSource.",
  "created_at":"2021-04-16T17:32:27Z",
  "id":821328686,
  "issue":334,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTMyODY4Ng==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-04-16T17:32:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!\r\n\r\n(Sheesh\u2014division by zero!)",
  "created_at":"2021-04-16T18:15:50Z",
  "id":821396176,
  "issue":335,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTM5NjE3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-16T18:15:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, thank you for that. The fact that there's a separate (public) method `shutdown` from the context manager, `__exit__` makes it possible for it to not be linked up, but the centralization of the hook means it's really fixed. That prmon is really useful!",
  "created_at":"2021-04-17T18:39:25Z",
  "id":821868340,
  "issue":338,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTg2ODM0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-17T18:39:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thank you for this! The test error is easy to fix:\r\n\r\n```\r\nsrc/uproot/source/http.py:90:28: E711 comparison to None should be 'if cond is None:'\r\n```\r\n\r\nI see that you're parsing the URL to get the username; isn't that something that urllib.parse would do for you? (That parsing has already been done earlier in uproot/rrading.py or uproot/_util.py, but it can be done again.)\r\n\r\nI don't see how the password/secret is passed in. It's not part of the URL, naturally. Should it (and the username itself, optionally) be passed in as `uproot.open` options? That's an open-ended set of keyword options whose defaults are set in uproot/reading.py, but they're passed through into the Source constructors.",
  "created_at":"2021-04-18T13:44:00Z",
  "id":821993835,
  "issue":339,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMTk5MzgzNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-18T13:44:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi Jim,\r\n\r\nThanks for the quick response! \r\n\r\nYes that error is easy to fix, but I'm more concerned about the build error on 2.7 (apparently the combined_dict = {**dict1,**dict2} trick was only introduced in python3.5 :( ). I'll try to fix these later today. \r\n\r\nI'm not adding any extra parsing step here, urllib is already parsing the username and password (see the table in https://docs.python.org/3/library/urllib.parse.html#url-parsing ) from the URL, since apparently the http(s)://user:password@website format is somewhat standard.  I'm just checking to see if the credentials are present in the parser result (by comparing username and password to None), and if so, creating the appropriate header and including it with every HTTP request. \r\n\r\nIt might be a good idea to support passing it in outside of the URL as that might be more obvious than putting it in the URL, but putting it in the URL lets you treat local and HTTP data directories uniformly. Perhaps providing a helper method to craft the URL from a URL/username/password may be a better interface?",
  "created_at":"2021-04-18T18:45:15Z",
  "id":822040353,
  "issue":339,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjA0MDM1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-18T18:45:15Z",
  "user":"MDQ6VXNlcjkyMDY1Njk="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't realize that the password was coming in through the URL, too. At least everything is handled in a standard way (and I'm glad that it's the urllib parser that's doing it\u2014note that the module name was different in Python 2.7, but you might have already noticed that).\r\n\r\nAs for the syntax error in Python 2.7, this would work:\r\n\r\n```python\r\ndict(first, **second)\r\n```\r\n\r\nThe syntax for this has just been getting incrementally better.\r\n\r\nIt could also be useful to add a prominent note about this new feature in the `uproot.open` docstring, since it would likely go unnoticed otherwise. We Want people to know about this!",
  "created_at":"2021-04-18T21:20:55Z",
  "id":822063727,
  "issue":339,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMjA2MzcyNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-18T21:20:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi Jim,\r\n\r\nIt looks like my latest attempt has passed all checks. I'm also willing to write a test, although I would need to find a semi-permanent URL I can use that uses basic authentication. \r\n\r\nThanks,\r\n-Cosmin",
  "created_at":"2021-04-20T17:15:35Z",
  "id":823458497,
  "issue":339,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMzQ1ODQ5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-20T17:15:35Z",
  "user":"MDQ6VXNlcjkyMDY1Njk="
 },
 {
  "author_association":"MEMBER",
  "body":"This looks great! I have read it over carefully and there's nothing I'd ask you to change. It would be better to have a test, but this is one of those circumstances in which we're constrained by what's available. I don't know how to set up an HTTPS service with password authentication and such a thing would have to be maintained as long as Uproot is maintained, which would be hard to do. It's already saying something that it _has_ been tested (on `users.rcc.uchicago.edu`), which is a huge step above not having tested it at all.\r\n\r\nThanks for contributing!\r\n\r\n@all-contributors please add @cozzyd for code",
  "created_at":"2021-04-20T17:30:55Z",
  "id":823468008,
  "issue":339,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMzQ2ODAwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-20T17:30:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/342) to add @cozzyd! :tada:",
  "created_at":"2021-04-20T17:31:05Z",
  "id":823468099,
  "issue":339,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgyMzQ2ODA5OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-04-20T17:31:05Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"It passes tests, but let me know when it's out of \"draft\" status (when you think it's ready for merging).\r\n\r\nA comment: I'd prefer to not introduce new specialized exception types; it looks like both of these could be ValueError. I know that having more exception types can be good for writing more fine-grained `try`-`catch` logic, but it's hard to be consistent in defining exception types for different things; the system tends to get lopsided with specialized exceptions for one thing and not others. Uproot has a few specialized exceptions (subclasses of the exceptions you'd expect, such as KeyInFileError for KeyError), but all of those cases involve getting more information to users. (KeyInFileError gives a list of other keys, sorted by string difference from the one you gave; DeserializationError is part of a try, try-again-with-TStreamerInfo pattern, etc.)",
  "created_at":"2021-05-04T18:29:51Z",
  "id":832153224,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMjE1MzIyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-04T18:29:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know when this is done!",
  "created_at":"2021-05-20T18:01:52Z",
  "id":845346008,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTM0NjAwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-20T18:01:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sorry, I was a little busy with other things. Here's the test file I'd like to use:\r\n\r\n[test-rooplot.zip](https://github.com/scikit-hep/uproot4/files/6526721/test-rooplot.zip)\r\n",
  "created_at":"2021-05-22T15:48:19Z",
  "id":846426486,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NjQyNjQ4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-22T15:48:19Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "author_association":"MEMBER",
  "body":"If you add that file in the directory below with name \"uproot-issue-350.root\", we can make a real test around it:\r\n\r\nhttps://github.com/scikit-hep/scikit-hep-testdata/tree/main/src/skhep_testdata/data\r\n\r\nI'll approve that scikit-hep-testdata PR and make a new version quickly so that you can test it here.",
  "created_at":"2021-05-22T22:40:41Z",
  "id":846472091,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NjQ3MjA5MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-22T22:40:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"PR made [here](https://github.com/scikit-hep/scikit-hep-testdata/pull/62). Sorry for the delay.",
  "created_at":"2021-06-05T20:09:21Z",
  "id":855289502,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NTI4OTUwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-05T20:09:21Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "author_association":"MEMBER",
  "body":"@beojan, uproot-issue-350.root is now available in [scikit-hep-testdata 0.4.4](https://pypi.org/project/scikit-hep-testdata/0.4.4). Tests using it should now work!",
  "created_at":"2021-06-08T20:46:36Z",
  "id":857125824,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1NzEyNTgyNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-08T20:46:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Tests now added. It looks like it failed because of a conda issue though.",
  "created_at":"2021-06-21T15:59:42Z",
  "id":865152813,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTE1MjgxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-21T15:59:42Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "author_association":"MEMBER",
  "body":"I restarted the tests, and they're failing in a relevant part now:\r\n\r\n```\r\n______________________ ERROR at setup of test_interpolate ______________________\r\nfile /home/runner/work/uproot4/uproot4/tests/test_0350-read-RooCurve-RooHist.py, line 117\r\n  def test_interpolate(roocurve, roocurve_err):\r\nE       fixture 'roocurve' not found\r\n>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, datafile, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_classes, roocurve_err, roocurve_hist, roohist, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory\r\n>       use 'pytest --fixtures [testpath]' for help on them.\r\n```\r\n\r\nMaybe you've set up the new fixture locally but haven't checked it in?\r\n\r\n(Unless it simplifies things in a noticeable way, I don't think it's necessary to use a lot of pytest features, such as fixtures. For instance, is this fixture used in many places, such that replacing redundancy with indirection is a good trade-off? I generally see tests as requiring a lower level of DRY than the main codebase, since the majority of them are written once and never updated, whereas the main codebase is frequently operated upon.)",
  "created_at":"2021-06-21T16:42:47Z",
  "id":865185545,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTE4NTU0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-21T16:42:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"```\r\n_____________________________ test_interpretation ______________________________\r\n\r\nroohist = <RooHist (version 1) at 0x7f66e4954650>\r\nroocurve = <RooCurve (version 1) at 0x7f66fc0707d0>\r\nroocurve_err = <RooCurve (version 1) at 0x7f66d7f49c10>\r\n\r\n    def test_interpretation(roohist, roocurve, roocurve_err):\r\n        assert roohist.classname == \"RooHist\"\r\n>       assert roohist.behaviors[0] == uproot.behaviours.RooHist.RooHist\r\nE       AttributeError: module 'uproot' has no attribute 'behaviours'\r\n\r\ntests/test_0350-read-RooCurve-RooHist.py:38: AttributeError\r\n```\r\n\r\nNeeds an explicit `import uproot.behaviors.RooHist` in the testing file. Behaviors aren't loaded preemptively because they might grow to be a large amount of code someday.\r\n\r\n```\r\n________________________________ test_to_boost _________________________________\r\n\r\nroohist = <RooHist (version 1) at 0x7f66d75c7890>\r\nroocurve = <RooCurve (version 1) at 0x7f66d75d6910>\r\nroocurve_err = <RooCurve (version 1) at 0x7f66d75cf910>\r\n\r\n    def test_to_boost(roohist, roocurve, roocurve_err):\r\n        rh_boost = roohist.to_boost()\r\n>       assert rh_boost.axes[0].edges == numpy.arange(0.0, 51.0)\r\nE       ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n\r\ntests/test_0350-read-RooCurve-RooHist.py:51: ValueError\r\n```\r\n\r\nCould use NumPy's `array_equal`, or convert both sides with `tolist`, or use `all` as they suggest.",
  "created_at":"2021-06-21T17:17:13Z",
  "id":865208386,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTIwODM4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-21T17:17:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(I'm going offline now. Might be unresponsive; sorry!)",
  "created_at":"2021-06-21T17:24:52Z",
  "id":865213443,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTIxMzQ0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-21T17:24:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Needs an explicit `import uproot.behaviors.RooHist` in the testing file.\r\n\r\nThat wasn't it (I copied the TGraphAsymmErrors test). I just spelled \"behaviors\" the British way.\r\n\r\nI've also fixed the other test.",
  "created_at":"2021-06-21T17:27:11Z",
  "id":865214855,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTIxNDg1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-21T17:27:11Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"All tests passed. I think this is ready for review.",
  "created_at":"2021-06-22T13:15:06Z",
  "id":865973745,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTk3Mzc0NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T13:15:06Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"By the way, are you planning to cut a new release soon? It's not urgent (I can just build from git), I'm just curious.",
  "created_at":"2021-06-25T09:14:45Z",
  "id":868358152,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODM1ODE1Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-25T09:14:45Z",
  "user":"MDQ6VXNlcjM3Mjc5MjU="
 },
 {
  "author_association":"MEMBER",
  "body":"I am. A lot of things have been coming in on Awkward and I'm looking for a good gap in which to cut a release there, and I would then do both, but there's no reason I shouldn't do it for Uproot now.",
  "created_at":"2021-06-25T11:05:29Z",
  "id":868420592,
  "issue":350,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2ODQyMDU5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-25T11:05:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Interestingly I can't reproduce it on lxplus but I can when reading the data from outside of the CERN. Now to figure out why...",
  "created_at":"2021-05-06T07:03:15Z",
  "id":833282584,
  "issue":351,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzMzI4MjU4NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-06T07:03:15Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"This isn't really an uproot problem (ROOT also had the same issue) but I'll summarise how I debugged it in case others run in to the same issue.\r\n\r\n1. Repeating the test many times showed that it only ever crashed on a subset of the files mentioned in this issue\r\n2. `eos fileinfo /eos/lhcb/...` showed that they all contained a replica that was on the same server\r\n3. It was reported at https://ggus.eu/?mode=ticket_info&ticket_id=151899 and indeed there was a server with a problem that prevented it from being made visible outside the firewall.",
  "created_at":"2021-05-07T06:56:46Z",
  "id":834117202,
  "issue":351,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDExNzIwMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T06:56:46Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"That's good news. Thanks for taking care of it @chrisburr!",
  "created_at":"2021-05-07T07:53:22Z",
  "id":834148101,
  "issue":351,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDE0ODEwMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T07:53:22Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks, @chrisburr! To make sure your solution remains visible, I'll convert this into a discussion.",
  "created_at":"2021-05-07T14:59:38Z",
  "id":834487642,
  "issue":351,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDQ4NzY0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T14:59:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"As a heads up to myself (and to make this easier to track in the future when the next `uproot` release comes out) it looks from [`pyhf`'s nightly CI using the head of our dependencies](https://github.com/scikit-hep/pyhf/actions/workflows/dependencies-head.yml) that this breaks (the `uproot4` CI [broke on 2021-05-05](https://github.com/scikit-hep/pyhf/actions/runs/815279416))\r\n\r\n- A breaking test: [all of `tests/test_mixins.py` due to `pyhf.readxml.parse` failing](https://github.com/scikit-hep/pyhf/blob/a38e7410b7dcf2d5b3c66b03beef85468dee04ff/tests/test_mixins.py#L6-L14) on [`hist.to_numpy()[0].tolist()`](https://github.com/scikit-hep/pyhf/blob/a38e7410b7dcf2d5b3c66b03beef85468dee04ff/src/pyhf/readxml.py#L75)\r\n\r\n```pytb\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nsrc/pyhf/readxml.py:349: in parse\r\n    channel, data, samples, channel_parameter_configs = process_channel(\r\nsrc/pyhf/readxml.py:228: in process_channel\r\n    parsed_data = process_data(data[0], rootdir, inputfile, histopath)\r\nsrc/pyhf/readxml.py:212: in process_data\r\n    data, _ = import_root_histogram(rootdir, inputfile, histopath, histoname)\r\nsrc/pyhf/readxml.py:75: in import_root_histogram\r\n    return hist.to_numpy()[0].tolist(), extract_error(hist)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <TH1F (version 2) at 0x7fd23819ecd0>, flow = False, dd = False\r\n\r\n    def to_numpy(self, flow=False, dd=False):\r\n        \"\"\"\r\n        Args:\r\n            flow (bool): If True, include underflow and overflow bins; otherwise,\r\n                only normal (finite-width) bins are included.\r\n            dd (bool): If True, the return type follows\r\n                `numpy.histogramdd <https://numpy.org/doc/stable/reference/generated/numpy.histogramdd.html>`__;\r\n                otherwise, it follows `numpy.histogram <https://numpy.org/doc/stable/reference/generated/numpy.histogram.html>`__\r\n                and `numpy.histogram2d <https://numpy.org/doc/stable/reference/generated/numpy.histogram2d.html>`__.\r\n    \r\n        Converts the histogram into a form like the ones produced by the NumPy\r\n        histogram functions.\r\n        \"\"\"\r\n        values = self.values(flow=flow)\r\n>       xedges = self.axis(0).edges(flow=flow)\r\nE       AttributeError: 'Model_TAxis_v10' object has no attribute 'edges'\r\n```",
  "created_at":"2021-05-16T03:11:36Z",
  "id":841758979,
  "issue":352,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MTc1ODk3OQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-16T03:11:36Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"For the record, the above is fixed in #363.",
  "created_at":"2021-05-17T18:06:18Z",
  "id":842525808,
  "issue":352,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjUyNTgwOA==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2021-05-17T18:06:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The smallest unit you can read is a TBasket, which is an interval of entries in a TBranch, so maybe you can go a little smaller, but there's a limit. The limit is set by the format itself, which is optimized for reading many events sequentially. (That's why it's a \"columnar\" format\u2014for fast random access to elements, you'd want a format that's more similar to Google Flatbuffers, but that's not the usual use-case in HEP.)\r\n\r\nIf you're only reading one entry, you can use `entry_start=4, entry_stop=5` to get an array of only entry `4`, for instance. This will read (and decompress) as little as possible\u2014the TBasket containing the entry\u2014to provide it. If the containing TBasket has 100 entries, then you've only read 99 extra, which is better than reading all of the entries in the entire TBranch (some number greater than 100).\r\n\r\nIf you want to read individual events a lot, TTree is the wrong format. That's not to say that ROOT is the wrong format: if the events you want to read are significantly larger than a TKey (about 80 bytes), then you might want to make them inherit from TObject and save them directly in a TDirectory, rather than putting them in a TTree. That would make them individually readable, at the cost of each one having its own TKey (two copies of the TKey, actually).\r\n\r\nIf you're interested in reading 8-byte or 4\u00d78-byte objects like this `vertex` randomly, then there isn't any good solution, even down to the hardware level. Even disks access data with a granularity of about 4 kB; there's no way to physically read less than that. If you're in this situation and it has to be random access (i.e. you can't just load everything you expect to need in memory and then access the memory buffer as needed), then the best you can do is the `entry_start`/`entry_stop` method with _uncompressed_ data. (The operating system will each recently accessed 4 kB page in memory and at least you're not decompressing the same block of data over and over.)",
  "created_at":"2021-05-07T15:37:51Z",
  "id":834541750,
  "issue":356,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDU0MTc1MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T15:37:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for replying. I think I didn't explain my issue well enough or maybe I'm misunderstanding your reply. My problem isn't about reading specific rows as fast as possible, my problem is reading the values of a column where the entries are arrays (horizontal vs. vertical problem). Back to the ``vertex`` example, what I want to do is:\r\n```\r\n[0] f[\"foo\"].arrays(\"vertex[4][0]\", library=\"pd\")\r\n0       2168.348682\r\n1       2374.811102\r\n    ....\r\n99      2838.820183\r\n100     1743.911113\r\n```\r\n\r\nBut I can't select the ``0``th element of ``vertex`` without loading the entire array then sub-selecting in the resulting dataframe like:\r\n\r\n``[0] f[\"foo\"].arrays(\"vertex[4]\", library=\"pd\")[\"vertex[4][0]\"]``\r\n\r\nI assume in most people's ROOT files they have a more reasonable structure where there would be separate ``vx``, ``vy``, ``vz`` and ``vt`` so you could do something like:\r\n\r\n``[0] f[\"foo\"].arrays(\"vx\", library=\"pd\")[\"vertex[4][0]\"]``\r\n\r\nbut unfortunately this is the structure I'm stuck with, I have multiple branches like this, including one with 200+ values from which I only need one. It's a big performance hit to load all 200+ values and then select the column I need.",
  "created_at":"2021-05-07T16:22:18Z",
  "id":834590522,
  "issue":356,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDU5MDUyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T16:22:18Z",
  "user":"MDQ6VXNlcjE1Njk3NjY3"
 },
 {
  "author_association":"MEMBER",
  "body":"Oh! I did misunderstand your problem. If you have a TBranch of 4-vectors, it _is_ necessary to read all 4 components to get one1component. In this data structure (a TBranch of 4-element arrays), the other 3 components are interleaved between these.\r\n\r\nIf (in C++), you made a `struct { float x; float y; float z; float t; }` and turned on splitting, the `x`, `y`, `z`, and `t` would get split into 4 distinct TBranches, which would show up independently in Uproot. But a `float[4]` does not get split at any split-level setting.",
  "created_at":"2021-05-07T17:17:29Z",
  "id":834633664,
  "issue":356,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDYzMzY2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T17:17:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I keep trying to answer these too quickly\u2014you clearly state that you can't change the structure.\r\n\r\nIf you want only the zeroth component of each, you can read the whole thing into an `array` and do\r\n\r\n```python\r\narray[:, 0]\r\n```\r\n\r\nto get an array of the zeroth component, if that's what you're asking. Maybe use `library=\"np\"` or `library=\"ak\"` to get an array (not DataFrame) with that slicing syntax and then wrap it as a Pandas Series/DataFrame.",
  "created_at":"2021-05-07T17:21:27Z",
  "id":834635960,
  "issue":356,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDYzNTk2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T17:21:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"No problem, sorry for the misunderstanding!\r\n\r\nI hadn't thought to go via NumPy or Awkward before wrapping in a DF, I don't know how much of the slowdown is due to Pandas specifically but I'll give it a go, thank you for the advice!",
  "created_at":"2021-05-07T23:56:18Z",
  "id":834876715,
  "issue":356,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDg3NjcxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-07T23:56:28Z",
  "user":"MDQ6VXNlcjE1Njk3NjY3"
 },
 {
  "author_association":"MEMBER",
  "body":"If you have any other issues, let me know!",
  "created_at":"2021-05-08T00:10:59Z",
  "id":834886972,
  "issue":356,
  "node_id":"MDEyOklzc3VlQ29tbWVudDgzNDg4Njk3Mg==",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2021-05-08T00:10:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Does `pkg_resources.get_distribution(\"XRootD\")` actually import XRootD? Because if it does, then this change means that importing Uproot will always import XRootD on startup, even if a user isn't using it.\r\n\r\nEven if it doesn't, changing the default of an input parameter based on another package feels a bit too magical\u2014it's the sort of surprise that could lead to another bug, especially of the \"works on my computer, not on yours\" variety. (Such a bug might not be XRootD-related, further confusing the issue.)\r\n\r\nInstead of changing `uproot.open`'s default, could the default always be XRootDSource and when the XRootDSource is starting (at the end of its `__init__`), old versions of XRootD make it immediately turn on its fallback mechanism? The effect would be the same: old versions of XRootD don't attempt to use vector_read, new versions try it and fall back if necessary, but both of these would use the same mechanism and `uproot.open`'s defaults would be constants.",
  "created_at":"2021-05-13T13:12:51Z",
  "id":840552460,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MDU1MjQ2MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-13T13:12:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Does `pkg_resources.get_distribution(\"XRootD\")` actually import XRootD?\r\n\r\nIt doesn't import XRootD.\r\n\r\n> Even if it doesn't, changing the default of an input parameter based on another package feels a bit too magical\r\n\r\nI see what you mean though I though the bug this is avoiding is a relatively small memory leak so it's not really possible to \"fallback\" to avoid the problem. I see `XRootDSource` being between 2-10 times faster than `MultithreadedXRootDSource` (on real workloads, often at the higher end of that range). The leak is small enough to ignore so I really wouldn't want the vector-reading based approach to be unavailable (plus XRootD 5.2.0 is still a release candidate so I can't upgrade yet).\r\n\r\nI don't see the default changing as being that magical. It's no different to using more threads on machines with more cores or using runtime dispatch to use vector instructions on some CPUs.\r\n\r\nThis is a bit opinionated but personally I'd be tempted to always default to `XRootDSource` as the speed difference feels more dramatic than the memory leak and the bug isn't an uproot issue. If somebody is doing something that is long running they can change the default to be `MultithreadedXRootDSource` and take the performance hit or petition the XRootD devs to make a release so they can upgrade. I know you dislike them but this might be case where a warning might be useful after XRootD 5.2 is out as it's hard for a user to understand either a) why memory is leaking or b) why uproot is so slow compared to reading local files or using plain old ROOT (if they realise at all).\r\n\r\nI'll let you decide how you want to handle it and update the PR accordingly.\r\n\r\n",
  "created_at":"2021-05-13T20:15:42Z",
  "id":840807183,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MDgwNzE4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-13T20:15:42Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"My mistake! The XRootDSource doesn't have a `_fallback()` method the way that HTTPSource does.\r\n\r\nIn that case, I'd say that XRootDSource should be the default and if the version is old (check in `extras.py` when we know sometime is actually using XRootD), we should raise a warning, saying that the bad version of XRootD is deprecated and will be excluded in Uproot 4.1.0. (I like temporary warnings for deprecations; it should be `FutureWarning`, since `DeprecationWarning` is hidden.)\r\n\r\nIs the new version of XRootD accessible in pip as well as conda? Is like to give an estimate for when we can start excluding the old version. (Uproot does have minimum versions for third party packages, even runtime requirements.)\r\n\r\n@nsmith- should be involved in this discussion, since he advocated the default change in the other direction. Is the problem you saw fixed?",
  "created_at":"2021-05-14T11:21:53Z",
  "id":841183168,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MTE4MzE2OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-14T11:21:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Is the new version of XRootD accessible in pip as well as conda?\r\n\r\nIt's not available anywhere yet though I believe updating PyPI is part of there release process so it should be available promptly once the final 5.2.0 release is available. I would be tempted to leave it as a warning for the foreseeable future as I suspect people are slow to upgrade (the XRootD 4.x series is still widely used).\r\n\r\nMaybe it is best to split this PR into two parts, one which conditionally applies the `atexit` handler and another which raises the `FutureWarning` and can wait until XRootD 5.2.0 is released on PyPI and conda-forge. I can ask on Monday to see if anyone knows how long this will take, I suspect it won't be that long looking at past release cycles.",
  "created_at":"2021-05-15T09:46:04Z",
  "id":841631422,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MTYzMTQyMg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-15T09:46:04Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"I've converted the version-dependent default into a warning, and that warning tells users to either upgrade or manually change the default. That warning would probably get annoying, but XRootD 5.2.0 is available on both [PyPI](https://pypi.org/project/xrootd/) and [conda-forge](https://anaconda.org/conda-forge/xrootd) now, so most users would be able to do the right thing and upgrade.\r\n\r\nIf you're okay with this, I can merge it and put it in Uproot [4.0.8](https://github.com/scikit-hep/uproot4/releases/tag/untagged-5ea6c0581ff469b1becf).\r\n\r\nLet me know; thanks!\r\n\r\n(I'll go fix whatever that error is.)",
  "created_at":"2021-05-20T18:30:16Z",
  "id":845364223,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTM2NDIyMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-20T18:30:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@nsmith- It looks like the new `pyxrootd.File` objects can't be pickled. You added `__getstate__`/`__setstate__` methods in #302. How important is it to pickle these objects?",
  "created_at":"2021-05-20T18:34:40Z",
  "id":845367555,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTM2NzU1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-20T18:34:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":">relatively small memory leak\r\n\r\nI'm confused, with current version of xrootd, 100% of the bytes read leak. How is this small?\r\n\r\nRegarding pickling, I though I didn't pickle the pyxrootd objects, only the URL with which they were instantiated? Edit: I must have only done this for the multi-threaded source and didn't put in a test for both sources. I'll prepare a patch, sorry about that",
  "created_at":"2021-05-20T18:39:19Z",
  "id":845370286,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTM3MDI4Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-20T18:42:31Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"On pickling, it was attempting to pickle the XRootDResource, and that one hadn't been pickle-safe. I just made it so. Perhaps you intended the XRootDResources to not get pickled, but they are. Where we decide to draw the line between what is pickleable and what isn't is not too important.\r\n\r\nOn the \"relatively small\" memory leak, there's some miscommunication, but\u2014the bug is fixed now, right? Small or large, is it true that there's no leak now?",
  "created_at":"2021-05-20T18:45:47Z",
  "id":845378077,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTM3ODA3Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-20T18:45:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"You're too fast! Maybe add a test for the alternative source alongside the default in https://github.com/scikit-hep/uproot4/blob/main/tests/test_0302-pickle.py#L60",
  "created_at":"2021-05-20T18:47:32Z",
  "id":845381544,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTM4MTU0NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-20T18:47:32Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Well, I assumed that the test was everything you wanted to test and made it work again. (The test you're pointing to is the one that fails; the code path gets to the c272886 addition.) Besides, the change to XRootDResource exactly mirrors your `__getstate__`/`__setstate__` in XRootDSource. It's just extending that capability in a very straightforward way.\r\n\r\n**Edit:** Sorry that I missed your edit! (It doesn't send an email.)",
  "created_at":"2021-05-20T18:52:19Z",
  "id":845385874,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTM4NTg3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-20T19:42:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes the test I put together in #294 now shows no large increase in RSS over the duration of reading:\r\n<img width=\"607\" alt=\"Screen Shot 2021-05-20 at 3 34 10 PM\" src=\"https://user-images.githubusercontent.com/6587412/119045263-d37bf200-b980-11eb-9129-8b2022976c1b.png\">\r\n",
  "created_at":"2021-05-20T20:34:31Z",
  "id":845458363,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTQ1ODM2Mw==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-05-20T20:34:31Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"@jpivarski Thanks for finishing this off, I think this is good to merge now that XRootD is updated (you must have checked a only few minutes after the conda-forge binary hit the CDN).\r\n\r\n(Also thanks @nsmith- for verifying that the bug is actually fixed!)",
  "created_at":"2021-05-21T08:07:20Z",
  "id":845749040,
  "issue":358,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0NTc0OTA0MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-21T08:07:20Z",
  "user":"MDQ6VXNlcjUyMjA1MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"If you include an example file here, I'll try it out.",
  "created_at":"2021-05-13T13:57:12Z",
  "id":840579185,
  "issue":359,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MDU3OTE4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-13T13:57:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"\r\n[correlations.zip](https://github.com/scikit-hep/uproot4/files/6473028/correlations.zip)\r\n\r\nThe TMatrixTSym is called: \"correlation_matrix\"",
  "created_at":"2021-05-13T14:23:59Z",
  "id":840596290,
  "issue":359,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MDU5NjI5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-13T14:23:59Z",
  "user":"MDQ6VXNlcjU2Njg5NDM4"
 },
 {
  "author_association":"MEMBER",
  "body":"The problem is that this object is unrecognized, and a dummy (`Unknown`) object was returned in its place:\r\n\r\n```python\r\n>>> rootdir = uproot.open(\"uproot-issue-359.root\")\r\n>>> rootdir[\"correlation_matrix\"]\r\n<Unknown TMatrixTSym<double> at 0x7fa0433e0dc0>\r\n>>> isinstance(rootdir[\"correlation_matrix\"], uproot.model.UnknownClass)\r\nTrue\r\n```\r\n\r\nThe reason for that is that the file doesn't contain any streamers to say how to turn the raw bytes into an object:\r\n\r\n```python\r\n>>> [x for x in f.file.streamers if \"matrix\" in x.lower()]\r\n['TMatrixTBase<double>']\r\n```\r\n\r\nStrangely, there's a streamer for one of TMatrixTSym's superclasses, but not for TMatrixTSym itself.\r\n\r\nThis ROOT file is not self-describing; neither Uproot nor ROOT should be able to read the TMatrixTSym object, though ROOT contains a version of all of these classes and might use its built-in TMatrixTSym class definition if the version happens to be the same.\r\n\r\nIn this case, the class version of [TMatrixTSym is 2](https://github.com/root-project/root/blob/bb7c58f472716ac834f2409d13ec11b0379d247d/math/matrix/inc/TMatrixTSym.h#L176) and has been since the [first appearance of this file](https://github.com/root-project/root/blob/4ec19d757bb61243ad4ca0d14c3f5720c84c35a3/matrix/inc/TMatrixTSym.h#L172), so there is not yet any ROOT version that would fail to read TMatrixTSym specifically, but some classes change more frequently than that.\r\n\r\nI don't know how the streamer got dropped from this file but in principle, that's the problem.\r\n\r\nYou get me wondering if I should make a database of streamers for all versions of every class in ROOT... I'm not sure how I would go about doing that, but it would solve problems like this, since Uproot doesn't (shouldn't!) have a manual reimplementation of every class in the ROOT codebase. Such a database also wouldn't be able to include user-defined classes. If those aren't included in the streamers, no program would be able to read them.",
  "created_at":"2021-05-13T14:53:11Z",
  "id":840615404,
  "issue":359,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MDYxNTQwNA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-13T14:53:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi, I am in the same position as OP, it seems. Trying to use a covariance matrix from a ROOT file of a data release, but the returned object is `UnknownClass`. Is there any way around this issue? Should I ask the creators of said ROOT file to do something differently?",
  "created_at":"2021-10-20T13:31:21Z",
  "id":947670828,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss44fE8s",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T13:31:21Z",
  "user":"MDQ6VXNlcjU4ODQwNjU="
 },
 {
  "author_association":"MEMBER",
  "body":"In PyROOT, do\r\n\r\n```python\r\n>>> import ROOT\r\n>>> f = ROOT.TFile(\"path/to/your/file.root\")\r\n>>> f.ShowStreamerInfo()\r\n```\r\n\r\nIf the class of the object you're trying to access (TMatrixTSym?) has no streamer in the list of streamers that the above should print out, then I'd say that the file is erroneous and we should find out why files are being written without this streamer info. Specifically, it lacks schema evolution, so even if this file can be opened by a specific version of ROOT (because that version of ROOT contains a class definition with the right version), another version of ROOT might not be able to open it.\r\n\r\nThe title of this issue is about the possibility of adding to Uproot a database of streamer info for all known class-version combinations, which would improve Uproot's ability to read classes that lack streamer info, but it would never be 100%. There may be new versions of ROOT that are unknown to the database, class-version combinations that don't match official releases (because somebody manually compiled ROOT between releases and wrote a file with it), user-defined classes, etc. The only way to completely get it right is to ensure that every class instance in a file has that class's streamer info embedded in the file.",
  "created_at":"2021-10-20T13:54:23Z",
  "id":947691088,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss44fJ5Q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T13:54:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Seems to be the same as OP's file:\r\n\r\n```\r\n>>> f.ShowStreamerInfo()\r\nOBJ: TList\tTList\tDoubly linked list : 0\r\n\r\nStreamerInfo for class: TMatrixTBase<double>, version=5, checksum=0x8b1ac221\r\n  TObject        BASE            offset=  0 type=66 Basic ROOT object   \r\n  Int_t          fNrows          offset=  0 type= 3 number of rows      \r\n  Int_t          fNcols          offset=  0 type= 3 number of columns   \r\n  Int_t          fRowLwb         offset=  0 type= 3 lower bound of the row index\r\n  Int_t          fColLwb         offset=  0 type= 3 lower bound of the col index\r\n  Int_t          fNelems         offset=  0 type= 3 number of elements in matrix\r\n  Int_t          fNrowIndex      offset=  0 type= 3 length of row index array (= fNrows+1) wich is only used for sparse matrices\r\n  double         fTol            offset=  0 type= 8 sqrt(epsilon); epsilon is smallest number number so that  1+epsilon > 1\r\n```\r\n\r\nI asked the people who created the file in question and they do not remember doing anything special. The matrix was probably stored just like you would any other object. Though they said they would try to dig up the script that created the file.\r\n\r\nAnd sorry if this is deviating from the topic of this issue too much. I am happy to open a new one if that would be preferable.",
  "created_at":"2021-10-21T16:19:53Z",
  "id":948773236,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss44jSF0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-21T16:19:53Z",
  "user":"MDQ6VXNlcjU4ODQwNjU="
 },
 {
  "author_association":"MEMBER",
  "body":"This is not getting too far from the thread's original intention.\r\n\r\nPutting aside the more general solution of a database of streamers, I did some digging to find out what this class's streamer ought to be. I can't find a way to print it out nicely, as it would be if it were in a file, but on the ROOT prompt,\r\n\r\n```\r\nroot [0] auto s = dynamic_cast<TStreamerInfo*>(TClass::GetClass(\"TMatrixTSym<double>\")->GetStreamerInfo())\r\n(TStreamerInfo *) @0x7ffe9ba4eb10\r\nroot [1] s->GetNelement()\r\n(int) 2\r\nroot [2] s->GetElement(0)->GetName()\r\n(const char *) \"TMatrixTBase<double>\"\r\nroot [3] s->GetElement(0)->GetTypeName()\r\n(const char *) \"BASE\"\r\nroot [4] s->GetElement(1)->GetName()\r\n(const char *) \"fElements\"\r\nroot [5] s->GetElement(1)->GetTypeName()\r\n(const char *) \"double*\"\r\nroot [6] s->GetClassVersion()\r\n(int) 2\r\n```\r\n\r\nTMatrixTSym<double> has two members: (1) the TMatrixTBase<double>, which _is_ in the file, so there's no need to simulate that, and (2) an array of doubles\u2014the actual data. It should be possible to make an Uproot Model by hand (in the [src/uproot/models](https://github.com/scikit-hep/uproot4/tree/main/src/uproot/models) directory) that relies on TMatrixTBase being present.\r\n\r\nIt might look like [Model_TGraph_v4](https://github.com/scikit-hep/uproot4/blob/main/src/uproot/models/TGraph.py), in that it has one `self._bases.append` to load the TMatrixTBase as its only base class, and then `cursor.array` for the `fElements`. I'm guessing that the length of that array will be `fNelems`, a member datum of the TMatrixTBase. `read_members` is the only method that would need a non-trivial implementation; `read_member_n`, `strided_interpretation`, `awkward_form`, and `_serialize` can all immediately raise exceptions. The name of that class would have to be `Model_TMatrixTSym_3c_double_3e__v2`. I'm not sure whether the array needs a \"speedbump\" byte or not; that would have to be determined experimentally.\r\n\r\nThis would be much easier if the streamer info were just encoded in the file. Actually, let me try to see if it can be done in some semi-automatic way, from PyROOT. I need to recompile ROOT because I upgraded Python, though...",
  "created_at":"2021-10-21T17:16:28Z",
  "id":948836000,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss44jhag",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-21T17:16:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm going to give you a _technique_ that may become a _feature_ someday. As it turns out, we can get streamer info from an active ROOT process:\r\n\r\n```python\r\n>>> import ROOT\r\n>>> import uproot\r\n>>> streamer_bytes = uproot.pyroot.pyroot_to_buffer(\r\n...     ROOT.TClass.GetClass(\"TMatrixTSym<double>\").GetStreamerInfo()\r\n... )\r\n>>> streamer_bytes\r\narray([ 64,   0,   1, 148, 255, 255, 255, 255,  84,  83, 116, 114, 101,\r\n        97, 109, 101, 114,  73, 110, 102, 111,   0,  64,   0,   1, 126,\r\n         0,   9,  64,   0,   0,  33,   0,   1,   0,   1,   0,   0,   0,\r\n         0,   3,   1,   0,   0,  19,  84,  77,  97, 116, 114, 105, 120,\r\n        84,  83, 121, 109,  60, 100, 111, 117,  98, 108, 101,  62,   0,\r\n       200, 115, 115,  45,   0,   0,   0,   2,  64,   0,   1,  75, 255,\r\n       255, 255, 255,  84,  79,  98, 106,  65, 114, 114,  97, 121,   0,\r\n        64,   0,   1,  57,   0,   3,   0,   1,   0,   0,   0,   0,   2,\r\n         0,   0,   0,   0,   0,   0,   0,   2,   0,   0,   0,   0,  64,\r\n         0,   0, 113, 255, 255, 255, 255,  84,  83, 116, 114, 101,  97,\r\n       109, 101, 114,  66,  97, 115, 101,   0,  64,   0,   0,  91,   0,\r\n         3,  64,   0,   0,  81,   0,   4,  64,   0,   0,  34,   0,   1,\r\n         0,   1,   0,   0,   0,   0,   3,   0,   0,   0,  20,  84,  77,\r\n        97, 116, 114, 105, 120,  84,  66,  97, 115, 101,  60, 100, 111,\r\n       117,  98, 108, 101,  62,   0,   0,   0,   0,   0,   0,   0,   0,\r\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n       139,  26, 194,  33,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n         0,   0,   0,   4,  66,  65,  83,  69,   0,   0,   0,   5,  64,\r\n         0,   0, 171, 255, 255, 255, 255,  84,  83, 116, 114, 101,  97,\r\n       109, 101, 114,  66,  97, 115, 105,  99,  80, 111, 105, 110, 116,\r\n       101, 114,   0,  64,   0,   0, 141,   0,   2,  64,   0,   0, 102,\r\n         0,   4,  64,   0,   0,  52,   0,   1,   0,   1,   0,   0,   0,\r\n         0,   3,   0,   0,   0,   9, 102,  69, 108, 101, 109, 101, 110,\r\n       116, 115,  29,  91, 102,  78, 101, 108, 101, 109, 115,  93,  32,\r\n       101, 108, 101, 109, 101, 110, 116, 115,  32, 116, 104, 101, 109,\r\n       115, 101, 108, 118, 101, 115,   0,   0,   0,  48,   0,   0,   0,\r\n         8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n         0,   0,   0,   7, 100, 111, 117,  98, 108, 101,  42,   0,   0,\r\n         0,   5,   7, 102,  78, 101, 108, 101, 109, 115,  20,  84,  77,\r\n        97, 116, 114, 105, 120,  84,  66,  97, 115, 101,  60, 100, 111,\r\n       117,  98, 108, 101,  62], dtype=uint8)\r\n>>> chunk = uproot.source.chunk.Chunk.wrap(None, streamer_bytes)\r\n>>> cursor = uproot.source.cursor.Cursor(0)\r\n>>> fake_file = uproot.writing._cascade._ReadForUpdate(\"<none>\", None)   # need a class_named method\r\n>>> uproot_streamer = uproot.deserialization.read_object_any(chunk, cursor, {}, fake_file, None, None)\r\n>>> uproot_streamer\r\n<TStreamerInfo for TMatrixTSym<double> version 2 at 0x7f0b70254c40>\r\n>>> uproot_streamer.show()\r\nTMatrixTSym<double> (v2): TMatrixTBase<double> (v5)\r\n    fElements: double* (TStreamerBasicPointer)\r\n```\r\n\r\nEven if you don't have access to ROOT and the final reading process in the same Python, this `uproot_streamer` can be pickled:\r\n\r\n```python\r\n>>> import pickle\r\n>>> pickle.loads(pickle.dumps(uproot_streamer)).show()\r\nTMatrixTSym<double> (v2): TMatrixTBase<double> (v5)\r\n    fElements: double* (TStreamerBasicPointer)\r\n```\r\n\r\nTo use it, you'd want to put it into the file that you're reading a TMatrixTSym from:\r\n\r\n```python\r\n>>> real_file = uproot.open(\"...\")\r\n>>> real_file.file.streamers\r\n{'TNamed': {1: <TStreamerInfo for TNamed version 1 at 0x7f0b6fdf47f0>},\r\n 'TObject': {1: <TStreamerInfo for TObject version 1 at 0x7f0b6fde7af0>},\r\n ...\r\n}\r\n```\r\n\r\nThe following is untested, because I don't have a file with a TMatrixTSym object in it:\r\n\r\n```python\r\n>>> real_file.file.streamers[uproot_streamer.name] = {uproot_streamer.class_version: uproot_streamer}\r\n```\r\n\r\n(It's a dict of dicts: class name \u2192 class version \u2192 streamer object. If the class name already exists with the wrong version, you'd want to add this version to its dict, not replace the whole dict.)\r\n\r\nThen you ought to be able to read that TMatrixTSym object, because when the file is serving up the data and checking its streamers to determine how to interpret it, it should find this streamer. Fingers crossed!\r\n\r\nIncidentally, if we put the TMatrixTSym Model directly into Uproot, then this is the code we would have had to write:\r\n\r\n```python\r\n>>> print(uproot_streamer.class_code())\r\nclass Model_TMatrixTSym_3c_double_3e__v2(uproot.model.VersionedModel):\r\n    def read_members(self, chunk, cursor, context, file):\r\n        if self.is_memberwise:\r\n            raise NotImplementedError(\r\n                \"memberwise serialization of {0}\\nin file {1}\".format(type(self).__name__, self.file.file_path)\r\n            )\r\n        self._bases.append(c('TMatrixTBase<double>', 5).read(chunk, cursor, context, file, self._file, self._parent, concrete=self.concrete))\r\n        tmp = self._dtype0\r\n        if context.get('speedbump', True):\r\n            cursor.skip(1)\r\n        self._members['fElements'] = cursor.array(chunk, self.member('fNelems'), tmp, context)\r\n...\r\n```\r\n\r\nGetting the streamer from ROOT saved us the trouble! (Indeed, there _is_ a speedbump byte, and `fNelems` is the length of the array.)",
  "created_at":"2021-10-21T18:54:32Z",
  "id":948910433,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss44jzlh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-21T18:54:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I tried the method you suggested, but I get an error:\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nDeserializationError                      Traceback (most recent call last)\r\n~/.local/lib/python3.7/site-packages/uproot/reading.py in get(self)\r\n   2484             try:\r\n-> 2485                 out = cls.read(chunk, cursor, context, self._file, selffile, parent)\r\n   2486 \r\n\r\n~/.local/lib/python3.7/site-packages/uproot/model.py in read(cls, chunk, cursor, context, file, selffile, parent, concrete)\r\n   1310             versioned_cls.read(\r\n-> 1311                 chunk, cursor, context, file, selffile, parent, concrete=concrete\r\n   1312             ),\r\n\r\n~/.local/lib/python3.7/site-packages/uproot/model.py in read(cls, chunk, cursor, context, file, selffile, parent, concrete)\r\n    821 \r\n--> 822             self.read_members(chunk, cursor, context, file)\r\n    823 \r\n\r\n<dynamic> in read_members(self, chunk, cursor, context, file)\r\n\r\n~/.local/lib/python3.7/site-packages/uproot/source/cursor.py in array(self, chunk, length, dtype, context, move)\r\n    326             self._index = stop\r\n--> 327         return numpy.frombuffer(chunk.get(start, stop, self, context), dtype=dtype)\r\n    328 \r\n\r\n~/.local/lib/python3.7/site-packages/uproot/source/chunk.py in get(self, start, stop, cursor, context)\r\n    401                 context,\r\n--> 402                 self._source.file_path,\r\n    403             )\r\n\r\nDeserializationError: while reading\r\n\r\n    TMatrixTSym<double> version 5 as uproot.dynamic.Model_TMatrixTSym_3c_double_3e__v2 (48 bytes)\r\n        (base): <TMatrixTBase<double> (version 5) at 0x7f914b6266d8>\r\nBase classes for TMatrixTSym<double>: (TMatrixTBase<double>)\r\nMembers for TMatrixTSym<double>: fElements\r\n\r\nattempting to get bytes 51:440926259\r\noutside expected range 0:3528 for this Chunk\r\nin file DataRelease/covmatrix_noreg.root\r\nin object /covmatrixCbin;1\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDeserializationError                      Traceback (most recent call last)\r\n<ipython-input-5-1e5bf2aa6a37> in <module>\r\n      2 F = up.open(\"DataRelease/covmatrix_noreg.root\")\r\n      3 fix_streamer(F)\r\n----> 4 cov_unfolded = F[\"covmatrixCbin\"].to_numpy()[0]\r\n      5 cor_unfolded = cov_unfolded / (\r\n      6     np.sqrt(np.diag(cov_unfolded))[:, None] * np.sqrt(np.diag(cov_unfolded))[None, :]\r\n\r\n~/.local/lib/python3.7/site-packages/uproot/reading.py in __getitem__(self, where)\r\n   2080 \r\n   2081         else:\r\n-> 2082             return self.key(where).get()\r\n   2083 \r\n   2084     @property\r\n\r\n~/.local/lib/python3.7/site-packages/uproot/reading.py in get(self)\r\n   2509                 context = {\"breadcrumbs\": (), \"TKey\": self}\r\n   2510 \r\n-> 2511                 out = cls.read(chunk, cursor, context, self._file, selffile, parent)\r\n   2512 \r\n   2513         if self._fClassName not in must_be_attached:\r\n\r\n~/.local/lib/python3.7/site-packages/uproot/model.py in read(cls, chunk, cursor, context, file, selffile, parent, concrete)\r\n   1309         return cls.postprocess(\r\n   1310             versioned_cls.read(\r\n-> 1311                 chunk, cursor, context, file, selffile, parent, concrete=concrete\r\n   1312             ),\r\n   1313             chunk,\r\n\r\n~/.local/lib/python3.7/site-packages/uproot/model.py in read(cls, chunk, cursor, context, file, selffile, parent, concrete)\r\n    820             )\r\n    821 \r\n--> 822             self.read_members(chunk, cursor, context, file)\r\n    823 \r\n    824             self.hook_after_read_members(\r\n\r\n<dynamic> in read_members(self, chunk, cursor, context, file)\r\n\r\n~/.local/lib/python3.7/site-packages/uproot/source/cursor.py in array(self, chunk, length, dtype, context, move)\r\n    325         if move:\r\n    326             self._index = stop\r\n--> 327         return numpy.frombuffer(chunk.get(start, stop, self, context), dtype=dtype)\r\n    328 \r\n    329     _u1 = numpy.dtype(\"u1\")\r\n\r\n~/.local/lib/python3.7/site-packages/uproot/source/chunk.py in get(self, start, stop, cursor, context)\r\n    400                 cursor.copy(),\r\n    401                 context,\r\n--> 402                 self._source.file_path,\r\n    403             )\r\n    404 \r\n\r\nDeserializationError: while reading\r\n\r\n    TMatrixTSym<double> version 5 as uproot.dynamic.Model_TMatrixTSym_3c_double_3e__v2 (48 bytes)\r\n        (base): <TMatrixTBase<double> (version 5) at 0x7f914b62ef60>\r\nBase classes for TMatrixTSym<double>: (TMatrixTBase<double>)\r\nMembers for TMatrixTSym<double>: fElements\r\n\r\nattempting to get bytes 51:440926259\r\noutside expected range 0:3528 for this Chunk\r\nin file DataRelease/covmatrix_noreg.root\r\nin object /covmatrixCbin;1\r\n```\r\n\r\nI attached the ROOT file I am trying to read the matrix from, in case you want to test this yourself: [covmatrix_noreg.zip](https://github.com/scikit-hep/uproot4/files/7397431/covmatrix_noreg.zip)\r\n",
  "created_at":"2021-10-22T13:25:28Z",
  "id":949630225,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss44mjUR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-22T13:25:28Z",
  "user":"MDQ6VXNlcjU4ODQwNjU="
 },
 {
  "author_association":"MEMBER",
  "body":"It successfully read the TMatrixTBase and then failed trying to get the `fElements`, thinking that it needed to read bytes 51 through 440926259 to get it\u2014that upper bound is clearly bogus (unless you have 55115776 double-precision numbers in `fElements`!). Since the upper bound comes from the TMatrixTBase (`fNelems`), maybe that was filled and TMatrixTBase has the right number of bytes, but the value is wrong.\r\n\r\nSo the automatically generated streamer is not right either, even when we manage to get it directly from ROOT. I'd have to look at the file directly. I'm looking at the file you attached.",
  "created_at":"2021-10-22T14:07:20Z",
  "id":949665358,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss44mr5O",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-22T14:08:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've started #484, which can read the matrices in the file you sent, but it's very weird: it doesn't have a separate header for the TMatrixTSym, as opposed to the TMatrixBase, switch is what got it off-track and garbled the `fNelems`, causing it to read past the end of the object. However, even when that number is not garbled, it's not the number of serialized values: it's equal to `N**2`, rather than `(N*(N + 1)/2`. And _then_ those elements are not counted in the number of bytes ) the number of bytes corresponds to the TMatrixBase, rather than the TMatrixTSym.\r\n\r\nYour file was written with ROOT 5, which makes me a little wary, but the serialization of these classes hasn't been changed in 16 years. Maybe that's early enough that they don't follow standard conventions for streamers\u2014it seems to be a \"custom streamer,\" and having the streamer in the file doesn't help. (Maybe that's why it was not included in the first place...)\r\n\r\nI'd feel more confident if we could test more cases, especially non-symmetric subclasses of TMatrixBase. Do you have any of those?",
  "created_at":"2021-10-22T16:17:05Z",
  "id":949774467,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss44nGiD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-22T16:17:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The relevant subclasses are TMatrixT, TMatrixTSparse, and TMatrixTSym, and I can guess that they can be specialized with any numerical type, but I think in practice, only `double` is ever used. PR #484 should work for you, but I'd be more comfortable if we could include tests of all three subclasses. I think this whole system is based on custom streamers.",
  "created_at":"2021-10-22T16:59:32Z",
  "id":949808597,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss44nO3V",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-22T16:59:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm putting that PR into draft mode so that I don't accidentally merge it. We'd need to be more certain that this is the right thing to do before including it.",
  "created_at":"2021-10-27T21:43:54Z",
  "id":953334123,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss440rlr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-27T21:43:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Would adding some tests for TMatrixT, TMatrixTSparse, and TMatrixTSym be enough for that? Maybe both with ROOT 5 and ROOT 6 files? Or is the concern more about the principle of the matter?",
  "created_at":"2021-10-28T09:29:13Z",
  "id":953673526,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss441-c2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T09:29:13Z",
  "user":"MDQ6VXNlcjU4ODQwNjU="
 },
 {
  "author_association":"MEMBER",
  "body":"It's not an in-principle thing: just an example of each of the subclasses would do. The changes I had to make for the symmetric matrix were arbitrary\u2014assuming that no changes are needed for the other cases would likely be wrong, and fixing one but not the others would really muddle things.\r\n\r\nAlso I don't think it has to be both ROOT 5 and 6\u2014the classes look old. I don't think they were changed recently, even on the timescale of the ROOT 5 \u2192 6 transition.",
  "created_at":"2021-10-28T11:38:37Z",
  "id":953762619,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss442UM7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T11:38:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I just tried to load the matrix with the PR branch of uproot 4, and it seems to work as expected.\r\n\r\nThough now I have the problem that the resulting object does not have a `to_numpy` method. How do I actually access the matrix elements?",
  "created_at":"2021-11-30T18:02:26Z",
  "id":982884093,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss46lZ79",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T18:02:26Z",
  "user":"MDQ6VXNlcjU4ODQwNjU="
 },
 {
  "author_association":"MEMBER",
  "body":"The raw values are in `obj.member(\"fElements\")`, but these haven't been corrected for the symmetric-matrix packing (only the diagonal and above\u2014or maybe below\u2014are stored in that 1-dimensional array). A `to_numpy` method that presents it as a 2-dimensional matrix would be useful; it would have to be written as a member of the\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/73f494980f310756d40a736030f0c96f7610f016/src/uproot/models/TMatrixT.py#L18\r\n\r\nclass, and similar for the TMatrixT and TMatrixTSparse cases.",
  "created_at":"2021-11-30T18:19:30Z",
  "id":982897627,
  "issue":359,
  "node_id":"IC_kwDOD6Q_ss46ldPb",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T18:19:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right: these are definitely errors in the documentation. There never was a `TTree.lazy`; I'm sorry for the confusion that typo caused. These examples were copy-pasted from tests performed while writing the tutorial\u2014I don't know how that got garbled.\r\n\r\nThe other case of Awkward Arrays not having a `cache` property is a change in the Awkward Array library (not Uproot). Awkward Arrays are trees that can, in principle, include VirtualArrays with different caches, so there isn't a single cache to expose as a parameter. The example could be changed to access `_caches` (plural, a tuple of length 1), but it's not good to promote the use of hidden (underscored) attributes. Instead, I changed the example to use an explicitly passed-in cache.\r\n\r\nEverything is updated: https://uproot.readthedocs.io/en/latest/basic.html#reading-on-demand-with-lazy-arrays\r\n\r\nI agree that having automated tests for the documentation would solve these problems. Since the reference documentation (such as the [uproot.lazy](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.lazy.html) page) is generated from docstrings and the signature of the function itself, they're up-to-date. Testing examples, whether in the tutorials or in the reference, is much harder. We're trying to do exactly that with the Awkward Array documentation\u2014instead of Sphinx restructured text pages, they're all [Jupytext markdown](https://github.com/scikit-hep/awkward-1.0/tree/main/docs-src), which means that the outputs are generated when the HTML is built, thereby testing them. Unfortunately, it doesn't actually stop the build if there are errors, so we've added [an explicit CI test](https://github.com/scikit-hep/awkward-1.0/blob/3b7009a23075a53f84cbe0333568325772d2fb95/.ci/azure-doctest-awkward.yml#L85-L93) to hunt for errors and stop a PR from being merged if it introduces an error. Nevertheless, there are still some kinds of errors that get through (pages missing from the table of contents, and therefore not included on the site), so that still needs to be improved.\r\n\r\nAnother issue with that is that the JupyterBooks that runs the Jupytext is separate from the Sphinx documentation that is generated from the docstrings and can be uploaded to ReadTheDocs, so Awkward Array has to have two documentation sites, one of which is hosted by Netlify. The examples in the reference documentation (docstrings) aren't tested, so it's still not a perfect sieve.\r\n\r\nSo I agree that it's desirable to have automated checks on documentation, but it will take some work to get that up and running.\r\n\r\nOn the other hand, thank you very much for reporting this! At least this error is now corrected.\r\n\r\n@all-contributors please add @peguerosdc for tests",
  "created_at":"2021-05-14T13:43:22Z",
  "id":841252997,
  "issue":360,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MTI1Mjk5Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-14T13:43:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/361) to add @peguerosdc! :tada:",
  "created_at":"2021-05-14T13:43:32Z",
  "id":841253083,
  "issue":360,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MTI1MzA4Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-14T13:43:32Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"NONE",
  "body":"awesome! I have checked the docs and indeed they are correct now, so I will close this issue as the error is fixed :smiley: ",
  "created_at":"2021-05-17T20:36:35Z",
  "id":842621737,
  "issue":360,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjYyMTczNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-17T20:36:35Z",
  "user":"MDQ6VXNlcjc4ODk3MjY="
 },
 {
  "author_association":"MEMBER",
  "body":"> can you let me know if this fixes your CI?\r\n\r\nYeah, let me setup a custom branch real quick that targets this branch instead of `master` for `uproot4` and then I'll trigger the CI.",
  "created_at":"2021-05-17T17:47:24Z",
  "id":842514253,
  "issue":363,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjUxNDI1Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-17T17:47:24Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay this is running now on \r\n\r\nhttps://github.com/scikit-hep/pyhf/runs/2603396161?check_suite_focus=true\r\n\r\nusing\r\n\r\n```yaml\r\n    - name: Install dependencies\r\n      run: |\r\n        python -m pip install --upgrade pip setuptools wheel\r\n        python -m pip --no-cache-dir --quiet install --upgrade --editable .[test]\r\n        python -m pip uninstall --yes uproot\r\n        python -m pip install --upgrade git+git://github.com/scikit-hep/uproot4.git@jpivarski/explicit-models-need-explicit-behaviors\r\n```",
  "created_at":"2021-05-17T17:55:27Z",
  "id":842519085,
  "issue":363,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0MjUxOTA4NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-17T17:55:27Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"NONE",
  "body":"The public link on CERNBox: https://cernbox.cern.ch/index.php/s/yme329A3Ts3jF7S",
  "created_at":"2021-05-25T17:53:41Z",
  "id":848088742,
  "issue":368,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODA4ODc0Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T17:53:41Z",
  "user":"MDQ6VXNlcjY2ODYxNzk="
 },
 {
  "author_association":"MEMBER",
  "body":"What happened here is that two rules clashed:\r\n\r\n   * If two branches have a different number of items in any entry, they can't be fields of the same records.\r\n   * Record names are assigned using the prefix of a branch name\u2014the string up to a `/` or a `.`.\r\n\r\nIn this case, `\"TauTracksAuxDyn.trackPt\"` and `\"TauTracksAuxDyn.trackEta\"` have the same number of items in all entries and `\"TauTracksAuxDyn.IsPileupTrackQ\"` has a different number of items in just 3 out of the 83932 entries (probably a mistake in the code that made this file). Therefore, they can't be in the same record, but they have the same prefix, so adding the second (`\"TauTracksAuxDyn.IsPileupTrackQ\"`) overwrote the first (`\"TauTracksAuxDyn.trackPt\"` and `\"TauTracksAuxDyn.trackEta\"`).\r\n\r\nTo handle situations like this, PR #369 adds a new rule:\r\n\r\n   * If two branches have a different number of items in any entry but they have the same prefix, the second one has to have an ugly name like `\"jagged1\"`.\r\n\r\nSo this is what we get from your sample:\r\n\r\n```python\r\n>>> import uproot\r\n>>> f = uproot.open(\"uproot-issue-368.root\")\r\n>>> t = f[\"CollectionTree\"]\r\n>>> a = t.arrays([\"TauTracksAuxDyn.trackPt\", \"TauTracksAuxDyn.trackEta\", \"TauTracksAuxDyn.IsPileupTrackQ\"], how=\"zip\")\r\n>>> a.type\r\n83932 * {\"TauTracksAuxDyn\": var * {\"trackPt\": float32, \"trackEta\": float32}, \"jagged1\": var * {\"TauTracksAuxDyn.IsPileupTrackQ\": int32}}\r\n>>> a[\"TauTracksAuxDyn\"].type\r\n83932 * var * {\"trackPt\": float32, \"trackEta\": float32}\r\n>>> a[\"jagged1\"].type\r\n83932 * var * {\"TauTracksAuxDyn.IsPileupTrackQ\": int32}\r\n```\r\n\r\nIf you want to try to find the likely error in the code that made this file, the entries 64567, 66884, and 75191 have 1, 2, and 1 `trackPt`/`trackEta` but 0 `IsPileupTrackQ`s.\r\n\r\n```python\r\n>>> ak.num(t.arrays([\"TauTracksAuxDyn.trackPt\", \"TauTracksAuxDyn.trackEta\", \"TauTracksAuxDyn.IsPileupTrackQ\"])[[64567, 66884, 75191]]).tolist()\r\n[\r\n    {'TauTracksAuxDyn.trackPt': 1, 'TauTracksAuxDyn.trackEta': 1, 'TauTracksAuxDyn.IsPileupTrackQ': 0},\r\n    {'TauTracksAuxDyn.trackPt': 2, 'TauTracksAuxDyn.trackEta': 2, 'TauTracksAuxDyn.IsPileupTrackQ': 0},\r\n    {'TauTracksAuxDyn.trackPt': 1, 'TauTracksAuxDyn.trackEta': 1, 'TauTracksAuxDyn.IsPileupTrackQ': 0}\r\n]\r\n```\r\n\r\nIn all the other thousands of entries, the number of items is the same.\r\n\r\n```python\r\n>>> ak.num(t.arrays([\"TauTracksAuxDyn.trackPt\", \"TauTracksAuxDyn.trackEta\", \"TauTracksAuxDyn.IsPileupTrackQ\"])[:10]).tolist()\r\n[\r\n    {'TauTracksAuxDyn.trackPt': 27, 'TauTracksAuxDyn.trackEta': 27, 'TauTracksAuxDyn.IsPileupTrackQ': 27},\r\n    {'TauTracksAuxDyn.trackPt': 37, 'TauTracksAuxDyn.trackEta': 37, 'TauTracksAuxDyn.IsPileupTrackQ': 37},\r\n    {'TauTracksAuxDyn.trackPt': 44, 'TauTracksAuxDyn.trackEta': 44, 'TauTracksAuxDyn.IsPileupTrackQ': 44},\r\n    {'TauTracksAuxDyn.trackPt': 16, 'TauTracksAuxDyn.trackEta': 16, 'TauTracksAuxDyn.IsPileupTrackQ': 16},\r\n    {'TauTracksAuxDyn.trackPt': 46, 'TauTracksAuxDyn.trackEta': 46, 'TauTracksAuxDyn.IsPileupTrackQ': 46},\r\n    {'TauTracksAuxDyn.trackPt': 32, 'TauTracksAuxDyn.trackEta': 32, 'TauTracksAuxDyn.IsPileupTrackQ': 32},\r\n    {'TauTracksAuxDyn.trackPt': 34, 'TauTracksAuxDyn.trackEta': 34, 'TauTracksAuxDyn.IsPileupTrackQ': 34},\r\n    {'TauTracksAuxDyn.trackPt': 29, 'TauTracksAuxDyn.trackEta': 29, 'TauTracksAuxDyn.IsPileupTrackQ': 29},\r\n    {'TauTracksAuxDyn.trackPt': 11, 'TauTracksAuxDyn.trackEta': 11, 'TauTracksAuxDyn.IsPileupTrackQ': 11},\r\n    {'TauTracksAuxDyn.trackPt': 44, 'TauTracksAuxDyn.trackEta': 44, 'TauTracksAuxDyn.IsPileupTrackQ': 44}\r\n]\r\n```",
  "created_at":"2021-05-25T18:32:44Z",
  "id":848115910,
  "issue":368,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODExNTkxMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T18:32:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski,\r\n\r\nThanks for investigating the problem and from the pointers! Indeed there could be rare cases where the variable IsPileupTrackQ is not filled. I will fix this in the code that made that root file. \r\n\r\nIn any case, I guess it's not a bad idea to have this case handled by uproot if only to not confuse the user with a crash but instead with a different data structure.\r\n\r\nBest,\r\nQuentin",
  "created_at":"2021-05-26T13:00:57Z",
  "id":848749313,
  "issue":368,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODc0OTMxMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-26T13:00:57Z",
  "user":"MDQ6VXNlcjY2ODYxNzk="
 },
 {
  "author_association":"MEMBER",
  "body":"What it was doing before was definitely wrong behavior (a.k.a. \"bug\"). It might still be a bit mysterious why `IsPileupTrackQ` comes out in a different field from `trackPt` and `trackEta` if the user is still unaware that they sometimes have different lengths. One good thing about the array-oriented approach is that this tiny difference in cardinality, which could easily be overlooked in a for-loop analysis, is highly visible in the array-oriented approach. Maybe we need better error messages to explain why that's happening, but at least it doesn't go silently unnoticed.",
  "created_at":"2021-05-26T13:39:21Z",
  "id":848778343,
  "issue":368,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODc3ODM0Mw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-26T13:39:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, on this, I'm stumped. If you're using a ProcessPool, then the Uproot instances are completely independent. (You're running Uproot _inside_ the ProcessPool, not passing ProcessPool as a `decompression_executor`, right? If you use a ProcessPool as a `decompression_executor`, then Uproot would need to copy compressed data to each process and copy uncompressed data back; probably not what you want, since it would be more efficient if they were completely independent. Still, it shouldn't break.)\r\n\r\nHere's a question: does it always fail for the same file? In retrospect, I should have put a try-catch around `zlib.decompress(data)` to add a filename in case of failure: it's [src/uproot/compression.py#280](https://github.com/scikit-hep/uproot4/blob/efc6fd0ca773de30a524a081fff655414f09ee99/src/uproot/compression.py#L280) and you can get the filename from `chunk.source.file_path` if you want to try it. If it's always different files, then the data are being delivered to you in a non-deterministic way, which would most likely show up in a decompression checksum. (The largest blocks of data have to be decompressed, and any bit error would be caught by the checksum with high probability.)\r\n\r\nIf it's always the same file, then that's more perplexing. Then I'd wonder if you get a different chunk of compressed `data` when running a single file as when many other files are also running in parallel.\r\n\r\nBut if it's always the same file and the same raw bytes of `data` and `zlib.decompress` just hits an error when in parallel but not when running alone, then I don't know. (Different version of Python/zlib running on the remote machines when you distribute it than when you run it interactively on the head node?)",
  "created_at":"2021-05-25T19:56:28Z",
  "id":848218188,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODIxODE4OA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T19:56:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I also get the zlib.error when using dask instead of ProcessPoolExecutor. Yeah i'm running Uproot inside ProcessPool. \r\n\r\nIt's does not always fail at the same file, it's a different file each time and only 1 file is failing each time i've tried it. It seems to be when it get's to around ~4600 files processed then one of the root files gives this zlib.error.\r\n\r\nI just running a singular python script with ProcessPoolExecutor so there wouldn't be different versions of Python/zlib running as it should just be separate processes of the same python kernel.\r\nzlib version:\r\n```\r\nzlib                      1.2.11            h516909a_1010    conda-forge\r\n```",
  "created_at":"2021-05-25T20:12:06Z",
  "id":848227562,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODIyNzU2Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T20:12:06Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "author_association":"MEMBER",
  "body":"So running \"on the head node\" is not a different machine from \"in parallel.\" (I thought maybe they were.)\r\n\r\nIf it's a different file each time, that looks like non-deterministic data. At a large enough scale, that would happen, and then the only recourse would be to try/retry. The decompression checksum provides some protection against corrupt data, but there are parts of the file that are not compressed/checksummed. Fortunately, they're small.\r\n\r\nIf you want to try to implement a per-chunk retry, it would be possible with the information that exists in that part of the code. [Chunk objects](https://uproot.readthedocs.io/en/latest/uproot.source.chunk.Chunk.html) have a `chunk.start` and `chunk.stop`, indicating their byte ranges in the file, and `chunk.source` is an object representing the file/HTTP/XRootD source, which has a [chunk method](https://uproot.readthedocs.io/en/latest/uproot.source.chunk.Source.html#chunk) to eagerly extract the chunk again.\r\n\r\n```python\r\nif failed:\r\n    chunk = chunk.source.chunk(chunk.start, chunk.stop)\r\n```\r\n\r\nThe hardest part would be to configure it in such a way that it doesn't retry forever. Chunks don't have a \"number of retries\" attribute.\r\n\r\nAnother thought, before getting to that: if it's a filesystem file (not HTTP or XRootD), have you tried passing\r\n\r\n   * `file_handler=uproot.MemmapSource` or\r\n   * `file_handler=uproot.MultithreadedFileSource` (with `num_workers=1` if need be) or\r\n   * `path=open(filename, \"rb\")` (pass a Python file object so that Uproot doesn't get to decide how to access it)\r\n\r\nas alternative options to [uproot.open](https://uproot.readthedocs.io/en/latest/uproot.source.chunk.Source.html#chunk)? Maybe the default memory-mapping is giving you problems. (Memory-mapping is an OS operation that sees across processes, maybe broken?)",
  "created_at":"2021-05-25T20:42:51Z",
  "id":848246372,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODI0NjM3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T20:42:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Bizarre behaviour; split the 6000 root file list into 2, process them separately. 2nd part of the list gave me a zlib error, so i split that 3000 root file list (I re-ran the 2nd part of the list and again got a zlib error but on a different file; seem like the data was taken on the same day). I split the 2nd root file into 2, process those two root files lists separately but got no zlib errors at all when processing either of them. \r\n",
  "created_at":"2021-05-25T20:50:37Z",
  "id":848255170,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODI1NTE3MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T20:50:37Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "author_association":"NONE",
  "body":"Yeah sorry if i didn't make that clear, I'm just running on a single node. \r\n\r\nI can try implementing a retry X times try catch statement. \r\n\r\nYeah i can try changing the file_hander and see if I stop getting the zlib error.\r\n",
  "created_at":"2021-05-25T20:53:57Z",
  "id":848257125,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODI1NzEyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T20:53:57Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "author_association":"MEMBER",
  "body":"Retrying the `cls.decompress(data, block_uncompressed_bytes)` line (line 280) would do it if it's an error in the ZLIB function, but you'd have to re-load the chunk if it's something giving you the wrong data.\r\n\r\nAlso, you have the old `data` still in scope; you could see if it's giving you different bytestrings with repeated `chunk.source.chunk(chunk.start, chunk.stop)` calls. If so, that's evidence that you could send to NERSC, demonstrating that file-reads aren't deterministic.",
  "created_at":"2021-05-25T21:02:01Z",
  "id":848261574,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODI2MTU3NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T21:02:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"One thing i noticed that using `file_handler=uproot.MultithreadedFileSource` with `num_workers=1` is quite alot slower than `file_handler=uproot.MemmapSource`. \r\n\r\nThis time when running on an interactive node all different methods seemed to fail on the same file this time. I also tried to open the file the same way in a different compute node with the same python kernel and got a zlib.error. (previously i could open the bad zlib error file fine in a separate compute node with uproot)\r\n\r\nAlso tried to open this file in root and now i'm getting a zlib error:\r\n```\r\nroot -l path_to_root_file.root\r\nroot [0]\r\nAttaching file path_to_root_file.root as _file0...\r\nR__unzip: error -5 in inflate (zlib)\r\nInfo in <TFile::GetStreamerInfoList>: cannot find the StreamerInfo record in file path_to_root_file.root\r\n(TFile *) nullptr\r\n```\r\nSo it seems like it's actually just a bad file, I was pretty sure that at the beginning it wasn't the same file everytime that had the zlib error but now it seems like only 1 file is giving the error. ",
  "created_at":"2021-05-25T21:34:12Z",
  "id":848281741,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODI4MTc0MQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-25T21:34:12Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "author_association":"MEMBER",
  "body":"Can you open that file (at this point in the TTree data, or maybe just all of the events) with ROOT?",
  "created_at":"2021-05-26T11:56:11Z",
  "id":848707290,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0ODcwNzI5MA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-26T11:56:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"At the time I couldn't open that particular root file with ROOT, I was getting a zlib error. That particular file has now been replaced at NERSC with an uncorrupted version and now i've been able to run uproot over all the files with ProcessPoolExecutor and Dask. So i'll close the issue as it seems like the issue was just 1 corrupted root file that was causing the problem, thanks",
  "created_at":"2021-05-27T09:18:10Z",
  "id":849477754,
  "issue":370,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0OTQ3Nzc1NA==",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-05-27T09:18:10Z",
  "user":"MDQ6VXNlcjMyNTIyNTk0"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2021-05-27T18:39:13Z",
  "id":849854531,
  "issue":371,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg0OTg1NDUzMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-05-27T18:39:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's probably getting confused by the double underscores. The `how=\"zip\"` parameter makes some assumptions about the naming convention of branches that are looser than NanoAOD's, but not infinitely loose. It has to know what to call the outer and inner fields.\r\n\r\nIt's intended as a quick convenience, so if you're able to construct what you want manually, that's a better option.",
  "created_at":"2021-06-10T19:54:21Z",
  "id":858991425,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODk5MTQyNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T19:54:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"It only gets confused for a handful of files in the entire dataset though.",
  "created_at":"2021-06-10T19:56:40Z",
  "id":858994403,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1ODk5NDQwMw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T19:56:40Z",
  "user":"MDQ6VXNlcjY2ODYxNzk="
 },
 {
  "author_association":"MEMBER",
  "body":"So the branches in question have a different number of items per entry in some files than others. Another heuristic thing about `how=\"zip\"` is that it determines whether it can put branch data in the same list or not based on whether it's possible to line them up. If the alignment and the name portions (up to an underscore or dot) match, they can be joined; if either of those fail, like the alignment, in some files, then they can't be joined for those files.",
  "created_at":"2021-06-10T20:02:20Z",
  "id":859000736,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTAwMDczNg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T20:02:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hmmm..\r\nIn most files, the code behaves as I intended: it zips truth particles together and taus together. However for a few files, it gets confused between truth particles and taus. As soon as I remove the taus or the truth particles, then the zipping works fine again.\r\n\r\nIf your hypothesis about naming confusion is right, I would have expected that when the two containers are of same size (which they should not be), then it would zip then all together, no?",
  "created_at":"2021-06-10T20:07:54Z",
  "id":859007021,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTAwNzAyMQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T20:07:54Z",
  "user":"MDQ6VXNlcjY2ODYxNzk="
 },
 {
  "author_association":"MEMBER",
  "body":"Try looking at `ak.num(tree[\"branch\"].array())` across the branches and files of interest. The merging is based on the names and the number of items in each entry. Since the names are constant across files, it must be the number of items that are varying.\r\n\r\nIf they're _supposed to_ have the same number of items and it turns out they don't, it won't be the first time that `how=\"zip\"` has revealed that. It's particularly sensitive to this kind of alignment issue, which would get more easily overlooked in a for loop.\r\n\r\n(On the other hand, the assumptions it needs to make about naming conventions are probably too convoluted, and it would be better to just require you to zip them manually. Then it would also be easier to see where it fails, since the `arrays` method is doing a lot of other things in the same call.)",
  "created_at":"2021-06-10T20:18:45Z",
  "id":859018496,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTAxODQ5Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T20:18:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I think I'm starting to understand.. in the problematic file,  TruthParticles and Taus have the same number of entries for all events:\r\n\r\n```\r\nTauJets___NominalAuxDyn.pt [2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.eta [2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.phi [2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.m [2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.TATTruthMatch [2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.isRNNMedium [2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.nTracks [2, 2, 2, 2, 2]\r\nTruthParticles___NominalAuxDyn.pdgId [2, 2, 2, 2, 2]\r\nTruthParticles___NominalAuxDyn.px [2, 2, 2, 2, 2]\r\nTruthParticles___NominalAuxDyn.py [2, 2, 2, 2, 2]\r\nTruthParticles___NominalAuxDyn.pz [2, 2, 2, 2, 2]\r\nTruthParticles___NominalAuxDyn.e [2, 2, 2, 2, 2]\r\n```\r\n\r\nIn a healthy file, it is not the case:\r\n\r\n```\r\nTauJets___NominalAuxDyn.pt [3, 2, 2, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, ... 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.eta [3, 2, 2, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, ... 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.phi [3, 2, 2, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, ... 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.m [3, 2, 2, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, ... 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.TATTruthMatch [3, 2, 2, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, ... 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.isRNNMedium [3, 2, 2, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, ... 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\r\nTauJets___NominalAuxDyn.nTracks [3, 2, 2, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, ... 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\r\nTruthParticles___NominalAuxDyn.pdgId [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ... 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2]\r\nTruthParticles___NominalAuxDyn.px [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ... 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2]\r\nTruthParticles___NominalAuxDyn.py [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ... 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2]\r\nTruthParticles___NominalAuxDyn.pz [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ... 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2]\r\nTruthParticles___NominalAuxDyn.e [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ... 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2]\r\n```\r\n\r\nSo it looks like the fact that TruthParticles and TauJets have the same length by coincidence is throwing uproot off and it puts all the fields in the 'jagged0' array.",
  "created_at":"2021-06-10T21:09:47Z",
  "id":859070915,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg1OTA3MDkxNQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-10T21:11:18Z",
  "user":"MDQ6VXNlcjY2ODYxNzk="
 },
 {
  "author_association":"MEMBER",
  "body":"I think this is resolved, right? Uproot's zipping is determined by whether the arrays fit together or not, and your data have rare cases that don't fit together (maybe that's reason enough for me to remove the feature\u2014it makes the data type depend on the data values, which is usually undesirable).",
  "created_at":"2021-06-21T17:10:59Z",
  "id":865204008,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTIwNDAwOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-21T17:10:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski,\r\n\r\nYes I understood the source of the 'error'. I agree with you that there is no perfect solution. Ultimately the 'jagged0' array is not a bad outcome but one needs to be aware of that feature in the code to understand.\r\n\r\nA possible improvement (but maybe it is already there and I missed it) would be to be able to retrieve the decision on what fields where zipped together (at least in verbose mode) and / or to be able to force uproot to zip as we intend it.\r\n\r\nBest,\r\nQuentin \r\n",
  "created_at":"2021-06-21T21:34:48Z",
  "id":865360738,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTM2MDczOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-21T21:34:48Z",
  "user":"MDQ6VXNlcjY2ODYxNzk="
 },
 {
  "author_association":"MEMBER",
  "body":"To have a \"verbose mode\" and to be able to force it (somehow, by changing the numbers of items in lists) argues for this to not be a convenience option in the array-fetching function, but to make it a separate step, so that it can be more fully parameterized. Already, this step can be performed manually in Awkward Array, maybe the option in the array-fetching function should be deprecated with a link to a tutorial on how to do that.\r\n\r\nFor now, though, there isn't a bug in the feature, so I'll close this.",
  "created_at":"2021-06-22T11:29:58Z",
  "id":865903917,
  "issue":377,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg2NTkwMzkxNw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-22T11:29:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I know what happened: the `uproot.behavior_of` function tries to pass template specializations to behavior classes, and TMatrix/TVector are templated. However, no specializations for these classes have been written in Python, so it was trying to pass that specialization to `None`.\r\n\r\nI've moved the function (as I've been wanting to do for some time) and fixed the bug, probably, but I don't have any examples of files with a TMatrix or TVector in it. I could add such a test to the PR if you give me a few lines of PyROOT that produces one (I'm guessing that this is simpler than adding a real-data file to scikit-hep-testdata). Otherwise, could you test PR #384 with your file before I merge it and make a new release for it? Thanks!\r\n\r\n(My preference is to add a test based on a small file made dynamically by PyROOT, if you have code for that offhand. Then the test will stay in the codebase but I won't have to mess with scikit-hep-testdata. ROOT is already a dependency for testing.)",
  "created_at":"2021-06-29T17:59:42Z",
  "id":870801692,
  "issue":383,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MDgwMTY5Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-29T17:59:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@nfoppiani does this reproduce the bug for you?\r\n```python3\r\nimport ROOT\r\nimport uproot\r\n\r\npath = \"/tmp/test-file.root\"\r\nkey = \"mat\"\r\n\r\n# Write test file\r\nf = ROOT.TFile(path, \"RECREATE\")\r\nmat = ROOT.TMatrixD(3, 3)\r\nmat[0, 1] = 4\r\nmat[1, 0] = 8\r\nmat[2, 2] = 3\r\nmat.Write(key\")\r\nf.Close()\r\n\r\n    \r\n# Read test file\r\nwith uproot.open(path) as f:\r\n    mat = f[key]\r\n```",
  "created_at":"2021-06-30T11:06:25Z",
  "id":871307287,
  "issue":383,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTMwNzI4Nw==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T11:08:30Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"If the name of this class is \"TMatrixD\", then the template specialization that's breaking must be in a superclass. I'll try using this as a test, thanks! (If it has `cls is None` but `specialization is not None`, than it's a reproducer.)",
  "created_at":"2021-06-30T12:08:22Z",
  "id":871347129,
  "issue":383,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTM0NzEyOQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T12:08:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"> (If it has cls is None but specialization is not None, than it's a reproducer.)\r\n\r\nThis should be the case - we don't define a behaviour for `TMatrix` AFAICT, so the `cls` should be `None`.",
  "created_at":"2021-06-30T12:10:11Z",
  "id":871348272,
  "issue":383,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTM0ODI3Mg==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T12:10:11Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"NONE",
  "body":"Thanks, @agoose77. Indeed I obtain the same error.\r\nThe pyROOT object `mat` in your code is of type `<cppyy.gbl.TMatrixT<double> object at 0x55e22a3b73e0>`, which is the same as the one I was encountering before, aside from the \"specialization\" to double.",
  "created_at":"2021-06-30T13:08:23Z",
  "id":871389465,
  "issue":383,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTM4OTQ2NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T13:08:23Z",
  "user":"MDQ6VXNlcjEzNDQ4MTM0"
 },
 {
  "author_association":"NONE",
  "body":"> Otherwise, could you test PR #384 with your file before I merge it and make a new release for it? Thanks!\r\n\r\n@jpivarski I tested PR #384 with my file and the example provided by @agoose77: it worked in both cases, and I could access the data by calling `.member(\"fElements\")`.\r\n\r\nI think the dynamically generated example is good enough; otherwise, I can give you the file I was using, but it adds more complication than there needs to be, in my opinion.",
  "created_at":"2021-06-30T13:27:31Z",
  "id":871404276,
  "issue":383,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTQwNDI3Ng==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T13:27:31Z",
  "user":"MDQ6VXNlcjEzNDQ4MTM0"
 },
 {
  "author_association":"MEMBER",
  "body":"The fix is in [Uproot 4.0.11](https://pypi.org/project/uproot/4.0.11/).",
  "created_at":"2021-06-30T18:22:04Z",
  "id":871629130,
  "issue":383,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3MTYyOTEzMA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-06-30T18:22:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Hopefully #388 fixes this. The trick is that old versions of XRootD need to use one default (because of bugs in XRootD) and new versions need to use a different default, which is something that we don't want to have happen automatically because that would be too magical, so we make the issue visible to the user as a warning, which has to be raised when the default is set, i.e. at Uproot import time. This does not import XRootD; it is based on which version is installed. You must have an old version of XRootD _installed_, but you're not _using_ it, so you should not get the warning.\r\n\r\nPR #388 makes the `uproot.open.defaults` dict into a slightly smarter dict that only gets/sets the `xrootd_handler` option if the user explicitly asks for it or assigns it or if ReadOnlyFile accesses it because a URL schema is \"root://\" (1d38083cb2e089d7d9b97ce0c027511d16e7f092). It should work regardless of whether the URL is in `uproot.open`, `uproot.iterate`, or `uproot.lazy`, though it will be tricky to test because the full behavior (when it shows the warning and when it doesn't) depends on whether XRootD is installed on one's system and which version it has, which is not easy to swap out in a pytest test.\r\n\r\nAs a first step, does the PR branch make the warning go away on your system? (The \"has old XRootD installed; doesn't use it\" case.)",
  "created_at":"2021-07-06T12:54:28Z",
  "id":874733802,
  "issue":387,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NDczMzgwMg==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-06T12:54:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I can confirm that this patch removes the warning for me, thanks for the super quick response!",
  "created_at":"2021-07-07T12:44:45Z",
  "id":875571964,
  "issue":388,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTU3MTk2NA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T12:44:45Z",
  "user":"MDQ6VXNlcjI2MzE1ODY="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, great! I'll merge it into main now.",
  "created_at":"2021-07-07T12:59:23Z",
  "id":875581655,
  "issue":388,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3NTU4MTY1NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-07T12:59:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I bet I can change it through the web interface...",
  "created_at":"2021-07-14T14:13:49Z",
  "id":879929338,
  "issue":395,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTkyOTMzOA==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-14T14:13:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks, @dcervenkov and @chrisburr! If there are no complaints with this version, I'll be merging it once it passes tests.",
  "created_at":"2021-07-14T14:16:59Z",
  "id":879932048,
  "issue":395,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTkzMjA0OA==",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-14T14:16:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Definitely agree that the `TypeError` exception is a better approach. Thanks for the quick reactions.",
  "created_at":"2021-07-14T14:20:45Z",
  "id":879934995,
  "issue":395,
  "node_id":"MDEyOklzc3VlQ29tbWVudDg3OTkzNDk5NQ==",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-14T14:20:45Z",
  "user":"MDQ6VXNlcjIzMDUyMDU0"
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2021-07-17T02:08:17Z",
  "id":881799509,
  "issue":396,
  "node_id":"IC_kwDOD6Q_ss40jzFV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-17T02:08:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"First bit of triage: renaming the file as issue-398.root...\r\n\r\n```python\r\n>>> import uproot\r\n>>> tree = uproot.open(\"issue-398.root:orange\")\r\n>>> tree.show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\norange               | struct {int32_t Evtak... | AsDtype(\"[('Evtake_iwant', ...\r\n>>> tree[\"orange\"].interpretation\r\nAsDtype(\"[('Evtake_iwant', '>i4'), ('Mc_x', '>f4'), ('Mc_y', '>f4'), ('Mc_q2', '>f4'), ('Mc_x_cr', '>f4'), ('Mc_q2_cr', '>f4'), ('Sierror', '>i4'), ('Sincand', '>i4'), ('Siprob', '>f4'), ('Sipos', '>f4'), ('Sipt', '>f4'), ('Sidca', '>f4'), ('Sitrkp', '>f4'), ('Siein', '>f4'), ('Sienin', '>f4'), ('Zvtx', '>f4'), ('Sith', '>f4'), ('Siecorr', '>f4'), ('Sizuhmom', '>f4'), ('Sicchmom', '>f4'), ('Siccempz', '>f4'), ('Sixel', '>f4'), ('Siq2el', '>f4'), ('Siyel', '>f4'), ('Sixjb', '>f4'), ('Siq2jb', '>f4'), ('Siyjb', '>f4'), ('Sixda', '>f4'), ('Siq2da', '>f4'), ('Siyda', '>f4')]\")\r\n>>> len(tree[\"orange\"].interpretation.from_dtype)\r\n30\r\n>>> tree[\"orange\"].interpretation.from_dtype.itemsize\r\n120\r\n>>> tree[\"orange\"].title\r\n'Evtake_iwant/I:Mc_x/F:Mc_y/F:Mc_q2/F:Mc_x_cr/F:Mc_q2_cr/F:Sierror/I:Sincand/I:Siprob/F:Sipos[3]/F:Sipt/F:Sidca/F:Sitrkp/F:Siein/F:Sienin/F:Zvtx/F:Sith/F:Siecorr[3]/F:Sizuhmom[4]/F:Sicchmom[4]/F:Siccempz/F:Sixel/F:Siq2el/F:Siyel/F:Sixjb/F:Siq2jb/F:Siyjb/F:Sixda/F:Siq2da/F:Siyda/F'\r\n```\r\n\r\nIt thinks the correct interpretation is one flat struct per event with 30 fields, all of which are 4 bytes (ints and floats), and the total itemsize for a single struct is 120 bytes. That interpretation came from the branch title, which is the only way I know of to find a leaf-list like this. (The original struct is not saved as a streamer; I didn't find any user-defined classes in `tree.file.show_streamers()`.)\r\n\r\n```python\r\n>>> tree[\"orange\"].basket(0).num_entries\r\n71\r\n>>> tree[\"orange\"].basket(0).data\r\narray([  0,   0,   0, ...,  85,  91, 251], dtype=uint8)\r\n>>> len(tree[\"orange\"].basket(0).data)\r\n11360\r\n>>> 11360 / 71\r\n160.0\r\n```\r\n\r\nThe first basket has 71 entries and 11360 bytes of data, so we expect each entry to have 160 bytes. 160 != 120, so that's why we get the following error message.\r\n\r\n```\r\n>>> tree[\"orange\"].array()\r\nTraceback (most recent call last):\r\n  File \"/home/jpivarski/irishep/uproot4/src/uproot/interpretation/numerical.py\", line 328, in basket_array\r\n    output = data.view(dtype).reshape((-1,) + shape)\r\nValueError: When changing to a larger dtype, its size must be a divisor of the total size in bytes of the last axis of the array.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/uproot4/src/uproot/behaviors/TBranch.py\", line 2079, in array\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"/home/jpivarski/irishep/uproot4/src/uproot/behaviors/TBranch.py\", line 3479, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"/home/jpivarski/irishep/uproot4/src/uproot/source/futures.py\", line 46, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"/home/jpivarski/irishep/uproot4/src/uproot/behaviors/TBranch.py\", line 3423, in basket_to_array\r\n    basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n  File \"/home/jpivarski/irishep/uproot4/src/uproot/interpretation/numerical.py\", line 330, in basket_array\r\n    raise ValueError(\r\nValueError: basket 0 in tree/branch /orange;1:orange has the wrong number of bytes (11360) for interpretation AsDtype(\"[('Evtake_iwant', '>i4'), ('Mc_x', '>f4'), ('Mc_y', '>f4'), ('Mc_q2', '>f4'), ('Mc_x_cr', '>f4'), ('Mc_q2_cr', '>f4'), ('Sierror', '>i4'), ('Sincand', '>i4'), ('Siprob', '>f4'), ('Sipos', '>f4'), ('Sipt', '>f4'), ('Sidca', '>f4'), ('Sitrkp', '>f4'), ('Siein', '>f4'), ('Sienin', '>f4'), ('Zvtx', '>f4'), ('Sith', '>f4'), ('Siecorr', '>f4'), ('Sizuhmom', '>f4'), ('Sicchmom', '>f4'), ('Siccempz', '>f4'), ('Sixel', '>f4'), ('Siq2el', '>f4'), ('Siyel', '>f4'), ('Sixjb', '>f4'), ('Siq2jb', '>f4'), ('Siyjb', '>f4'), ('Sixda', '>f4'), ('Siq2da', '>f4'), ('Siyda', '>f4')]\")\r\nin file issue-398.root\r\n```\r\n\r\n11360 _is_ the wrong number of bytes for 71 instances of a 120 byte struct.\r\n\r\nPerhaps the structs are padded? (If so, then I didn't know ROOT did that, or under what circumstances it would write padded data to disk.) I'll try adding 160 \u2012 120 = 40 bytes of \"junk\" to the interpretation.\r\n\r\n```python\r\n>>> fields = [(k, v[0]) for k, v in tree[\"orange\"].interpretation.from_dtype.fields.items()]\r\n>>> fields.append((\"junk\", \"S40\"))\r\n>>> new_interpretation = uproot.AsDtype(np.dtype(fields))\r\n>>> new_interpretation.from_dtype.itemsize\r\n160\r\n>>> array = tree[\"orange\"].array(new_interpretation)\r\n>>> array\r\n<Array [{Evtake_iwant: 1, ... ] type='71 * {\"Evtake_iwant\": int32, \"Mc_x\": float...'>\r\n```\r\n\r\nIt was successfully read (merely because the number of bytes in the data is an integer multiple of the interpretation of one entry). Let's check individual fields, to see that they're not junk:\r\n\r\n```python\r\n>>> array[\"Evtake_iwant\"]\r\n<Array [1, 2, 2, 1, 1, 1, ... 1, 1, 1, 2, 1, 1] type='71 * int32'>\r\n>>> array[\"Mc_x\"]\r\n<Array [0.0593, 0.00322, ... 0.0788, 0.00466] type='71 * float32'>\r\n>>> array[\"Mc_y\"]\r\n<Array [0.0688, 0.32, 0.593, ... 0.415, 0.215] type='71 * float32'>\r\n>>> array[\"Siyda\"]\r\n<Array [97.7, 50.5, 98.3, ... 93.5, 87.1, 110] type='71 * float32'>\r\n```\r\n\r\nLooks good. Now the padding field (the one named \"junk\"), what's in that?\r\n\r\n```python\r\n>>> array[\"junk\", 0]\r\nb'@\\x91%\\x1cDy\\xf9\\x9aC\\xe19\\xa9\\xbc6\\xb6+=N\\x12^C\\xe0\\xc7/=\\xb4p*=C\\xe4\\x14C\\xcc\\x014=\\xacF\\n'\r\n>>> np.frombuffer(array[\"junk\", 0], np.uint8)\r\narray([ 64, 145,  37,  28,  68, 121, 249, 154,  67, 225,  57, 169, 188,\r\n        54, 182,  43,  61,  78,  18,  94,  67, 224, 199,  47,  61, 180,\r\n       112,  42,  61,  67, 228,  20,  67, 204,   1,  52,  61, 172,  70,\r\n        10], dtype=uint8)\r\n>>> np.frombuffer(array[\"junk\", 1], np.uint8)\r\narray([ 65,  91, 250, 221,  59,  82, 103,   0,  66, 209,  99, 231,  62,\r\n       164, 160,  73,  59,  39,  35, 146,  66, 145,  56, 142,  62, 143,\r\n       186, 159,  59,  78, 165,  82,  66, 208,  44, 104,  62, 166, 165,\r\n        16], dtype=uint8)\r\n>>> np.frombuffer(array[\"junk\", 2], np.uint8)\r\narray([ 65, 170,  61,  51,  59,  21,  20, 114,  67,   6, 147, 110,  63,\r\n        21,  83, 246,  58, 243,  49, 148,  66, 218, 226, 212,  63,  20,\r\n       227,  61,  59,   7,  29, 170,  66, 253, 217,  69,  63,  27, 100,\r\n       156], dtype=uint8)\r\n```\r\n\r\nI don't know what to make of that. It might actually be \"junk\", random bytes copied from memory into the ROOT file because of struct-padding.\r\n\r\nOn a hunch (those numbers in the high 60's every 4th byte look like the sign bits of floats to me), I reinterpreted the 40 bytes of so-called \"junk\" as 10 single-precision floating point numbers.\r\n\r\n```python\r\n>>> fields = [(k, v[0]) for k, v in tree[\"orange\"].interpretation.from_dtype.fields.items()]\r\n>>> fields.extend([(f\"junk{i}\", \">f4\") for i in range(10)])\r\n>>> new_interpretation = uproot.AsDtype(np.dtype(fields))\r\n>>> new_interpretation.from_dtype.itemsize\r\n160\r\n>>> array = tree[\"orange\"].array(new_interpretation)\r\n>>> array[\"junk0\"]\r\n<Array [4.54, 13.7, 21.3, ... 13.4, 21, 5.5] type='71 * float32'>\r\n>>> array[\"junk1\"]\r\n<Array [1e+03, 0.00321, ... 0.0555, 0.00531] type='71 * float32'>\r\n>>> array[\"junk2\"]\r\n<Array [450, 105, 135, ... 76.9, 2.83e+03, 104] type='71 * float32'>\r\n>>> array[\"junk3\"]\r\n<Array [-0.0112, 0.322, ... 0.502, 0.193] type='71 * float32'>\r\n>>> array[\"junk4\"]\r\n<Array [0.0503, 0.00255, ... 0.0605, 0.00642] type='71 * float32'>\r\n>>> array[\"junk5\"]\r\n<Array [450, 72.6, 109, ... 198, 2.55e+03, 169] type='71 * float32'>\r\n>>> array[\"junk6\"]\r\n<Array [0.0881, 0.281, 0.582, ... 0.415, 0.259] type='71 * float32'>\r\n>>> array[\"junk7\"]\r\n<Array [0.0478, 0.00315, ... 0.069, 0.00483] type='71 * float32'>\r\n>>> array[\"junk8\"]\r\n<Array [408, 104, 127, ... 116, 3.13e+03, 102] type='71 * float32'>\r\n>>> array[\"junk9\"]\r\n<Array [0.0841, 0.325, 0.607, ... 0.448, 0.208] type='71 * float32'>\r\n```\r\n\r\nMost of these values have consistent orders of magnitude (apart from an occasional ~10\u00b3), which is suggestive of real data. That's not inconsistent with it being padding: perhaps the region of memory that the data in your file-writing process eventually went into was previously used to hold other meaningful data, and it just wasn't cleared before being dumped into the file. (In which case, I'm getting a raw view into what your writing-process was doing just before it wrote the data\u2014good for scraping passwords...)\r\n\r\nSo, my questions to you:\r\n\r\n   1. Do you expect only the 30 fields in your data? (In which case, I should be padding from 120 bytes to 160 bytes, ignoring the last 40.) Or do you expect 40 fields in your data? (In which case, I have to find out where that information is stored in the ROOT file, because they're not in the branch title.)\r\n   2. Are the above numbers meaningful in your dataset? Do they look like some 10 fields whose names don't appear in the dtype? Or do they look like the previous entry's values?\r\n\r\nIncidentally, you can get a Pandas dataframe from this `array`:\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> ak.to_pandas(array)\r\n       Evtake_iwant      Mc_x      Mc_y  ...     junk7        junk8     junk9\r\nentry                                    ...                                 \r\n0                 1  0.059289  0.068799  ...  0.047825   408.009399  0.084118\r\n1                 2  0.003217  0.320340  ...  0.003153   104.086731  0.325478\r\n2                 2  0.002187  0.592977  ...  0.002062   126.924355  0.607004\r\n3                 1  0.006302  0.678945  ...  0.006119   425.734497  0.686061\r\n4                 1  0.010722  0.228303  ...  0.008879   239.647629  0.266133\r\n...             ...       ...       ...  ...       ...          ...       ...\r\n66                1  0.009806  0.132353  ...  0.009608   134.701599  0.138226\r\n67                1  0.011639  0.677099  ...  0.012813   849.059021  0.653374\r\n68                2  0.003481  0.283885  ...  0.005565   116.068558  0.205644\r\n69                1  0.078797  0.415351  ...  0.069017  3133.369873  0.447643\r\n70                1  0.004663  0.214653  ...  0.004830   102.058426  0.208359\r\n\r\n[71 rows x 40 columns]\r\n```\r\n\r\nBut I'd like to know the answers to the above questions so that I can fix the interpretation. Thanks!",
  "created_at":"2021-07-28T16:14:40Z",
  "id":888439326,
  "issue":398,
  "node_id":"IC_kwDOD6Q_ss409IIe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-28T16:14:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski ,\r\nsorry for not including the structure of the data.  But it is actually in the file and can be extracted as \r\n\r\n```\r\nTChain* C = new TChain(\"orange\")\r\nC->Add(\"test.root.txt\")\r\nC->MakeClass(\"somename\")\r\n```\r\nThe  produced header will contain the structure of tree \"as ROOT understands it\".\r\n\r\n```\r\n   // Declaration of leaf types\r\n   Int_t           orange_Evtake_iwant;\r\n   Float_t         orange_Mc_x;\r\n   Float_t         orange_Mc_y;\r\n   Float_t         orange_Mc_q2;\r\n   Float_t         orange_Mc_x_cr;\r\n   Float_t         orange_Mc_q2_cr;\r\n   Int_t           orange_Sierror;\r\n   Int_t           orange_Sincand;\r\n   Float_t         orange_Siprob;\r\n   Float_t         orange_Sipos[3];\r\n   Float_t         orange_Sipt;\r\n   Float_t         orange_Sidca;\r\n   Float_t         orange_Sitrkp;\r\n   Float_t         orange_Siein;\r\n   Float_t         orange_Sienin;\r\n   Float_t         orange_Zvtx;\r\n   Float_t         orange_Sith;\r\n   Float_t         orange_Siecorr[3];\r\n   Float_t         orange_Sizuhmom[4];\r\n   Float_t         orange_Sicchmom[4];\r\n   Float_t         orange_Siccempz;\r\n   Float_t         orange_Sixel;\r\n   Float_t         orange_Siq2el;\r\n   Float_t         orange_Siyel;\r\n   Float_t         orange_Sixjb;\r\n   Float_t         orange_Siq2jb;\r\n   Float_t         orange_Siyjb;\r\n   Float_t         orange_Sixda;\r\n   Float_t         orange_Siq2da;\r\n   Float_t         orange_Siyda;\r\n```\r\n\r\nThe file was created in a standard  way with branch specifiers separated with semicolons.\r\nPlease tell if more information is needed.\r\n\r\n\r\nBest regards,\r\n\r\nAndrii\r\n\r\n",
  "created_at":"2021-07-28T17:41:53Z",
  "id":888496390,
  "issue":398,
  "node_id":"IC_kwDOD6Q_ss409WEG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-28T17:41:53Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "author_association":"MEMBER",
  "body":"About an hour after sending my message, I thought, \"I should try opening that in ROOT.\"\r\n\r\nHowever, the error is evident in your print-out: four of the fields are arrays. That was also in the branch title that _I_ printed out, but didn't look closely enough to see. What I considered \"10 missing fields\" was the fact that `Sipos[3]`, `Siecorr[3]`, `Sizuhmom[4]`, and `Sicchmom[4]` were each counted once: `2 + 2 + 3 + 3 = 10`.\r\n\r\nSo ROOT is _not_ padding structs when it writes them out; I just need to add array interpretations to four of the fields. I think I know where this is being missed.",
  "created_at":"2021-07-28T18:06:24Z",
  "id":888512936,
  "issue":398,
  "node_id":"IC_kwDOD6Q_ss409aGo",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-28T18:06:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski ,\r\n1) wow, that was fast.\r\n2) Thank you!\r\n3) At some point I was looking at the code of TTree::MakeClass, TTree:MakeSelector, etc. to understand to the types of leaves are detected. \r\nMaybe that would be useful for you at some point.\r\n\r\n\r\nBest regards,\r\n\r\nAndrii",
  "created_at":"2021-07-28T21:17:59Z",
  "id":888628069,
  "issue":398,
  "node_id":"IC_kwDOD6Q_ss4092Nl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-28T21:17:59Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski ,\r\n\r\nif you are interested, I actually can provide a background for this issue, why there structures are important, etc. Either here or in a separate mail.\r\n\r\nBest regards,\r\n\r\nAndrii",
  "created_at":"2021-07-30T10:26:00Z",
  "id":889798704,
  "issue":400,
  "node_id":"IC_kwDOD6Q_ss41CUAw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T10:26:00Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm not unconvinced that these data structures are important, though it is new to me to see data with this logical structure in the TLeaves of a single TBranch. The way I've been mapping ROOT structures onto arrays is to take TBranches to be separate NumPy arrays and TLeaves to be components of a single [structured array](https://numpy.org/doc/stable/user/basics.rec.html), which has the same contiguousness properties (TBranches are separate; TLeaves of a single TBranch are contiguous in memory, just like the fields of a structured array).\r\n\r\nIn the previous case, #398, some of those TLeaves has multiple components, though a fixed number of them. I tried mapping that directly into NumPy, and I was surprised that NumPy is able to deal with this (maybe it's a new thing\u2014I don't remember this being possible in the past):\r\n\r\n```python\r\n>>> np.array([(1, [1.1, 2.2, 3.3]), (10, [10.1, 20.2, 30.3])],\r\n...          dtype=[(\"one\", np.int32), (\"three\", np.float32, (3,))])\r\narray([( 1, [ 1.1,  2.2,  3.3]), (10, [10.1, 20.2, 30.3])],\r\n      dtype=[('one', '<i4'), ('three', '<f4', (3,))])\r\n```\r\n\r\nWith a fixed number of components, it's logically possible; I just didn't realize that NumPy would accept a mixture of scalar and array types in a structured dtype.\r\n\r\nThis new example goes beyond that, though: the fact that `Trk_ntracks` is variable means that each object has a different size on disk. (Your C struct always has a memory footprint of 200 tracks, but the objects in the ROOT file are shrink-wrapped to the actual number of tracks. ROOT's `MakeClass` is evidently scanning the data to find the largest entry, length 109, but that won't work if you `MakeClass` on one file and try to use that generated code to read another file written by the same script.)\r\n\r\nUproot does not look ahead to find the largest object and then pad data. When people make files with this kind of structure in split TBranches or unsplit TBranchElements, Uproot produces an Awkward Array with arbitrary jaggedness (offsets counter, rather than padding). Such an array can be converted into a Pandas DataFrame if there's only one variable list length (`Trk_ntracks`), by expressing the list boundaries in a MultiIndex. If there's more than one (e.g. `Trk_ntracks` and `Sh_nshowers`), then it has to be converted into more than one DataFrame because a DataFrame can have only one index. I just tried modifying your generating code, and it looks like it's not possible to have more than one counter variable.\r\n\r\nI'll need to write some custom code for this because even the closest equivalent, unsplit TBranchElements, have a \"counter\" variable that is in a specified position relative to the repeated data, such as the `size` of a `std::vector` immediately before the repeated elements of the vector. The `Trk_ntracks` can come anywhere before the `Trk_px[Trk_ntracks]`, such as in\r\n\r\n```\r\nTrk_ntracks/I:Evtake_iwant/I:Mc_x/F:Mc_y/F:Trk_px[Trk_ntracks]/F:Trk_py[Trk_ntracks]/F:Mc_q2/F\r\n^^^^^^^^^^^^^                              ^^^^^^^^^^^^^^^^^^^^^\r\n```\r\n\r\nso the generated code will have to have long-distance memory. Through experimentation with your snippet, I found that it is _not_ possible to put the counter after the counted variable:\r\n\r\n```\r\nError in <TBranch::TBranch>: Illegal leaf: orange/Evtake_iwant/I:Mc_x/F:Mc_y/F:Trk_px[Trk_ntracks]/F:Trk_py[Trk_ntracks]/F:Trk_ntracks/I:Mc_q2/F. If this is a variable size C array it's possible that the branch holding the size is not available.\r\n```\r\n\r\nand it doesn't seem possible to have two counters. (I get segmentation faults when I try it.)",
  "created_at":"2021-07-30T12:53:21Z",
  "id":889872446,
  "issue":400,
  "node_id":"IC_kwDOD6Q_ss41CmA-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T12:53:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski ,\r\n\r\n\r\n> MakeClass is evidently scanning the data to find the largest entry\r\n\r\nNot always, I think. I've send you a  small ROOT file from a dataset of interest and somehow ROOT is able to deduce the \"correct\" \r\nmaximal size of array.\r\n\r\n> I found that it is not possible to put the counter after the counted variable\r\n\r\nIf I remember correcly -- yes.   But  a) I might be wrong. b) Practically, for any structues of variable size the size of variable size arrays precedes the array.\r\n\r\n> it doesn't seem possible to have two counters.\r\n\r\nI've tried  with the file I've send you\r\n\r\n```\r\nimport pandas as pd\r\nimport uproot\r\nuproot.default_library=\"pd\"\r\ndf=uproot.open(\"short.root\")['orange']\r\ndf.show(interpretation_width=1500)\r\nprint( df.keys() )\r\narr = df.arrays(library=\"pd\")\r\nprint(dir(arr))\r\nprint(arr)\r\nprint(arr[1][\"Mc_x\"])\r\ndf_uproot = pd.DataFrame(arr[1])[\"Mc_x\"]\r\n```\r\n\r\nand it looks the content is in place, but the structure looks wrong.\r\n\r\n\r\nBest regards,\r\n\r\nAndrii\r\n\r\n",
  "created_at":"2021-07-30T14:28:50Z",
  "id":889931168,
  "issue":400,
  "node_id":"IC_kwDOD6Q_ss41C0Wg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T14:28:50Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "author_association":"MEMBER",
  "body":"> > MakeClass is evidently scanning the data to find the largest entry\r\n> \r\n> Not always, I think. I've send you a small ROOT file from a dataset of interest and somehow ROOT is able to deduce the \"correct\"\r\n> maximal size of array.\r\n\r\n`MakeClass` came up with the number 109, but another file like this one could have more tracks. (Your `max_tracks=200`, but ROOT can't know that.) It must be scanning the file to get this number, and then the C++ code it generates won't be able to handle files with more tracks than this. That's just a general problem with padding.\r\n\r\n> > I found that it is not possible to put the counter after the counted variable\r\n> \r\n> If I remember correcly -- yes. But a) I might be wrong. b) Practically, for any structues of variable size the size of variable size arrays precedes the array.\r\n\r\nFor reading it back, it is logically necessary for the counter to precede the counted data. If the counter were after the variable-size data, then we'd need to get to the counter first to find out how much variable-sized data to read, but we wouldn't be able to skip past the variable-sized data because we don't know how big it is. (Unless the variable-sized data ends with a terminator, like `\\x00` at the end of a C string, but that would introduce a value that can't appear in the middle of the variable-sized data\u2014C strings can't contain `\\x00` in the middle. That could be resolved by quoting the inexpressible values, but that gets even more complicated; I doubt ROOT does that.) Here, I was just making sure that ROOT can't write data with the counter after the counted data, so that the reader I'll be coding up can assume that.\r\n\r\nThat's also why I was looking into the question of whether there can be more than one counter, to know how general the reader needs to be. Although the counters must be before the counted data (unless ROOT introduces a terminator and quoting, which I doubt), there's nothing wrong in principle with having multiple counters. I'll allow it in the reader, though I haven't been able to make ROOT write data like that.",
  "created_at":"2021-07-30T14:49:30Z",
  "id":889944153,
  "issue":400,
  "node_id":"IC_kwDOD6Q_ss41C3hZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T14:49:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"FYI, you (@jpivarski) can have a look at `groot` (which seems to be able to read that file correctly) for inspiration.",
  "created_at":"2021-08-02T08:46:11Z",
  "id":890845980,
  "issue":400,
  "node_id":"IC_kwDOD6Q_ss41GTsc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-02T08:46:11Z",
  "user":"MDQ6VXNlcjEwNzA5MjA="
 },
 {
  "author_association":"MEMBER",
  "body":"If you want to manage that, go ahead.\r\n\r\nOne question, though: whenever there's a new version of Uproot, will you need to manually update the RedHat packages? That could be a lot of work. I usually don't install Python packages with my Linux distribution's package manager because Python packages update more frequently than the OS and large applications. As long as pip (or conda, but more likely pip) is available in EPEL + Fedora, why would those users want to install Uproot from the distro package?\r\n\r\nAlso, what kinds of problems could EPEL + Fedora testing discover that our continuous integration wouldn't catch? Uproot (intentionally) doesn't depend on much. New versions of Python or NumPy? (We test for that.) I'm not being antagonistic\u2014I'm open to the possibility that this would be helpful\u2014but I can't answer this question myself.",
  "created_at":"2021-07-30T13:08:17Z",
  "id":889881431,
  "issue":401,
  "node_id":"IC_kwDOD6Q_ss41CoNX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T13:08:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":">  will you need to manually update the RedHat packages\r\n\r\nOnly the spec file. When there are no major changes and the package intentionally does not depend on many things \r\nthe whole update is just an update of version string.\r\nHere is a real world example https://src.fedoraproject.org/rpms/python-wrapt/blob/rawhide/f/python-wrapt.spec\r\n\r\n> why would those users want to install Uproot from the distro package\r\n\r\nThere are few advantages, but those do exist.\r\n\r\na) Repositories offer a lot of stability/reproduciablility and assure that \"everything is compatible with everything\".\r\nA good example could be  installing root and uproot4. It is easy to get root from the package manager e.g. on Fedora.\r\nIn the full installation root has numpy as a dependence, if I remember correctly.\r\nInstalling uproot via pip would bring its own version of numpy.\r\nb) A system-wide installation.\r\n\r\n\r\n> New versions of Python or NumPy?\r\n\r\nThat might be the simplest example. Sounds trivial, but they do that really good and on multiple less popular platforms as aarch64 or s390.\r\n\r\nBest regards,\r\n\r\nAndrii\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
  "created_at":"2021-07-30T14:55:34Z",
  "id":889948082,
  "issue":401,
  "node_id":"IC_kwDOD6Q_ss41C4ey",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T14:55:34Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, just be aware that you're committing to updating the spec file at random times\u2014whenever somebody finds even a minor bug, I push a new version of Uproot to PyPI.\r\n\r\n> a) Repositories offer a lot of stability/reproduciablility and assure that \"everything is compatible with everything\".\r\n\r\nFor what definition of \"compatible\"? Running Uproot's test suite?\r\n\r\n> A good example could be installing root and uproot4. It is easy to get root from the package manager e.g. on Fedora.\r\n> In the full installation root has numpy as a dependence, if I remember correctly.\r\n> Installing uproot via pip would bring its own version of numpy.\r\n\r\nJust to be clear, there are no dependencies between ROOT and Uproot. Neither uses the other, so I don't know what compatibility would mean here, other than changes in the file format, which are intended to be independent of ROOT version (backward compatibility is a strong goal).\r\n\r\n> b) A system-wide installation.\r\n\r\nPip is system-wide by default; you have to go through extra hoops to install a Python package for one user only. Conda isn't, though. I think conda has only a single-user model.",
  "created_at":"2021-07-30T15:12:04Z",
  "id":889958888,
  "issue":401,
  "node_id":"IC_kwDOD6Q_ss41C7Ho",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T15:12:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":">  that you're \r\n\r\nWait, wait :-). The question was if there is an interest.  I would be fine with maintaining the spec files but uproot4 should be there  first and that would take time. \r\n\r\n> there are no dependencies between ROOT\r\n\r\nI know. Instead of ROOT one can name some other useful package with numpy as a dependency.\r\nThe compatibility I meant is the absense of dependency hell, multiple versions of same software, etc.\r\n\r\n> Pip is system-wide by default\r\n\r\nOn many systems for security reasons the only accepted method for the system-wide installation is from the \"trusted repositories\". \r\nSo I can ask cluster UI admins to install root, LHAPDF or similar things from EPEL system-wide, have a reproducible environment, never bother about updates and have empty .bashrc. But requests to install something  system-wide with pip will be rejected.\r\n\r\n\r\n\r\n\r\n",
  "created_at":"2021-07-30T15:39:16Z",
  "id":889976140,
  "issue":401,
  "node_id":"IC_kwDOD6Q_ss41C_VM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T15:39:16Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "author_association":"MEMBER",
  "body":"My interpretation of your first message was that you would maintain the spec files and the problem of getting Uproot into the EPEL/Fedora stack. Updating the spec by hand, even if only a digit in the version number, would eventually become burdensome\u2014think about having to do it years from now when you've moved on to other projects\u2014though perhaps it can be scripted, as I think the pip \u2192 conda-forge is.\r\n\r\nI'm not volunteering to do it because I'm not convinced that there is a need, though I wouldn't be opposed if you do it. The only downside to me that I can imagine is if the Uproot version in EPEL/Fedora gets very old and people report bugs that have long been fixed, but I don't see that as being much of a problem, since I always ask for a reproducer and if it doesn't reproduce in the latest version, I'd just let them know that it's fixed in a new release. So that \"downside to me\" is not much of a downside.\r\n\r\nThus, go ahead and do this if you think it's worthwhile. If you're asking me to start the process of adding the package to EPEL/Fedora, sorry, I won't be doing that.",
  "created_at":"2021-07-30T16:07:53Z",
  "id":889993711,
  "issue":401,
  "node_id":"IC_kwDOD6Q_ss41DDnv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T16:07:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski ,\r\n\r\nthat is not how the EPEL or Fedora works in many aspects. There are well working procedures to deal with the potential issues you've (correctly!) described. Just taking the \"bugreports on the old versions\" as an example: In a very simplified picture, in Fedora, an absence of  updates/stalled issues  would mean \"package is not maintained in Fedora\" and it will be excluded from the future releases. Because Fedora supports only two last versions, bugreports can be created only for the last two versions+dev, i.e. releases that are not older than 1 year.\r\n\r\nBut the main thing is that you are fine with having uproot4 there. So I might try to put it in the EPEL or Fedora at some point and meanwhile will close this issue.\r\nThank you!\r\n\r\nBest regards,\r\n\r\nAndrii\r\n  ",
  "created_at":"2021-08-01T13:31:37Z",
  "id":890521769,
  "issue":401,
  "node_id":"IC_kwDOD6Q_ss41FEip",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-08-01T13:31:37Z",
  "user":"MDQ6VXNlcjcyMDg2NzM="
 },
 {
  "author_association":"NONE",
  "body":"My `awkward` version in the above test was: `1.2.3`.\r\nI have updated to latest (`1.4.0`) and see the same error.",
  "created_at":"2021-07-30T13:36:09Z",
  "id":889898202,
  "issue":402,
  "node_id":"IC_kwDOD6Q_ss41CsTa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T13:36:09Z",
  "user":"MDQ6VXNlcjk4NDUwMzQ="
 },
 {
  "author_association":"MEMBER",
  "body":"I'll take a look at example.zip, although it's strange that it fails while this one succeeds (also a `std::vector<TLorentzVector>`):\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> skhep_testdata.data_path(\"uproot-HZZ-objects.root\")\r\n'/home/jpivarski/.local/skhepdata/uproot-HZZ-objects.root'\r\n>>> f = uproot.open(skhep_testdata.data_path(\"uproot-HZZ-objects.root\"))\r\n>>> t = f[\"events\"]\r\n>>> t.show()\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\njetp4                | std::vector<TLorentzV... | AsJagged(AsStridedObjects(M...\r\njetbtag              | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\r\njetid                | std::vector<bool>        | AsJagged(AsDtype('bool'), h...\r\nmuonp4               | std::vector<TLorentzV... | AsJagged(AsStridedObjects(M...\r\nmuonq                | std::vector<int32_t>     | AsJagged(AsDtype('>i4'), he...\r\nmuoniso              | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\r\nelectronp4           | std::vector<TLorentzV... | AsJagged(AsStridedObjects(M...\r\nelectronq            | std::vector<int32_t>     | AsJagged(AsDtype('>i4'), he...\r\nelectroniso          | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\r\nphotonp4             | std::vector<TLorentzV... | AsJagged(AsStridedObjects(M...\r\nphotoniso            | std::vector<float>       | AsJagged(AsDtype('>f4'), he...\r\nMET                  | TVector2                 | AsStridedObjects(Model_TVec...\r\nMC_bquarkhadronic    | TVector3                 | AsStridedObjects(Model_TVec...\r\nMC_bquarkleptonic    | TVector3                 | AsStridedObjects(Model_TVec...\r\nMC_wdecayb           | TVector3                 | AsStridedObjects(Model_TVec...\r\nMC_wdecaybbar        | TVector3                 | AsStridedObjects(Model_TVec...\r\nMC_lepton            | TVector3                 | AsStridedObjects(Model_TVec...\r\nMC_leptonpdgid       | int32_t                  | AsDtype('>i4')\r\nMC_neutrino          | TVector3                 | AsStridedObjects(Model_TVec...\r\nnum_primaryvertex    | int32_t                  | AsDtype('>i4')\r\ntrigger_isomu24      | bool                     | AsDtype('bool')\r\neventweight          | float                    | AsDtype('>f4')\r\n>>> t[\"jetp4\"].array()\r\n<Array [[], [{fP: {, ... fE: 33.9}], []] type='2421 * var * TLorentzVector[\"fP\":...'>\r\n```",
  "created_at":"2021-07-30T16:34:42Z",
  "id":890009323,
  "issue":402,
  "node_id":"IC_kwDOD6Q_ss41DHbr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T16:34:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Nevermind: I missed the fact that you saw this behavior in `uproot.lazy` and not in eager array-reading. Got it.",
  "created_at":"2021-07-30T16:55:19Z",
  "id":890025442,
  "issue":402,
  "node_id":"IC_kwDOD6Q_ss41DLXi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T16:55:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes that's right. It seems to just be `lazy` causing errors. Everything else seems to work as expected.\r\n\r\nI have just tried the example file that you are looking at and I see the same error if I do:\r\n\r\n```shell\r\nwget \"https://github.com/scikit-hep/scikit-hep-testdata/blob/main/src/skhep_testdata/data/uproot-HZZ-objects.root?raw=true\" -O hzz.root\r\npython -c \"import uproot4; print(uproot4.lazy('hzz.root:events'));\"\r\n```\r\n\r\nSo it seems to be reproducible with different input files. Of course, that doesn't eliminate the possibility that it might just be a problem on my system / installation.",
  "created_at":"2021-07-30T17:11:43Z",
  "id":890034799,
  "issue":402,
  "node_id":"IC_kwDOD6Q_ss41DNpv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-07-30T17:11:43Z",
  "user":"MDQ6VXNlcjk4NDUwMzQ="
 },
 {
  "author_association":"MEMBER",
  "body":"It's not an installation issue; PR #403 should fix it.\r\n\r\nThe fields that start with \"@\" symbols are TObject members that are unlikely to be meaningful in an analysis, but they're necessary for deserialization. The default Interpretations drop these fields (I put the \"@\" sign there as an indicator to later code of what to drop). In lazy arrays, the reading is deferred for later, but there are a lot of operations that can proceed if they only know the type of the data, so lazy arrays have a Form object (detailed type) to answer those questions without invoking a disk-read. However, when the array is finally read, if its actual type differs from what it's been telling everybody, that needs to be an error. In this case, the \"@\" fields were not removed from the Form as they are removed from the data that gets read\u2014this was inconsistent, and Awkward Array (which knows nothing about all this) raised an error because of the inconsistency. That's what its error was saying: the first Form had a bunch of fields with \"@\" signs in them; the second didn't. The fix was to make the Forms consistent with the data by removing fields that start with \"@\" signs.\r\n\r\nThat's the long explanation.",
  "created_at":"2021-07-30T17:21:23Z",
  "id":890040166,
  "issue":402,
  "node_id":"IC_kwDOD6Q_ss41DO9m",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-07-30T17:21:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Should this be linked in with\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/c1c95c0ad6b6f1fe830ed7187a71420d5fff913b/src/uproot/_util.py#L814-L839\r\n\r\nwhich convert ROOT's \"datime\" integers into Python `datetime` objects?",
  "created_at":"2021-08-09T13:03:49Z",
  "id":895204680,
  "issue":407,
  "node_id":"IC_kwDOD6Q_ss41W71I",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T13:03:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"> Should this be linked in with\r\n\r\nFor sure, but in what way? Modify during read, or in `Model.postprocess` or add as a method?",
  "created_at":"2021-08-09T13:10:44Z",
  "id":895209833,
  "issue":407,
  "node_id":"IC_kwDOD6Q_ss41W9Fp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T13:10:44Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"> For sure, but in what way? Modify during read, or in `Model.postprocess` or add as a method?\r\n\r\nThe canonical way would be to add a `src/behaviors/TDatime.py` containing a `TDatime` class and have this `Model_TDatime` inherit from that behavior. The behavior would then define a\r\n\r\n```python\r\n    def to_datetime(self):\r\n        return uproot._util.code_to_datetime(self._members[\"fDatime\"])\r\n```\r\n\r\nmethod for `Model_TDatime` to inherit.\r\n\r\nThat's how the methods are usually named: histograms have `to_numpy()`, `to_hist()` (methods, not properties, to allow for future parameterization). The goal is not to rewrite the functionality of ROOT in Uproot, but to export objects to their Python equivalents (which generally provide the functionality of ROOT by themselves; for instance, `datetime` has all the methods for getting year, day, hours, isoformat, etc.).",
  "created_at":"2021-08-09T14:07:17Z",
  "id":895253989,
  "issue":407,
  "node_id":"IC_kwDOD6Q_ss41XH3l",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T14:07:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"[scikit-hep-testdata 0.4.6](https://pypi.org/project/scikit-hep-testdata/) is now available. Tests that are looking for `uproot-issue-407.root` should find it.",
  "created_at":"2021-08-09T14:22:16Z",
  "id":895266649,
  "issue":407,
  "node_id":"IC_kwDOD6Q_ss41XK9Z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-09T14:22:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Could someone trigger the tests? I believe this is ready.",
  "created_at":"2021-08-18T15:58:56Z",
  "id":901233793,
  "issue":407,
  "node_id":"IC_kwDOD6Q_ss41t7yB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-18T15:58:56Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry, I didn't realize that it was being held up.",
  "created_at":"2021-08-18T16:13:14Z",
  "id":901244549,
  "issue":407,
  "node_id":"IC_kwDOD6Q_ss41t-aF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-18T16:13:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Right: the `uproot` directory has moved. Thanks!",
  "created_at":"2021-08-12T19:26:17Z",
  "id":897908137,
  "issue":410,
  "node_id":"IC_kwDOD6Q_ss41hP2p",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-12T19:26:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This should help with test failures like this one https://github.com/scikit-hep/uproot4/pull/407/checks?check_run_id=3363231826#step:9:360",
  "created_at":"2021-08-18T18:26:25Z",
  "id":901334456,
  "issue":413,
  "node_id":"IC_kwDOD6Q_ss41uUW4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-18T18:26:25Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks! I didn't know about that.",
  "created_at":"2021-08-18T18:36:13Z",
  "id":901340520,
  "issue":413,
  "node_id":"IC_kwDOD6Q_ss41uV1o",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-18T18:36:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Strangely there are now some new failures for python3.5 in version solver of conda.",
  "created_at":"2021-08-18T19:00:15Z",
  "id":901355143,
  "issue":413,
  "node_id":"IC_kwDOD6Q_ss41uZaH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-18T19:00:15Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I think this is good to go. I've restarted the jobs three times in my fork and didn't see any failures (although I did not see any HTTP errors). As usual, let me know if you encounter any problems with this one.",
  "created_at":"2021-08-18T23:04:21Z",
  "id":901484898,
  "issue":413,
  "node_id":"IC_kwDOD6Q_ss41u5Fi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-18T23:04:21Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Total Uproot 4 [SLOC](https://github.com/AlDanial/cloc): 28177.\r\n\r\nIt seems reasonable that 17% of Uproot is code for writing files, 83% is for reading. Uproot 3 had nearly the same breakdown (28%: 2200 for writing, 7873 total), though Uproot 4 is 3.5\u00d7 larger than Uproot 3. (That also makes sense.)\r\n\r\n(Note: cloc [counts Python docstrings as comments](https://github.com/AlDanial/cloc/issues/375). There aren't huge commented-out blocks of code in these files. Maybe `TH.py` could be broken up into files for each histogram type, but that would be 21 files! Or maybe just 1D, 2D, 3D for 3 files.)\r\n\r\n```\r\n% cloc --by-file src/uproot\r\n      76 text files.\r\n      76 unique files.                              \r\n       0 files ignored.\r\n\r\ngithub.com/AlDanial/cloc v 1.82  T=0.19 s (398.0 files/s, 218637.1 lines/s)\r\n--------------------------------------------------------------------------------------------\r\nFile                                                     blank        comment           code\r\n--------------------------------------------------------------------------------------------\r\nsrc/uproot/models/TH.py                                    324            158           4440\r\nsrc/uproot/_writing.py                                     548            131           2972\r\nsrc/uproot/behaviors/TBranch.py                            415           1081           2091\r\nsrc/uproot/writing.py                                      317            555           1944\r\nsrc/uproot/reading.py                                      414           1001           1218\r\nsrc/uproot/streamers.py                                    241            202           1206\r\nsrc/uproot/interpretation/identify.py                      153             63           1024\r\nsrc/uproot/containers.py                                   253            251           1015\r\nsrc/uproot/interpretation/library.py                       168            158            885\r\nsrc/uproot/models/TTree.py                                  56             33            781\r\nsrc/uproot/models/TGraph.py                                 50             22            767\r\nsrc/uproot/models/TBranch.py                                74             47            726\r\nsrc/uproot/model.py                                        246            695            675\r\nsrc/uproot/models/TAtt.py                                   83             47            653\r\nsrc/uproot/_util.py                                        109            107            642\r\nsrc/uproot/source/http.py                                  126            172            475\r\nsrc/uproot/interpretation/numerical.py                     103            138            433\r\nsrc/uproot/interpretation/objects.py                       102            166            423\r\nsrc/uproot/models/TLeaf.py                                 109            103            371\r\nsrc/uproot/language/python.py                               67             64            354\r\nsrc/uproot/interpretation/jagged.py                         65             75            275\r\nsrc/uproot/interpretation/strings.py                        64             71            273\r\nsrc/uproot/source/xrootd.py                                 96            111            270\r\nsrc/uproot/deserialization.py                               89            178            249\r\nsrc/uproot/source/cursor.py                                 72            258            248\r\nsrc/uproot/behaviors/TProfile.py                            70             70            195\r\nsrc/uproot/compression.py                                   70             87            182\r\nsrc/uproot/source/futures.py                                75            155            181\r\nsrc/uproot/models/TBasket.py                                57             93            171\r\nsrc/uproot/source/chunk.py                                  73            197            171\r\nsrc/uproot/source/file.py                                   55             57            163\r\nsrc/uproot/behaviors/TH1.py                                 64            112            152\r\nsrc/uproot/models/TRef.py                                   33             28            144\r\nsrc/uproot/behaviors/TAxis.py                               65            110            136\r\nsrc/uproot/models/TObjArray.py                              26             13            113\r\nsrc/uproot/cache.py                                         48             89            108\r\nsrc/uproot/extras.py                                        56             89            107\r\nsrc/uproot/models/TObject.py                                18              8            107\r\nsrc/uproot/sink/file.py                                     25             41            106\r\nsrc/uproot/behaviors/TH3.py                                 27             20            105\r\nsrc/uproot/models/TArray.py                                 54             37             99\r\nsrc/uproot/behaviors/TProfile3D.py                          16              7             95\r\nsrc/uproot/behaviors/TH2.py                                 26             20             94\r\nsrc/uproot/behaviors/RooCurve.py                            27             54             93\r\nsrc/uproot/__init__.py                                      32             59             90\r\nsrc/uproot/interpretation/grouped.py                        21             34             86\r\nsrc/uproot/models/TList.py                                  25              8             85\r\nsrc/uproot/models/TNamed.py                                 15              8             84\r\nsrc/uproot/const.py                                         22             12             83\r\nsrc/uproot/behaviors/TProfile2D.py                          16              7             83\r\nsrc/uproot/models/TObjString.py                             16              9             80\r\nsrc/uproot/behavior.py                                      17             21             67\r\nsrc/uproot/models/TDatime.py                                15              7             55\r\nsrc/uproot/models/TString.py                                19              9             53\r\nsrc/uproot/exceptions.py                                    13             21             53\r\nsrc/uproot/behaviors/RooHist.py                             16             36             50\r\nsrc/uproot/behaviors/TParameter.py                          23             16             47\r\nsrc/uproot/behaviors/TTree.py                               22             62             46\r\nsrc/uproot/behaviors/TGraphAsymmErrors.py                   13             33             43\r\nsrc/uproot/interpretation/__init__.py                       29            125             42\r\nsrc/uproot/models/THashList.py                              12              8             39\r\nsrc/uproot/source/object.py                                 30             50             39\r\nsrc/uproot/serialization.py                                 18             13             32\r\nsrc/uproot/models/RNTuple.py                                11              8             27\r\nsrc/uproot/behaviors/TGraph.py                               9             17             17\r\nsrc/uproot/behaviors/TH2Poly.py                              8              7             14\r\nsrc/uproot/dynamic.py                                        7             11              6\r\nsrc/uproot/version.py                                        7              7              6\r\nsrc/uproot/behaviors/TDatime.py                              6              7              5\r\nsrc/uproot/behaviors/TBranchElement.py                       9             16              4\r\nsrc/uproot/language/__init__.py                              8             13              3\r\nsrc/uproot/behaviors/TGraphErrors.py                         5              8              2\r\nsrc/uproot/behaviors/__init__.py                             7             20              1\r\nsrc/uproot/models/__init__.py                               12             34              1\r\nsrc/uproot/sink/__init__.py                                  2              4              1\r\nsrc/uproot/source/__init__.py                                4             15              1\r\n--------------------------------------------------------------------------------------------\r\nSUM:                                                      5698           7879          28177\r\n--------------------------------------------------------------------------------------------\r\n```",
  "created_at":"2021-08-19T21:00:23Z",
  "id":902241087,
  "issue":415,
  "node_id":"IC_kwDOD6Q_ss41xxs_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-19T21:00:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This looks very complete, but was it necessary to override the models? Can they not be read using streamers? If you want to add methods and properties to classes that _can_ be read generically, that's what behaviors are for. That is, if it's possible to provide a high-level method by calling low-level `self.member(\"fWhatever\")`, then defining behaviors means that the reading code can still be auto-generated from streamers, which allows them to get new Uproot features (e.g. the planned AwkwardForth update) and new class versions in ROOT. On the other hand, if TTable was removed from ROOT, then there won't be any new versions, and that's not an issue.\r\n\r\nThe answer might be \"no, it's not possible.\" I'm looking at the column-appending, and if that's custom (not just loading a bunch of TArrays, which can be described with streamers), then this is the only way and we should go ahead with it. Keep in mind that if there are multiple versions of this class floating around, defining the model means that only this version can be read (whereas defining a behavior would mix those methods in to any auto-generated version).",
  "created_at":"2021-08-24T16:17:02Z",
  "id":904783953,
  "issue":418,
  "node_id":"IC_kwDOD6Q_ss417ehR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-24T16:17:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I don't know if TTable can be encoded with TStreamerInfo, but the actual implementation uses custom streamers, so that option is not available. Regarding versions, I think ROOT supported only v3 and v4. The v3 should be trivial to implement if needed, but I haven't looked into producing a test file.",
  "created_at":"2021-08-27T21:22:45Z",
  "id":907483796,
  "issue":418,
  "node_id":"IC_kwDOD6Q_ss42FxqU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-27T21:22:45Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"If it's custom streamers, then there is no hope: it must be implemented as a Model, as you have done. Those models will only work for the version that you implemented, but if v3 is so old that it's unlikely to be encountered, then it's up to you whether you want to add it. As it stands, a class defined through custom streamers could not have been read by Uproot before, so you've added a capability and taken none away.\r\n\r\nWe can make another release for this when this PR is ready. Usually, there isn't such a long time between Uproot releases\u2014they're lightweight and we can make them relatively often. Also, I think I found a performance bug in file-writing that I'll want to fix soon, so we might be able to catch the same train.",
  "created_at":"2021-08-27T21:33:08Z",
  "id":907488309,
  "issue":418,
  "node_id":"IC_kwDOD6Q_ss42Fyw1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-27T21:33:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Looks like you're done. Thanks! I'll include this in Uproot 4.1.1.",
  "created_at":"2021-08-30T21:33:54Z",
  "id":908715520,
  "issue":418,
  "node_id":"IC_kwDOD6Q_ss42KeYA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-30T21:33:54Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Isn't `TMessage` restricted to serializing objects inheriting from `TObject`? Would `TBufferFile` have worked for this data transfer?",
  "created_at":"2021-08-26T15:51:43Z",
  "id":906532487,
  "issue":420,
  "node_id":"IC_kwDOD6Q_ss42CJaH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-26T15:51:43Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"They do have to inherit from TObject (there's a guard to check for that), but I think that's true of any object that can be added to a TDirectory. If it can't be a standalone object in a TDirectory, how could TBufferFile be used?",
  "created_at":"2021-08-26T16:20:32Z",
  "id":906555930,
  "issue":420,
  "node_id":"IC_kwDOD6Q_ss42CPIa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-26T16:20:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I'm not saying this is the best idea but it does work:\r\n```\r\n$ root -l\r\nroot [0] TFile f(\"f.root\", \"recreate\");\r\nroot [1] std::string a(\"hi there\");\r\nroot [2] f.WriteObjectAny(&a, \"std::string\", \"\");\r\nroot [3] f.Close()\r\nroot [4] .q\r\n$ root -l f.root\r\nroot [0]\r\nAttaching file f.root as _file0...\r\n(TFile *) 0x7f85d7992a70\r\nroot [1] auto s = (std::string*) _file0->Get(\"string\");\r\nroot [2] *s\r\n(std::basic_string &) \"hi there\"\r\n```",
  "created_at":"2021-08-26T17:13:50Z",
  "id":906592840,
  "issue":420,
  "node_id":"IC_kwDOD6Q_ss42CYJI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-26T17:13:50Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I didn't know that you could put non-TObjects in a TDirectory. Actually, now I wonder if Uproot would be able to read that `std::string`. Your approach could be made more lightweight by replacing TFile with TMemFile; I don't know if a TBufferFile can be directly used and if it can, what the difference between that and a TMemFile would be. I also don't know if this is going to be writing TStreamerInfo, which is something we need for each new class, but not for each new object. I thought that TMessages would be more lightweight for that, since file-like methods would have to write at least a file header, TDirectory, and FreeSegments record, but probably also TStreamerInfo.\r\n\r\nThanks for setting me straight on non-TObjects in TDirectories! But I'll be keeping the current implementation with its known constraints, at least for now.",
  "created_at":"2021-08-26T17:49:36Z",
  "id":906616250,
  "issue":420,
  "node_id":"IC_kwDOD6Q_ss42Cd26",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-26T17:49:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"TMessage inherits from TBufferFile, so I think it is just a matter of using `TBufferIO::WriteObjectAny` [doc](https://root.cern.ch/doc/master/classTBufferIO.html#ae8dbce7f66a7d89a39d3d4eeffd16736) rather than `TBufferIO::WriteObject` and knowing the correct class name.",
  "created_at":"2021-08-26T18:04:20Z",
  "id":906626306,
  "issue":420,
  "node_id":"IC_kwDOD6Q_ss42CgUC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-26T18:04:20Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"NONE",
  "body":"Thank you, @jpivarski, for putting this together. This was a really essential project for the interoperability of ROOT, and we\u2019re all better off for your effort.",
  "created_at":"2021-08-27T01:29:02Z",
  "id":906852526,
  "issue":421,
  "node_id":"IC_kwDOD6Q_ss42DXiu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-27T01:29:02Z",
  "user":"MDQ6VXNlcjE1Nzc0ODM4"
 },
 {
  "author_association":"MEMBER",
  "body":"I think that combining the functionality of `filter_name` and `expressions` would severely complicate them. They're kept separate because `*` has a very different meaning in regular expression search than it does in mathematical formulas. Even if there's a way to make this two-pass thing work, isn't that going to lead to more confusion?\r\n\r\nI have to ask, why is it important for so much of the analysis to be done in a one-line function call? I don't know of a real reason to use `expressions` as mathematical formulas at all, except for the fact that the functionality had to be implemented somewhere to support TTree's built-in aliases, so it might as well be exposed more generally. Why not use `filter_name` to get a set of uncomputed arrays and do array manipulation in Python statements, rather than in a string? It _is_ the same evaluation; there is no performance advantage or anything.",
  "created_at":"2021-08-27T17:52:53Z",
  "id":907372223,
  "issue":423,
  "node_id":"IC_kwDOD6Q_ss42FWa_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-27T17:52:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski, \r\n\r\nThanks a lot for the quick reply.\r\n\r\nYou make a fair point about the confusion. I\u2019m not entirely sure how regex processing is taking place in Uproot backend, but I would have thought the use of the syntax `/pattern/i` would distinguish between an expression and a regex pattern? \r\n\r\nThe reason I was looking into this is because we are writing a framework which will parse from a configuration file, so the exact expressions are unknown until runtime. I can\u2019t think of a straight-forward way (that doesn\u2019t involve writing up an interpreter) to breakdown the expression from the config, call the branches individually then perform the calculation on them. There is also the\r\nproblem of interpreting slicing expressions (e.g. Branch[:,0])",
  "created_at":"2021-08-27T20:02:09Z",
  "id":907444986,
  "issue":423,
  "node_id":"IC_kwDOD6Q_ss42FoL6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-27T20:21:01Z",
  "user":"MDQ6VXNlcjg5MTQ3NDc4"
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, the slashes at the beginning and end of the string (`/pattern/` with possible flags like `i`) is the signal that this is a regex filter:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/a17aca8cc07ba203e48fe63dc0d5f0b6749de4a1/src/uproot/_util.py#L156-L161\r\n\r\nwith\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/a17aca8cc07ba203e48fe63dc0d5f0b6749de4a1/src/uproot/_util.py#L119\r\n\r\nbut if you need the calculation to go in stages, first a regex, then a computation, that would get messy. Should there be two strings? Should characters like `*` be quoted as `\\*` in the regex phase, then interpreted as operators in the computation phase? I'm just having trouble seeing that as making people's lives easier, rather than harder.\r\n\r\nAnd even then, the `expressions` are not very constrained: they can be any Python _expression_, which is about half of the Python language (the other half being statements). If I'm not misunderstanding which direction you're going in, you could impose constraints in the configuration language and use that to generate Python. If you're trying to go the other way, taking a Python script and turning it into something declarative, that direction is not possible.\r\n\r\nSo back to the direction I think you're going in: if you're defining a configuration file language, you can add knobs and dials that generate Python code as easily as it could generate Uproot `expression` strings. After all, they're both the same language, Python. You mention \"writing up an interpreter\"\u2014it wouldn't need to be all that, since generating the text of Python code from templates is not as much work as building an interpreter, it's more of a source code to source code translation. Also, by \"configuration file language,\" I don't mean writing a parser, I mean interpreting nested elements in YAML: that's pretty common for configuration files now. The main thing is that I don't see how what you need to generate is worse or harder if you're generating Python source code that gets run through `exec` than it would be if you were generating one line of Python source code that _I_ run through `exec`!\r\n\r\nIt happens here:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/a17aca8cc07ba203e48fe63dc0d5f0b6749de4a1/src/uproot/language/python.py#L159-L184\r\n\r\nThere's some manipulation of the Python AST to recognize `\"nested.branch.name\"` as a single variable, rather than unpacking a class instance, but other than that, it all goes through Python's `eval` in the end.\r\n\r\nI should mention that there's an [ongoing project](https://github.com/AdvaitDhingra/formulate1) to add TTree::Draw as a language for `expressions`, possibly as a future default. Interpreting the strings as Python happened to be the easiest to implement, not our highest preference.",
  "created_at":"2021-08-27T21:25:42Z",
  "id":907485185,
  "issue":423,
  "node_id":"IC_kwDOD6Q_ss42FyAB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-27T21:25:42Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski \r\n\r\nThanks a lot for your reply. I will like into your suggestions. I wasn't looking for anything particularly complicated -- the goal was simply allow a user to specify something like \r\nnew_branch = Branch(expression = (branch1*branch2)/2)\r\nregex_banch = Branch(filter=*pattern*)\r\n and in the back-end I would change ```pattern -> /pattern/i ``` to ensure it is explicitly defiend as a regex pattern and I had hoped i would be able to pass this expression for the new branch at the same time to ```uproot.iterate()``` for example so that i can read both branches requested by user. Anyways, thanks a lot for the very detailed and helpful answers :) ",
  "created_at":"2021-09-01T10:30:27Z",
  "id":910152676,
  "issue":423,
  "node_id":"IC_kwDOD6Q_ss42P9Pk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T10:30:27Z",
  "user":"MDQ6VXNlcjg5MTQ3NDc4"
 },
 {
  "author_association":"MEMBER",
  "body":"Performance testing, just Uproot and C++ ROOT this time. Uses the same (singly) jagged dataset as https://arxiv.org/abs/2102.13516 .\r\n\r\n**scaling-trees-uproot.py**\r\n\r\n```python\r\nimport time\r\n\r\nimport numpy as np\r\nimport awkward as ak\r\nimport uproot\r\n\r\ncontent = np.fromfile(\"/home/jpivarski/storage/data/chep-2021-jagged-jagged-jagged/sample-content.float32\", dtype=np.float32)\r\noffsets = np.fromfile(\"/home/jpivarski/storage/data/chep-2021-jagged-jagged-jagged/sample-offsets1.int64\", dtype=np.int64)\r\n\r\narray = ak.Array(ak.layout.ListOffsetArray64(ak.layout.Index64(offsets), ak.layout.NumpyArray(content)))\r\n\r\nprint(repr(array))\r\n# <Array [[-0.423, 2.34, -0.757, ... 1.41, 0.72]] type='134217728 * var * float32'>\r\n\r\nENTRIES_PER_BASKET = 50000\r\n\r\nstarttime = time.time()\r\n\r\nwith uproot.recreate(\"/tmp/output.root\", compression=None) as fout:\r\n    fout.mktree(\"tree\", {\"branch\": array.type})\r\n    for start in range(0, len(array), ENTRIES_PER_BASKET):\r\n        fout[\"tree\"].extend({\"branch\": array[start : start + ENTRIES_PER_BASKET]})\r\n\r\nprint(f\"entries per basket {ENTRIES_PER_BASKET}, time {time.time() - starttime}\")\r\n```\r\n\r\n```\r\nentries per basket 1000, time 61.869351863861084\r\nentries per basket 1000000, time 15.633187770843506\r\nentries per basket 2000, time 34.52807021141052\r\nentries per basket 500000, time 14.124958038330078\r\nentries per basket 5000, time 16.41567373275757\r\nentries per basket 200000, time 13.47899842262268\r\nentries per basket 10000, time 15.130658864974976\r\nentries per basket 100000, time 13.349303483963013\r\nentries per basket 20000, time 13.790864944458008\r\nentries per basket 50000, time 13.850778579711914\r\n```\r\n\r\n**scaling-trees-ROOT.cpp**\r\n\r\n```c++\r\n#include <iostream>\r\n#include <chrono>\r\n\r\n#include \"TFile.h\"\r\n#include \"TTree.h\"\r\n#include \"TBranch.h\"\r\n\r\nconst int64_t ENTRIES_PER_BASKET = 50000;\r\n\r\nint main(int argc, char** argv) {\r\n  FILE* content_file = fopen(\"/home/jpivarski/storage/data/chep-2021-jagged-jagged-jagged/sample-content.float32\", \"r\");\r\n  FILE* offsets_file = fopen(\"/home/jpivarski/storage/data/chep-2021-jagged-jagged-jagged/sample-offsets1.int64\", \"r\");\r\n\r\n  float* content = new float[1073741824];\r\n  int64_t* offsets = new int64_t[134217729];\r\n\r\n  for (int64_t step = 0;  step < 1073741824;  step += 1024*1024) {\r\n    fread(&content[step], sizeof(float), 1024*1024, content_file);\r\n  }\r\n  fread(offsets, sizeof(int64_t), 134217729, offsets_file);\r\n\r\n  auto starttime = std::chrono::high_resolution_clock::now();\r\n\r\n  auto f = new TFile(\"/tmp/output.root\", \"RECREATE\");\r\n  f->SetCompressionLevel(0);\r\n  auto t = new TTree(\"tree\", \"\");\r\n  t->SetAutoFlush(0);\r\n  t->SetAutoSave(0);\r\n\r\n  uint32_t counter;\r\n  float array[16];  // this is big enough\r\n\r\n  t->Branch(\"nbranch\", &counter, \"nbranch/I\", 100*1024*1024);      // temp TBasket must be bigger\r\n  t->Branch(\"branch\", array, \"branch[nbranch]/F\", 100*1024*1024);  // than written TBasket size\r\n\r\n  for (int64_t i = 0;  i < 134217729 - 1;  i++) {\r\n    int64_t start = offsets[i];\r\n    int64_t stop = offsets[i + 1];\r\n    counter = stop - start;\r\n\r\n    for (int64_t j = start;  j < stop;  j++) {\r\n      array[j - start] = content[j];\r\n    }\r\n\r\n    t->Fill();\r\n    if (i % ENTRIES_PER_BASKET == ENTRIES_PER_BASKET - 1) {\r\n      t->FlushBaskets();\r\n    }\r\n  }\r\n\r\n  t->Write();\r\n  f->Close();\r\n\r\n  auto stoptime = std::chrono::high_resolution_clock::now();\r\n\r\n  std::cout << \"entries per basket \" << ENTRIES_PER_BASKET << \", \"\r\n            << \"time \" << std::chrono::duration_cast<std::chrono::microseconds>(\r\n                 stoptime - starttime\r\n               ).count() / 1e6\r\n            << std::endl;\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n```\r\nentries per basket 1000, time 26.1168\r\nentries per basket 1000000, time 15.4534\r\nentries per basket 2000, time 16.5195\r\nentries per basket 500000, time 15.6402\r\nentries per basket 5000, time 14.8063\r\nentries per basket 200000, time 15.2132\r\nentries per basket 10000, time 15.4887\r\nentries per basket 100000, time 15.0006\r\nentries per basket 20000, time 14.5771\r\nentries per basket 50000, time 14.309\r\n```\r\n\r\nNow the most significant effect is the number of entries per TBasket/`extend`/flush/cluster. For these data, 1 entry is very nearly 40 bytes, so 1000 entries is 40 kB and 1000000 entries is 40 MB.\r\n\r\n<img src=\"https://user-images.githubusercontent.com/1852447/131406058-5bdeba69-e743-4472-aeaa-2df60dcbcadd.png\" width=\"100%\">\r\n\r\nSo for _writing_ performance on par with C++ ROOT, you need to be writing at least 100 kB per branch in each call to `extend`. (The lines converge at 5000 entries per flush, which is 200 kB, but that's for 2 branches, and the data vs overhead scales with the number of branches, so that's why the rule is \"100 kB per branch in each call to `extend`.)",
  "created_at":"2021-08-30T21:16:08Z",
  "id":908703486,
  "issue":428,
  "node_id":"IC_kwDOD6Q_ss42Kbb-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-08-30T21:16:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I thought I had reversed the `not`, but it looks like I didn't. Anyway, this version would always do the right thing: whether the file had previously existed or not, you end up with an empty, existing file before any writing starts.",
  "created_at":"2021-09-01T14:41:50Z",
  "id":910353070,
  "issue":431,
  "node_id":"IC_kwDOD6Q_ss42QuKu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T14:41:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Windows Python 2.7 is the _only_ test that does not install Awkward Array, and it demonstrates that Uproot really can be used with nothin' but NumPy.\r\n\r\nNow all the tests run in ~10 minutes, much better!",
  "created_at":"2021-09-01T16:12:48Z",
  "id":910437244,
  "issue":432,
  "node_id":"IC_kwDOD6Q_ss42RCt8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T16:12:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@MoAly98 Moved Discussion to Issue because it's a bug-report.",
  "created_at":"2021-09-01T14:36:30Z",
  "id":910348338,
  "issue":433,
  "node_id":"IC_kwDOD6Q_ss42QtAy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T14:36:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The key thing that was happening here is that branch1 and branch2 have different numbers of values in each entry (\"_differently_ jagged\"), so they can't be in the same DataFrame, so `iterate` was yielding _tuples_ of DataFrames rather than just DataFrames. When you iterated over more than one file, it tried to set the index of all files after the first one to globally increasing numbers (so that indexes would not go back to zero just because you're opening a new file), and Uproot's `global_index` function assumes that it gets a DataFrame, rather than a tuple of DataFrames. That last assumption is a bug and it's fixed with\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/0bc31f3729b1876814e05261698baa0584d9c207/src/uproot/interpretation/library.py#L1087-L1089\r\n\r\nin PR #432.\r\n\r\nSince the error message pointed to a line in `global_index` with a comment about `pandas<0.24.0`, I tested it with Pandas 0.22.0 and found another issue: branch3 would come up all NaN because of an old behavior in Pandas. I added a comment to the codebase for future generations, but this seems like a really old version of Pandas and I think we shouldn't be supporting anything that old. Still, I don't know when it was fixed in Pandas to put a Pandas version dependency in Uproot. The Pandas version you're using is likely to be able to put branch3 in your DataFrames without NaN.\r\n\r\nUproot 4.0.0 is also rather old: December 5, 2020 if I remember right (because it was an auspicious version number update). On Monday, 4.1.1 was released, and the bug-fix in PR #432 will go into an Uproot 4.1.2 soon.\r\n\r\nIf you need this working without waiting for a new release, you could have `uproot.iterate` yield Awkward Arrays with `library=\"ak\"` and call [ak.to_pandas](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_pandas.html) to make your DataFrames.",
  "created_at":"2021-09-01T14:36:36Z",
  "id":910348412,
  "issue":433,
  "node_id":"IC_kwDOD6Q_ss42QtB8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T14:36:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Hi @jpivarski,\r\n\r\nThanks a lot for the quick and helpful reply. I should add that I see the same error when I pass just one file rather than a list of files. I guess you mean when a new chunk of data is being processed then the indexing causes trouble?\r\n\r\nUnfortunately the ```ak.to_pandas()```method is not doing what I need (which is another story that probably belongs in the ```awkward``` Q&A). It could be useful to this discussion that I outline this issue:\r\nWhat I would like to achieve is a structure like (shown for 1 event):\r\n\r\n|       |           |           | Branch1 | Branch2 | Branch3 |\r\n|:-----:|:---------:|:---------:|:-------:|:-------:|:-------:|\r\n| entry | subentry1 | subentry2 |         |         |         |\r\n|   0   |     0     |     0     |    X0   |    Y0   |    Z    |\r\n|       |           |     1     |    X0   |    Y1   |    Z    |\r\n|       |           |     2     |    X0   |    Y2   |    Z    |\r\n|       |           |     3     |    X0   |    Y3   |    Z    |\r\n|       |     1     |     0     |    X1   |    Y0   |    Z    |\r\n|       |           |     1     |    X1   |    Y1   |    Z    |\r\n|       |           |     2     |    X1   |    Y2   |    Z    |\r\n|       |           |     3     |    X1  |    Y3   |    Z    |\r\n\r\nwhere ```subentryX``` is the index for ```BranchX ```. However, ```ak.to_pandas()```  seem to be collapsing the indices such that I get:\r\n|       |          | Branch1 | Branch2 | Branch3 |\r\n|-------|----------|---------|---------|---------|\r\n| entry | subentry |         |         |         |\r\n| 0     | 0        | X0      | Y0      | Z       |\r\n|       | 1        | X1      | Y1      | Z       |\r\n|       | 2        | X2      | Y2      | Z       |\r\n|       | 3        | X3      | Y3      | Z       |\r\n\r\nand I can't seem to achieve the structure I want from playing around with the ```how``` argument in ```ak.to_pandas()```. \r\n\r\nI'm not sure if I'm doing someting wrong, or if this is a bug in the way ```uproot.iterate()``` is saving the information in the awkward arrays causing the loss of information. ",
  "created_at":"2021-09-01T15:11:10Z",
  "id":910380677,
  "issue":433,
  "node_id":"IC_kwDOD6Q_ss42Q06F",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T15:11:10Z",
  "user":"MDQ6VXNlcjg5MTQ3NDc4"
 },
 {
  "author_association":"MEMBER",
  "body":"You'll only get a table like the first one if the data were doubly jagged, like a `std::vector<std::vector<float>>` for each entry. You said that branch1 and branch2 both have interpretation `AsJagged(AsDtype('>f4'))`, which is what you'd get with `std::vector<float>` data (or variable-length arrays: there are multiple C++ types that lead to the same interpretation).\r\n\r\nBut if branch1 and branch2 are \"_differently_ jagged,\" they have different numbers of values in each entry, then naively they can't go into the same DataFrame. You can force them into the same DataFrame by specifying `how=\"inner\"`, `\"outer\"`, etc., which tells [pd.merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) how to match subentry indexes of branch1 with subentry indexes of branch2, because there is a different number of them.\r\n\r\nIf I'm wrong in my interpretation that branch1 and branch2 are differently jagged, then I don't know how you got the error. I was only able to reproduce it with differently jagged data, because only then do you get a tuple of DataFrames instead of just one DataFrame, and that's what the error message was saying (obliquely: `index` is a method for tuples and a property for DataFrames, and it was complaining that a method does not have an attribute `arrays`, which a DataFrame index does).",
  "created_at":"2021-09-01T15:49:17Z",
  "id":910414855,
  "issue":433,
  "node_id":"IC_kwDOD6Q_ss42Q9QH",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T15:49:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"You were right in your interpretation -- as a concrete example, I'm trying to read in ```leptons_pt ```and ```jets_pt ``` branches which are variable-length arrays for each event. For such case, I would like to retain for each event values of pt for all jets as well as pt of all leptons.  I am able to generate the first table using some pandas ```join``` method manipulation using the dataframes tuple, but understandably ```ak.to_pandas()``` is not able to do this for me. \r\n\r\nIs there an indication on when we can expect v4.1.2 ? \r\n\r\nThanks again for all the amazing work on ```uproot``` and other packages for pyhep ! ",
  "created_at":"2021-09-01T16:16:51Z",
  "id":910440611,
  "issue":433,
  "node_id":"IC_kwDOD6Q_ss42RDij",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T16:17:06Z",
  "user":"MDQ6VXNlcjg5MTQ3NDc4"
 },
 {
  "author_association":"MEMBER",
  "body":"The `how` parameter of [uproot.iterate](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TBranch.iterate.html) is passed to Pandas for merging, but if it has to be done in a separate line of code, I don't think that's a bad thing. If you want to be very careful about this, so that a chunk of events that _happen to have_ the same number of leptons as jets (for all events in that chunk) don't get accidentally merged into the same DataFrame, you could do two `uproot.iterate` calls, set the `step_size` of both to an exact number of entries (as opposed to an approximate number of bytes), and then use Python's `zip` to walk over both of them at the same time.\r\n\r\n```python\r\nevents = uproot.iterate(files, [\"event_branches\", ...], step_size=10000)\r\nleptons = uproot.iterate(files, [\"lepton_branches\", ...], step_size=10000)\r\njets = uproot.iterate(files, [\"jet_branches\", ...], step_size=10000)\r\n\r\nfor df_event, df_lep, df_jet in zip(events, leptons, jets):\r\n    # carefully merge the DataFrames in a way that won't break in extreme cases\r\n```\r\n\r\nIncidentally, this would be a way of doing a \"friend TTree\" analysis, too.\r\n\r\nUproot releases don't have a fixed schedule, so if you need PR #432 to be in a fixed release, test it for your use-case in main ([like this](https://adamj.eu/tech/2019/03/11/pip-install-from-a-git-repository/) or clone the GitHub repo and do `pip install .` from its directory) and if it works, I'll make the release.",
  "created_at":"2021-09-01T16:32:40Z",
  "id":910453743,
  "issue":433,
  "node_id":"IC_kwDOD6Q_ss42RGvv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-01T16:32:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"AHA! I hadn't thought of this extreme case so thanks a lot for pointing it out :)\r\n\r\nI tested the bug-fix and it seems to solve my issue of processing multiple jagged arrays with ```uproot.iterate()```. Thanks a lot !",
  "created_at":"2021-09-03T16:43:54Z",
  "id":912671554,
  "issue":433,
  "node_id":"IC_kwDOD6Q_ss42ZkNC",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-09-03T16:43:54Z",
  "user":"MDQ6VXNlcjg5MTQ3NDc4"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Also by mistake I added the fix to `TRefArray` member `refs`, which should be a ListArray.",
  "created_at":"2021-09-16T22:37:47Z",
  "id":921305581,
  "issue":441,
  "node_id":"IC_kwDOD6Q_ss426gHt",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-16T22:37:47Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Merging because all looks good and this is a blocker.\r\n\r\n(Note for posterity: @nsmith- and I talked about these issues on Slack while he was discovering them.)",
  "created_at":"2021-09-18T01:58:01Z",
  "id":922159980,
  "issue":441,
  "node_id":"IC_kwDOD6Q_ss429wts",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-18T01:58:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"The test will require merging https://github.com/scikit-hep/scikit-hep-testdata/pull/65",
  "created_at":"2021-09-17T22:42:59Z",
  "id":922119104,
  "issue":442,
  "node_id":"IC_kwDOD6Q_ss429mvA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-17T22:42:59Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This will have issues with `delphes_tree[\"GenJet04/GenJet04.Constituents\"].array(library=\"np\")` once #441 is merged, need to look a bit more",
  "created_at":"2021-09-17T23:04:57Z",
  "id":922125443,
  "issue":442,
  "node_id":"IC_kwDOD6Q_ss429oSD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-17T23:04:57Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> The test will require merging [scikit-hep/scikit-hep-testdata#65](https://github.com/scikit-hep/scikit-hep-testdata/pull/65)\r\n\r\nSee \r\n\r\nhttps://github.com/scikit-hep/scikit-hep-testdata/actions/runs/1247612638\r\n\r\nto file the progress of releasing 0.4.8.",
  "created_at":"2021-09-18T02:16:11Z",
  "id":922162606,
  "issue":442,
  "node_id":"IC_kwDOD6Q_ss429xWu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-18T02:16:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"And ping me again when you're ready for this to be merged. (The changes so far look good.)",
  "created_at":"2021-09-18T02:17:16Z",
  "id":922162742,
  "issue":442,
  "node_id":"IC_kwDOD6Q_ss429xY2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-18T02:19:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski that CI run failed looking at the link!",
  "created_at":"2021-09-18T21:47:59Z",
  "id":922378408,
  "issue":442,
  "node_id":"IC_kwDOD6Q_ss42-mCo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-18T21:47:59Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Actually the problem was much more subtle: essentially by chance the data was such that numpy converted a list of `TRefArray`s with the same number of refs into a 2D array. This is because by default:\r\n```python\r\n>>> numpy.array([ [1,2], [3,4] ], dtype=object)\r\narray([[1, 2],\r\n       [3, 4]], dtype=object)\r\n```",
  "created_at":"2021-09-20T14:51:34Z",
  "id":923000199,
  "issue":442,
  "node_id":"IC_kwDOD6Q_ss43A92H",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-20T14:51:34Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@jpivarski this is ready",
  "created_at":"2021-09-20T18:19:51Z",
  "id":923167269,
  "issue":442,
  "node_id":"IC_kwDOD6Q_ss43Bmol",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-20T18:19:51Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"NONE",
  "body":"The user-facing interface strategy would be to support `s3http://` and `s3https://` URLs with specification of the S3 credentials in one of the following methods:\r\n- environment variables `S3_ACCESS_KEY` and `S3_SECRET_KEY` (as done by TS3WebFile),\r\n- syntax `s3http://S3_ACCESS_KEY:S3_SECRET_KEY@example.com` to emulate the `AUTH=S3_ACCESS_KEY:S3_SECRET_KEY` optional argument in TS3WebFile (this mirrors the approach for basic authentication for https connections).",
  "created_at":"2021-09-18T20:22:07Z",
  "id":922368423,
  "issue":443,
  "node_id":"IC_kwDOD6Q_ss42-jmn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-18T20:22:07Z",
  "user":"MDQ6VXNlcjQ2NTYzOTE="
 },
 {
  "author_association":"MEMBER",
  "body":"Also in reading.py, there's code that activates the HTTPSource or MultithreadedHTTPSource if the schema is HTTP. To send S3HTTP schemas this way, it will have to be changed there, too.",
  "created_at":"2021-09-20T02:42:09Z",
  "id":922603619,
  "issue":444,
  "node_id":"IC_kwDOD6Q_ss42_dBj",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-09-20T02:42:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Filters are combined by union: of a name passes either of these regexes, it passes. Does that fit with what you're seeing?",
  "created_at":"2021-09-20T19:07:39Z",
  "id":923200860,
  "issue":445,
  "node_id":"IC_kwDOD6Q_ss43Bu1c",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-20T19:07:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yes, so in case of two negative regexes, this translates to no filter applied at all. ",
  "created_at":"2021-09-20T19:47:38Z",
  "id":923243019,
  "issue":445,
  "node_id":"IC_kwDOD6Q_ss43B5IL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-20T19:47:38Z",
  "user":"MDQ6VXNlcjg5MTQ3NDc4"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, so in that case, it's not an error, right? Joining the regexes is one good way of dealing with it; replacing the steering with a function that returns True or False is another (which lets you use `and` and `or`).\r\n\r\nOf there's no bug, we can close this.",
  "created_at":"2021-09-20T20:52:38Z",
  "id":923292426,
  "issue":445,
  "node_id":"IC_kwDOD6Q_ss43CFMK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-20T20:52:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"thanks a lot! Yes, I'm happy to close this. I think though that having support for a chain of negative filters could be useful !",
  "created_at":"2021-09-22T14:11:45Z",
  "id":924970605,
  "issue":445,
  "node_id":"IC_kwDOD6Q_ss43Ie5t",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-22T14:11:45Z",
  "user":"MDQ6VXNlcjg5MTQ3NDc4"
 },
 {
  "author_association":"MEMBER",
  "body":"PR #451.\r\n\r\nI intend to fix a lot of these issues this week, but I'm going backward, starting with the most recently posted.",
  "created_at":"2021-09-27T19:17:23Z",
  "id":928196061,
  "issue":450,
  "node_id":"IC_kwDOD6Q_ss43UyXd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-09-27T19:17:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I was about to say that we'd need a reproducible test, but I guess we could add an intentional Exception to the callback, at least for a manual test. I don't know how we could do that in CI, but at least for coming up with a solution.\r\n\r\nThe HTTP Source passes exceptions on to the main thread. The same kind of thing should be possible here.",
  "created_at":"2021-10-01T20:13:57Z",
  "id":932521548,
  "issue":456,
  "node_id":"IC_kwDOD6Q_ss43lSZM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-01T20:13:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Without knowing what I'm really doing, I hardcoded\r\nhttps://github.com/scikit-hep/uproot4/blob/03cb08ccb1f47ce0b082117f41a112af1ab403ee/src/uproot/writing/_cascadetree.py#L945-L948\r\nto\r\n```python\r\nif datum[\"counter\"] is None:\r\n    dims = \"\"\r\nelse:\r\n    dims = \"[nbranch]\"\r\n```\r\nand then the output file has the correct behavior in the ROOT REPL, which should match the behavior of the output of this pyROOT script:\r\n```python\r\nfrom array import array\r\nimport ROOT\r\n\r\nf = ROOT.TFile(\"blah.root\", \"recreate\")\r\nt = ROOT.TTree(\"t\",\"\")\r\n\r\nn = array(\"i\", [0])\r\nb = array(\"d\", 10*[0.])\r\nt.Branch(\"nbranch\", n, \"nbranch/I\")\r\nt.Branch(\"branch\", b, \"branch[nbranch]/D\")\r\n\r\nvals = [[1.1, 2.2, 3.3], [], [4.4, 5.5], [6.6]]\r\nfor e in vals:\r\n    n[0] = len(e)\r\n    for i in range(len(e)):\r\n        b[i] = e[i]\r\n    t.Fill()\r\nt.Write()\r\nf.Close()\r\n```",
  "created_at":"2021-10-04T01:22:40Z",
  "id":933072779,
  "issue":457,
  "node_id":"IC_kwDOD6Q_ss43nY-L",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-04T01:35:42Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Doesn't this:\r\n\r\n```python\r\n>>> import ROOT\r\n>>> ch = ROOT.TChain(\"t\")\r\n>>> ch.Add(\"test.root\")\r\n>>> [list(evt.branch) for evt in ch] # works\r\n[[1.1, 2.2, 3.3], [], [4.4, 5.5], [6.6]]\r\n```\r\n\r\nmean that ROOT is reading it back correctly? (I was taking that as a definition of \"correct\".) PyROOT is ROOT\u2014the Python interface shouldn't affect anything in the I/O layer. For instance, in PyROOT, you can say\r\n\r\n```python\r\n>>> ch.Scan(\"*\")\r\n```\r\n\r\nand the output ought to be exactly the same as in C++ ROOT.\r\n\r\nThe Uproot code you changed only changes the leaf title from `\"branch\"` to `\"branch[nbranch]\"`. Does ROOT's `TTree::Scan` behavior depend on the leaf title? If so, then perhaps a more appropriate title would be\r\n\r\n```python\r\n  if datum[\"shape\"] == (): \r\n    dims = \"\"\r\n  else:\r\n    dims = \"\".join(\"[\" + str(x) + \"]\" for x in datum[\"shape\"])\r\n\r\n  if datum[\"counter\"] is not None:\r\n      dims = \"[nbranch]\" + dims\r\n```\r\n\r\nso that the variable-length dimension would be followed by the fixed-length dimensions (if either exist).\r\n\r\nIt should be possible to perform the same experiments in ROOT. After creating the branch,\r\n\r\n```python\r\nt.GetBranch(\"branch\").GetListOfLeaves()[0].SetTitle(\"anything\")\r\n```\r\n\r\nshould let you set the leaf title to anything. Does changing it to an arbitrary value break `TTree::Scan`?",
  "created_at":"2021-10-04T03:29:53Z",
  "id":933113474,
  "issue":457,
  "node_id":"IC_kwDOD6Q_ss43ni6C",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-04T03:29:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I would say it's \"mostly correct\". If you only get back the values with\r\n`[list(evt.branch) for evt in ch]` then it's correct, but `ch.Scan()/ch.Draw()` (pyROOT) will give the same (incorrect) behavior as on the ROOT REPL since as you say they're calling the same underlying code:\r\n```python\r\n>>> ch.Scan(\"*\")\r\n************************************\r\n*    Row   *   nbranch *    branch *\r\n************************************\r\n*        0 *         3 *       1.1 *\r\n*        1 *         0 *           *\r\n*        2 *         2 *       4.4 *\r\n*        3 *         1 *       6.6 *\r\n************************************\r\n>>> ch.Draw(\"Length$(branch)\") # gives 4 entries in the 1 bin\r\nInfo in <TCanvas::MakeDefCanvas>:  created default TCanvas with name c1\r\n>>> ch.Draw(\"branch\", \"\", \"goff\") # should give 6\r\n4\r\n```\r\n\r\nThanks for the line of code. Indeed it gives the same result as my `dims = ...` modification:\r\n```python\r\n>>> ch.GetBranch(\"branch\").GetListOfLeaves()[0].SetTitle(\"branch[nbranch]\")\r\n>>> ch.Scan(\"*\")\r\n***********************************************\r\n*    Row   * Instance *   nbranch *    branch *\r\n***********************************************\r\n*        0 *        0 *         3 *       1.1 *\r\n*        0 *        1 *         3 *       2.2 *\r\n*        0 *        2 *         3 *       3.3 *\r\n*        1 *        0 *         0 *           *\r\n*        2 *        0 *         2 *       4.4 *\r\n*        2 *        1 *         2 *       5.5 *\r\n*        3 *        0 *         1 *       6.6 *\r\n***********************************************\r\n>>> ch.Draw(\"Length$(branch)\") # bin counts of [1,1,1,1]\r\nInfo in <TCanvas::MakeDefCanvas>:  created default TCanvas with name c1\r\n>>> ch.Draw(\"branch\", \"\", \"goff\")\r\n6\r\n```\r\n\r\nand about breaking Scan(),\r\n```python\r\n>>> ch.GetBranch(\"branch\").GetListOfLeaves()[0].SetTitle(\"blah\")\r\n>>> ch.Scan(\"*\")\r\n************************************\r\n*    Row   *   nbranch *    branch *\r\n************************************\r\n*        0 *         3 *       1.1 *\r\n*        1 *         0 *           *\r\n*        2 *         2 *       4.4 *\r\n*        3 *         1 *       6.6 *\r\n************************************\r\n```",
  "created_at":"2021-10-04T03:38:24Z",
  "id":933115651,
  "issue":457,
  "node_id":"IC_kwDOD6Q_ss43njcD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-04T03:41:07Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"NONE",
  "body":"Not sure how `[list(evt.branch) for evt in ch]` gives the right answer when even TTreeReader fails:\r\n```cpp\r\nroot [1] TTreeReader reader(t);\r\nroot [2] TTreeReaderArray<double> b(reader, \"branch\");\r\nroot [3] while (reader.Next()) std::cout << b.GetSize() << \" \" << b[0] << std::endl;\r\n1 1.1\r\n1 1.1\r\n1 4.4\r\n1 6.6\r\n```",
  "created_at":"2021-10-04T03:51:46Z",
  "id":933118647,
  "issue":457,
  "node_id":"IC_kwDOD6Q_ss43nkK3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-04T03:51:46Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"MEMBER",
  "body":"The branch's leaf has a `fLeafCount` member that points to its counter, but `TTree::Scan`, `TTree::Draw`, and `TTreeReader` seem to be ignoring it or not using it exclusively to find the counter; you've shown that they depend on the branch's leaf's title as well. The title is not merely informative; those functions are evidently parsing it for behavior. PR #458 adds this information to the titles that Uproot writes so that these ROOT functions will work.\r\n\r\n(Ironically, the counter is not even needed. The information about how many subentries each entry has is also contained in the baskets of the jagged branch as offsets, rather than counts. Uproot completely ignores counters when reading jagged arrays.)",
  "created_at":"2021-10-04T14:00:38Z",
  "id":933515632,
  "issue":457,
  "node_id":"IC_kwDOD6Q_ss43pFFw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-04T14:00:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Makes sense. Thanks for the fast fix!",
  "created_at":"2021-10-04T21:41:37Z",
  "id":933878109,
  "issue":457,
  "node_id":"IC_kwDOD6Q_ss43qdld",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-04T21:41:37Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Uproot writing (`uproot.recreate`, `uproot.create`, and `uproot.update`) was introduced in version 4.1.0 (with an important bug-fix in 4.1.1). Upgrading is the only way to get it.\r\n\r\nI hope you don't mind me closing this issue quickly, but the above ought to fix it.",
  "created_at":"2021-10-05T17:19:36Z",
  "id":934607750,
  "issue":459,
  "node_id":"IC_kwDOD6Q_ss43tPuG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-05T17:19:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Do you know what characters ROOT doesn't support? (I'm assuming these are characters in the branch names.)\r\n\r\nIf it shows up in `TTree::Print` but not in the TBrowser, I'm inclined to take `TTree::Print` as the definition of \"ROOT supports it.\" That's the low-level view of the data.\r\n\r\nIn fact, since this is a proposal to emit a warning when creating branch names, the real question is whether you can create TTrees with these names using ROOT (directly with `TTree::Branch`). If ROOT can make these TTrees, but can't render them in TBrowser, that's an inconsistency within ROOT.",
  "created_at":"2021-10-06T14:19:49Z",
  "id":936324509,
  "issue":460,
  "node_id":"IC_kwDOD6Q_ss43zy2d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-06T14:19:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"My ROOT knowledge is very limited and doesn't go much beyond TBrowser, since I had last directly used ROOT ages ago. Usually I just convert the NTuples that I get from the basf2 framework directly to dataframes with uproot.\r\n\r\nTherefore, I don't have first-hand knowledge which characters ROOT supports, but the basf2 framework that I use has a [`makeROOTCompatible` function](https://github.com/belle2/basf2/blob/main/framework/utilities/src/MakeROOTCompatible.cc) (thankfully we are mirrored on github since recently, so I can link to it), which does character-replacement basf2-internal variable names in order to be able to store them in ROOT files. I'm not sure what exact ROOT limitations motivated that. I also have written a python version of that makeROOTCompatible function, where I just changed the C++ map to a python dict, if you want I can share that.\r\n\r\nI opened my original file in PyROOT and my variables with parantheses didn't show up in the python version of `TTree::Print`, i.e. `tree.Print`. However, strangely, creating a branch with parantheses worked and it showed up when printing, see the IPython session below. I also tried converting the ROOT file to pandas with `root_pandas.read_root`, which is based on PyROOT and there I also only got the columns `index`, `x` and `y`.\r\n\r\n``` python\r\nIn [30]: df = uproot.open(\"mc14_example_tuple.root\")[\"variables\"].arrays(library=\"pd\")                                                                                                                                            \r\nIn [32]: df.columns\r\nOut[32]: \r\nIndex(['index', '__experiment__', '__run__', '__event__',\r\n       'daughter(1, useCMSFrame(p))', 'extraInfo(decayModeID)',\r\n       'daughter(1, extraInfo(decayModeID))',\r\n       'daughter(1, daughter(0, extraInfo(decayModeID)))',\r\n       'daughter(0, extraInfo(decayModeID))', 'daughter(0, Mbc)',\r\n       'daughter(0, deltaE)', 'roeEextra(pt_mask)',\r\n       'daughter(1, daughter(0, M))', 'daughter(1, daughter(0, p))',\r\n       'nROE_Tracks(pt_mask)', 'daughter(1, daughter(1, p))',\r\n       'daughter(1, daughter(1, daughter(0, p)))',\r\n       'daughter(1, daughter(1, theta))',\r\n       'daughter(1, daughter(1, daughter(0, theta)))',\r\n       'daughter(1, daughter(1, useCMSFrame(E)))',\r\n       'daughter(1, daughter(0, useCMSFrame(E)))', 'm2RecoilSignalSide',\r\n       'daughter(1, daughter(1, PDG))', 'ifNANgiveX(foxWolframR2, 1)',\r\n       'isContinuumEvent', 'daughter(1, daughter(1, mcPDG))',\r\n       'daughter(1, daughter(1, genMotherPDG))',\r\n       'daughter(1, daughter(1, genMotherPDG(1)))',\r\n       'daughter(1, daughter(1, mcMother(mcDaughter(0, PDG))))',\r\n       'daughter(1, daughter(1, mcMother(mcDaughter(1, PDG))))',\r\n       'daughter(1, daughter(1, mcMother(mcDaughter(2, PDG))))',\r\n       'daughter(1, daughter(1, mcMother(mcDaughter(3, PDG))))',\r\n       'daughter(0, PDG)', 'E_miss', 'pre_bremscorr_lep_p',\r\n       'pre_bremscorr_lep_theta'],\r\n      dtype='object')\r\n      \r\nIn [38]: file = TFile(\"mc14_example_tuple.root\")\r\nIn [41]: tree = file.Get(\"variables\")\r\nIn [42]: tree.Print()\r\n******************************************************************************\r\n*Tree    :variables :                                                        *\r\n*Entries :     1000 : Total =           26034 bytes  File  Size =       8003 *\r\n*        :          : Tree compression factor =   3.93                       *\r\n******************************************************************************\r\n*Br    0 :index     : index/L                                                *\r\n*Entries :     1000 : Total  Size=       8570 bytes  File Size  =       1650 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   4.90     *\r\n*............................................................................*\r\n*Br    1 :x         : x/L                                                    *\r\n*Entries :     1000 : Total  Size=       8550 bytes  File Size  =       1646 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   4.90     *\r\n*............................................................................*\r\n*Br    2 :y         : y/D                                                    *\r\n*Entries :     1000 : Total  Size=       8550 bytes  File Size  =       2873 *\r\n*Baskets :        1 : Basket Size=      32000 bytes  Compression=   2.81     *\r\n*............................................................................*\r\n\r\n:        1 : Basket Size=      32000 bytes  Compression=   2.81     *\r\n*............................................................................*\r\n\r\nIn [44]: tree.Branch(\"test(test ( test) )\", np.arange(10), \"test(test ( test) )/I\")\r\nOut[44]: <cppyy.gbl.TBranch object at 0x55667328f160>\r\n\r\nIn [45]: tree.Show()\r\n======> EVENT:-1\r\n index           = 0\r\n x               = 0\r\n y               = 0\r\n test(test ( test) ) = 0\r\n\r\n```",
  "created_at":"2021-10-06T15:20:19Z",
  "id":936484403,
  "issue":460,
  "node_id":"IC_kwDOD6Q_ss430Z4z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-06T15:20:19Z",
  "user":"MDQ6VXNlcjUxMjE4MjQ="
 },
 {
  "author_association":"NONE",
  "body":"Ok I can't reproduce it... I just wanted to modify the data-frame so I can share it here without worries, but when I created the new ROOT file the branch names showed up in ROOT/TBrowser without any issue :man_shrugging: \r\n![image](https://user-images.githubusercontent.com/5121824/136240619-9af99391-701f-4a25-a385-0158202f9ba5.png)\r\n",
  "created_at":"2021-10-06T16:00:22Z",
  "id":936562616,
  "issue":460,
  "node_id":"IC_kwDOD6Q_ss430s-4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-06T16:00:22Z",
  "user":"MDQ6VXNlcjUxMjE4MjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm confused about the fact that the DataFrame does not have any columns named `\"x\"` and `\"y\"`, but these are the branches that `TTree::Print` shows. Okay, maybe `TTree::Print` can only show branches that satisfy a particular naming convention, so it should show a _subset_ of the branches. `\"x\"` and `\"y\"` aren't in the set.\r\n\r\nI was going to ask if there was a mistake in your tests (like, looking at the wrong file...), and the fact that you can now open them in TBrowser suggests that there was. No problem if it was; I do that kind of thing all the time.\r\n\r\nIn the end, is there an issue reading branches with weird names? If not, let's close the issue. Thanks!",
  "created_at":"2021-10-06T16:03:12Z",
  "id":936568111,
  "issue":460,
  "node_id":"IC_kwDOD6Q_ss430uUv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-06T16:03:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I was also thinking about closing this with my last comment but wanted to let you decide about it. When I opened this PR I thought these not-allowed special characters in ROOT would be common knowledge and thus didn't do many cross-checks of what I saw and just assumed it to be an uproot issue.\r\n\r\n> I was going to ask if there was a mistake in your tests (like, looking at the wrong file...), and the fact that you can now open them in TBrowser suggests that there was. \r\n\r\nWhat I could open in TBrowser was a new ROOT file that I newly created with `uproot.recreate` from my dataframe to see if I can reproduce the issue. However, the original ROOT file which sparked this issue still exists and I still see these weird x and y in there. I know uploaded it for reference here: https://syncandshare.desy.de/index.php/s/mBWYXK8yA9YXpP2\r\n\r\nHowever, I think I discovered some clues regarding the issue. I just noticed it has three partial trees (forgot the proper ROOT name for this):\r\n\r\n``` python\r\nIn [31]: f.keys()\r\nOut[31]: ['variables;1', 'variables;2', 'variables;3']\r\n```\r\n\r\nHowever, I found that when reading that in with uproot, I get the `x` and `y` when opening `variables;1`, otherwise I get the full column set (with `;2` and `;3` and when omitting the number):\r\n\r\n``` python\r\nIn [38]: f['variables;1'].keys()\r\nOut[38]: ['index', 'x', 'y']\r\n\r\nIn [39]: f['variables;2'].keys()\r\nOut[39]:\r\n['index',\r\n '__experiment__',\r\n '__run__',\r\n '__event__',\r\n 'daughter__bo1__cm__spuseCMSFrame__bop__bc__bc',\r\n 'extraInfo__bodecayModeID__bc',\r\n 'daughter__bo1__cm__spextraInfo__bodecayModeID__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo0__cm__spextraInfo__bodecayModeID__bc__bc__bc',\r\n 'daughter__bo0__cm__spextraInfo__bodecayModeID__bc__bc',\r\n 'daughter__bo0__cm__spMbc__bc',\r\n 'daughter__bo0__cm__spdeltaE__bc',\r\n 'roeEextra__bopt_mask__bc',\r\n 'daughter__bo1__cm__spdaughter__bo0__cm__spM__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo0__cm__spp__bc__bc',\r\n 'nROE_Tracks__bopt_mask__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spp__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spdaughter__bo0__cm__spp__bc__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__sptheta__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spdaughter__bo0__cm__sptheta__bc__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spuseCMSFrame__boE__bc__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo0__cm__spuseCMSFrame__boE__bc__bc__bc',\r\n 'm2RecoilSignalSide',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spPDG__bc__bc',\r\n 'ifNANgiveX__bofoxWolframR2__cm__sp1__bc',\r\n 'isContinuumEvent',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spmcPDG__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spgenMotherPDG__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spgenMotherPDG__bo1__bc__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spmcMother__bomcDaughter__bo0__cm__spPDG__bc__bc__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spmcMother__bomcDaughter__bo1__cm__spPDG__bc__bc__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spmcMother__bomcDaughter__bo2__cm__spPDG__bc__bc__bc__bc',\r\n 'daughter__bo1__cm__spdaughter__bo1__cm__spmcMother__bomcDaughter__bo3__cm__spPDG__bc__bc__bc__bc',\r\n 'daughter__bo0__cm__spPDG__bc',\r\n 'E_miss',\r\n 'pre_bremscorr_lep_p',\r\n 'pre_bremscorr_lep_theta']\r\n\r\nIn [40]: f['variables'].keys()\r\nOut[40]:\r\n['index',\r\n '__experiment__',\r\n '__run__',\r\n '__event__',\r\n 'daughter(1, useCMSFrame(p))',\r\n 'extraInfo(decayModeID)',\r\n 'daughter(1, extraInfo(decayModeID))',\r\n 'daughter(1, daughter(0, extraInfo(decayModeID)))',\r\n 'daughter(0, extraInfo(decayModeID))',\r\n 'daughter(0, Mbc)',\r\n 'daughter(0, deltaE)',\r\n 'roeEextra(pt_mask)',\r\n 'daughter(1, daughter(0, M))',\r\n 'daughter(1, daughter(0, p))',\r\n 'nROE_Tracks(pt_mask)',\r\n 'daughter(1, daughter(1, p))',\r\n 'daughter(1, daughter(1, daughter(0, p)))',\r\n 'daughter(1, daughter(1, theta))',\r\n 'daughter(1, daughter(1, daughter(0, theta)))',\r\n 'daughter(1, daughter(1, useCMSFrame(E)))',\r\n 'daughter(1, daughter(0, useCMSFrame(E)))',\r\n 'm2RecoilSignalSide',\r\n 'daughter(1, daughter(1, PDG))',\r\n 'ifNANgiveX(foxWolframR2, 1)',\r\n 'isContinuumEvent',\r\n 'daughter(1, daughter(1, mcPDG))',\r\n 'daughter(1, daughter(1, genMotherPDG))',\r\n 'daughter(1, daughter(1, genMotherPDG(1)))',\r\n 'daughter(1, daughter(1, mcMother(mcDaughter(0, PDG))))',\r\n 'daughter(1, daughter(1, mcMother(mcDaughter(1, PDG))))',\r\n 'daughter(1, daughter(1, mcMother(mcDaughter(2, PDG))))',\r\n 'daughter(1, daughter(1, mcMother(mcDaughter(3, PDG))))',\r\n 'daughter(0, PDG)',\r\n 'E_miss',\r\n 'pre_bremscorr_lep_p',\r\n 'pre_bremscorr_lep_theta']\r\n```\r\nThe `variables;1` was probably created when I tested whether `uproot.recreate` works by copy-pasting some example code from the uproot getting started guide. So when I use `recreate` again , I guess it creates a new tree with the same name and when reading again (omitting the number after `;`) ignores the old trees?\r\n\r\nThe weird thing is that in root/TBrowser, I get the `x` and `y` still in all three trees:\r\n![image](https://user-images.githubusercontent.com/5121824/136287248-b727328c-5721-4635-ae1d-1a2e5716ac8d.png)\r\n\r\nAnd same when reading the tree while omitting the `;`-number with ROOT. There is definitely some of my stupidity involved, but it also looks like there is some inconsistent behaviour which I don't fully understand.\r\n\r\nIf with my new discoveries the issue is obvious for you, I would like to hear your theory, otherwise, don't feel pressured to open the linked ROOT file and waste your time on this. And in any case this particular issue with this name should stay closed.",
  "created_at":"2021-10-06T21:55:16Z",
  "id":937216282,
  "issue":460,
  "node_id":"IC_kwDOD6Q_ss433Mka",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-06T21:56:54Z",
  "user":"MDQ6VXNlcjUxMjE4MjQ="
 },
 {
  "author_association":"MEMBER",
  "body":"@oshadura ",
  "created_at":"2021-10-07T17:19:03Z",
  "id":937996975,
  "issue":463,
  "node_id":"IC_kwDOD6Q_ss436LKv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-07T17:19:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Actually, the problem was that I totally forgot to add the streamers for TTree*, and all of my tests had a modern version of ROOT installed. Now it should be implemented in PR #472. I can't test it with an old version of ROOT, at least not one old enough to have TTree version < 20, because of when ROOT was first added to conda-forge. If you check out that branch, you can check it. (Also, I have it set to auto-merge, so if the branch is already merged, you can just check out `main`.)\r\n\r\n*Now that I think about it, several months separated the implementation of generic ROOT I/O, which is focused on things like streamers, and the implementation of TTree-writing. That's probably why I forgot.",
  "created_at":"2021-10-13T19:26:18Z",
  "id":942645999,
  "issue":471,
  "node_id":"IC_kwDOD6Q_ss44L6Lv",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-10-13T19:26:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I can confirm that this does now work properly on an old version (6.08/06) of ROOT, thank you!",
  "created_at":"2021-10-13T20:39:52Z",
  "id":942703205,
  "issue":471,
  "node_id":"IC_kwDOD6Q_ss44MIJl",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-13T20:39:52Z",
  "user":"MDQ6VXNlcjM2OTc3ODc5"
 },
 {
  "author_association":"MEMBER",
  "body":"Great, and it's merged, too. I'll make a new release because streamer info is important. We don't want a lot of files out there to not have it. Thanks for alerting me to this issue!",
  "created_at":"2021-10-13T20:48:31Z",
  "id":942708790,
  "issue":471,
  "node_id":"IC_kwDOD6Q_ss44MJg2",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-10-13T20:48:31Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Assertions (and NotImplementedErrors) are not user errors. ValueErrors and IndexErrors usually are, and often KeyErrors are (trying to read an object that doesn't exist, of a misspelling), but internal missing values are also fairly common, so it's hard to say with KeyErrors. However, assertions are deliberately put into the code by a programmer who thinks, \"This can't happen, so I'll assert to stop a runaway process.\" I can't think of a legitimate reason an assertion would be the user's fault.\r\n\r\nThis one is complaining that the \"cycle number\" of an object, probably that TTree because it's the only thing around, is 0. They're supposed to start at 1, so either that was a mistake in the writing step or the reading step is not on the byte it thinks it is (\"got off the rails\"), and it's interpreting some 0 as the cycle number. I'll take a look.",
  "created_at":"2021-10-14T11:34:37Z",
  "id":943272045,
  "issue":474,
  "node_id":"IC_kwDOD6Q_ss44OTBt",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-14T11:34:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"When I tried the exact code above, I didn't get the error:\r\n\r\n```python\r\n>>> import numpy\r\n>>> import uproot\r\n>>> b1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n>>> b2 = [0.0, 1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9]\r\n>>> with uproot.recreate(\"test.root\", compression=None) as fout:\r\n...     tree = fout.mktree(\"t\", {\"b1\": numpy.int32, \"b2\": numpy.float64}, \"title\")\r\n...     tree.extend({\"b1\": b1, \"b2\": b2})\r\n... \r\n>>> with uproot.update(\"test.root\") as fout:\r\n...     tree = fout.mktree(\"t2\", {\"b1\": numpy.int32, \"b2\": numpy.float64}, \"title\")\r\n...     tree.extend({\"b1\": b1, \"b2\": b2})\r\n... \r\n>>> print(\"?\")\r\n?\r\n>>> uproot.__version__\r\n'4.1.4'\r\n```\r\n\r\nBut I investigated it further and found that the first TTree is rendered unreadable by the second. As it turns out, the second was overwriting the places in the file that the first wrote to, because I forgot to update the \"FreeSegments\" record that declares these to be off limits for future writing. That should be fixed in PR #475.\r\n\r\nNothing specific should have changed between 4.1.3 and 4.1.4 that would cause this difference in behavior, but not declaring the first TTree's data as \"in use\" (the on-disk equivalent of writing to memory without `malloc` to ensure that something else doesn't write there) is undefined behavior, and some incidental difference between 4.1.3 and 4.1.4 might explain it.\r\n\r\nAnyway, it should be fixed now, and this is important so it will be released as Uproot 4.1.5.",
  "created_at":"2021-10-14T13:44:25Z",
  "id":943372617,
  "issue":474,
  "node_id":"IC_kwDOD6Q_ss44OrlJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-14T13:44:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks @jpivarski!",
  "created_at":"2021-10-14T14:45:45Z",
  "id":943429542,
  "issue":474,
  "node_id":"IC_kwDOD6Q_ss44O5em",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-14T14:45:45Z",
  "user":"MDQ6VXNlcjE2MTg1MzA="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @duncanmmacleod for infra",
  "created_at":"2021-10-27T21:46:38Z",
  "id":953335567,
  "issue":477,
  "node_id":"IC_kwDOD6Q_ss440r8P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-27T21:46:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/489) to add @duncanmmacleod! :tada:",
  "created_at":"2021-10-27T21:46:47Z",
  "id":953335640,
  "issue":477,
  "node_id":"IC_kwDOD6Q_ss440r9Y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-27T21:46:47Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"I'd recommend adding this version cap to the extras in setup.py, too.",
  "created_at":"2021-10-28T03:58:15Z",
  "id":953480912,
  "issue":478,
  "node_id":"IC_kwDOD6Q_ss441PbQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T03:58:15Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Good point! I'll do that.",
  "created_at":"2021-10-28T11:31:34Z",
  "id":953758168,
  "issue":478,
  "node_id":"IC_kwDOD6Q_ss442THY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T11:31:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I've just updated it to see how it would look like without re-raising the exception.  For the callback it looks natural, as the futures are simply updated with the exception if there is an error, or with the correct data chunks otherwise.",
  "created_at":"2021-10-20T16:42:20Z",
  "id":947844223,
  "issue":480,
  "node_id":"IC_kwDOD6Q_ss44fvR_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T16:42:20Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sounds good @jpivarski, it looks good to me.\r\n\r\nI agree with the difficulty of the testing.  I tried it here setting the timeouts to low values to trigger Operation Expired errors. The current pr does recover from those errors, but it was hard to reliably test it.\r\n",
  "created_at":"2021-10-20T17:46:36Z",
  "id":947897400,
  "issue":480,
  "node_id":"IC_kwDOD6Q_ss44f8Q4",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T17:46:36Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, then I'll take it once pre-commit passes (https://results.pre-commit.ci/run/github/262422450/1634748035.QZ3W9c9lRrKb9ZLne5bcAQ). It's failing because you declared `e` (the exception object) and then didn't use it.\r\n\r\nI'll turn on auto-merge, so that it merges once that's fixed.",
  "created_at":"2021-10-20T17:48:23Z",
  "id":947898704,
  "issue":480,
  "node_id":"IC_kwDOD6Q_ss44f8lQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T17:48:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This thing is so flaky...\r\n\r\n@all-contributors please add @btovar for code! Open says-a-me! A la peanut butter sandwiches!",
  "created_at":"2021-10-20T17:50:56Z",
  "id":947900541,
  "issue":480,
  "node_id":"IC_kwDOD6Q_ss44f9B9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T17:50:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @btovar for code",
  "created_at":"2021-10-20T17:57:59Z",
  "id":947905496,
  "issue":480,
  "node_id":"IC_kwDOD6Q_ss44f-PY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T17:57:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"`except Exception:` rather than `except:` or `except Exception as e:`. Sorry about the pickiness.",
  "created_at":"2021-10-20T17:58:55Z",
  "id":947906210,
  "issue":480,
  "node_id":"IC_kwDOD6Q_ss44f-ai",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T17:58:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Great, thanks Jim.",
  "created_at":"2021-10-20T18:24:22Z",
  "id":947925667,
  "issue":480,
  "node_id":"IC_kwDOD6Q_ss44gDKj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T18:24:22Z",
  "user":"MDQ6VXNlcjMwODE4MjY="
 },
 {
  "author_association":"MEMBER",
  "body":"TNonSplitBrowsable is not the type of the data, it's a detail of how the TBrowser works.\r\n\r\nIf it's failing with a KeyError, especially a KeyErrorInFile (which is a subclass of KeyError), then it isn't that the type is unreadable; it's that no object has that name. A KeyErrorInFile provides a (sorted) list of alternatives, in case it was spelled wrong or is otherwise named differently.\r\n\r\nBy \"before\" and \"after,\" are you talking about different files or different Uproot versions? It might help to list everything in the TTree with [TTree.show](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#show) or [TTree.keys](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TTree.TTree.html#keys). That will list all of the names, so that you can see if yours is in there.\r\n\r\nIf you see the object's name and it's a KeyError anyway, print that information here. Copy-pasted error output (with [triple backticks for formatting](https://docs.github.com/en/github/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks#fenced-code-blocks)) from Uproot is more useful than screenshots from ROOT.",
  "created_at":"2021-10-20T20:20:20Z",
  "id":948006240,
  "issue":482,
  "node_id":"IC_kwDOD6Q_ss44gW1g",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T20:20:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the quick reply! \r\n\r\nThis is the error message I'm getting:\r\n\r\n```\r\n`uproot.exceptions.KeyInFileError: not found: 'fX'\r\n\r\n    Available keys: (none!)`\r\n```\r\nWhen running Tree.Show() I get the output:\r\n\r\n`hitPosExtrap         | TVector3                 | AsStridedObjects(Model_TVec...`\r\n\r\nThe fX I'm trying to access is within this hitPosExtrap. I used to be able to access it as 'hitPosExtrap/fX' when reading it in, but not this fails. When I mentioned 'different version' I meant different ROOT versions, as I think moving from ROOT 5 to ROOT 6 means that this is not a TNonSplitBrowsable in TBrowser, when it used to be TBranchElement.\r\n\r\nWhen I look at a older file (made using ROOT 5), this is what I get when I input tree.show()\r\n\r\n```\r\nhitPosExtrap         | TVector3                 | AsGroup(<TBranchElement 'hi...\r\nhitPosExtrap/TObject | (group of fUniqueID:u... | AsGroup(<TBranchElement 'TO...\r\nhitPosExtrap/TObj... | uint32_t                 | AsDtype('>u4')\r\nhitPosExtrap/TObj... | uint8_t                  | AsDtype('uint8')\r\nhitPosExtrap/fX      | double                   | AsDtype('>f8')\r\nhitPosExtrap/fY      | double                   | AsDtype('>f8')\r\nhitPosExtrap/fZ      | double                   | AsDtype('>f8')\r\n```\r\n\r\nAnd the only difference I can see is that fX is TNonSplitBrowsable in the new case, and TBranchElement in the old case (where uproot is able to read this branch in).",
  "created_at":"2021-10-20T20:28:05Z",
  "id":948011716,
  "issue":482,
  "node_id":"IC_kwDOD6Q_ss44gYLE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T20:33:54Z",
  "user":"MDQ6VXNlcjczMTc1MTI2"
 },
 {
  "author_association":"MEMBER",
  "body":"It's still some instance of TBranch (possibly a subclass like TBranchElement), [TNonSplitBrowsable](https://root.cern/doc/master/classTNonSplitBrowsable.html) isn't any kind of TBranch. That's just how the TBrowser is displaying its context menu; it can't be the type of the data themselves.\r\n\r\nYou can read these data into an array using `\"hitPosExtrap\"` as the key name. In the Awkward Array that you get out, you can say `.fX` to get the x components. Supposing your tree is `t`,\r\n\r\n```python\r\nt[\"hitPosExtrap\"].array().fX\r\n```\r\n\r\nought to do it.\r\n\r\nThe difference with respect to ROOT 5 is related to splitting: the object used to be written as a split object\u2014each private member in a different subbranch\u2014but now it's being written as an unsplit object\u2014you have to read the whole thing to get any component. That's probably why the TBrowser is showing different words in its context menu.\r\n\r\nIf you can read the data with the above, feel free to close the issue. Thanks!",
  "created_at":"2021-10-20T20:37:20Z",
  "id":948017838,
  "issue":482,
  "node_id":"IC_kwDOD6Q_ss44gZqu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T20:37:20Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Yep that works, thanks for the help! I'll close the issue.",
  "created_at":"2021-10-20T20:41:58Z",
  "id":948020961,
  "issue":482,
  "node_id":"IC_kwDOD6Q_ss44gabh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-20T20:41:58Z",
  "user":"MDQ6VXNlcjczMTc1MTI2"
 },
 {
  "author_association":"MEMBER",
  "body":"Not \"nearly\" deprecated! (The switchover was December 5, 2020.)\r\n\r\nI think this is the bug in Uproot 4 that was fixed in scikit-hep/uproot4#472, Uproot release [4.1.4](https://github.com/scikit-hep/uproot4/releases/tag/4.1.4) (but [4.1.5](https://github.com/scikit-hep/uproot4/releases/tag/4.1.5) is the latest). Before that fix, Uproot 4 was not writing streamer info for TTrees, so any reader that did not have TTree version 20 built-in would not be able to read it. The latest version of ROOT and Uproot have TTree streamers bulit-in, but Uproot 3 did not.\r\n\r\nSo that's my guess. Good luck!",
  "created_at":"2021-10-21T12:22:41Z",
  "id":948560806,
  "issue":483,
  "node_id":"IC_kwDOD6Q_ss44ieOm",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-21T12:22:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Not \"nearly\" deprecated! (The switchover was December 5, 2020.)\r\n> \r\n> I think this is the bug in Uproot 4 that was fixed in #472, Uproot release [4.1.4](https://github.com/scikit-hep/uproot4/releases/tag/4.1.4) (but [4.1.5](https://github.com/scikit-hep/uproot4/releases/tag/4.1.5) is the latest). Before that fix, Uproot 4 was not writing streamer info for TTrees, so any reader that did not have TTree version 20 built-in would not be able to read it. The latest version of ROOT and Uproot have TTree streamers bulit-in, but Uproot 3 did not.\r\n> \r\n> So that's my guess. Good luck!\r\n\r\nThanks for your quick reply! Indeed using the latest uproot 4.1.5 fixes this issue (And thanks for pointing the link! I see it is discussed in the release note of 4.1.4)",
  "created_at":"2021-10-21T12:33:53Z",
  "id":948569954,
  "issue":483,
  "node_id":"IC_kwDOD6Q_ss44igdi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-21T12:33:53Z",
  "user":"MDQ6VXNlcjQ0ODg1NDAw"
 },
 {
  "author_association":"MEMBER",
  "body":"I did a test and got no errors, though I had to make up a starting point; I should find this `cernstaff.root` file and start with that.\r\n\r\n```\r\n% python\r\nPython 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import ROOT\r\n>>> file = ROOT.TFile(\"tmp.root\", \"recreate\")\r\n>>> h = ROOT.TH1F(\"h\", \"title\", 100, -5, 5)\r\n>>> h.Write()\r\n261\r\n>>> file.Close()\r\n>>> \r\n% python\r\nPython 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import pandas as pd\r\n>>> import uproot\r\n>>> df = pd.DataFrame({\"b1\": [1, 2, 3], \"b2\": [4, 5, 6]})\r\n>>> df\r\n   b1  b2\r\n0   1   4\r\n1   2   5\r\n2   3   6\r\n>>> file = uproot.update(\"tmp.root\")\r\n>>> file.mktree(\"t\", {\"b1\": \"i8\", \"b2\": \"i8\"}, \"title\")\r\n<WritableTree '/t' at 0x7fe5b4afe3a0>\r\n>>> file[\"t\"].extend({\"b1\": df.loc[:, \"b1\"], \"b2\": df.loc[:, \"b2\"]})\r\n>>> file[\"t2\"] = df\r\n>>> \r\n% python\r\nPython 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot\r\n>>> file = uproot.open(\"tmp.root\")\r\n>>> file[\"t\"].arrays(library=\"pd\")\r\n   b1  b2\r\n0   1   4\r\n1   2   5\r\n2   3   6\r\n>>> file[\"t2\"].arrays(library=\"pd\")\r\n   index  b1  b2\r\n0      0   1   4\r\n1      1   2   5\r\n2      2   3   6\r\n>>> file[\"h\"]\r\n<TH1F (version 3) at 0x7f8a2267d070>\r\n```\r\n\r\nYour code examples have some copy-paste errors, such as an unclosed quotation mark. One that I'm not sure of is that you are using a TTree's title to try to access it, `file[\"title\"]`, rather than its name `file[\"t\"]`. Titles are optional and not used for lookup. The result of assigning a DataFrame directly, as opposed to splitting it up into branches, is different because we try to preserve the index information in the TTree, but that becomes a column when you read it back as a DataFrame (so you'll need to do a [DataFrame.set_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html) to fully round-trip it.)",
  "created_at":"2021-10-26T15:18:40Z",
  "id":952046125,
  "issue":485,
  "node_id":"IC_kwDOD6Q_ss44vxIt",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-26T15:18:40Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, and your use of `with` statements is correct; it's _more_ cautious than I'm being here. However, I just happen to know that Uproot doesn't have any extra clean-up to do after an object has been assigned into it, or after a `mkdir`, `mktree`, `extend` function has been called.",
  "created_at":"2021-10-26T15:20:44Z",
  "id":952048078,
  "issue":485,
  "node_id":"IC_kwDOD6Q_ss44vxnO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-26T15:20:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I found [cernstaff.dat](https://github.com/root-project/root/blob/master/tutorials/tree/cernstaff.dat) and [cernbuild.C](https://github.com/root-project/root/blob/master/tutorials/tree/cernbuild.C) to make a `cernstaff.root` file, and I repeated the above using that file as a base. My ROOT is 6.24/04, so this is a modern ROOT file.\r\n\r\n```\r\n% ls\r\ncernstaff.dat  cernbuild.C  cernstaff.root\r\n% python\r\nPython 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame({\"b1\": [1, 2, 3], \"b2\": [4, 5, 6]})\r\n>>> df\r\n   b1  b2\r\n0   1   4\r\n1   2   5\r\n2   3   6\r\n>>> file = uproot.update(\"cernstaff.root\")\r\n>>> file.mktree(\"t\", {\"b1\": \"i8\", \"b2\": \"i8\"}, \"title\")\r\n<WritableTree '/t' at 0x7fd6ab3dc760>\r\n>>> file[\"t\"].extend({\"b1\": df.loc[:, \"b1\"], \"b2\": df.loc[:, \"b2\"]})\r\n>>> file[\"t2\"] = df\r\n>>> \r\n% python\r\nPython 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot\r\n>>> file = uproot.open(\"cernstaff.root\")\r\n>>> file[\"t\"].arrays(library=\"pd\")\r\n   b1  b2\r\n0   1   4\r\n1   2   5\r\n2   3   6\r\n>>> file[\"t2\"].arrays(library=\"pd\")\r\n   index  b1  b2\r\n0      0   1   4\r\n1      1   2   5\r\n2      2   3   6\r\n>>> file[\"T\"].arrays(library=\"pd\")\r\n      Category  Flag  Age  Service  ...  Hrweek   Cost  Division  Nation\r\n0          202    15   58       28  ...      40  11975        PS      DE\r\n1          530    15   63       33  ...      40  10228        EP      CH\r\n2          316    15   56       31  ...      40  10730        PS      FR\r\n3          361    15   61       35  ...      40   9311        PS      FR\r\n4          302    15   52       24  ...      40   9966        PS      DE\r\n...        ...   ...  ...      ...  ...     ...    ...       ...     ...\r\n3349       500     7   51        0  ...      40  16995        DG      FR\r\n3350       415     5   25        0  ...      40   4631       TIS      FR\r\n3351       565     4   35        0  ...      20   3053        DD      FR\r\n3352       204     3   28        0  ...      40   6981        EP      DK\r\n3353       500     5   43        0  ...      40  12716        DG      ZZ\r\n\r\n[3354 rows x 11 columns]\r\n```",
  "created_at":"2021-10-26T15:31:41Z",
  "id":952058290,
  "issue":485,
  "node_id":"IC_kwDOD6Q_ss44v0Gy",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-26T15:31:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(I narrowly avoided a naming collision: I've been naming this small TTree as lower-case `\"t\"` and the cernstaff.root TTree is named capital `\"T\"`. If they had been the same name, they would both be in the file, distinguishable only by cycle number; I'd have to say `file[\"T;1\"]` vs `file[\"T;2\"]`.)",
  "created_at":"2021-10-26T15:33:26Z",
  "id":952059839,
  "issue":485,
  "node_id":"IC_kwDOD6Q_ss44v0e_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-26T15:33:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I didn't realise I had to use the file name rather than title - thank you! However, I am still getting the error - it's when I open the updated file in ROOT. I've changed my code to almost exactly what you've done, with another open statement to show I can reopen the file in python and access the added data:\r\n```\r\nwith uproot.open(\"cernstaff.root\") as file1:\r\n  df1 = file1[\"T\"].arrays([\"Age\"], library='pd')\r\n  df1[\"label\"] = 0\r\n  df1[\"source\"] = 1\r\n  print (df1)\r\nwith uproot.update(\"cernstaff.root\") as file2:\r\n  file2.mktree(\"M\", {\"label\":\"i8\", \"source\":\"i8\"}, \"ML_data\")\r\n  file2[\"M\"].extend({\"label\":df1.loc[:,\"label\"], \"source\":df1.loc[:,\"source\"]})\r\nwith uproot.open(\"cernstaff.root\") as file3:\r\n  df = file3[\"M\"].arrays(library=\"pd\")\r\n  print (df)\r\n```\r\nThis runs with no errors, however I still get the same error when I try and attach the file and open it in ROOT (i.e. just doing `root cernstaff.root`):\r\n```\r\nAttaching file cernstaff.root as _file0...\r\nError in <TBufferFile::CheckByteCount>: object of class TList read too few bytes: 17533 instead of 17919\r\nError in <TBufferFile::CheckByteCount>: Byte count probably corrupted around buffer position 64:\r\n\t17919 for a possible maximum of -5\r\n(TFile *) 0x557bbce39830\r\n```\r\nI'm using ROOT 6.18/00 as my collaboration uses it for most other work - would that be an issue? The file does still attach but accessing the new data gives a lot of warnings. ",
  "created_at":"2021-10-27T12:32:11Z",
  "id":952878414,
  "issue":485,
  "node_id":"IC_kwDOD6Q_ss44y8VO",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-27T12:32:11Z",
  "user":"MDQ6VXNlcjc0NDIyMzc3"
 },
 {
  "author_association":"MEMBER",
  "body":"There were a few bugs here, but none of them were related to Pandas or prematurely closed files.\r\n\r\n   * One was that the `ROOT::TIOFeatures` TStreamerInfo was written twice, once with the wrong name.\r\n   * Another was that the string data included among the TStreamerInfo was not counted in the list of TStreamerInfo's size, and that's why ROOT couldn't read the file: it allocated a block of data that was too small and then ran off the end of it. Uproot does the same thing if we explicitly ask to read the TStreamerInfo (we don't if we don't have to, which is why the bug didn't show up earlier). We only see this when updating a file that had TTrees in it because only TTrees put string data in the TStreamerInfo list. (Other cases, with histograms, have been tested.)\r\n   * Beyond that, the strings in the TStreamerInfo have to be kept in the order ROOT made them because Uproot doesn't update cross-references. The class object for the strings (not the strings themselves) is cross-referenced.\r\n   * Then, after that, if a TBranch is configured to be compressed (`fCompress != 0`) but the data in a TBasket are not compressed\u2014because small enough data are smaller when uncompressed, an optimization ROOT does, too\u2014then ROOT was trying to read it as compressed data, which it is not. This one was the most perplexing: there must be a bit somewhere that tells ROOT \"this TBasket is uncompressed; don't try to decompress it,\" but I couldn't find that bit: the TBaskets ROOT made with this optimization were identical to the ones Uproot made, apart from the timestamps. This also only comes up in TTrees written to files that updated preexisting files (regardless of whether ROOT or Uproot originally created the file). So I punted on this, taking advantage of a fact about ROOT: if you _declare_ a TBranch to be uncompressed (`fCompress == 0`), you can still put compressed data in it because ROOT double-checks it.\r\n   * There's one more that I didn't fix: ROOT is complaining about the TLeafL streamer differing from the TLeafL serialization by 8 bytes. It only complains on updated files (regardless of whether ROOT or Uproot originally created the file), and it only complains about TLeafL. The streamer is correct, and because of this, ROOT can read the data. I don't know what's going on here. It's an error message, but it doesn't prevent you from reading the data, so I gave up on it.\r\n\r\nHere's a demonstration:\r\n\r\n```\r\n% python\r\nPython 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame({\"b1\": [1.1, 2.2, 3.3], \"b2\": [4.4, 5.5, 6.6]})\r\n>>> df\r\n    b1   b2\r\n0  1.1  4.4\r\n1  2.2  5.5\r\n2  3.3  6.6\r\n>>> file = uproot.update(\"cernstaff.root\")\r\n>>> file.mktree(\"t\", {\"b1\": \"f8\", \"b2\": \"f8\"}, \"title\")\r\n<WritableTree '/t' at 0x7ff2a58fb730>\r\n>>> file[\"t\"].extend({\"b1\": df.loc[:, \"b1\"], \"b2\": df.loc[:, \"b2\"]})\r\n>>> file[\"t2\"] = df\r\n>>> \r\n% python\r\nPython 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import ROOT\r\n>>> file = ROOT.TFile(\"cernstaff.root\")\r\n>>> t = file.Get(\"t\")\r\n>>> t.Scan()\r\n************************************\r\n*    Row   *     b1.b1 *     b2.b2 *\r\n************************************\r\n*        0 *       1.1 *       4.4 *\r\n*        1 *       2.2 *       5.5 *\r\n*        2 *       3.3 *       6.6 *\r\n************************************\r\n3\r\n>>> t2 = file.Get(\"t2\")\r\nError in <TBufferFile::CheckByteCount>: object of class TLeafL read too many bytes: 78 instead of 70\r\nTBufferFile::CheckByteCount:0: RuntimeWarning: TLeafL::Streamer() not in sync with data on file cernstaff.root, fix Streamer()\r\n>>> t2.Scan()\r\n************************************************\r\n*    Row   * index.ind *     b1.b1 *     b2.b2 *\r\n************************************************\r\n*        0 *         0 *       1.1 *       4.4 *\r\n*        1 *         1 *       2.2 *       5.5 *\r\n*        2 *         2 *       3.3 *       6.6 *\r\n************************************************\r\n3\r\n>>> T = file.Get(\"T\")\r\n>>> T.Scan()\r\n************************************************************************************************************\r\n*    Row   * Category. * Flag.Flag *   Age.Age * Service.S * Children. * Grade.Gra * Step.Step * Hrweek.Hr *\r\n************************************************************************************************************\r\n*        0 *       202 *        15 *        58 *        28 *         0 *        10 *        13 *        40 *\r\n*        1 *       530 *        15 *        63 *        33 *         0 *         9 *        13 *        40 *\r\n*        2 *       316 *        15 *        56 *        31 *         2 *         9 *        13 *        40 *\r\n...\r\n```",
  "created_at":"2021-10-27T21:35:45Z",
  "id":953329666,
  "issue":485,
  "node_id":"IC_kwDOD6Q_ss440qgC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-27T21:35:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Is the `xrootd` package actually `pyxrootd` for the versions before 4.11.1 ([see list](https://pypi.org/project/xrootd/#history))? Uproot never imports `pyxrootd`; it (now) always uses the name `xrootd`.\r\n\r\nIf you have `xrootd < 5.2.0`, then you'll get the fix and the warning; if you have a newer `xrootd`, you won't, and if you don't have a package named `xrootd`\u2014perhaps you have a package named `pyxrootd`, but we don't look at that\u2014then you just won't be able to use `root://` URLs, right?",
  "created_at":"2021-10-26T16:32:02Z",
  "id":952112140,
  "issue":486,
  "node_id":"IC_kwDOD6Q_ss44wBQM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-26T16:32:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"That would be logical, but yeah, this is where things get weird:\r\n\r\n```python\r\n>>> import XRootD\r\n>>> XRootD.client\r\n<module 'XRootD.client' from '/home/mproffit/miniconda3/envs/xrootd_test/lib/python3.8/site-packages/XRootD/client/__init__.py'>\r\n\r\n>>> import pkg_resources\r\n>>> pkg_resources.get_distribution('XRootD')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/mproffit/miniconda3/envs/xrootd_test/lib/python3.8/site-packages/pkg_resources/__init__.py\", line 466, in get_distribution\r\n    dist = get_provider(dist)\r\n  File \"/home/mproffit/miniconda3/envs/xrootd_test/lib/python3.8/site-packages/pkg_resources/__init__.py\", line 342, in get_provider\r\n    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\r\n  File \"/home/mproffit/miniconda3/envs/xrootd_test/lib/python3.8/site-packages/pkg_resources/__init__.py\", line 886, in require\r\n    needed = self.resolve(parse_requirements(requirements))\r\n  File \"/home/mproffit/miniconda3/envs/xrootd_test/lib/python3.8/site-packages/pkg_resources/__init__.py\", line 772, in resolve\r\n    raise DistributionNotFound(req, requirers)\r\npkg_resources.DistributionNotFound: The 'XRootD' distribution was not found and is required by the application\r\n\r\n>>> pkg_resources.get_distribution('pyxrootd')\r\npyxrootd 4.11.0 (/home/mproffit/miniconda3/envs/xrootd_test/lib/python3.8/site-packages)\r\n\r\n>>> import uproot\r\n>>> uproot.__version__\r\n'4.1.5'\r\n>>> uproot.open(\"root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root\").keys()\r\n['Events;1']\r\n```\r\n\r\nThis is with `conda install -c conda-forge xrootd==4.11.0`.",
  "created_at":"2021-10-26T17:25:49Z",
  "id":952153585,
  "issue":486,
  "node_id":"IC_kwDOD6Q_ss44wLXx",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-26T17:25:49Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"So for this version of the package, `pkg_resources.get_distribution('XRootD')` raises an exception, `pkg_resources.get_distribution('pyxrootd')` returns a value, and `import XRootD` works? I don't know how that's possible; I would have thought that the `pkg_resources` name would have to be the `import` name.\r\n\r\nIt could be acceptable to stop supporting XRootD < 5.2.0 (there seems to be some confusion above as to whether it's 5.1.0 or 5.2.0). What do you think? Do you know of any groups of users who might be stuck on an old XRootD for some reason? (E.g. glued to a collaboration's software distribution version or something?)",
  "created_at":"2021-10-26T17:45:23Z",
  "id":952168441,
  "issue":486,
  "node_id":"IC_kwDOD6Q_ss44wO_5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-26T17:45:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yeah, I just checked the latest ATLAS AnalsisBase image and it has 4.12.2. Right now, I'm dealing with old versions of XRootD because that's what's in the ServiceX Uproot transformer image. It's currently using 4.11.0, and updating up through 5.1.1 seems to work fine, but updating to v5.2.0 or higher looks like it's going to require updating `conda` and bring a lot of new dependency conflicts.\r\n\r\nMy inclination would be to just keep the limited support in but to add a `try` for `pkg_resources.get_distribution('pyxrootd')` after `try`ing `'XRootD'` so that earlier versions still get detected and have the workaround applied and show the warning.",
  "created_at":"2021-10-26T21:16:34Z",
  "id":952333680,
  "issue":486,
  "node_id":"IC_kwDOD6Q_ss44w3Vw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-26T21:16:34Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"That sounds good. If you add the check for both \"pyxrootd\" and \"XRootD\" in a PR, I'll review it and release a new version of Uproot quickly.",
  "created_at":"2021-10-26T21:27:39Z",
  "id":952341363,
  "issue":486,
  "node_id":"IC_kwDOD6Q_ss44w5Nz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-26T21:27:39Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the positive feedback !\r\n\r\nI didn't find a CI failure due to `library=\"ak\"`... I'd be surprised because I didn't prepare any tests for this `AsDtypeInplace` implementation. Is this something I should add ? If so any suggestion on what to look for an example ?\r\n\r\nI agree that automatically overriding the `library` argument would be too ambiguous anywhere else than in `Tbranch.array()`  (and to be honest I wouldn't be sure were and how to do it properly).\r\nFor the `TBranch.array()` case, I guess it would be enough to add a line to `behaviors.TBranch.py` like : \r\n\r\n```\r\n    if isinstance(interpretation, uproot.interpretation.AsDtypeInplace) and library != \"np\" :\r\n        raise SomeException( ...)\r\n```\r\naround line 2040 ?\r\nI don't have a strong opinion on this case, but can do it if you think it helps.\r\n",
  "created_at":"2021-10-27T14:51:30Z",
  "id":953008808,
  "issue":487,
  "node_id":"IC_kwDOD6Q_ss44zcKo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-27T14:51:30Z",
  "user":"MDQ6VXNlcjEyMTk4Njg="
 },
 {
  "author_association":"MEMBER",
  "body":"Having it override `library` is something that we can choose to do in the future (because it would only change the behavior of what is an error now), but it would be hard to back out of if it was a mistake (because it would convert something that works into an error; backward incompatibility). So let's forget that for now.\r\n\r\nAdding a test for this case should be pretty easy: prepare an array, pass it to a TTree with flat data (Zmumu.root is a good candidate), and not only check that the returned array has some of the TTree's data, but also the original array. If you make the original array with `np.zeros`, then finding it with nonzero values (e.g. Z daughter momenta) would be a positive test that it has changed. (Starting with `np.empty` has a slight chance of being right by accident, in a platform-dependent way.)\r\n\r\nAnother thought: what happens if the provided array is too small? Right now, I think the user would get a NumPy error, which would look like an Uproot bug but is actually a user mistake. Maybe that should be caught with an error message that makes it clear that the provided array is too small.",
  "created_at":"2021-10-27T14:58:27Z",
  "id":953015301,
  "issue":487,
  "node_id":"IC_kwDOD6Q_ss44zdwF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-27T14:58:27Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'll add a test for `AsDtypeInplace` ... but naive question : how do you run the existing tests  ? also do you prefer I create a new `tests/test_000XX-AsDtypeInplace.py` or should I add the test to an existing file  ?\r\n",
  "created_at":"2021-10-27T22:14:33Z",
  "id":953350544,
  "issue":487,
  "node_id":"IC_kwDOD6Q_ss440vmQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-27T22:14:33Z",
  "user":"MDQ6VXNlcjEyMTk4Njg="
 },
 {
  "author_association":"MEMBER",
  "body":"I'd prefer a new file: `tests/test_0487-implement-asdtypeinplace.py`. Tests pin down a behavior so that it can't change without notice, so we use the issue or pull request number as a way to be reminded _why_ a particular behavior was defined: we can look it up on GitHub. That's not to say that we never change old tests, but we're mostly just adding new ones.\r\n\r\nAssuming that you're running this version of Uproot by\r\n\r\n```bash\r\npip install -e .\r\n```\r\n\r\nin the base directory of the repository, you can run all the tests with\r\n\r\n```bash\r\npytest\r\n```\r\n\r\nthough I prefer\r\n\r\n```bash\r\npytest -m \"not network\"\r\n```\r\n\r\nto skip past the tests that require the network (some take a long time to wait for timeouts). The network tests run more quickly on GitHub Actions, anyway. You can also run just your test with\r\n\r\n```bash\r\npytest tests/test_0487-implement-asdtypeinplace.py\r\n```\r\n\r\nfor faster turn-around times. (Another reason for putting them in separate files.) To run the pre-commit hooks (and save yourself more time), you can do\r\n\r\n```bash\r\npre-commit run -a\r\n```\r\n\r\nbut if it's a hassle installing pre-commit, just let GitHub Actions do it for you and\r\n\r\n```bash\r\ngit pull\r\n```\r\n\r\nto get its reformatting changes.",
  "created_at":"2021-10-27T22:52:04Z",
  "id":953367482,
  "issue":487,
  "node_id":"IC_kwDOD6Q_ss440zu6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-27T22:52:04Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ok I added `tests/test_0487-implement-asdtypeinplace.py` . It's rather minimal, maybe there are other tests I can add ?\r\n\r\nOtherwise, please let me know if anything else is needed to complete this MR\r\n",
  "created_at":"2021-10-28T10:27:05Z",
  "id":953715754,
  "issue":487,
  "node_id":"IC_kwDOD6Q_ss442Iwq",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T10:27:05Z",
  "user":"MDQ6VXNlcjEyMTk4Njg="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @mpad for code",
  "created_at":"2021-10-28T17:03:02Z",
  "id":954033707,
  "issue":487,
  "node_id":"IC_kwDOD6Q_ss443WYr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T17:03:02Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/491) to add @mpad! :tada:",
  "created_at":"2021-10-28T17:03:11Z",
  "id":954033843,
  "issue":487,
  "node_id":"IC_kwDOD6Q_ss443Waz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-10-28T17:03:11Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"Should it just assume the highest one? Or do so with a default option?",
  "created_at":"2021-11-03T12:00:55Z",
  "id":958963823,
  "issue":494,
  "node_id":"IC_kwDOD6Q_ss45KKBv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-03T12:00:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I would think it should do whatever `uproot.open('file.root').key('Events')` does if there are multiple cycle numbers present for `Events`, which seems to be taking the highest one:\r\n```python\r\n>>> uproot.open('root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root').key('Events')\r\n<ReadOnlyKey Events;75: TTree (seek pos 2244422054) at 0x7f6d397692e0>\r\n```",
  "created_at":"2021-11-05T14:01:57Z",
  "id":961920273,
  "issue":494,
  "node_id":"IC_kwDOD6Q_ss45Vb0R",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-05T14:01:57Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"The actual selection of TTree follows the same code path in lazy and non-lazy cases; what's different is the preparation of key names to ask for. So the substantial part of PR #500 (!) is\r\n\r\n```diff\r\n@@ -2965,7 +2965,7 @@ def _regularize_object_path(\r\n             **options  # NOTE: a comma after **options breaks Python 2\r\n         ).root_directory\r\n         if object_path is None:\r\n-            trees = [k for k, v in file.classnames().items() if v == \"TTree\"]\r\n+            trees = file.keys(filter_classname=\"TTree\", cycle=False)\r\n             if len(trees) == 0:\r\n                 if allow_missing:\r\n                     return None\r\n```\r\n\r\nI also had to fix `ReadOnlyFile.keys` (and `values`, `items`, `classnames`) to not return duplicates when the disambiguating `cycle` number is suppressed. Actually, that part could be considered a bug, but no one ever brought it up before. (`Mapping.keys` should return unique strings...)",
  "created_at":"2021-11-05T18:19:57Z",
  "id":962116300,
  "issue":494,
  "node_id":"IC_kwDOD6Q_ss45WLrM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-05T18:19:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Used versions:\r\nUproot version 4.1.7\r\nboost_histogram version 1.2.1\r\nhist version 2.5.1\r\nnumpy version 1.21.3",
  "created_at":"2021-11-04T14:42:20Z",
  "id":961081763,
  "issue":495,
  "node_id":"IC_kwDOD6Q_ss45SPGj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T14:46:39Z",
  "user":"MDQ6VXNlcjMwMDQxMDcz"
 },
 {
  "author_association":"MEMBER",
  "body":"Yes, it's not bit-for-bit the same:\r\n\r\n```python\r\n>>> assert hist[1].tolist() == list(range(23))   # okay\r\n>>> assert uproot_hist.axis(0).edges().tolist() == list(range(23))   # okay\r\n>>> assert uproot_hist.to_hist().axes[0].edges.tolist() == list(range(23))   # uh-oh!\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAssertionError\r\n```\r\n\r\nBut it's in the least significant bit:\r\n\r\n```python\r\n>>> uproot_hist.to_hist().axes[0].edges.tolist()\r\n[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 14.999999999999998, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0]\r\n```\r\n\r\nAll NumPy histograms have in-principle variable binned edges (each edge is explicitly given as a floating-point number), but boost-histogram/hist (and ROOT) have the option of regular binning (edges are defined in terms of a min, max, and number of bins). The variable bins in this NumPy histogram are close enough that they're converted into a boost-histogram/hist Regular axis:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/8da183190c35804ee3d37e7d3cd5d1be83877a34/src/uproot/behaviors/TH1.py#L27-L37\r\n\r\n```python\r\n>>> uproot_hist.to_hist().axes[0]\r\nRegular(22, 0, 22, name='xaxis', label='xaxis')\r\n```\r\n\r\nSomewhere within boost-histogram/hist, the calculation of edges from the min, max, and number of bins has some roundoff error. At this level of precision, I'm not surprised. I don't think the point where this happens is anywhere in the Uproot codebase (because we just hand off the min, max, and number of bins; boost-histogram/hist calculates the specific edges from that), but I also don't think it's going to be a very useful thing to dive in and find that point. Even if this round-off gets \"fixed,\" there's probably other round-off errors on this level of precision. We generally don't expect bit-by-bit agreement when two codebases perform algebraically equivalent operations with (very likely) different low-level instructions.\r\n\r\nAnyway, if it's alright, I'm going to close this issue here.",
  "created_at":"2021-11-04T14:55:08Z",
  "id":961108335,
  "issue":495,
  "node_id":"IC_kwDOD6Q_ss45SVlv",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-04T14:55:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Regardless of whether the jagged array is an Awkward Array or is a NumPy dtype=\"O\" array, there has to be a counter branch. This counter branch is not needed for Uproot to read the data back, but it is needed for ROOT to read the data back. In particular, you wouldn't be able to do this workflow in ROOT without it:\r\n\r\n```python\r\n>>> import uproot\r\n>>> file = uproot.recreate(\"tmp.root\")\r\n>>> file[\"tree\"] = {\"branch\": [[1.1, 2.2, 3.3], [], [4.4, 5.5]]}\r\n```\r\n\r\n```c++\r\nroot [0] auto file = new TFile(\"tmp.root\")\r\n(TFile *) @0x7ffd501efaa0\r\nroot [1] TTree* tree\r\n(TTree *) nullptr\r\nroot [2] file->GetObject(\"tree\", tree)\r\nroot [3] int maxlength = tree->GetBranch(\"branch\")->GetLeaf(\"branch\")->GetLeafCount()->GetMaximum()\r\n(int) 3\r\nroot [4] double* buffer = new double[maxlength]\r\n(double *) 0x55e17a709000\r\nroot [5] tree->SetBranchAddress(\"branch\", &buffer)\r\n(int) 0\r\n```\r\n\r\n(etc.) Traditional ROOT TTree-reading reuses the same array buffer as it steps through entries, and it needs to know how large to allocate this buffer before running over any data. The maximum size is stored in the counter leaf. Also, some of the methods that I use to test reading in ROOT (`TTree::Scan`, iteration in PyROOT) don't work without the counter leaf.\r\n\r\nGiven that the counter leaf has to be there, your problem is that the generated counters conflict with the counters that you're providing by passing the array from `uproot.iterate` directly into the output. That is, the output of `uproot.iterate` includes the counters as arrays, and writing takes both those arrays and also generates new counters from your jagged arrays.\r\n\r\n(Parenthetically, any NumPy arrays with dtype=\"O\" are internally converted into Awkward Arrays to do these manipulations. You're not saving any time or reducing dependencies by making them NumPy arrays. The opposite, actually.)\r\n\r\nPR #499 checks for conflicts between explicitly provided arrays and generated counters. If they're exactly the same, it only writes it once and all is well. If they differ, perhaps because a cut was applied to nested lists within a jagged array but not a purported counter, then it will raise a ValueError. This doesn't save you from having to look at the data extracted from the input and be conscious of what you write to the output, but the alternative would be to silently overwrite explicit arrays with generated counters, which would cause a subtle bug for somebody.",
  "created_at":"2021-11-05T17:09:18Z",
  "id":962067750,
  "issue":498,
  "node_id":"IC_kwDOD6Q_ss45V_0m",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-05T17:09:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know when you're done with this by requesting a review. Thanks!",
  "created_at":"2021-11-05T21:40:28Z",
  "id":962238297,
  "issue":501,
  "node_id":"IC_kwDOD6Q_ss45WpdZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-05T21:40:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Okay, I can't directly request a review, but it is done",
  "created_at":"2021-11-06T16:29:10Z",
  "id":962475801,
  "issue":501,
  "node_id":"IC_kwDOD6Q_ss45XjcZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-06T16:29:10Z",
  "user":"MDQ6VXNlcjMyNzczMzA0"
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @masonproffitt  for code",
  "created_at":"2021-11-07T12:31:26Z",
  "id":962602465,
  "issue":501,
  "node_id":"IC_kwDOD6Q_ss45YCXh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-07T12:31:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\n@masonproffitt already contributed before to code",
  "created_at":"2021-11-07T12:31:28Z",
  "id":962602467,
  "issue":501,
  "node_id":"IC_kwDOD6Q_ss45YCXj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-07T12:31:28Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"You beat me to it, so here's the additional content I was writing:\r\n\r\nInstead, `ReadOnlyDirectory.__contains__` should\r\n\r\n```python\r\ndef __contains__(self, where):\r\n    if not hasattr(self, \"_cached_keys\"):\r\n        self._cached_keys = set(self.iterkeys()) | set(self.iterkeys(cycle=False))\r\n    return where in self._cached_keys\r\n```\r\n\r\nThus, any string `where` that can be used in `__getitem__`, which is in the `keys()` with cycle numbers or without cycle numbers, would return True from `__contains__`.\r\n\r\nThen `__contains__` would be the fast function we expect it would be.",
  "created_at":"2021-11-10T16:46:24Z",
  "id":965534384,
  "issue":504,
  "node_id":"IC_kwDOD6Q_ss45jOKw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T16:46:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"In addition, the `damerau_levenshtein` function should be skipped when there's a very large number of keys. I haven't looked at the implementation recently, but I'll bet it's _O(n\u00b2)_.",
  "created_at":"2021-11-10T16:47:55Z",
  "id":965535753,
  "issue":504,
  "node_id":"IC_kwDOD6Q_ss45jOgJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T16:47:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"In `pyhf` where we have histograms in two possible paths (because of how HistFactory works) - avoiding the `DeserializationError` by comparing against a (cached) set of keys gives us a speedup from 20hrs down to ~10seconds.",
  "created_at":"2021-11-10T17:03:28Z",
  "id":965549719,
  "issue":504,
  "node_id":"IC_kwDOD6Q_ss45jR6X",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T17:03:28Z",
  "user":"MDQ6VXNlcjc2MTQ4Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Error handling code, for errors that are supposed to propagate to the user, need not be fast. As long as the error gets raised on a human timescale (< 1 second), it's fine.\r\n\r\nIt's only a problem when errors are caught as non-exceptional cases (e.g. `StopIteration`). The fact that try-except allows us to use exceptions for these two very different purposes puts \"Do I have enough information in the error message?\" versus \"Is this fast enough?\" into conflict.",
  "created_at":"2021-11-10T17:16:00Z",
  "id":965561053,
  "issue":504,
  "node_id":"IC_kwDOD6Q_ss45jUrd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T17:16:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm able to replicate this on `uproot` `v4.1.8`, with the exception of being able to get it working on the second attempt (still getting failure there)\r\n\r\n```console\r\n$ docker run --rm -ti python:3.9 /bin/bash\r\nroot@1b8a542902a8:/# python -m venv ~/venv && . ~/venv/bin/activate\r\n(venv) root@1b8a542902a8:/# python -m pip --quiet install --upgrade pip setuptools wheel\r\n(venv) root@1b8a542902a8:/# python -m pip --quiet install uproot\r\n(venv) root@1b8a542902a8:/# pip show uproot\r\nName: uproot\r\nVersion: 4.1.8\r\nSummary: ROOT I/O in pure Python and NumPy.\r\nHome-page: https://github.com/scikit-hep/uproot4\r\nAuthor: Jim Pivarski\r\nAuthor-email: pivarski@princeton.edu\r\nLicense: BSD-3-Clause\r\nLocation: /root/venv/lib/python3.9/site-packages\r\nRequires: numpy, setuptools\r\nRequired-by: \r\n(venv) root@1b8a542902a8:/# curl -sLO https://github.com/scikit-hep/pyhf/raw/c493c7c5ad3e5aeda7d022cdf5fe77e9fdab43a2/validation/xmlimport_input/data/example.root\r\n(venv) root@1b8a542902a8:/# python\r\nPython 3.9.7 (default, Oct 13 2021, 09:00:49) \r\n[GCC 10.2.1 20210110] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import uproot\r\n>>> file = uproot.update(\"example.root\")\r\n>>> file.keys()\r\n['data;1', 'signal;1', 'background1;1', 'background2;1', 'background1_statUncert;1']\r\n>>> del file[\"signal\"]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/writable.py\", line 972, in __delitem__\r\n    return self._get_del_search(where, False)\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/writable.py\", line 957, in _get_del_search\r\n    return self._del(item, cycle)\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/writable.py\", line 1029, in _del\r\n    self._cascading.freesegments.release(start, stop)\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/_cascade.py\", line 757, in release\r\n    new_slices = self._another_slice(self._data.slices, start, stop)\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/_cascade.py\", line 714, in _another_slice\r\n    raise RuntimeError(\r\nRuntimeError: segment of data to release overlaps one already marked as free: releasing [1778, 2070) but [2045, 5045) is free\r\n>>> del file[\"signal\"]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/writable.py\", line 972, in __delitem__\r\n    return self._get_del_search(where, False)\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/writable.py\", line 957, in _get_del_search\r\n    return self._del(item, cycle)\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/writable.py\", line 1029, in _del\r\n    self._cascading.freesegments.release(start, stop)\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/_cascade.py\", line 757, in release\r\n    new_slices = self._another_slice(self._data.slices, start, stop)\r\n  File \"/root/venv/lib/python3.9/site-packages/uproot/writing/_cascade.py\", line 714, in _another_slice\r\n    raise RuntimeError(\r\nRuntimeError: segment of data to release overlaps one already marked as free: releasing [1778, 2070) but [2045, 5045) is free\r\n```",
  "created_at":"2021-11-10T21:37:00Z",
  "id":965766358,
  "issue":505,
  "node_id":"IC_kwDOD6Q_ss45kGzW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-10T21:38:24Z",
  "user":"MDQ6VXNlcjUxNDIzOTQ="
 },
 {
  "author_association":"MEMBER",
  "body":"The style everywhere else in the codebase is to use properties, so that they can be read-only or types can be checked upon assignment. By itself, what you have here looks fine, but it goes against the grain of everything else.\r\n\r\nI only came to this style of \"underscore + property for nearly everything\" gradually, but Uproot (and Awkward Array) were written entirely in this style. It puts a lot of ceremony around class attributes, which I think is good because they should be kept at a minimum and mutability should be discouraged.\r\n\r\nAs for whether this particular class should have attributes, rather than properties, I'll let you decide, but I just want to let you know what the rest of the codebase is doing. (And anyway, Python lets you postpone that decision, since the public semantics are identical.)",
  "created_at":"2021-11-12T13:14:50Z",
  "id":967110199,
  "issue":506,
  "node_id":"IC_kwDOD6Q_ss45pO43",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-12T13:14:50Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@veprbl Is this ready to merge? It looks like it is.",
  "created_at":"2021-12-02T21:10:57Z",
  "id":985003817,
  "issue":506,
  "node_id":"IC_kwDOD6Q_ss46tfcp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T21:10:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Yes, it is.",
  "created_at":"2021-12-03T00:03:33Z",
  "id":985099939,
  "issue":506,
  "node_id":"IC_kwDOD6Q_ss46t26j",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-03T00:03:33Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, then I'll merge it to get it in the next release (maybe tomorrow). Thanks!",
  "created_at":"2021-12-03T00:19:06Z",
  "id":985107396,
  "issue":506,
  "node_id":"IC_kwDOD6Q_ss46t4vE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-03T00:19:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This HTTP server requires some special care: it's sending unrecognized responses. You don't see that when loading all 50 fields in one go\u2014failures are hidden by attempts to fall back on different methods and try again, before the whole thing times out.\r\n\r\n```python\r\n>>> import uproot\r\n>>> f = uproot.open(\"https://rebrand.ly/00vvyzg\")\r\n>>> f.keys()\r\n['b0phiKs;2', 'b0phiKs;1']\r\n>>> t = f[\"b0phiKs\"]\r\n>>> f.file.source\r\n<HTTPSource '...ly/00vvyzg' at 0x7f83f02755b0>\r\n```\r\n\r\nNo problem so far because opening a file and navigating through it only ever asks for one chunk of the file at a time. (That is, a directory listing is in bytes X through Y, so Uproot asks the HTTP server for those bytes, waits until it gets them, then uses what it gets\u2014all synchronously.)\r\n\r\nWhen reading an array, the first attempt gives the HTTP server a list of possibly discontiguous byte ranges, the locations of all the TBaskets, and asks for them all to be returned in one response. This is a \"multi-part GET\" in HTTP, and not all servers support it. (Not attempting this was a big inefficiency in Uproot 3.)\r\n\r\nLet's start with just one TBranch:\r\n\r\n```python\r\n>>> t[\"B0_M\"].array()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n...\r\n  File \"/home/jpivarski/irishep/uproot4/src/uproot/source/http.py\", line 360, in task\r\n    resource.handle_multipart(source, futures, results, response)\r\n  File \"/home/jpivarski/irishep/uproot4/src/uproot/source/http.py\", line 423, in handle_multipart\r\n    raise OSError(\r\nOSError: found 1 of 27 expected headers in HTTP multipart\r\nfor URL https://rebrand.ly/00vvyzg\r\n```\r\n\r\nThis server does not support multi-part GETs, but I don't know why Uproot hasn't switched over to using its fallback \"one HTTP volley per TBasket\" mode yet:\r\n\r\n```python\r\n>>> f.file.source\r\n<HTTPSource '...ly/00vvyzg' at 0x7f83f02755b0>\r\n```\r\n\r\n(doesn't say \"with fallback\").\r\n\r\nI tried reading a small number of entries:\r\n\r\n```python\r\n>>> t[\"B0_M\"].array(entry_stop=5)\r\n<Array [5.02, 5.11, 5.12, 5.36, 5.3] type='5 * float32'>\r\n```\r\n\r\nMaybe that worked because these are all in the first TBasket, so therefore only one byte range is being sent from the HTTP server to Uproot? How many baskets are there and how big is the first one?\r\n\r\n```python\r\n>>> t[\"B0_M\"].num_baskets\r\n28\r\n>>> t[\"B0_M\"].basket_entry_start_stop(0)\r\n(0, 7981)\r\n```\r\n\r\nSo the first 7981 entries are all in the first TBasket. Reading up to that point should be no problem.\r\n\r\n```python\r\n>>> t[\"B0_M\"].array(entry_stop=7981)\r\n<Array [5.02, 5.11, 5.12, ... 5.18, 5.45, 5.15] type='7981 * float32'>\r\n```\r\n\r\nPresumably, we should start to see the problem when reading past that point, which asks for two TBaskets (discontiguous byte ranges in the file). But it doesn't:\r\n\r\n```python\r\n>>> t[\"B0_M\"].array(entry_stop=7982)\r\n<Array [5.02, 5.11, 5.12, ... 5.45, 5.15, 5.14] type='7982 * float32'>\r\n>>> t[\"B0_M\"].array(entry_stop=14000)\r\n<Array [5.02, 5.11, 5.12, ... 5.1, 5.07, 5.3] type='14000 * float32'>\r\n>>> t[\"B0_M\"].array(entry_stop=28000)\r\n<Array [5.02, 5.11, 5.12, ... 5.13, 5.37, 5.15] type='28000 * float32'>\r\n```\r\n\r\nEven if we read all 28 TBaskets in this TBranch:\r\n\r\n```python\r\n>>> t[\"B0_M\"].array()\r\n<Array [5.02, 5.11, 5.12, ... 5.25, 5.25, 5.26] type='329135 * float32'>\r\n```\r\n\r\nBut that's because Uproot's HTTP client is already in fallback mode:\r\n\r\n```python\r\n>>> f.file.source\r\n<HTTPSource '...ly/00vvyzg' with fallback at 0x7f83f02755b0>\r\n```\r\n\r\nI shut down Python and tried again, and it seems that Uproot correctly goes into fallback mode if `entry_stop` is given but not if it isn't. **That's an Uproot bug.**\r\n\r\nBut the bigger issue for you is that you can't use multi-part GET with this HTTP server. Knowing that, you can go directly into fallback mode: `uproot.HTTPServer` attempts the multi-part GET with a fallback, `uproot.MultithreadedHTTPSource` is that fallback: it launches a suite of worker threads and has each one open a single HTTP connection. Too few workers and you'll be latency dominated, waiting for the server to individually respond for each TBasket of data, too many workers and you'll clog the server\u2014it will get all the requests at once and will need to manage that. Both of these considerations depend on the quality of the network between you and the server and the quality of the server, but you can tune the `num_workers`. (See [uproot.open](https://uproot.readthedocs.io/en/latest/uproot.reading.open.html). If you're lucky and most of the directory data and TTree metadata is at the beginning of the file, you can increase `begin_chunk_size` to preemptively read more and\u2014with luck\u2014not have to make several volleys to get directory data.)\r\n\r\n```python\r\n>>> import uproot\r\n>>> f = uproot.open(\"https://rebrand.ly/00vvyzg\", http_handler=uproot.MultithreadedHTTPSource, num_workers=10)\r\n>>> f.file.source\r\n<MultithreadedHTTPSource '...ly/00vvyzg' (10 workers) at 0x7ff5438545b0>\r\n>>> t = f[\"b0phiKs\"]\r\n>>> t[\"B0_M\"].array()\r\n<Array [5.02, 5.11, 5.12, ... 5.25, 5.25, 5.26] type='329135 * float32'>\r\n>>> import time\r\n>>> starttime = time.time(); df = t.arrays(library=\"pd\"); print(time.time() - starttime)\r\n160.00439739227295\r\n>>> df\r\n        exp_no  run_no    evt_no      B0_M  ...   B0_hoo3   B0_hoo4  nCands  iCand\r\n0            0       0  23045033  5.024452  ...  0.004035 -0.003992       1      0\r\n1            0       0  23046788  5.107935  ... -0.001334  0.000621       2      0\r\n2            0       0  23046788  5.119214  ... -0.001160 -0.000464       2      1\r\n3            0       0  23046989  5.361363  ... -0.002333  0.019902       1      0\r\n4            0       0  23049068  5.301050  ...  0.002522  0.001435       1      0\r\n...        ...     ...       ...       ...  ...       ...       ...     ...    ...\r\n329130       0       0  71575996  5.093135  ... -0.007649  0.031542       1      0\r\n329131       0       0  71646148  5.096963  ... -0.000318  0.004670       1      0\r\n329132       0       0  71647975  5.250312  ...  0.000000  0.057552       1      0\r\n329133       0       0  71649052  5.250185  ...  0.017649  0.021859       1      0\r\n329134       0       0  71649130  5.262949  ...  0.003130  0.004816       1      0\r\n\r\n[329135 rows x 50 columns]\r\n```\r\n\r\nTwo and a half minutes for me. I watched this with [iftop](https://www.ex-parrot.com/pdw/iftop/) and see all the traffic from desycloud.desy.de. Oh, hi Kilian!",
  "created_at":"2021-11-15T16:40:38Z",
  "id":969095432,
  "issue":507,
  "node_id":"IC_kwDOD6Q_ss45wzkI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T16:40:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Hi @jpivarski ! Wow, thanks a lot for taking me through the whole journey of getting to the bottom of this! This has been a very interesting read :)   \r\nAlso good to know of the limitations of this server! It's not really used for data files usually (it's only used in this tutorial to make it more portable for users who don't have their official accounts yet), and now we know it really shouldn't be!",
  "created_at":"2021-11-22T09:41:49Z",
  "id":975335596,
  "issue":507,
  "node_id":"IC_kwDOD6Q_ss46InCs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T09:41:49Z",
  "user":"MDQ6VXNlcjEzNjAyNDY4"
 },
 {
  "author_association":"MEMBER",
  "body":"Might it be parentheses around the `AD==2`?\r\n\r\nIn Python, `AD==2 & (PromptZ > 0)` means `AD` equality-comparison with `2 & (PromptZ > 0)`, where `&` is bitwise-and. The `==` equality-comparison has lower operator precedence than `&` bitwise-and.\r\n\r\nHere's a dump of Python's syntax tree for that expression:\r\n\r\n```python\r\n>>> import ast\r\n>>> print(ast.dump(ast.parse(\"AD==2 & (PromptZ > 0)\"), indent=4))\r\nModule(\r\n    body=[\r\n        Expr(\r\n            value=Compare(\r\n                left=Name(id='AD', ctx=Load()),\r\n                ops=[\r\n                    Eq()],\r\n                comparators=[\r\n                    BinOp(\r\n                        left=Constant(value=2),\r\n                        op=BitAnd(),\r\n                        right=Compare(\r\n                            left=Name(id='PromptZ', ctx=Load()),\r\n                            ops=[\r\n                                Gt()],\r\n                            comparators=[\r\n                                Constant(value=0)]))]))],\r\n    type_ignores=[])\r\n```\r\n\r\nThe `Name(id='AD', ctx=Load())` (representing the \"`AD`\" part of the expression) is being equality-compared (`Compare( ... ops=[Eq()], ...)`) with a tree computed by the binary operation (`BinOp`) that is the bitwise and (`BitAnd()`).\r\n\r\n**Okay, maybe that was too much information.** Maybe you just want to try\r\n\r\n```python\r\nCut1=\"(AD==2) & (PromptZ > 0)\"\r\n```\r\n\r\nBy contrast, `Cut2=\"AD==2 && (PromptZ > 0)\"` is TTreeFormula syntax (why `&&` is needed instead of `&`, it's a different language). The `&&` means logical-and, and `==` has a higher operator precedence than `&&`.\r\n\r\nMy biggest pet peeve about Python+NumPy is that there's no operator for array-at-a-time logical-and. Bitwise-and works, since the arrays are just booleans, but the operator precedence is ~~wrong~~ highly unexpected.\r\n\r\nOne of the open items we'd like to fill is to get [formulate](https://github.com/scikit-hep/formulate) up to date so that Uproot's cut strings can use TTreeFormula syntax instead of Python syntax (as an option, default being TTreeFormula).",
  "created_at":"2021-11-15T16:56:11Z",
  "id":969110007,
  "issue":508,
  "node_id":"IC_kwDOD6Q_ss45w3H3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-15T16:56:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"> Might it be parentheses around the `AD==2`?\r\n> \r\n> In Python, `AD==2 & (PromptZ > 0)` means `AD` equality-comparison with `2 & (PromptZ > 0)`, where `&` is bitwise-and. The `==` equality-comparison has lower operator precedence than `&` bitwise-and.\r\n> \r\n> Here's a dump of Python's syntax tree for that expression:\r\n> \r\n> ```python\r\n> >>> import ast\r\n> >>> print(ast.dump(ast.parse(\"AD==2 & (PromptZ > 0)\"), indent=4))\r\n> Module(\r\n>     body=[\r\n>         Expr(\r\n>             value=Compare(\r\n>                 left=Name(id='AD', ctx=Load()),\r\n>                 ops=[\r\n>                     Eq()],\r\n>                 comparators=[\r\n>                     BinOp(\r\n>                         left=Constant(value=2),\r\n>                         op=BitAnd(),\r\n>                         right=Compare(\r\n>                             left=Name(id='PromptZ', ctx=Load()),\r\n>                             ops=[\r\n>                                 Gt()],\r\n>                             comparators=[\r\n>                                 Constant(value=0)]))]))],\r\n>     type_ignores=[])\r\n> ```\r\n> \r\n> The `Name(id='AD', ctx=Load())` (representing the \"`AD`\" part of the expression) is being equality-compared (`Compare( ... ops=[Eq()], ...)`) with a tree computed by the binary operation (`BinOp`) that is the bitwise and (`BitAnd()`).\r\n> \r\n> **Okay, maybe that was too much information.** Maybe you just want to try\r\n> \r\n> ```python\r\n> Cut1=\"(AD==2) & (PromptZ > 0)\"\r\n> ```\r\n> \r\n> By contrast, `Cut2=\"AD==2 && (PromptZ > 0)\"` is TTreeFormula syntax (why `&&` is needed instead of `&`, it's a different language). The `&&` means logical-and, and `==` has a higher operator precedence than `&&`.\r\n> \r\n> My biggest pet peeve about Python+NumPy is that there's no operator for array-at-a-time logical-and. Bitwise-and works, since the arrays are just booleans, but the operator precedence is ~wrong~ highly unexpected.\r\n> \r\n> One of the open items we'd like to fill is to get [formulate](https://github.com/scikit-hep/formulate) up to date so that Uproot's cut strings can use TTreeFormula syntax instead of Python syntax (as an option, default being TTreeFormula).\r\n\r\nYou are exactly right! When I use Cut1=\"(AD==2) & (PromptZ > 0)\", it works.\r\nMany thanks to your detailed explanation!\r\nGreat project!",
  "created_at":"2021-11-16T02:00:06Z",
  "id":969672558,
  "issue":508,
  "node_id":"IC_kwDOD6Q_ss45zAdu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-16T02:00:06Z",
  "user":"MDQ6VXNlcjQwODc1NDc0"
 },
 {
  "author_association":"MEMBER",
  "body":"I'm glad it's working for you!",
  "created_at":"2021-11-16T14:46:36Z",
  "id":970344699,
  "issue":508,
  "node_id":"IC_kwDOD6Q_ss451kj7",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "heart":1,
   "total_count":2
  },
  "updated_at":"2021-11-16T14:46:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"We've struggled with the performance of Pandas conversations before. If the data types are not numeric or singly jagged-numeric, then just the interpretation before getting to Pandas is going to be slow (until we start using AwkwardForth next summer). To see if it's the interpretation or the Pandas conversion, try reading the data into Awkward Arrays (no `library` option). It's probably not the concatenation, but you can test that, too, by using `iterate` instead of `concatenate`. If it's the Pandas conversion, Awkward Arrays have an [ak.to_pandas](https://awkward-array.readthedocs.io/en/latest/_auto/ak.to_pandas.html) function that is independent of Uproot's, and it might be faster. (Uproot's can't assume the existence of Awkward Array, which constrains it more.)\r\n\r\nI hope these suggestions help! Oh, and using a lot of memory on a machine that has a lot of memory is not unusual\u2014Python won't clean up intermediate objects until it has to, so the memory used doesn't represent the data that would remain after a garbage collection pass. However, the fact that there are so many intermediate objects and it's taking this long suggests to me that O(n) Python objects are being created (where n is the length of the array), which only happens if the data type is not one of the \"numeric or singly jagged-numeric\" that are castable without intermediate Python from the way that ROOT files are structured. The most immediate thing is, what's the data type?",
  "created_at":"2021-11-20T12:44:57Z",
  "id":974645052,
  "issue":511,
  "node_id":"IC_kwDOD6Q_ss46F-c8",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-20T12:44:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for the advice! going from awkward array to pandas is actually a good 20% faster when not importing all branches. Beyond that, I found that a few of the branches are actually vectors! which are able to be read into awkward arrays without an issue but hit memory limit when attempting to read directly into pandas. Omitting these branches let me read all the others in no problem. ",
  "created_at":"2021-11-21T14:50:16Z",
  "id":974830622,
  "issue":511,
  "node_id":"IC_kwDOD6Q_ss46Grwe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-21T14:50:16Z",
  "user":"MDQ6VXNlcjIwOTExOTg3"
 },
 {
  "author_association":"MEMBER",
  "body":"In Pandas, the vectors have to become Python lists, because a Pandas cell has to be something with a NumPy-like dtype (the set of Pandas dtypes is a little broader than NumPy's, but only adding things decimal number formats), and that doesn't include nested lists, unless you go to dtype=\"O\", arbitrary Python objects.\r\n\r\nIf the branches have the same jagged structure as each other, the numerical values are placed in the cells and the jagged structure is represented by a MultiIndex. Awkward Array's `to_pandas` can go arbitrarily many levels of nesting deep, but I don't remember if Uproot's can. More than one level deep is stored in a completely different way in ROOT, a way that isn't favorable to accessing as a column. (That's what \"AwkwardForth\" intends to address: it's a fast mini-language for iterating over non-columnar data and turning it into columns. We're preparing for a major project next summer to switch all of Uproot's non-columnar handling to AwkwardForth.)",
  "created_at":"2021-11-22T12:40:21Z",
  "id":975482068,
  "issue":511,
  "node_id":"IC_kwDOD6Q_ss46JKzU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T12:40:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"XYZVectors are not stored in a single branch, but in a _group_ of branches. (Am I right about that? Check with `trkana['demcent/_mom'].show()`.) There had been a few bugs in interpreting groups of branches, but they've been fixed since Uproot 4.0.0; the latest is 4.1.8.\r\n\r\nYou probably want to read each branch into components of a [Vector](https://github.com/scikit-hep/vector). For instance,\r\n\r\n```python\r\nimport awkward as ak\r\nimport vector\r\nvector.register_awkward()\r\n\r\nxyz = trkana['demcent/_mom'].array()\r\narray = ak.zip({\"px\": xyz[\"fX\"], \"py\": xyz[\"fY\"], \"pz\": xyz[\"fZ\"]}, with_name=\"Momentum3D\")\r\n```\r\n\r\n(or something similar; I'm writing this from memory.)",
  "created_at":"2021-11-22T20:11:28Z",
  "id":975879647,
  "issue":513,
  "node_id":"IC_kwDOD6Q_ss46Kr3f",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T20:11:28Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for your reply Jim, but I still get the same error after upgrading. Also, it doesn't look like the XYZVector is being stored as a group of branches, see here:\r\n\r\n```python\r\nimport awkward as ak\r\nimport uproot\r\nprint(uproot.__version__)\r\nimport vector\r\nvector.register_awkward()\r\n\r\nfile = uproot.open(\"TAtest2.root.txt\")\r\ntrkana = file['TrkAnaNeg/trkana']\r\nprint(trkana['demcent/_mom'].show())\r\nxyz = trkana['demcent/_mom'].array()\r\narray = ak.zip({\"px\": xyz[\"fX\"], \"py\": xyz[\"fY\"], \"pz\": xyz[\"fZ\"]}, with_name=\"Momentum3D\")\r\n```\r\n\r\nThis code returns:\r\n```python\r\n4.1.8\r\nname                 | typename                 | interpretation                \r\n---------------------+--------------------------+-------------------------------\r\n_mom                 | ROOT::Math::Displacement | AsStridedObjects(Model_ROOT_3a\r\nNone\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/interpretation/numerical.py in basket_array(self, data, byte_offsets, basket, branch, context, cursor_offset, library)\r\n    341         try:\r\n--> 342             output = data.view(dtype).reshape((-1,) + shape)\r\n    343         except ValueError:\r\n\r\nValueError: When changing to a larger dtype, its size must be a divisor of the total size in bytes of the last axis of the array.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-68794709770c> in <module>\r\n      8 trkana = file['TrkAnaNeg/trkana']\r\n      9 print(trkana['demcent/_mom'].show())\r\n---> 10 xyz = trkana['demcent/_mom'].array()\r\n     11 array = ak.zip({\"px\": xyz[\"fX\"], \"py\": xyz[\"fY\"], \"pz\": xyz[\"fZ\"]}, with_name=\"Momentum3D\")\r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in array(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library)\r\n   2093                         ranges_or_baskets.append((branch, basket_num, range_or_basket))\r\n   2094 \r\n-> 2095         _ranges_or_baskets_to_arrays(\r\n   2096             self,\r\n   2097             ranges_or_baskets,\r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in _ranges_or_baskets_to_arrays(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays, update_ranges_or_baskets)\r\n   3508 \r\n   3509         elif isinstance(obj, tuple) and len(obj) == 3:\r\n-> 3510             uproot.source.futures.delayed_raise(*obj)\r\n   3511 \r\n   3512         else:\r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/source/futures.py in delayed_raise(exception_class, exception_value, traceback)\r\n     44         exec(\"raise exception_class, exception_value, traceback\")\r\n     45     else:\r\n---> 46         raise exception_value.with_traceback(traceback)\r\n     47 \r\n     48 \r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/behaviors/TBranch.py in basket_to_array(basket)\r\n   3452             basket_arrays = branchid_arrays[branch.cache_key]\r\n   3453 \r\n-> 3454             basket_arrays[basket.basket_num] = interpretation.basket_array(\r\n   3455                 basket.data,\r\n   3456                 basket.byte_offsets,\r\n\r\n/opt/miniconda3/lib/python3.8/site-packages/uproot/interpretation/numerical.py in basket_array(self, data, byte_offsets, basket, branch, context, cursor_offset, library)\r\n    342             output = data.view(dtype).reshape((-1,) + shape)\r\n    343         except ValueError:\r\n--> 344             raise ValueError(\r\n    345                 \"\"\"basket {0} in tree/branch {1} has the wrong number of bytes ({2}) \"\"\"\r\n    346                 \"\"\"for interpretation {3}\r\n\r\nValueError: basket 0 in tree/branch /TrkAnaNeg/trkana;1:demcent/_mom has the wrong number of bytes (3232) for interpretation AsStridedObjects(Model_ROOT_3a3a_Math_3a3a_DisplacementVector3D_3c_ROOT_3a3a_Math_3a3a_Cartesian3D_3c_float_3e2c_ROOT_3a3a_Math_3a3a_DefaultCoordinateSystemTag_3e__v1)\r\nin file TAtest2.root.txt```",
  "created_at":"2021-11-22T20:19:20Z",
  "id":975885649,
  "issue":513,
  "node_id":"IC_kwDOD6Q_ss46KtVR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T20:19:20Z",
  "user":"MDQ6VXNlcjY1MzUyNTM="
 },
 {
  "author_association":"MEMBER",
  "body":"You're right: that's not a group (branch with subbranches). It's failing because \"AsStridedObjects\" means \"try to interpret the buffer as\r\n\r\n```python\r\nnp.dtype([(\"fX\", np.float64), (\"fY\", np.float64), (\"fZ\", np.float64)])\r\n```\r\n\r\nwithout doing any Python iteration,\" but the buffer does not have N \u00d7 3 \u00d7 sizeof(component) bytes for any integer N. (I don't know what the component size is, whether it's really `np.float64` or `np.float32`; I made that up by way of example.)\r\n\r\nCould I see the file? I can try to find out how it's really encoded, and whether AsStridedObjects can't be used on this type.",
  "created_at":"2021-11-22T20:29:07Z",
  "id":975892990,
  "issue":513,
  "node_id":"IC_kwDOD6Q_ss46KvH-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T20:29:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you, I linked the file in the first comment, you can find it here: https://github.com/scikit-hep/uproot4/files/7583947/TAtest2.root.txt. Thanks for looking into this!",
  "created_at":"2021-11-22T20:31:08Z",
  "id":975894512,
  "issue":513,
  "node_id":"IC_kwDOD6Q_ss46Kvfw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T20:31:08Z",
  "user":"MDQ6VXNlcjY1MzUyNTM="
 },
 {
  "author_association":"MEMBER",
  "body":"Oh, thanks! I didn't see that.\r\n\r\nWhat are some expected values in the first entry? I found the class name for this object:\r\n\r\n```python\r\n>>> uproot.model.classname_decode(trkana['demcent/_mom'].interpretation.model.__name__)\r\n('ROOT::Math::DisplacementVector3D<ROOT::Math::Cartesian3D<float>,ROOT::Math::DefaultCoordinateSystemTag>', 1)\r\n```\r\n\r\n(That's a long one!) And here's its streamer:\r\n\r\n```python\r\n>>> file.file.streamer_named('ROOT::Math::DisplacementVector3D<ROOT::Math::Cartesian3D<float>,ROOT::Math::DefaultCoordinateSystemTag>').show()\r\nROOT::Math::DisplacementVector3D<ROOT::Math::Cartesian3D<float>,ROOT::Math::DefaultCoordinateSystemTag> (v1)\r\n    fCoordinates: ROOT::Math::Cartesian3D<float> (TStreamerObjectAny)\r\n```\r\n\r\nOkay, so it only contains one thing, a `ROOT::Math::Cartesian3D<float>`. Here's the streamer for that:\r\n\r\n```python\r\n>>> file.file.streamer_named('ROOT::Math::Cartesian3D<float>').show()\r\nROOT::Math::Cartesian3D<float> (v1)\r\n    fX: float (TStreamerBasicType)\r\n    fY: float (TStreamerBasicType)\r\n    fZ: float (TStreamerBasicType)\r\n```\r\n\r\nOkay, so all of the values are `float32` (not `float64`).\r\n\r\nLet's dump the raw bytes of the first event, first in a debugging form and then as an array:\r\n\r\n```python\r\n>>> trkana['demcent/_mom'].debug(0)\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n 64   0   0  28   0   0  84  86  37 100  64   0   0  18   0   0 234 251 160  10\r\n  @ --- --- --- --- ---   T   V   %   d   @ --- --- --- --- --- --- --- --- ---\r\n--+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-\r\n193  50 176 181  66 170 107  13  66 103 210   9\r\n---   2 --- ---   B ---   k ---   B   g --- ---\r\n>>> trkana['demcent/_mom'].debug_array(0)\r\narray([ 64,   0,   0,  28,   0,   0,  84,  86,  37, 100,  64,   0,   0,\r\n        18,   0,   0, 234, 251, 160,  10, 193,  50, 176, 181,  66, 170,\r\n       107,  13,  66, 103, 210,   9], dtype=uint8)\r\n```\r\n\r\nOh, wait a minute: the non-strided interpretation works:\r\n\r\n```python\r\n>>> slow_interpretation = uproot.interpretation.identify.interpretation_of(trkana['demcent/_mom'], {}, False)\r\n>>> trkana['demcent/_mom'].array(slow_interpretation)\r\n<Array [{fCoordinates: {fX: -11.2, ... ] type='101 * struct[[\"fCoordinates\"], [s...'>\r\n>>> trkana['demcent/_mom'].array(slow_interpretation)[0].tolist()\r\n{'fCoordinates': {'fX': -11.16814136505127, 'fY': 85.2090835571289, 'fZ': 57.95511245727539}}\r\n```\r\n\r\nIt's called \"`slow_interpretation`\" because it gives up trying to interpret the buffer in one NumPy array cast and instead iterates over it in Python. (That's the `False` in `interpretation_of`.) If this is successfully interpreting it, what code is it using?\r\n\r\n```python\r\n>>> print(file.file.class_named('ROOT::Math::Cartesian3D<float>').known_versions[1].class_code)\r\nclass Model_ROOT_3a3a_Math_3a3a_Cartesian3D_3c_float_3e__v1(uproot.model.VersionedModel):\r\n    def read_members(self, chunk, cursor, context, file):\r\n        if self.is_memberwise:\r\n            raise NotImplementedError(\r\n                \"memberwise serialization of {0}\\nin file {1}\".format(type(self).__name__, self.file.file_path)\r\n            )\r\n        self._members['fX'], self._members['fY'], self._members['fZ'] = cursor.fields(chunk, self._format0, context)\r\n    ...\r\n    _format0 = struct.Struct('>fff')\r\n    _format_memberwise0 = struct.Struct('>f')\r\n    _format_memberwise1 = struct.Struct('>f')\r\n    _format_memberwise2 = struct.Struct('>f')\r\n    base_names_versions = []\r\n    member_names = ['fX', 'fY', 'fZ']\r\n    class_flags = {}\r\n```\r\n\r\nWell, that's just three consecutive floats (`struct.Struct('>fff')`); I don't see what the problem is with the strided interpretation.\r\n\r\nThe problem is that there's 20 bytes of stuff before the three floats. The `slow_interpretation` knows to skip that, but the strided interpretation does not. So for instance, a correct strided interpretation would be:\r\n\r\n```python\r\n>>> strided_interpretation = uproot.AsDtype([(\"???\", \"S20\"), (\"fX\", \">f4\"), (\"fY\", \">f4\"), (\"fZ\", \">f4\")])\r\n>>> trkana['demcent/_mom'].array(strided_interpretation)\r\n<Array [...] type='101 * {\"???\": bytes, \"fX\": float32, \"fY\": float32, \"fZ\": floa...'>\r\n>>> trkana['demcent/_mom'].array(strided_interpretation)[0].tolist()\r\n{'???': b'@\\x00\\x00\\x1c\\x00\\x00TV%d@\\x00\\x00\\x12\\x00\\x00\\xea\\xfb\\xa0\\n', 'fX': -11.16814136505127, 'fY': 85.2090835571289, 'fZ': 57.95511245727539}\r\n```\r\n\r\nand we should just ignore the field named \"`???`\". I'm surprised that the `slow_interpretation` got those 20 bytes right: I don't see anything here that would be telling it that. This bug is in the conversion of a general (slow) interpretation into a strided one: it should not have allowed the conversion or it should have known to insert the 20 byte header. Until I can figure out how to make it automatically do either one of those things, I shouldn't be closing this issue.\r\n\r\nAnyway, to solve your problem, you need (for now) to pass a custom interpretation, either\r\n\r\n```python\r\nslow_interpretation = uproot.interpretation.identify.interpretation_of(trkana['demcent/_mom'], {}, False)\r\n```\r\n\r\nor\r\n\r\n```python\r\nstrided_interpretation = uproot.AsDtype([(\"???\", \"S20\"), (\"fX\", \">f4\"), (\"fY\", \">f4\"), (\"fZ\", \">f4\")])\r\n```",
  "created_at":"2021-11-22T21:16:18Z",
  "id":975925636,
  "issue":513,
  "node_id":"IC_kwDOD6Q_ss46K3GE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T21:16:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you very much for such a thorough investigation. I will use the custom interpretation for now. ",
  "created_at":"2021-11-22T22:04:57Z",
  "id":975958470,
  "issue":513,
  "node_id":"IC_kwDOD6Q_ss46K_HG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-22T22:04:57Z",
  "user":"MDQ6VXNlcjY1MzUyNTM="
 },
 {
  "author_association":"MEMBER",
  "body":"With #517, we'll at least get a NotImplementedError message. You saw the exception because\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/624b0338995d2cd35bfd7a6c06da3c5cc62330de/src/uproot/writing/identify.py#L78-L81\r\n\r\njust checks to see if the object is from the `pandas` library and then assumes it's a DataFrame. `pandas.core.arrays.categorical.Categorical` is from the `pandas` library, but it's a kind of array; Series objects are like that, too. What we really wanted was\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/8bdd6839cb86ec571e96fcd3b03d0783be3c53b1/src/uproot/writing/identify.py#L79-L85\r\n\r\n(which is what's done elsewhere in that file).\r\n\r\nUnfortunately, you still can't write it because I haven't implemented strings ([TLeafC](https://root.cern.ch/doc/master/classTLeafC.html)), just the numeric types. The \"categoricalness\" (fact that each unique string is stored only once and referred to by integers; a.k.a \"dictionary encoding\") would not be preserved in any case because I don't think ROOT I/O has a type like that\u2014definitely not one of the basic TLeaf types. But the data are compressed (if you don't opt out), and a compression algorithm effectively does dictionary encoding.\r\n\r\nAt least, though, PR #517 has Uproot return an error message like this:\r\n\r\n```\r\nNotImplementedError: array of strings\r\n```\r\n\r\nwhich would have been clearer than what you got.\r\n\r\nI'll leave this issue open, since it's a request for writing string data. In the meantime, you might want to use [np.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) with `return_inverse=True`, which you can use to convert strings into numerical categories. It's also possible to write TObjString objects directly into directories, which can be JSON metadata of unique strings and their corresponding integers.",
  "created_at":"2021-11-30T15:51:11Z",
  "id":982766757,
  "issue":516,
  "node_id":"IC_kwDOD6Q_ss46k9Sl",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-11-30T15:51:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks for the quick reaction and for the tipps how to workaround this.\r\n\r\n> and a compression algorithm effectively does dictionary encoding.\r\nYes, I guess that pandas has this type because the dataframe is not compressed in-memory (I assume), but as ROOT workflows are mostly out-of-memory, I guess that's not really needed.\r\n\r\nAnother reason I personally had used Categorical columns is because the categories are ordered and I currently use this metadata in my plotting/histogramming functions. I have a MC-category in a  categorical column and then my histogramming/plotting functions use the ordered categories to define the MC-category-axis of a 2D histogram and in the end this also decides the order in which the components of my stacked histogram are plotted. But I'm sure I could achieve this otherwise with string columns or as you say I could just use an integer encoding.",
  "created_at":"2021-11-30T18:05:31Z",
  "id":982886618,
  "issue":516,
  "node_id":"IC_kwDOD6Q_ss46laja",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-11-30T18:05:31Z",
  "user":"MDQ6VXNlcjUxMjE4MjQ="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"the first screenshot is on two different branches of the TTree, the point was to that it's not limited to flat branch (one might think that's the only place where decompression dominates)",
  "created_at":"2021-12-02T00:47:03Z",
  "id":984189762,
  "issue":518,
  "node_id":"IC_kwDOD6Q_ss46qYtC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T00:49:39Z",
  "user":"MDQ6VXNlcjUzMDYyMTM="
 },
 {
  "author_association":"MEMBER",
  "body":"> the first screenshot is on two different branches of the TTree, the point was to that it's not limited to flat branch (one might think that's the only place where decompression dominates)\r\n\r\nI updated the original message with a better presentation of the demo.",
  "created_at":"2021-12-02T00:55:16Z",
  "id":984193846,
  "issue":518,
  "node_id":"IC_kwDOD6Q_ss46qZs2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T00:55:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"UnROOT issue/PR: https://github.com/tamasgal/UnROOT.jl/pull/137\r\nWith some more discussion linked within\r\n\r\nThe benchmark file comes from http://opendata.cern.ch/record/12341 ",
  "created_at":"2021-12-02T01:01:58Z",
  "id":984196943,
  "issue":518,
  "node_id":"IC_kwDOD6Q_ss46qadP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T01:03:41Z",
  "user":"MDQ6VXNlcjU3NjAwMjc="
 },
 {
  "author_association":"MEMBER",
  "body":"Originally, MemmapSource didn't copy, but there was a problem: if uncopied data gets included in an object that continues to live after the file is closed, later accessing that object raises a segfault. Let me see if I can demo that...\r\n\r\n(The test failure might be an example of that already.)\r\n\r\nIn Uproot's main branch:\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> file = uproot.open(skhep_testdata.data_path(\"uproot-Zmumu-uncompressed.root\"))\r\n>>> basket = file[\"events/px1\"].basket(0)\r\n>>> file.close()\r\n>>> basket.data\r\narray([192,  68, 152, ..., 241,  47, 230], dtype=uint8)\r\n```\r\n\r\nIn this PR branch:\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> file = uproot.open(skhep_testdata.data_path(\"uproot-Zmumu-uncompressed.root\"))\r\n>>> basket = file[\"events/px1\"].basket(0)\r\n>>> file.close()\r\n>>> basket.data\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nNote that it only segfaults if the data are not ever copied. So, for instance, if you ask for an array, there's a copy because the baskets get concatenated (still in the PR branch):\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> file = uproot.open(skhep_testdata.data_path(\"uproot-Zmumu-uncompressed.root\"))\r\n>>> array = file[\"events/px1\"].array(library=\"np\")  # NumPy to keep things simple\r\n>>> file.close()\r\n>>> array\r\narray([-41.19528764,  35.11804977,  35.11804977, ...,  32.37749196,\r\n        32.37749196,  32.48539387])\r\n```\r\n\r\nAlso, there's a copy if you have to decompress something:\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> file = uproot.open(skhep_testdata.data_path(\"uproot-Zmumu-zlib.root\"))\r\n>>> basket = file[\"events/px1\"].basket(0)\r\n>>> file.close()\r\n>>> basket.data\r\narray([192,  68, 152, ..., 241,  47, 230], dtype=uint8)\r\n```\r\n\r\nWhether something is a copy or not is haphazard. It might be unusual for a regular user to ask for a basket, rather than an array, but you could also get the segfault if you ask for an uncompressed histogram\u2014I think the bin contents are viewed from the original array, rather than copied. Randomly getting segfaults is no good.\r\n\r\nActually, I think I have an idea of how to make this better. I'm going to try committing to the PR branch.",
  "created_at":"2021-12-02T18:28:13Z",
  "id":984887131,
  "issue":519,
  "node_id":"IC_kwDOD6Q_ss46tC9b",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T18:28:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thank you very much for your detailed reply @jpivarski !\r\nI did not think about accessing data from a file after closing it, I personally would then make a copy of that data before closing by myself. (This would feel the most natural way for me.)\r\nAlso this is how it also would be in plain `numpy`:\r\n```python\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: np.save(\"foo.npy\", np.array([1,2,3,4]))\r\n\r\nIn [3]: data = np.memmap(\"foo.npy\", dtype=np.uint8, mode=\"r\")\r\n\r\nIn [4]: data\r\nOut[4]:\r\nmemmap([147,  78,  85,  77,  80,  89,   1,   0, 118,   0, 123,  39, 100,\r\n        101, 115,  99, 114,  39,  58,  32,  39,  60, 105,  56,  39,  44,\r\n         32,  39, 102, 111, 114, 116, 114,  97, 110,  95, 111, 114, 100,\r\n        101, 114,  39,  58,  32,  70,  97, 108, 115, 101,  44,  32,  39,\r\n        115, 104,  97, 112, 101,  39,  58,  32,  40,  52,  44,  41,  44,\r\n         32, 125,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,\r\n         32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,\r\n         32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,\r\n         32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  32,\r\n         32,  32,  32,  32,  32,  32,  32,  32,  32,  32,  10,   1,   0,\r\n          0,   0,   0,   0,   0,   0,   2,   0,   0,   0,   0,   0,   0,\r\n          0,   3,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,   0,\r\n          0,   0,   0,   0], dtype=uint8)\r\n\r\nIn [5]: data._mmap.close()\r\n\r\nIn [6]: data\r\nOut[6]: 'ipython' terminated by signal SIGSEGV (Address boundary error)\r\n```\r\nHowever a segementation fault is obviously the worst a user can get back. So I am very happy that you have already an idea! I'll keep an eye on your update :) \r\n\r\nBest, Peter",
  "created_at":"2021-12-02T18:47:36Z",
  "id":984901288,
  "issue":519,
  "node_id":"IC_kwDOD6Q_ss46tGao",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T18:47:36Z",
  "user":"MDQ6VXNlcjE4NDYzNTgy"
 },
 {
  "author_association":"MEMBER",
  "body":"If we let memmap arrays go uncopied, we now have to ensure that `detach_memmap()` is called on any Chunk that could be passed to the user. Fortunately, there aren't too many places where this could happen: if it's an object from a TDirectory (such as a histogram), we know whether it's compressed or not and call `detach_memmap()` if it's uncompressed. If it's a TBasket of a TTree, we find out whether it's compressed or not when constructing the TBasket object (`read_members`). The only other place where we might be asking for new Chunks is in RNTuple handling.\r\n\r\nIf a Chunk ever slips by and gets out of the scope in which it was created, even as an array sliced out of it by `cursor.bytes(chunk, ...)`, then it will be a subtle segfault bug that happens sometime after the file gets closed. We'd really like to have something like Rust's borrow checker to validate it! But, reading the code carefully and rereading it, I'm pretty sure this is right.",
  "created_at":"2021-12-02T19:52:03Z",
  "id":984951509,
  "issue":519,
  "node_id":"IC_kwDOD6Q_ss46tSrV",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2021-12-02T19:52:03Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @pfackeldey for code",
  "created_at":"2021-12-08T16:29:23Z",
  "id":988970526,
  "issue":519,
  "node_id":"IC_kwDOD6Q_ss468n4e",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2021-12-08T16:29:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot4/pull/523) to add @pfackeldey! :tada:",
  "created_at":"2021-12-08T16:29:31Z",
  "id":988970670,
  "issue":519,
  "node_id":"IC_kwDOD6Q_ss468n6u",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-08T16:29:31Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"It depends on the Python process because an object made `from_pyroot` has a dynamically created class: the data from PyROOT may be a type we've never seen before, so we have to create the class object before creating the instance.\r\n\r\nHere's a histogram made entirely with non-dynamic classes: `to_writable` makes an object using some hard-coded classes (i.e. the standard way) because the object is something we might want to write to a file, so we have to know how to serialize it.\r\n\r\n```python\r\n>>> import uproot\r\n>>> import numpy as np\r\n>>> h_not_dynamic = uproot.to_writable(np.histogram([1, 2, 3]))\r\n>>> type(h_not_dynamic).mro()\r\n[<class 'uproot.models.TH.Model_TH1D_v3'>,\r\n <class 'uproot.behaviors.TH1.TH1'>,\r\n <class 'uproot.behaviors.TH1.Histogram'>,\r\n <class 'uproot.model.VersionedModel'>,\r\n <class 'uproot.model.Model'>, <class 'object'>]\r\n```\r\n\r\nHere's an object made `from_pyroot`. In principle, it could have any type from the ROOT library or even types defined by users. We create a class object for it and put that in the `uproot.dynamic` namespace, so that we can instantiate the histogram.\r\n\r\n```python\r\n>>> import ROOT\r\n>>> h_root = ROOT.TH1D(\"h\", \"\", 100, 0, 1)\r\n>>> h_dynamic = uproot.from_pyroot(h_root)\r\n>>> type(h_dynamic).mro()\r\n[<class 'uproot.dynamic.Model_TH1D_v3'>,\r\n <class 'uproot.behaviors.TH1.TH1'>,\r\n <class 'uproot.behaviors.TH1.Histogram'>,\r\n <class 'uproot.dynamic.Model_TH1D_v3'>,\r\n <class 'uproot.model.VersionedModel'>,\r\n <class 'uproot.model.Model'>, <class 'object'>]\r\n```\r\n\r\nWe can pickle them both:\r\n\r\n```python\r\n>>> import pickle\r\n>>> pickle.dump(h_not_dynamic, open(\"h_not_dynamic.pkl\", \"wb\"))\r\n>>> pickle.dump(h_dynamic, open(\"h_dynamic.pkl\", \"wb\"))\r\n```\r\n\r\nand if I tried to unpickle them, they'd both be fine (you've already seen that). The `uproot.dynamic` namespace has a lot of classes in it, created by the `from_pyroot`.\r\n\r\n```python\r\n>>> dir(uproot.dynamic)\r\n['Model_TAttAxis',\r\n 'Model_TAttAxis_v4',\r\n 'Model_TAttFill',\r\n 'Model_TAttFill_v2',\r\n 'Model_TAttLine',\r\n 'Model_TAttLine_v2',\r\n 'Model_TAttMarker',\r\n 'Model_TAttMarker_v2',\r\n 'Model_TAxis',\r\n 'Model_TAxis_v10',\r\n 'Model_TH1',\r\n 'Model_TH1D',\r\n 'Model_TH1D_v3',\r\n 'Model_TH1_v8',\r\n 'Model_TNamed',\r\n 'Model_TNamed_v1',\r\n 'Model_TObject',\r\n 'Model_TObject_v1',\r\n '__builtins__',\r\n '__cached__',\r\n '__doc__',\r\n '__file__',\r\n '__getattr__',\r\n '__loader__',\r\n '__name__',\r\n '__package__',\r\n '__spec__']\r\n```\r\n\r\nWhen we start a new Python process, the `uproot.dynamic` namespace is empty:\r\n\r\n```python\r\n>>> import uproot\r\n>>> dir(uproot.dynamic)\r\n['__builtins__',\r\n '__cached__',\r\n '__doc__',\r\n '__file__',\r\n '__getattr__',\r\n '__loader__',\r\n '__name__',\r\n '__package__',\r\n '__spec__']\r\n```\r\n\r\nWe can read the first one (no dynamic classes), but not the second one.\r\n\r\n```python\r\n>>> pickle.load(open(\"h_not_dynamic.pkl\", \"rb\"))\r\n<TH1D (version 3) at 0x7f7f734e6dc0>\r\n>>> pickle.load(open(\"h_dynamic.pkl\", \"rb\"))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jpivarski/irishep/uproot4/src/uproot/model.py\", line 1638, in __setstate__\r\n    cls.__bases__ = (\r\nTypeError: __bases__ assignment: 'TAxis' object layout differs from 'DynamicModel'\r\n```\r\n\r\nWhat's supposed to happen is that the unpickler tries to access a class from the `uproot.dynamic` namespace and then this code (the only thing in that file) generates it on demand:\r\n\r\nhttps://github.com/scikit-hep/uproot4/blob/8ae497a89ec92f06724980fccafa1c4286ce6040/src/uproot/dynamic.py#L17-L24\r\n\r\nPython's unpickler is something we can't control. It looks at the class name in the pickle data and tries to find such a class. If it doesn't find the class, it gives up before we have a chance to do anything about that; that's why any attempt to access a name from `uproot.dynamic` results in a subclass of `uproot.model.DynamicModel` being created on the spot (unless such a subclass has already been created once).\r\n\r\nSo far, so good, but in the unpickling process, we know that this histogram has convenience methods ([uproot.behaviors.TH1.TH1](https://uproot.readthedocs.io/en/latest/uproot.behaviors.TH1.TH1.html)) but the dynamic class has been created without them. We should be able to change the `__bases__` to add them, since behaviors are stateless collections of methods (they're \"mixins\"). But Python is complaining about that. I don't know why. I don't think it violates any of the rules [described here](https://stackoverflow.com/a/3308895/1623645).\r\n\r\nI'm going to look into it.",
  "created_at":"2021-12-02T20:42:04Z",
  "id":984985268,
  "issue":520,
  "node_id":"IC_kwDOD6Q_ss46ta60",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T20:44:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I found it. The TAxis behavior was a subclass of `collections.abc.Sequence`, which apparently isn't a normal Python object. So that's a new rule for us: behaviors can't be collections. PR #521.",
  "created_at":"2021-12-02T21:02:23Z",
  "id":984998389,
  "issue":520,
  "node_id":"IC_kwDOD6Q_ss46teH1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-02T21:02:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thanks so much for the speedy response!",
  "created_at":"2021-12-03T19:12:57Z",
  "id":985763866,
  "issue":520,
  "node_id":"IC_kwDOD6Q_ss46wZAa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-03T19:12:57Z",
  "user":"MDQ6VXNlcjEyNjE2NDU4"
 },
 {
  "author_association":"MEMBER",
  "body":"These are great, but please don't do this on the Awkward codebase yet: I'm in the middle of a \"change everything\" PR (scikit-hep/awkward-1.0#1184) that wouldn't merge well with this sort of thing.",
  "created_at":"2021-12-10T16:48:01Z",
  "id":991128986,
  "issue":526,
  "node_id":"IC_kwDOD6Q_ss47E22a",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-10T16:48:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, no problem. Will stick with this. :) I'm going to do a couple more changes (I should be able to detect and replace `{repr(x)}` with `{x!r}`). \r\n\r\nI think the main question: do you want to chain or hide the ModuleNotFoundErrors in extras?",
  "created_at":"2021-12-10T16:49:40Z",
  "id":991130228,
  "issue":526,
  "node_id":"IC_kwDOD6Q_ss47E3J0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-10T16:49:40Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I've also got a few more changes I want to annotate above, so I put it into draft",
  "created_at":"2021-12-10T16:50:13Z",
  "id":991130634,
  "issue":526,
  "node_id":"IC_kwDOD6Q_ss47E3QK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-10T16:50:13Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, I think I'm done with this. Some things (like + -> strings) could be worked on in the future, but don't need to be here.",
  "created_at":"2021-12-10T17:34:40Z",
  "id":991162557,
  "issue":526,
  "node_id":"IC_kwDOD6Q_ss47E_C9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-10T17:34:40Z",
  "user":"MDQ6VXNlcjQ2MTY5MDY="
 },
 {
  "author_association":"MEMBER",
  "body":"I scanned through all the changes and it looks good! With so many changes, the real proof that it's safe is in the tests. After it's merged, the Coffea tests should pick it up (is that right, @nsmith-?) and as a squashed commit, we have the option to revert it if there's a problem.\r\n\r\nThank you very much! (Also for that note about when `tobytes` was added to NumPy. We're still testing from 1.13.1 onward.)",
  "created_at":"2021-12-10T17:49:24Z",
  "id":991172429,
  "issue":526,
  "node_id":"IC_kwDOD6Q_ss47FBdN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-10T17:49:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes coffea requires `awkward>=1.5.1,<2` and the daily tests will end up picking the latest version once its on pypi",
  "created_at":"2021-12-10T20:19:44Z",
  "id":991268042,
  "issue":526,
  "node_id":"IC_kwDOD6Q_ss47FYzK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-10T20:19:44Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"This was a bug (fixed in #530). The cut string has the same performance\u2014in terms of speed and memory use\u2014as doing the same thing in Python because it _is_ doing the same thing in Python. This bug came about because it wasn't exactly the same thing; the fix was to make it the same thing.\r\n\r\nWhat was happening here is that the first column, `batch[\"event\"]`, is not jagged and the cut, `batch[\"genpart_pid\"] == 22`, is jagged. The implementation of the cut string was slicing `batch[\"event\"]` as a Series with a simple RangeIndex with `batch[\"genpart_pid\"] == 22`, a Series with a MultiIndex. The `x[y]` expression where `x` is a RangeIndex and `y` is a MultiIndex was failing. I would have thought that it would broadcast the RangeIndex (the event numbers) into the first part of the MultiIndex (also event numbers), but Pandas just failed with a not-very-descriptive error.\r\n\r\nWhen you were applying the same cut outside of the cut string, `batch[\"event\"]` was already integrated into the DataFrame that has a MultiIndex\u2014its event numbers were already broadcasted to the entry-subentry pairs. The cut worked outside of the cut string because Pandas could easily match the MultiIndex with the MultiIndex.\r\n\r\nThe fix, therefore, was to detect these situations and manually broadcast individual columns into a MultiIndex if the cut has one, eliminating this corner-case distinction between what the cut string was doing and what would happen if you computed it outside of the cut string.\r\n\r\nIf I sound a bit grumbly, it's because I think that Pandas's whole MultiIndex thing is not very well thought-through, and I think I might have made my life difficult by including this cut string option, especially seeing how it combines in weird ways with Pandas. This bug is fixed; I hope there aren't any more.\r\n\r\n`:)`",
  "created_at":"2021-12-15T17:19:00Z",
  "id":994999782,
  "issue":529,
  "node_id":"IC_kwDOD6Q_ss47Tn3m",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-15T17:19:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I can't see the file (IP address isn't public\u2014not refused; it's probably an internal network).\r\n\r\nBut anyway, `TestBit` checks the `fBits` of a TObject. In ROOT, a TFile is a TObject, but I don't know how the bytes on disk becomes a TObject (it's not the way _contained_ objects become TObjects, such as TH1F, for instance). You've probably seen that it's a ReadOnlyFile, one of the three classes that isn't generically mappable to the corresponding ROOT type (along with ReadOnlyKey and ReadOnlyDirectory, because they're too fundamental).\r\n\r\nBut anyway, I think that the `kRecover` bit is only set _in_ ROOT's `Recover` function, so the `Recover` has to be invoked before you know that a file needs recovering. The general strategy that Uproot takes is to be very hands-off: there are a lot of internal consistency conditions in the ROOT format that it does not check. Under normal circumstances, it doesn't even read the streamers (to reduce file-opening time).\r\n\r\nDoing a `Recover`, or even just verifying that one would be fruitful, isn't as bad as a \"grep\" for TKeys, though I can't think of a byte string that could be grepped for, anyway. It would be a walk over the data that _isn't_ listed in the TFree list of unused data. Regions of the file that aren't labeled by TFree as being garbage need to be back-to-back TKey-object pairs, where each TKey specifies the size of the object (compressed), and therefore where to jump to next. Regardless of language (Python, C++, Julia), that would be an expensive thing to check on remote files: it's a long series of data round-trips\u2014you only know where to seek next based on the values you see in the current TKey. A big file would have _a lot_ of TKeys.\r\n\r\nThis function would definitely have to be opt-in, and you only know that something's wrong if it has run.\r\n\r\nROOT maintains a list of TKeys in a file independent of the objects that need it (such as TTrees pointing to TBaskets, or TDirectories pointing to the data they contain). I don't know what that second, unstructured list is good for. This recovery function can tell us that something's wrong\u2014a TKey-object pair exists with no path to it\u2014but how does that help if there's no path to it? How does a user access that data?\r\n\r\nNevertheless, checking consistency is one positive benefit. The function would be named something like, \"Is this file okay?\"\r\n\r\nWhen UnROOT's cursor is running away, do you know what it's attempting to do? What it's trying to read?\r\n\r\n-------\r\n\r\nOn a completely different topic, I'd like to ask you about another potential project. What's the best way to reach you for back-and-forth conversation, like Gitter/Slack/Mattermost/Zoom?",
  "created_at":"2021-12-21T14:58:08Z",
  "id":998846651,
  "issue":531,
  "node_id":"IC_kwDOD6Q_ss47iTC7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-21T14:58:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the detailed answer, as always \u02cb:)\u02cb\r\n\r\nAh my bad, I forgot to start the Webserver on my machine, the file is now available if you want to play around.\r\n\r\nMy naive hope that there is a simple global flag somewhere which indicates that a file was properly closed is obviously destroyed \u02cb;)\u02cb Yes, I found \u02cb ReadOnlyFile\u02cb and thought I could simply dig there to figure out how to get to that magical bit but I guess you are right about \u02cbkRecover\u02cb being some runtime product.\r\n\r\nThe idea to check the \u02cbTKey\u02cb list and pairing those up to find missing pieces sounds like an easy way to indicates. For our use-case at least the recovery is not so important. I\u2019ll have a look tomorrow to see if I find some hints. \r\n\r\nRegarding \u02cbUnROOT.jl\u02cb, I have not debugged further yet but it might be related to the streamer part, which is always being parsed completely. Let\u2019s see \u02cb;)\u02cb\r\n\r\n***\r\n\r\nCurious about that project! You can find me e.g. on the Julia Slack but I also still have PyHEP2021 in my Slack workspace. Zoom is of course fine too.\r\n\r\n",
  "created_at":"2021-12-22T02:10:31Z",
  "id":999227688,
  "issue":531,
  "node_id":"IC_kwDOD6Q_ss47jwEo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2021-12-22T02:10:31Z",
  "user":"MDQ6VXNlcjE3MzAzNTA="
 }
]
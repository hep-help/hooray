[
 {
  "author_association":"MEMBER",
  "body":"@lobis, @nsmith-, is this something that can be done with [fsspec-xrootd](https://github.com/CoffeaTeam/fsspec-xrootd) now?\r\n\r\nIt could be cool to finish off an issue with a low number like #5!",
  "created_at":"2024-01-30T15:51:17Z",
  "id":1917313347,
  "issue":5,
  "node_id":"IC_kwDOD6Q_ss5yR-FD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:51:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I think this ties very closely with https://github.com/CoffeaTeam/fsspec-xrootd/issues/36 The implementations would share a lot of the same logic w.r.t. keeping open connections to multiple servers and ensuring we use the best server for a file at any given time.",
  "created_at":"2024-01-30T16:04:33Z",
  "id":1917361733,
  "issue":5,
  "node_id":"IC_kwDOD6Q_ss5ySJ5F",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2024-01-30T16:04:33Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"MEMBER",
  "body":"This must be effectively done by now.",
  "created_at":"2024-01-30T15:52:51Z",
  "id":1917318994,
  "issue":41,
  "node_id":"IC_kwDOD6Q_ss5yR_dS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:52:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I don't think anyone cares about histograms-as-Pandas-DataFrames. Although (I think) it's an interesting concept, boost-histogram and hist are well developed now, and users would much prefer that interface over some modification of Pandas.",
  "created_at":"2024-01-30T15:55:19Z",
  "id":1917328605,
  "issue":91,
  "node_id":"IC_kwDOD6Q_ss5ySBzd",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:55:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"With XRootD being handled by [fsspec-xrootd](https://github.com/CoffeaTeam/fsspec-xrootd) now, I believe this is working. (I remember seeing some tests in the test suite that used wildcards on an XRootD server.)",
  "created_at":"2024-01-30T16:00:23Z",
  "id":1917346069,
  "issue":193,
  "node_id":"IC_kwDOD6Q_ss5ySGEV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T16:00:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It looks like this issue was taken up in #277 and solved. If I'm mistaken, please open a new issue with whatever is still remaining to be solved. Thanks!",
  "created_at":"2024-01-30T16:06:09Z",
  "id":1917367091,
  "issue":268,
  "node_id":"IC_kwDOD6Q_ss5ySLMz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T16:06:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Sorry that I missed this 3 years ago, and I know that it's a small commit, but I want to be sure to include everyone who helped with this repo. Thanks!\r\n\r\n@all-contributors please add @bnavigator for test\r\n",
  "created_at":"2024-01-18T16:58:11Z",
  "id":1898865940,
  "issue":396,
  "node_id":"IC_kwDOD6Q_ss5xLmUU",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-18T16:58:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/1087) to add @bnavigator! :tada:",
  "created_at":"2024-01-18T16:58:21Z",
  "id":1898866215,
  "issue":396,
  "node_id":"IC_kwDOD6Q_ss5xLmYn",
  "performed_via_github_app":"MDM6QXBwMjMxODY=",
  "reactions":{},
  "updated_at":"2024-01-18T16:58:21Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"MEMBER",
  "body":"I think I kept this open to be informative, but we can do that with Discussions now, so I'll convert this Issue into a Discussion.",
  "created_at":"2024-01-30T16:17:08Z",
  "id":1917400656,
  "issue":423,
  "node_id":"IC_kwDOD6Q_ss5ySTZQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T16:17:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This issue was fixed in #595 ",
  "created_at":"2024-02-15T15:16:59Z",
  "id":1946303123,
  "issue":504,
  "node_id":"IC_kwDOD6Q_ss50AjqT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-15T15:16:59Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"This could piggyback on #1090's interface to use alternate ZLIB libraries. That PR introduces a back-door to add ISAL.\r\n\r\nHow does deflate compare with ISAL (in decompression time and compressed file size)? Do we want two back doors?",
  "created_at":"2024-01-30T15:42:41Z",
  "id":1917280147,
  "issue":518,
  "node_id":"IC_kwDOD6Q_ss5yR1-T",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:42:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm closing this because it's over a year old and we can't proceed without a test file. Also, `uproot.dask` has changed a lot in that time; maybe it works now.\r\n\r\nIf this is still an issue, ask to have the issue reopened (with a file that we can use to reproduce it). Thanks!",
  "created_at":"2024-01-30T15:34:07Z",
  "id":1917240738,
  "issue":811,
  "node_id":"IC_kwDOD6Q_ss5yRsWi",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:34:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #1000 ensures that TProfiles can be read from ROOT and written to ROOT with 100% fidelity. It doesn't show that they can be taken from or to hist/boost-histogram, but that's a separate issue. When this is closed by PR #1000 getting merged, you may still want to check that.\r\n\r\nThe thing I'm uncertain about is whether hist/boost-histogram's `WeightedMean` histograms have enough per-bin data to represent a ROOT (HBOOK, SUMX, ...)-style \"profile plot,\" which is [more involved](https://root.cern.ch/doc/master/classTProfile.html) than simply tracking the weighted mean of each bin.",
  "created_at":"2024-01-18T15:35:37Z",
  "id":1898711688,
  "issue":908,
  "node_id":"IC_kwDOD6Q_ss5xLAqI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-18T15:35:37Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for implementing this! I ran some tests as a brief follow up and writing `hist` objects works as requested, thanks!\r\n\r\nFor my overall goal of merging and saving profiles, I think something is still going wrong around the to/from hist conversion (as you suspected). eg. If I add two `WeightedMean()` profiles created via hist, I get the correct answer, but if I read a profile via uproot, convert to_hist and then add them, I get the wrong answer. In any case, the new writing support is good progress, and I'll have to dig into this further in another issue some other time",
  "created_at":"2024-01-30T19:28:01Z",
  "id":1917739926,
  "issue":908,
  "node_id":"IC_kwDOD6Q_ss5yTmOW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T19:28:01Z",
  "user":"MDQ6VXNlcjE1NzE5Mjc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I looked into this error and attempted to fix it. I didn\u2019t find the fix, but I did isolate where the issue is coming from. Thus, I will write here my findings. \u2028\u2028\u2028\r\n\r\nThe problem is in `writable.py`:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/c128e4bebdd5aad2bfaa5bc5f4ce896fe7bd9e72/src/uproot/writing/writable.py#L1323-L1334\r\n\r\n\r\n\r\nIf we only have L1333, i.e. `uproot.models.TTree.Model_TTree_v20` in the list of models, then the Map() issue disappears. \r\n\r\nThus, the issue comes from some of the streamers in `TLeaf` and `TBranch`. In each `TLeaf` type the issue goes away if we comment out this part: \r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/c128e4bebdd5aad2bfaa5bc5f4ce896fe7bd9e72/src/uproot/models/TLeaf.py#L185-L192\r\n\r\nWhere `_rawstreamer_TLeaf_v2` is defined here:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/c128e4bebdd5aad2bfaa5bc5f4ce896fe7bd9e72/src/uproot/models/TLeaf.py#L14-L19\r\n\r\nAs for `TBranch`, the issue is fixed when commenting out lines 562 to 568: \r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/c128e4bebdd5aad2bfaa5bc5f4ce896fe7bd9e72/src/uproot/models/TBranch.py#L559-L569\r\n\r\n\r\n",
  "created_at":"2024-01-25T14:56:22Z",
  "id":1910375083,
  "issue":931,
  "node_id":"IC_kwDOD6Q_ss5x3gKr",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T14:56:22Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"Part of \"The Bad\" (\"Awkward Array does not support arrays with object dtypes\") seems to be the same as Issue #1101.",
  "created_at":"2024-01-25T15:53:08Z",
  "id":1910487315,
  "issue":951,
  "node_id":"IC_kwDOD6Q_ss5x37kT",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:53:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The new **docs/readthedocs.org:uproot** test requirement can be satisfied by merging #1084/[jpivarski/fix-readthedocs-documentation](https://github.com/scikit-hep/uproot5/tree/jpivarski/fix-readthedocs-documentation) into this PR (or by merging in `main`, which has it).",
  "created_at":"2024-01-16T19:08:11Z",
  "id":1894351313,
  "issue":1000,
  "node_id":"IC_kwDOD6Q_ss5w6YHR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-16T19:08:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This PR is ready to be merged; it adds the ability to write TProfile objects to ROOT files and the test is 100% showing that they can be round-tripped from ROOT to Uproot to ROOT again.\r\n\r\nThis doesn't say anything about whether these TProfiles can come from or go to boost-histogram, but that's a separate problem for boost-histogram/hist if it turns out that they can't.",
  "created_at":"2024-01-18T15:32:25Z",
  "id":1898706025,
  "issue":1000,
  "node_id":"IC_kwDOD6Q_ss5xK_Rp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-18T15:32:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The new **docs/readthedocs.org:uproot** test requirement can be satisfied by merging #1084/[jpivarski/fix-readthedocs-documentation](https://github.com/scikit-hep/uproot5/tree/jpivarski/fix-readthedocs-documentation) into this PR (or by merging in `main`, which has it).",
  "created_at":"2024-01-16T19:08:10Z",
  "id":1894351283,
  "issue":1004,
  "node_id":"IC_kwDOD6Q_ss5w6YGz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-16T19:08:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The new **docs/readthedocs.org:uproot** test requirement can be satisfied by merging #1084/[jpivarski/fix-readthedocs-documentation](https://github.com/scikit-hep/uproot5/tree/jpivarski/fix-readthedocs-documentation) into this PR (or by merging in `main`, which has it).",
  "created_at":"2024-01-16T19:08:08Z",
  "id":1894351256,
  "issue":1026,
  "node_id":"IC_kwDOD6Q_ss5w6YGY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-16T19:08:08Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"If this is for histograms only, could this be a hist issue? (I can transfer it.) If so, then the workflow would be\r\n\r\n```python\r\nroot_file[\"hist_name\"].to_hist().plotly()\r\n```\r\n\r\nI can transfer this issue if that's the case.\r\n\r\nIf this is also for TGraphs, then something would have to be done in Uproot as well.\r\n\r\nIt might also make sense for this to be an extremely lightweight package, `uproot-plotly`, that provides functions that extract data from Uproot (and hist's?) public APIs and sends them to plotly. It's already the case that hist's Matplotlib plotting is handled by a lightweight package, [mplhep](https://github.com/scikit-hep/mplhep).\r\n\r\n[scientific-python/cookie](https://github.com/scientific-python/cookie) makes short work of creating a new package. (Dividing work-to-do into small packages that only access other packages through public APIs is a good way to keep the ecosystem maintainable, and it provides exposure to early career developers because each package has a clear author.)",
  "created_at":"2024-01-30T15:29:43Z",
  "id":1917218380,
  "issue":1037,
  "node_id":"IC_kwDOD6Q_ss5yRm5M",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:29:43Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"ok,thanks.",
  "created_at":"2024-01-31T05:09:55Z",
  "id":1918394796,
  "issue":1037,
  "node_id":"IC_kwDOD6Q_ss5yWGGs",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-31T05:09:55Z",
  "user":"MDQ6VXNlcjczMDU4MDYy"
 },
 {
  "author_association":"MEMBER",
  "body":"I just did a test with a new conda environment in which `python=3.9`.\r\n\r\n```bash\r\nconda install -c conda-forge uproot   # also with mamba\r\n```\r\n\r\ninstalled Uproot 5.2.1. If it's still not working, it's something conda-specific that's holding it back\u2014maybe mixed channels?\r\n\r\nAnyway, I don't see anything that we can do here (we can't _remove_ old versions of Uproot that are associated with an old version of Python), so I'll be closing this now.",
  "created_at":"2024-01-18T15:04:11Z",
  "id":1898652195,
  "issue":1044,
  "node_id":"IC_kwDOD6Q_ss5xKyIj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-18T15:04:11Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"This might be related to #1048, with the Daskified version encountering errors where an eager version does not encounter errors. If the eager version does not encounter errors, the Daskified should not encounter errors either\u2014it's calling the same code, just at a later time on a Dask worker instead of the head node.\r\n\r\nOh!!! Maybe the Dask worker has an outdated version of Uproot? Maybe that's why you see different errors when running eagerly or lazily, because it's running different versions of Uproot in the two cases?",
  "created_at":"2024-01-25T15:45:10Z",
  "id":1910472152,
  "issue":1046,
  "node_id":"IC_kwDOD6Q_ss5x333Y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:45:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Asking for the Daskified mode to raise or not raise the same _interpretation or deserialization_ errors (something that nothing to do with delaying computations) as the eager mode is not a feature request.",
  "created_at":"2024-01-25T15:46:21Z",
  "id":1910474471,
  "issue":1046,
  "node_id":"IC_kwDOD6Q_ss5x34bn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:46:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"> Oh!!! Maybe the Dask worker has an outdated version of Uproot? Maybe that's why you see different errors when running eagerly or lazily, because it's running different versions of Uproot in the two cases?\r\n\r\nI was not sure how I ran this originally, but I just reproduced this locally on my laptop so I think it is not an issue with different uproot versions. A list of possibly relevant package versions (just did a new install):\r\n```\r\nPackage            Version\r\n------------------ ---------\r\nawkward            2.5.2\r\nawkward-cpp        28\r\ndask               2024.1.0\r\ndask-awkward       2024.1.2\r\nfsspec             2023.12.2\r\nnumpy              1.26.3\r\nuproot             5.2.1\r\nzstandard          0.22.0\r\n```\r\n\r\nWe now also have a publicly available file in the same format that can be used to reproduce the behavior above, sitting on EOS:\r\n```bash\r\nxrdcp root://eosuser.cern.ch//eos/user/f/feickert/physlite_public_testing/DAOD_PHYSLITE.34858087._000001.pool.root.1 .\r\n```",
  "created_at":"2024-01-25T16:08:24Z",
  "id":1910519676,
  "issue":1046,
  "node_id":"IC_kwDOD6Q_ss5x4Dd8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T16:08:24Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"I agree that the second error message makes sense: you can't fill a histogram with an unflattenable ragged array.\r\n\r\nThe first error message text is the text you would get with a DeserializationError, but the type of the exception is ValueError. It's begin swapped somewhere in dask-awkward.\r\n\r\nBut also, they're not the same error. With the DeserializationError (TBasket seems to have the wrong number of bytes), you never get to the stage where you have an Awkward Array. With the histogram-filling error, the error is in filling the histogram with an existing Awkward Array.\r\n\r\nWhat we're seeing here is that when dask-awkward asks Uproot to interpret the file, Uproot can't read it, but when calling Uproot directly to read the file, it can. The hisogram-filling problem afterward is not related to that because it comes much later. It doesn't make sense to me that dask-awkward would encounter a DeserializationError on the same file the Uproot can read, unless it's an intermittent DeserializationError (server sends good and bad versions of the file randomly?) that you just happened to see on a dask-awkward run and not on the only-Uproot run.\r\n\r\nThere's a problem here to solve, but it doesn't have to do with histograms. We might need more tests to narrow in on what's actually going wrong.",
  "created_at":"2024-01-25T15:36:46Z",
  "id":1910455385,
  "issue":1048,
  "node_id":"IC_kwDOD6Q_ss5x3zxZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:36:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(When we have a better idea of what's going on, we'll need to change the title of the Issue.)",
  "created_at":"2024-01-25T15:37:14Z",
  "id":1910456390,
  "issue":1048,
  "node_id":"IC_kwDOD6Q_ss5x30BG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:37:14Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think this issue has resolved itself: I just tried it again with updated dependencies and now obtain a reasonable error message:\r\n\r\n```pytb\r\n    self._hist.fill(*args_ars, weight=weight_ars, sample=sample_ars)  # type: ignore[arg-type]\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: All arrays must be 1D\r\n```\r\nThis is using the following:\r\n```\r\nawkward            2.5.2\r\nawkward-cpp        28\r\nboost-histogram    1.4.0\r\ncoffea             2024.1.2\r\ndask               2024.1.0\r\ndask-awkward       2024.1.2\r\ndask-histogram     2023.10.0\r\nhist               2.7.2\r\nuproot             5.2.1\r\n```\r\n\r\nGiven that it now seems fine, from my side we can close this one.\r\n\r\n> unless it's an intermittent DeserializationError (server sends good and bad versions of the file randomly?) that you just happened to see on a dask-awkward run and not on the only-Uproot run\r\n\r\nI was running locally with a local file, so this was presumably not the case. Some updates in dask-awkward or elsewhere must have fixed this instead I assume.",
  "created_at":"2024-01-25T16:32:58Z",
  "id":1910566131,
  "issue":1048,
  "node_id":"IC_kwDOD6Q_ss5x4Ozz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T16:32:58Z",
  "user":"MDQ6VXNlcjQ1MDA5MzU1"
 },
 {
  "author_association":"MEMBER",
  "body":"> Some updates in dask-awkward or elsewhere must have fixed this instead I assume.\r\n\r\nThat's entirely likely, and I'm also willing to be optimistic, so I'll close this issue now. Thanks!",
  "created_at":"2024-01-25T16:40:25Z",
  "id":1910579523,
  "issue":1048,
  "node_id":"IC_kwDOD6Q_ss5x4SFD",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T16:40:25Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski reminder of this one, but again low priority.",
  "created_at":"2024-01-03T19:17:21Z",
  "id":1875847045,
  "issue":1062,
  "node_id":"IC_kwDOD6Q_ss5vzyeF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-03T19:17:21Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Yeah, I know. Okay, I'll try it out now.",
  "created_at":"2024-01-03T21:35:06Z",
  "id":1876002138,
  "issue":1062,
  "node_id":"IC_kwDOD6Q_ss5v0YVa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-03T21:35:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It worked, and I don't see any reason to not use hatch-vcs. Awkward Array uses hatchling as well, though its release process is more complex because there's an awkward-cpp part that absolutely must be versioned by hand. (Its version number is essentially an ABI between the two parts of the project, so we want to control that.)",
  "created_at":"2024-01-03T22:10:12Z",
  "id":1876036712,
  "issue":1062,
  "node_id":"IC_kwDOD6Q_ss5v0gxo",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-03T22:10:12Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This was not resolved:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File /lib/python3.11/site-packages/distributed/protocol/pickle.py\", line 63, in dumps\r\n    result = pickle.dumps(x, **dump_kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File /lib/python3.11/site-packages/distributed/protocol/pickle.py\", line 68, in dumps\r\n    pickler.dump(x)\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File /lib/python3.11/site-packages/distributed/protocol/serialize.py\", line 353, in serialize\r\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File /lib/python3.11/site-packages/distributed/protocol/serialize.py\", line 76, in pickle_dumps\r\n    frames[0] = pickle.dumps(\r\n                ^^^^^^^^^^^^^\r\n  File /lib/python3.11/site-packages/distributed/protocol/pickle.py\", line 81, in dumps\r\n    result = cloudpickle.dumps(x, **dump_kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File /lib/python3.11/site-packages/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\r\n    cp.dump(obj)\r\n  File /lib/python3.11/site-packages/cloudpickle/cloudpickle_fast.py\", line 632, in dump\r\n    return Pickler.dump(self, obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: cannot pickle '_thread.lock' object\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 11, in <module>\r\n    print(dask.compute(events[\"branch\"]))\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File /lib/python3.11/site-packages/dask/base.py\", line 628, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File /lib/python3.11/site-packages/distributed/protocol/serialize.py\", line 379, in serialize\r\n    raise TypeError(msg, str_x) from exc\r\nTypeError: ('Could not serialize object of type HighLevelGraph', '<ToPickle: HighLevelGraph with 1 layers.\\n<dask.highlevelgraph.HighLevelGraph object at 0x7f19ff0041d0>\\n 0. branch-f4983d883434c3c122dafd5d2e31979f\\n>')\r\n```",
  "created_at":"2024-01-06T18:14:25Z",
  "id":1879772927,
  "issue":1063,
  "node_id":"IC_kwDOD6Q_ss5wCw7_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-06T18:14:25Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"This seems like a user error, `_thread.lock` should never show up as an object in a task graph to begin with.\r\n\r\nPlease provide a reproducer!",
  "created_at":"2024-01-06T19:14:56Z",
  "id":1879791174,
  "issue":1063,
  "node_id":"IC_kwDOD6Q_ss5wC1ZG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-06T19:14:56Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"As well as versions of awkward/dask-awkward/uproot.",
  "created_at":"2024-01-06T19:16:21Z",
  "id":1879791432,
  "issue":1063,
  "node_id":"IC_kwDOD6Q_ss5wC1dI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-06T19:16:21Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@lgray please checkout https://github.com/veprbl/uproot-issue1063\r\n\r\nThis is reproducible with\r\n```\r\nawkward.__version__ 2.5.1\r\ndask.__version__ 2023.12.0\r\ndask_awkward.__version__ 2023.12.2\r\ndistributed.__version__ 2023.12.0\r\nuproot.__version__ 5.2.0\r\n```",
  "created_at":"2024-01-06T21:15:31Z",
  "id":1879831459,
  "issue":1063,
  "node_id":"IC_kwDOD6Q_ss5wC_Oj",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-06T21:15:31Z",
  "user":"MDQ6VXNlcjI0NTU3Mw=="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Cool, glad it's not user error. We'll see what's up with it.\r\n\r\n@lobis ",
  "created_at":"2024-01-06T21:23:35Z",
  "id":1879833327,
  "issue":1063,
  "node_id":"IC_kwDOD6Q_ss5wC_rv",
  "performed_via_github_app":null,
  "reactions":{
   "heart":1,
   "total_count":1
  },
  "updated_at":"2024-01-06T21:23:35Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Some history of the `_pandas_memory_efficient` function:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/63bb43f9686d6af0511ee4ac6caa08bad18469f7/src/uproot/interpretation/library.py#L787-L803\r\n\r\nIt was added in #281 to solve #277. It was one of many different attempts to construct a `pd.DataFrame`, and it's not completely satisfactory because it calls `gc.collect()` explicitly, which we usually shouldn't do in production code. Since we already have the data in arrays, our goal is to give them to Pandas in such a way that Pandas doesn't copy, rewrite, or iterate through them, and I'm surprised at how many attempts it has taken to try to do that. There are many ways to create a `pd.DataFrame`; we want to find the way that takes over our arrays so that we can just hand them off.\r\n\r\nOriginally, that function was for a special case, but in #734, it became the only way. (Kush removed the complex code that \"exploded\" ragged arrays into columns with `pd.MultiIndex` in favor of using [awkward-pandas](https://github.com/intake/awkward-pandas), and now `_pandas_memory_efficient` is the only path that creates a `pd.DataFrame`, I believe.)\r\n\r\n@ioanaif is investigating `pd.DataFrame` construction.",
  "created_at":"2024-01-11T14:52:53Z",
  "id":1887350852,
  "issue":1070,
  "node_id":"IC_kwDOD6Q_ss5wfrBE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-11T14:52:53Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Appending to a dataframe in pandas has O(n^2) complexity because in each iteration a new dataframe is created and the data is copied over. Thus the call to [pandas_memory_efficient](https://github.com/scikit-hep/uproot5/blob/63bb43f9686d6af0511ee4ac6caa08bad18469f7/src/uproot/interpretation/library.py#L879\u00a0) is not needed and indeed it raises a performance warning when a lot of columns are involved. \u2028\u2028With pandas 2.1.4 the best course of action is to build the dataframe directly from a dictionary of arrays and list of column names. We already build these two in [_pandas_only_series](https://github.com/scikit-hep/uproot5/blob/63bb43f9686d6af0511ee4ac6caa08bad18469f7/src/uproot/interpretation/library.py#L778\u2028\u2028)\r\n\r\nEnlarging a dataframe through \r\n\r\n`df.loc[len(df)] = new_row`\r\nor \r\n`df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)`\r\n\r\nis much slower than constructing a dataframe once: \r\n\r\n[<img src=\"https://i.stack.imgur.com/iz3wR.png\">](https://stackoverflow.com/a/76132725)\r\n",
  "created_at":"2024-01-18T17:05:15Z",
  "id":1898878091,
  "issue":1070,
  "node_id":"IC_kwDOD6Q_ss5xLpSL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-18T17:05:15Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"The new **docs/readthedocs.org:uproot** test requirement can be satisfied by merging #1084/[jpivarski/fix-readthedocs-documentation](https://github.com/scikit-hep/uproot5/tree/jpivarski/fix-readthedocs-documentation) into this PR (or by merging in `main`, which has it).",
  "created_at":"2024-01-16T19:08:07Z",
  "id":1894351221,
  "issue":1074,
  "node_id":"IC_kwDOD6Q_ss5w6YF1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-16T19:08:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"(Both `decompression_executor` and `interpretation_executor`; they go together as a pair.)",
  "created_at":"2024-01-25T15:27:41Z",
  "id":1910438074,
  "issue":1079,
  "node_id":"IC_kwDOD6Q_ss5x3vi6",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:27:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"https://numpy.org/devdocs/release/2.0.0-notes.html\r\n\r\nhttps://pythonspeed.com/articles/numpy-2/",
  "created_at":"2024-01-11T16:29:48Z",
  "id":1887530452,
  "issue":1080,
  "node_id":"IC_kwDOD6Q_ss5wgW3U",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-11T17:56:10Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"NumPy 2.0's [release notes](https://numpy.org/devdocs/release/2.0.0-notes.html) serve as a migration guide, and their umbrella issue, numpy/numpy/issues/24300, is a good source of news.",
  "created_at":"2024-01-30T16:25:39Z",
  "id":1917418927,
  "issue":1080,
  "node_id":"IC_kwDOD6Q_ss5ySX2v",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T16:47:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The development wheels are here: https://anaconda.org/scientific-python-nightly-wheels/numpy!",
  "created_at":"2024-01-30T16:45:57Z",
  "id":1917464043,
  "issue":1080,
  "node_id":"IC_kwDOD6Q_ss5ySi3r",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T16:45:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The new **docs/readthedocs.org:uproot** test requirement can be satisfied by merging #1084/[jpivarski/fix-readthedocs-documentation](https://github.com/scikit-hep/uproot5/tree/jpivarski/fix-readthedocs-documentation) into this PR (or by merging in `main`, which has it).",
  "created_at":"2024-01-16T19:08:06Z",
  "id":1894351194,
  "issue":1082,
  "node_id":"IC_kwDOD6Q_ss5w6YFa",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-16T19:08:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I got an email and there's the docs/readthedocs.org:uproot test failure above.\r\n\r\nI made it a required test, so I probably don't need the emails. PRs will need to keep the docs working.",
  "created_at":"2024-01-16T17:43:36Z",
  "id":1894216179,
  "issue":1084,
  "node_id":"IC_kwDOD6Q_ss5w53Hz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-16T17:43:36Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Next issue:\r\n\r\n> At least one item should be provided in \"tools\" or \"commands\".",
  "created_at":"2024-01-16T17:46:07Z",
  "id":1894219838,
  "issue":1084,
  "node_id":"IC_kwDOD6Q_ss5w54A-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-16T17:46:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Despite all of these build failures, I only got an email notification from the first one in this PR. That's also not particularly useful... I'm turning off the email notifications now.",
  "created_at":"2024-01-16T18:16:29Z",
  "id":1894268038,
  "issue":1084,
  "node_id":"IC_kwDOD6Q_ss5w6DyG",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-16T18:16:29Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"That worked, and we see the difference between [this build](https://uproot--1084.org.readthedocs.build/en/1084/) and [the latest](https://uproot.readthedocs.io/en/latest/) (which will change after I merge this PR).\r\n\r\nA snapshot of differences in `uproot.open`:\r\n\r\n**Latest:**\r\n\r\n![image](https://github.com/scikit-hep/uproot5/assets/1852447/6afddebd-9e61-425c-8e6b-3f7bb0784870)\r\n\r\n...\r\n\r\n![image](https://github.com/scikit-hep/uproot5/assets/1852447/45858619-e84d-4739-a636-70639fc2818b)\r\n\r\n**This PR:**\r\n\r\n![image](https://github.com/scikit-hep/uproot5/assets/1852447/d2d16915-8f71-458f-866a-74587798d4a8)\r\n\r\n...\r\n\r\n![image](https://github.com/scikit-hep/uproot5/assets/1852447/ab0dfd6f-400a-4af8-a39d-be90ce3cd786)\r\n\r\nNobody complained that the docs were out of date...",
  "created_at":"2024-01-16T18:28:16Z",
  "id":1894297951,
  "issue":1084,
  "node_id":"IC_kwDOD6Q_ss5w6LFf",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-16T18:28:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Just a stylistic comment but I think the function would be better situated within uproot as `uproot.dask.to_root`, thoughts?\r\n\r\nI think it would match dask-awkward and awkward better in that regard.\r\n\r\n@jpivarski",
  "created_at":"2024-01-19T14:34:38Z",
  "id":1900528157,
  "issue":1085,
  "node_id":"IC_kwDOD6Q_ss5xR8Id",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T14:35:01Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"You mean the function would be an attribute of another function, `uproot.dask`?\r\n\r\nI've gone back and forth on that\u2014there used to be an `ak.to_parquet.dataset` for converting already-written Parquet files into a dataset (the directory with metadata convention). But by not being in the same level of hierarchy as the rest of the `ak.*` functions, it was hard to discover.\r\n\r\nI understand the logic of writing-from-Dask being associated with reading-from-Dask. Uproot doesn't already have the flat bundle of functions that Awkward Array has, so there's less to maintain. But still, locating functions as attributes on other functions is unusual enough in Python as to be an antipattern\u2014if someone is looking for it, they're unlikely to think of looking there.\r\n\r\nPerhaps it should just be a naming convention: `uproot.dask_write`? From a typing point of view, that only turns a dot into an underscore, but from an organizational point of view, there's no nesting of functions. (I don't think it needs \"to ROOT\" because this is Uproot\u2014it's only ever going to read from or write to ROOT.)",
  "created_at":"2024-01-19T14:49:48Z",
  "id":1900551834,
  "issue":1085,
  "node_id":"IC_kwDOD6Q_ss5xSB6a",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T14:49:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski ok that naming scheme makes sense, and I am fine with it!",
  "created_at":"2024-01-19T14:59:57Z",
  "id":1900581232,
  "issue":1085,
  "node_id":"IC_kwDOD6Q_ss5xSJFw",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T14:59:57Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"In Uproot and Awkward, name the tests by PR number:\r\n\r\n```\r\ntests/test_1085_dask_to_root.py\r\n```",
  "created_at":"2024-01-19T15:18:22Z",
  "id":1900610177,
  "issue":1085,
  "node_id":"IC_kwDOD6Q_ss5xSQKB",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T15:18:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK I checked it with nanoevents, repartitioning, concatenating, all using a distributed Client and it seems to be robust. It touches data correctly and fills the output TTree well. \r\n\r\nLooks good to me :-)",
  "created_at":"2024-01-19T18:35:49Z",
  "id":1900914775,
  "issue":1085,
  "node_id":"IC_kwDOD6Q_ss5xTahX",
  "performed_via_github_app":null,
  "reactions":{
   "hooray":1,
   "total_count":1
  },
  "updated_at":"2024-01-19T21:42:27Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"The example file is small; it has 1000 branches, but they're not filled with much data. Unfortunately, we can't keep large test files in our CI, so this is something that can only be tested manually.\r\n\r\nI found a CMS NanoAOD file (which I can share with you privately). By selecting `filter_typename=\"bool\"`, we can get approximately 1000 branches with a simple data type that should be easy to wrap in Pandas. In `main` (with the file in warm cache using [vmtouch](https://anaconda.org/conda-forge/vmtouch)),\r\n\r\n```python\r\nimport uproot, time\r\ntree = uproot.open(\"Run2018D-DoubleMuon-Nano25Oct2019\"\r\n    \"_ver2-v1-974F28EE-0FCE-4940-92B5-870859F880B1.root:Events\")\r\ntick = time.perf_counter()\r\nfor _ in tree.iterate(filter_typename=\"bool\", step_size=100000, library=\"np\"):\r\n    tock = time.perf_counter()\r\n    print(tock - tick)\r\n    tick = tock\r\n```\r\n\r\nprints times on average 3.4 \u00b1 0.1 sec, the time needed to read the arrays at all (using NumPy). Swapping `library=\"np\"` for `library=\"pd\"` prints lots and lots of warnings, but the times are 4.19 \u00b1 0.08, so the overhead of `_pandas_memory_efficient` is 0.8 sec.\r\n\r\nIn `ioanaif/fix-pandas-memory-issue-1070`, the `library=\"np\"` time is 3.0 \u00b1 0.2 sec and the `library=\"pd\"` time is 4.1 \u00b1 0.2. The Pandas `PerformanceWarning` is gone, but there's no noticeable impact on the actual performance.\r\n\r\nI found another file, from issue #288, which has a lot of large, simple-typed branches that can be selected with\r\n\r\n```python\r\nfilter_typename=[\"double\", \"/double\\[[0-9]+\\]/\", \"bool\", \"/bool\\[[0-9]+\\]/\"]\r\n```\r\n\r\nIt takes `main` 2.50 \u00b1 0.06 sec to read this file with `library=\"np\"` and 2.89 \u00b1 0.02 sec to read with `library=\"pd\"`, but in `ioanaif/fix-pandas-memory-issue-1070`, it takes 2.58 \u00b1 0.07 sec to read with `library=\"np\"` and 9.7 \u00b1 0.1 sec to read with `library=\"pd\"`. **The Pandas overhead went from 0.4 sec to 7.1 sec by introducing this PR. That's a regression.**\r\n\r\nThe `PerformanceWarning` suggests using `pd.concat`, not the `pd.DataFrame` constructor. I wonder if that is relevant. With the following diff in this PR,\r\n\r\n```diff\r\n--- a/src/uproot/interpretation/library.py\r\n+++ b/src/uproot/interpretation/library.py\r\n@@ -856,7 +856,9 @@ class Pandas(Library):\r\n \r\n         elif isinstance(how, str) or how is None:\r\n             arrays, names = _pandas_only_series(pandas, arrays, expression_context)\r\n-            return pandas.DataFrame(data=arrays, columns=names)\r\n+            out = pandas.concat(arrays, axis=1, ignore_index=True)\r\n+            out.columns = names\r\n+            return out\r\n \r\n         else:\r\n             raise TypeError(\r\n```\r\n\r\nit now takes 2.6 \u00b1 0.1 sec for `library=\"np\"` and 2.96 \u00b1 0.08 sec for `library=\"pd\"`, which is no regression (0.4 sec of Pandas overhead is almost exactly what the `_pandas_memory_efficient` function in `main` had).\r\n\r\nGoing back to the NanoAOD file, it's 3.1 \u00b1 0.2 sec with `library=\"np\"` and 3.6 \u00b1 0.2 sec with `library=\"pd\"`, still acceptable.\r\n\r\nSo the bottom line is that using the `pandas.DataFrame` constructor introduces a performance regression (while eliminating Pandas's warning!) in one out of the two cases tried, but using the `pandas.concat` function does not. After this comment, I'll add it as a commit here.",
  "created_at":"2024-01-19T18:54:55Z",
  "id":1900944599,
  "issue":1086,
  "node_id":"IC_kwDOD6Q_ss5xThzX",
  "performed_via_github_app":null,
  "reactions":{
   "confused":1,
   "total_count":1
  },
  "updated_at":"2024-01-19T18:54:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"After adjusting the code so that it passes all tests (54cab87319329dba0a7498870c0168d7c910e8c6), the time with the file from from issue #288 is still 2.6 \u00b1 0.1 sec with `library=\"np\"` and 3.0 \u00b1 0.1 sec with `library=\"pd\"`. The 7-second issue didn't come back.\r\n\r\n... until I manually reverted the code to\r\n\r\n```python\r\npandas.DataFrame(data=arrays, columns=names)\r\n```\r\n\r\njust to be sure that it's persistent, that I'm not just imagining things. It is definitely the case that Pandas is doing something bad when we run its constructor. (Surely that was the first thing that I tried, way back when, and encountered some bad behavior that made me write `_pandas_memory_efficient` in the first place.)\r\n\r\nOh, if I run `gc.disable()` and then `gc.collect()` before the constructor, the time isn't as bad (6.0 \u00b1 0.1 sec, rather than 9.6 \u00b1 0.1 sec). Whatever bad thing Pandas is doing in its constructor, it involves creating a lot of short-lived objects.",
  "created_at":"2024-01-19T19:17:38Z",
  "id":1900973241,
  "issue":1086,
  "node_id":"IC_kwDOD6Q_ss5xToy5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T19:17:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm merging this for the same reason as https://github.com/scikit-hep/uproot5/pull/1104#issuecomment-1912432150.",
  "created_at":"2024-01-26T19:04:24Z",
  "id":1912550271,
  "issue":1088,
  "node_id":"IC_kwDOD6Q_ss5x_zN_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-26T19:04:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Just to finish the chain this was mentioned to me by @martindurant.",
  "created_at":"2024-01-19T15:54:04Z",
  "id":1900670883,
  "issue":1089,
  "node_id":"IC_kwDOD6Q_ss5xSe-j",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T15:54:04Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"NONE",
  "body":"> it allows to declare the output length so it can pre-allocate the buffer\r\n\r\nYou can also allocate buffers yourself and decompress_into - I don't know if there's a use case for that.",
  "created_at":"2024-01-19T16:06:57Z",
  "id":1900691011,
  "issue":1089,
  "node_id":"IC_kwDOD6Q_ss5xSj5D",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T16:06:57Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Actually, yes! One thing uproot is often doing is decompressing many small chunks and then concatenating them into a larger contiguous buffer. We could save some additional allocation and copy time if we can decompress into a buffer at an arbitrary offset.",
  "created_at":"2024-01-19T16:17:11Z",
  "id":1900706532,
  "issue":1089,
  "node_id":"IC_kwDOD6Q_ss5xSnrk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T16:17:11Z",
  "user":"MDQ6VXNlcjY1ODc0MTI="
 },
 {
  "author_association":"NONE",
  "body":"> if we can decompress into a buffer at an arbitrary offset.\r\n\r\nyes, certainly you can, I think by just slicing the base numpy array",
  "created_at":"2024-01-19T16:19:09Z",
  "id":1900709551,
  "issue":1089,
  "node_id":"IC_kwDOD6Q_ss5xSoav",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T16:19:09Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"Since we want Uproot to work in Pyodide, it's important to note that [cramjam works in Pyodide](https://github.com/pyodide/pyodide/tree/main/packages/cramjam).\r\n\r\n![image](https://github.com/scikit-hep/uproot5/assets/1852447/df38ff41-df74-4d90-9e8a-d61114549223)\r\n\r\nWriting into a single, contiguous buffer with `decompress_into` would require some rearchitecting\u2014possible, but a major project. Also, it could only work for non-ragged data (or only the outer indexes of ragged data). It could perhaps be an extension of [uproot.AsDtypeInPlace](https://uproot.readthedocs.io/en/latest/uproot.interpretation.numerical.AsDtypeInPlace.html).",
  "created_at":"2024-01-19T20:48:19Z",
  "id":1901093837,
  "issue":1089,
  "node_id":"IC_kwDOD6Q_ss5xUGPN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-19T20:48:19Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I've split out the request to use cramjam's decompress-in-place into a new issue; this will be closed when #1090 is.",
  "created_at":"2024-01-30T17:41:52Z",
  "id":1917566729,
  "issue":1089,
  "node_id":"IC_kwDOD6Q_ss5yS78J",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T17:41:52Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Since it's only replacing lz4 and zstd, I suppose I could put this in the uproot_extras area and use it that way?",
  "created_at":"2024-01-20T19:34:39Z",
  "id":1902247606,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xYf62",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-20T19:34:39Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"(This is happening faster than I had been thinking it would. Thanks for opening a PR!)\r\n\r\nDependencies can only change at the minor version numbers, and since we're starting from a state in which `lz4`/`xxhash` and `zstd` are both optional dependencies, phase 1 would have `cramjam` be an optional dependency, instead (in uproot_extras). The thing I have to think about is whether there will be a phase 2 in which `cramjam` becomes a strict dependency (in the pyproject.toml).\r\n\r\nEarly on, the fact that Uproot was pure Python was considered very valuable. (Most people who answered my question about why they're using it mentioned pure Python portability.) But since Uproot 5, Awkward Array has been a strict dependency, and that's a compiled extension. Cramjam is a much \"better behaved\" compiled extension than lz4. (On some platforms, the lz4 package _contains_ its decompression code; on other platforms, it _expects to find it_ elsewhere in the operating system.)\r\n\r\nI guess it would be okay. After all, [cramjam is on all platforms](https://pypi.org/project/cramjam/#files), including Pyodide.\r\n\r\nThere's usually 2 months between changes like this\u2014at least, we do a cadence of 2 months between Awkward minor versions, and use the minor versions to apply deprecations and change dependencies. Uproot hasn't been as regular, but we don't want to rock peoples' boats unnecessarily.",
  "created_at":"2024-01-20T19:49:16Z",
  "id":1902250366,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xYgl-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-20T19:49:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"As far as pacing is concerned - I was curious and it turned out to be really easy so I just did it.\r\n\r\nWe can release this as makes sense to your typical release cycle. There is no major rush from my side.\r\n\r\nInsofar as extras vs. strict deps are concerned I think this fits nicely as an extra anyway. I'm fine to update that, especially since we can drop the lz4 / xxhash / zstd extras.\r\n\r\nPerhaps if there's a positive reaction to requesting lzma being added to cramjam and we replace lzma with the cramjam impl we move it to a strict dependency?",
  "created_at":"2024-01-20T19:54:04Z",
  "id":1902251255,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xYgz3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-20T19:54:04Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Ah, hmm, looks like we need to deal with the checksum!",
  "created_at":"2024-01-20T19:54:22Z",
  "id":1902251310,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xYg0u",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-20T19:54:22Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"Since Python 3, LZMA has been in the standard library, so there's no reason to change that. Except possible performance.\r\n\r\nWhich reminds me, if cramjam has much worse performance, that would be a reason to stop and reconsider, but I highly doubt it. (They probably just plugged in the same algorithms, maybe translated to Rust.)",
  "created_at":"2024-01-20T20:08:55Z",
  "id":1902254395,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xYhk7",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-20T20:08:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"The main performance advantage appears to be hinting the buffer sizes, which the python lzma library doesn't support. The rust implementations, however, do.",
  "created_at":"2024-01-20T20:11:28Z",
  "id":1902254962,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xYhty",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-20T20:46:02Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"While cramjam does include benchmarks none of them demonstrate this particular feature.\r\n\r\nLooking through the benchmarks the python implementations and cramjam without hints are roughly equal, with some variation on the kind of data ingested. It is typically marginally to significantly better.",
  "created_at":"2024-01-20T20:16:13Z",
  "id":1902255858,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xYh7y",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-20T20:19:51Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"I'll convert this to a draft while we discuss the finer points.",
  "created_at":"2024-01-20T20:17:45Z",
  "id":1902256160,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xYiAg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-20T20:17:45Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"While we're at it: cramjam specifically recommends using https://github.com/pycompression/python-isal instead of itself for zlib compression (and indeed the performance differences are significant).\r\n\r\nDespite being the Intel library for accelerated compression it supports all the standard conda architectures.\r\n\r\n~~I guess it may not immediately support Pyodide? How do I check?~~ Figured it out, it does not. However, it could be worth it as a performance optimization to add for non-wasm.",
  "created_at":"2024-01-20T20:28:02Z",
  "id":1902258108,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xYie8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-20T21:12:11Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Wow - the response for adding lzma was super quick https://github.com/milesgranger/cramjam/pull/127\r\n\r\nTested in a local branch and it immediately passes tests. No performance improvement with a small file though, but the interface to compression is much more uniform and clean and we can always use the output size hints.\r\n\r\nUpdate: There's a ~10% performance improvement over the python lzma implementation when testing in single threaded reading on a large file (arm64/macos). Not huge but also not nothing.",
  "created_at":"2024-01-21T19:30:39Z",
  "id":1902736670,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xaXUe",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2024-01-21T21:20:55Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Added `zlib` to extras such that it tries to use `isal` if it is available but otherwise returns the standard python zlib.\r\n\r\nThe cost of failing the exception is miniscule compared to decompression time in realistic use cases. `isal` is typically 3x-5x faster than python's implementation, so this is definitely a useful optimization.\r\n\r\nI also changed the way zlib is imported in the compression class such that the library try/except import is only run once, so there is no meaningful overhead to using either library.",
  "created_at":"2024-01-21T21:44:38Z",
  "id":1902776213,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xag-V",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-21T23:10:51Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Apologies for the noise. Aside from the forthcoming lzma feature in cramjam, I do not expect to make any further major changes to this PR.",
  "created_at":"2024-01-21T23:34:00Z",
  "id":1902804098,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5xanyC",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-21T23:34:15Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski a review of the present state of this draft PR would be appreciated!",
  "created_at":"2024-01-25T17:36:27Z",
  "id":1910684491,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5x4rtL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T17:37:09Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Thanks for the comments - I'll clean things up.",
  "created_at":"2024-01-25T23:07:36Z",
  "id":1911139577,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5x6az5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T23:07:36Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski after these changes I think this PR is in a pretty good place - I'm going to pop it out of draft mode. Thanks!",
  "created_at":"2024-01-26T14:36:19Z",
  "id":1912167420,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5x-Vv8",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-26T14:36:19Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"I see that cramjam 2.8.1 will be out in minutes: https://github.com/milesgranger/cramjam/issues/126#issuecomment-1912301225. As long as this PR smoothly transitions from the experimental namespace to the eventual namespace, then it will be future-proof and ready to go into Uproot.\r\n\r\nIt's something that should go into a minor release, Uproot 5.3.0, just as the fsspec stuff needed a minor release. For fsspec, we kept two \"main\" branches so that Luis could get all of his updates in without affecting the 5.1.x series. This is simpler: it's just one PR. (I also doubt there will be any merge conflicts in the near future\u2014this PR only affects compression, which is separated out from everything else.)\r\n\r\nUproot doesn't have a minor release schedule the way that Awkward Array does, but Awkward Array's policy of giving each minor release (at least) 2 months is probably a good idea. [Uproot 5.2.0](https://github.com/scikit-hep/uproot5/releases/tag/v5.2.0) was released on December 14, so I just made a calendar reminder to myself to merge this PR and release Uproot 5.3.0 on February 14.",
  "created_at":"2024-01-26T16:12:16Z",
  "id":1912316791,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5x-6N3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-26T16:12:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"OK - lzma from cramjam is now in. This one is ready for a final review.",
  "created_at":"2024-01-26T16:57:00Z",
  "id":1912380304,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5x_JuQ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-26T16:57:29Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski one little tweak - I moved cramjam to be a required dependency since it is used for lzma instead of the python standard library now.",
  "created_at":"2024-01-26T18:29:59Z",
  "id":1912505179,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5x_oNb",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2024-01-26T18:29:59Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"I came across issue #518, which is asking for a way to use another alternate compression algorithm compatible with ZLIB's format.\r\n\r\nBefore merging this PR, I'd like to replace the\r\n\r\n```python\r\nZLIB.use_isal: bool\r\n```\r\n\r\nwith\r\n\r\n```python\r\nZLIB.library: Literal[\"zlib\", \"isal\"]\r\n```\r\n\r\nso that libraries like `\"deflate\"` or `\"awesomest_zlib_ever\"` can be added to that list, just to keep it open to the possibility.",
  "created_at":"2024-01-30T15:47:55Z",
  "id":1917300788,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5yR7A0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:47:55Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Sure - please feel free to edit.",
  "created_at":"2024-01-30T15:50:01Z",
  "id":1917308823,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5yR8-X",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:50:01Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Notably - deflate doesn't build the zlib headers and fails tests.",
  "created_at":"2024-01-30T15:50:34Z",
  "id":1917310873,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5yR9eZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:50:34Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"NONE",
  "body":"> libraries like \"deflate\"\r\n\r\nI was about to say that! \"deflate\" is the compression method of the data itself (by whatever alforithms), and zlib adds the header. zlib in theory supports other compressors, but in practice it, zip and gzip always do DEFLATE.",
  "created_at":"2024-01-30T15:51:53Z",
  "id":1917315721,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5yR-qJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T15:51:53Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"I had heard of the zlib-deflate distinction before, but I thought what was being discussed in #518 was a particular Python library called `\"deflate\"` (which is somehow better). ROOT has its own alternate headers, for some level of headers (the compressed blocks that it uses aren't like standalone ZIP files). Maybe I'm confusing headers with headers.\r\n\r\nSo this is for `\"awesomest_zlib_ever\"`, then.",
  "created_at":"2024-01-30T17:59:24Z",
  "id":1917594487,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5yTCt3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T17:59:24Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Now this should be ready to merge, except that we won't do that until close to Feb 14: https://github.com/scikit-hep/uproot5/pull/1090#issuecomment-1912316791.\r\n\r\nTangentially, if anyone runs the tests in parallel mode, it's possible for some of the ISAL tests to be executed as non-ISAL and vice-versa. Such a test run shouldn't fail because the two algorithms are supposed to be equivalent, but it wouldn't be testing what we want to test. Fortunately, CI runs non-parallel tests, so it always gets tested properly at some point.",
  "created_at":"2024-01-30T21:46:44Z",
  "id":1917949849,
  "issue":1090,
  "node_id":"IC_kwDOD6Q_ss5yUZeZ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-30T21:46:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"At a glance I would say this is a problem with the xrootd installation not with fsspec-xrootd and definitely not with uproot.\n\nCan you run xrootd directly (cli) to confirm this?",
  "created_at":"2024-01-22T13:57:42Z",
  "id":1904059230,
  "issue":1091,
  "node_id":"IC_kwDOD6Q_ss5xfaNe",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-22T13:59:24Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"NONE",
  "body":"Could you please let me know the CLI command? I am very new to xrootd and its documentation has almost no straightforward examples.",
  "created_at":"2024-01-22T20:16:41Z",
  "id":1904736128,
  "issue":1091,
  "node_id":"IC_kwDOD6Q_ss5xh_eA",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-22T20:16:41Z",
  "user":"MDQ6VXNlcjc0MDU1MTAy"
 },
 {
  "author_association":"MEMBER",
  "body":"> At a glance I would say this is a problem with the xrootd installation not with fsspec-xrootd and definitely not with uproot.\r\n\r\nI agree\u2014can you install into a new environment and see if it's still an issue?",
  "created_at":"2024-01-25T15:38:44Z",
  "id":1910459402,
  "issue":1091,
  "node_id":"IC_kwDOD6Q_ss5x30wK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:38:44Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Created a new environment, definitely looks like a xrootd error -\r\n```python\r\n>>> import uproot\r\n>>> file = uproot.open(\"file://binder/Run2012B_DoubleMuParked.root\")\r\n>>> file = uproot.open(\"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\")\r\nTraceback (most recent call last):\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/fsspec/registry.py\", line 236, in get_filesystem_class\r\n    register_implementation(protocol, _import_class(bit[\"class\"]))\r\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/fsspec/registry.py\", line 271, in _import_class\r\n    mod = importlib.import_module(mod)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/fsspec_xrootd/__init__.py\", line 12, in <module>\r\n    from .xrootd import XRootDFile, XRootDFileSystem\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/fsspec_xrootd/xrootd.py\", line 18, in <module>\r\n    from XRootD import client  # type: ignore[import-not-found]\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/XRootD/client/__init__.py\", line 3, in <module>\r\n    from .glob_funcs import glob, iglob\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/XRootD/client/glob_funcs.py\", line 26, in <module>\r\n    from XRootD.client.filesystem import FileSystem\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/XRootD/client/filesystem.py\", line 26, in <module>\r\n    from pyxrootd import client\r\nImportError: dlopen(/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/pyxrootd/client.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/libXrdCl.3.dylib\r\n  Referenced from: <F6155528-E476-321C-80E1-EAC70A48E0AA> /Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/pyxrootd/client.cpython-311-darwin.so\r\n  Reason: tried: '/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/pyxrootd/../lib/libXrdCl.3.dylib' (no such file), '/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/pyxrootd/../lib/libXrdCl.3.dylib' (no such file), '/opt/homebrew/lib/libXrdCl.3.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libXrdCl.3.dylib' (no such file), '/opt/homebrew/lib/libXrdCl.3.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libXrdCl.3.dylib' (no such file)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/uproot/reading.py\", line 141, in open\r\n    file = ReadOnlyFile(\r\n           ^^^^^^^^^^^^^\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/uproot/reading.py\", line 559, in __init__\r\n    self._source = source_cls(file_path, **self._options)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/uproot/source/fsspec.py\", line 31, in __init__\r\n    self._fs, self._file_path = fsspec.core.url_to_fs(\r\n                                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/fsspec/core.py\", line 383, in url_to_fs\r\n    chain = _un_chain(url, kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/fsspec/core.py\", line 332, in _un_chain\r\n    cls = get_filesystem_class(protocol)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/saransh/Code/HEP/coffea/env/lib/python3.11/site-packages/fsspec/registry.py\", line 238, in get_filesystem_class\r\n    raise ImportError(bit[\"err\"]) from e\r\nImportError: Unable to load filesystem from EntryPoint(name='root', value='fsspec_xrootd:XRootDFileSystem', group='fsspec.specs')\r\n```\r\n```\r\n(env) saransh@saranshmacair coffea % python3 -m pip freeze\r\nawkward==2.5.2\r\nawkward-cpp==28\r\nfsspec==2023.12.2\r\nfsspec-xrootd==0.2.4\r\nimportlib-metadata==7.0.1\r\nnumpy==1.26.3\r\npackaging==23.2\r\nuproot==5.2.1\r\nxrootd==5.6.6\r\nzipp==3.17.0\r\n```\r\n\r\nIf possible, could you please let me know a standalone xrootd command through which I can reproduce this? I'll then create an issue on the GitHub repository.",
  "created_at":"2024-01-25T20:43:44Z",
  "id":1910967665,
  "issue":1091,
  "node_id":"IC_kwDOD6Q_ss5x5w1x",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T20:43:44Z",
  "user":"MDQ6VXNlcjc0MDU1MTAy"
 },
 {
  "author_association":"MEMBER",
  "body":"This is true, and the memory use in `uproot.dask` is over and above `uproot.open` (and getting the TTree metadata).\r\n\r\n```python\r\nimport gc\r\nimport psutil\r\nimport uproot\r\n\r\nthis_process = psutil.Process()\r\n\r\ndef memory_diff(task):\r\n    gc.disable()\r\n    gc.collect()\r\n    start_memory = this_process.memory_full_info().uss\r\n    task()\r\n    gc.collect()\r\n    stop_memory = this_process.memory_full_info().uss\r\n    gc.enable()\r\n    return stop_memory - start_memory\r\n\r\ndef task():\r\n    with uproot.open(\r\n        {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n    ) as tree:\r\n        pass\r\n\r\nfor _ in range(200):\r\n    print(f\"{memory_diff(task) * 1e-6:.3f} MB\")\r\n```\r\n\r\nreports\r\n\r\n```\r\n28.156 MB\r\n1.483 MB\r\n0.000 MB\r\n0.008 MB\r\n0.012 MB\r\n0.004 MB\r\n3.932 MB\r\n0.262 MB\r\n0.000 MB\r\n0.000 MB\r\n0.000 MB\r\n-1.040 MB\r\n0.000 MB\r\n0.803 MB\r\n0.246 MB\r\n0.000 MB\r\n0.000 MB\r\n0.000 MB\r\n0.000 MB\r\n-1.040 MB\r\n0.807 MB\r\n0.242 MB\r\n0.004 MB\r\n0.000 MB\r\n0.000 MB\r\n0.000 MB\r\n...\r\n```\r\n\r\nChange the `task` to\r\n\r\n```python\r\ndef task():\r\n    tree = uproot.open(\r\n        {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n    )\r\n```\r\n\r\nso that it leaks file handles, and it's\r\n\r\n```\r\n26.059 MB\r\n1.499 MB\r\n0.004 MB\r\n0.012 MB\r\n0.033 MB\r\n0.004 MB\r\n0.000 MB\r\n-0.012 MB\r\n6.046 MB\r\n0.258 MB\r\n0.004 MB\r\n0.000 MB\r\n-1.049 MB\r\n0.008 MB\r\n-1.049 MB\r\n0.000 MB\r\n0.000 MB\r\n...\r\n```\r\n\r\nWe'd eventually run out of file handles this way, but apparently not memory (on the MB scale).\r\n\r\nNow\r\n\r\n```python\r\ndef task():\r\n    lazy = uproot.dask(\r\n        {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n    )\r\n```\r\n\r\n```\r\n87.364 MB\r\n27.988 MB\r\n25.252 MB\r\n28.987 MB\r\n25.227 MB\r\n26.903 MB\r\n25.219 MB\r\n30.024 MB\r\n23.155 MB\r\n27.898 MB\r\n26.325 MB\r\n28.975 MB\r\n24.158 MB\r\n28.991 MB\r\n...\r\n```\r\n\r\nThis is a problem. (Also, it's noticeably slower, though there might be good reasons for that.)\r\n\r\nUsing [Pympler](https://pympler.readthedocs.io/en/latest/),\r\n\r\n```python\r\n>>> import gc\r\n>>> import pympler.tracker\r\n>>> import uproot\r\n>>> \r\n>>> summary_tracker = pympler.tracker.SummaryTracker()\r\n>>> \r\n>>> # run it once to get past the necessary first-time things (filling uproot.classes, etc.)\r\n>>> lazy = uproot.dask(\r\n...     {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n... )\r\n>>> del lazy\r\n>>> gc.collect()\r\n0\r\n>>> # run print_diff enough times to get to the quiescent state\r\n>>> summary_tracker.print_diff()\r\n...\r\n>>> summary_tracker.print_diff()\r\n  types |   # objects |   total size\r\n======= | =========== | ============\r\n>>> \r\n>>> # what does an Uproot Dask array bring in?\r\n>>> lazy = uproot.dask(\r\n...     {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n... )\r\n>>> summary_tracker.print_diff()\r\n                                            types |   # objects |   total size\r\n================================================= | =========== | ============\r\n                                             dict |       72059 |     11.54 MB\r\n                                            bytes |           3 |      5.66 MB\r\n                                             list |       24007 |      1.73 MB\r\n                                      numpy.int64 |       37491 |      1.14 MB\r\n                      uproot.source.cursor.Cursor |       21000 |    984.38 KB\r\n                                    numpy.ndarray |        4501 |    492.30 KB\r\n                                              str |        4886 |    436.10 KB\r\n              uproot.models.TObject.Model_TObject |        5999 |    281.20 KB\r\n                                            tuple |        3385 |    147.00 KB\r\n          uproot.models.TObjArray.Model_TObjArray |        3000 |    140.62 KB\r\n                uproot.models.TNamed.Model_TNamed |        2999 |    140.58 KB\r\n                                        frozenset |           1 |    128.21 KB\r\n                                              int |        3492 |     95.51 KB\r\n      awkward._nplikes.typetracer.TypeTracerArray |        1878 |     88.03 KB\r\n  uproot.models.TTree.Model_ROOT_3a3a_TIOFeatures |        1500 |     70.31 KB\r\n>>> \r\n>>> # what goes away when we delete it?\r\n>>> del lazy\r\n>>> gc.collect()\r\n14\r\n>>> gc.collect()\r\n0\r\n>>> summary_tracker.print_diff()\r\n                                         types |   # objects |   total size\r\n============================================== | =========== | ============\r\n                                          code |           0 |     37     B\r\n                  aiohttp.helpers.TimerContext |          -1 |    -48     B\r\n      awkward.contents.recordarray.RecordArray |          -1 |    -48     B\r\n            dask.highlevelgraph.HighLevelGraph |          -1 |    -48     B\r\n             dask_awkward.utils.LazyInputsDict |          -1 |    -48     B\r\n               dask.blockwise.BlockwiseDepDict |          -1 |    -48     B\r\n                       awkward.highlevel.Array |          -1 |    -48     B\r\n  dask_awkward.layers.layers.AwkwardInputLayer |          -1 |    -48     B\r\n                   dask_awkward.lib.core.Array |          -1 |    -48     B\r\n                asyncio.trsock.TransportSocket |          -2 |    -80     B\r\n                                 ssl.SSLObject |          -2 |    -96     B\r\n                  aiohttp.streams.StreamReader |          -2 |    -96     B\r\n                  asyncio.sslproto.SSLProtocol |          -2 |    -96     B\r\n                     asyncio.sslproto._SSLPipe |          -2 |    -96     B\r\n                                     bytearray |          -2 |   -112     B\r\n```\r\n\r\nHardly anything goes away when `lazy` is deleted! That's not good!\r\n\r\nThis TTree has 1499 TBranches. So having approximately that many TIOFeatures, TypeTracerArray, twice that many Model_TNamed, Model_TObjArray (TBranch and TLeaf), and four times as many Model_TObject make sense.\r\n\r\nThere are only 3 bytes objects, but they comprise 5.66 MB. I don't know, offhand, what they could be, but I think they're more likely Uproot than Dask. There are a lot of big dicts, which is not too surprising, and I can't say offhand whether I expect more in Uproot or more in Dask.\r\n\r\nThe one, major problem is that `del lazy` followed by `gc.collect()` does not get rid of as many objects as were brought in. It may be reasonable for the dask-awkward array of a large TTree to be 30 MB, but it's not reasonable for it to still be around after deleting.\r\n\r\nWho gets a reference to it and doesn't let go? It might be possible to find out with `gc.get_referrers`, but it might not be at the level of `lazy` (the `ak.Array` and `dak.Array` are listed as objects that go away). Let me think about that...",
  "created_at":"2024-01-23T18:32:48Z",
  "id":1906681825,
  "issue":1093,
  "node_id":"IC_kwDOD6Q_ss5xpafh",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-23T18:32:48Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, setting up to follow this object with `gc.get_referrers`,\r\n\r\n```python\r\n>>> import uproot\r\n>>> import gc\r\n>>> lazy = uproot.dask(\r\n...     {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n... )\r\n>>> type(lazy)\r\n<class 'dask_awkward.lib.core.Array'>\r\n>>> type(lazy._meta)\r\n<class 'awkward.highlevel.Array'>\r\n```\r\n\r\nI'll be looking at lists and one reference will be the list I'm using to look at it, so I make that a special class that's easier to ignore in a print-out of type names.\r\n\r\n```python\r\n>>> class IgnoreMeList(list):\r\n...     pass\r\n... \r\n>>> def show(follow):\r\n...     print(\"\\n\".join(f\"{i:2d} {type(x).__module__}.{type(x).__name__}\" for i, x in enumerate(follow)))\r\n... \r\n```\r\n\r\nIn the Pympler output, we saw that the TypeTracerArrays were not deleted when `lazy` went out of scope (and `gc.collect()` was called). So this is a good starting point to walk outward and find out who's holding a reference to it.\r\n\r\n```python\r\n>>> follow = IgnoreMeList([lazy._meta.layout.content(\"Muon_pt\").content.data])\r\n>>> show(follow)\r\n 0 awkward._nplikes.typetracer.TypeTracerArray\r\n```\r\n\r\nNow I'll just walk along the graph of its referrers, ignoring the `IgnoreMeList` because that's the `follow` list itself, and seeing what else is in the list.\r\n\r\n```python\r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[0]))\r\n>>> show(follow)\r\n 0 __main__.IgnoreMeList\r\n 1 builtins.dict\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[1]))\r\n>>> show(follow)\r\n 0 __main__.IgnoreMeList\r\n 1 awkward.contents.numpyarray.NumpyArray\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[1]))\r\n>>> show(follow)\r\n 0 __main__.IgnoreMeList\r\n 1 builtins.dict\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[1]))\r\n>>> show(follow)\r\n 0 __main__.IgnoreMeList\r\n 1 awkward.contents.listoffsetarray.ListOffsetArray\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[1]))\r\n>>> show(follow)\r\n 0 __main__.IgnoreMeList\r\n 1 builtins.list\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[1]))\r\n>>> show(follow)\r\n 0 builtins.dict\r\n 1 __main__.IgnoreMeList\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[0]))\r\n>>> show(follow)\r\n 0 awkward.contents.recordarray.RecordArray\r\n 1 __main__.IgnoreMeList\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[0]))\r\n>>> show(follow)\r\n 0 builtins.dict\r\n 1 __main__.IgnoreMeList\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[0]))\r\n>>> show(follow)\r\n 0 awkward.highlevel.Array\r\n 1 __main__.IgnoreMeList\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[0]))\r\n>>> show(follow)\r\n 0 builtins.dict\r\n 1 __main__.IgnoreMeList\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[0]))\r\n>>> show(follow)\r\n 0 dask_awkward.lib.core.Array\r\n 1 __main__.IgnoreMeList\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[0]))\r\n>>> show(follow)\r\n 0 __main__.IgnoreMeList\r\n 1 builtins.dict\r\n>>> \r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[1]))\r\n>>> show(follow)\r\n 0 builtins.function\r\n 1 builtins.dict\r\n 2 __main__.IgnoreMeList\r\n 3 builtins.module\r\n```\r\n\r\nOkay! The dict and the module are just `__main__`:\r\n\r\n```python\r\n>>> follow[3]\r\n<module '__main__' (built-in)>\r\n>>> follow[1].keys()\r\ndict_keys(['use_main_ns', 'namespace', 'matches'])\r\n>>> follow[1][\"use_main_ns\"]\r\n1\r\n>>> follow[1][\"matches\"]\r\n['follow']\r\n>>> type(follow[1][\"namespace\"])\r\n<class 'dict'>\r\n>>> follow[1][\"namespace\"].keys()\r\ndict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__annotations__', '__builtins__', 'uproot', 'gc', 'lazy', 'IgnoreMeList', 'show', 'follow'])\r\n```\r\n\r\nSo what about the function?\r\n\r\n```python\r\n>>> follow[0]\r\n<function show at 0x77a70b5889d0>\r\n```\r\n\r\nNope. All of this is either Python's infrastructure or the infrastructure I set up in the `__main__` namespace.\r\n\r\nSo what gives? I didn't see any other referrers along the way. Who's holding a reference to this object? If nobody is, why isn't it deleted (why is it not negative in the Pympler list) when the `ak.Array` that holds it is deleted?\r\n\r\nDoes anyone have any ideas?",
  "created_at":"2024-01-23T19:13:38Z",
  "id":1906754380,
  "issue":1093,
  "node_id":"IC_kwDOD6Q_ss5xpsNM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-23T19:13:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Even more to the point, following the advice of https://stackoverflow.com/a/28406001/1623645\r\n\r\n```python\r\n>>> import uproot\r\n>>> import gc\r\n>>> lazy = uproot.dask(\r\n...     {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n... )\r\n>>> class IgnoreMeList(list):\r\n...     pass\r\n... \r\n>>> def show(follow):\r\n...     print(\"\\n\".join(f\"{i:2d} {type(x).__module__}.{type(x).__name__}\" for i, x in enumerate(follow)))\r\n... \r\n>>> follow = IgnoreMeList([lazy._meta.layout.content(\"Muon_pt\").content.data])\r\n>>> del lazy\r\n>>> gc.collect()\r\n17\r\n>>> gc.collect()\r\n0\r\n>>> show(follow)\r\n 0 awkward._nplikes.typetracer.TypeTracerArray\r\n>>> follow = IgnoreMeList(gc.get_referrers(follow[0]))\r\n>>> gc.collect()\r\n0\r\n>>> show(follow)\r\n 0 __main__.IgnoreMeList\r\n```\r\n\r\nThe TypeTracerArray goes away. I don't know why this disagrees with Pympler (and the fact that 30 MB of USS doesn't go away).",
  "created_at":"2024-01-23T19:18:24Z",
  "id":1906764754,
  "issue":1093,
  "node_id":"IC_kwDOD6Q_ss5xpuvS",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-23T19:18:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"It looks to me like this might be in dask. Add the following to the loop body:\r\n```python\r\nimport gc\r\nimport dask.base\r\ndask.base.function_cache.clear()\r\ngc.collect()\r\n```\r\n\r\nI notice that the total memory usage remains fairly stable.",
  "created_at":"2024-01-24T09:45:57Z",
  "id":1907769396,
  "issue":1093,
  "node_id":"IC_kwDOD6Q_ss5xtkA0",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T09:45:57Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"MEMBER",
  "body":"If it's referenced in `dask.base.function_cache`, I would have thought that `gc.get_referrers` would have shown us that. Also, clearing this function cache doesn't show the allocated data getting removed in Pympler:\r\n\r\n```python\r\n>>> import gc\r\n>>> import pympler.tracker\r\n>>> import dask.base\r\n>>> import uproot\r\n>>> \r\n>>> summary_tracker = pympler.tracker.SummaryTracker()\r\n>>> \r\n>>> lazy = uproot.dask(\r\n...     {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n... )\r\n>>> del lazy\r\n>>> gc.collect()\r\n3\r\n>>> gc.collect()\r\n0\r\n>>> summary_tracker.print_diff()\r\n#                         ... several times ...                         #\r\n>>> summary_tracker.print_diff()\r\n  types |   # objects |   total size\r\n======= | =========== | ============\r\n>>> lazy = uproot.dask(\r\n...     {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n... )\r\n>>> summary_tracker.print_diff()\r\n                                        types |   # objects |   total size\r\n============================================= | =========== | ============\r\n                                         dict |       72060 |     11.54 MB\r\n                                        bytes |           3 |      5.66 MB\r\n                                         list |       24007 |      1.73 MB\r\n                                  numpy.int64 |       37491 |      1.14 MB\r\n                  uproot.source.cursor.Cursor |       21000 |    984.38 KB\r\n                                numpy.ndarray |        4501 |    492.30 KB\r\n                                          str |        4886 |    436.10 KB\r\n          uproot.models.TObject.Model_TObject |        5999 |    281.20 KB\r\n                                        tuple |        3385 |    147.00 KB\r\n      uproot.models.TObjArray.Model_TObjArray |        3000 |    140.62 KB\r\n            uproot.models.TNamed.Model_TNamed |        2999 |    140.58 KB\r\n                                    frozenset |           1 |    128.21 KB\r\n                                          int |        3490 |     95.45 KB\r\n  awkward._nplikes.typetracer.TypeTracerArray |        1878 |     88.03 KB\r\n         uproot.models.TAtt.Model_TAttFill_v2 |        1500 |     70.31 KB\r\n>>> del lazy\r\n>>> dask.base.function_cache.clear()   # clearing Dask's function cache\r\n>>> gc.collect()\r\n221296\r\n>>> gc.collect()\r\n0\r\n>>> summary_tracker.print_diff()\r\n                                         types |   # objects |   total size\r\n============================================== | =========== | ============\r\n                                          code |           0 |     37     B\r\n                       awkward.highlevel.Array |          -1 |    -48     B\r\n               dask.blockwise.BlockwiseDepDict |          -1 |    -48     B\r\n                  aiohttp.helpers.TimerContext |          -1 |    -48     B\r\n      awkward.contents.recordarray.RecordArray |          -1 |    -48     B\r\n  dask_awkward.layers.layers.AwkwardInputLayer |          -1 |    -48     B\r\n            dask.highlevelgraph.HighLevelGraph |          -1 |    -48     B\r\n                   dask_awkward.lib.core.Array |          -1 |    -48     B\r\n             dask_awkward.utils.LazyInputsDict |          -1 |    -48     B\r\n                asyncio.trsock.TransportSocket |          -2 |    -80     B\r\n                     asyncio.sslproto._SSLPipe |          -2 |    -96     B\r\n                     fsspec.caching.BytesCache |          -2 |    -96     B\r\n                                 ssl.SSLObject |          -2 |    -96     B\r\n           uproot._dask.TrivialFormMappingInfo |          -2 |    -96     B\r\n           uproot.models.TTree.Model_TTree_v20 |          -2 |    -96     B\r\n```\r\n\r\nIt's still the case that creating `lazy` adds 30 MB of TTree metadata objects and deleting it _and_ the functions from the function cache doesn't make them appear with a minus sign in Pympler.\r\n\r\nOh, but the total USS memory usage does go down:\r\n\r\n```python\r\nimport gc\r\nimport psutil\r\nimport dask.base\r\nimport uproot\r\n\r\nthis_process = psutil.Process()\r\n\r\ndef memory_diff(task):\r\n    gc.disable()\r\n    gc.collect()\r\n    start_memory = this_process.memory_full_info().uss\r\n    task()\r\n    gc.collect()\r\n    stop_memory = this_process.memory_full_info().uss\r\n    gc.enable()\r\n    return stop_memory - start_memory\r\n\r\ndef task():\r\n    lazy = uproot.dask(\r\n        {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n    )\r\n    del lazy\r\n    dask.base.function_cache.clear()\r\n\r\nfor _ in range(200):\r\n    print(f\"{memory_diff(task) * 1e-6:.3f} MB\")\r\n```\r\n\r\nresults in\r\n\r\n```\r\n62.751 MB\r\n18.416 MB\r\n1.073 MB\r\n-3.138 MB\r\n2.105 MB\r\n0.053 MB\r\n-2.064 MB\r\n2.077 MB\r\n-1.032 MB\r\n0.000 MB\r\n2.109 MB\r\n-4.170 MB\r\n4.174 MB\r\n-2.077 MB\r\n-0.020 MB\r\n0.020 MB\r\n1.016 MB\r\n-1.008 MB\r\n-0.016 MB\r\n0.016 MB\r\n2.089 MB\r\n-2.073 MB\r\n2.085 MB\r\n...\r\n```\r\n\r\nwhereas removing the `function_cache.clear()` results in\r\n\r\n```\r\n79.806 MB\r\n27.992 MB\r\n25.338 MB\r\n30.044 MB\r\n21.045 MB\r\n30.056 MB\r\n23.114 MB\r\n31.076 MB\r\n24.187 MB\r\n27.918 MB\r\n24.207 MB\r\n26.882 MB\r\n26.259 MB\r\n...\r\n```\r\n\r\nSo that _is_ what's holding all of the memory. It must be some connection that Python doesn't see\u2014maybe it goes through a reference in an extension module? (Maybe it goes through a NumPy object array? numpy/numpy#6581)\r\n\r\nSince this is a Dask feature, what do we want to do about it? @lgray, would it be sufficient to have Coffea clear the Dask function cache?",
  "created_at":"2024-01-24T19:24:22Z",
  "id":1908775887,
  "issue":1093,
  "node_id":"IC_kwDOD6Q_ss5xxZvP",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T19:24:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"That'll certainly fix it for coffea.\r\n\r\nOne thing I noticed in function_cache is that it's holding the uncompressed `pickle` of the function and the dask.base.tokenize for the function key changes with every uproot.dask open.\r\n\r\nThat's probably why we don't see a connection to `lazy` above.",
  "created_at":"2024-01-24T20:00:30Z",
  "id":1908826826,
  "issue":1093,
  "node_id":"IC_kwDOD6Q_ss5xxmLK",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T20:00:30Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"That's it then! That's why it costs memory, but can't be seen as objects of the expected types.\r\n\r\nSo, in the end, the recommendation for everyone is to check their Dask function cache. I'll convert this into a Discussion as a way for others to find this conclusion.\r\n\r\n---------------\r\n\r\nActually, it could\u2014possibly\u2014be fixed in Uproot by replacing the TTree metadata data structure with a streamlined data structure containing only that which is necessary to fetch arrays. (Mostly the `fBasketSeek`, etc.)\r\n\r\n```python\r\n>>> import sys\r\n>>> import uproot\r\n>>> tree = uproot.open(\r\n...     {\"https://github.com/CoffeaTeam/coffea/raw/master/tests/samples/nano_dy.root\": \"Events\"}\r\n... )\r\n>>> minimal = {}\r\n>>> for k, v in tree.items():\r\n...     minimal[k, \"seek\"] = v.member(\"fBasketSeek\")[:v.num_baskets]\r\n...     minimal[k, \"bytes\"] = v.member(\"fBasketBytes\")[:v.num_baskets]\r\n...     minimal[k, \"entry\"] = v.member(\"fBasketEntry\")[:v.num_baskets + 1]\r\n... \r\n>>> sum(sys.getsizeof(k) + sys.getsizeof(v) for k, v in minimal.items()) / 1024**2\r\n0.7204971313476562\r\n```\r\n\r\ni.e. something like 0.7 MiB for this file, but larger if it had more baskets. It's likely that I'm forgetting some other essential metadata, which would bring this figure up.",
  "created_at":"2024-01-24T20:22:37Z",
  "id":1908858113,
  "issue":1093,
  "node_id":"IC_kwDOD6Q_ss5xxt0B",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T20:24:01Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"yep this is good to go from my end!",
  "created_at":"2024-01-23T20:02:55Z",
  "id":1906832226,
  "issue":1094,
  "node_id":"IC_kwDOD6Q_ss5xp_Ni",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-23T20:02:55Z",
  "user":"MDQ6VXNlcjMyMDIwOTA="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"@gordonwatts thanks for reporting this! I suspect this will follow from recent changes to our file name handling. @lobis any clues? :)",
  "created_at":"2024-01-24T15:16:59Z",
  "id":1908340470,
  "issue":1095,
  "node_id":"IC_kwDOD6Q_ss5xvvb2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T15:16:59Z",
  "user":"MDQ6VXNlcjEyNDg0MTM="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Yes, this is expected behaviour that was added at some point in the 5.2.0 release. There should be a mention in the release notes but I haven't checked (at least there was a PR with this).\r\n\r\nWe chose to only support files ending in `.root` when the `file:object` syntax is used. We chose to do this because it was not possible to support the same kind of complex url-chain patterns that fsspec supports if we had to also support the `file:object` syntax (it may be possible but very complex and prone to error). In clonclusion: `file:object` syntax won't work if the files does not end in `.root` and this is intended.\r\n\r\nYou can always use the `dict` syntax (`{\"file.root.1\": \"object\"}`) to achieve the same effect (I actually prefer this) and this will work regardless of the file extension.",
  "created_at":"2024-01-24T15:26:04Z",
  "id":1908358281,
  "issue":1095,
  "node_id":"IC_kwDOD6Q_ss5xvzyJ",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T15:26:04Z",
  "user":"MDQ6VXNlcjM1ODAzMjgw"
 },
 {
  "author_association":"MEMBER",
  "body":"That's right: the colon syntax has been hard to maintain, so Uproot 5.2.x simplified it. I used to keep a list of Issues and Discussions about it, but it's more than a dozen now. Page 19 of [this talk](https://indico.jlab.org/event/459/contributions/11547/) shows a screenshot of all those issues and an analysis of user code, which demonstrates that people do use it and we can't get rid of it.\r\n\r\nSo now we only support `path/in/filesystem.root:path/inside/file` if the filesystem name ends in `.root`. If it doesn't, that's what the `{\"path/in/filesystem.root\": \"path/inside/file\"}` syntax is for\u2014it's not a workaround, it's the intended use.\r\n\r\nI'm going to make this a Discussion because it's not a work-item but it would be useful for others to (hopefully) find if they run into it.",
  "created_at":"2024-01-24T17:48:05Z",
  "id":1908634189,
  "issue":1095,
  "node_id":"IC_kwDOD6Q_ss5xw3JN",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T17:48:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I put a \"thumbs up\" on this. In the context of https://github.com/intake/awkward-pandas, we've been thinking about Polars, too. (Extending awkward-pandas would be a necessary step to get ragged arrays in Polars, though in principle it could be added to Uproot for flat arrays now.)\r\n\r\nIncidentally, all of the Pandas conversion happens in one file:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/e592ae333c2d8d2ee7d65f1a2de1fcb3a8fea13f/src/uproot/interpretation/library.py#L746-L923\r\n\r\nSome of the preparation steps are not Pandas-specific and can be reused in a new library (fourth after `library=\"np\"`, `library=\"ak\"`, and `library=\"pd\"`). Do you know the Polars data constructors well enough to do that?\r\n\r\nActually, now that I think of it, Polars columns are in Apache Arrow format. Maybe we could use [ak.to_arrow](https://awkward-array.org/doc/main/reference/generated/ak.to_arrow.html) or [ak.to_arrow_table](https://awkward-array.org/doc/main/reference/generated/ak.to_arrow_table.html) instead of expanding awkward-pandas. @Esword618, do you know enough about getting data into Polars to know if there's an easy way to do it with a pyarrow array or Table?",
  "created_at":"2024-01-24T16:06:34Z",
  "id":1908440352,
  "issue":1096,
  "node_id":"IC_kwDOD6Q_ss5xwH0g",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T16:06:34Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I'm also new to Polars and not very familiar with it, but I'm willing to learn about it and try to add this feature to uproot.",
  "created_at":"2024-01-25T02:24:27Z",
  "id":1909239183,
  "issue":1096,
  "node_id":"IC_kwDOD6Q_ss5xzK2P",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T02:24:27Z",
  "user":"MDQ6VXNlcjczMDU4MDYy"
 },
 {
  "author_association":"MEMBER",
  "body":"Okay, thanks! The first question that could make short work of this is to see if Polars has any constructor that turns a pyarrow array or a pyarrow Table into a DataFrame. If this is true, then there would be almost no work on our side.\r\n\r\nHere's a way to make a pyarrow array or Table (other than using pyarrow's own constructors; I think Awkward Arrays are easier):\r\n\r\n```python\r\n>>> import awkward as ak\r\n>>> ak_array = ak.Array([\r\n...     {\"col1\": 1.1, \"col2\": [1]},\r\n...     {\"col1\": 2.2, \"col2\": [1, 2]},\r\n...     {\"col1\": 3.3, \"col2\": [1, 2, 3]},\r\n... ])\r\n>>> ak.to_arrow(ak_array)\r\n<awkward._connect.pyarrow.AwkwardArrowArray object at 0x738fb207b880>\r\n-- is_valid: all not null\r\n-- child 0 type: extension<awkward<AwkwardArrowType>>\r\n  [\r\n    1.1,\r\n    2.2,\r\n    3.3\r\n  ]\r\n-- child 1 type: extension<awkward<AwkwardArrowType>>\r\n  [\r\n    [\r\n      1\r\n    ],\r\n    [\r\n      1,\r\n      2\r\n    ],\r\n    [\r\n      1,\r\n      2,\r\n      3\r\n    ]\r\n  ]\r\n>>> ak.to_arrow_table(ak_array)\r\npyarrow.Table\r\ncol1: extension<awkward<AwkwardArrowType>> not null\r\ncol2: extension<awkward<AwkwardArrowType>> not null\r\n----\r\ncol1: [[1.1,2.2,3.3]]\r\ncol2: [[[1],[1,2],[1,2,3]]]\r\n```\r\n\r\nI'd expect pyarrow array to be something like a Series and a pyarrow Table to be something like a DataFrame. Arrow makes a distinction between records with named fields in an array and the top-level fields of a Table. You might try different `ak_arrays`, including simpler ones like\r\n\r\n```python\r\n>>> ak_array = ak.Array([1.1, 2.2, 3.3])\r\n```\r\n\r\nand more complex ones like\r\n\r\n```python\r\n>>> ak_array = ak.Array([1.1, 2.2, 3.3, [1, 2, 3, None]])\r\n```",
  "created_at":"2024-01-25T13:50:56Z",
  "id":1910259619,
  "issue":1096,
  "node_id":"IC_kwDOD6Q_ss5x3D-j",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T13:50:56Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Dear @jpivarski:\r\nI did try, I read the root file, read the TTree inside the data, converted to np format, and then converted to polars. It's gonna work. What should I do next? Of course you can have other suggestions as well, I want to make something for uproot.\r\n<img width=\"739\" alt=\"image\" src=\"https://github.com/scikit-hep/uproot5/assets/73058062/1139ab28-368f-47db-9481-4940f0d68d1e\">\r\n ",
  "created_at":"2024-01-27T06:09:19Z",
  "id":1913024155,
  "issue":1096,
  "node_id":"IC_kwDOD6Q_ss5yBm6b",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-27T06:09:19Z",
  "user":"MDQ6VXNlcjczMDU4MDYy"
 },
 {
  "author_association":"MEMBER",
  "body":"That's great! There's an inefficiency in that pathway, though: those NumPy arrays have `dtype=object`, so `arrays` nested inside `arrays` are separate Python objects, with all of the memory bloat and extra CPU cycles that implies. (If it were just a few percent, I wouldn't bother mentioning it, but it's usually an order of magnitude effect.)\r\n\r\nIs it possible to do this?\r\n\r\n```python\r\nimport awkward as ak\r\nimport polars as pl\r\n\r\ndict_of_awkward_arrays = mu_tree.arrays(..., library=\"ak\", how=dict)\r\ndict_of_arrow_arrays = {k: ak.to_arrow(v, extensionarray=False) for k, v in dict_of_awkward_arrays.items()}\r\nlist_of_polars_series = [pl.Series(k, v) for k, v in dict_of_arrow_arrays.items()]\r\npolars_df = pl.DataFrame(list_of_polars_series)\r\npolars_df\r\n```\r\n\r\nOr this?\r\n\r\n```python\r\nimport awkward as ak\r\nimport pyarrow as pa\r\nimport polars as pl\r\n\r\nawkward_array = mu_tree.arrays(..., library=\"ak\")\r\narrow_table = ak.to_arrow_table(awkward_array, extensionarray=False)\r\npolars_df = pl.DataFrame(arrow_table)\r\npolars_df\r\n```\r\n\r\n(Replace `...` with desired branches, like `[\"evtID\", \"MuMult\", \"PDG\"]`.)\r\n\r\nLooking at the Polars documentation, [pl.Series](https://docs.pola.rs/py-polars/html/reference/series/index.html) allows pyarrow arrays (`pa.array`) as one of its [ArrayLike](https://github.com/pola-rs/polars/blob/5da14a0d32dc3cf7825e5f25ca2de8c74277939b/py-polars/polars/series/series.py#L155-L163) types, and [pl.DataFrame](https://docs.pola.rs/py-polars/html/reference/dataframe/index.html) allows pyarrow Tables (`pa.Table`) as one of its [FrameInitTypes](https://github.com/pola-rs/polars/blob/5da14a0d32dc3cf7825e5f25ca2de8c74277939b/py-polars/polars/type_aliases.py#L170-L176), so I think both of the above should work. The route that goes through Series is more explicit and perhaps better for that reason.\r\n\r\n(The `extensionarray=False` argument, described in the [ak.to_arrow](https://awkward-array.org/doc/main/reference/generated/ak.to_arrow.html) and [ak.to_arrow_table](https://awkward-array.org/doc/main/reference/generated/ak.to_arrow_table.html) documentation, prevents Awkward Array from adding metadata to the pyarrow arrays or Tables that would be necessary to convert back to an Awkward Array without losing any information, but not all Arrow-compliant tools know what to do with ExtensionArrays, so for safety, I've left it off. Does Polars know what to do with it? If it doesn't mind consuming an array with `extensionarray=True`, then I wonder what would happen if the output of [pl.DataFrame.to_arrow](https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.to_arrow.html) or [pl.Series.to_arrow](https://docs.pola.rs/py-polars/html/reference/series/api/polars.Series.to_arrow.html) is passed through [ak.from_arrow](https://awkward-array.org/doc/main/reference/generated/ak.from_arrow.html): would it return an Awkward Array without loss?)",
  "created_at":"2024-01-29T18:58:00Z",
  "id":1915367294,
  "issue":1096,
  "node_id":"IC_kwDOD6Q_ss5yKi9-",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-29T18:58:00Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The first thing I noticed is that this TBranch includes an `ElementLink` data type.\r\n\r\n```python\r\n>>> tree[\"PrimaryVerticesAuxDyn.trackParticleLinks\"].typename\r\n'std::vector<std::vector<ElementLink<DataVector<xAOD::TrackParticle_v1>>>>'\r\n```\r\n\r\nI wonder if it's related to #951.\r\n\r\nThe error message is wrong: if it is the case that we can't deserialize it, it should be a DeserializationError. This error comes from thinking the `context[\"forth\"].gen` is still active when it's not. I'll check that.\r\n\r\nMeanwhile, we can circumvent Uproot's attempt to use AwkwardForth by loading it with `library=\"np\"`.\r\n\r\n```python\r\n>>> tree[\"PrimaryVerticesAuxDyn.trackParticleLinks\"].array(library=\"np\")\r\narray([<STLVector [[]] at 0x751ef3eb3910>,\r\n       <STLVector [[]] at 0x751ef3eb38b0>,\r\n       <STLVector [[]] at 0x751ef3eb3130>, ...,\r\n       <STLVector [[]] at 0x751ef3569ed0>,\r\n       <STLVector [[]] at 0x751ef3569f90>,\r\n       <STLVector [[]] at 0x751ef356a050>], dtype=object)\r\n```\r\n\r\nIt worked! Okay, so this is a TBranch that we can deserialize, but possibly not with AwkwardForth (and that line with `context[\"forth\"].gen` is insufficiently guarded).",
  "created_at":"2024-01-24T21:09:38Z",
  "id":1908919320,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5xx8wY",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T21:09:38Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I observed the same error with Physlite files.\r\nIn my case, it happens with `parentLinks` and `childLinks` in the truth records. \r\nFor some information (e.g. taus) both branches can be read.\r\nFor other particles (e.g. muons) the `childLinks` always fail.\r\nThe most interesting are the `parentLinks`. I observed that if I try to read this branch multiple times it most of the times results in the `AttributeError: 'NoneType' object has no attribute 'reset_active_node'` error but sometimes it does not throw an error and then the `parentLinks` are filled correctly.\r\nI don't need the links at the moment. So my workaround is simply to not read these branches. ",
  "created_at":"2024-01-24T21:40:08Z",
  "id":1908960260,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5xyGwE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T21:40:21Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"It fails in cases in which whole TBaskets consist of empty lists. The AwkwardForth-discovery process is in a state in which the Forth code hasn't been generated yet because it hasn't seen a full example datum, but it hasn't given up yet because it might still find a full datum. This was tested in our small (mostly single-TBucket) test files, but the cases you've seen, @ivukotic and @Superharz, are in this state when transitioning from one TBasket to the next. The indicator of this state is when `context[\"forth\"].vm` doesn't exist at startup _or is equal to None_ after a TBasket, and the latter state wasn't correctly checked.\r\n\r\nBut, fortunately, the data are readable. It's not related to #951.",
  "created_at":"2024-01-24T22:02:46Z",
  "id":1908988634,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5xyNra",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T22:02:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Thank you for the explanation.\r\nBut why does it sometimes succeeds with reading a file and most of the times not while reading the exact same file?\r\nI should mention that I test this in a Jupyter Notebook by simply re-runnig the cell to read the branch until it does not throw an error. So maybe it could be some IPython stuff.",
  "created_at":"2024-01-24T22:14:51Z",
  "id":1909002535,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5xyREn",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T22:14:51Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"That... does not make sense. Unless maybe you're using an `interpretation_executor` to read the TBaskets with threads, in which case, there could be a race condition? If read sequentially without threads (the default), this ought to be deterministic.",
  "created_at":"2024-01-24T22:19:30Z",
  "id":1909007667,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5xySUz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T22:19:30Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"This is all I am doing right now:\r\n\r\n```\r\nwith uproot.open(path + filename) as f:\r\n    f[tree][\"TruthBottomAuxDyn.parentLinks\"].array()\r\n```\r\n\r\nIt sometimes works, but most of the times does not work. This behavior also stays the same if I restart the Python kernel after each try.",
  "created_at":"2024-01-24T22:27:54Z",
  "id":1909016429,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5xyUdt",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T22:27:54Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"Well, I can't _think_ of anything non-deterministic in this process, but it is a complex process.\r\n\r\nThe variable in question is an attribute of a [thread-local variable](https://docs.python.org/3/library/threading.html#thread-local-data) so that it behaves properly if multithreading is involved (though most of the time, it's not). Maybe this isn't as deterministic as I think it is?\r\n\r\nActually, the TBaskets can arrive in any order\u2014the server is not obliged to send them in file-order if it doesn't want to, and then Uproot would deal with them in the order they're received. That's a source of non-determinism. I don't think it can apply to local files, even though we're getting them through fsspec now.\r\n\r\nBut anyway, the point may be moot, since the fix has been merged into `main`. If you `pip install -e .` from Uproot in the `main` branch, you shouldn't see the issue at all. Is that the case?",
  "created_at":"2024-01-24T22:37:05Z",
  "id":1909031273,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5xyYFp",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T22:37:05Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I tested it.\r\nFirst: Sometimes it works:\r\n\r\n```\r\n>>> with uproot.open(path + filename) as f:\r\n...     f[tree][\"TruthBottomAuxDyn.childLinks\"].array()\r\n...\r\n<Array [[], [], [], [], ..., [], [], [], []] type='40000 * var * var * stru...'>\r\n```\r\n\r\nBut sometimes this non-deterministic error happens with the exact same input file. However, the error message is now different from before:\r\n\r\n```\r\n>>> with uproot.open(path + filename) as f:\r\n...     f[tree][\"TruthBottomAuxDyn.childLinks\"].array()\r\n...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\behaviors\\TBranch.py\", line 1815, in array\r\n    _ranges_or_baskets_to_arrays(\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\behaviors\\TBranch.py\", line 3142, in _ranges_or_baskets_to_arrays\r\n    uproot.source.futures.delayed_raise(*obj)\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\source\\futures.py\", line 38, in delayed_raise\r\n    raise exception_value.with_traceback(traceback)\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\behaviors\\TBranch.py\", line 3111, in basket_to_array\r\n    arrays[branch.cache_key] = interpretation.final_array(\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\interpretation\\objects.py\", line 475, in final_array\r\n    output = numpy.concatenate(trimmed)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\highlevel.py\", line 1527, in __array_function__\r\n    return ak._connect.numpy.array_function(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_connect\\numpy.py\", line 102, in array_function\r\n    return function(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_connect\\numpy.py\", line 142, in ensure_valid_args\r\n    return function(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_dispatch.py\", line 62, in dispatch\r\n    next(gen_or_result)\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_concatenate.py\", line 66, in concatenate\r\n    return _impl(arrays, axis, mergebool, highlevel, behavior, attrs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_concatenate.py\", line 114, in _impl\r\n    content_or_others = ensure_same_backend(\r\n                        ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_concatenate.py\", line 116, in <genexpr>\r\n    ctx.unwrap(\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_layout.py\", line 146, in unwrap\r\n    return to_layout_impl(\r\n           ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_to_layout.py\", line 177, in _impl\r\n    promoted_layout = ak.operations.from_numpy(\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_dispatch.py\", line 39, in dispatch\r\n    gen_or_result = func(*args, **kwargs)\r\n                    ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\operations\\ak_from_numpy.py\", line 55, in from_numpy\r\n    from_arraylib(array, regulararray, recordarray),\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\awkward\\_layout.py\", line 347, in from_arraylib\r\n    raise TypeError(\"Awkward Array does not support arrays with object dtypes.\")\r\nTypeError: Awkward Array does not support arrays with object dtypes.\r\n\r\nThis error occurred while calling\r\n\r\n    ak.concatenate(\r\n        [<Array [[], [], [], [], ..., [], [], [], []] type='45 * var * var * ...\r\n    )\r\n```",
  "created_at":"2024-01-24T23:17:38Z",
  "id":1909082780,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5xykqc",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T23:17:38Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"Open this as a new issue. I think the non-deterministic part might be something unrelated to the first issue. If you can provide an example file, that would help a lot.\r\n\r\nI think it's going wrong here:\r\n\r\n```\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\behaviors\\TBranch.py\", line 3111, in basket_to_array\r\n    arrays[branch.cache_key] = interpretation.final_array(\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\flori\\miniforge3\\envs\\test\\Lib\\site-packages\\uproot\\interpretation\\objects.py\", line 475, in final_array\r\n    output = numpy.concatenate(trimmed)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n```\r\n\r\nIn the subsequent output, it's trying to make an Awkward Array out of a NumPy array with `dtype=object`, which isn't allowed. On the line above, I wonder if the list of arrays, `trimmed`, accidentally has a mix of Awkward Arrays and NumPy `dtype=object` arrays. (The latter need an additional step to be turned into Awkward Arrays.)",
  "created_at":"2024-01-25T00:02:26Z",
  "id":1909123709,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5xyup9",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T00:02:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I opened the issue in #1101 ",
  "created_at":"2024-01-25T08:55:05Z",
  "id":1909686446,
  "issue":1099,
  "node_id":"IC_kwDOD6Q_ss5x04Cu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T08:55:05Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"The rest of this PR is making it possible for me to skip Dask distributed tests, which complain about Jupyter (?!?) on my computer. I've conda-updated; I don't know what's wrong with Dask, distributed, or Jupyter.",
  "created_at":"2024-01-24T21:56:58Z",
  "id":1908981578,
  "issue":1100,
  "node_id":"IC_kwDOD6Q_ss5xyL9K",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T21:56:58Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@SethBendigo, if this looks good to you, give me an approval and I'll merge it.",
  "created_at":"2024-01-24T21:57:26Z",
  "id":1908982101,
  "issue":1100,
  "node_id":"IC_kwDOD6Q_ss5xyMFV",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2024-01-24T21:57:26Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"Thanks!",
  "created_at":"2024-01-24T22:19:57Z",
  "id":1909008185,
  "issue":1100,
  "node_id":"IC_kwDOD6Q_ss5xySc5",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-24T22:19:57Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"It looks similar to issue #951\r\nHowever, my file has been created with a newer ATLAS Athena release: `Athena_24.0.12`",
  "created_at":"2024-01-25T09:09:25Z",
  "id":1909709875,
  "issue":1101,
  "node_id":"IC_kwDOD6Q_ss5x09wz",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T09:09:25Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"Could you provide a file? I think this depends on the exact combination of empty `std::vectors` in baskets, which I think is leading to the `trimmed` list having a mix of Awkward Arrays and NumPy `dtype=object` arrays. The latter need to be converted into Awkward Arrays (we have a utility function for this) to fix it, but I want to see it failing to do this in a test-driven way.",
  "created_at":"2024-01-25T15:08:16Z",
  "id":1910398491,
  "issue":1101,
  "node_id":"IC_kwDOD6Q_ss5x3l4b",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:08:16Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Can you access ATLAS internal files?",
  "created_at":"2024-01-25T15:20:44Z",
  "id":1910425041,
  "issue":1101,
  "node_id":"IC_kwDOD6Q_ss5x3sXR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:20:44Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"MEMBER",
  "body":"> Can you access ATLAS internal files?\r\n\r\nNope. (I can't access CMS internal files, either, and I'm a CMS member.)\r\n\r\nBut I may have files from #951; it seems to be the same issue.",
  "created_at":"2024-01-25T15:53:51Z",
  "id":1910488634,
  "issue":1101,
  "node_id":"IC_kwDOD6Q_ss5x3746",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T15:53:51Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"We can provide some latest ATLAS PHYSLITE files to non-collaborators. But let me ask @Superharz , do you test on a PHYSLITE file containing real data or Monte Carlo events? Because we see some different behaviour that we try to understand/disentangle...",
  "created_at":"2024-01-25T16:30:21Z",
  "id":1910561348,
  "issue":1101,
  "node_id":"IC_kwDOD6Q_ss5x4NpE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T16:30:21Z",
  "user":"MDQ6VXNlcjE5NTU2OTM0"
 },
 {
  "author_association":"NONE",
  "body":"I am using MC events from the MC23 campaign. \r\nI am using file `DAOD_PHYSLITE.35010028._000001.pool.root.1` from\r\n`mc23_13p6TeV.601191.PhPy8EG_AZNLO_Ztautau.deriv.DAOD_PHYSLITE.e8514_s4162_r14622_p5855`",
  "created_at":"2024-01-25T16:49:03Z",
  "id":1910597655,
  "issue":1101,
  "node_id":"IC_kwDOD6Q_ss5x4WgX",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T16:49:03Z",
  "user":"MDQ6VXNlcjExMzA2NzMy"
 },
 {
  "author_association":"NONE",
  "body":"to make things easier I got that file in xcache. Here:\r\nroot://xcache.af.uchicago.edu//root://xrootd.echo.stfc.ac.uk:1094/atlas:datadisk/rucio/mc23_13p6TeV/26/c7/DAOD_PHYSLITE.35010028._000001.pool.root.1\r\n\r\n",
  "created_at":"2024-01-25T20:09:41Z",
  "id":1910913865,
  "issue":1101,
  "node_id":"IC_kwDOD6Q_ss5x5jtJ",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2024-01-25T20:09:53Z",
  "user":"MDQ6VXNlcjE3MzY5ODQ="
 },
 {
  "author_association":"MEMBER",
  "body":"@martindurant (and @douglasdavis), should this be a [dask-awkward](https://github.com/dask-contrib/dask-awkward) issue? (I don't have permissions to transfer it.)",
  "created_at":"2024-01-25T19:28:22Z",
  "id":1910846263,
  "issue":1102,
  "node_id":"IC_kwDOD6Q_ss5x5TM3",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T19:28:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"It says that the graph contains a lock object. That must have been introduced in the from_map call that I think uproot uses - perhaps proxying an open file? Printing out the details within the graph or translating it to a low-level-graph might help.",
  "created_at":"2024-01-25T19:42:12Z",
  "id":1910868556,
  "issue":1102,
  "node_id":"IC_kwDOD6Q_ss5x5YpM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T19:42:12Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"> It says that the graph contains a lock object.\r\n\r\nUh oh, I missed that. (I was looking at the bottom of the stack trace.) I'll figure out what that lock is for.",
  "created_at":"2024-01-25T19:43:59Z",
  "id":1910872789,
  "issue":1102,
  "node_id":"IC_kwDOD6Q_ss5x5ZrV",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T19:43:59Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I wish we had a better tool to tell you where in the object the unpicklable thing lives",
  "created_at":"2024-01-25T19:45:22Z",
  "id":1910876484,
  "issue":1102,
  "node_id":"IC_kwDOD6Q_ss5x5alE",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T19:45:22Z",
  "user":"MDQ6VXNlcjYwNDIyMTI="
 },
 {
  "author_association":"MEMBER",
  "body":"There's no issue in a random ROOT file,\r\n\r\n```python\r\n>>> import uproot, skhep_testdata\r\n>>> from dask.distributed import Client\r\n>>> client = Client()\r\n>>> \r\n>>> a = uproot.dask({skhep_testdata.data_path(\"uproot-HZZ.root\"): \"events\"})\r\n>>> a.compute()\r\n<Array [{NJet: 0, Jet_Px: [], ...}, ..., {...}] type='2421 * {NJet: int32, ...'>\r\n```\r\n\r\nso it must be something special in DAOD_PHYSLITE. Yes, it is.",
  "created_at":"2024-01-25T19:51:41Z",
  "id":1910890342,
  "issue":1102,
  "node_id":"IC_kwDOD6Q_ss5x5d9m",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T19:51:41Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"It's this one:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/c128e4bebdd5aad2bfaa5bc5f4ce896fe7bd9e72/src/uproot/behaviors/TBranch.py#L2473\r\n\r\nThe thing that's special about this file (these branches, to be specific) is that it has incompletely written (\"embedded\") TBaskets, which have to be read single-threaded because reading it changes the TBranch object.\r\n\r\nI'll adapt the `__setstate__`/`__getstate__` to drop and recreate the lock on pickling.",
  "created_at":"2024-01-25T20:13:20Z",
  "id":1910918309,
  "issue":1102,
  "node_id":"IC_kwDOD6Q_ss5x5kyl",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2024-01-25T20:16:06Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"PR #1103 fixes this.",
  "created_at":"2024-01-25T20:59:45Z",
  "id":1910986891,
  "issue":1102,
  "node_id":"IC_kwDOD6Q_ss5x51iL",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T20:59:45Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"LGTM. However, one of the other tests times out on the Ubuntu-latest 3.8 machine; It doesn't seem to be related to this fix though ",
  "created_at":"2024-01-26T09:36:23Z",
  "id":1911748960,
  "issue":1103,
  "node_id":"IC_kwDOD6Q_ss5x8vlg",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-26T09:36:23Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"Requests for remote files sometimes hang.\r\n\r\nThanks!",
  "created_at":"2024-01-26T14:22:46Z",
  "id":1912147415,
  "issue":1103,
  "node_id":"IC_kwDOD6Q_ss5x-Q3X",
  "performed_via_github_app":null,
  "reactions":{
   "laugh":1,
   "total_count":1
  },
  "updated_at":"2024-01-26T14:22:46Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski this one should go in with the next release of uproot.",
  "created_at":"2024-01-25T23:48:21Z",
  "id":1911180557,
  "issue":1104,
  "node_id":"IC_kwDOD6Q_ss5x6k0N",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-25T23:48:21Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"Agreed - probably good for a patch release now!",
  "created_at":"2024-01-26T16:33:04Z",
  "id":1912347364,
  "issue":1104,
  "node_id":"IC_kwDOD6Q_ss5x_Brk",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-26T16:33:04Z",
  "user":"MDQ6VXNlcjEwNjgwODk="
 },
 {
  "author_association":"MEMBER",
  "body":"fsspec-xrootd file-globbing of\r\n\r\n```\r\nroot://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_*.root\r\n```\r\n\r\nkeeps killing the GitHub actions because it's timing out. That doesn't seem to be what this PR is about. I wonder if the server is just down or slow today. I ran it in a terminal: it takes about a minute for each of the two files, which is a lot for any one test, and most of the time was not spent receiving data in iftop.\r\n\r\nIf it's not what this is about, I'll merge this PR as-is, but I always do a test on `main` before releasing, and it might fail again there. If so, I'll add a `@pytest.mark.skip(reason=\"flaky server\")` to that test.",
  "created_at":"2024-01-26T17:37:07Z",
  "id":1912432150,
  "issue":1104,
  "node_id":"IC_kwDOD6Q_ss5x_WYW",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-26T17:37:07Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I'm merging this even though it has these specific test failures because of the reasoning described in https://github.com/scikit-hep/uproot5/pull/1104#issuecomment-1912432150.",
  "created_at":"2024-01-26T19:07:23Z",
  "id":1912554030,
  "issue":1105,
  "node_id":"IC_kwDOD6Q_ss5x_0Iu",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-01-26T19:07:23Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"One way we can get AttributeErrors when introducing a new default like FSSpecSource is by expecting it on all Sources but the new Source doesn't have it. That's not what happened here: `_fh` is only on FSSpecSource, so it's an implementation detail of that particular class. For the attribute to be missing, there must be multiple ways to create the class, and the case that broke for you, @torresramiro350, is one that took an alternative that failed to make the attribute.\r\n\r\nReading the code, it looks like `_fh` is a transient file handle, and it's allowed to be `None`. (That's its initial state in `__init__`.) In fact, it looks like it's missing from `__setstate__`, which reconstitutes an object during unpickling:\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/1cee9606748482c8a06828646144bd44caac4b8f/src/uproot/source/fsspec.py#L63-L73\r\n\r\nSince `_fh` and `_file` are both introduced with an initial value of `None` by `__init__` (just before `_open`), it seems like they ought to be reintroduced by `__setstate__`, so I did that in #1118.\r\n\r\nI have _not tested it_, so let me know, @torresramiro350, if that's right.\r\n\r\n@lobis, is this correct? I'll add you as a reviewer to the PR so that you can weigh in there, but do you see anything else that might be a problem here?",
  "created_at":"2024-02-05T19:12:17Z",
  "id":1927859481,
  "issue":1117,
  "node_id":"IC_kwDOD6Q_ss5y6M0Z",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-05T19:12:17Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"Introducing `self._fh = None` in `__setstate__` does seem to fix the issue.",
  "created_at":"2024-02-05T19:24:20Z",
  "id":1927877879,
  "issue":1117,
  "node_id":"IC_kwDOD6Q_ss5y6RT3",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2024-02-05T19:24:20Z",
  "user":"MDQ6VXNlcjgyNjkzMDI0"
 },
 {
  "author_association":"MEMBER",
  "body":"When @lobis approves it, I'll merge it. Thanks for testing!",
  "created_at":"2024-02-05T19:27:09Z",
  "id":1927881792,
  "issue":1117,
  "node_id":"IC_kwDOD6Q_ss5y6SRA",
  "performed_via_github_app":null,
  "reactions":{
   "+1":2,
   "total_count":2
  },
  "updated_at":"2024-02-05T19:27:09Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"NONE",
  "body":"I also tried running the example above with uproot 5.2.2 and it seems to take considerably longer to read a ROOT file of ~ 1GB than in version 5.1.2. I'll try to generate a file with fake numbers that can somehow help to diagnose it further. \r\n\r\nI realize that will have to be on another issue. Thanks for the help!",
  "created_at":"2024-02-05T19:27:58Z",
  "id":1927882944,
  "issue":1117,
  "node_id":"IC_kwDOD6Q_ss5y6SjA",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2024-02-05T19:30:30Z",
  "user":"MDQ6VXNlcjgyNjkzMDI0"
 },
 {
  "author_association":"MEMBER",
  "body":"That should be a different issue. I've been hearing a few things about local file access being slower; maybe we should revert to MemoryMappedSource for local files and use fsspec only for remote files...",
  "created_at":"2024-02-06T16:30:32Z",
  "id":1930257032,
  "issue":1117,
  "node_id":"IC_kwDOD6Q_ss5zDWKI",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-06T16:30:32Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I linked this issue to the PR after merging the PR. Closing the issue manually...",
  "created_at":"2024-02-06T16:31:49Z",
  "id":1930264074,
  "issue":1117,
  "node_id":"IC_kwDOD6Q_ss5zDX4K",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-06T16:31:49Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"The one failure is due to #1125. Ignoring...",
  "created_at":"2024-02-14T17:43:13Z",
  "id":1944306998,
  "issue":1119,
  "node_id":"IC_kwDOD6Q_ss5z48U2",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-14T17:43:13Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"This PR doesn't add tests; however, it was tested locally to make sure the executors are exposed correctly. \r\n\r\nI used the tests in file `test_1058_dask_awkward_report.py` to which I added a `decompression_executor` and an `interpretation_executor` with the `submit` function modified to change a variable when called. \r\n\r\nAll seems to be working well :) ",
  "created_at":"2024-02-09T12:42:26Z",
  "id":1935863146,
  "issue":1120,
  "node_id":"IC_kwDOD6Q_ss5zYu1q",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-09T12:42:26Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"(I must have clicked the wrong button when it went to \"changes requested.\")",
  "created_at":"2024-02-07T21:24:22Z",
  "id":1932959285,
  "issue":1122,
  "node_id":"IC_kwDOD6Q_ss5zNp41",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-07T21:24:22Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"COLLABORATOR",
  "body":"Hi @torresramiro350 \r\n\r\nWhat is the definition of `data_bin_defintion` in `directory_key = data_bin_defintion(bin_id)`? \r\n\r\nI randomly picked `directory_key` to equal 'BinInfo/name' and I could not reproduce the behaviour. \r\n\r\n",
  "created_at":"2024-02-09T14:09:32Z",
  "id":1935998347,
  "issue":1124,
  "node_id":"IC_kwDOD6Q_ss5zZP2L",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-09T14:09:32Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"NONE",
  "body":"Yes, sorry I missed to include this. Here's the definition:\r\n```python\r\ndef data_bin_defintion(bin_id: str) -> str:\r\n    return f\"nHit{bin_id}/data/count\"\r\n```",
  "created_at":"2024-02-09T14:14:51Z",
  "id":1936006685,
  "issue":1124,
  "node_id":"IC_kwDOD6Q_ss5zZR4d",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-09T14:14:51Z",
  "user":"MDQ6VXNlcjgyNjkzMDI0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"I just tested this with `main` and `5.1.2` and I am getting comparable results in terms of time. It also does not hang for me in either cases:\r\n\r\n\r\n`main`:\r\n```\r\n(rootanduproot) ioana@Ioanas-MBP uproot5 % python simulate_maptree.py \r\nuproot version:  5.2.3.dev4+ge50b9a5\r\n--- 5.348618984222412 seconds ---\r\ndict_keys(['B0on', 'B0off', 'B1on', 'B1off', 'B2on', 'B2off', 'B3on', 'B3off', 'B4on', 'B4off', 'B5on', 'B5off', 'B6on', 'B6off', 'B7on', 'B7off', 'B8on', 'B8off', 'B9on', 'B9off', 'B10on', 'B10off'])\r\n(rootanduproot) ioana@Ioanas-MBP uproot5 % python simulate_maptree.py \r\nuproot version:  5.2.3.dev4+ge50b9a5\r\n--- 5.045531749725342 seconds ---\r\ndict_keys(['B0on', 'B0off', 'B1on', 'B1off', 'B2on', 'B2off', 'B3on', 'B3off', 'B4on', 'B4off', 'B5on', 'B5off', 'B6on', 'B6off', 'B7on', 'B7off', 'B8on', 'B8off', 'B9on', 'B9off', 'B10on', 'B10off'])\r\n(rootanduproot) ioana@Ioanas-MBP uproot5 % python simulate_maptree.py \r\nuproot version:  5.2.3.dev4+ge50b9a5\r\n--- 5.168824195861816 seconds ---\r\ndict_keys(['B0on', 'B0off', 'B1on', 'B1off', 'B2on', 'B2off', 'B3on', 'B3off', 'B4on', 'B4off', 'B5on', 'B5off', 'B6on', 'B6off', 'B7on', 'B7off', 'B8on', 'B8off', 'B9on', 'B9off', 'B10on', 'B10off'])\r\n```\r\n\r\n`5.1.2`:\r\n```\r\n(rootanduproot) ioana@macmac uproot5 % python simulate_maptree.py\r\nuproot version:  5.1.2\r\n--- 5.188451051712036 seconds ---\r\ndict_keys(['B0on', 'B0off', 'B1on', 'B1off', 'B2on', 'B2off', 'B3on', 'B3off', 'B4on', 'B4off', 'B5on', 'B5off', 'B6on', 'B6off', 'B7on', 'B7off', 'B8on', 'B8off', 'B9on', 'B9off', 'B10on', 'B10off'])\r\n(rootanduproot) ioana@macmac uproot5 % python simulate_maptree.py\r\nuproot version:  5.1.2\r\n--- 4.953443765640259 seconds ---\r\ndict_keys(['B0on', 'B0off', 'B1on', 'B1off', 'B2on', 'B2off', 'B3on', 'B3off', 'B4on', 'B4off', 'B5on', 'B5off', 'B6on', 'B6off', 'B7on', 'B7off', 'B8on', 'B8off', 'B9on', 'B9off', 'B10on', 'B10off'])\r\n(rootanduproot) ioana@macmac uproot5 % python simulate_maptree.py\r\nuproot version:  5.1.2\r\n--- 5.174794912338257 seconds ---\r\ndict_keys(['B0on', 'B0off', 'B1on', 'B1off', 'B2on', 'B2off', 'B3on', 'B3off', 'B4on', 'B4off', 'B5on', 'B5off', 'B6on', 'B6off', 'B7on', 'B7off', 'B8on', 'B8off', 'B9on', 'B9off', 'B10on', 'B10off'])\r\n\r\n```\r\n\r\n\r\nYou are reading the file from local storage, no? -> `simulate_maptree.py -o sample.root`\r\n\r\n",
  "created_at":"2024-02-09T14:27:28Z",
  "id":1936031733,
  "issue":1124,
  "node_id":"IC_kwDOD6Q_ss5zZX_1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-09T14:27:28Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"NONE",
  "body":"Yes, I generated the sample file locally. I just tried a different implementation and it seems to be an issue with `multiprocessing.Pool` itself and not uproot. You can close this issue. Thanks for your quick help. Just for reference, what is the python version that you ran the above code with?",
  "created_at":"2024-02-09T15:49:46Z",
  "id":1936165905,
  "issue":1124,
  "node_id":"IC_kwDOD6Q_ss5zZ4wR",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-09T15:49:46Z",
  "user":"MDQ6VXNlcjgyNjkzMDI0"
 },
 {
  "author_association":"COLLABORATOR",
  "body":"No worries! I ran it with 3.11.3 \u263a\ufe0f",
  "created_at":"2024-02-09T20:57:26Z",
  "id":1936592522,
  "issue":1124,
  "node_id":"IC_kwDOD6Q_ss5zbg6K",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-09T20:57:26Z",
  "user":"MDQ6VXNlcjk3NTE4NzE="
 },
 {
  "author_association":"MEMBER",
  "body":"Let me know if you're done and I'll merge it!",
  "created_at":"2024-02-16T19:24:47Z",
  "id":1949192460,
  "issue":1128,
  "node_id":"IC_kwDOD6Q_ss50LlEM",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-16T19:24:47Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"I think this is done. I'm merging it now. Thanks!",
  "created_at":"2024-02-19T14:13:15Z",
  "id":1952536752,
  "issue":1128,
  "node_id":"IC_kwDOD6Q_ss50YViw",
  "performed_via_github_app":null,
  "reactions":{
   "+1":1,
   "total_count":1
  },
  "updated_at":"2024-02-19T14:13:15Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"MEMBER",
  "body":"@all-contributors please add @YSelfTool for code",
  "created_at":"2024-02-19T14:14:18Z",
  "id":1952538757,
  "issue":1128,
  "node_id":"IC_kwDOD6Q_ss50YWCF",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-19T14:14:18Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 },
 {
  "author_association":"CONTRIBUTOR",
  "body":"@jpivarski \n\nI've put up [a pull request](https://github.com/scikit-hep/uproot5/pull/1131) to add @YSelfTool! :tada:",
  "created_at":"2024-02-19T14:14:27Z",
  "id":1952539075,
  "issue":1128,
  "node_id":"IC_kwDOD6Q_ss50YWHD",
  "performed_via_github_app":"MDM6QXBwMjMxODY=",
  "reactions":{},
  "updated_at":"2024-02-19T14:14:27Z",
  "user":"MDM6Qm90NDY0NDczMjE="
 },
 {
  "author_association":"NONE",
  "body":"## Another comparison:\r\n\r\n### Version failing\r\n\r\nCode:\r\n```\r\nimport time\r\nfrom pathlib import Path\r\nimport h5py\r\nfrom hdf import peak_count\r\nimport uproot\r\n\r\n\r\ndef convert_hdf_to_ntuple(input_path: Path):\r\n    ntuple_path = input_path.with_suffix('.root')\r\n    print(f\"Saving ntuple to {ntuple_path}\")\r\n    before_write = time.time()\r\n    ntuple_path.unlink(missing_ok=True)\r\n\r\n    saving_ok = True\r\n\r\n    with h5py.File(input_path, 'r') as f, uproot.recreate(ntuple_path, compression=None) as fout:\r\n        for channel_no in range(4):\r\n            print(f\"Processing channel {channel_no}\")\r\n            gain_mV = f[f'channel_{channel_no}'].attrs['gain_mV']\r\n            offset_mV = f[f'channel_{channel_no}'].attrs['offset_mV']\r\n            horiz_interval_ns = f[f'channel_{channel_no}'].attrs['horiz_interval_ns']\r\n            fout[f'channel_{channel_no}/gain_mV'] = str(gain_mV)\r\n            fout[f'channel_{channel_no}/offset_mV'] = str(offset_mV)\r\n            fout[f'channel_{channel_no}/horiz_interval_ns'] = str(horiz_interval_ns)\r\n\r\n            peaks_in_bucket = 10000000\r\n            for peak_type in ['positive', 'negative']:\r\n                print(f\"Processing {peak_type} peaks\")\r\n                total_number_of_peaks = peak_count(f, channel_no, peak_type)\r\n                for i in range(0, total_number_of_peaks, peaks_in_bucket):\r\n                    dict_bucket = {}\r\n                    for name, dataset in f[f'channel_{channel_no}/{peak_type}'].items():\r\n                        dict_bucket[name] = dataset[i:i + peaks_in_bucket]\r\n                    dict_bucket['peak_value_mV'] = dict_bucket['peak_value'] * gain_mV\r\n                    dict_bucket['peak_length_ns'] = dict_bucket['peak_length'] * horiz_interval_ns\r\n                    dict_bucket['peak_start_us'] = dict_bucket['peak_start'] * horiz_interval_ns / 1000\r\n                    dict_bucket['peak_cfd_us'] = dict_bucket['peak_cfd_index'] * horiz_interval_ns / 1000\r\n                    dict_bucket['peak_rise_ns'] = dict_bucket['rise_time'] * horiz_interval_ns\r\n                    dict_bucket['peak_area_ns_mV'] = dict_bucket['peak_area'] * horiz_interval_ns * gain_mV\r\n                    dict_bucket['peak_baseline_mV'] = dict_bucket['peak_baseline'] * gain_mV - offset_mV\r\n                    dict_bucket['peak_noise_mV'] = dict_bucket['peak_noise'] * gain_mV\r\n                    dict_bucket['peak_fwhm_ns'] = dict_bucket['peak_fwhm'] * horiz_interval_ns\r\n\r\n                    try:\r\n                        if i == 0:\r\n                            fout[f'channel_{channel_no}/{peak_type}'] = dict_bucket\r\n                        else:\r\n                            fout[f'channel_{channel_no}/{peak_type}'].extend(dict_bucket)\r\n                    except Exception as e:\r\n                        print(f\"Error {e} while writing {i} to {i + peaks_in_bucket}\")\r\n                        saving_ok = False\r\n                        break\r\n                    \r\n    if not saving_ok:\r\n        #print the size of saved file\r\n        print(f\"Generated file with {ntuple_path.stat().st_size} bytes\")\r\n        ntuple_path.unlink()\r\n\r\n    after_write = time.time()\r\n    print(f\"Writing took {after_write - before_write:.3f} s\")\r\n```\r\n\r\nEffect:\r\n```\r\n[ares][plgkongruencj@ac0084 2022-krakow-lgad]$ time poetry run src/convert_from_lv1_to_lv2.py /memfs/7649613/8nA.slim.hdf --save-ntuple\r\nConverting from LV1 to LV2\r\nCommand used: convert_from_lv1_to_lv2.py /memfs/7649613/8nA.slim.hdf --save-ntuple\r\nInput path: /memfs/7649613/8nA.slim.hdf\r\nSaving ntuple\r\nSaving ntuple to /memfs/7649613/8nA.slim.root\r\nProcessing channel 0\r\nProcessing positive peaks\r\nProcessing negative peaks\r\nProcessing channel 1\r\nProcessing positive peaks\r\nError 'i' format requires -2147483648 <= number <= 2147483647 while writing 0 to 10000000\r\nProcessing negative peaks\r\nError 'i' format requires -2147483648 <= number <= 2147483647 while writing 0 to 10000000\r\nProcessing channel 2\r\nProcessing positive peaks\r\nProcessing negative peaks\r\nProcessing channel 3\r\nProcessing positive peaks\r\nProcessing negative peaks\r\nGenerated file with 7498169826 bytes\r\nWriting took 18.070 s\r\n\r\nreal    0m19.113s\r\nuser    0m8.029s\r\nsys     0m10.919s\r\n```\r\n\r\n### Version working:\r\n\r\nIt seems that enabling default compression makes possible to write correct files larger than 2 GB.\r\n\r\nCode:\r\n```\r\nimport time\r\nfrom pathlib import Path\r\nimport h5py\r\nfrom hdf import peak_count\r\nimport uproot\r\n\r\n\r\ndef convert_hdf_to_ntuple(input_path: Path):\r\n    ntuple_path = input_path.with_suffix('.root')\r\n    print(f\"Saving ntuple to {ntuple_path}\")\r\n    before_write = time.time()\r\n    ntuple_path.unlink(missing_ok=True)\r\n\r\n    saving_ok = True\r\n\r\n    with h5py.File(input_path, 'r') as f, uproot.recreate(ntuple_path) as fout:\r\n        for channel_no in range(4):\r\n            print(f\"Processing channel {channel_no}\")\r\n            gain_mV = f[f'channel_{channel_no}'].attrs['gain_mV']\r\n            offset_mV = f[f'channel_{channel_no}'].attrs['offset_mV']\r\n            horiz_interval_ns = f[f'channel_{channel_no}'].attrs['horiz_interval_ns']\r\n            fout[f'channel_{channel_no}/gain_mV'] = str(gain_mV)\r\n            fout[f'channel_{channel_no}/offset_mV'] = str(offset_mV)\r\n            fout[f'channel_{channel_no}/horiz_interval_ns'] = str(horiz_interval_ns)\r\n\r\n            peaks_in_bucket = 10000000\r\n            for peak_type in ['positive', 'negative']:\r\n                print(f\"Processing {peak_type} peaks\")\r\n                total_number_of_peaks = peak_count(f, channel_no, peak_type)\r\n                for i in range(0, total_number_of_peaks, peaks_in_bucket):\r\n                    dict_bucket = {}\r\n                    for name, dataset in f[f'channel_{channel_no}/{peak_type}'].items():\r\n                        dict_bucket[name] = dataset[i:i + peaks_in_bucket]\r\n                    dict_bucket['peak_value_mV'] = dict_bucket['peak_value'] * gain_mV\r\n                    dict_bucket['peak_length_ns'] = dict_bucket['peak_length'] * horiz_interval_ns\r\n                    dict_bucket['peak_start_us'] = dict_bucket['peak_start'] * horiz_interval_ns / 1000\r\n                    dict_bucket['peak_cfd_us'] = dict_bucket['peak_cfd_index'] * horiz_interval_ns / 1000\r\n                    dict_bucket['peak_rise_ns'] = dict_bucket['rise_time'] * horiz_interval_ns\r\n                    dict_bucket['peak_area_ns_mV'] = dict_bucket['peak_area'] * horiz_interval_ns * gain_mV\r\n                    dict_bucket['peak_baseline_mV'] = dict_bucket['peak_baseline'] * gain_mV - offset_mV\r\n                    dict_bucket['peak_noise_mV'] = dict_bucket['peak_noise'] * gain_mV\r\n                    dict_bucket['peak_fwhm_ns'] = dict_bucket['peak_fwhm'] * horiz_interval_ns\r\n\r\n                    try:\r\n                        if i == 0:\r\n                            fout[f'channel_{channel_no}/{peak_type}'] = dict_bucket\r\n                        else:\r\n                            fout[f'channel_{channel_no}/{peak_type}'].extend(dict_bucket)\r\n                    except Exception as e:\r\n                        print(f\"Error {e} while writing {i} to {i + peaks_in_bucket}\")\r\n                        saving_ok = False\r\n                        break\r\n                    \r\n    if not saving_ok:\r\n        #print the size of saved file\r\n        print(f\"Generated file with {ntuple_path.stat().st_size} bytes\")\r\n        ntuple_path.unlink()\r\n\r\n    after_write = time.time()\r\n    print(f\"Writing took {after_write - before_write:.3f} s\")\r\n```\r\n\r\nEffect:\r\n```\r\n[ares][plgkongruencj@ac0084 2022-krakow-lgad]$ time poetry run src/convert_from_lv1_to_lv2.py /memfs/7649613/8nA.slim.hdf --save-ntuple\r\nConverting from LV1 to LV2\r\nCommand used: convert_from_lv1_to_lv2.py /memfs/7649613/8nA.slim.hdf --save-ntuple\r\nInput path: /memfs/7649613/8nA.slim.hdf\r\nSaving ntuple\r\nSaving ntuple to /memfs/7649613/8nA.slim.root\r\nProcessing channel 0\r\nProcessing positive peaks\r\nProcessing negative peaks\r\nProcessing channel 1\r\nProcessing positive peaks\r\nProcessing negative peaks\r\nProcessing channel 2\r\nProcessing positive peaks\r\nProcessing negative peaks\r\nProcessing channel 3\r\nProcessing positive peaks\r\nProcessing negative peaks\r\nWriting took 205.975 s\r\n\r\nreal    3m26.941s\r\nuser    3m12.136s\r\nsys     0m14.046s\r\n[ares][plgkongruencj@ac0084 2022-krakow-lgad]$ ls -alh /memfs/7649613/\r\ntotal 9.4G\r\ndrwx------ 5 plgkongruencj root    180 Feb 16 23:19 .\r\ndrwxr-xr-x 3 root          root     60 Feb 16 15:34 ..\r\ndrwxr-xr-x 2 plgkongruencj plgrid   80 Feb 16 20:40 20231204m2\r\n-rw-r--r-- 1 plgkongruencj plgrid 4.3G Feb 16 20:49 8nA.slim.hdf\r\n-rw-r--r-- 1 plgkongruencj plgrid 5.1G Feb 16 23:22 8nA.slim.root\r\n-rwxr-xr-x 1 plgkongruencj plgrid  20M Feb 13 20:15 code\r\ndrwxr-xr-x 5 plgkongruencj plgrid  100 Feb 16 18:47 poetry_cache\r\ndrwxr-xr-x 2 plgkongruencj plgrid   60 Feb 16 19:42 poetry_config\r\n-rw-r--r-- 1 plgkongruencj plgrid 8.2M Feb 16 15:34 vscode_cli.tar.gz\r\n```\r\n",
  "created_at":"2024-02-16T22:34:39Z",
  "id":1949429325,
  "issue":1130,
  "node_id":"IC_kwDOD6Q_ss50Me5N",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-16T22:34:39Z",
  "user":"MDQ6VXNlcjczNzQ3MzM="
 },
 {
  "author_association":"NONE",
  "body":"The simplest code to reproduce the problem (path to the output file can be adjusted):\r\n\r\n```\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport uproot\r\nntuple_path = Path('/memfs/7680475/file.root')\r\ndata_size = 1000_000_000\r\ndata_dict = {\r\n        \"x\": np.ones(data_size, dtype=np.float64),\r\n}\r\nwith uproot.recreate(ntuple_path, compression=None) as fout:\r\n    fout[\"tree\"] = data_dict\r\n```\r\n\r\nThis snippet gives me the error:\r\n```\r\n---------------------------------------------------------------------------\r\nerror                                     Traceback (most recent call last)\r\nCell In[4], [line 2](vscode-notebook-cell:?execution_count=4&line=2)\r\n      [1](vscode-notebook-cell:?execution_count=4&line=1) with uproot.recreate(ntuple_path, compression=None) as fout:\r\n----> [2](vscode-notebook-cell:?execution_count=4&line=2)     fout[\"tree\"] = data_dict\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:984](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:984), in WritableDirectory.__setitem__(self, where, what)\r\n    [982](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:982) if self._file.sink.closed:\r\n    [983](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:983)     raise ValueError(\"cannot write data to a closed file\")\r\n--> [984](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:984) self.update({where: what})\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1555](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1555), in WritableDirectory.update(self, pairs, **more_pairs)\r\n   [1552](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1552)     for item in path:\r\n   [1553](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1553)         directory = directory[item]\r\n-> [1555](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1555)     uproot.writing.identify.add_to_directory(v, name, directory, streamers)\r\n   [1557](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1557) self._file._cascading.streamers.update_streamers(self._file.sink, streamers)\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:152](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:152), in add_to_directory(obj, name, directory, streamers)\r\n    [150](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:150) if is_ttree:\r\n    [151](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:151)     tree = directory.mktree(name, metadata)\r\n--> [152](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:152)     tree.extend(data)\r\n    [154](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:154) else:\r\n    [155](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:155)     writable = to_writable(obj)\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1834](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1834), in WritableTree.extend(self, data)\r\n   [1807](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1807) def extend(self, data):\r\n   [1808](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1808)     \"\"\"\r\n   [1809](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1809)     Args:\r\n   [1810](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1810)         data (dict of str \\u2192 arrays): More array data to add to the TTree.\r\n   (...)\r\n   [1832](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1832)         **As a word of warning,** be sure that each call to :ref:`uproot.writing.writable.WritableTree.extend` includes at least 100 kB per branch/array. (NumPy and Awkward Arrays have an `nbytes <[https://numpy.org/doc/stable/reference/generated/numpy.ndarray.nbytes.html>`__](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.nbytes.html%3E%60__) property; you want at least ``100000`` per array.) If you ask Uproot to write very small TBaskets, it will spend more time working on TBasket overhead than actually writing data. The absolute worst case is one-entry-per-:ref:`uproot.writing.writable.WritableTree.extend`. See `#428 (comment) <[https://github.com/scikit-hep/uproot5/pull/428#issuecomment-908703486>`__](https://github.com/scikit-hep/uproot5/pull/428#issuecomment-908703486%3E%60__).\r\n   [1833](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1833)     \"\"\"\r\n-> [1834](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1834)     self._cascading.extend(self._file, self._file.sink, data)\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:816](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:816), in Tree.extend(self, file, sink, data)\r\n    [813](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:813)     datum[\"fEntryOffsetLen\"] = 4 * (len(big_endian_offsets) - 1)\r\n    [815](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:815) elif big_endian_offsets is None:\r\n--> [816](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:816)     totbytes, zipbytes, location = self.write_np_basket(\r\n    [817](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:817)         sink, branch_name, compression, big_endian\r\n    [818](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:818)     )\r\n    [819](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:819) else:\r\n    [820](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:820)     totbytes, zipbytes, location = self.write_jagged_basket(\r\n    [821](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:821)         sink, branch_name, compression, big_endian, big_endian_offsets\r\n    [822](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:822)     )\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1399](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1399), in Tree.write_np_basket(self, sink, branch_name, compression, array)\r\n   [1395](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1395) location = self._freesegments.allocate(fNbytes, dry_run=False)\r\n   [1397](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1397) out = []\r\n   [1398](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1398) out.append(\r\n-> [1399](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1399)     uproot.reading._key_format_big.pack(\r\n   [1400](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1400)         fNbytes,\r\n   [1401](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1401)         1004,  # fVersion\r\n   [1402](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1402)         fObjlen,\r\n   [1403](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1403)         uproot._util.datetime_to_code(datetime.datetime.now()),  # fDatime\r\n   [1404](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1404)         fKeylen,\r\n   [1405](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1405)         0,  # fCycle\r\n   [1406](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1406)         location,  # fSeekKey\r\n   [1407](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1407)         parent_location,  # fSeekPdir\r\n   [1408](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1408)     )\r\n   [1409](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1409) )\r\n   [1410](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1410) out.append(fClassName)\r\n   [1411](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1411) out.append(fName)\r\n\r\nerror: 'i' format requires -2147483648 <= number <= 2147483647---------------------------------------------------------------------------\r\nerror                                     Traceback (most recent call last)\r\nCell In[4], [line 2](vscode-notebook-cell:?execution_count=4&line=2)\r\n      [1](vscode-notebook-cell:?execution_count=4&line=1) with uproot.recreate(ntuple_path, compression=None) as fout:\r\n----> [2](vscode-notebook-cell:?execution_count=4&line=2)     fout[\"tree\"] = data_dict\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:984](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:984), in WritableDirectory.__setitem__(self, where, what)\r\n    [982](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:982) if self._file.sink.closed:\r\n    [983](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:983)     raise ValueError(\"cannot write data to a closed file\")\r\n--> [984](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:984) self.update({where: what})\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1555](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1555), in WritableDirectory.update(self, pairs, **more_pairs)\r\n   [1552](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1552)     for item in path:\r\n   [1553](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1553)         directory = directory[item]\r\n-> [1555](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1555)     uproot.writing.identify.add_to_directory(v, name, directory, streamers)\r\n   [1557](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1557) self._file._cascading.streamers.update_streamers(self._file.sink, streamers)\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:152](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:152), in add_to_directory(obj, name, directory, streamers)\r\n    [150](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:150) if is_ttree:\r\n    [151](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:151)     tree = directory.mktree(name, metadata)\r\n--> [152](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:152)     tree.extend(data)\r\n    [154](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:154) else:\r\n    [155](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/identify.py:155)     writable = to_writable(obj)\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1834](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1834), in WritableTree.extend(self, data)\r\n   [1807](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1807) def extend(self, data):\r\n   [1808](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1808)     \"\"\"\r\n   [1809](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1809)     Args:\r\n   [1810](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1810)         data (dict of str \\u2192 arrays): More array data to add to the TTree.\r\n   (...)\r\n   [1832](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1832)         **As a word of warning,** be sure that each call to :ref:`uproot.writing.writable.WritableTree.extend` includes at least 100 kB per branch/array. (NumPy and Awkward Arrays have an `nbytes <[https://numpy.org/doc/stable/reference/generated/numpy.ndarray.nbytes.html>`__](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.nbytes.html%3E%60__) property; you want at least ``100000`` per array.) If you ask Uproot to write very small TBaskets, it will spend more time working on TBasket overhead than actually writing data. The absolute worst case is one-entry-per-:ref:`uproot.writing.writable.WritableTree.extend`. See `#428 (comment) <[https://github.com/scikit-hep/uproot5/pull/428#issuecomment-908703486>`__](https://github.com/scikit-hep/uproot5/pull/428#issuecomment-908703486%3E%60__).\r\n   [1833](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1833)     \"\"\"\r\n-> [1834](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/writable.py:1834)     self._cascading.extend(self._file, self._file.sink, data)\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:816](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:816), in Tree.extend(self, file, sink, data)\r\n    [813](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:813)     datum[\"fEntryOffsetLen\"] = 4 * (len(big_endian_offsets) - 1)\r\n    [815](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:815) elif big_endian_offsets is None:\r\n--> [816](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:816)     totbytes, zipbytes, location = self.write_np_basket(\r\n    [817](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:817)         sink, branch_name, compression, big_endian\r\n    [818](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:818)     )\r\n    [819](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:819) else:\r\n    [820](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:820)     totbytes, zipbytes, location = self.write_jagged_basket(\r\n    [821](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:821)         sink, branch_name, compression, big_endian, big_endian_offsets\r\n    [822](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:822)     )\r\n\r\nFile [/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1399](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1399), in Tree.write_np_basket(self, sink, branch_name, compression, array)\r\n   [1395](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1395) location = self._freesegments.allocate(fNbytes, dry_run=False)\r\n   [1397](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1397) out = []\r\n   [1398](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1398) out.append(\r\n-> [1399](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1399)     uproot.reading._key_format_big.pack(\r\n   [1400](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1400)         fNbytes,\r\n   [1401](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1401)         1004,  # fVersion\r\n   [1402](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1402)         fObjlen,\r\n   [1403](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1403)         uproot._util.datetime_to_code(datetime.datetime.now()),  # fDatime\r\n   [1404](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1404)         fKeylen,\r\n   [1405](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1405)         0,  # fCycle\r\n   [1406](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1406)         location,  # fSeekKey\r\n   [1407](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1407)         parent_location,  # fSeekPdir\r\n   [1408](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1408)     )\r\n   [1409](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1409) )\r\n   [1410](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1410) out.append(fClassName)\r\n   [1411](https://vscode-remote+tunnel-002bac0018.vscode-resource.vscode-cdn.net/memfs/7680475/poetry_cache/virtualenvs/2022-krakow-lgad-_qGHPVZk-py3.11/lib/python3.11/site-packages/uproot/writing/_cascadetree.py:1411) out.append(fName)\r\n\r\nerror: 'i' format requires -2147483648 <= number <= 2147483647\r\n```",
  "created_at":"2024-02-19T11:59:13Z",
  "id":1952300656,
  "issue":1130,
  "node_id":"IC_kwDOD6Q_ss50Xb5w",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-19T11:59:13Z",
  "user":"MDQ6VXNlcjczNzQ3MzM="
 },
 {
  "author_association":"NONE",
  "body":"Interesting, I've tried as well the same simple code but with compression enabled:\r\n\r\n```python\r\nfrom pathlib import Path\r\nimport numpy as np\r\nimport uproot\r\nntuple_path = Path('file.root')\r\ndata_size = 1000_000_000\r\ndata_dict = {\r\n        \"x\": np.ones(data_size, dtype=np.float64),\r\n}\r\nwith uproot.recreate(ntuple_path) as fout:\r\n    fout[\"tree\"] = data_dict\r\n```\r\n\r\nThe code took 20 min to run on my cluster, used ~40GB of RAM and crashed with:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/memfs/7685922/bug.py\", line 10, in <module>\r\n    fout[\"tree\"] = data_dict\r\n    ~~~~^^^^^^^^\r\n  File \"/memfs/7685922/venv/lib/python3.11/site-packages/uproot/writing/writable.py\", line 984, in __setitem__\r\n    self.update({where: what})\r\n  File \"/memfs/7685922/venv/lib/python3.11/site-packages/uproot/writing/writable.py\", line 1555, in update\r\n    uproot.writing.identify.add_to_directory(v, name, directory, streamers)\r\n  File \"/memfs/7685922/venv/lib/python3.11/site-packages/uproot/writing/identify.py\", line 152, in add_to_directory\r\n    tree.extend(data)\r\n  File \"/memfs/7685922/venv/lib/python3.11/site-packages/uproot/writing/writable.py\", line 1834, in extend\r\n    self._cascading.extend(self._file, self._file.sink, data)\r\n  File \"/memfs/7685922/venv/lib/python3.11/site-packages/uproot/writing/_cascadetree.py\", line 816, in extend\r\n    totbytes, zipbytes, location = self.write_np_basket(\r\n                                   ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/memfs/7685922/venv/lib/python3.11/site-packages/uproot/writing/_cascadetree.py\", line 1399, in write_np_basket\r\n    uproot.reading._key_format_big.pack(\r\nstruct.error: 'i' format requires -2147483648 <= number <= 2147483647\r\n```\r\n",
  "created_at":"2024-02-19T14:34:50Z",
  "id":1952577013,
  "issue":1130,
  "node_id":"IC_kwDOD6Q_ss50YfX1",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-19T14:34:50Z",
  "user":"MDQ6VXNlcjczNzQ3MzM="
 },
 {
  "author_association":"MEMBER",
  "body":"I've been meaning to get back to this. Maybe we could add an error message, but the ROOT format itself does not allow TBaskets to be bigger than 2 GB because both the `fNbytes` (compressed size) and `fObjlen` (uncompressed size) are 32-bit integers. Here's where it's trying to write a TKey for the TBasket (where you see the exception):\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/94c085b392d66f01497d12d116a13d567543c909/src/uproot/writing/_cascadetree.py#L1399-L1408\r\n\r\nand here's the definition of `_key_format_big` (a \"big\" TKey uses 64-bit integers for the _location_ of the TBasket, so that _files_ can be bigger than 2 GB, but no single object, such as a TBasket, can be that large).\r\n\r\nhttps://github.com/scikit-hep/uproot5/blob/94c085b392d66f01497d12d116a13d567543c909/src/uproot/reading.py#L2196\r\n\r\nHere's the ROOT definition of a TKey:\r\n\r\nhttps://root.cern.ch/doc/master/classTKey.html#ab2e59bcc49663466e74286cabd3d42c1\r\n\r\nin which `fNbytes` and `fObjlen` are declared to be type `Int_t`, which is 32-bits.\r\n\r\n------------------\r\n\r\nDo you know that\r\n\r\n```python\r\nfile[\"tree_name\"] = {\"branch\": branch_data}\r\n```\r\n\r\nwrites all of the `branch_data` as one TBasket? TBaskets are the granular unit of reading and writing ROOT TTrees, so if you write all of the data in one TBasket, any reader (ROOT, Uproot, UnROOT) will have to read it all into memory at once\u2014ROOT TTrees can only be read piecemeal if they're written as multiple TBaskets. In Uproot, the way to do that is\r\n\r\n```python\r\nfile[\"tree_name\"] = {\"branch\": first_basket}\r\nfile[\"tree_name\"].extend({\"branch\": second_basket})\r\nfile[\"tree_name\"].extend({\"branch\": third_basket})\r\n...\r\n```\r\n\r\nIt can't be an interface that takes all of the data in one call because the TBasket data might not fit in memory, especially if you have many TBranches (each with one TBasket). This interface is [documented here](https://uproot.readthedocs.io/en/latest/basic.html#writing-ttrees-to-a-file).\r\n\r\n--------------\r\n\r\nIn most files, ROOT TBaskets tend to be too small: they tend to be on the order of kilobytes, when it would be more efficient to read if they were megabytes. If you ask ROOT to make big TBaskets, on the order of megabytes or bigger, it just doesn't do it\u2014there seems to be some internal limit. Uproot does exactly what you ask, and you were asking for gigabyte-sized TBaskets. If you didn't run into the 2 GB limit, I wonder if ROOT would be able to read them. Since it prevents the writing of TBaskets that large, I wouldn't be surprised if there's an implicit assumption in the reading code. Did you ever write 1 GB TBaskets and then read them back in ROOT?\r\n\r\n--------------\r\n\r\nAbout this issue, I think I can close it because the format simply doesn't accept integers of that size, and most likely, you intended to write multiple TBaskets with [uproot.WritableTree.extend](https://uproot.readthedocs.io/en/latest/uproot.writing.writable.WritableTree.html#extend).",
  "created_at":"2024-02-19T15:59:21Z",
  "id":1952765439,
  "issue":1130,
  "node_id":"IC_kwDOD6Q_ss50ZNX_",
  "performed_via_github_app":null,
  "reactions":{},
  "updated_at":"2024-02-19T15:59:21Z",
  "user":"MDQ6VXNlcjE4NTI0NDc="
 }
]